{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef discovery_redis(self):\n        # Install the bundle\n        self.context.install_bundle(\"pelix.remote.discovery.redis\").start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_DISCOVERY_REDIS,\n                \"pelix-discovery-redis\",\n                {\n                    \"application.id\": \"sample.rs\",\n                    \"redis.host\": self.arguments.redis_host,\n                    \"redis.port\": self.arguments.redis_port,\n                },\n            )", "response": "Installs the Redis discovery bundles and instantiates components\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall the ZooKeeper discovery bundles and instantiates components", "response": "def discovery_zookeeper(self):\n        \"\"\"\n        Installs the ZooKeeper discovery bundles and instantiates components\n        \"\"\"\n        # Install the bundle\n        self.context.install_bundle(\"pelix.remote.discovery.zookeeper\").start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_DISCOVERY_ZOOKEEPER,\n                \"pelix-discovery-zookeeper\",\n                {\n                    \"application.id\": \"sample.rs\",\n                    \"zookeeper.hosts\": self.arguments.zk_hosts,\n                    \"zookeeper.prefix\": self.arguments.zk_prefix,\n                },\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling the JSON - RPC transport bundles and instantiates components", "response": "def transport_jsonrpc(self):\n        \"\"\"\n        Installs the JSON-RPC transport bundles and instantiates components\n        \"\"\"\n        # Install the bundle\n        self.context.install_bundle(\"pelix.remote.json_rpc\").start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_JSONRPC_EXPORTER, \"pelix-jsonrpc-exporter\"\n            )\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_JSONRPC_IMPORTER, \"pelix-jsonrpc-importer\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls the JABSORB - RPC transport bundles and instantiates components", "response": "def transport_jabsorbrpc(self):\n        \"\"\"\n        Installs the JABSORB-RPC transport bundles and instantiates components\n        \"\"\"\n        # Install the bundle\n        self.context.install_bundle(\n            \"pelix.remote.transport.jabsorb_rpc\"\n        ).start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_JABSORBRPC_EXPORTER,\n                \"pelix-jabsorbrpc-exporter\",\n            )\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_JABSORBRPC_IMPORTER,\n                \"pelix-jabsorbrpc-importer\",\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transport_mqttrpc(self):\n        # Install the bundle\n        self.context.install_bundle(\"pelix.remote.transport.mqtt_rpc\").start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_MQTTRPC_EXPORTER,\n                \"pelix-mqttrpc-exporter\",\n                {\n                    \"mqtt.host\": self.arguments.mqtt_host,\n                    \"mqtt.port\": self.arguments.mqtt_port,\n                },\n            )\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_MQTTRPC_IMPORTER,\n                \"pelix-mqttrpc-importer\",\n                {\n                    \"mqtt.host\": self.arguments.mqtt_host,\n                    \"mqtt.port\": self.arguments.mqtt_port,\n                },\n            )", "response": "Creates the MQTT - RPC transport bundles and instantiates components\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninstalls the XML - RPC transport bundles and instantiates components", "response": "def transport_xmlrpc(self):\n        \"\"\"\n        Installs the XML-RPC transport bundles and instantiates components\n        \"\"\"\n        # Install the bundle\n        self.context.install_bundle(\"pelix.remote.xml_rpc\").start()\n\n        with use_waiting_list(self.context) as ipopo:\n            # Instantiate the discovery\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_XMLRPC_EXPORTER, \"pelix-xmlrpc-exporter\"\n            )\n            ipopo.add(\n                rs.FACTORY_TRANSPORT_XMLRPC_IMPORTER, \"pelix-xmlrpc-importer\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_handlers(self, component_context, instance):\n        # Extract information from the context\n        requirements = component_context.get_handler(\n            ipopo_constants.HANDLER_REQUIRES_BEST\n        )\n        requires_filters = component_context.properties.get(\n            ipopo_constants.IPOPO_REQUIRES_FILTERS, None\n        )\n\n        # Prepare requirements\n        requirements = self._prepare_requirements(\n            requirements, requires_filters\n        )\n\n        # Set up the runtime dependency handlers\n        return [\n            BestDependency(field, requirement)\n            for field, requirement in requirements.items()\n        ]", "response": "Returns the list of handlers associated to the given component."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when a service is arrived in the framework.", "response": "def on_service_arrival(self, svc_ref):\n        \"\"\"\n        Called when a service has been registered in the framework\n\n        :param svc_ref: A service reference\n        \"\"\"\n        with self._lock:\n            new_ranking = svc_ref.get_property(SERVICE_RANKING)\n            if self._current_ranking is not None:\n                if new_ranking > self._current_ranking:\n                    # New service with better ranking: use it\n                    self._pending_ref = svc_ref\n                    old_ref = self.reference\n                    old_value = self._value\n\n                    # Clean up like for a departure\n                    self._current_ranking = None\n                    self._value = None\n                    self.reference = None\n\n                    # Unbind (new binding will be done afterwards)\n                    self._ipopo_instance.unbind(self, old_value, old_ref)\n            else:\n                # No ranking yet: inject the service\n                self.reference = svc_ref\n                self._value = self._context.get_service(svc_ref)\n                self._current_ranking = new_ranking\n                self._pending_ref = None\n\n                self._ipopo_instance.bind(self, self._value, self.reference)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_service_departure(self, svc_ref):\n        with self._lock:\n            if svc_ref is self.reference:\n                # Injected service going away...\n                service = self._value\n\n                # Clear the instance values\n                self._current_ranking = None\n                self._value = None\n                self.reference = None\n\n                if self.requirement.immediate_rebind:\n                    # Look for a replacement\n                    self._pending_ref = self._context.get_service_reference(\n                        self.requirement.specification, self.requirement.filter\n                    )\n                else:\n                    self._pending_ref = None\n\n                self._ipopo_instance.unbind(self, service, svc_ref)", "response": "Called when a service is deactivated from the framework"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_service_modify(self, svc_ref, old_properties):\n        with self._lock:\n            if self.reference is None:\n                # A previously registered service now matches our filter\n                return self.on_service_arrival(svc_ref)\n            else:\n                # Check if the ranking changed the service to inject\n                best_ref = self._context.get_service_reference(\n                    self.requirement.specification, self.requirement.filter\n                )\n                if best_ref is self.reference:\n                    # Still the best service: notify the property modification\n                    if svc_ref is self.reference:\n                        # Call update only if necessary\n                        self._ipopo_instance.update(\n                            self, self._value, svc_ref, old_properties\n                        )\n                else:\n                    # A new service is now the best: start a departure loop\n                    self.on_service_departure(self.reference)\n\n            return None", "response": "Called when a service has been modified in the framework"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_properties(self):\n        with self.__lock:\n            if self.__deleted:\n                raise ValueError(\"{0} has been deleted\".format(self.__pid))\n\n            elif not self.__updated:\n                # Fresh configuration\n                return None\n\n            # Filter a copy of the properties\n            props = self.__properties.copy()\n\n            try:\n                del props[services.CONFIG_PROP_BUNDLE_LOCATION]\n            except KeyError:\n                # Ignore\n                pass\n\n            return props", "response": "Returns the properties of this Configuration object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __properties_update(self, properties):\n        if not properties:\n            # Nothing to do\n            return False\n\n        with self.__lock:\n            # Make a copy of the properties\n            properties = properties.copy()\n\n            # Override properties\n            properties[services.CONFIG_PROP_PID] = self.__pid\n\n            if self.__location:\n                properties[\n                    services.CONFIG_PROP_BUNDLE_LOCATION\n                ] = self.__location\n\n            if self.__factory_pid:\n                properties[\n                    services.CONFIG_PROP_FACTORY_PID\n                ] = self.__factory_pid\n\n            # See if new properties are different\n            if properties == self.__properties:\n                return False\n\n            # Store the copy (before storing data)\n            self.__properties = properties\n            self.__updated = True\n\n            # Store the data\n            # it will cause FileInstall to update this configuration again, but\n            # this will ignored because self.__properties has already been\n            # saved\n            self.__persistence.store(self.__pid, properties)\n            return True", "response": "Internal method to update the properties of the current configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the properties of this Configuration object.", "response": "def update(self, properties=None):\n        # pylint: disable=W0212\n        \"\"\"\n        If called without properties, only notifies listeners\n\n        Update the properties of this Configuration object.\n        Stores the properties in persistent storage after adding or overwriting\n        the following properties:\n\n        * \"service.pid\" : is set to be the PID of this configuration.\n        * \"service.factoryPid\" : if this is a factory configuration it is set\n          to the factory PID else it is not set.\n\n        These system properties are all of type String.\n\n        If the corresponding Managed Service/Managed Service Factory is\n        registered, its updated method must be called asynchronously.\n        Else, this callback is delayed until aforementioned registration\n        occurs.\n\n        Also initiates an asynchronous call to all ConfigurationListeners with\n        a ConfigurationEvent.CM_UPDATED event.\n\n        :param properties: the new set of properties for this configuration\n        :raise IOError: Error storing the configuration\n        \"\"\"\n        with self.__lock:\n            # Update properties\n            if self.__properties_update(properties):\n                # Update configurations, if something changed\n                self.__config_admin._update(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, directory_updated=False):\n        # pylint: disable=W0212\n        \"\"\"\n        Delete this configuration\n\n        :param directory_updated: If True, tell ConfigurationAdmin to not\n                                  recall the directory of this deletion\n                                  (internal use only)\n        \"\"\"\n        with self.__lock:\n            if self.__deleted:\n                # Nothing to do\n                return\n\n            # Update status\n            self.__deleted = True\n\n            # Notify ConfigurationAdmin, notify services only if the\n            # configuration had been updated before\n            self.__config_admin._delete(self, self.__updated, directory_updated)\n\n            # Remove the file\n            self.__persistence.delete(self.__pid)\n\n            # Clean up\n            if self.__properties:\n                self.__properties.clear()\n\n            self.__persistence = None\n            self.__pid = None", "response": "Delete this configuration from the persistence and properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef matches(self, ldap_filter):\n        if not self.is_valid():\n            # Do not test invalid configurations\n            return False\n\n        return ldap_filter.matches(self.__properties)", "response": "Tests if this configuration matches the given LDAP filter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef walk_modules(path):\n    if path is None or not os.path.isdir(path):\n        return\n\n    yielded = set()\n    try:\n        file_names = os.listdir(path)\n    except OSError:\n        # ignore unreadable directories like import does\n        file_names = []\n\n    # handle packages before same-named modules\n    file_names.sort()\n\n    for filename in file_names:\n        modname = inspect.getmodulename(filename)\n        if modname == \"__init__\" or modname in yielded:\n            continue\n\n        file_path = os.path.join(path, filename)\n        is_package = False\n\n        if not modname and os.path.isdir(file_path) and \".\" not in filename:\n            modname = filename\n            try:\n                dir_contents = os.listdir(file_path)\n            except OSError:\n                # ignore unreadable directories like import does\n                dir_contents = []\n\n            for sub_filename in dir_contents:\n                sub_name = inspect.getmodulename(sub_filename)\n                if sub_name == \"__init__\":\n                    is_package = True\n                    break\n            else:\n                # not a package\n                continue\n\n        if modname and \".\" not in modname:\n            yielded.add(modname)\n            yield modname, is_package", "response": "Generator to walk through a folder and yields all loadable packages and modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_framework(\n    bundles,\n    properties=None,\n    auto_start=False,\n    wait_for_stop=False,\n    auto_delete=False,\n):\n    # type: (Union[list, tuple], dict, bool, bool, bool) -> Framework\n    \"\"\"\n    Creates a Pelix framework, installs the given bundles and returns its\n    instance reference.\n    If *auto_start* is True, the framework will be started once all bundles\n    will have been installed\n    If *wait_for_stop* is True, the method will return only when the framework\n    will have stopped. This requires *auto_start* to be True.\n    If *auto_delete* is True, the framework will be deleted once it has\n    stopped, and the method will return None.\n    This requires *wait_for_stop* and *auto_start* to be True.\n\n    :param bundles: Bundles to initially install (shouldn't be empty if\n                    *wait_for_stop* is True)\n    :param properties: Optional framework properties\n    :param auto_start: If True, the framework will be started immediately\n    :param wait_for_stop: If True, the method will return only when the\n                          framework will have stopped\n    :param auto_delete: If True, deletes the framework once it stopped.\n    :return: The framework instance\n    :raise ValueError: Only one framework can run at a time\n    \"\"\"\n    # Test if a framework already exists\n    if FrameworkFactory.is_framework_running(None):\n        raise ValueError(\"A framework is already running\")\n\n    # Create the framework\n    framework = FrameworkFactory.get_framework(properties)\n\n    # Install bundles\n    context = framework.get_bundle_context()\n    for bundle in bundles:\n        context.install_bundle(bundle)\n\n    if auto_start:\n        # Automatically start the framework\n        framework.start()\n\n        if wait_for_stop:\n            # Wait for the framework to stop\n            try:\n                framework.wait_for_stop(None)\n            except KeyboardInterrupt:\n                # Stop keyboard interruptions\n                if framework.get_state() == Bundle.ACTIVE:\n                    framework.stop()\n\n            if auto_delete:\n                # Delete the framework\n                FrameworkFactory.delete_framework(framework)\n                framework = None\n\n    return framework", "response": "Creates a Pelix framework and returns its base object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _package_exists(path):\n    # type: (str) -> bool\n    \"\"\"\n    Checks if the given Python path matches a valid file or a valid container\n    file\n\n    :param path: A Python path\n    :return: True if the module or its container exists\n    \"\"\"\n    while path:\n        if os.path.exists(path):\n            return True\n        else:\n            path = os.path.dirname(path)\n\n    return False", "response": "Checks if the given Python path matches a valid file or a valid container\n    file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes sys. path to avoid the use of relative folders", "response": "def normalize_path():\n    \"\"\"\n    Normalizes sys.path to avoid the use of relative folders\n    \"\"\"\n    # Normalize Python paths\n    whole_path = [\n        os.path.abspath(path) for path in sys.path if os.path.exists(path)\n    ]\n\n    # Keep the \"dynamic\" current folder indicator and add the \"static\"\n    # current path\n    # Use an OrderedDict to have a faster lookup (path not in whole_set)\n    whole_set = collections.OrderedDict(((\"\", 1), (os.getcwd(), 1)))\n\n    # Add original path entries\n    for path in whole_path:\n        if path not in whole_set:\n            whole_set[path] = 1\n\n    # Set the new content of sys.path (still ordered thanks to OrderedDict)\n    sys.path = list(whole_set)\n\n    # Normalize paths in loaded modules\n    for module_ in sys.modules.values():\n        try:\n            module_.__path__ = [\n                os.path.abspath(path)\n                for path in module_.__path__\n                if _package_exists(path)\n            ]\n        except AttributeError:\n            # builtin modules don't have a __path__\n            pass\n        except ImportError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the requested method of the activator or returns None if no method is available.", "response": "def __get_activator_method(self, method_name):\n        \"\"\"\n        Retrieves the requested method of the activator, or returns None\n\n        :param method_name: A method name\n        :return: A method, or None\n        \"\"\"\n        # Get the activator\n        activator = getattr(self.__module, ACTIVATOR, None)\n        if activator is None:\n            # Get the old activator\n            activator = getattr(self.__module, ACTIVATOR_LEGACY, None)\n            if activator is not None:\n                # Old activator found: print a deprecation warning\n                _logger.warning(\n                    \"Bundle %s uses the deprecated '%s' to declare\"\n                    \" its activator. Use @BundleActivator instead.\",\n                    self.__name,\n                    ACTIVATOR_LEGACY,\n                )\n        return getattr(activator, method_name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fire_bundle_event(self, kind):\n        # type: (int) -> None\n        \"\"\"\n        Fires a bundle event of the given kind\n\n        :param kind: Kind of event\n        \"\"\"\n        self.__framework._dispatcher.fire_bundle_event(BundleEvent(kind, self))", "response": "Fires a bundle event of the given kind"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning this bundle s ServiceReference list for all services it has registered or an empty list if the bundle is uninstalled.", "response": "def get_registered_services(self):\n        # type: () -> List[ServiceReference]\n        \"\"\"\n        Returns this bundle's ServiceReference list for all services it has\n        registered or an empty list\n\n        The list is valid at the time of the call to this method, however, as\n        the Framework is a very dynamic environment, services can be modified\n        or unregistered at any time.\n\n        :return: An array of ServiceReference objects\n        :raise BundleException: If the bundle has been uninstalled\n        \"\"\"\n        if self._state == Bundle.UNINSTALLED:\n            raise BundleException(\n                \"Can't call 'get_registered_services' on an \"\n                \"uninstalled bundle\"\n            )\n        return self.__framework._registry.get_bundle_registered_services(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning this bundle s ServiceReference list for all services it is using or an empty list if the bundle is uninstalled.", "response": "def get_services_in_use(self):\n        # type: () -> List[ServiceReference]\n        \"\"\"\n        Returns this bundle's ServiceReference list for all services it is\n        using or an empty list.\n        A bundle is considered to be using a service if its use count for that\n        service is greater than zero.\n\n        The list is valid at the time of the call to this method, however, as\n        the Framework is a very dynamic environment, services can be modified\n        or unregistered at any time.\n\n        :return: An array of ServiceReference objects\n        :raise BundleException: If the bundle has been uninstalled\n        \"\"\"\n        if self._state == Bundle.UNINSTALLED:\n            raise BundleException(\n                \"Can't call 'get_services_in_use' on an uninstalled bundle\"\n            )\n        return self.__framework._registry.get_bundle_imported_services(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the bundle version using the version of the module.", "response": "def get_version(self):\n        # type: () -> str\n        \"\"\"\n        Retrieves the bundle version, using the ``__version__`` or\n        ``__version_info__`` attributes of its module.\n\n        :return: The bundle version, \"0.0.0\" by default\n        \"\"\"\n        # Get the version value\n        version = getattr(self.__module, \"__version__\", None)\n        if version:\n            return version\n\n        # Convert the __version_info__ entry\n        info = getattr(self.__module, \"__version_info__\", None)\n        if info:\n            return \".\".join(str(part) for part in __version_info__)\n\n        # No version\n        return \"0.0.0\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the bundle. Does nothing if the bundle is already starting or active. :raise BundleException: The framework is not yet started or the bundle activator failed.", "response": "def start(self):\n        \"\"\"\n        Starts the bundle. Does nothing if the bundle is already starting or\n        active.\n\n        :raise BundleException: The framework is not yet started or the bundle\n                                activator failed.\n        \"\"\"\n        if self.__framework._state not in (Bundle.STARTING, Bundle.ACTIVE):\n            # Framework is not running\n            raise BundleException(\n                \"Framework must be started before its bundles\"\n            )\n\n        with self._lock:\n            if self._state in (Bundle.ACTIVE, Bundle.STARTING):\n                # Already started bundle, do nothing\n                return\n\n            # Store the bundle current state\n            previous_state = self._state\n\n            # Starting...\n            self._state = Bundle.STARTING\n            self._fire_bundle_event(BundleEvent.STARTING)\n\n            # Call the activator, if any\n            starter = self.__get_activator_method(\"start\")\n            if starter is not None:\n                try:\n                    # Call the start method\n                    starter(self.__context)\n                except (FrameworkException, BundleException):\n                    # Restore previous state\n                    self._state = previous_state\n\n                    # Re-raise directly Pelix exceptions\n                    _logger.exception(\n                        \"Pelix error raised by %s while starting\", self.__name\n                    )\n                    raise\n                except Exception as ex:\n                    # Restore previous state\n                    self._state = previous_state\n\n                    # Raise the error\n                    _logger.exception(\n                        \"Error raised by %s while starting\", self.__name\n                    )\n                    raise BundleException(ex)\n\n            # Bundle is now active\n            self._state = Bundle.ACTIVE\n            self._fire_bundle_event(BundleEvent.STARTED)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop the bundle. Does nothing if the bundle is already stopped. :raise BundleException: The bundle activator failed.", "response": "def stop(self):\n        \"\"\"\n        Stops the bundle. Does nothing if the bundle is already stopped.\n\n        :raise BundleException: The bundle activator failed.\n        \"\"\"\n        if self._state != Bundle.ACTIVE:\n            # Invalid state\n            return\n\n        exception = None\n        with self._lock:\n            # Store the bundle current state\n            previous_state = self._state\n\n            # Stopping...\n            self._state = Bundle.STOPPING\n            self._fire_bundle_event(BundleEvent.STOPPING)\n\n            # Call the activator, if any\n            stopper = self.__get_activator_method(\"stop\")\n            if stopper is not None:\n                try:\n                    # Call the start method\n                    stopper(self.__context)\n                except (FrameworkException, BundleException) as ex:\n                    # Restore previous state\n                    self._state = previous_state\n\n                    # Re-raise directly Pelix exceptions\n                    _logger.exception(\n                        \"Pelix error raised by %s while stopping\", self.__name\n                    )\n                    exception = ex\n                except Exception as ex:\n                    _logger.exception(\n                        \"Error raised by %s while stopping\", self.__name\n                    )\n                    # Store the exception (raised after service clean up)\n                    exception = BundleException(ex)\n\n            # Hide remaining services\n            self.__framework._hide_bundle_services(self)\n\n            # Intermediate bundle event : activator should have cleaned up\n            # everything, but some element could stay (iPOPO components, ...)\n            self._fire_bundle_event(BundleEvent.STOPPING_PRECLEAN)\n\n            # Remove remaining services (the hard way)\n            self.__unregister_services()\n\n            # Cleanup service usages\n            self.__framework._unget_used_services(self)\n\n            # Bundle is now stopped and all its services have been unregistered\n            self._state = Bundle.RESOLVED\n            self._fire_bundle_event(BundleEvent.STOPPED)\n\n        # Raise the exception, if any\n        # pylint: disable=E0702\n        # Pylint seems to miss the \"is not None\" check below\n        if exception is not None:\n            raise exception"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the bundle with the latest information from the module.", "response": "def update(self):\n        \"\"\"\n        Updates the bundle\n        \"\"\"\n        with self._lock:\n            # Was it active ?\n            restart = self._state == Bundle.ACTIVE\n\n            # Send the update event\n            self._fire_bundle_event(BundleEvent.UPDATE_BEGIN)\n\n            try:\n                # Stop the bundle\n                self.stop()\n            except:\n                # Something wrong occurred, notify listeners\n                self._fire_bundle_event(BundleEvent.UPDATE_FAILED)\n                raise\n\n            # Change the source file age\n            module_stat = None\n            module_file = getattr(self.__module, \"__file__\", None)\n            if module_file is not None and os.path.isfile(module_file):\n                try:\n                    module_stat = os.stat(module_file)\n\n                    # Change modification time to bypass weak time resolution\n                    # of the underlying file system\n                    os.utime(\n                        module_file,\n                        (module_stat.st_atime, module_stat.st_mtime + 1),\n                    )\n                except OSError:\n                    # Can't touch the file\n                    _logger.warning(\n                        \"Failed to update the modification time of '%s'. \"\n                        \"The bundle update might not reflect the latest \"\n                        \"changes.\",\n                        module_file,\n                    )\n\n            # Clean up the module constants (otherwise kept by reload)\n            # Keep special members (__name__, __file__, ...)\n            old_content = self.__module.__dict__.copy()\n            for name in list(self.__module.__dict__):\n                if not (name.startswith(\"__\") and name.endswith(\"__\")):\n                    del self.__module.__dict__[name]\n\n            try:\n                # Reload the module\n                reload_module(self.__module)\n            except (ImportError, SyntaxError) as ex:\n                # Exception raised if the file is unreadable\n                _logger.exception(\"Error updating %s: %s\", self.__name, ex)\n\n                # Reset module content\n                self.__module.__dict__.clear()\n                self.__module.__dict__.update(old_content)\n\n            if module_stat is not None:\n                try:\n                    # Reset times\n                    os.utime(\n                        module_file,\n                        (module_stat.st_atime, module_stat.st_mtime),\n                    )\n                except OSError:\n                    # Shouldn't occur, since we succeeded before the update\n                    _logger.debug(\n                        \"Failed to reset the modification time of '%s'\",\n                        module_file,\n                    )\n\n            if restart:\n                try:\n                    # Re-start the bundle\n                    self.start()\n                except:\n                    # Something wrong occurred, notify listeners\n                    self._fire_bundle_event(BundleEvent.UPDATE_FAILED)\n                    raise\n\n            # Bundle update finished\n            self._fire_bundle_event(BundleEvent.UPDATED)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_property(self, name, value):\n        # type: (str, object) -> bool\n        \"\"\"\n        Adds a property to the framework **if it is not yet set**.\n\n        If the property already exists (same name), then nothing is done.\n        Properties can't be updated.\n\n        :param name: The property name\n        :param value: The value to set\n        :return: True if the property was stored, else False\n        \"\"\"\n        with self.__properties_lock:\n            if name in self.__properties:\n                # Already stored property\n                return False\n\n            self.__properties[name] = value\n            return True", "response": "Adds a property to the framework if it is not already set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of service references matching the given filter.", "response": "def find_service_references(\n        self, clazz=None, ldap_filter=None, only_one=False\n    ):\n        # type: (Optional[str], Optional[str], bool) -> Optional[List[ServiceReference]]\n        \"\"\"\n        Finds all services references matching the given filter.\n\n        :param clazz: Class implemented by the service\n        :param ldap_filter: Service filter\n        :param only_one: Return the first matching service reference only\n        :return: A list of found reference, or None\n        :raise BundleException: An error occurred looking for service\n                                references\n        \"\"\"\n        return self._registry.find_service_references(\n            clazz, ldap_filter, only_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bundle_by_id(self, bundle_id):\n        # type: (int) -> Union[Bundle, Framework]\n        \"\"\"\n        Retrieves the bundle with the given ID\n\n        :param bundle_id: ID of an installed bundle\n        :return: The requested bundle\n        :raise BundleException: The ID is invalid\n        \"\"\"\n        if bundle_id == 0:\n            # \"System bundle\"\n            return self\n\n        with self.__bundles_lock:\n            if bundle_id not in self.__bundles:\n                raise BundleException(\"Invalid bundle ID {0}\".format(bundle_id))\n\n            return self.__bundles[bundle_id]", "response": "Returns the bundle with the given ID or raises BundleException if the bundle is not installed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bundle_by_name(self, bundle_name):\n        # type: (str) -> Optional[Bundle]\n        \"\"\"\n        Retrieves the bundle with the given name\n\n        :param bundle_name: Name of the bundle to look for\n        :return: The requested bundle, None if not found\n        \"\"\"\n        if bundle_name is None:\n            # Nothing to do\n            return None\n\n        if bundle_name is self.get_symbolic_name():\n            # System bundle requested\n            return self\n\n        with self.__bundles_lock:\n            for bundle in self.__bundles.values():\n                if bundle_name == bundle.get_symbolic_name():\n                    # Found !\n                    return bundle\n\n            # Not found...\n            return None", "response": "Returns the bundle with the given name or None if not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bundles(self):\n        # type: () -> List[Bundle]\n        \"\"\"\n        Returns the list of all installed bundles\n\n        :return: the list of all installed bundles\n        \"\"\"\n        with self.__bundles_lock:\n            return [\n                self.__bundles[bundle_id]\n                for bundle_id in sorted(self.__bundles.keys())\n            ]", "response": "Returns the list of all installed bundles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_property(self, name):\n        # type: (str) -> object\n        \"\"\"\n        Retrieves a framework or system property. As framework properties don't\n        change while it's running, this method don't need to be protected.\n\n        :param name: The property name\n        \"\"\"\n        with self.__properties_lock:\n            return self.__properties.get(name, os.getenv(name))", "response": "Retrieves a framework or system property."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the service corresponding to the given bundle and reference.", "response": "def get_service(self, bundle, reference):\n        # type: (Bundle, ServiceReference) -> Any\n        \"\"\"\n        Retrieves the service corresponding to the given reference\n\n        :param bundle: The bundle requiring the service\n        :param reference: A service reference\n        :return: The requested service\n        :raise BundleException: The service could not be found\n        :raise TypeError: The argument is not a ServiceReference object\n        \"\"\"\n        if not isinstance(bundle, Bundle):\n            raise TypeError(\"First argument must be a Bundle object\")\n        elif not isinstance(reference, ServiceReference):\n            raise TypeError(\"Second argument must be a ServiceReference object\")\n\n        try:\n            # Unregistering service, just give it\n            return self.__unregistering_services[reference]\n        except KeyError:\n            return self._registry.get_service(bundle, reference)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_bundle(self, name, path=None):\n        # type: (str, str) -> Bundle\n        \"\"\"\n        Installs the bundle with the given name\n\n        *Note:* Before Pelix 0.5.0, this method returned the ID of the\n        installed bundle, instead of the Bundle object.\n\n        **WARNING:** The behavior of the loading process is subject to changes,\n        as it does not allow to safely run multiple frameworks in the same\n        Python interpreter, as they might share global module values.\n\n        :param name: A bundle name\n        :param path: Preferred path to load the module\n        :return: The installed Bundle object\n        :raise BundleException: Something happened\n        \"\"\"\n        with self.__bundles_lock:\n            # A bundle can't be installed twice\n            for bundle in self.__bundles.values():\n                if bundle.get_symbolic_name() == name:\n                    _logger.debug(\"Already installed bundle: %s\", name)\n                    return bundle\n\n            # Load the module\n            try:\n                if path:\n                    # Use the given path in priority\n                    sys.path.insert(0, path)\n\n                try:\n                    # The module has already been loaded\n                    module_ = sys.modules[name]\n                except KeyError:\n                    # Load the module\n                    #  __import__(name) -> package level\n                    # import_module -> module level\n                    module_ = importlib.import_module(name)\n            except (ImportError, IOError) as ex:\n                # Error importing the module\n                raise BundleException(\n                    \"Error installing bundle {0}: {1}\".format(name, ex)\n                )\n            finally:\n                if path:\n                    # Clean up the path. The loaded module(s) might\n                    # have changed the path content, so do not use an\n                    # index\n                    sys.path.remove(path)\n\n            # Add the module to sys.modules, just to be sure\n            sys.modules[name] = module_\n\n            # Compute the bundle ID\n            bundle_id = self.__next_bundle_id\n\n            # Prepare the bundle object and its context\n            bundle = Bundle(self, bundle_id, name, module_)\n\n            # Store the bundle\n            self.__bundles[bundle_id] = bundle\n\n            # Update the bundle ID counter\n            self.__next_bundle_id += 1\n\n        # Fire the bundle installed event\n        event = BundleEvent(BundleEvent.INSTALLED, bundle)\n        self._dispatcher.fire_bundle_event(event)\n        return bundle", "response": "Installs a bundle with the given name and returns the ID of the bundle object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall all the modules found in the given path and returns a list of bundles and the list of failed modules names.", "response": "def install_package(self, path, recursive=False, prefix=None):\n        # type: (str, bool, str) -> tuple\n        \"\"\"\n        Installs all the modules found in the given package\n\n        :param path: Path of the package (folder)\n        :param recursive: If True, install the sub-packages too\n        :param prefix: (**internal**) Prefix for all found modules\n        :return: A 2-tuple, with the list of installed bundles and the list\n                 of failed modules names\n        :raise ValueError: Invalid path\n        \"\"\"\n        if not path:\n            raise ValueError(\"Empty path\")\n        elif not is_string(path):\n            raise ValueError(\"Path must be a string\")\n\n        # Use an absolute path\n        path = os.path.abspath(path)\n        if not os.path.exists(path):\n            raise ValueError(\"Nonexistent path: {0}\".format(path))\n\n        # Create a simple visitor\n        def visitor(fullname, is_package, module_path):\n            # pylint: disable=W0613\n            \"\"\"\n            Package visitor: accepts everything in recursive mode,\n            else avoids packages\n            \"\"\"\n            return recursive or not is_package\n\n        # Set up the prefix if needed\n        if prefix is None:\n            prefix = os.path.basename(path)\n\n        bundles = set()  # type: Set[Bundle]\n        failed = set()  # type: Set[str]\n\n        with self.__bundles_lock:\n            try:\n                # Install the package first, resolved from the parent directory\n                bundles.add(self.install_bundle(prefix, os.path.dirname(path)))\n\n                # Visit the package\n                visited, sub_failed = self.install_visiting(\n                    path, visitor, prefix\n                )\n\n                # Update the sets\n                bundles.update(visited)\n                failed.update(sub_failed)\n            except BundleException as ex:\n                # Error loading the module\n                _logger.warning(\"Error loading package %s: %s\", prefix, ex)\n                failed.add(prefix)\n\n        return bundles, failed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef install_visiting(self, path, visitor, prefix=None):\n        # Validate the path\n        if not path:\n            raise ValueError(\"Empty path\")\n        elif not is_string(path):\n            raise ValueError(\"Path must be a string\")\n\n        # Validate the visitor\n        if visitor is None:\n            raise ValueError(\"No visitor method given\")\n\n        # Use an absolute path\n        path = os.path.abspath(path)\n        if not os.path.exists(path):\n            raise ValueError(\"Inexistent path: {0}\".format(path))\n\n        # Set up the prefix if needed\n        if prefix is None:\n            prefix = os.path.basename(path)\n\n        bundles = set()\n        failed = set()\n\n        with self.__bundles_lock:\n            # Walk through the folder to find modules\n            for name, is_package in walk_modules(path):\n                # Ignore '__main__' modules\n                if name == \"__main__\":\n                    continue\n\n                # Compute the full name of the module\n                fullname = \".\".join((prefix, name)) if prefix else name\n                try:\n                    if visitor(fullname, is_package, path):\n                        if is_package:\n                            # Install the package\n                            bundles.add(self.install_bundle(fullname, path))\n\n                            # Visit the package\n                            sub_path = os.path.join(path, name)\n                            sub_bundles, sub_failed = self.install_visiting(\n                                sub_path, visitor, fullname\n                            )\n                            bundles.update(sub_bundles)\n                            failed.update(sub_failed)\n                        else:\n                            # Install the bundle\n                            bundles.add(self.install_bundle(fullname, path))\n                except BundleException as ex:\n                    # Error loading the module\n                    _logger.warning(\"Error visiting %s: %s\", fullname, ex)\n\n                    # Try the next module\n                    failed.add(fullname)\n                    continue\n\n        return bundles, failed", "response": "Installs all the modules found in the given path if they are accepted by the visitor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a service and calls the listeners of the given service.", "response": "def register_service(\n        self,\n        bundle,\n        clazz,\n        service,\n        properties,\n        send_event,\n        factory=False,\n        prototype=False,\n    ):\n        # type: (Bundle, Union[List[Any], type, str], object, dict, bool, bool, bool) -> ServiceRegistration\n        \"\"\"\n        Registers a service and calls the listeners\n\n        :param bundle: The bundle registering the service\n        :param clazz: Name(s) of the interface(s) implemented by service\n        :param service: The service to register\n        :param properties: Service properties\n        :param send_event: If not, doesn't trigger a service registered event\n        :param factory: If True, the given service is a service factory\n        :param prototype: If True, the given service is a prototype service\n                          factory (the factory argument is considered True)\n        :return: A ServiceRegistration object\n        :raise BundleException: An error occurred while registering the service\n        \"\"\"\n        if bundle is None or service is None or not clazz:\n            raise BundleException(\"Invalid registration parameters\")\n\n        if not isinstance(properties, dict):\n            # Be sure we have a valid dictionary\n            properties = {}\n        else:\n            # Use a copy of the given properties\n            properties = properties.copy()\n\n        # Prepare the class specification\n        if not isinstance(clazz, (list, tuple)):\n            # Make a list from the single class\n            clazz = [clazz]\n\n        # Test the list content\n        classes = []\n        for svc_clazz in clazz:\n            if inspect.isclass(svc_clazz):\n                # Keep the type name\n                svc_clazz = svc_clazz.__name__\n\n            if not svc_clazz or not is_string(svc_clazz):\n                # Invalid class name\n                raise BundleException(\n                    \"Invalid class name: {0}\".format(svc_clazz)\n                )\n\n            # Class OK\n            classes.append(svc_clazz)\n\n        # Make the service registration\n        registration = self._registry.register(\n            bundle, classes, properties, service, factory, prototype\n        )\n\n        # Update the bundle registration information\n        bundle._registered_service(registration)\n\n        if send_event:\n            # Call the listeners\n            event = ServiceEvent(\n                ServiceEvent.REGISTERED, registration.get_reference()\n            )\n            self._dispatcher.fire_service_event(event)\n\n        return registration"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        # type: () -> bool\n        \"\"\"\n        Starts the framework\n\n        :return: True if the bundle has been started, False if it was already\n                 running\n        :raise BundleException: A bundle failed to start\n        \"\"\"\n        with self._lock:\n            if self._state in (Bundle.STARTING, Bundle.ACTIVE):\n                # Already started framework\n                return False\n\n            # Reset the stop event\n            self._fw_stop_event.clear()\n\n            # Starting...\n            self._state = Bundle.STARTING\n            self._dispatcher.fire_bundle_event(\n                BundleEvent(BundleEvent.STARTING, self)\n            )\n\n            # Start all registered bundles (use a copy, just in case...)\n            for bundle in self.__bundles.copy().values():\n                try:\n                    bundle.start()\n                except FrameworkException as ex:\n                    # Important error\n                    _logger.exception(\n                        \"Important error starting bundle: %s\", bundle\n                    )\n                    if ex.needs_stop:\n                        # Stop the framework (has to be in active state)\n                        self._state = Bundle.ACTIVE\n                        self.stop()\n                        return False\n                except BundleException:\n                    # A bundle failed to start : just log\n                    _logger.exception(\"Error starting bundle: %s\", bundle)\n\n            # Bundle is now active\n            self._state = Bundle.ACTIVE\n            return True", "response": "Starts the framework and returns True if the framework has been started False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the framework and all its associated services.", "response": "def stop(self):\n        # type: () -> bool\n        \"\"\"\n        Stops the framework\n\n        :return: True if the framework stopped, False it wasn't running\n        \"\"\"\n        with self._lock:\n            if self._state != Bundle.ACTIVE:\n                # Invalid state\n                return False\n\n            # Hide all services (they will be deleted by bundle.stop())\n            for bundle in self.__bundles.values():\n                self._registry.hide_bundle_services(bundle)\n\n            # Stopping...\n            self._state = Bundle.STOPPING\n            self._dispatcher.fire_bundle_event(\n                BundleEvent(BundleEvent.STOPPING, self)\n            )\n\n            # Notify listeners that the bundle is stopping\n            self._dispatcher.fire_framework_stopping()\n\n            bid = self.__next_bundle_id - 1\n            while bid > 0:\n                bundle = self.__bundles.get(bid)\n                bid -= 1\n\n                if bundle is None or bundle.get_state() != Bundle.ACTIVE:\n                    # Ignore inactive bundle\n                    continue\n\n                try:\n                    bundle.stop()\n                except Exception as ex:\n                    # Just log exceptions\n                    _logger.exception(\n                        \"Error stopping bundle %s: %s\",\n                        bundle.get_symbolic_name(),\n                        ex,\n                    )\n\n            # Framework is now stopped\n            self._state = Bundle.RESOLVED\n            self._dispatcher.fire_bundle_event(\n                BundleEvent(BundleEvent.STOPPED, self)\n            )\n\n            # All bundles have been stopped, release \"wait_for_stop\"\n            self._fw_stop_event.set()\n\n            # Force the registry clean up\n            self._registry.clear()\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, force=False):\n        if not force and self._state not in (\n            Bundle.INSTALLED,\n            Bundle.RESOLVED,\n            Bundle.STOPPING,\n        ):\n            _logger.warning(\"Trying to delete an active framework\")\n            return False\n\n        return FrameworkFactory.delete_framework(self)", "response": "Deletes the current framework"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nend the uninstallation of the given bundle (must be called by Bundle) :param bundle: The bundle to uninstall :raise BundleException: Invalid bundle", "response": "def uninstall_bundle(self, bundle):\n        # type: (Bundle) -> None\n        \"\"\"\n        Ends the uninstallation of the given bundle (must be called by Bundle)\n\n        :param bundle: The bundle to uninstall\n        :raise BundleException: Invalid bundle\n        \"\"\"\n        if bundle is None:\n            # Do nothing\n            return\n\n        with self.__bundles_lock:\n            # Stop the bundle first\n            bundle.stop()\n\n            bundle_id = bundle.get_bundle_id()\n            if bundle_id not in self.__bundles:\n                raise BundleException(\"Invalid bundle {0}\".format(bundle))\n\n            # Notify listeners\n            self._dispatcher.fire_bundle_event(\n                BundleEvent(BundleEvent.UNINSTALLED, bundle)\n            )\n\n            # Remove it from the dictionary\n            del self.__bundles[bundle_id]\n\n            # Remove it from the system => avoid unintended behaviors and\n            # forces a complete module reload if it is re-installed\n            name = bundle.get_symbolic_name()\n            try:\n                del sys.modules[name]\n            except KeyError:\n                # Ignore\n                pass\n\n            try:\n                # Clear reference in parent\n                parent, basename = name.rsplit(\".\", 1)\n                if parent:\n                    delattr(sys.modules[parent], basename)\n            except (KeyError, AttributeError, ValueError):\n                # Ignore errors\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unregister_service(self, registration):\n        # type: (ServiceRegistration) -> bool\n        \"\"\"\n        Unregisters the given service\n\n        :param registration: A ServiceRegistration to the service to unregister\n        :raise BundleException: Invalid reference\n        \"\"\"\n        # Get the Service Reference\n        reference = registration.get_reference()\n\n        # Remove the service from the registry\n        svc_instance = self._registry.unregister(reference)\n\n        # Keep a track of the unregistering reference\n        self.__unregistering_services[reference] = svc_instance\n\n        # Call the listeners\n        event = ServiceEvent(ServiceEvent.UNREGISTERING, reference)\n        self._dispatcher.fire_service_event(event)\n\n        # Update the bundle registration information\n        bundle = reference.get_bundle()\n        bundle._unregistered_service(registration)\n\n        # Remove the unregistering reference\n        del self.__unregistering_services[reference]\n        return True", "response": "Unregisters the given service from the registry"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the framework state.", "response": "def update(self):\n        \"\"\"\n        Stops and starts the framework, if the framework is active.\n\n        :raise BundleException: Something wrong occurred while stopping or\n                                starting the framework.\n        \"\"\"\n        with self._lock:\n            if self._state == Bundle.ACTIVE:\n                self.stop()\n                self.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for the framework to stop.", "response": "def wait_for_stop(self, timeout=None):\n        # type: (Optional[int]) -> bool\n        \"\"\"\n        Waits for the framework to stop. Does nothing if the framework bundle\n        is not in ACTIVE state.\n\n        Uses a threading.Condition object\n\n        :param timeout: The maximum time to wait (in seconds)\n        :return: True if the framework has stopped, False if the timeout raised\n        \"\"\"\n        if self._state != Bundle.ACTIVE:\n            # Inactive framework, ignore the call\n            return True\n\n        self._fw_stop_event.wait(timeout)\n\n        with self._lock:\n            # If the timeout raised, we should be in another state\n            return self._state == Bundle.RESOLVED"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a service object from the registry.", "response": "def unget_service(self, service):\n        # type: (Any) -> bool\n        \"\"\"\n        Releases a service object for the associated service.\n\n        :param service: An instance of a service returned by ``get_service()``\n        :return: True if the bundle usage has been removed\n        \"\"\"\n        return self.__registry.unget_service(\n            self.__bundle, self.__reference, service\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_service_listener(\n        self, listener, ldap_filter=None, specification=None\n    ):\n        \"\"\"\n        Registers a service listener\n\n        The service listener must have a method with the following prototype::\n\n           def service_changed(self, event):\n               '''\n               Called by Pelix when some service properties changes\n\n               event: A ServiceEvent object\n               '''\n               # ...\n\n        :param bundle_context:  This bundle context\n        :param listener: The listener to register\n        :param ldap_filter: Filter that must match the service properties\n                            (optional, None to accept all services)\n        :param specification: The specification that must provide the service\n                              (optional, None to accept all services)\n        :return: True if the listener has been successfully registered\n        \"\"\"\n        return self.__framework._dispatcher.add_service_listener(\n            self, listener, specification, ldap_filter\n        )", "response": "Adds a service listener to the bundle context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the bundle object associated with the given ID.", "response": "def get_bundle(self, bundle_id=None):\n        # type: (Union[Bundle, int]) -> Bundle\n        \"\"\"\n        Retrieves the :class:`~pelix.framework.Bundle` object for the bundle\n        matching the given ID (int). If no ID is given (None), the bundle\n        associated to this context is returned.\n\n        :param bundle_id: A bundle ID (optional)\n        :return: The requested :class:`~pelix.framework.Bundle` object\n        :raise BundleException: The given ID doesn't exist or is invalid\n        \"\"\"\n        if bundle_id is None:\n            # Current bundle\n            return self.__bundle\n        elif isinstance(bundle_id, Bundle):\n            # Got a bundle (compatibility with older install_bundle())\n            bundle_id = bundle_id.get_bundle_id()\n\n        return self.__framework.get_bundle_by_id(bundle_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a ServiceReference object for a service that implements and was registered under the specified class.", "response": "def get_service_reference(self, clazz, ldap_filter=None):\n        # type: (Optional[str], Optional[str]) -> Optional[ServiceReference]\n        \"\"\"\n        Returns a ServiceReference object for a service that implements and\n        was registered under the specified class\n\n        :param clazz: The class name with which the service was registered.\n        :param ldap_filter: A filter on service properties\n        :return: A service reference, None if not found\n        \"\"\"\n        result = self.__framework.find_service_references(\n            clazz, ldap_filter, True\n        )\n        try:\n            return result[0]\n        except TypeError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_service_references(self, clazz, ldap_filter=None):\n        # type: (Optional[str], Optional[str]) -> Optional[List[ServiceReference]]\n        \"\"\"\n        Returns the service references for services that were registered under\n        the specified class by this bundle and matching the given filter\n\n        :param clazz: The class name with which the service was registered.\n        :param ldap_filter: A filter on service properties\n        :return: The list of references to the services registered by the\n                 calling bundle and matching the filters.\n        \"\"\"\n        refs = self.__framework.find_service_references(clazz, ldap_filter)\n        if refs:\n            for ref in refs:\n                if ref.get_bundle() is not self.__bundle:\n                    refs.remove(ref)\n\n        return refs", "response": "Returns the list of service references that were registered under the specified class by this bundle and matching the given filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninstalls all the modules found in the given path.", "response": "def install_package(self, path, recursive=False):\n        # type: (str, bool) -> tuple\n        \"\"\"\n        Installs all the modules found in the given package (directory).\n        It is a utility method working like\n        :meth:`~pelix.framework.BundleContext.install_visiting`, with a visitor\n        accepting every module found.\n\n        :param path: Path of the package (folder)\n        :param recursive: If True, installs the modules found in sub-directories\n        :return: A 2-tuple, with the list of installed bundles\n                 (:class:`~pelix.framework.Bundle`) and the list of the names\n                 of the modules which import failed.\n        :raise ValueError: The given path is invalid\n        \"\"\"\n        return self.__framework.install_package(path, recursive)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a service in the current bundle.", "response": "def register_service(\n        self,\n        clazz,\n        service,\n        properties,\n        send_event=True,\n        factory=False,\n        prototype=False,\n    ):\n        # type: (Union[List[Any], type, str], object, dict, bool, bool, bool) -> ServiceRegistration\n        \"\"\"\n        Registers a service\n\n        :param clazz: Class or Classes (list) implemented by this service\n        :param service: The service instance\n        :param properties: The services properties (dictionary)\n        :param send_event: If not, doesn't trigger a service registered event\n        :param factory: If True, the given service is a service factory\n        :param prototype: If True, the given service is a prototype service\n                          factory (the factory argument is considered True)\n        :return: A ServiceRegistration object\n        :raise BundleException: An error occurred while registering the service\n        \"\"\"\n        return self.__framework.register_service(\n            self.__bundle,\n            clazz,\n            service,\n            properties,\n            send_event,\n            factory,\n            prototype,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unget_service(self, reference):\n        # type: (ServiceReference) -> bool\n        \"\"\"\n        Disables a reference to the service\n\n        :return: True if the bundle was using this reference, else False\n        \"\"\"\n        # Lose the dependency\n        return self.__framework._registry.unget_service(\n            self.__bundle, reference\n        )", "response": "Disables a reference to the service"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the current framework instance.", "response": "def get_framework(cls, properties=None):\n        # type: (Optional[dict]) -> Framework\n        \"\"\"\n        If it doesn't exist yet, creates a framework with the given properties,\n        else returns the current framework instance.\n\n        :return: A Pelix instance\n        \"\"\"\n        if cls.__singleton is None:\n            # Normalize sys.path\n            normalize_path()\n            cls.__singleton = Framework(properties)\n\n        return cls.__singleton"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the framework singleton from the class.", "response": "def delete_framework(cls, framework=None):\n        # type: (Optional[Framework]) -> bool\n        # pylint: disable=W0212\n        \"\"\"\n        Removes the framework singleton\n\n        :return: True on success, else False\n        \"\"\"\n        if framework is None:\n            framework = cls.__singleton\n\n        if framework is cls.__singleton:\n            # Stop the framework\n            try:\n                framework.stop()\n            except:\n                _logger.exception(\"Error stopping the framework\")\n\n            # Uninstall its bundles\n            bundles = framework.get_bundles()\n            for bundle in bundles:\n                try:\n                    bundle.uninstall()\n                except:\n                    _logger.exception(\n                        \"Error uninstalling bundle %s\",\n                        bundle.get_symbolic_name(),\n                    )\n\n            # Clear the event dispatcher\n            framework._dispatcher.clear()\n\n            # Clear the singleton\n            cls.__singleton = None\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_matching_interfaces(object_class, exported_intfs):\n    # type: (List[str], Optional[List[str]]) -> Optional[List[str]]\n    \"\"\"\n    Returns the list of interfaces matching the export property\n\n    :param object_class: The specifications of the service\n    :param exported_intfs: The declared exported interfaces\n    :return: The list of declared exported interfaces\n    \"\"\"\n    if object_class is None or exported_intfs is None:\n        return None\n\n    if isinstance(exported_intfs, str) and exported_intfs == \"*\":\n        return object_class\n\n    # after this exported_intfs will be list\n    exported_intfs = get_string_plus_property_value(exported_intfs)\n    if len(exported_intfs) == 1 and exported_intfs[0] == \"*\":\n        return object_class\n\n    return exported_intfs", "response": "Returns the list of interfaces that are matching the exported property"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_prop_value(name, props, default=None):\n    # type: (str, Dict[str, Any], Any) -> Any\n    \"\"\"\n    Returns the value of a property or the default one\n\n    :param name: Name of a property\n    :param props: Dictionary of properties\n    :param default: Default value\n    :return: The value of the property or the default one\n    \"\"\"\n    if not props:\n        return default\n\n    try:\n        return props[name]\n    except KeyError:\n        return default", "response": "Returns the value of a property or the default value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_prop_if_null(name, props, if_null):\n    # type: (str, Dict[str, Any], Any) -> None\n    \"\"\"\n    Updates the value of a property if the previous one was None\n\n    :param name: Name of the property\n    :param props: Dictionary of properties\n    :param if_null: Value to insert if the previous was None\n    \"\"\"\n    value = get_prop_value(name, props)\n    if value is None:\n        props[name] = if_null", "response": "Updates the value of a property if it is None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_string_plus_property_value(value):\n    # type: (Any) -> Optional[List[str]]\n    \"\"\"\n    Converts a string or list of string into a list of strings\n\n    :param value: A string or a list of strings\n    :return: A list of strings or None\n    \"\"\"\n    if value:\n        if isinstance(value, str):\n            return [value]\n        if isinstance(value, list):\n            return value\n        if isinstance(value, tuple):\n            return list(value)\n\n    return None", "response": "Converts a string or list of strings into a list of strings or None if the value is not a string or a list of strings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_string_plus_property(name, props, default=None):\n    # type: (str, Dict[str, Any], Optional[Any]) -> Any\n    \"\"\"\n    Returns the value of the given property or the default value\n\n    :param name: A property name\n    :param props: A dictionary of properties\n    :param default: Value to return if the property doesn't exist\n    :return: The property value or the default one\n    \"\"\"\n    val = get_string_plus_property_value(get_prop_value(name, props, default))\n    return default if val is None else val", "response": "Returns the value of the given property or the default value if the property doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_exported_interfaces(svc_ref, overriding_props=None):\n    # type: (ServiceReference, Optional[Dict[str, Any]]) -> Optional[List[str]]\n    \"\"\"\n    Looks for the interfaces exported by a service\n\n    :param svc_ref: Service reference\n    :param overriding_props: Properties overriding service ones\n    :return: The list of exported interfaces\n    \"\"\"\n    # first check overriding_props for service.exported.interfaces\n    exported_intfs = get_prop_value(\n        SERVICE_EXPORTED_INTERFACES, overriding_props\n    )\n    # then check svc_ref property\n    if not exported_intfs:\n        exported_intfs = svc_ref.get_property(SERVICE_EXPORTED_INTERFACES)\n\n    if not exported_intfs:\n        return None\n\n    return get_matching_interfaces(\n        svc_ref.get_property(constants.OBJECTCLASS), exported_intfs\n    )", "response": "Returns the list of interfaces exported by a service."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the exported interfaces are all provided by the service", "response": "def validate_exported_interfaces(object_class, exported_intfs):\n    # type: (List[str], List[str]) -> bool\n    \"\"\"\n    Validates that the exported interfaces are all provided by the service\n\n    :param object_class: The specifications of a service\n    :param exported_intfs: The exported specifications\n    :return: True if the exported specifications are all provided by the service\n    \"\"\"\n    if (\n        not exported_intfs\n        or not isinstance(exported_intfs, list)\n        or not exported_intfs\n    ):\n        return False\n    else:\n        for exintf in exported_intfs:\n            if exintf not in object_class:\n                return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the package version of interfaces", "response": "def get_package_versions(intfs, props):\n    # type: (List[str], Dict[str, Any]) -> List[Tuple[str, str]]\n    \"\"\"\n    Gets the package version of interfaces\n\n    :param intfs: A list of interfaces\n    :param props: A dictionary containing endpoint package versions\n    :return: A list of tuples (package name, version)\n    \"\"\"\n    result = []\n    for intf in intfs:\n        pkg_name = get_package_from_classname(intf)\n        if pkg_name:\n            key = ENDPOINT_PACKAGE_VERSION_ + pkg_name\n            val = props.get(key, None)\n            if val:\n                result.append((key, val))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_rsa_props(\n    object_class,\n    exported_cfgs,\n    remote_intents=None,\n    ep_svc_id=None,\n    fw_id=None,\n    pkg_vers=None,\n    service_intents=None,\n):\n    \"\"\"\n    Constructs a dictionary of RSA properties from the given arguments\n\n    :param object_class: Service specifications\n    :param exported_cfgs: Export configurations\n    :param remote_intents: Supported remote intents\n    :param ep_svc_id: Endpoint service ID\n    :param fw_id: Remote Framework ID\n    :param pkg_vers: Version number of the specification package\n    :param service_intents: Service intents\n    :return: A dictionary of properties\n    \"\"\"\n    results = {}\n    if not object_class:\n        raise ArgumentError(\n            \"object_class\", \"object_class must be an [] of Strings\"\n        )\n    results[\"objectClass\"] = object_class\n    if not exported_cfgs:\n        raise ArgumentError(\n            \"exported_cfgs\", \"exported_cfgs must be an array of Strings\"\n        )\n    results[REMOTE_CONFIGS_SUPPORTED] = exported_cfgs\n    results[SERVICE_IMPORTED_CONFIGS] = exported_cfgs\n    if remote_intents:\n        results[REMOTE_INTENTS_SUPPORTED] = remote_intents\n    if service_intents:\n        results[SERVICE_INTENTS] = service_intents\n    if not ep_svc_id:\n        ep_svc_id = get_next_rsid()\n    results[ENDPOINT_SERVICE_ID] = ep_svc_id\n    results[SERVICE_ID] = ep_svc_id\n    if not fw_id:\n        # No framework ID means an error\n        fw_id = \"endpoint-in-error\"\n    results[ENDPOINT_FRAMEWORK_UUID] = fw_id\n    if pkg_vers:\n        if isinstance(pkg_vers, type(tuple())):\n            pkg_vers = [pkg_vers]\n        for pkg_ver in pkg_vers:\n            results[pkg_ver[0]] = pkg_ver[1]\n    results[ENDPOINT_ID] = create_uuid()\n    results[SERVICE_IMPORTED] = \"true\"\n    return results", "response": "Constructs a dictionary of RSA properties from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ecf_props(ep_id, ep_id_ns, rsvc_id=None, ep_ts=None):\n    results = {}\n    if not ep_id:\n        raise ArgumentError(\"ep_id\", \"ep_id must be a valid endpoint id\")\n    results[ECF_ENDPOINT_ID] = ep_id\n    if not ep_id_ns:\n        raise ArgumentError(\"ep_id_ns\", \"ep_id_ns must be a valid namespace\")\n    results[ECF_ENDPOINT_CONTAINERID_NAMESPACE] = ep_id_ns\n    if not rsvc_id:\n        rsvc_id = get_next_rsid()\n    results[ECF_RSVC_ID] = rsvc_id\n    if not ep_ts:\n        ep_ts = time_since_epoch()\n    results[ECF_ENDPOINT_TIMESTAMP] = ep_ts\n    return results", "response": "Returns a dictionary of properties that are required to create an Endpoint Container ID and a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_extra_props(props):\n    # type: (Dict[str, Any]) -> Dict[str, Any]\n    \"\"\"\n    Returns the extra properties, *i.e.* non-ECF, non-RSA properties\n\n    :param props: A dictionary of properties\n    :return: A filtered dictionary\n    \"\"\"\n    return {\n        key: value\n        for key, value in props.items()\n        if key not in ECFPROPNAMES\n        and key not in RSA_PROP_NAMES\n        and not key.startswith(ENDPOINT_PACKAGE_VERSION_)\n    }", "response": "Returns the extra properties that are not part of an ECF or RSA container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_edef_props(\n    object_class,\n    exported_cfgs,\n    ep_namespace,\n    ep_id,\n    ecf_ep_id,\n    ep_rsvc_id,\n    ep_ts,\n    remote_intents=None,\n    fw_id=None,\n    pkg_ver=None,\n    service_intents=None,\n):\n    \"\"\"\n    Prepares the EDEF properties of an endpoint, merge of RSA and ECF\n    properties\n    \"\"\"\n    osgi_props = get_rsa_props(\n        object_class,\n        exported_cfgs,\n        remote_intents,\n        ep_rsvc_id,\n        fw_id,\n        pkg_ver,\n        service_intents,\n    )\n    ecf_props = get_ecf_props(ecf_ep_id, ep_namespace, ep_rsvc_id, ep_ts)\n    return merge_dicts(osgi_props, ecf_props)", "response": "Get the properties of an EDEF object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dot_properties(prefix, props, remove_prefix):\n    # type: (str, Dict[str, Any], bool) -> Dict[str, Any]\n    \"\"\"\n    Gets the properties starting with the given prefix\n    \"\"\"\n    result_props = {}\n    if props:\n        dot_keys = [x for x in props.keys() if x.startswith(prefix + \".\")]\n        for dot_key in dot_keys:\n            if remove_prefix:\n                new_key = dot_key[len(prefix) + 1 :]\n            else:\n                new_key = dot_key\n            result_props[new_key] = props.get(dot_key)\n    return result_props", "response": "Returns the properties starting with the given prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy all properties with non - reserved names from props to target.", "response": "def copy_non_reserved(props, target):\n    # type: (Dict[str, Any], Dict[str, Any]) -> Dict[str, Any]\n    \"\"\"\n    Copies all properties with non-reserved names from ``props`` to ``target``\n\n    :param props: A dictionary of properties\n    :param target: Another dictionary\n    :return: The target dictionary\n    \"\"\"\n    target.update(\n        {\n            key: value\n            for key, value in props.items()\n            if not is_reserved_property(key)\n        }\n    )\n    return target"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy_non_ecf(props, target):\n    # type: (Dict[str, Any], Dict[str, Any]) -> Dict[str, Any]\n    \"\"\"\n    Copies non-ECF properties from ``props`` to ``target``\n\n    :param props: An input dictionary\n    :param target: The dictionary to copy non-ECF properties to\n    :return: The ``target`` dictionary\n    \"\"\"\n    target.update(\n        {key: value for key, value in props.items() if key not in ECFPROPNAMES}\n    )\n    return target", "response": "Copies non - ECF properties from props to target"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the given item to the given set.", "response": "def set_append(input_set, item):\n    # type: (set, Any) -> set\n    \"\"\"\n    Appends in-place the given item to the set.\n    If the item is a list, all elements are added to the set.\n\n    :param input_set: An existing set\n    :param item: The item or list of items to add\n    :return: The given set\n    \"\"\"\n    if item:\n        if isinstance(item, (list, tuple)):\n            input_set.update(item)\n        else:\n            input_set.add(item)\n    return input_set"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a RemoteServiceAdminEvent object from an ImportRegistration", "response": "def fromimportreg(cls, bundle, import_reg):\n        # type: (Bundle, ImportRegistration) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from an ImportRegistration\n        \"\"\"\n        exc = import_reg.get_exception()\n        if exc:\n            return RemoteServiceAdminEvent(\n                RemoteServiceAdminEvent.IMPORT_ERROR,\n                bundle,\n                import_reg.get_import_container_id(),\n                import_reg.get_remoteservice_id(),\n                None,\n                None,\n                exc,\n                import_reg.get_description(),\n            )\n\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.IMPORT_REGISTRATION,\n            bundle,\n            import_reg.get_import_container_id(),\n            import_reg.get_remoteservice_id(),\n            import_reg.get_import_reference(),\n            None,\n            None,\n            import_reg.get_description(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromexportreg(cls, bundle, export_reg):\n        # type: (Bundle, ExportRegistration) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from an ExportRegistration\n        \"\"\"\n        exc = export_reg.get_exception()\n        if exc:\n            return RemoteServiceAdminEvent(\n                RemoteServiceAdminEvent.EXPORT_ERROR,\n                bundle,\n                export_reg.get_export_container_id(),\n                export_reg.get_remoteservice_id(),\n                None,\n                None,\n                exc,\n                export_reg.get_description(),\n            )\n\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.EXPORT_REGISTRATION,\n            bundle,\n            export_reg.get_export_container_id(),\n            export_reg.get_remoteservice_id(),\n            None,\n            export_reg.get_export_reference(),\n            None,\n            export_reg.get_description(),\n        )", "response": "Creates a RemoteServiceAdminEvent object from an ExportRegistration object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fromexportupdate(cls, bundle, export_reg):\n        # type: (Bundle, ExportRegistration) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from the update of an\n        ExportRegistration\n        \"\"\"\n        exc = export_reg.get_exception()\n        if exc:\n            return RemoteServiceAdminEvent(\n                RemoteServiceAdminEvent.EXPORT_ERROR,\n                bundle,\n                export_reg.get_export_container_id(),\n                export_reg.get_remoteservice_id(),\n                None,\n                export_reg.get_export_reference(),\n                None,\n                export_reg.get_description(),\n            )\n\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.EXPORT_UPDATE,\n            bundle,\n            export_reg.get_export_container_id(),\n            export_reg.get_remoteservice_id(),\n            None,\n            export_reg.get_export_reference(),\n            None,\n            export_reg.get_description(),\n        )", "response": "Creates a RemoteServiceAdminEvent object from an ExportRegistration"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fromimportupdate(cls, bundle, import_reg):\n        # type: (Bundle, ImportRegistration) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from the update of an\n        ImportRegistration\n        \"\"\"\n        exc = import_reg.get_exception()\n        if exc:\n            return RemoteServiceAdminEvent(\n                RemoteServiceAdminEvent.IMPORT_ERROR,\n                bundle,\n                import_reg.get_import_container_id(),\n                import_reg.get_remoteservice_id(),\n                None,\n                None,\n                exc,\n                import_reg.get_description(),\n            )\n\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.IMPORT_UPDATE,\n            bundle,\n            import_reg.get_import_container_id(),\n            import_reg.get_remoteservice_id(),\n            import_reg.get_import_reference(),\n            None,\n            None,\n            import_reg.get_description(),\n        )", "response": "Creates a RemoteServiceAdminEvent object from an ImportRegistration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a RemoteServiceAdminEvent object from the departure of an ImportRegistration", "response": "def fromimportunreg(\n        cls, bundle, cid, rsid, import_ref, exception, endpoint\n    ):\n        # type: (Bundle, Tuple[str, str], Tuple[Tuple[str, str], int], ImportReference, Optional[Tuple[Any, Any, Any]], EndpointDescription) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from the departure of an\n        ImportRegistration\n        \"\"\"\n        return RemoteServiceAdminEvent(\n            typ=RemoteServiceAdminEvent.IMPORT_UNREGISTRATION,\n            bundle=bundle,\n            cid=cid,\n            rsid=rsid,\n            import_ref=import_ref,\n            exception=exception,\n            endpoint=endpoint,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromexportunreg(\n        cls, bundle, exporterid, rsid, export_ref, exception, endpoint\n    ):\n        # type: (Bundle, Tuple[str, str], Tuple[Tuple[str, str], int], ExportReference, Optional[Tuple[Any, Any, Any]], EndpointDescription) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from the departure of an\n        ExportRegistration\n        \"\"\"\n        return RemoteServiceAdminEvent(\n            typ=RemoteServiceAdminEvent.EXPORT_UNREGISTRATION,\n            bundle=bundle,\n            cid=exporterid,\n            rsid=rsid,\n            export_ref=export_ref,\n            exception=exception,\n            endpoint=endpoint,\n        )", "response": "Creates a RemoteServiceAdminEvent object from the departure of an ExportRegistration\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromimporterror(cls, bundle, importerid, rsid, exception, endpoint):\n        # type: (Bundle, Tuple[str, str], Tuple[Tuple[str, str], int], Optional[Tuple[Any, Any, Any]], EndpointDescription) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from an import error\n        \"\"\"\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.IMPORT_ERROR,\n            bundle,\n            importerid,\n            rsid,\n            None,\n            None,\n            exception,\n            endpoint,\n        )", "response": "Creates a RemoteServiceAdminEvent object from an import error."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fromexporterror(cls, bundle, exporterid, rsid, exception, endpoint):\n        # type: (Bundle, Tuple[str, str], Tuple[Tuple[str, str], int], Optional[Tuple[Any, Any, Any]], EndpointDescription) -> RemoteServiceAdminEvent\n        \"\"\"\n        Creates a RemoteServiceAdminEvent object from an export error\n        \"\"\"\n        return RemoteServiceAdminEvent(\n            RemoteServiceAdminEvent.EXPORT_ERROR,\n            bundle,\n            exporterid,\n            rsid,\n            None,\n            None,\n            exception,\n            endpoint,\n        )", "response": "Creates a RemoteServiceAdminEvent object from an export error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _try_instantiate(self, ipopo, factory, component):\n        # type: (Any, str, str) -> None\n        \"\"\"\n        Tries to instantiate a component from the queue. Hides all exceptions.\n\n        :param ipopo: The iPOPO service\n        :param factory: Component factory\n        :param component: Component name\n        \"\"\"\n        try:\n            # Get component properties\n            with self.__lock:\n                properties = self.__queue[factory][component]\n        except KeyError:\n            # Component not in queue\n            return\n        else:\n            try:\n                # Try instantiation\n                ipopo.instantiate(factory, component, properties)\n            except TypeError:\n                # Unknown factory: try later\n                pass\n            except ValueError as ex:\n                # Already known component\n                _logger.error(\"Component already running: %s\", ex)\n            except Exception as ex:\n                # Other error\n                _logger.exception(\"Error instantiating component: %s\", ex)", "response": "Tries to instantiate a component from the queue. Hides all exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _start(self):\n        try:\n            # Try to register to factory events\n            with use_ipopo(self.__context) as ipopo:\n                ipopo.add_listener(self)\n        except BundleException:\n            # Service not yet present\n            pass\n\n        # Register the iPOPO service listener\n        self.__context.add_service_listener(self, specification=SERVICE_IPOPO)", "response": "Starts the instantiation queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _stop(self):\n        # Unregisters the iPOPO service listener\n        self.__context.remove_service_listener(self)\n\n        try:\n            # Try to register to factory events\n            with use_ipopo(self.__context) as ipopo:\n                ipopo.remove_listener(self)\n        except BundleException:\n            # Service not present anymore\n            pass", "response": "Stops the instantiation queue"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing all references to this object", "response": "def _clear(self):\n        \"\"\"\n        Clear all references (called by its bundle activator)\n        \"\"\"\n        self.__names.clear()\n        self.__queue.clear()\n        self.__context = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle an event about the iPOPO service", "response": "def service_changed(self, event):\n        # type: (ServiceEvent) -> None\n        \"\"\"\n        Handles an event about the iPOPO service\n        \"\"\"\n        kind = event.get_kind()\n        if kind == ServiceEvent.REGISTERED:\n            # iPOPO service registered: register to factory events\n            with use_ipopo(self.__context) as ipopo:\n                ipopo.add_listener(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_ipopo_event(self, event):\n        # type: (IPopoEvent) -> None\n        \"\"\"\n        Handles an iPOPO event\n\n        :param event: iPOPO event bean\n        \"\"\"\n        kind = event.get_kind()\n        if kind == IPopoEvent.REGISTERED:\n            # A factory has been registered\n            try:\n                with use_ipopo(self.__context) as ipopo:\n                    factory = event.get_factory_name()\n\n                    with self.__lock:\n                        # Copy the list of components names for this factory\n                        components = self.__queue[factory].copy()\n\n                    for component in components:\n                        self._try_instantiate(ipopo, factory, component)\n            except BundleException:\n                # iPOPO not yet started\n                pass\n            except KeyError:\n                # No components for this new factory\n                pass", "response": "Handles an iPOPO event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, factory, component, properties=None):\n        # type: (str, str, dict) -> None\n        \"\"\"\n        Enqueues the instantiation of the given component\n\n        :param factory: Factory name\n        :param component: Component name\n        :param properties: Component properties\n        :raise ValueError: Component name already reserved in the queue\n        :raise Exception: Error instantiating the component\n        \"\"\"\n        with self.__lock:\n            if component in self.__names:\n                raise ValueError(\n                    \"Component name already queued: {0}\".format(component)\n                )\n\n            # Normalize properties\n            if properties is None:\n                properties = {}\n\n            # Store component description\n            self.__names[component] = factory\n            self.__queue.setdefault(factory, {})[component] = properties\n\n            try:\n                with use_ipopo(self.__context) as ipopo:\n                    # Try to instantiate the component right now\n                    self._try_instantiate(ipopo, factory, component)\n            except BundleException:\n                # iPOPO not yet started\n                pass", "response": "Adds a new component to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, component):\n        # type: (str) -> None\n        \"\"\"\n        Kills/Removes the component with the given name\n\n        :param component: A component name\n        :raise KeyError: Unknown component\n        \"\"\"\n        with self.__lock:\n            # Find its factory\n            factory = self.__names.pop(component)\n            components = self.__queue[factory]\n\n            # Clear the queue\n            del components[component]\n            if not components:\n                # No more component for this factory\n                del self.__queue[factory]\n\n            # Kill the component\n            try:\n                with use_ipopo(self.__context) as ipopo:\n                    # Try to instantiate the component right now\n                    ipopo.kill(component)\n            except (BundleException, ValueError):\n                # iPOPO not yet started or component not instantiated\n                pass", "response": "Removes the component with the given name from the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_server(\n    shell,\n    server_address,\n    port,\n    cert_file=None,\n    key_file=None,\n    key_password=None,\n    ca_file=None,\n):\n    \"\"\"\n    Creates the TCP console on the given address and port\n\n    :param shell: The remote shell handler\n    :param server_address: Server bound address\n    :param port: Server port\n    :param cert_file: Path to the server certificate\n    :param key_file: Path to the server private key\n    :param key_password: Password for the key file\n    :param ca_file: Path to Certificate Authority to authenticate clients\n    :return: A tuple: Server thread, TCP server object, Server active flag\n    \"\"\"\n    # Set up the request handler creator\n    active_flag = SharedBoolean(True)\n\n    def request_handler(*rh_args):\n        \"\"\"\n        Constructs a RemoteConsole as TCP request handler\n        \"\"\"\n        return RemoteConsole(shell, active_flag, *rh_args)\n\n    # Set up the server\n    server = ThreadingTCPServerFamily(\n        (server_address, port),\n        request_handler,\n        cert_file,\n        key_file,\n        key_password,\n        ca_file,\n    )\n\n    # Set flags\n    server.daemon_threads = True\n    server.allow_reuse_address = True\n\n    # Activate the server\n    server.server_bind()\n    server.server_activate()\n\n    # Serve clients\n    server_thread = threading.Thread(\n        target=server.serve_forever, name=\"RemoteShell-{0}\".format(port)\n    )\n    server_thread.daemon = True\n    server_thread.start()\n\n    return server_thread, server, active_flag", "response": "Create a TCP server on the given address and port."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a Python interpreter console and blocks until the user exits it.", "response": "def _run_interpreter(variables, banner):\n    \"\"\"\n    Runs a Python interpreter console and blocks until the user exits it.\n\n    :param variables: Interpreters variables (locals)\n    :param banner: Start-up banners\n    \"\"\"\n    # Script-only imports\n    import code\n\n    try:\n        import readline\n        import rlcompleter\n\n        readline.set_completer(rlcompleter.Completer(variables).complete)\n        readline.parse_and_bind(\"tab: complete\")\n    except ImportError:\n        # readline is not available: ignore\n        pass\n\n    # Start the console\n    shell = code.InteractiveConsole(variables)\n    shell.interact(banner)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(argv=None):\n    # Prepare arguments\n    parser = argparse.ArgumentParser(\n        prog=\"pelix.shell.remote\",\n        parents=[make_common_parser()],\n        description=\"Pelix Remote Shell ({} SSL support)\".format(\n            \"with\" if ssl is not None else \"without\"\n        ),\n    )\n\n    # Remote shell options\n    group = parser.add_argument_group(\"Remote Shell options\")\n    group.add_argument(\n        \"-a\",\n        \"--address\",\n        default=\"localhost\",\n        help=\"The remote shell binding address\",\n    )\n    group.add_argument(\n        \"-p\",\n        \"--port\",\n        type=int,\n        default=9000,\n        help=\"The remote shell binding port\",\n    )\n\n    if ssl is not None:\n        # Remote Shell TLS options\n        group = parser.add_argument_group(\"TLS Options\")\n        group.add_argument(\"--cert\", help=\"Path to the server certificate file\")\n        group.add_argument(\n            \"--key\",\n            help=\"Path to the server key file \"\n            \"(can be omitted if the key is in the certificate)\",\n        )\n        group.add_argument(\n            \"--key-password\",\n            help=\"Password of the server key.\"\n            \"Set to '-' for a password request.\",\n        )\n        group.add_argument(\n            \"--ca-chain\",\n            help=\"Path to the CA chain file to authenticate clients\",\n        )\n\n    # Local options\n    group = parser.add_argument_group(\"Local options\")\n    group.add_argument(\n        \"--no-input\",\n        action=\"store_true\",\n        help=\"Run without input (for daemon mode)\",\n    )\n\n    # Parse them\n    args = parser.parse_args(argv)\n\n    # Handle arguments\n    init = handle_common_arguments(args)\n\n    # Set the initial bundles\n    bundles = [\n        \"pelix.ipopo.core\",\n        \"pelix.shell.core\",\n        \"pelix.shell.ipopo\",\n        \"pelix.shell.remote\",\n    ]\n    bundles.extend(init.bundles)\n\n    # Start a Pelix framework\n    framework = pelix.framework.create_framework(\n        utilities.remove_duplicates(bundles), init.properties\n    )\n    framework.start()\n    context = framework.get_bundle_context()\n\n    # Instantiate configured components\n    init.instantiate_components(framework.get_bundle_context())\n\n    # Instantiate a Remote Shell, if necessary\n    with use_ipopo(context) as ipopo:\n        rshell_name = \"remote-shell\"\n        try:\n            ipopo.get_instance_details(rshell_name)\n        except ValueError:\n            # Component doesn't exist, we can instantiate it.\n\n            if ssl is not None:\n                # Copy parsed arguments\n                ca_chain = args.ca_chain\n                cert = args.cert\n                key = args.key\n\n                # Normalize the TLS key file password argument\n                if args.key_password == \"-\":\n                    import getpass\n\n                    key_password = getpass.getpass(\n                        \"Password for {}: \".format(args.key or args.cert)\n                    )\n                else:\n                    key_password = args.key_password\n            else:\n                # SSL support is missing:\n                # Ensure the SSL arguments are defined but set to None\n                ca_chain = None\n                cert = None\n                key = None\n                key_password = None\n\n            # Setup the component\n            rshell = ipopo.instantiate(\n                pelix.shell.FACTORY_REMOTE_SHELL,\n                rshell_name,\n                {\n                    \"pelix.shell.address\": args.address,\n                    \"pelix.shell.port\": args.port,\n                    \"pelix.shell.ssl.ca\": ca_chain,\n                    \"pelix.shell.ssl.cert\": cert,\n                    \"pelix.shell.ssl.key\": key,\n                    \"pelix.shell.ssl.key_password\": key_password,\n                },\n            )\n\n            # Avoid loose reference to the password\n            del key_password\n        else:\n            logging.error(\n                \"A remote shell component (%s) is already \"\n                \"configured. Abandon.\",\n                rshell_name,\n            )\n            return 1\n\n    # Prepare a banner\n    host, port = rshell.get_access()\n    try:\n        if args.no_input:\n            # No input required: just print the access to the shell\n            print(\"Remote shell bound to:\", host, \"- port:\", port)\n\n            try:\n                while not framework.wait_for_stop(1):\n                    # Awake from wait every second to let KeyboardInterrupt\n                    # exception to raise\n                    pass\n            except KeyboardInterrupt:\n                print(\"Got Ctrl+C: exiting.\")\n                return 127\n        else:\n            # Prepare interpreter variables\n            variables = {\n                \"__name__\": \"__console__\",\n                \"__doc__\": None,\n                \"__package__\": None,\n                \"framework\": framework,\n                \"context\": context,\n                \"use_ipopo\": use_ipopo,\n            }\n\n            banner = (\n                \"{lines}\\nPython interpreter with Pelix Remote Shell\\n\"\n                \"Remote shell bound to: {host}:{port}\\n{lines}\\n\"\n                \"Python version: {version}\\n\".format(\n                    lines=\"-\" * 80, version=sys.version, host=host, port=port\n                )\n            )\n\n            # Run an interpreter\n            _run_interpreter(variables, banner)\n    finally:\n        # Stop the framework\n        framework.stop()", "response": "Entry point for the script."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend data to the client.", "response": "def send(self, data):\n        \"\"\"\n        Tries to send data to the client.\n\n        :param data: Data to be sent\n        :return: True if the data was sent, False on error\n        \"\"\"\n        if data is not None:\n            data = data.encode(\"UTF-8\")\n\n        try:\n            self.wfile.write(data)\n            self.wfile.flush()\n            return True\n\n        except IOError:\n            # An error occurred, mask it\n            # -> This allows to handle the command even if the client has been\n            # disconnect (i.e. \"echo stop 0 | nc localhost 9000\")\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naccepting a new client. Sets up SSL wrapping if necessary.", "response": "def get_request(self):\n        \"\"\"\n        Accepts a new client. Sets up SSL wrapping if necessary.\n\n        :return: A tuple: (client socket, client address tuple)\n        \"\"\"\n        # Accept the client\n        client_socket, client_address = self.socket.accept()\n\n        if ssl is not None and self.cert_file:\n            # Setup an SSL context to accept clients with a certificate\n            # signed by a known chain of authority.\n            # Other clients will be rejected during handshake.\n            context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n            try:\n                # Force a valid/signed client-side certificate\n                context.verify_mode = ssl.CERT_REQUIRED\n\n                # Load the server certificate\n                context.load_cert_chain(\n                    certfile=self.cert_file,\n                    keyfile=self.key_file,\n                    password=self.key_password,\n                )\n\n                if self.ca_file:\n                    # Load the given authority chain\n                    context.load_verify_locations(self.ca_file)\n                else:\n                    # Load the default chain if none given\n                    context.load_default_certs(ssl.Purpose.CLIENT_AUTH)\n            except Exception as ex:\n                # Explicitly log the error as the default behaviour hides it\n                _logger.error(\"Error setting up the SSL context: %s\", ex)\n                raise\n\n            try:\n                # SSL handshake\n                client_stream = context.wrap_socket(\n                    client_socket, server_side=True\n                )\n            except ssl.SSLError as ex:\n                # Explicitly log the exception before re-raising it\n                _logger.warning(\n                    \"Error during SSL handshake with %s: %s\", client_address, ex\n                )\n                raise\n        else:\n            # Nothing to do, use the raw socket\n            client_stream = client_socket\n\n        return client_stream, client_address"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a new thread to process the request.", "response": "def process_request(self, request, client_address):\n        \"\"\"\n        Starts a new thread to process the request, adding the client address\n        in its name.\n        \"\"\"\n        thread = threading.Thread(\n            name=\"RemoteShell-{0}-Client-{1}\".format(\n                self.server_address[1], client_address[:2]\n            ),\n            target=self.process_request_thread,\n            args=(request, client_address),\n        )\n        thread.daemon = self.daemon_threads\n        thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_hook(prompt, session, context, matches, longest_match_len):\n        # type: (str, ShellSession, BundleContext, List[str], int) -> None\n        \"\"\"\n        Displays the available services matches and the service details\n\n        :param prompt: Shell prompt string\n        :param session: Current shell session (for display)\n        :param context: BundleContext of the shell\n        :param matches: List of words matching the substitution\n        :param longest_match_len: Length of the largest match\n        \"\"\"\n        # Prepare a line pattern for each match (-1 for the trailing space)\n        match_pattern = \"{{0: <{}}} from {{1}}\".format(longest_match_len - 1)\n\n        # Sort matching names\n        matches = sorted(match for match in matches)\n\n        # Print the match and the associated name\n        session.write_line()\n        with use_ipopo(context) as ipopo:\n            for factory_name in matches:\n                # Remove the spaces added for the completion\n                factory_name = factory_name.strip()\n                bnd = ipopo.get_factory_bundle(factory_name)\n                session.write_line(\n                    match_pattern, factory_name, bnd.get_symbolic_name()\n                )\n\n        # Print the prompt, then current line\n        session.write(prompt)\n        session.write_line_no_feed(readline.get_line_buffer())\n        readline.redisplay()", "response": "Display the available services matches and the service details."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complete(\n        self, config, prompt, session, context, current_arguments, current\n    ):\n        # type: (CompletionInfo, str, ShellSession, BundleContext, List[str], str) -> List[str]\n        \"\"\"\n        Returns the list of services IDs matching the current state\n\n        :param config: Configuration of the current completion\n        :param prompt: Shell prompt (for re-display)\n        :param session: Shell session (to display in shell)\n        :param context: Bundle context of the Shell bundle\n        :param current_arguments: Current arguments (without the command itself)\n        :param current: Current word\n        :return: A list of matches\n        \"\"\"\n        # Register a method to display helpful completion\n        self.set_display_hook(self.display_hook, prompt, session, context)\n\n        # Return a list of component factories\n        with use_ipopo(context) as ipopo:\n            return [\n                \"{} \".format(factory)\n                for factory in ipopo.get_factories()\n                if factory.startswith(current)\n            ]", "response": "Completes the current word with the current word."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_hook(prompt, session, context, matches, longest_match_len):\n        # type: (str, ShellSession, BundleContext, List[str], int) -> None\n        \"\"\"\n        Displays the available services matches and the service details\n\n        :param prompt: Shell prompt string\n        :param session: Current shell session (for display)\n        :param context: BundleContext of the shell\n        :param matches: List of words matching the substitution\n        :param longest_match_len: Length of the largest match\n        \"\"\"\n        # Prepare a line pattern for each match (-1 for the trailing space)\n        match_pattern = \"{{0: <{}}} from {{1}}\".format(longest_match_len - 1)\n\n        # Sort matching names\n        matches = sorted(match for match in matches)\n\n        # Print the match and the associated name\n        session.write_line()\n        with use_ipopo(context) as ipopo:\n            for name in matches:\n                # Remove the spaces added for the completion\n                name = name.strip()\n                details = ipopo.get_instance_details(name)\n                description = \"of {factory} ({state})\".format(**details)\n                session.write_line(match_pattern, name, description)\n\n        # Print the prompt, then current line\n        session.write(prompt)\n        session.write_line_no_feed(readline.get_line_buffer())\n        readline.redisplay()", "response": "Display the available services matches and the service details"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompleting the current state of the current word.", "response": "def complete(\n        self, config, prompt, session, context, current_arguments, current\n    ):\n        # type: (CompletionInfo, str, ShellSession, BundleContext, List[str], str) -> List[str]\n        \"\"\"\n        Returns the list of services IDs matching the current state\n\n        :param config: Configuration of the current completion\n        :param prompt: Shell prompt (for re-display)\n        :param session: Shell session (to display in shell)\n        :param context: Bundle context of the Shell bundle\n        :param current_arguments: Current arguments (without the command itself)\n        :param current: Current word\n        :return: A list of matches\n        \"\"\"\n        # Register a method to display helpful completion\n        self.set_display_hook(self.display_hook, prompt, session, context)\n\n        # Return a list of component factories\n        with use_ipopo(context) as ipopo:\n            return [\n                \"{} \".format(name)\n                for name, _, _ in ipopo.get_instances()\n                if name.startswith(current)\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef complete(\n        self, config, prompt, session, context, current_arguments, current\n    ):\n        # type: (CompletionInfo, str, ShellSession, BundleContext, List[str], str) -> List[str]\n        \"\"\"\n        Returns the list of services IDs matching the current state\n\n        :param config: Configuration of the current completion\n        :param prompt: Shell prompt (for re-display)\n        :param session: Shell session (to display in shell)\n        :param context: Bundle context of the Shell bundle\n        :param current_arguments: Current arguments (without the command itself)\n        :param current: Current word\n        :return: A list of matches\n        \"\"\"\n        with use_ipopo(context) as ipopo:\n            try:\n                # Find the factory name\n                for idx, completer_id in enumerate(config.completers):\n                    if completer_id == FACTORY:\n                        factory_name = current_arguments[idx]\n                        break\n                else:\n                    # No factory completer found in signature\n                    for idx, completer_id in enumerate(config.completers):\n                        if completer_id == COMPONENT:\n                            name = current_arguments[idx]\n                            details = ipopo.get_instance_details(name)\n                            factory_name = details[\"factory\"]\n                            break\n                    else:\n                        # No factory name can be found\n                        return []\n\n                # Get the details about this factory\n                details = ipopo.get_factory_details(factory_name)\n                properties = details[\"properties\"]\n            except (IndexError, ValueError):\n                # No/unknown factory name\n                return []\n            else:\n                return [\n                    \"{}=\".format(key)\n                    for key in properties\n                    if key.startswith(current)\n                ]", "response": "Returns a list of services IDs that match the current word."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the value of a header.", "response": "def get_header(self, name, default=None):\n        \"\"\"\n        Retrieves the value of a header\n        \"\"\"\n        return self._handler.headers.get(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nending the headers part", "response": "def end_headers(self):\n        \"\"\"\n        Ends the headers part\n        \"\"\"\n        # Send them all at once\n        for name, value in self._headers.items():\n            self._handler.send_header(name, value)\n\n        self._handler.end_headers()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog an error message with the specified arguments.", "response": "def log_error(self, message, *args, **kwargs):\n        # pylint: disable=W0221\n        \"\"\"\n        Log server error\n        \"\"\"\n        self._service.log(logging.ERROR, message, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs a request to the server.", "response": "def log_request(self, code=\"-\", size=\"-\"):\n        \"\"\"\n        Logs a request to the server\n        \"\"\"\n        self._service.log(logging.DEBUG, '\"%s\" %s', self.requestline, code)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_no_servlet_response(self):\n        # Use the helper to send the error page\n        response = _HTTPServletResponse(self)\n        response.send_content(404, self._service.make_not_found_page(self.path))", "response": "Default response sent when no servlet is found for the requested path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_exception(self, response):\n        # Get a formatted stack trace\n        stack = traceback.format_exc()\n\n        # Log the error\n        self.log_error(\n            \"Error handling request upon: %s\\n%s\\n\", self.path, stack\n        )\n\n        # Send the page\n        response.send_content(\n            500, self._service.make_exception_page(self.path, stack)\n        )", "response": "Send an exception page with a 500 error code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef server_bind(self):\n        TCPServer.server_bind(self)\n        host, port = self.socket.getsockname()[:2]\n        self.server_port = port\n        try:\n            self.server_name = socket.getfqdn(host)\n        except ValueError:\n            # Use the local host name in case of error, like CPython does\n            self.server_name = socket.gethostname()", "response": "Override server_bind to store the server name even in CPython."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_list(key, list_):\n    # type: (str, Iterable) -> Dict[str, str]\n    \"\"\"\n    Converts a list into a space-separated string and puts it in a dictionary\n\n    :param key: Dictionary key to store the list\n    :param list_: A list of objects\n    :return: A dictionary key->string or an empty dictionary\n    \"\"\"\n    if not list_:\n        return {}\n    return {key: \" \".join(str(i) for i in list_)}", "response": "Converts a list into a space - separated string and puts it in a dictionary\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the package name of the given module name", "response": "def package_name(package):\n    # type: (str) -> str\n    \"\"\"\n    Returns the package name of the given module name\n    \"\"\"\n    if not package:\n        return \"\"\n\n    lastdot = package.rfind(\".\")\n    if lastdot == -1:\n        return package\n\n    return package[:lastdot]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_osgi_props(ed):\n    # type: (EndpointDescription) -> Dict[str, str]\n    \"\"\"\n    Prepares a dictionary of OSGi properties for the given EndpointDescription\n    \"\"\"\n    result_props = {}\n    intfs = ed.get_interfaces()\n    result_props[OBJECTCLASS] = \" \".join(intfs)\n    for intf in intfs:\n        pkg_name = package_name(intf)\n        ver = ed.get_package_version(pkg_name)\n        if ver and not ver == (0, 0, 0):\n            result_props[ENDPOINT_PACKAGE_VERSION_] = \".\".join(\n                str(v) for v in ver\n            )\n\n    result_props[ENDPOINT_ID] = ed.get_id()\n    result_props[ENDPOINT_SERVICE_ID] = \"{0}\".format(ed.get_service_id())\n    result_props[ENDPOINT_FRAMEWORK_UUID] = ed.get_framework_uuid()\n    imp_configs = ed.get_imported_configs()\n    if imp_configs:\n        result_props[SERVICE_IMPORTED_CONFIGS] = \" \".join(\n            ed.get_imported_configs()\n        )\n    intents = ed.get_intents()\n    if intents:\n        result_props[SERVICE_INTENTS] = \" \".join(intents)\n    remote_configs = ed.get_remote_configs_supported()\n    if remote_configs:\n        result_props[REMOTE_CONFIGS_SUPPORTED] = \" \".join(remote_configs)\n    remote_intents = ed.get_remote_intents_supported()\n    if remote_intents:\n        result_props[REMOTE_INTENTS_SUPPORTED] = \" \".join(remote_intents)\n    return result_props", "response": "Encodes the properties of the given EndpointDescription into a dictionary of OSGi properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecodes a space - separated list of properties", "response": "def decode_list(input_props, name):\n    # type: (Dict[str, str], str) -> List[str]\n    \"\"\"\n    Decodes a space-separated list\n    \"\"\"\n    val_str = input_props.get(name, None)\n    if val_str:\n        return val_str.split(\" \")\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_osgi_props(input_props):\n    # type: (Dict[str, Any]) -> Dict[str, Any]\n    \"\"\"\n    Decodes the OSGi properties of the given endpoint properties\n    \"\"\"\n    result_props = {}\n    intfs = decode_list(input_props, OBJECTCLASS)\n    result_props[OBJECTCLASS] = intfs\n    for intf in intfs:\n        package_key = ENDPOINT_PACKAGE_VERSION_ + package_name(intf)\n        intfversionstr = input_props.get(package_key, None)\n        if intfversionstr:\n            result_props[package_key] = intfversionstr\n    result_props[ENDPOINT_ID] = input_props[ENDPOINT_ID]\n    result_props[ENDPOINT_SERVICE_ID] = input_props[ENDPOINT_SERVICE_ID]\n    result_props[ENDPOINT_FRAMEWORK_UUID] = input_props[ENDPOINT_FRAMEWORK_UUID]\n    imp_configs = decode_list(input_props, SERVICE_IMPORTED_CONFIGS)\n    if imp_configs:\n        result_props[SERVICE_IMPORTED_CONFIGS] = imp_configs\n    intents = decode_list(input_props, SERVICE_INTENTS)\n    if intents:\n        result_props[SERVICE_INTENTS] = intents\n    remote_configs = decode_list(input_props, REMOTE_CONFIGS_SUPPORTED)\n    if remote_configs:\n        result_props[REMOTE_CONFIGS_SUPPORTED] = remote_configs\n    remote_intents = decode_list(input_props, REMOTE_INTENTS_SUPPORTED)\n    if remote_intents:\n        result_props[REMOTE_INTENTS_SUPPORTED] = remote_intents\n    return result_props", "response": "Decodes the OSGi properties of the given endpoint properties into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_endpoint_props(input_props):\n    # type: (Dict) -> Dict[str, Any]\n    \"\"\"\n    Decodes the endpoint properties from the given dictionary\n    \"\"\"\n    ed_props = decode_osgi_props(input_props)\n    ed_props[ECF_ENDPOINT_CONTAINERID_NAMESPACE] = input_props[\n        ECF_ENDPOINT_CONTAINERID_NAMESPACE\n    ]\n    ed_props[ECF_RSVC_ID] = int(input_props[ECF_RSVC_ID])\n    ed_props[ECF_ENDPOINT_ID] = input_props[ECF_ENDPOINT_ID]\n    ed_props[ECF_ENDPOINT_TIMESTAMP] = int(input_props[ECF_ENDPOINT_TIMESTAMP])\n    target_id = input_props.get(ECF_ENDPOINT_CONNECTTARGET_ID, None)\n    if target_id:\n        ed_props[ECF_ENDPOINT_CONNECTTARGET_ID] = target_id\n    id_filters = decode_list(input_props, ECF_ENDPOINT_IDFILTER_IDS)\n    if id_filters:\n        ed_props[ECF_ENDPOINT_IDFILTER_IDS] = id_filters\n    rs_filter = input_props.get(ECF_ENDPOINT_REMOTESERVICE_FILTER, None)\n    if rs_filter:\n        ed_props[ECF_ENDPOINT_REMOTESERVICE_FILTER] = rs_filter\n    async_intfs = input_props.get(ECF_SERVICE_EXPORTED_ASYNC_INTERFACES, None)\n    if async_intfs:\n        if async_intfs == \"*\":\n            ed_props[ECF_SERVICE_EXPORTED_ASYNC_INTERFACES] = async_intfs\n        else:\n            async_intfs = decode_list(\n                input_props, ECF_SERVICE_EXPORTED_ASYNC_INTERFACES\n            )\n            if async_intfs:\n                ed_props[ECF_SERVICE_EXPORTED_ASYNC_INTERFACES] = async_intfs\n\n    for key in input_props.keys():\n        if not is_reserved_property(key):\n            val = input_props.get(key, None)\n            if val:\n                ed_props[key] = val\n    return ed_props", "response": "Decodes the endpoint properties from the given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_endpoint_props(ed):\n    props = encode_osgi_props(ed)\n    props[ECF_RSVC_ID] = \"{0}\".format(ed.get_remoteservice_id()[1])\n    props[ECF_ENDPOINT_ID] = \"{0}\".format(ed.get_container_id()[1])\n    props[ECF_ENDPOINT_CONTAINERID_NAMESPACE] = \"{0}\".format(\n        ed.get_container_id()[0]\n    )\n    props[ECF_ENDPOINT_TIMESTAMP] = \"{0}\".format(ed.get_timestamp())\n    ctid = ed.get_connect_target_id()\n    if ctid:\n        props[ECF_ENDPOINT_CONNECTTARGET_ID] = \"{0}\".format(ctid)\n    id_filters = ed.get_id_filters()\n    if id_filters:\n        props[ECF_ENDPOINT_IDFILTER_IDS] = \" \".join([x[1] for x in id_filters])\n    rs_filter = ed.get_remoteservice_filter()\n    if rs_filter:\n        props[ECF_ENDPOINT_REMOTESERVICE_FILTER] = ed.get_remoteservice_filter()\n    async_intfs = ed.get_async_interfaces()\n    if async_intfs:\n        props[ECF_SERVICE_EXPORTED_ASYNC_INTERFACES] = \" \".join(async_intfs)\n\n    all_props = ed.get_properties()\n    other_props = {\n        key: all_props[key]\n        for key in all_props.keys()\n        if not is_reserved_property(key)\n    }\n    return merge_dicts(props, other_props)", "response": "Encodes the properties of the given EndpointDescription into a dictionary of properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproviding the version of the given package name.", "response": "def get_package_version(self, package):\n        # type: (str) -> Tuple[int, int, int]\n        \"\"\"\n        Provides the version of the given package name.\n\n        :param package: The name of the package\n        :return: The version of the specified package as a tuple or (0,0,0)\n        \"\"\"\n        name = \"{0}{1}\".format(ENDPOINT_PACKAGE_VERSION_, package)\n        try:\n            # Get the version string\n            version = self._properties[name]\n            # Split dots ('.')\n            return tuple(version.split(\".\"))\n        except KeyError:\n            # No version\n            return 0, 0, 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_same_service(self, endpoint):\n        # type: (EndpointDescription) -> bool\n        \"\"\"\n        Tests if this endpoint and the given one have the same framework UUID\n        and service ID\n\n        :param endpoint: Another endpoint\n        :return: True if both endpoints represent the same remote service\n        \"\"\"\n        return (\n            self.get_framework_uuid() == endpoint.get_framework_uuid()\n            and self.get_service_id() == endpoint.get_service_id()\n        )", "response": "Tests if this endpoint and the given one have the same framework UUID\n        and service ID\n        and framework ID\n        and service ID\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_id(cls, prefix=\"pelix-\"):\n        if not prefix:\n            # Normalize string\n            prefix = \"\"\n        else:\n            # Truncate long prefixes\n            prefix = prefix[:8]\n\n        # Prepare the missing part\n        nb_bytes = (23 - len(prefix)) // 2\n\n        random_bytes = os.urandom(nb_bytes)\n        if sys.version_info[0] >= 3:\n            random_ints = [char for char in random_bytes]\n        else:\n            random_ints = [ord(char) for char in random_bytes]\n\n        random_id = \"\".join(\"{0:02x}\".format(value) for value in random_ints)\n        return \"{0}{1}\".format(prefix, random_id)", "response": "Generates a random MQTT client ID"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_will(self, topic, payload, qos=0, retain=False):\n        self.__mqtt.will_set(topic, payload, qos, retain=retain)", "response": "Sets up the will message for the given topic and payload."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, host=\"localhost\", port=1883, keepalive=60):\n        # Disconnect first (it also stops the timer)\n        self.disconnect()\n\n        # Prepare the connection\n        self.__mqtt.connect(host, port, keepalive)\n\n        # Start the MQTT loop\n        self.__mqtt.loop_start()", "response": "Connects to the MQTT server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disconnect(self):\n        # Stop the timer\n        self.__stop_timer()\n\n        # Unlock all publishers\n        for event in self.__in_flight.values():\n            event.set()\n\n        # Disconnect from the server\n        self.__mqtt.disconnect()\n\n        # Stop the MQTT loop thread\n        # Use a thread to avoid a dead lock in Paho\n        thread = threading.Thread(target=self.__mqtt.loop_stop)\n        thread.daemon = True\n        thread.start()\n\n        # Give it some time\n        thread.join(4)", "response": "Disconnects from the MQTT server and unlocks all publishers and queues."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message through the MQTT connection and waits for the message to be published.", "response": "def publish(self, topic, payload, qos=0, retain=False, wait=False):\n        \"\"\"\n        Sends a message through the MQTT connection\n\n        :param topic: Message topic\n        :param payload: Message content\n        :param qos: Quality of Service\n        :param retain: Retain flag\n        :param wait: If True, prepares an event to wait for the message to be\n                     published\n        :return: The local message ID, None on error\n        \"\"\"\n        result = self.__mqtt.publish(topic, payload, qos, retain)\n        if wait and not result[0]:\n            # Publish packet sent, wait for it to return\n            self.__in_flight[result[1]] = threading.Event()\n            _logger.debug(\"Waiting for publication of %s\", topic)\n\n        return result[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the reconnection timer", "response": "def __start_timer(self, delay):\n        \"\"\"\n        Starts the reconnection timer\n\n        :param delay: Delay (in seconds) before calling the reconnection method\n        \"\"\"\n        self.__timer = threading.Timer(delay, self.__reconnect)\n        self.__timer.daemon = True\n        self.__timer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to reconnect to the MQTT server and returns None", "response": "def __reconnect(self):\n        \"\"\"\n        Tries to connect to the MQTT server\n        \"\"\"\n        # Cancel the timer, if any\n        self.__stop_timer()\n\n        try:\n            # Try to reconnect the server\n            result_code = self.__mqtt.reconnect()\n            if result_code:\n                # Something wrong happened\n                message = \"Error connecting the MQTT server: {0} ({1})\".format(\n                    result_code, paho.error_string(result_code)\n                )\n                _logger.error(message)\n                raise ValueError(message)\n        except Exception as ex:\n            # Something went wrong: log it\n            _logger.error(\"Exception connecting server: %s\", ex)\n        finally:\n            # Prepare a reconnection timer. It will be cancelled by the\n            # on_connect callback\n            self.__start_timer(10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __on_connect(self, client, userdata, flags, result_code):\n        # pylint: disable=W0613\n        \"\"\"\n        Client connected to the server\n\n        :param client: Connected Paho client\n        :param userdata: User data (unused)\n        :param flags: Response flags sent by the broker\n        :param result_code: Connection result code (0: success, others: error)\n        \"\"\"\n        if result_code:\n            # result_code != 0: something wrong happened\n            _logger.error(\n                \"Error connecting the MQTT server: %s (%d)\",\n                paho.connack_string(result_code),\n                result_code,\n            )\n        else:\n            # Connection is OK: stop the reconnection timer\n            self.__stop_timer()\n\n        # Notify the caller, if any\n        if self.on_connect is not None:\n            try:\n                self.on_connect(self, result_code)\n            except Exception as ex:\n                _logger.exception(\"Error notifying MQTT listener: %s\", ex)", "response": "Callback function called by the broker when a connection is made."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __on_message(self, client, userdata, msg):\n        # pylint: disable=W0613\n        \"\"\"\n        A message has been received from a server\n\n        :param client: Client that received the message\n        :param userdata: User data (unused)\n        :param msg: A MQTTMessage bean\n        \"\"\"\n        # Notify the caller, if any\n        if self.on_message is not None:\n            try:\n                self.on_message(self, msg)\n            except Exception as ex:\n                _logger.exception(\"Error notifying MQTT listener: %s\", ex)", "response": "Called by the client when a message is received from a server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall when a message is published by a server.", "response": "def __on_publish(self, client, userdata, mid):\n        # pylint: disable=W0613\n        \"\"\"\n        A message has been published by a server\n\n        :param client: Client that received the message\n        :param userdata: User data (unused)\n        :param mid: Message ID\n        \"\"\"\n        try:\n            self.__in_flight[mid].set()\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_import_properties(properties):\n    # type: (dict) -> dict\n    \"\"\"\n    Returns a dictionary where export properties have been replaced by import\n    ones\n\n    :param properties: A dictionary of service properties (with export keys)\n    :return: A dictionary with import properties\n    \"\"\"\n    # Copy the given dictionary\n    props = properties.copy()\n\n    # Add the \"imported\" property\n    props[pelix.remote.PROP_IMPORTED] = True\n\n    # Remote service ID\n    try:\n        props[pelix.remote.PROP_ENDPOINT_SERVICE_ID] = props.pop(\n            pelix.constants.SERVICE_ID\n        )\n    except KeyError:\n        # No service ID\n        pass\n\n    # Replace the \"export configs\"\n    configs = props.pop(pelix.remote.PROP_EXPORTED_CONFIGS, None)\n    if configs:\n        props[pelix.remote.PROP_IMPORTED_CONFIGS] = configs\n\n    # Clear other export properties\n    for key in (\n        pelix.remote.PROP_EXPORTED_INTENTS,\n        pelix.remote.PROP_EXPORTED_INTENTS_EXTRA,\n        pelix.remote.PROP_EXPORTED_INTERFACES,\n    ):\n        try:\n            del props[key]\n        except KeyError:\n            # Key wasn't there\n            pass\n\n    return props", "response": "Converts a dictionary of export properties to a dictionary of service properties that have been replaced by import properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_exported_specifications(svc_ref):\n    # type: (pelix.framework.ServiceReference) -> List[str]\n    \"\"\"\n    Computes the list of specifications exported by the given service\n\n    :param svc_ref: A ServiceReference\n    :return: The list of exported specifications (or an empty list)\n    \"\"\"\n    if svc_ref.get_property(pelix.remote.PROP_EXPORT_NONE):\n        # The export of this service is explicitly forbidden, stop here\n        return []\n\n    # Service specifications\n    specs = svc_ref.get_property(pelix.constants.OBJECTCLASS)\n\n    # Exported specifications\n    exported_specs = svc_ref.get_property(pelix.remote.PROP_EXPORTED_INTERFACES)\n\n    if exported_specs and exported_specs != \"*\":\n        # A set of specifications is exported, replace \"objectClass\"\n        iterable_exports = pelix.utilities.to_iterable(exported_specs, False)\n        all_exported_specs = [\n            spec for spec in specs if spec in iterable_exports\n        ]\n    else:\n        # Export everything\n        all_exported_specs = pelix.utilities.to_iterable(specs)\n\n    # Authorized and rejected specifications\n    export_only_specs = pelix.utilities.to_iterable(\n        svc_ref.get_property(pelix.remote.PROP_EXPORT_ONLY), False\n    )\n\n    if export_only_specs:\n        # Filter specifications (keep authorized specifications)\n        return [\n            spec for spec in all_exported_specs if spec in export_only_specs\n        ]\n\n    # Filter specifications (reject)\n    rejected_specs = pelix.utilities.to_iterable(\n        svc_ref.get_property(pelix.remote.PROP_EXPORT_REJECT), False\n    )\n    return [spec for spec in all_exported_specs if spec not in rejected_specs]", "response": "Computes the list of exported specifications for the given service"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert python:/ name specifications to name. Keeps the other specifications as is.", "response": "def extract_specifications(specifications, properties):\n    # type: (Any[str, List[str]], dict) -> List[str]\n    \"\"\"\n    Converts \"python:/name\" specifications to \"name\". Keeps the other\n    specifications as is.\n\n    :param specifications: The specifications found in a remote registration\n    :param properties: Service properties\n    :return: The filtered specifications (as a list)\n    \"\"\"\n    all_specs = set(pelix.utilities.to_iterable(specifications))\n    try:\n        synonyms = pelix.utilities.to_iterable(\n            properties[pelix.remote.PROP_SYNONYMS], False\n        )\n        all_specs.update(synonyms)\n    except KeyError:\n        # No synonyms property\n        pass\n\n    filtered_specs = set()\n    for original in all_specs:\n        try:\n            # Extract information\n            lang, spec = _extract_specification_parts(original)\n            if lang == PYTHON_LANGUAGE:\n                # Language match: keep the name only\n                filtered_specs.add(spec)\n            else:\n                # Keep the name as is\n                filtered_specs.add(original)\n        except ValueError:\n            # Ignore invalid specifications\n            pass\n\n    return list(filtered_specs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform the interfaces names into URI strings with the interface implementation language as a scheme.", "response": "def format_specifications(specifications):\n    # type: (Iterable[str]) -> List[str]\n    \"\"\"\n    Transforms the interfaces names into URI strings, with the interface\n    implementation language as a scheme.\n\n    :param specifications: Specifications to transform\n    :return: The transformed names\n    \"\"\"\n    transformed = set()\n    for original in specifications:\n        try:\n            lang, spec = _extract_specification_parts(original)\n            transformed.add(_format_specification(lang, spec))\n        except ValueError:\n            # Ignore invalid specifications\n            pass\n\n    return list(transformed)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_specification_parts(specification):\n    # type: (str) -> Tuple[str, str]\n    \"\"\"\n    Extract the language and the interface from a \"language:/interface\"\n    interface name\n\n    :param specification: The formatted interface name\n    :return: A (language, interface name) tuple\n    :raise ValueError: Invalid specification content\n    \"\"\"\n    try:\n        # Parse the URI-like string\n        parsed = urlparse(specification)\n    except:\n        # Invalid URL\n        raise ValueError(\"Invalid specification URL: {0}\".format(specification))\n\n    # Extract the interface name\n    interface = parsed.path\n\n    # Extract the language, if given\n    language = parsed.scheme\n    if not language:\n        # Simple name, without scheme\n        language = PYTHON_LANGUAGE\n    else:\n        # Formatted name: un-escape it, without the starting '/'\n        interface = _unescape_specification(interface[1:])\n\n    return language, interface", "response": "Extracts the language and interface from a language - specific specification."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn merged properties with the properties of the current object class and service ID.", "response": "def get_properties(self):\n        # type: () -> dict\n        \"\"\"\n        Returns merged properties\n\n        :return: Endpoint merged properties\n        \"\"\"\n        # Get service properties\n        properties = self.__reference.get_properties()\n\n        # Merge with local properties\n        properties.update(self.__properties)\n\n        # Some properties can't be merged\n        for key in pelix.constants.OBJECTCLASS, pelix.constants.SERVICE_ID:\n            properties[key] = self.__reference.get_property(key)\n\n        # Force the exported configurations\n        properties[pelix.remote.PROP_EXPORTED_CONFIGS] = self.configurations\n\n        return properties"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_import_properties(self):\n        # type: () -> dict\n        \"\"\"\n        Returns the properties of this endpoint where export properties have\n        been replaced by import ones\n\n        :return: A dictionary with import properties\n        \"\"\"\n        # Convert merged properties\n        props = to_import_properties(self.get_properties())\n\n        # Add the framework UID\n        props[pelix.remote.PROP_ENDPOINT_FRAMEWORK_UUID] = self.__fw_uid\n        return props", "response": "Returns the properties of this endpoint where export properties have\n        been replaced by import ones"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the given dictionary doesn t have export keys and has import keys.", "response": "def __check_properties(props):\n        # type: (dict) -> None\n        \"\"\"\n        Checks that the given dictionary doesn't have export keys and has\n        import keys\n\n        :param props: Properties to validate\n        :raise ValueError: Invalid properties\n        \"\"\"\n        # Mandatory properties\n        mandatory = (\n            pelix.remote.PROP_ENDPOINT_ID,\n            pelix.remote.PROP_IMPORTED_CONFIGS,\n            pelix.constants.OBJECTCLASS,\n        )\n        for key in mandatory:\n            if key not in props:\n                raise ValueError(\"Missing property: {0}\".format(key))\n\n        # Export/Import properties\n        props_export = (\n            pelix.remote.PROP_EXPORTED_CONFIGS,\n            pelix.remote.PROP_EXPORTED_INTERFACES,\n        )\n\n        for key in props_export:\n            if key in props:\n                raise ValueError(\"Export property found: {0}\".format(key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprovides the version of the given package name.", "response": "def get_package_version(self, package):\n        # type: (str) -> Tuple[int, ...]\n        \"\"\"\n        Provides the version of the given package name.\n\n        :param package: The name of the package\n        :return: The version of the specified package as a tuple or (0,0,0)\n        \"\"\"\n        name = \"{0}{1}\".format(\n            pelix.remote.PROP_ENDPOINT_PACKAGE_VERSION_, package\n        )\n        try:\n            # Get the version string\n            version = self.__properties[name]\n\n            # Split dots ('.')\n            return tuple(version.split(\".\"))\n        except KeyError:\n            # No version\n            return 0, 0, 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef matches(self, ldap_filter):\n        # type: (Any[str, pelix.ldapfilter.LDAPFilter]) -> bool\n        \"\"\"\n        Tests the properties of this EndpointDescription against the given\n        filter\n\n        :param ldap_filter: A filter\n        :return: True if properties matches the filter\n        \"\"\"\n        return pelix.ldapfilter.get_ldap_filter(ldap_filter).matches(\n            self.__properties\n        )", "response": "Tests if the given ldap_filter matches the properties of this EndpointDescription."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_import(self):\n        # type: () -> ImportEndpoint\n        \"\"\"\n        Converts an EndpointDescription bean to an ImportEndpoint\n\n        :return: An ImportEndpoint bean\n        \"\"\"\n        # Properties\n        properties = self.get_properties()\n\n        # Framework UUID\n        fw_uid = self.get_framework_uuid()\n\n        # Endpoint name\n        try:\n            # From Pelix UID\n            name = properties[pelix.remote.PROP_ENDPOINT_NAME]\n        except KeyError:\n            # Generated\n            name = \"{0}.{1}\".format(fw_uid, self.get_service_id())\n\n        # Configuration / kind\n        configurations = self.get_configuration_types()\n\n        # Interfaces\n        specifications = self.get_interfaces()\n\n        return ImportEndpoint(\n            self.get_id(),\n            fw_uid,\n            configurations,\n            name,\n            specifications,\n            properties,\n        )", "response": "Converts an EndpointDescription bean to an ImportEndpoint bean"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an ExportEndpoint bean to an EndpointDescription object.", "response": "def from_export(cls, endpoint):\n        # type: (ExportEndpoint) -> EndpointDescription\n        \"\"\"\n        Converts an ExportEndpoint bean to an EndpointDescription\n\n        :param endpoint: An ExportEndpoint bean\n        :return: An EndpointDescription bean\n        \"\"\"\n        assert isinstance(endpoint, ExportEndpoint)\n\n        # Service properties\n        properties = endpoint.get_properties()\n\n        # Set import keys\n        properties[pelix.remote.PROP_ENDPOINT_ID] = endpoint.uid\n        properties[pelix.remote.PROP_IMPORTED_CONFIGS] = endpoint.configurations\n        properties[\n            pelix.remote.PROP_EXPORTED_INTERFACES\n        ] = endpoint.specifications\n\n        # Remove export keys\n        for key in (\n            pelix.remote.PROP_EXPORTED_CONFIGS,\n            pelix.remote.PROP_EXPORTED_INTERFACES,\n            pelix.remote.PROP_EXPORTED_INTENTS,\n            pelix.remote.PROP_EXPORTED_INTENTS_EXTRA,\n        ):\n            try:\n                del properties[key]\n            except KeyError:\n                pass\n\n        # Other information\n        properties[pelix.remote.PROP_ENDPOINT_NAME] = endpoint.name\n        properties[\n            pelix.remote.PROP_ENDPOINT_FRAMEWORK_UUID\n        ] = endpoint.framework\n\n        return EndpointDescription(None, properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_description(self, node):\n        # type: (ElementTree.Element) -> EndpointDescription\n        \"\"\"\n        Parse an endpoint description node\n\n        :param node: The endpoint description node\n        :return: The parsed EndpointDescription bean\n        :raise KeyError: Attribute missing\n        :raise ValueError: Invalid description\n        \"\"\"\n        endpoint = {}\n        for prop_node in node.findall(TAG_PROPERTY):\n            name, value = self._parse_property(prop_node)\n            endpoint[name] = value\n\n        return EndpointDescription(None, endpoint)", "response": "Parses an endpoint description node and returns a EndpointDescription object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a property node and returns a name and value tuple.", "response": "def _parse_property(self, node):\n        # type: (ElementTree.Element) -> Tuple[str, Any]\n        \"\"\"\n        Parses a property node\n\n        :param node: The property node\n        :return: A (name, value) tuple\n        :raise KeyError: Attribute missing\n        \"\"\"\n        # Get information\n        name = node.attrib[ATTR_NAME]\n        vtype = node.attrib.get(ATTR_VALUE_TYPE, TYPE_STRING)\n\n        # Look for a value as a single child node\n        try:\n            value_node = next(iter(node))\n            value = self._parse_value_node(vtype, value_node)\n        except StopIteration:\n            # Value is an attribute\n            value = self._convert_value(vtype, node.attrib[ATTR_VALUE])\n\n        return name, value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_value_node(self, vtype, node):\n        # type: (str, ElementTree.Element) -> Any\n        \"\"\"\n        Parses a value node\n\n        :param vtype: The value type\n        :param node: The value node\n        :return: The parsed value\n        \"\"\"\n        kind = node.tag\n        if kind == TAG_XML:\n            # Raw XML value\n            return next(iter(node))\n\n        elif kind == TAG_LIST or kind == TAG_ARRAY:\n            # List\n            return [\n                self._convert_value(vtype, value_node.text)\n                for value_node in node.findall(TAG_VALUE)\n            ]\n\n        elif kind == TAG_SET:\n            # Set\n            return set(\n                self._convert_value(vtype, value_node.text)\n                for value_node in node.findall(TAG_VALUE)\n            )\n\n        else:\n            # Unknown\n            raise ValueError(\"Unknown value tag: {0}\".format(kind))", "response": "Parses a value node and returns the parsed value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an EDEF XML string containing an EndpointDescription.", "response": "def parse(self, xml_str):\n        # type: (str) -> List[EndpointDescription]\n        \"\"\"\n        Parses an EDEF XML string\n\n        :param xml_str: An XML string\n        :return: The list of parsed EndpointDescription\n        \"\"\"\n        # Parse the document\n        root = ElementTree.fromstring(xml_str)\n        if root.tag != TAG_ENDPOINT_DESCRIPTIONS:\n            raise ValueError(\"Not an EDEF XML: {0}\".format(root.tag))\n\n        # Parse content\n        return [\n            self._parse_description(node)\n            for node in root.findall(TAG_ENDPOINT_DESCRIPTION)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_endpoint(self, root_node, endpoint):\n        # type: (ElementTree.Element, EndpointDescription) -> None\n        \"\"\"\n        Converts the given endpoint bean to an XML Element\n\n        :param root_node: The XML root Element\n        :param endpoint: An EndpointDescription bean\n        \"\"\"\n        endpoint_node = ElementTree.SubElement(\n            root_node, TAG_ENDPOINT_DESCRIPTION\n        )\n\n        for name, value in endpoint.get_properties().items():\n            # Compute value type\n            vtype = self._get_type(name, value)\n\n            # Prepare the property node\n            prop_node = ElementTree.SubElement(\n                endpoint_node, TAG_PROPERTY, {ATTR_NAME: name}\n            )\n\n            if vtype == XML_VALUE:\n                # Special case, we have to store the value as a child\n                # without a value-type attribute\n                prop_node.append(value)\n                continue\n\n            # Set the value type\n            prop_node.set(ATTR_VALUE_TYPE, vtype)\n\n            # Compute value node or attribute\n            if isinstance(value, tuple):\n                # Array\n                self._add_container(prop_node, TAG_ARRAY, value)\n\n            elif isinstance(value, list):\n                # List\n                self._add_container(prop_node, TAG_ARRAY, value)\n\n            elif isinstance(value, set):\n                # Set\n                self._add_container(prop_node, TAG_SET, value)\n\n            elif isinstance(value, type(root_node)):\n                # XML (direct addition)\n                prop_node.append(value)\n\n            else:\n                # Simple value -> Attribute\n                prop_node.set(ATTR_VALUE, str(value))", "response": "Converts the given endpoint bean to an XML Element"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the given endpoint description beans into an XML Element containing the XML document.", "response": "def _make_xml(self, endpoints):\n        # type: (List[EndpointDescription]) -> ElementTree.Element\n        \"\"\"\n        Converts the given endpoint description beans into an XML Element\n\n        :param endpoints: A list of EndpointDescription beans\n        :return: A string containing an XML document\n        \"\"\"\n        root = ElementTree.Element(TAG_ENDPOINT_DESCRIPTIONS)\n        for endpoint in endpoints:\n            self._make_endpoint(root, endpoint)\n\n        # Prepare pretty-printing\n        self._indent(root)\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write(self, endpoints, filename):\n        # type: (List[EndpointDescription], str) -> None\n        \"\"\"\n        Writes the given endpoint descriptions to the given file\n\n        :param endpoints: A list of EndpointDescription beans\n        :param filename: Name of the file where to write the XML\n        :raise IOError: Error writing the file\n        \"\"\"\n        with open(filename, \"w\") as filep:\n            filep.write(self.to_string(endpoints))", "response": "Writes the given endpoint descriptions to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntesting if the current attribute value is shared by a parent of the given base class.", "response": "def is_from_parent(cls, attribute_name, value=None):\n    # type: (type, str, bool) -> bool\n    \"\"\"\n    Tests if the current attribute value is shared by a parent of the given\n    class.\n\n    Returns None if the attribute value is None.\n\n    :param cls: Child class with the requested attribute\n    :param attribute_name: Name of the attribute to be tested\n    :param value: The exact value in the child class (optional)\n    :return: True if the attribute value is shared with a parent class\n    \"\"\"\n    if value is None:\n        try:\n            # Get the current value\n            value = getattr(cls, attribute_name)\n        except AttributeError:\n            # No need to go further: the attribute does not exist\n            return False\n\n    for base in cls.__bases__:\n        # Look for the value in each parent class\n        try:\n            return getattr(base, attribute_name) is value\n        except AttributeError:\n            pass\n\n    # Attribute value not found in parent classes\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the factory context object associated to a factory class. Creates it if needed.", "response": "def get_factory_context(cls):\n    # type: (type) -> FactoryContext\n    \"\"\"\n    Retrieves the factory context object associated to a factory. Creates it\n    if needed\n\n    :param cls: The factory class\n    :return: The factory class context\n    \"\"\"\n    context = getattr(cls, constants.IPOPO_FACTORY_CONTEXT, None)\n\n    if context is None:\n        # Class not yet manipulated\n        context = FactoryContext()\n    elif is_from_parent(cls, constants.IPOPO_FACTORY_CONTEXT):\n        # Create a copy the context\n        context = context.copy(True)\n        # * Manipulation has not been applied yet\n        context.completed = False\n    else:\n        # Nothing special to do\n        return context\n\n    # Context has been created or copied, inject the new bean\n    setattr(cls, constants.IPOPO_FACTORY_CONTEXT, context)\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_method_description(method):\n    # type: (Callable) -> str\n    \"\"\"\n    Retrieves a description of the given method. If possible, the description\n    contains the source file name and line.\n\n    :param method: A method\n    :return: A description of the method (at least its name)\n    :raise AttributeError: Given object has no __name__ attribute\n    \"\"\"\n    try:\n        try:\n            line_no = inspect.getsourcelines(method)[1]\n        except IOError:\n            # Error reading the source file\n            line_no = -1\n\n        return \"'{method}' ({file}:{line})\".format(\n            method=method.__name__, file=inspect.getfile(method), line=line_no\n        )\n    except TypeError:\n        # Method can't be inspected\n        return \"'{0}'\".format(method.__name__)", "response": "Returns a description of the given method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if the decorated method has sufficient number of parameters.", "response": "def validate_method_arity(method, *needed_args):\n    # type: (Callable, *str) -> None\n    \"\"\"\n    Tests if the decorated method has a sufficient number of parameters.\n\n    :param method: The method to be tested\n    :param needed_args: The name (for description only) of the needed\n                        arguments, without \"self\".\n    :return: Nothing\n    :raise TypeError: Invalid number of parameter\n    \"\"\"\n    nb_needed_args = len(needed_args)\n\n    # Test the number of parameters\n    arg_spec = get_method_arguments(method)\n    method_args = arg_spec.args\n\n    try:\n        # Remove the self argument when present\n        if method_args[0] == \"self\":\n            del method_args[0]\n    except IndexError:\n        pass\n\n    nb_args = len(method_args)\n\n    if arg_spec.varargs is not None:\n        # Variable arguments\n        if nb_args != 0:\n            # Other arguments detected\n            raise TypeError(\n                \"When using '*args', the decorated {0} method must only \"\n                \"accept the 'self' argument\".format(\n                    get_method_description(method)\n                )\n            )\n    elif arg_spec.keywords is not None:\n        raise TypeError(\"Methods using '**kwargs' are not handled\")\n    elif nb_args != nb_needed_args:\n        # \"Normal\" arguments\n        raise TypeError(\n            \"The decorated method {0} must accept exactly {1} parameters: \"\n            \"(self, {2})\".format(\n                get_method_description(method),\n                nb_needed_args + 1,\n                \", \".join(needed_args),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ipopo_setup_callback(cls, context):\n    # type: (type, FactoryContext) -> None\n    \"\"\"\n    Sets up the class _callback dictionary\n\n    :param cls: The class to handle\n    :param context: The factory class context\n    \"\"\"\n    assert inspect.isclass(cls)\n    assert isinstance(context, FactoryContext)\n\n    if context.callbacks is not None:\n        callbacks = context.callbacks.copy()\n    else:\n        callbacks = {}\n\n    functions = inspect.getmembers(cls, inspect.isroutine)\n\n    for _, func in functions:\n        if not hasattr(func, constants.IPOPO_METHOD_CALLBACKS):\n            # No attribute, get the next member\n            continue\n\n        method_callbacks = getattr(func, constants.IPOPO_METHOD_CALLBACKS)\n        if not isinstance(method_callbacks, list):\n            # Invalid content\n            _logger.warning(\n                \"Invalid callback information %s in %s\",\n                constants.IPOPO_METHOD_CALLBACKS,\n                get_method_description(func),\n            )\n            continue\n\n        # Keeping it allows inheritance : by removing it, only the first\n        # child will see the attribute -> Don't remove it\n\n        # Store the call backs\n        for _callback in method_callbacks:\n            if _callback in callbacks and not is_from_parent(\n                cls, callbacks[_callback].__name__, callbacks[_callback]\n            ):\n                _logger.warning(\n                    \"Redefining the callback %s in class '%s'.\\n\"\n                    \"\\tPrevious callback : %s\\n\"\n                    \"\\tNew callback : %s\",\n                    _callback,\n                    cls.__name__,\n                    get_method_description(callbacks[_callback]),\n                    get_method_description(func),\n                )\n\n            callbacks[_callback] = func\n\n    # Update the factory context\n    context.callbacks.clear()\n    context.callbacks.update(callbacks)", "response": "Sets up the class _callback dictionary for the class cls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the class _field_callback dictionary for the class cls.", "response": "def _ipopo_setup_field_callback(cls, context):\n    # type: (type, FactoryContext) -> None\n    \"\"\"\n    Sets up the class _field_callback dictionary\n\n    :param cls: The class to handle\n    :param context: The factory class context\n    \"\"\"\n    assert inspect.isclass(cls)\n    assert isinstance(context, FactoryContext)\n\n    if context.field_callbacks is not None:\n        callbacks = context.field_callbacks.copy()\n    else:\n        callbacks = {}\n\n    functions = inspect.getmembers(cls, inspect.isroutine)\n    for name, func in functions:\n        if not hasattr(func, constants.IPOPO_METHOD_FIELD_CALLBACKS):\n            # No attribute, get the next member\n            continue\n\n        method_callbacks = getattr(func, constants.IPOPO_METHOD_FIELD_CALLBACKS)\n        if not isinstance(method_callbacks, list):\n            # Invalid content\n            _logger.warning(\n                \"Invalid attribute %s in %s\",\n                constants.IPOPO_METHOD_FIELD_CALLBACKS,\n                name,\n            )\n            continue\n\n        # Keeping it allows inheritance : by removing it, only the first\n        # child will see the attribute -> Don't remove it\n\n        # Store the call backs\n        for kind, field, if_valid in method_callbacks:\n            fields_cbs = callbacks.setdefault(field, {})\n\n            if kind in fields_cbs and not is_from_parent(\n                cls, fields_cbs[kind][0].__name__\n            ):\n                _logger.warning(\n                    \"Redefining the callback %s in '%s'. \"\n                    \"Previous callback : '%s' (%s). \"\n                    \"New callback : %s\",\n                    kind,\n                    name,\n                    fields_cbs[kind][0].__name__,\n                    fields_cbs[kind][0],\n                    func,\n                )\n\n            fields_cbs[kind] = (func, if_valid)\n\n    # Update the factory context\n    context.field_callbacks.clear()\n    context.field_callbacks.update(callbacks)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _append_object_entry(obj, list_name, entry):\n    # type: (Any, str, Any) -> None\n    \"\"\"\n    Appends the given entry in the given object list.\n    Creates the list field if needed.\n\n    :param obj: The object that contains the list\n    :param list_name: The name of the list member in *obj*\n    :param entry: The entry to be added to the list\n    :raise ValueError: Invalid attribute content\n    \"\"\"\n    # Get the list\n    obj_list = getattr(obj, list_name, None)\n    if obj_list is None:\n        # We'll have to create it\n        obj_list = []\n        setattr(obj, list_name, obj_list)\n\n    assert isinstance(obj_list, list)\n\n    # Set up the property, if needed\n    if entry not in obj_list:\n        obj_list.append(entry)", "response": "Adds the given entry to the given object list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a property that will be set on the iPOPO class field with the given name and default value.", "response": "def _ipopo_class_field_property(name, value, methods_prefix):\n    # type: (str, Any, str) -> property\n    \"\"\"\n    Sets up an iPOPO field property, using Python property() capabilities\n\n    :param name: The property name\n    :param value: The property default value\n    :param methods_prefix: The common prefix of the getter and setter injected\n                           methods\n    :return: A generated Python property()\n    \"\"\"\n    # The property lock\n    lock = threading.RLock()\n\n    # Prepare the methods names\n    getter_name = \"{0}{1}\".format(methods_prefix, constants.IPOPO_GETTER_SUFFIX)\n    setter_name = \"{0}{1}\".format(methods_prefix, constants.IPOPO_SETTER_SUFFIX)\n\n    local_holder = Holder(value)\n\n    def get_value(self):\n        \"\"\"\n        Retrieves the property value, from the iPOPO dictionaries\n        \"\"\"\n        getter = getattr(self, getter_name, None)\n        if getter is not None:\n            # Use the component getter\n            with lock:\n                return getter(self, name)\n        else:\n            # Use the local holder\n            return local_holder.value\n\n    def set_value(self, new_value):\n        \"\"\"\n        Sets the property value and trigger an update event\n\n        :param new_value: The new property value\n        \"\"\"\n        setter = getattr(self, setter_name, None)\n        if setter is not None:\n            # Use the component setter\n            with lock:\n                setter(self, name, new_value)\n        else:\n            # Change the local holder\n            local_holder.value = new_value\n\n    return property(get_value, set_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_specifications(specifications):\n    if not specifications or specifications is object:\n        raise ValueError(\"No specifications given\")\n    elif inspect.isclass(specifications):\n        if Provides.USE_MODULE_QUALNAME:\n            if sys.version_info < (3, 3, 0):\n                raise ValueError(\n                    \"Qualified name capability requires Python 3.3+\"\n                )\n\n            # Get the name of the class\n            if not specifications.__module__:\n                return [specifications.__qualname__]\n\n            return [\n                \"{0}.{1}\".format(\n                    specifications.__module__, specifications.__qualname__\n                )\n            ]\n        else:\n            # Legacy behavior\n            return [specifications.__name__]\n    elif is_string(specifications):\n        # Specification name\n        specifications = specifications.strip()\n        if not specifications:\n            raise ValueError(\"Empty specification given\")\n        return [specifications]\n    elif isinstance(specifications, (list, tuple)):\n        # List given: normalize its content\n        results = []\n        for specification in specifications:\n            results.extend(_get_specifications(specification))\n        return results\n    else:\n        raise ValueError(\n            \"Unhandled specifications type : {0}\".format(\n                type(specifications).__name__\n            )\n        )", "response": "Returns the list of strings corresponding to the given specifications."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Bind(method):\n    # pylint: disable=C0103\n    \"\"\"\n    The ``@Bind`` callback decorator is called when a component is bound to a\n    dependency.\n\n    The decorated method must accept the injected service object and its\n    :class:`~pelix.framework.ServiceReference` as arguments::\n\n       @Bind\n       def bind_method(self, service, service_reference):\n           '''\n           service: The injected service instance.\n           service_reference: The injected service ServiceReference\n           '''\n           # ...\n\n    If the service is a required one, the bind callback is called **before**\n    the component is validated.\n\n    The service reference can be stored *if it is released on unbind*.\n\n    Exceptions raised by a bind callback are ignored.\n\n    :param method: The decorated method\n    :raise TypeError: The decorated element is not a valid function\n    \"\"\"\n    if not inspect.isroutine(method):\n        raise TypeError(\"@Bind can only be applied on functions\")\n\n    # Tests the number of parameters\n    validate_method_arity(method, \"service\", \"service_reference\")\n\n    _append_object_entry(\n        method, constants.IPOPO_METHOD_CALLBACKS, constants.IPOPO_CALLBACK_BIND\n    )\n    return method", "response": "Decorator for binding a component to a base class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Unbind(method):\n    # pylint: disable=C0103\n    \"\"\"\n    The ``@Unbind`` callback decorator is called when a component dependency is\n    unbound.\n\n    The decorated method must accept the injected service object and its\n    :class:`~pelix.framework.ServiceReference` as arguments::\n\n       @Unbind\n       def unbind_method(self, service, service_reference):\n           '''\n           service: The previously injected service instance.\n           service_reference: Its ServiceReference\n           '''\n           # ...\n\n    If the service is a required one, the unbind callback is called **after**\n    the component has been invalidated.\n\n    Exceptions raised by an unbind callback are ignored.\n\n    :param method: The decorated method\n    :raise TypeError: The decorated element is not a valid function\n    \"\"\"\n    if not isinstance(method, types.FunctionType):\n        raise TypeError(\"@Unbind can only be applied on functions\")\n\n    # Tests the number of parameters\n    validate_method_arity(method, \"service\", \"service_reference\")\n\n    _append_object_entry(\n        method,\n        constants.IPOPO_METHOD_CALLBACKS,\n        constants.IPOPO_CALLBACK_UNBIND,\n    )\n    return method", "response": "Decorator to mark a method as unbound."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PostRegistration(method):\n    # pylint: disable=C0103\n    \"\"\"\n    The service post-registration callback decorator is called after a service\n    of the component has been registered to the framework.\n\n    The decorated method must accept the\n    :class:`~pelix.framework.ServiceReference` of the registered\n    service as argument::\n\n       @PostRegistration\n       def callback_method(self, service_reference):\n           '''\n           service_reference: The ServiceReference of the provided service\n           '''\n           # ...\n\n    :param method: The decorated method\n    :raise TypeError: The decorated element is not a valid function\n    \"\"\"\n    if not isinstance(method, types.FunctionType):\n        raise TypeError(\"@PostRegistration can only be applied on functions\")\n\n    # Tests the number of parameters\n    validate_method_arity(method, \"service_reference\")\n    _append_object_entry(\n        method,\n        constants.IPOPO_METHOD_CALLBACKS,\n        constants.IPOPO_CALLBACK_POST_REGISTRATION,\n    )\n    return method", "response": "Decorator for registering a new service with the framework."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bundlestate_to_str(state):\n        states = {\n            pelix.Bundle.INSTALLED: \"INSTALLED\",\n            pelix.Bundle.ACTIVE: \"ACTIVE\",\n            pelix.Bundle.RESOLVED: \"RESOLVED\",\n            pelix.Bundle.STARTING: \"STARTING\",\n            pelix.Bundle.STOPPING: \"STOPPING\",\n            pelix.Bundle.UNINSTALLED: \"UNINSTALLED\",\n        }\n\n        return states.get(state, \"Unknown state ({0})\".format(state))", "response": "Converts a bundle state integer to a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an ASCII table according to the given headers and lines.", "response": "def make_table(headers, lines, prefix=None):\n        \"\"\"\n        Generates an ASCII table according to the given headers and lines\n\n        :param headers: List of table headers (N-tuple)\n        :param lines: List of table lines (N-tuples)\n        :param prefix: Optional prefix for each line\n        :return: The ASCII representation of the table\n        :raise ValueError: Different number of columns between headers and\n                           lines\n        \"\"\"\n        # Normalize the prefix\n        prefix = str(prefix or \"\")\n\n        # Maximum lengths\n        lengths = [len(title) for title in headers]\n\n        # Store the number of columns (0-based)\n        nb_columns = len(lengths) - 1\n\n        # Lines\n        str_lines = []\n        for idx, line in enumerate(lines):\n            # Recompute lengths\n            str_line = []\n            str_lines.append(str_line)\n            column = -1\n\n            try:\n                for column, entry in enumerate(line):\n                    str_entry = str(entry)\n                    str_line.append(str_entry)\n\n                    if len(str_entry) > lengths[column]:\n                        lengths[column] = len(str_entry)\n\n            except IndexError:\n                # Line too small/big\n                raise ValueError(\n                    \"Different sizes for header and lines \"\n                    \"(line {0})\".format(idx + 1)\n                )\n\n            except (TypeError, AttributeError):\n                # Invalid type of line\n                raise ValueError(\n                    \"Invalid type of line: %s\", type(line).__name__\n                )\n\n            else:\n                if column != nb_columns:\n                    # Check if all lines have the same number of columns\n                    raise ValueError(\n                        \"Different sizes for header and lines \"\n                        \"(line {0})\".format(idx + 1)\n                    )\n\n        # Prepare the head (centered text)\n        format_str = \"{0}|\".format(prefix)\n        for column, length in enumerate(lengths):\n            format_str += \" {%d:^%d} |\" % (column, length)\n\n        head_str = format_str.format(*headers)\n\n        # Prepare the separator, according the length of the headers string\n        separator = \"{0}{1}\".format(prefix, \"-\" * (len(head_str) - len(prefix)))\n        idx = head_str.find(\"|\")\n        while idx != -1:\n            separator = \"+\".join((separator[:idx], separator[idx + 1 :]))\n            idx = head_str.find(\"|\", idx + 1)\n\n        # Prepare the output\n        output = [separator, head_str, separator.replace(\"-\", \"=\")]\n\n        # Compute the lines\n        format_str = format_str.replace(\"^\", \"<\")\n        for line in str_lines:\n            output.append(format_str.format(*line))\n            output.append(separator)\n\n        # Force the last end of line\n        output.append(\"\")\n\n        # Join'em\n        return \"\\n\".join(output)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind_handler(self, svc_ref):\n        if svc_ref in self._bound_references:\n            # Already bound service\n            return False\n\n        # Get the service\n        handler = self._context.get_service(svc_ref)\n\n        # Get its name space\n        namespace = handler.get_namespace()\n        commands = []\n\n        # Register all service methods directly\n        for command, method in handler.get_methods():\n            self.register_command(namespace, command, method)\n            commands.append(command)\n\n        # Store the reference\n        self._bound_references[svc_ref] = handler\n        self._reference_commands[svc_ref] = (namespace, commands)\n        return True", "response": "Called if a command service has been found. Registers the methods of this service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unbind_handler(self, svc_ref):\n        if svc_ref not in self._bound_references:\n            # Unknown reference\n            return False\n\n        # Unregister its commands\n        namespace, commands = self._reference_commands[svc_ref]\n        for command in commands:\n            self.unregister(namespace, command)\n\n        # Release the service\n        self._context.unget_service(svc_ref)\n        del self._bound_references[svc_ref]\n        del self._reference_commands[svc_ref]\n        return True", "response": "Unregisters the commands for a specific service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef var_set(self, session, **kwargs):\n        if not kwargs:\n            session.write_line(\n                self._utils.make_table(\n                    (\"Name\", \"Value\"), session.variables.items()\n                )\n            )\n        else:\n            for name, value in kwargs.items():\n                name = name.strip()\n                session.set(name, value)\n                session.write_line(\"{0}={1}\", name, value)", "response": "set answer = 42"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bundle_details(self, io_handler, bundle_id):\n        bundle = None\n\n        try:\n            # Convert the given ID into an integer\n            bundle_id = int(bundle_id)\n        except ValueError:\n            # Not an integer, suppose it's a bundle name\n            for bundle in self._context.get_bundles():\n                if bundle.get_symbolic_name() == bundle_id:\n                    break\n            else:\n                # Bundle not found\n                bundle = None\n        else:\n            # Integer ID: direct access\n            try:\n                bundle = self._context.get_bundle(bundle_id)\n            except constants.BundleException:\n                pass\n\n        if bundle is None:\n            # No matching bundle\n            io_handler.write_line(\"Unknown bundle ID: {0}\", bundle_id)\n            return False\n\n        lines = [\n            \"ID......: {0}\".format(bundle.get_bundle_id()),\n            \"Name....: {0}\".format(bundle.get_symbolic_name()),\n            \"Version.: {0}\".format(bundle.get_version()),\n            \"State...: {0}\".format(\n                self._utils.bundlestate_to_str(bundle.get_state())\n            ),\n            \"Location: {0}\".format(bundle.get_location()),\n            \"Published services:\",\n        ]\n        try:\n            services = bundle.get_registered_services()\n            if services:\n                for svc_ref in services:\n                    lines.append(\"\\t{0}\".format(svc_ref))\n            else:\n                lines.append(\"\\tn/a\")\n        except constants.BundleException as ex:\n            # Bundle in a invalid state\n            lines.append(\"\\tError: {0}\".format(ex))\n\n        lines.append(\"Services used by this bundle:\")\n        try:\n            services = bundle.get_services_in_use()\n            if services:\n                for svc_ref in services:\n                    lines.append(\"\\t{0}\".format(svc_ref))\n            else:\n                lines.append(\"\\tn/a\")\n        except constants.BundleException as ex:\n            # Bundle in a invalid state\n            lines.append(\"\\tError: {0}\".format(ex))\n\n        lines.append(\"\")\n        io_handler.write(\"\\n\".join(lines))\n        return None", "response": "Prints the details of the bundle with the given ID or name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef service_details(self, io_handler, service_id):\n        svc_ref = self._context.get_service_reference(\n            None, \"({0}={1})\".format(constants.SERVICE_ID, service_id)\n        )\n        if svc_ref is None:\n            io_handler.write_line(\"Service not found: {0}\", service_id)\n            return False\n\n        lines = [\n            \"ID............: {0}\".format(\n                svc_ref.get_property(constants.SERVICE_ID)\n            ),\n            \"Rank..........: {0}\".format(\n                svc_ref.get_property(constants.SERVICE_RANKING)\n            ),\n            \"Specifications: {0}\".format(\n                svc_ref.get_property(constants.OBJECTCLASS)\n            ),\n            \"Bundle........: {0}\".format(svc_ref.get_bundle()),\n            \"Properties....:\",\n        ]\n        for key, value in sorted(svc_ref.get_properties().items()):\n            lines.append(\"\\t{0} = {1}\".format(key, value))\n\n        lines.append(\"Bundles using this service:\")\n        for bundle in svc_ref.get_using_bundles():\n            lines.append(\"\\t{0}\".format(bundle))\n\n        lines.append(\"\")\n        io_handler.write(\"\\n\".join(lines))\n        return None", "response": "Prints the details of the service with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting the services in the framework. Possibility to filter on an exact specification.", "response": "def services_list(self, io_handler, specification=None):\n        \"\"\"\n        Lists the services in the framework. Possibility to filter on an exact\n        specification.\n        \"\"\"\n        # Head of the table\n        headers = (\"ID\", \"Specifications\", \"Bundle\", \"Ranking\")\n\n        # Lines\n        references = (\n            self._context.get_all_service_references(specification, None) or []\n        )\n\n        # Construct the list of services\n        lines = [\n            [\n                str(entry)\n                for entry in (\n                    ref.get_property(constants.SERVICE_ID),\n                    ref.get_property(constants.OBJECTCLASS),\n                    ref.get_bundle(),\n                    ref.get_property(constants.SERVICE_RANKING),\n                )\n            ]\n            for ref in references\n        ]\n\n        if not lines and specification:\n            # No matching service found\n            io_handler.write_line(\"No service provides '{0}'\", specification)\n            return False\n\n        # Print'em all\n        io_handler.write(self._utils.make_table(headers, lines))\n        io_handler.write_line(\"{0} services registered\", len(lines))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the properties of the framework", "response": "def properties_list(self, io_handler):\n        \"\"\"\n        Lists the properties of the framework\n        \"\"\"\n        # Get the framework\n        framework = self._context.get_framework()\n\n        # Head of the table\n        headers = (\"Property Name\", \"Value\")\n\n        # Lines\n        lines = [item for item in framework.get_properties().items()]\n\n        # Sort lines\n        lines.sort()\n\n        # Print the table\n        io_handler.write(self._utils.make_table(headers, lines))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the value of the given property.", "response": "def property_value(self, io_handler, name):\n        \"\"\"\n        Prints the value of the given property, looking into\n        framework properties then environment variables.\n        \"\"\"\n        value = self._context.get_property(name)\n        if value is None:\n            # Avoid printing \"None\"\n            value = \"\"\n\n        io_handler.write_line(str(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef environment_list(self, io_handler):\n        # Head of the table\n        headers = (\"Environment Variable\", \"Value\")\n\n        # Lines\n        lines = [item for item in os.environ.items()]\n\n        # Sort lines\n        lines.sort()\n\n        # Print the table\n        io_handler.write(self._utils.make_table(headers, lines))", "response": "Lists the framework process environment variables"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists the active threads and their current code line.", "response": "def threads_list(io_handler, max_depth=1):\n        \"\"\"\n        Lists the active threads and their current code line\n        \"\"\"\n        # Normalize maximum depth\n        try:\n            max_depth = int(max_depth)\n            if max_depth < 1:\n                max_depth = None\n        except (ValueError, TypeError):\n            max_depth = None\n\n        # pylint: disable=W0212\n        try:\n            # Extract frames\n            frames = sys._current_frames()\n\n            # Get the thread ID -> Thread mapping\n            names = threading._active.copy()\n        except AttributeError:\n            io_handler.write_line(\"sys._current_frames() is not available.\")\n            return\n\n        # Sort by thread ID\n        thread_ids = sorted(frames.keys())\n        lines = []\n        for thread_id in thread_ids:\n            # Get the corresponding stack\n            stack = frames[thread_id]\n\n            # Try to get the thread name\n            try:\n                name = names[thread_id].name\n            except KeyError:\n                name = \"<unknown>\"\n\n            # Construct the code position\n            lines.append(\"Thread ID: {0} - Name: {1}\".format(thread_id, name))\n            lines.append(\"Stack Trace:\")\n\n            trace_lines = []\n            depth = 0\n            frame = stack\n            while frame is not None and (\n                max_depth is None or depth < max_depth\n            ):\n                # Store the line information\n                trace_lines.append(format_frame_info(frame))\n\n                # Previous frame...\n                frame = frame.f_back\n                depth += 1\n\n            # Reverse the lines\n            trace_lines.reverse()\n\n            # Add them to the printed lines\n            lines.extend(trace_lines)\n            lines.append(\"\")\n\n        lines.append(\"\")\n\n        # Sort the lines\n        io_handler.write(\"\\n\".join(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints details about the thread with the given ID.", "response": "def thread_details(io_handler, thread_id, max_depth=0):\n        \"\"\"\n        Prints details about the thread with the given ID (not its name)\n        \"\"\"\n        # Normalize maximum depth\n        try:\n            max_depth = int(max_depth)\n            if max_depth < 1:\n                max_depth = None\n        except (ValueError, TypeError):\n            max_depth = None\n\n        # pylint: disable=W0212\n        try:\n            # Get the stack\n            thread_id = int(thread_id)\n            stack = sys._current_frames()[thread_id]\n        except KeyError:\n            io_handler.write_line(\"Unknown thread ID: {0}\", thread_id)\n        except ValueError:\n            io_handler.write_line(\"Invalid thread ID: {0}\", thread_id)\n        except AttributeError:\n            io_handler.write_line(\"sys._current_frames() is not available.\")\n        else:\n            # Get the name\n            try:\n                name = threading._active[thread_id].name\n            except KeyError:\n                name = \"<unknown>\"\n\n            lines = [\n                \"Thread ID: {0} - Name: {1}\".format(thread_id, name),\n                \"Stack trace:\",\n            ]\n\n            trace_lines = []\n            depth = 0\n            frame = stack\n            while frame is not None and (\n                max_depth is None or depth < max_depth\n            ):\n                # Store the line information\n                trace_lines.append(format_frame_info(frame))\n\n                # Previous frame...\n                frame = frame.f_back\n                depth += 1\n\n            # Reverse the lines\n            trace_lines.reverse()\n\n            # Add them to the printed lines\n            lines.extend(trace_lines)\n\n            lines.append(\"\")\n            io_handler.write(\"\\n\".join(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_level(io_handler, level=None, name=None):\n        # Get the logger\n        logger = logging.getLogger(name)\n\n        # Normalize the name\n        if not name:\n            name = \"Root\"\n\n        if not level:\n            # Level not given: print the logger level\n            io_handler.write_line(\n                \"{0} log level: {1} (real: {2})\",\n                name,\n                logging.getLevelName(logger.getEffectiveLevel()),\n                logging.getLevelName(logger.level),\n            )\n        else:\n            # Set the logger level\n            try:\n                logger.setLevel(level.upper())\n                io_handler.write_line(\"New level for {0}: {1}\", name, level)\n            except ValueError:\n                io_handler.write_line(\"Invalid log level: {0}\", level)", "response": "Prints or changes the log level for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the working directory of the user.", "response": "def change_dir(self, session, path):\n        \"\"\"\n        Changes the working directory\n        \"\"\"\n        if path == \"-\":\n            # Previous directory\n            path = self._previous_path or \".\"\n\n        try:\n            previous = os.getcwd()\n            os.chdir(path)\n        except IOError as ex:\n            # Can't change directory\n            session.write_line(\"Error changing directory: {0}\", ex)\n        else:\n            # Store previous path\n            self._previous_path = previous\n            session.write_line(os.getcwd())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_bundle(self, io_handler, bundle_id):\n        try:\n            bundle_id = int(bundle_id)\n            return self._context.get_bundle(bundle_id)\n        except (TypeError, ValueError):\n            io_handler.write_line(\"Invalid bundle ID: {0}\", bundle_id)\n        except constants.BundleException:\n            io_handler.write_line(\"Unknown bundle: {0}\", bundle_id)", "response": "Reads the bundle with the given ID. Writes errors\n            through the I/O handler."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the bundles with the given IDs.", "response": "def start(self, io_handler, bundle_id, *bundles_ids):\n        \"\"\"\n        Starts the bundles with the given IDs. Stops on first failure.\n        \"\"\"\n        for bid in (bundle_id,) + bundles_ids:\n            try:\n                # Got an int => it's a bundle ID\n                bid = int(bid)\n            except ValueError:\n                # Got something else, we will try to install it first\n                bid = self.install(io_handler, bid)\n\n            bundle = self.__get_bundle(io_handler, bid)\n            if bundle is not None:\n                io_handler.write_line(\n                    \"Starting bundle {0} ({1})...\",\n                    bid,\n                    bundle.get_symbolic_name(),\n                )\n                bundle.start()\n            else:\n                return False\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop(self, io_handler, bundle_id, *bundles_ids):\n        for bid in (bundle_id,) + bundles_ids:\n            bundle = self.__get_bundle(io_handler, bid)\n            if bundle is not None:\n                io_handler.write_line(\n                    \"Stopping bundle {0} ({1})...\",\n                    bid,\n                    bundle.get_symbolic_name(),\n                )\n                bundle.stop()\n            else:\n                return False\n\n        return None", "response": "Stops the bundles with the given IDs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install(self, io_handler, module_name):\n        bundle = self._context.install_bundle(module_name)\n        io_handler.write_line(\"Bundle ID: {0}\", bundle.get_bundle_id())\n        return bundle.get_bundle_id()", "response": "Installs the bundle with the given module name and returns the bundle ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prepare_configs(configs, requires_filters, temporal_timeouts):\n        if not isinstance(requires_filters, dict):\n            requires_filters = {}\n\n        if not isinstance(temporal_timeouts, dict):\n            temporal_timeouts = {}\n\n        if not requires_filters and not temporal_timeouts:\n            # No explicit configuration given\n            return configs\n\n        # We need to change a part of the requirements\n        new_configs = {}\n        for field, config in configs.items():\n            # Extract values from tuple\n            requirement, timeout = config\n            explicit_filter = requires_filters.get(field)\n            explicit_timeout = temporal_timeouts.get(field)\n\n            # Convert the timeout value\n            try:\n                explicit_timeout = int(explicit_timeout)\n                if explicit_timeout <= 0:\n                    explicit_timeout = timeout\n            except (ValueError, TypeError):\n                explicit_timeout = timeout\n\n            if not explicit_filter and not explicit_timeout:\n                # Nothing to do\n                new_configs[field] = config\n            else:\n                try:\n                    # Store an updated copy of the requirement\n                    requirement_copy = requirement.copy()\n                    if explicit_filter:\n                        requirement_copy.set_filter(explicit_filter)\n                    new_configs[field] = (requirement_copy, explicit_timeout)\n                except (TypeError, ValueError):\n                    # No information for this one, or invalid filter:\n                    # keep the factory requirement\n                    new_configs[field] = config\n\n        return new_configs", "response": "Prepare the given configurations for the component."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of handlers associated to the given component.", "response": "def get_handlers(self, component_context, instance):\n        \"\"\"\n        Sets up service providers for the given component\n\n        :param component_context: The ComponentContext bean\n        :param instance: The component instance\n        :return: The list of handlers associated to the given component\n        \"\"\"\n        # Extract information from the context\n        configs = component_context.get_handler(\n            ipopo_constants.HANDLER_TEMPORAL\n        )\n        requires_filters = component_context.properties.get(\n            ipopo_constants.IPOPO_REQUIRES_FILTERS, None\n        )\n        temporal_timeouts = component_context.properties.get(\n            ipopo_constants.IPOPO_TEMPORAL_TIMEOUTS, None\n        )\n\n        # Prepare requirements\n        new_configs = self._prepare_configs(\n            configs, requires_filters, temporal_timeouts\n        )\n\n        # Return handlers\n        return [\n            TemporalDependency(field, requirement, timeout)\n            for field, (requirement, timeout) in new_configs.items()\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear(self):\n        # Cancel timer\n        self.__cancel_timer()\n        self.__timer = None\n        self.__timer_args = None\n\n        self.__still_valid = False\n        self._value = None\n        super(TemporalDependency, self).clear()", "response": "Clears up the manager."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a service is arrived in the framework.", "response": "def on_service_arrival(self, svc_ref):\n        \"\"\"\n        Called when a service has been registered in the framework\n\n        :param svc_ref: A service reference\n        \"\"\"\n        with self._lock:\n            if self.reference is None:\n                # Inject the service\n                service = self._context.get_service(svc_ref)\n                self.reference = svc_ref\n                self._value.set_service(service)\n                self.__still_valid = True\n\n                # Cancel timer\n                self.__cancel_timer()\n\n                # Bind the service\n                self._ipopo_instance.bind(self, self._value, self.reference)\n                return True\n\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_service_departure(self, svc_ref):\n        with self._lock:\n            if svc_ref is self.reference:\n                # Forget about the service\n                self._value.unset_service()\n\n                # Clear the reference\n                self.reference = None\n\n                # Look for a replacement\n                self._pending_ref = self._context.get_service_reference(\n                    self.requirement.specification, self.requirement.filter\n                )\n\n                if self._pending_ref is None:\n                    # No replacement found yet, wait a little\n                    self.__still_valid = True\n                    self.__timer_args = (self._value, svc_ref)\n                    self.__timer = threading.Timer(\n                        self.__timeout, self.__unbind_call, (False,)\n                    )\n                    self.__timer.start()\n\n                else:\n                    # Notify iPOPO immediately\n                    self._ipopo_instance.unbind(self, self._value, svc_ref)\n                return True\n\n            return None", "response": "Called when a service is unregistered from the framework"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __cancel_timer(self):\n        if self.__timer is not None:\n            self.__timer.cancel()\n            self.__unbind_call(True)\n\n        self.__timer_args = None\n        self.__timer = None", "response": "Cancels the timer and calls its target method immediately"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __unbind_call(self, still_valid):\n        with self._lock:\n            if self.__timer is not None:\n                # Timeout expired, we're not valid anymore\n                self.__timer = None\n                self.__still_valid = still_valid\n                self._ipopo_instance.unbind(\n                    self, self.__timer_args[0], self.__timer_args[1]\n                )", "response": "Unbinds the iPOPO method from the iPOPO instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a POST request.", "response": "def do_POST(self, request, response):\n        # pylint: disable=C0103\n        \"\"\"\n        Handle a POST request\n\n        :param request: The HTTP request bean\n        :param response: The HTTP response handler\n        \"\"\"\n        # Get the request JSON content\n        data = jsonrpclib.loads(to_str(request.read_data()))\n\n        # Convert from Jabsorb\n        data = jabsorb.from_jabsorb(data)\n\n        # Dispatch\n        try:\n            result = self._unmarshaled_dispatch(data, self._simple_dispatch)\n        except NoMulticallResult:\n            # No result (never happens, but who knows...)\n            result = None\n\n        if result is not None:\n            # Convert result to Jabsorb\n            if \"result\" in result:\n                result[\"result\"] = jabsorb.to_jabsorb(result[\"result\"])\n\n            # Store JSON\n            result = jsonrpclib.jdumps(result)\n        else:\n            # It was a notification\n            result = \"\"\n\n        # Send the result\n        response.send_content(200, result, \"application/json-rpc\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rest_dispatch(self, request, response):\n        # type: (AbstractHTTPServletRequest, AbstractHTTPServletResponse) -> None\n        \"\"\"\n        Dispatches the request\n\n        :param request: Request bean\n        :param response: Response bean\n        \"\"\"\n        # Extract request information\n        http_verb = request.get_command()\n        sub_path = request.get_sub_path()\n\n        # Find the best matching method, according to the number of\n        # readable arguments\n        max_valid_args = -1\n        best_method = None\n        best_args = None\n        best_match = None\n\n        for route, method in self.__routes.get(http_verb, {}).items():\n            # Parse the request path\n            match = route.match(sub_path)\n            if not match:\n                continue\n\n            # Count the number of valid arguments\n            method_args = self.__methods_args[method]\n            nb_valid_args = 0\n            for name in method_args:\n                try:\n                    match.group(name)\n                    nb_valid_args += 1\n                except IndexError:\n                    # Argument not found\n                    pass\n\n            if nb_valid_args > max_valid_args:\n                # Found a better match\n                max_valid_args = nb_valid_args\n                best_method = method\n                best_args = method_args\n                best_match = match\n\n        if best_method is None:\n            # No match: return a 404 plain text error\n            response.send_content(\n                404,\n                \"No method to handle path {0}\".format(sub_path),\n                \"text/plain\",\n            )\n        else:\n            # Found a method\n            # ... convert arguments\n            kwargs = {}\n            if best_args:\n                for name, converter in best_args.items():\n                    try:\n                        str_value = best_match.group(name)\n                    except IndexError:\n                        # Argument is missing: do nothing\n                        pass\n                    else:\n                        if str_value:\n                            # Keep the default value when an argument is\n                            # missing, i.e. don't give it in kwargs\n                            if converter is not None:\n                                # Convert the argument\n                                kwargs[name] = converter(str_value)\n                            else:\n                                # Use the string value as is\n                                kwargs[name] = str_value\n\n            # Prepare positional arguments\n            extra_pos_args = []\n            if kwargs:\n                # Ignore the first two parameters (request and response)\n                method_args = get_method_arguments(best_method).args[:2]\n                for pos_arg in method_args:\n                    try:\n                        extra_pos_args.append(kwargs.pop(pos_arg))\n                    except KeyError:\n                        pass\n\n            # ... call the method (exceptions will be handled by the server)\n            best_method(request, response, *extra_pos_args, **kwargs)", "response": "Dispatches the request to the appropriate HTTP method and arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_rest_dispatcher(self):\n        for _, method in inspect.getmembers(self, inspect.isroutine):\n            try:\n                config = getattr(method, HTTP_ROUTE_ATTRIBUTE)\n            except AttributeError:\n                # Not a REST method\n                continue\n\n            for route in config[\"routes\"]:\n                pattern, arguments = self.__convert_route(route)\n                self.__methods_args.setdefault(method, {}).update(arguments)\n                for http_verb in config[\"methods\"]:\n                    self.__routes.setdefault(http_verb, {})[pattern] = method", "response": "Setups the dispatcher for REST methods"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a route string into a regex pattern and a dictionary associating arguments names and converters.", "response": "def __convert_route(route):\n        # type: (str) -> Tuple[Pattern[str], Dict[str, Callable[[str], Any]]]\n        \"\"\"\n        Converts a route pattern into a regex.\n        The result is a tuple containing the regex pattern to match and a\n        dictionary associating arguments names and their converter (if any)\n\n        A route can be: \"/hello/<name>/<age:int>\"\n\n        :param route: A route string, i.e. a path with type markers\n        :return: A tuple (pattern, {argument name: converter})\n        \"\"\"\n        arguments = {}  # type: Dict[str, Callable[[str], Any]]\n        last_idx = 0\n        final_pattern = []\n        match_iter = _MARKER_PATTERN.finditer(route)\n        for match_pattern in match_iter:\n            # Copy intermediate string\n            final_pattern.append(route[last_idx : match_pattern.start()])\n            last_idx = match_pattern.end() + 1\n\n            # Extract type declaration\n            match_type = _TYPED_MARKER_PATTERN.match(match_pattern.group())\n            if not match_type:\n                raise ValueError(\n                    \"Invalid argument declaration: {0}\".format(\n                        match_pattern.group()\n                    )\n                )\n\n            name, kind = match_type.groups()\n            if kind:\n                kind = kind.lower()\n\n            # Choose a pattern for each type (can raise a KeyError)\n            regex = TYPE_PATTERNS[kind]\n\n            # Keep track of argument name and converter\n            arguments[name] = TYPE_CONVERTERS.get(kind)\n\n            # Generate the regex pattern for this part\n            final_pattern.append(\"((?P<\")\n            final_pattern.append(match_type.group(1))\n            final_pattern.append(\">\")\n            final_pattern.append(regex)\n            final_pattern.append(\")/?)?\")\n\n        # Copy trailing string\n        final_pattern.append(route[last_idx:])\n\n        # Ensure we don't accept trailing values\n        final_pattern.append(\"$\")\n        return re.compile(\"\".join(final_pattern)), arguments"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntest if the given service event can be handled or ignored.", "response": "def check_event(self, event):\n        # type: (ServiceEvent) -> bool\n        \"\"\"\n        Tests if the given service event must be handled or ignored, based\n        on the state of the iPOPO service and on the content of the event.\n\n        :param event: A service event\n        :return: True if the event can be handled, False if it must be ignored\n        \"\"\"\n        with self._lock:\n            if self.state == StoredInstance.KILLED:\n                # This call may have been blocked by the internal state lock,\n                # ignore it\n                return False\n\n            return self.__safe_handlers_callback(\"check_event\", event)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind(self, dependency, svc, svc_ref):\n        # type: (Any, Any, ServiceReference) -> None\n        \"\"\"\n        Called by a dependency manager to inject a new service and update the\n        component life cycle.\n        \"\"\"\n        with self._lock:\n            self.__set_binding(dependency, svc, svc_ref)\n            self.check_lifecycle()", "response": "Bind a new service into the dependency manager."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by a dependency manager when the properties of an injected dependency have been updated.", "response": "def update(self, dependency, svc, svc_ref, old_properties, new_value=False):\n        # type: (Any, Any, ServiceReference, dict, bool) -> None\n        \"\"\"\n        Called by a dependency manager when the properties of an injected\n        dependency have been updated.\n\n        :param dependency: The dependency handler\n        :param svc: The injected service\n        :param svc_ref: The reference of the injected service\n        :param old_properties: Previous properties of the dependency\n        :param new_value: If True, inject the new value of the handler\n        \"\"\"\n        with self._lock:\n            self.__update_binding(\n                dependency, svc, svc_ref, old_properties, new_value\n            )\n            self.check_lifecycle()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unbind(self, dependency, svc, svc_ref):\n        # type: (Any, Any, ServiceReference) -> None\n        \"\"\"\n        Called by a dependency manager to remove an injected service and to\n        update the component life cycle.\n        \"\"\"\n        with self._lock:\n            # Invalidate first (if needed)\n            self.check_lifecycle()\n\n            # Call unbind() and remove the injection\n            self.__unset_binding(dependency, svc, svc_ref)\n\n            # Try a new configuration\n            if self.update_bindings():\n                self.check_lifecycle()", "response": "Unbinds an injection from the component and updates the component life cycle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_controller_state(self, name, value):\n        # type: (str, bool) -> None\n        \"\"\"\n        Sets the state of the controller with the given name\n\n        :param name: The name of the controller\n        :param value: The new value of the controller\n        \"\"\"\n        with self._lock:\n            self._controllers_state[name] = value\n            self.__safe_handlers_callback(\"on_controller_change\", name, value)", "response": "Sets the state of the controller with the given name and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_property(self, name, old_value, new_value):\n        # type: (str, Any, Any) -> None\n        \"\"\"\n        Handles a property changed event\n\n        :param name: The changed property name\n        :param old_value: The previous property value\n        :param new_value: The new property value\n        \"\"\"\n        with self._lock:\n            self.__safe_handlers_callback(\n                \"on_property_change\", name, old_value, new_value\n            )", "response": "Handles a property changed event."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle an hidden property changed event.", "response": "def update_hidden_property(self, name, old_value, new_value):\n        # type: (str, Any, Any) -> None\n        \"\"\"\n        Handles an hidden property changed event\n\n        :param name: The changed property name\n        :param old_value: The previous property value\n        :param new_value: The new property value\n        \"\"\"\n        with self._lock:\n            self.__safe_handlers_callback(\n                \"on_hidden_property_change\", name, old_value, new_value\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_handlers(self, kind=None):\n        with self._lock:\n            if kind is not None:\n                try:\n                    return self._handlers[kind][:]\n                except KeyError:\n                    return []\n\n            return self.__all_handlers.copy()", "response": "Returns the handlers of the given kind."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_lifecycle(self):\n        with self._lock:\n            # Validation flags\n            was_valid = self.state == StoredInstance.VALID\n            can_validate = self.state not in (\n                StoredInstance.VALIDATING,\n                StoredInstance.VALID,\n            )\n\n            # Test the validity of all handlers\n            handlers_valid = self.__safe_handlers_callback(\n                \"is_valid\", break_on_false=True\n            )\n\n            if was_valid and not handlers_valid:\n                # A dependency is missing\n                self.invalidate(True)\n            elif (\n                can_validate and handlers_valid and self._ipopo_service.running\n            ):\n                # We're all good\n                self.validate(True)", "response": "Tests if the state of the component is valid and if it is not valid and if it is not valid and if it is not valid and if it is not valid validate it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the bindings of the given component .", "response": "def update_bindings(self):\n        # type: () -> bool\n        \"\"\"\n        Updates the bindings of the given component\n\n        :return: True if the component can be validated\n        \"\"\"\n        with self._lock:\n            all_valid = True\n            for handler in self.get_handlers(handlers_const.KIND_DEPENDENCY):\n                # Try to bind\n                self.__safe_handler_callback(handler, \"try_binding\")\n\n                # Update the validity flag\n                all_valid &= self.__safe_handler_callback(\n                    handler, \"is_valid\", only_boolean=True, none_as_true=True\n                )\n            return all_valid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retry_erroneous(self, properties_update):\n        # type: (dict) -> int\n        \"\"\"\n        Removes the ERRONEOUS state from a component and retries a validation\n\n        :param properties_update: A dictionary to update component properties\n        :return: The new state of the component\n        \"\"\"\n        with self._lock:\n            if self.state != StoredInstance.ERRONEOUS:\n                # Not in erroneous state: ignore\n                return self.state\n\n            # Update properties\n            if properties_update:\n                self.context.properties.update(properties_update)\n\n            # Reset state\n            self.state = StoredInstance.INVALID\n            self.error_trace = None\n\n            # Retry\n            self.check_lifecycle()\n\n            # Check if the component is still erroneous\n            return self.state", "response": "Updates the component s ERRONEOUS state and retries a validation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the component invalidation.", "response": "def invalidate(self, callback=True):\n        # type: (bool) -> bool\n        \"\"\"\n        Applies the component invalidation.\n\n        :param callback: If True, call back the component before the\n                         invalidation\n        :return: False if the component wasn't valid\n        \"\"\"\n        with self._lock:\n            if self.state != StoredInstance.VALID:\n                # Instance is not running...\n                return False\n\n            # Change the state\n            self.state = StoredInstance.INVALID\n\n            # Call the handlers\n            self.__safe_handlers_callback(\"pre_invalidate\")\n\n            # Call the component\n            if callback:\n                # pylint: disable=W0212\n                self.__safe_validation_callback(\n                    constants.IPOPO_CALLBACK_INVALIDATE\n                )\n\n                # Trigger an \"Invalidated\" event\n                self._ipopo_service._fire_ipopo_event(\n                    constants.IPopoEvent.INVALIDATED,\n                    self.factory_name,\n                    self.name,\n                )\n\n            # Call the handlers\n            self.__safe_handlers_callback(\"post_invalidate\")\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kill(self):\n        # type: () -> bool\n        \"\"\"\n        This instance is killed : invalidate it if needed, clean up all members\n\n        When this method is called, this StoredInstance object must have\n        been removed from the registry\n\n        :return: True if the component has been killed, False if it already was\n        \"\"\"\n        with self._lock:\n            # Already dead...\n            if self.state == StoredInstance.KILLED:\n                return False\n\n            try:\n                self.invalidate(True)\n            except:\n                self._logger.exception(\n                    \"%s: Error invalidating the instance\", self.name\n                )\n\n            # Now that we are nearly clean, be sure we were in a good registry\n            # state\n            assert not self._ipopo_service.is_registered_instance(self.name)\n\n            # Stop all handlers (can tell to unset a binding)\n            for handler in self.get_handlers():\n                results = self.__safe_handler_callback(handler, \"stop\")\n                if results:\n                    try:\n                        for binding in results:\n                            self.__unset_binding(\n                                handler, binding[0], binding[1]\n                            )\n                    except Exception as ex:\n                        self._logger.exception(\n                            \"Error stopping handler '%s': %s\", handler, ex\n                        )\n\n            # Call the handlers\n            self.__safe_handlers_callback(\"clear\")\n\n            # Change the state\n            self.state = StoredInstance.KILLED\n\n            # Trigger the event\n            # pylint: disable=W0212\n            self._ipopo_service._fire_ipopo_event(\n                constants.IPopoEvent.KILLED, self.factory_name, self.name\n            )\n\n            # Clean up members\n            self._handlers.clear()\n            self.__all_handlers.clear()\n            self._handlers = None\n            self.__all_handlers = None\n            self.context = None\n            self.instance = None\n            self._ipopo_service = None\n            return True", "response": "This instance is killed : invalidate it if needed, clean up all members\n\n        When this method is called, this StoredInstance object must have\n        been removed from the registry\n\n        :return: True if the component has been killed, False if it already was"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate the iPOPO component.", "response": "def validate(self, safe_callback=True):\n        # type: (bool) -> bool\n        \"\"\"\n        Ends the component validation, registering services\n\n        :param safe_callback: If True, calls the component validation callback\n        :return: True if the component has been validated, else False\n        :raise RuntimeError: You try to awake a dead component\n        \"\"\"\n        with self._lock:\n            if self.state in (\n                StoredInstance.VALID,\n                StoredInstance.VALIDATING,\n                StoredInstance.ERRONEOUS,\n            ):\n                # No work to do (yet)\n                return False\n\n            if self.state == StoredInstance.KILLED:\n                raise RuntimeError(\"{0}: Zombies !\".format(self.name))\n\n            # Clear the error trace\n            self.error_trace = None\n\n            # Call the handlers\n            self.__safe_handlers_callback(\"pre_validate\")\n\n            if safe_callback:\n                # Safe call back needed and not yet passed\n                self.state = StoredInstance.VALIDATING\n\n                # Call @ValidateComponent first, then @Validate\n                if not self.__safe_validation_callback(\n                    constants.IPOPO_CALLBACK_VALIDATE\n                ):\n                    # Stop there if the callback failed\n                    self.state = StoredInstance.VALID\n                    self.invalidate(True)\n\n                    # Consider the component has erroneous\n                    self.state = StoredInstance.ERRONEOUS\n                    return False\n\n            # All good\n            self.state = StoredInstance.VALID\n\n            # Call the handlers\n            self.__safe_handlers_callback(\"post_validate\")\n\n            # We may have caused a framework error, so check if iPOPO is active\n            if self._ipopo_service is not None:\n                # pylint: disable=W0212\n                # Trigger the iPOPO event (after the service _registration)\n                self._ipopo_service._fire_ipopo_event(\n                    constants.IPopoEvent.VALIDATED, self.factory_name, self.name\n                )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls the registered method in the component for the given event and returns the result.", "response": "def __callback(self, event, *args, **kwargs):\n        # type: (str, *Any, **Any) -> Any\n        \"\"\"\n        Calls the registered method in the component for the given event\n\n        :param event: An event (IPOPO_CALLBACK_VALIDATE, ...)\n        :return: The callback result, or None\n        :raise Exception: Something went wrong\n        \"\"\"\n        comp_callback = self.context.get_callback(event)\n        if not comp_callback:\n            # No registered callback\n            return True\n\n        # Call it\n        result = comp_callback(self.instance, *args, **kwargs)\n        if result is None:\n            # Special case, if the call back returns nothing\n            return True\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __validation_callback(self, event):\n        # type: (str) -> Any\n        \"\"\"\n        Specific handling for the ``@ValidateComponent`` and\n        ``@InvalidateComponent`` callback, as it requires checking arguments\n        count and order\n\n        :param event: The kind of life-cycle callback (in/validation)\n        :return: The callback result, or None\n        :raise Exception: Something went wrong\n        \"\"\"\n        comp_callback = self.context.get_callback(event)\n        if not comp_callback:\n            # No registered callback\n            return True\n\n        # Get the list of arguments\n        try:\n            args = getattr(comp_callback, constants.IPOPO_VALIDATE_ARGS)\n        except AttributeError:\n            raise TypeError(\n                \"@ValidateComponent callback is missing internal description\"\n            )\n\n        # Associate values to arguments\n        mapping = {\n            constants.ARG_BUNDLE_CONTEXT: self.bundle_context,\n            constants.ARG_COMPONENT_CONTEXT: self.context,\n            constants.ARG_PROPERTIES: self.context.properties.copy(),\n        }\n        mapped_args = [mapping[arg] for arg in args]\n\n        # Call it\n        result = comp_callback(self.instance, *mapped_args)\n        if result is None:\n            # Special case, if the call back returns nothing\n            return True\n\n        return result", "response": "This method handles the validation of the life - cycle event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall the registered method in the component for the given field event and args and kwargs.", "response": "def __field_callback(self, field, event, *args, **kwargs):\n        # type: (str, str, *Any, **Any) -> Any\n        \"\"\"\n        Calls the registered method in the component for the given field event\n\n        :param field: A field name\n        :param event: An event (IPOPO_CALLBACK_VALIDATE, ...)\n        :return: The callback result, or None\n        :raise Exception: Something went wrong\n        \"\"\"\n        # Get the field callback info\n        cb_info = self.context.get_field_callback(field, event)\n        if not cb_info:\n            # No registered callback\n            return True\n\n        # Extract information\n        callback, if_valid = cb_info\n\n        if if_valid and self.state != StoredInstance.VALID:\n            # Don't call the method if the component state isn't satisfying\n            return True\n\n        # Call it\n        result = callback(self.instance, field, *args, **kwargs)\n        if result is None:\n            # Special case, if the call back returns nothing\n            return True\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_callback(self, event, *args, **kwargs):\n        # type: (str, *Any, **Any) -> Any\n        \"\"\"\n        Calls the registered method in the component for the given event,\n        ignoring raised exceptions\n\n        :param event: An event (IPOPO_CALLBACK_VALIDATE, ...)\n        :return: The callback result, or None\n        \"\"\"\n        if self.state == StoredInstance.KILLED:\n            # Invalid state\n            return None\n\n        try:\n            return self.__callback(event, *args, **kwargs)\n        except FrameworkException as ex:\n            # Important error\n            self._logger.exception(\n                \"Critical error calling back %s: %s\", self.name, ex\n            )\n\n            # Kill the component\n            self._ipopo_service.kill(self.name)\n\n            if ex.needs_stop:\n                # Framework must be stopped...\n                self._logger.error(\n                    \"%s said that the Framework must be stopped.\", self.name\n                )\n                self.bundle_context.get_framework().stop()\n            return False\n        except:\n            self._logger.exception(\n                \"Component '%s': error calling callback method for event %s\",\n                self.name,\n                event,\n            )\n            return False", "response": "Calls the registered method in the component for the given event ignoring raised exceptions and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __safe_validation_callback(self, event):\n        # type: (str) -> Any\n        \"\"\"\n        Calls the ``@ValidateComponent`` or ``@InvalidateComponent`` callback,\n        ignoring raised exceptions\n\n        :param event: The kind of life-cycle callback (in/validation)\n        :return: The callback result, or None\n        \"\"\"\n        if self.state == StoredInstance.KILLED:\n            # Invalid state\n            return None\n\n        try:\n            return self.__validation_callback(event)\n        except FrameworkException as ex:\n            # Important error\n            self._logger.exception(\n                \"Critical error calling back %s: %s\", self.name, ex\n            )\n\n            # Kill the component\n            self._ipopo_service.kill(self.name)\n\n            # Store the exception as it is a validation error\n            self.error_trace = traceback.format_exc()\n\n            if ex.needs_stop:\n                # Framework must be stopped...\n                self._logger.error(\n                    \"%s said that the Framework must be stopped.\", self.name\n                )\n                self.bundle_context.get_framework().stop()\n            return False\n        except:\n            self._logger.exception(\n                \"Component '%s': error calling @ValidateComponent callback\",\n                self.name,\n            )\n\n            # Store the exception as it is a validation error\n            self.error_trace = traceback.format_exc()\n\n            return False", "response": "Calls the validation callback ignoring raised exceptions and return False if the callback returns False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the given method with the given arguments in the given handler.", "response": "def __safe_handler_callback(self, handler, method_name, *args, **kwargs):\n        # type: (Any, str, *Any, **Any) -> Any\n        \"\"\"\n        Calls the given method with the given arguments in the given handler.\n        Logs exceptions, but doesn't propagate them.\n\n        Special arguments can be given in kwargs:\n\n        * 'none_as_true': If set to True and the method returned None or\n                          doesn't exist, the result is considered as True.\n                          If set to False, None result is kept as is.\n                          Default is False.\n        * 'only_boolean': If True, the result can only be True or False, else\n                          the result is the value returned by the method.\n                          Default is False.\n\n        :param handler: The handler to call\n        :param method_name: The name of the method to call\n        :param args: List of arguments for the method to call\n        :param kwargs: Dictionary of arguments for the method to call and to\n                       control the call\n        :return: The method result, or None on error\n        \"\"\"\n        if handler is None or method_name is None:\n            return None\n\n        # Behavior flags\n        only_boolean = kwargs.pop(\"only_boolean\", False)\n        none_as_true = kwargs.pop(\"none_as_true\", False)\n\n        # Get the method for each handler\n        try:\n            method = getattr(handler, method_name)\n        except AttributeError:\n            # Method not found\n            result = None\n        else:\n            try:\n                # Call it\n                result = method(*args, **kwargs)\n            except Exception as ex:\n                # No result\n                result = None\n\n                # Log error\n                self._logger.exception(\n                    \"Error calling handler '%s': %s\", handler, ex\n                )\n\n        if result is None and none_as_true:\n            # Consider None (nothing returned) as True\n            result = True\n\n        if only_boolean:\n            # Convert to a boolean result\n            return bool(result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the given method with the given arguments in all handlers and returns True or False.", "response": "def __safe_handlers_callback(self, method_name, *args, **kwargs):\n        # type: (str, *Any, **Any) -> bool\n        \"\"\"\n        Calls the given method with the given arguments in all handlers.\n        Logs exceptions, but doesn't propagate them.\n        Methods called in handlers must return None, True or False.\n\n        Special parameters can be given in kwargs:\n\n        * 'exception_as_error': if it is set to True and an exception is raised\n          by a handler, then this method will return False. By default, this\n          flag is set to False and exceptions are ignored.\n        * 'break_on_false': if it set to True, the loop calling the handler\n          will stop after an handler returned False. By default, this flag\n          is set to False, and all handlers are called.\n\n        :param method_name: Name of the method to call\n        :param args: List of arguments for the method to call\n        :param kwargs: Dictionary of arguments for the method to call and the\n                       behavior of the call\n        :return: True if all handlers returned True (or None), else False\n        \"\"\"\n        if self.state == StoredInstance.KILLED:\n            # Nothing to do\n            return False\n\n        # Behavior flags\n        exception_as_error = kwargs.pop(\"exception_as_error\", False)\n        break_on_false = kwargs.pop(\"break_on_false\", False)\n\n        result = True\n        for handler in self.get_handlers():\n            # Get the method for each handler\n            try:\n                method = getattr(handler, method_name)\n            except AttributeError:\n                # Ignore missing methods\n                pass\n            else:\n                try:\n                    # Call it\n                    res = method(*args, **kwargs)\n                    if res is not None and not res:\n                        # Ignore 'None' results\n                        result = False\n                except Exception as ex:\n                    # Log errors\n                    self._logger.exception(\n                        \"Error calling handler '%s': %s\", handler, ex\n                    )\n\n                    # We can consider exceptions as errors or ignore them\n                    result = result and not exception_as_error\n\n                if not handler and break_on_false:\n                    # The loop can stop here\n                    break\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __set_binding(self, dependency, service, reference):\n        # type: (Any, Any, ServiceReference) -> None\n        \"\"\"\n        Injects a service in the component\n\n        :param dependency: The dependency handler\n        :param service: The injected service\n        :param reference: The reference of the injected service\n        \"\"\"\n        # Set the value\n        setattr(self.instance, dependency.get_field(), dependency.get_value())\n\n        # Call the component back\n        self.safe_callback(constants.IPOPO_CALLBACK_BIND, service, reference)\n\n        self.__safe_field_callback(\n            dependency.get_field(),\n            constants.IPOPO_CALLBACK_BIND_FIELD,\n            service,\n            reference,\n        )", "response": "Injects a service in the component\n        and sets the value of the component\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __update_binding(\n        self, dependency, service, reference, old_properties, new_value\n    ):\n        # type: (Any, Any, ServiceReference, dict, bool) -> None\n        \"\"\"\n        Calls back component binding and field binding methods when the\n        properties of an injected dependency have been updated.\n\n        :param dependency: The dependency handler\n        :param service: The injected service\n        :param reference: The reference of the injected service\n        :param old_properties: Previous properties of the dependency\n        :param new_value: If True, inject the new value of the handler\n        \"\"\"\n        if new_value:\n            # Set the value\n            setattr(\n                self.instance, dependency.get_field(), dependency.get_value()\n            )\n\n        # Call the component back\n        self.__safe_field_callback(\n            dependency.get_field(),\n            constants.IPOPO_CALLBACK_UPDATE_FIELD,\n            service,\n            reference,\n            old_properties,\n        )\n\n        self.safe_callback(\n            constants.IPOPO_CALLBACK_UPDATE, service, reference, old_properties\n        )", "response": "Updates the value of the injected dependency with the new value of the injected dependency."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __unset_binding(self, dependency, service, reference):\n        # type: (Any, Any, ServiceReference) -> None\n        \"\"\"\n        Removes a service from the component\n\n        :param dependency: The dependency handler\n        :param service: The injected service\n        :param reference: The reference of the injected service\n        \"\"\"\n        # Call the component back\n        self.__safe_field_callback(\n            dependency.get_field(),\n            constants.IPOPO_CALLBACK_UNBIND_FIELD,\n            service,\n            reference,\n        )\n\n        self.safe_callback(constants.IPOPO_CALLBACK_UNBIND, service, reference)\n\n        # Update the injected field\n        setattr(self.instance, dependency.get_field(), dependency.get_value())\n\n        # Unget the service\n        self.bundle_context.unget_service(reference)", "response": "Unbinds a service from the component"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_boolean(value):\n    if not value:\n        return False\n\n    try:\n        # Lower string to check known \"false\" value\n        value = value.lower()\n        return value not in (\"none\", \"0\", \"false\", \"no\")\n    except AttributeError:\n        # Not a string, but has a value\n        return True", "response": "Parses a value into a boolean value corresponding to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlooking for the property keys in the filter string", "response": "def _find_keys(self):\n        \"\"\"\n        Looks for the property keys in the filter string\n\n        :return: A list of property keys\n        \"\"\"\n        formatter = string.Formatter()\n        return [\n            val[1] for val in formatter.parse(self._original_filter) if val[1]\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the filter according to the new properties", "response": "def update_filter(self):\n        \"\"\"\n        Update the filter according to the new properties\n\n        :return: True if the filter changed, else False\n        :raise ValueError: The filter is invalid\n        \"\"\"\n        # Consider the filter invalid\n        self.valid_filter = False\n\n        try:\n            # Format the new filter\n            filter_str = self._original_filter.format(\n                **self._component_context.properties\n            )\n        except KeyError as ex:\n            # An entry is missing: abandon\n            logging.warning(\"Missing filter value: %s\", ex)\n            raise ValueError(\"Missing filter value\")\n\n        try:\n            # Parse the new LDAP filter\n            new_filter = ldapfilter.get_ldap_filter(filter_str)\n        except (TypeError, ValueError) as ex:\n            logging.warning(\"Error parsing filter: %s\", ex)\n            raise ValueError(\"Error parsing filter\")\n\n        # The filter is valid\n        self.valid_filter = True\n\n        # Compare to the \"old\" one\n        if new_filter != self.requirement.filter:\n            # Replace the requirement filter\n            self.requirement.filter = new_filter\n            return True\n\n        # Same filter\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_property_change(self, name, old_value, new_value):\n        # pylint: disable=W0613\n        \"\"\"\n        A component property has been updated\n\n        :param name: Name of the property\n        :param old_value: Previous value of the property\n        :param new_value: New value of the property\n        \"\"\"\n        if name in self._keys:\n            try:\n                if self.update_filter():\n                    # This is a key for the filter and the filter has changed\n                    # => Force the handler to update its dependency\n                    self._reset()\n            except ValueError:\n                # Invalid filter: clear all references, this will invalidate\n                # the component\n                for svc_ref in self.get_bindings():\n                    self.on_service_departure(svc_ref)", "response": "Called when a component property has been changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the filter to the original value", "response": "def _reset(self):\n        \"\"\"\n        Called when the filter has been changed\n        \"\"\"\n        with self._lock:\n            # Start listening to services with the new filter\n            self.stop()\n            self.start()\n\n            for svc_ref in self.get_bindings():\n                # Check if the current reference matches the filter\n                if not self.requirement.filter.matches(\n                    svc_ref.get_properties()\n                ):\n                    # Not the case: emulate a service departure\n                    # The instance life cycle will be updated as well\n                    self.on_service_departure(svc_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set(self, data=None):\n        self.__data = data\n        self.__exception = None\n        self.__event.set()", "response": "Sets the event to the given data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raise_exception(self, exception):\n        self.__data = None\n        self.__exception = exception\n        self.__event.set()", "response": "Raise an exception in wait"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for the event or for the timeout.", "response": "def wait(self, timeout=None):\n        \"\"\"\n        Waits for the event or for the timeout\n\n        :param timeout: Wait timeout (in seconds)\n        :return: True if the event as been set, else False\n        \"\"\"\n        # The 'or' part is for Python 2.6\n        result = self.__event.wait(timeout)\n        # pylint: disable=E0702\n        # Pylint seems to miss the \"is None\" check below\n        if self.__exception is None:\n            return result\n        else:\n            raise self.__exception"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnotifies the callback of the execution of the execution of the resource.", "response": "def __notify(self):\n        \"\"\"\n        Notify the given callback about the result of the execution\n        \"\"\"\n        if self.__callback is not None:\n            try:\n                self.__callback(\n                    self._done_event.data,\n                    self._done_event.exception,\n                    self.__extra,\n                )\n            except Exception as ex:\n                self._logger.exception(\"Error calling back method: %s\", ex)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting a callback method to be called when the result has been computed or in case of exception.", "response": "def set_callback(self, method, extra=None):\n        \"\"\"\n        Sets a callback method, called once the result has been computed or in\n        case of exception.\n\n        The callback method must have the following signature:\n        ``callback(result, exception, extra)``.\n\n        :param method: The method to call back in the end of the execution\n        :param extra: Extra parameter to be given to the callback method\n        \"\"\"\n        self.__callback = method\n        self.__extra = extra\n        if self._done_event.is_set():\n            # The execution has already finished\n            self.__notify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the given method and stores its result.", "response": "def execute(self, method, args, kwargs):\n        \"\"\"\n        Execute the given method and stores its result.\n        The result is considered \"done\" even if the method raises an exception\n\n        :param method: The method to execute\n        :param args: Method positional arguments\n        :param kwargs: Method keyword arguments\n        :raise Exception: The exception raised by the method\n        \"\"\"\n        # Normalize arguments\n        if args is None:\n            args = []\n\n        if kwargs is None:\n            kwargs = {}\n\n        try:\n            # Call the method\n            result = method(*args, **kwargs)\n        except Exception as ex:\n            # Something went wrong: propagate to the event and to the caller\n            self._done_event.raise_exception(ex)\n            raise\n        else:\n            # Store the result\n            self._done_event.set(result)\n        finally:\n            # In any case: notify the call back (if any)\n            self.__notify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef result(self, timeout=None):\n        if self._done_event.wait(timeout):\n            return self._done_event.data\n        else:\n            raise OSError(\"Timeout raised\")", "response": "Waits up to timeout for the result of the threaded job. Returns immediately the result if the job has already been done."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start(self):\n        if not self._done_event.is_set():\n            # Stop event not set: we're running\n            return\n\n        # Clear the stop event\n        self._done_event.clear()\n\n        # Compute the number of threads to start to handle pending tasks\n        nb_pending_tasks = self._queue.qsize()\n        if nb_pending_tasks > self._max_threads:\n            nb_threads = self._max_threads\n            nb_pending_tasks = self._max_threads\n        elif nb_pending_tasks < self._min_threads:\n            nb_threads = self._min_threads\n        else:\n            nb_threads = nb_pending_tasks\n\n        # Create the threads\n        for _ in range(nb_pending_tasks):\n            self.__nb_pending_task += 1\n            self.__start_thread()\n        for _ in range(nb_threads - nb_pending_tasks):\n            self.__start_thread()", "response": "Starts the thread pool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __start_thread(self):\n        with self.__lock:\n            if self.__nb_threads >= self._max_threads:\n                # Can't create more threads\n                return False\n\n            if self._done_event.is_set():\n                # We're stopped: do nothing\n                return False\n\n            # Prepare thread and start it\n            name = \"{0}-{1}\".format(self._logger.name, self._thread_id)\n            self._thread_id += 1\n\n            thread = threading.Thread(target=self.__run, name=name)\n            thread.daemon = True\n            try:\n                self.__nb_threads += 1\n                thread.start()\n                self._threads.append(thread)\n                return True\n            except (RuntimeError, OSError):\n                self.__nb_threads -= 1\n                return False", "response": "Starts a new thread and returns True if it is started False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop the thread pool. Does nothing if the thread pool is already stopped.", "response": "def stop(self):\n        \"\"\"\n        Stops the thread pool. Does nothing if the pool is already stopped.\n        \"\"\"\n        if self._done_event.is_set():\n            # Stop event set: we're stopped\n            return\n\n        # Set the stop event\n        self._done_event.set()\n\n        with self.__lock:\n            # Add something in the queue (to unlock the join())\n            try:\n                for _ in self._threads:\n                    self._queue.put(self._done_event, True, self._timeout)\n            except queue.Full:\n                # There is already something in the queue\n                pass\n\n            # Copy the list of threads to wait for\n            threads = self._threads[:]\n\n        # Join threads outside the lock\n        for thread in threads:\n            while thread.is_alive():\n                # Wait 3 seconds\n                thread.join(3)\n                if thread.is_alive():\n                    # Thread is still alive: something might be wrong\n                    self._logger.warning(\n                        \"Thread %s is still alive...\", thread.name\n                    )\n\n        # Clear storage\n        del self._threads[:]\n        self.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueues a task in the pool :param method: Method to call :return: A FutureResult object, to get the result of the task :raise ValueError: Invalid method :raise Full: The task queue is full", "response": "def enqueue(self, method, *args, **kwargs):\n        \"\"\"\n        Queues a task in the pool\n\n        :param method: Method to call\n        :return: A FutureResult object, to get the result of the task\n        :raise ValueError: Invalid method\n        :raise Full: The task queue is full\n        \"\"\"\n        if not hasattr(method, \"__call__\"):\n            raise ValueError(\n                \"{0} has no __call__ member.\".format(method.__name__)\n            )\n\n        # Prepare the future result object\n        future = FutureResult(self._logger)\n\n        # Use a lock, as we might be \"resetting\" the queue\n        with self.__lock:\n            # Add the task to the queue\n            self._queue.put((method, args, kwargs, future), True, self._timeout)\n            self.__nb_pending_task += 1\n\n            if self.__nb_pending_task > self.__nb_threads:\n                # All threads are taken: start a new one\n                self.__start_thread()\n\n        return future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear(self):\n        with self.__lock:\n            # Empty the current queue\n            try:\n                while True:\n                    self._queue.get_nowait()\n                    self._queue.task_done()\n            except queue.Empty:\n                # Queue is now empty\n                pass\n\n            # Wait for the tasks currently executed\n            self.join()", "response": "Empties the current queue content. Returns once the queue content has been emptied."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef join(self, timeout=None):\n        if self._queue.empty():\n            # Nothing to wait for...\n            return True\n        elif timeout is None:\n            # Use the original join\n            self._queue.join()\n            return True\n        else:\n            # Wait for the condition\n            with self._queue.all_tasks_done:\n                self._queue.all_tasks_done.wait(timeout)\n                return not bool(self._queue.unfinished_tasks)", "response": "Waits for all the tasks to be executed and returns True if the queue is empty False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __run(self):\n        already_cleaned = False\n        try:\n            while not self._done_event.is_set():\n                try:\n                    # Wait for an action (blocking)\n                    task = self._queue.get(True, self._timeout)\n                    if task is self._done_event:\n                        # Stop event in the queue: get out\n                        self._queue.task_done()\n                        return\n                except queue.Empty:\n                    # Nothing to do yet\n                    pass\n                else:\n                    with self.__lock:\n                        self.__nb_active_threads += 1\n                    # Extract elements\n                    method, args, kwargs, future = task\n                    try:\n                        # Call the method\n                        future.execute(method, args, kwargs)\n                    except Exception as ex:\n                        self._logger.exception(\n                            \"Error executing %s: %s\", method.__name__, ex\n                        )\n                    finally:\n                        # Mark the action as executed\n                        self._queue.task_done()\n\n                        # Thread is not active anymore\n                        with self.__lock:\n                            self.__nb_pending_task -= 1\n                            self.__nb_active_threads -= 1\n\n                # Clean up thread if necessary\n                with self.__lock:\n                    extra_threads = self.__nb_threads - self.__nb_active_threads\n                    if (\n                        self.__nb_threads > self._min_threads\n                        and extra_threads > self._queue.qsize()\n                    ):\n                        # No more work for this thread\n                        # if there are more non active_thread than task\n                        # and we're above the  minimum number of threads:\n                        # stop this one\n                        self.__nb_threads -= 1\n\n                        # To avoid a race condition: decrease the number of\n                        # threads here and mark it as already accounted for\n                        already_cleaned = True\n                        return\n        finally:\n            # Always clean up\n            with self.__lock:\n                # Thread stops: clean up references\n                try:\n                    self._threads.remove(threading.current_thread())\n                except ValueError:\n                    pass\n\n                if not already_cleaned:\n                    self.__nb_threads -= 1", "response": "The main loop of the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef docid(url, encoding='ascii'):\n\n    if not isinstance(url, bytes):\n        url = url.encode(encoding)\n\n    parser = _URL_PARSER\n    idx = 0\n    for _c in url:\n        if _c not in _HEX:\n            if not (_c == _SYM_MINUS and (idx == _DOMAINID_LENGTH\n                                          or idx == _HOSTID_LENGTH + 1)):\n                return parser.parse(url, idx)\n        idx += 1\n        if idx > 4:\n            break\n    _l = len(url)\n    if _l == _DOCID_LENGTH:\n        parser = _DOCID_PARSER\n    elif _l == _READABLE_DOCID_LENGTH \\\n            and url[_DOMAINID_LENGTH] == _SYM_MINUS \\\n            and url[_HOSTID_LENGTH + 1] == _SYM_MINUS:\n        parser = _R_DOCID_PARSER\n    else:\n        parser = _PARSER\n\n    return parser.parse(url, idx)", "response": "Get the DocID from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_app(app):\n    formatter = LocalVarFormatter()\n\n    error_log_file = app.config.get('LOGFILE', 'error.log')\n    if error_log_file:  # Specify a falsy value in config to disable the log file\n        file_handler = logging.FileHandler(error_log_file)\n        file_handler.setFormatter(formatter)\n        file_handler.setLevel(logging.WARNING)\n        app.logger.addHandler(file_handler)\n\n    if app.config.get('ADMIN_NUMBERS'):\n        if all(key in app.config for key in ['SMS_EXOTEL_SID', 'SMS_EXOTEL_TOKEN', 'SMS_EXOTEL_FROM',\n                'SMS_TWILIO_SID', 'SMS_TWILIO_TOKEN', 'SMS_TWILIO_FROM']):\n\n            # A little trickery because directly creating\n            # an SMSHandler object didn't work\n            logging.handlers.SMSHandler = SMSHandler\n\n            sms_handler = logging.handlers.SMSHandler(\n                app_name=app.name,\n                exotel_sid=app.config['SMS_EXOTEL_SID'],\n                exotel_token=app.config['SMS_EXOTEL_TOKEN'],\n                exotel_from=app.config['SMS_EXOTEL_FROM'],\n                twilio_sid=app.config['SMS_TWILIO_SID'],\n                twilio_token=app.config['SMS_TWILIO_TOKEN'],\n                twilio_from=app.config['SMS_TWILIO_FROM'],\n                phonenumbers=app.config['ADMIN_NUMBERS'])\n            sms_handler.setLevel(logging.ERROR)\n            app.logger.addHandler(sms_handler)\n\n    if app.config.get('SLACK_LOGGING_WEBHOOKS'):\n        logging.handlers.SlackHandler = SlackHandler\n        slack_handler = logging.handlers.SlackHandler(\n            app_name=app.name, webhooks=app.config['SLACK_LOGGING_WEBHOOKS'])\n        slack_handler.setFormatter(formatter)\n        slack_handler.setLevel(logging.NOTSET)\n        app.logger.addHandler(slack_handler)\n\n    if app.config.get('ADMINS'):\n        # MAIL_DEFAULT_SENDER is the new setting for default mail sender in Flask-Mail\n        # DEFAULT_MAIL_SENDER is the old setting. We look for both\n        mail_sender = app.config.get('MAIL_DEFAULT_SENDER') or app.config.get(\n            'DEFAULT_MAIL_SENDER', 'logs@example.com')\n        if isinstance(mail_sender, (list, tuple)):\n            mail_sender = mail_sender[1]  # Get email from (name, email)\n        if app.config.get('MAIL_USERNAME') and app.config.get('MAIL_PASSWORD'):\n            credentials = (app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD'])\n        else:\n            credentials = None\n        mail_handler = logging.handlers.SMTPHandler(app.config.get('MAIL_SERVER', 'localhost'),\n            mail_sender,\n            app.config['ADMINS'],\n            '%s failure' % app.name,\n            credentials=credentials)\n        mail_handler.setFormatter(formatter)\n        mail_handler.setLevel(logging.ERROR)\n        app.logger.addHandler(mail_handler)", "response": "Initializes an application with the given app."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding the default format method to remove cache of the specified record.", "response": "def format(self, record):\n        \"\"\"\n        Format the specified record as text. Overrides\n        :meth:`logging.Formatter.format` to remove cache of\n        :attr:`record.exc_text` unless it was produced by this formatter.\n        \"\"\"\n        if record.exc_info:\n            if record.exc_text:\n                if \"Stack frames (most recent call first)\" not in record.exc_text:\n                    record.exc_text = None\n        return super(LocalVarFormatter, self).format(record)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstrip unwanted markup out of HTML.", "response": "def sanitize_html(value, valid_tags=VALID_TAGS, strip=True):\n    \"\"\"\n    Strips unwanted markup out of HTML.\n    \"\"\"\n    return bleach.clean(value, tags=list(VALID_TAGS.keys()), attributes=VALID_TAGS, strip=strip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef word_count(text, html=True):\n    if html:\n        text = _tag_re.sub(' ', text)\n    text = _strip_re.sub('', text)\n    text = _punctuation_re.sub(' ', text)\n    return len(text.split())", "response": "Returns the number of words in the given text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsimplifying text to allow comparison.", "response": "def simplify_text(text):\n    \"\"\"\n    Simplify text to allow comparison.\n\n    >>> simplify_text(\"Awesome Coder wanted at Awesome Company\")\n    'awesome coder wanted at awesome company'\n    >>> simplify_text(\"Awesome Coder, wanted  at Awesome Company! \")\n    'awesome coder wanted at awesome company'\n    >>> simplify_text(u\"Awesome Coder, wanted  at Awesome Company! \") == 'awesome coder wanted at awesome company'\n    True\n    \"\"\"\n    if isinstance(text, six.text_type):\n        if six.PY3:  # pragma: no cover\n            text = text.translate(text.maketrans(\"\", \"\", string.punctuation)).lower()\n        else:  # pragma: no cover\n            text = six.text_type(text.encode('utf-8').translate(string.maketrans(\"\", \"\"), string.punctuation).lower(), 'utf-8')\n    else:\n        text = text.translate(string.maketrans(\"\", \"\"), string.punctuation).lower()\n    return \" \".join(text.split())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract named entities from a list of text blocks.", "response": "def extract_named_entities(text_blocks):\n    \"\"\"\n    Return a list of named entities extracted from provided text blocks (list of text strings).\n    \"\"\"\n    sentences = []\n    for text in text_blocks:\n        sentences.extend(nltk.sent_tokenize(text))\n\n    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n\n    def extract_entity_names(t):\n        entity_names = []\n\n        if hasattr(t, 'label'):\n            if t.label() == 'NE':\n                entity_names.append(' '.join([child[0] for child in t]))\n            else:\n                for child in t:\n                    entity_names.extend(extract_entity_names(child))\n\n        return entity_names\n\n    entity_names = []\n    for tree in chunked_sentences:\n        entity_names.extend(extract_entity_names(tree))\n\n    return set(entity_names)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_alembic_revision(path=None):\n    config = Config()\n    try:\n        config.set_main_option(\"script_location\", path or \"migrations\")\n        script = ScriptDirectory.from_config(config)\n        head = script.get_current_head()\n        # create alembic table\n        metadata, alembic_version = alembic_table_metadata()\n        metadata.create_all()\n        item = manager.db.session.query(alembic_version).first()\n        if item and item.version_num != head:\n            item.version_num = head\n        else:\n            item = alembic_version.insert().values(version_num=head)\n            item.compile()\n            conn = manager.db.engine.connect()\n            conn.execute(item)\n        manager.db.session.commit()\n        stdout.write(\"alembic head is set to %s \\n\" % head)\n    except CommandError as e:\n        stdout.write(e.message)", "response": "Create alembic table to latest revision number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating database tables from sqlalchemy models", "response": "def createdb():\n    \"\"\"Create database tables from sqlalchemy models\"\"\"\n    manager.db.engine.echo = True\n    manager.db.create_all()\n    set_alembic_revision()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_manager(app, db, **kwargs):\n    manager.app = app\n    manager.db = db\n    manager.context = kwargs\n    manager.add_command('db', MigrateCommand)\n    manager.add_command('clean', Clean())\n    manager.add_command('showurls', ShowUrls())\n    manager.add_command('shell', Shell(make_context=shell_context))\n    manager.add_command('plainshell', Shell(make_context=shell_context,\n        use_ipython=False, use_bpython=False))\n    return manager", "response": "Initializes the base manager for the base object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_app(app, env=None):\n    # Make current_auth available to app templates\n    app.jinja_env.globals['current_auth'] = current_auth\n    # Make the current view available to app templates\n    app.jinja_env.globals['current_view'] = current_view\n    # Disable Flask-SQLAlchemy events.\n    # Apps that want it can turn it back on in their config\n    app.config.setdefault('SQLALCHEMY_TRACK_MODIFICATIONS', False)\n    # Load config from the app's settings.py\n    load_config_from_file(app, 'settings.py')\n    # Load additional settings from the app's environment-specific config file\n    if not env:\n        env = environ.get('FLASK_ENV', 'DEVELOPMENT')  # Uppercase for compatibility with Flask-Environments\n    additional = _additional_config.get(env.lower())  # Lowercase because that's how we define it\n    if additional:\n        load_config_from_file(app, additional)\n\n    logger.init_app(app)", "response": "Initializes a Flask application based on the environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_jinja_environment(self):\n        options = dict(self.jinja_options)\n        if 'autoescape' not in options:\n            options['autoescape'] = self.select_jinja_autoescape\n        rv = SandboxedEnvironment(self, **options)\n        rv.globals.update(\n            url_for=url_for,\n            get_flashed_messages=get_flashed_messages,\n            config=self.config,  # FIXME: Sandboxed templates shouldn't access full config\n            # request, session and g are normally added with the\n            # context processor for efficiency reasons but for imported\n            # templates we also want the proxies in there.\n            request=request,\n            session=session,\n            g=g  # FIXME: Similarly with g: no access for sandboxed templates\n        )\n        rv.filters['tojson'] = _tojson_filter\n        return rv", "response": "Creates the Jinja2 environment based on the passed in options and returns a SandboxedEnvironment object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef for_tsquery(text):\n    tokens = [_token_map.get(t, t) for t in _tsquery_tokens_re.split(\n        _whitespace_re.sub(' ', text.replace(\"'\", \" \").replace('\"', ' ').replace('\\0', '')))]\n    tokens = [t if t in ('&', '|', '!', ':*', '(', ')', ' ') else \"'\" + t.strip() + \"'\"\n        for t in tokens]\n    tokens = [t for t in tokens if t not in ('', ' ', \"''\")]\n    if not tokens:\n        return ''\n    counterlength = len(tokens)\n    counter = 1\n    while counter < counterlength:\n        if tokens[counter] == '!' and tokens[counter - 1] not in ('&', '|', '('):\n            tokens.insert(counter, '&')\n            counter += 1\n            counterlength += 1\n        elif tokens[counter] == '(' and tokens[counter - 1] not in ('&', '|', '!'):\n            tokens.insert(counter, '&')\n            counter += 1\n            counterlength += 1\n        elif tokens[counter] == ')' and tokens[counter - 1] == '(':\n            # Empty ()\n            tokens.pop(counter)\n            tokens.pop(counter - 1)\n            counter -= 2\n            counterlength -= 2\n            # Pop the join with previous segment too\n            if tokens and tokens[counter] in ('&', '|'):\n                tokens.pop(counter)\n                counter -= 1\n                counterlength -= 1\n            elif tokens and counter == 0 and tokens[counter] == '!':\n                tokens.pop(counter)\n                counter -= 1\n                counterlength -= 1\n            elif tokens and counter > 0 and tokens[counter - 1:counter + 1] in (['&', '!'], ['|', '!']):\n                tokens.pop(counter)\n                tokens.pop(counter - 1)\n                counter -= 2\n                counterlength -= 2\n        elif tokens[counter].startswith(\"'\") and tokens[counter - 1] not in ('&', '|', '!', '('):\n            tokens.insert(counter, '&')\n            counter += 1\n            counterlength += 1\n        elif (\n                tokens[counter] in ('&', '|') and tokens[counter - 1] in ('&', '|')) or (\n                tokens[counter] == '!' and tokens[counter - 1] not in ('&', '|')) or (\n                tokens[counter] == ':*' and not tokens[counter - 1].startswith(\"'\")):\n            # Invalid token: is a dupe or follows a token it shouldn't follow\n            tokens.pop(counter)\n            counter -= 1\n            counterlength -= 1\n        counter += 1\n    while tokens and tokens[0] in ('&', '|', ':*', ')', '!', '*'):\n        tokens.pop(0)  # Can't start with a binary or suffix operator\n    if tokens:\n        while tokens and tokens[-1] in ('&', '|', '!', '('):\n            tokens.pop(-1)  # Can't end with a binary or prefix operator\n    if not tokens:\n        return ''  # Did we just eliminate all tokens?\n    missing_brackets = sum([1 if t == '(' else -1 for t in tokens if t in ('(', ')')])\n    if missing_brackets > 0:\n        tokens.append(')' * missing_brackets)\n    elif missing_brackets < 0:\n        tokens.insert(0, '(' * -missing_brackets)\n    return ''.join(tokens)", "response": "r Tokenize text into a valid PostgreSQL to_tsquery query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the database identity for this model.", "response": "def id(cls):\n        \"\"\"\n        Database identity for this model, used for foreign key references from other models\n        \"\"\"\n        if cls.__uuid_primary_key__:\n            return immutable(Column(UUIDType(binary=False), default=uuid_.uuid4, primary_key=True, nullable=False))\n        else:\n            return immutable(Column(Integer, primary_key=True, nullable=False))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef url_id(cls):\n        if cls.__uuid_primary_key__:\n            def url_id_func(self):\n                \"\"\"The URL id, UUID primary key rendered as a hex string\"\"\"\n                return self.id.hex\n\n            def url_id_is(cls):\n                return SqlHexUuidComparator(cls.id)\n\n            url_id_func.__name__ = 'url_id'\n            url_id_property = hybrid_property(url_id_func)\n            url_id_property = url_id_property.comparator(url_id_is)\n            return url_id_property\n        else:\n            def url_id_func(self):\n                \"\"\"The URL id, integer primary key rendered as a string\"\"\"\n                return six.text_type(self.id)\n\n            def url_id_expression(cls):\n                \"\"\"The URL id, integer primary key\"\"\"\n                return cls.id\n\n            url_id_func.__name__ = 'url_id'\n            url_id_property = hybrid_property(url_id_func)\n            url_id_property = url_id_property.expression(url_id_expression)\n            return url_id_property", "response": "The URL id of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uuid(cls):\n        if hasattr(cls, '__uuid_primary_key__') and cls.__uuid_primary_key__:\n            return synonym('id')\n        else:\n            return immutable(Column(UUIDType(binary=False), default=uuid_.uuid4, unique=True, nullable=False))", "response": "Returns the UUID column or synonym to existing : attr : id column if that is a UUID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns public URL to this instance for a given action.", "response": "def url_for(self, action='view', **kwargs):\n        \"\"\"\n        Return public URL to this instance for a given action (default 'view')\n        \"\"\"\n        app = current_app._get_current_object() if current_app else None\n        if app is not None and action in self.url_for_endpoints.get(app, {}):\n            endpoint, paramattrs, _external = self.url_for_endpoints[app][action]\n        else:\n            try:\n                endpoint, paramattrs, _external = self.url_for_endpoints[None][action]\n            except KeyError:\n                raise BuildError(action, kwargs, 'GET')\n        params = {}\n        for param, attr in list(paramattrs.items()):\n            if isinstance(attr, tuple):\n                # attr is a tuple containing:\n                # 1. ('parent', 'name') --> self.parent.name\n                # 2. ('**entity', 'name') --> kwargs['entity'].name\n                if attr[0].startswith('**'):\n                    item = kwargs.pop(attr[0][2:])\n                    attr = attr[1:]\n                else:\n                    item = self\n                for subattr in attr:\n                    item = getattr(item, subattr)\n                params[param] = item\n            elif callable(attr):\n                params[param] = attr(self)\n            else:\n                params[param] = getattr(self, attr)\n        if _external is not None:\n            params['_external'] = _external\n        params.update(kwargs)  # Let kwargs override params\n\n        # url_for from flask\n        return url_for(endpoint, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_url_for(cls, _action, _endpoint=None, _app=None, _external=None, **paramattrs):\n        def decorator(f):\n            if 'url_for_endpoints' not in cls.__dict__:\n                cls.url_for_endpoints = {None: {}}  # Stick it into the class with the first endpoint\n            cls.url_for_endpoints.setdefault(_app, {})\n\n            for keyword in paramattrs:\n                if isinstance(paramattrs[keyword], six.string_types) and '.' in paramattrs[keyword]:\n                    paramattrs[keyword] = tuple(paramattrs[keyword].split('.'))\n            cls.url_for_endpoints[_app][_action] = _endpoint or f.__name__, paramattrs, _external\n            return f\n        return decorator", "response": "Decorator that registers the view as a url_for target."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_view_for(cls, app, action, classview, attr):\n        if 'view_for_endpoints' not in cls.__dict__:\n            cls.view_for_endpoints = {}\n        cls.view_for_endpoints.setdefault(app, {})[action] = (classview, attr)", "response": "Register a classview and viewhandler for a given app and action."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_for(self, action='view'):\n        app = current_app._get_current_object()\n        view, attr = self.view_for_endpoints[app][action]\n        return getattr(view(self), attr)", "response": "Return the classview viewhandler that handles the specified action"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the classview that contains the viewhandler for the specified action", "response": "def classview_for(self, action='view'):\n        \"\"\"\n        Return the classview that contains the viewhandler for the specified action\n        \"\"\"\n        app = current_app._get_current_object()\n        return self.view_for_endpoints[app][action][0](self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_fields(self, fields):\n        for f in fields:\n            if hasattr(self, f):\n                setattr(self, f, fields[f])\n            else:\n                raise TypeError(\"'{arg}' is an invalid argument for {instance_type}\".format(arg=f, instance_type=self.__class__.__name__))", "response": "Helper method for upsert in the various subclasses."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef title(cls):\n        if cls.__title_length__ is None:\n            column_type = UnicodeText()\n        else:\n            column_type = Unicode(cls.__title_length__)\n        return Column(column_type, nullable=False)", "response": "The title of this object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert or update an instance of the class with the given name.", "response": "def upsert(cls, name, **fields):\n        \"\"\"Insert or update an instance\"\"\"\n        instance = cls.get(name)\n        if instance:\n            instance._set_fields(fields)\n        else:\n            instance = cls(name=name, **fields)\n            instance = failsafe_add(cls.query.session, instance, name=name)\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an instance matching the parent and name", "response": "def get(cls, parent, name):\n        \"\"\"Get an instance matching the parent and name\"\"\"\n        return cls.query.filter_by(parent=parent, name=name).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an abbreviated title by subtracting the parent s title from this instance s title.", "response": "def short_title(self):\n        \"\"\"\n        Generates an abbreviated title by subtracting the parent's title from this instance's title.\n        \"\"\"\n        if self.title and self.parent is not None and hasattr(self.parent, 'title') and self.parent.title:\n            if self.title.startswith(self.parent.title):\n                short = self.title[len(self.parent.title):].strip()\n                match = _punctuation_re.match(short)\n                if match:\n                    short = short[match.end():].strip()\n                if short:\n                    return short\n        return self.title"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of permissions for this model plus permissions inherited from the parent.", "response": "def permissions(self, actor, inherited=None):\n        \"\"\"\n        Permissions for this model, plus permissions inherited from the parent.\n        \"\"\"\n        if inherited is not None:\n            return inherited | super(BaseScopedNameMixin, self).permissions(actor)\n        elif self.parent is not None and isinstance(self.parent, PermissionMixin):\n            return self.parent.permissions(actor) | super(BaseScopedNameMixin, self).permissions(actor)\n        else:\n            return super(BaseScopedNameMixin, self).permissions(actor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(cls, parent, url_id):\n        return cls.query.filter_by(parent=parent, url_id=url_id).one_or_none()", "response": "Get an instance matching the parent and url_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_id(self):\n        if self.url_id is None:  # Set id only if empty\n            self.url_id = select([func.coalesce(func.max(self.__class__.url_id + 1), 1)],\n                self.__class__.parent == self.parent)", "response": "Create a new URL id that is unique to the parent container"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_timestamp_columns():\n    return (\n        Column('created_at', DateTime, default=func.utcnow(), nullable=False),\n        Column('updated_at', DateTime, default=func.utcnow(), onupdate=func.utcnow(), nullable=False),\n    )", "response": "Return two columns created_at and updated_at with appropriate defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new instance to the database and commits a new one if the transaction fails.", "response": "def failsafe_add(_session, _instance, **filters):\n    \"\"\"\n    Add and commit a new instance in a nested transaction (using SQL SAVEPOINT),\n    gracefully handling failure in case a conflicting entry is already in the\n    database (which may occur due to parallel requests causing race conditions\n    in a production environment with multiple workers).\n\n    Returns the instance saved to database if no error occurred, or loaded from\n    database using the provided filters if an error occurred. If the filters fail\n    to load from the database, the original IntegrityError is re-raised, as it\n    is assumed to imply that the commit failed because of missing or invalid\n    data, not because of a duplicate entry.\n\n    However, when no filters are provided, nothing is returned and IntegrityError\n    is also suppressed as there is no way to distinguish between data validation\n    failure and an existing conflicting record in the database. Use this option\n    when failures are acceptable but the cost of verification is not.\n\n    Usage: ``failsafe_add(db.session, instance, **filters)`` where filters\n    are the parameters passed to ``Model.query.filter_by(**filters).one()``\n    to load the instance.\n\n    You must commit the transaction as usual after calling ``failsafe_add``.\n\n    :param _session: Database session\n    :param _instance: Instance to commit\n    :param filters: Filters required to load existing instance from the\n        database in case the commit fails (required)\n    :return: Instance that is in the database\n    \"\"\"\n    if _instance in _session:\n        # This instance is already in the session, most likely due to a\n        # save-update cascade. SQLAlchemy will flush before beginning a\n        # nested transaction, which defeats the purpose of nesting, so\n        # remove it for now and add it back inside the SAVEPOINT.\n        _session.expunge(_instance)\n    _session.begin_nested()\n    try:\n        _session.add(_instance)\n        _session.commit()\n        if filters:\n            return _instance\n    except IntegrityError as e:\n        _session.rollback()\n        if filters:\n            try:\n                return _session.query(_instance.__class__).filter_by(**filters).one()\n            except NoResultFound:  # Do not trap the other exception, MultipleResultsFound\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_primary_relationship(parent, childrel, child, parentrel, parentcol):\n\n    parent_table_name = parent.__tablename__\n    child_table_name = child.__tablename__\n    primary_table_name = parent_table_name + '_' + child_table_name + '_primary'\n    parent_id_columns = [c.name for c in inspect(parent).primary_key]\n    child_id_columns = [c.name for c in inspect(child).primary_key]\n\n    primary_table_columns = (\n        [Column(\n            parent_table_name + '_' + name,\n            None,\n            ForeignKey(parent_table_name + '.' + name, ondelete='CASCADE'),\n            primary_key=True,\n            nullable=False) for name in parent_id_columns]\n        + [Column(\n            child_table_name + '_' + name,\n            None,\n            ForeignKey(child_table_name + '.' + name, ondelete='CASCADE'),\n            nullable=False) for name in child_id_columns]\n        + list(make_timestamp_columns())\n    )\n\n    primary_table = Table(primary_table_name, parent.metadata, *primary_table_columns)\n    rel = relationship(child, uselist=False, secondary=primary_table)\n    setattr(parent, childrel, rel)\n\n    @event.listens_for(rel, 'set')\n    def _validate_child(target, value, oldvalue, initiator):\n        if value and getattr(value, parentrel) != target:\n            raise ValueError(\"The target is not affiliated with this parent\")\n\n    # XXX: To support multi-column primary keys, update this SQL function\n    event.listen(primary_table, 'after_create',\n        DDL('''\n            CREATE FUNCTION %(function)s() RETURNS TRIGGER AS $$\n            DECLARE\n                target RECORD;\n            BEGIN\n                IF (NEW.%(rhs)s IS NOT NULL) THEN\n                    SELECT %(parentcol)s INTO target FROM %(child_table_name)s WHERE %(child_id_column)s = NEW.%(rhs)s;\n                    IF (target.%(parentcol)s != NEW.%(lhs)s) THEN\n                        RAISE foreign_key_violation USING MESSAGE = 'The target is not affiliated with this parent';\n                    END IF;\n                END IF;\n                RETURN NEW;\n            END;\n            $$ LANGUAGE plpgsql;\n            CREATE TRIGGER %(trigger)s BEFORE INSERT OR UPDATE\n            ON %(table)s\n            FOR EACH ROW EXECUTE PROCEDURE %(function)s();\n            ''',\n            context={\n                'table': primary_table_name,\n                'function': '%s_validate' % primary_table_name,\n                'trigger': '%s_trigger' % primary_table_name,\n                'parentcol': parentcol,\n                'child_table_name': child_table_name,\n                'child_id_column': child_id_columns[0],\n                'lhs': '%s_%s' % (parent_table_name, parent_id_columns[0]),\n                'rhs': '%s_%s' % (child_table_name, child_id_columns[0]),\n            }\n        ).execute_if(dialect='postgresql')\n    )\n\n    event.listen(primary_table, 'before_drop',\n        DDL('''\n            DROP TRIGGER %(trigger)s ON %(table)s;\n            DROP FUNCTION %(function)s();\n            ''',\n            context={\n                'table': primary_table_name,\n                'trigger': '%s_trigger' % primary_table_name,\n                'function': '%s_validate' % primary_table_name,\n            }\n        ).execute_if(dialect='postgresql')\n    )", "response": "This function adds a primary relationship between two models."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the default value for a column when it s first accessed rather than first committed to the database.", "response": "def auto_init_default(column):\n    \"\"\"\n    Set the default value for a column when it's first accessed rather than\n    first committed to the database.\n    \"\"\"\n    if isinstance(column, ColumnProperty):\n        default = column.columns[0].default\n    else:\n        default = column.default\n\n    @event.listens_for(column, 'init_scalar', retval=True, propagate=True)\n    def init_scalar(target, value, dict_):\n        # A subclass may override the column and not provide a default. Watch out for that.\n        if default:\n            if default.is_callable:\n                value = default.arg(None)\n            elif default.is_scalar:\n                value = default.arg\n            else:\n                raise NotImplementedError(\"Can't invoke pre-default for a SQL-level column default\")\n            dict_[column.key] = value\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef requestargs(*vars, **config):\n    if config and list(config.keys()) != ['source']:\n        raise TypeError(\"Unrecognised parameters: %s\" % repr(config.keys()))\n\n    def inner(f):\n        namefilt = [(name[:-2], filt, True) if name.endswith('[]') else (name, filt, False)\n            for name, filt in\n                [(v[0], v[1]) if isinstance(v, (list, tuple)) else (v, None) for v in vars]]\n\n        if config and config.get('source') == 'form':\n            def datasource():\n                return request.form if request else {}\n        elif config and config.get('source') == 'query':\n            def datasource():\n                return request.args if request else {}\n        else:\n            def datasource():\n                return request.values if request else {}\n\n        @wraps(f)\n        def decorated_function(*args, **kw):\n            values = datasource()\n            for name, filt, is_list in namefilt:\n                # Process name if\n                # (a) it's not in the function's parameters, and\n                # (b) is in the form/query\n                if name not in kw and name in values:\n                    try:\n                        if is_list:\n                            kw[name] = values.getlist(name, type=filt)\n                        else:\n                            kw[name] = values.get(name, type=filt)\n                    except ValueError as e:\n                        raise RequestValueError(e)\n            try:\n                return f(*args, **kw)\n            except TypeError as e:\n                raise RequestTypeError(e)\n        return decorated_function\n    return inner", "response": "This decorator loads parameters from request. values if not specified in the\n    function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_models(*chain, **kwargs):\n    def inner(f):\n        @wraps(f)\n        def decorated_function(*args, **kw):\n            permissions = None\n            permission_required = kwargs.get('permission')\n            url_check_attributes = kwargs.get('urlcheck', [])\n            if isinstance(permission_required, six.string_types):\n                permission_required = set([permission_required])\n            elif permission_required is not None:\n                permission_required = set(permission_required)\n            result = {}\n            for models, attributes, parameter in chain:\n                if not isinstance(models, (list, tuple)):\n                    models = (models,)\n                item = None\n                for model in models:\n                    query = model.query\n                    url_check = False\n                    url_check_paramvalues = {}\n                    for k, v in attributes.items():\n                        if callable(v):\n                            query = query.filter_by(**{k: v(result, kw)})\n                        else:\n                            if '.' in v:\n                                first, attrs = v.split('.', 1)\n                                val = result.get(first)\n                                for attr in attrs.split('.'):\n                                    val = getattr(val, attr)\n                            else:\n                                val = result.get(v, kw.get(v))\n                            query = query.filter_by(**{k: val})\n                        if k in url_check_attributes:\n                            url_check = True\n                            url_check_paramvalues[k] = (v, val)\n                    item = query.first()\n                    if item is not None:\n                        # We found it, so don't look in additional models\n                        break\n                if item is None:\n                    abort(404)\n\n                if hasattr(item, 'redirect_view_args'):\n                    # This item is a redirect object. Redirect to destination\n                    view_args = dict(request.view_args)\n                    view_args.update(item.redirect_view_args())\n                    location = url_for(request.endpoint, **view_args)\n                    if request.query_string:\n                        location = location + u'?' + request.query_string.decode()\n                    return redirect(location, code=307)\n\n                if permission_required:\n                    permissions = item.permissions(current_auth.actor, inherited=permissions)\n                    addlperms = kwargs.get('addlperms') or []\n                    if callable(addlperms):\n                        addlperms = addlperms() or []\n                    permissions.update(addlperms)\n                if g:  # XXX: Deprecated\n                    g.permissions = permissions\n                if request:\n                    add_auth_attribute('permissions', permissions)\n                if url_check and request.method == 'GET':  # Only do urlcheck redirects on GET requests\n                    url_redirect = False\n                    view_args = None\n                    for k, v in url_check_paramvalues.items():\n                        uparam, uvalue = v\n                        if getattr(item, k) != uvalue:\n                            url_redirect = True\n                            if view_args is None:\n                                view_args = dict(request.view_args)\n                            view_args[uparam] = getattr(item, k)\n                    if url_redirect:\n                        location = url_for(request.endpoint, **view_args)\n                        if request.query_string:\n                            location = location + u'?' + request.query_string.decode()\n                        return redirect(location, code=302)\n                if parameter.startswith('g.'):\n                    parameter = parameter[2:]\n                    setattr(g, parameter, item)\n                result[parameter] = item\n            if permission_required and not permission_required & permissions:\n                abort(403)\n            if kwargs.get('kwargs'):\n                return f(*args, kwargs=kw, **result)\n            else:\n                return f(*args, **result)\n        return decorated_function\n    return inner", "response": "Decorator to load a chain of models from the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the parameter into a dictionary before calling jsonify", "response": "def dict_jsonify(param):\n    \"\"\"Convert the parameter into a dictionary before calling jsonify, if it's not already one\"\"\"\n    if not isinstance(param, dict):\n        param = dict(param)\n    return jsonify(param)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict_jsonp(param):\n    if not isinstance(param, dict):\n        param = dict(param)\n    return jsonp(param)", "response": "Convert the parameter into a dictionary before calling jsonp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd CORS headers to the decorated view function. :param origins: Allowed origins (see below) :param methods: A list of allowed HTTP methods :param headers: A list of allowed HTTP headers :param max_age: Duration in seconds for which the CORS response may be cached The :obj:`origins` parameter may be one of: 1. A callable that receives the origin as a parameter. 2. A list of origins. 3. ``*``, indicating that this resource is accessible by any origin. Example use:: from flask import Flask, Response from coaster.views import cors app = Flask(__name__) @app.route('/any') @cors('*') def any_origin(): return Response() @app.route('/static', methods=['GET', 'POST']) @cors(['https://hasgeek.com'], methods=['GET'], headers=['Content-Type', 'X-Requested-With'], max_age=3600) def static_list(): return Response() def check_origin(origin): # check if origin should be allowed return True @app.route('/callable') @cors(check_origin) def callable_function(): return Response()", "response": "def cors(origins,\n        methods=['HEAD', 'OPTIONS', 'GET', 'POST', 'PUT', 'PATCH', 'DELETE'],\n        headers=['Accept', 'Accept-Language', 'Content-Language', 'Content-Type', 'X-Requested-With'],\n        max_age=None):\n    \"\"\"\n    Adds CORS headers to the decorated view function.\n\n    :param origins: Allowed origins (see below)\n    :param methods: A list of allowed HTTP methods\n    :param headers: A list of allowed HTTP headers\n    :param max_age: Duration in seconds for which the CORS response may be cached\n\n    The :obj:`origins` parameter may be one of:\n\n    1. A callable that receives the origin as a parameter.\n    2. A list of origins.\n    3. ``*``, indicating that this resource is accessible by any origin.\n\n    Example use::\n\n        from flask import Flask, Response\n        from coaster.views import cors\n\n        app = Flask(__name__)\n\n        @app.route('/any')\n        @cors('*')\n        def any_origin():\n            return Response()\n\n        @app.route('/static', methods=['GET', 'POST'])\n        @cors(['https://hasgeek.com'], methods=['GET'], headers=['Content-Type', 'X-Requested-With'],\n            max_age=3600)\n        def static_list():\n            return Response()\n\n        def check_origin(origin):\n            # check if origin should be allowed\n            return True\n\n        @app.route('/callable')\n        @cors(check_origin)\n        def callable_function():\n            return Response()\n    \"\"\"\n    def inner(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            origin = request.headers.get('Origin')\n            if request.method not in methods:\n                abort(405)\n\n            if origins == '*':\n                pass\n            elif is_collection(origins) and origin in origins:\n                pass\n            elif callable(origins) and origins(origin):\n                pass\n            else:\n                abort(403)\n\n            if request.method == 'OPTIONS':\n                # pre-flight request\n                resp = Response()\n            else:\n                result = f(*args, **kwargs)\n                resp = make_response(result) if not isinstance(result,\n                    (Response, WerkzeugResponse, current_app.response_class)) else result\n\n            resp.headers['Access-Control-Allow-Origin'] = origin if origin else ''\n            resp.headers['Access-Control-Allow-Methods'] = ', '.join(methods)\n            resp.headers['Access-Control-Allow-Headers'] = ', '.join(headers)\n            if max_age:\n                resp.headers['Access-Control-Max-Age'] = str(max_age)\n            # Add 'Origin' to the Vary header since response will vary by origin\n            if 'Vary' in resp.headers:\n                vary_values = [item.strip() for item in resp.headers['Vary'].split(',')]\n                if 'Origin' not in vary_values:\n                    vary_values.append('Origin')\n                resp.headers['Vary'] = ', '.join(vary_values)\n            else:\n                resp.headers['Vary'] = 'Origin'\n\n            return resp\n        return wrapper\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nviews decorator that requires a certain permission to be present in ``current_auth.permissions`` before the view is allowed to proceed. Aborts with ``403 Forbidden`` if the permission is not present. The decorated view will have an ``is_available`` method that can be called to perform the same test. :param permission: Permission that is required. If a collection type is provided, any one permission must be available", "response": "def requires_permission(permission):\n    \"\"\"\n    View decorator that requires a certain permission to be present in\n    ``current_auth.permissions`` before the view is allowed to proceed.\n    Aborts with ``403 Forbidden`` if the permission is not present.\n\n    The decorated view will have an ``is_available`` method that can be called\n    to perform the same test.\n\n    :param permission: Permission that is required. If a collection type is\n        provided, any one permission must be available\n    \"\"\"\n    def inner(f):\n        def is_available_here():\n            if not hasattr(current_auth, 'permissions'):\n                return False\n            elif is_collection(permission):\n                return bool(current_auth.permissions.intersection(permission))\n            else:\n                return permission in current_auth.permissions\n\n        def is_available(context=None):\n            result = is_available_here()\n            if result and hasattr(f, 'is_available'):\n                # We passed, but we're wrapping another test, so ask there as well\n                return f.is_available(context)\n            return result\n\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            add_auth_attribute('login_required', True)\n            if not is_available_here():\n                abort(403)\n            return f(*args, **kwargs)\n\n        wrapper.requires_permission = permission\n        wrapper.is_available = is_available\n        return wrapper\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the item is a collection class.", "response": "def is_collection(item):\n    \"\"\"\n    Returns True if the item is a collection class: list, tuple, set, frozenset\n    or any other class that resembles one of these (using abstract base classes).\n\n    >>> is_collection(0)\n    False\n    >>> is_collection(0.1)\n    False\n    >>> is_collection('')\n    False\n    >>> is_collection({})\n    False\n    >>> is_collection({}.keys())\n    True\n    >>> is_collection([])\n    True\n    >>> is_collection(())\n    True\n    >>> is_collection(set())\n    True\n    >>> is_collection(frozenset())\n    True\n    >>> from coaster.utils import InspectableSet\n    >>> is_collection(InspectableSet({1, 2}))\n    True\n    \"\"\"\n    return not isinstance(item, six.string_types) and isinstance(item, (collections.Set, collections.Sequence))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef buid():\n    if six.PY3:  # pragma: no cover\n        return urlsafe_b64encode(uuid.uuid4().bytes).decode('utf-8').rstrip('=')\n    else:  # pragma: no cover\n        return six.text_type(urlsafe_b64encode(uuid.uuid4().bytes).rstrip('='))", "response": "Return a random id that is exactly 22 characters long and by encoding a UUID4 in URL - safe Base64."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uuid1mc_from_datetime(dt):\n    fields = list(uuid1mc().fields)\n    if isinstance(dt, datetime):\n        timeval = time.mktime(dt.timetuple()) + dt.microsecond / 1e6\n    else:\n        # Assume we got an actual timestamp\n        timeval = dt\n\n    # The following code is borrowed from the UUID module source:\n    nanoseconds = int(timeval * 1e9)\n    # 0x01b21dd213814000 is the number of 100-ns intervals between the\n    # UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.\n    timestamp = int(nanoseconds // 100) + 0x01b21dd213814000\n    time_low = timestamp & 0xffffffff\n    time_mid = (timestamp >> 32) & 0xffff\n    time_hi_version = (timestamp >> 48) & 0x0fff\n\n    fields[0] = time_low\n    fields[1] = time_mid\n    fields[2] = time_hi_version\n\n    return uuid.UUID(fields=tuple(fields))", "response": "Return a UUID1 with a random multicast MAC id and a timestamp matching the given datetime object or timestamp value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a UUID object to a 22 - char BUID string", "response": "def uuid2buid(value):\n    \"\"\"\n    Convert a UUID object to a 22-char BUID string\n\n    >>> u = uuid.UUID('33203dd2-f2ef-422f-aeb0-058d6f5f7089')\n    >>> uuid2buid(u)\n    'MyA90vLvQi-usAWNb19wiQ'\n    \"\"\"\n    if six.PY3:  # pragma: no cover\n        return urlsafe_b64encode(value.bytes).decode('utf-8').rstrip('=')\n    else:\n        return six.text_type(urlsafe_b64encode(value.bytes).rstrip('='))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef newpin(digits=4):\n    randnum = randint(0, 10 ** digits)\n    while len(str(randnum)) > digits:\n        randnum = randint(0, 10 ** digits)\n    return (u'%%0%dd' % digits) % randnum", "response": "Return a random numeric string with the specified number of digits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_name(text, delim=u'-', maxlength=50, checkused=None, counter=2):\n    name = text.replace('@', delim)\n    name = unidecode(name).replace('@', 'a')  # We don't know why unidecode uses '@' for 'a'-like chars\n    name = six.text_type(delim.join([_strip_re.sub('', x) for x in _punctuation_re.split(name.lower()) if x != '']))\n    if isinstance(text, six.text_type):\n        # Unidecode returns str. Restore to a unicode string if original was unicode\n        name = six.text_type(name)\n    candidate = name[:maxlength]\n    if candidate.endswith(delim):\n        candidate = candidate[:-1]\n    if checkused is None:\n        return candidate\n    existing = checkused(candidate)\n    while existing:\n        candidate = name[:maxlength - len(str(counter))] + str(counter)\n        counter += 1\n        existing = checkused(candidate)\n    return candidate", "response": "u Generate an ASCII name slug."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_password(password, encoding='BCRYPT'):\n    if encoding not in ['PLAIN', 'SSHA', 'BCRYPT']:\n        raise ValueError(\"Unknown encoding %s\" % encoding)\n    if encoding == 'PLAIN':\n        if isinstance(password, str) and six.PY2:\n            password = six.text_type(password, 'utf-8')\n        return '{PLAIN}%s' % password\n    elif encoding == 'SSHA':\n        # SSHA is a modification of the SHA digest scheme with a salt\n        # starting at byte 20 of the base64-encoded string.\n        # Source: http://developer.netscape.com/docs/technote/ldap/pass_sha.html\n        # This implementation is from Zope2's AccessControl.AuthEncoding.\n\n        salt = ''\n        for n in range(7):\n            salt += chr(randrange(256))\n        # b64encode accepts only bytes in Python 3, so salt also has to be encoded\n        salt = salt.encode('utf-8') if six.PY3 else salt\n        if isinstance(password, six.text_type):\n            password = password.encode('utf-8')\n        else:\n            password = str(password)\n        b64_encoded = b64encode(hashlib.sha1(password + salt).digest() + salt)\n        b64_encoded = b64_encoded.decode('utf-8') if six.PY3 else b64_encoded\n        return '{SSHA}%s' % b64_encoded\n    elif encoding == 'BCRYPT':\n        # BCRYPT is the recommended hash for secure passwords\n        password_hashed = bcrypt.hashpw(\n            password.encode('utf-8') if isinstance(password, six.text_type) else password,\n            bcrypt.gensalt())\n        if six.PY3:  # pragma: no cover\n            password_hashed = password_hashed.decode('utf-8')\n        return '{BCRYPT}%s' % password_hashed", "response": "Make a password with PLAIN SSHA or BCRYPT encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares a user s password with a user s attempt.", "response": "def check_password(reference, attempt):\n    \"\"\"\n    Compare a reference password with the user attempt.\n\n    >>> check_password('{PLAIN}foo', 'foo')\n    True\n    >>> check_password(u'{PLAIN}bar', 'bar')\n    True\n    >>> check_password(u'{UNKNOWN}baz', 'baz')\n    False\n    >>> check_password(u'no-encoding', u'no-encoding')\n    False\n    >>> check_password(u'{SSHA}q/uVU8r15k/9QhRi92CWUwMJu2DM6TUSpp25', u're-foo')\n    True\n    >>> check_password(u'{BCRYPT}$2b$12$NfKivgz7njR3/rWZ56EsDe7..PPum.fcmFLbdkbP.chtMTcS1s01C', 'foo')\n    True\n    \"\"\"\n    if reference.startswith(u'{PLAIN}'):\n        if reference[7:] == attempt:\n            return True\n    elif reference.startswith(u'{SSHA}'):\n        # In python3 b64decode takes inputtype as bytes as opposed to str in python 2, and returns\n        # binascii.Error as opposed to TypeError\n        if six.PY3:  # pragma: no cover\n            try:\n                if isinstance(reference, six.text_type):\n                    ref = b64decode(reference[6:].encode('utf-8'))\n                else:\n                    ref = b64decode(reference[6:])\n            except binascii.Error:\n                return False  # Not Base64\n        else:  # pragma: no cover\n            try:\n                ref = b64decode(reference[6:])\n            except TypeError:\n                return False  # Not Base64\n        if isinstance(attempt, six.text_type):\n            attempt = attempt.encode('utf-8')\n        salt = ref[20:]\n        b64_encoded = b64encode(hashlib.sha1(attempt + salt).digest() + salt)\n        if six.PY3:  # pragma: no cover\n            # type(b64_encoded) is bytes and can't be compared with type(reference) which is str\n            compare = six.text_type('{SSHA}%s' % b64_encoded.decode('utf-8') if type(b64_encoded) is bytes else b64_encoded)\n        else:  # pragma: no cover\n            compare = six.text_type('{SSHA}%s' % b64_encoded)\n        return compare == reference\n    elif reference.startswith(u'{BCRYPT}'):\n        # bcrypt.hashpw() accepts either a unicode encoded string or the basic string (python 2)\n        if isinstance(attempt, six.text_type) or isinstance(reference, six.text_type):\n            attempt = attempt.encode('utf-8')\n            reference = reference.encode('utf-8')\n        if six.PY3:  # pragma: no cover\n            return bcrypt.hashpw(attempt, reference[8:]) == reference[8:]\n        else:  # pragma: no cover\n            return bcrypt.hashpw(\n                attempt.encode('utf-8') if isinstance(attempt, six.text_type) else attempt,\n                str(reference[8:])) == reference[8:]\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_currency(value, decimals=2):\n    number, decimal = ((u'%%.%df' % decimals) % value).split(u'.')\n    parts = []\n    while len(number) > 3:\n        part, number = number[-3:], number[:-3]\n        parts.append(part)\n    parts.append(number)\n    parts.reverse()\n    if int(decimal) == 0:\n        return u','.join(parts)\n    else:\n        return u','.join(parts) + u'.' + decimal", "response": "Return a number suitably formatted for display as currency."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn md5sum of data as a 32 - character string.", "response": "def md5sum(data):\n    \"\"\"\n    Return md5sum of data as a 32-character string.\n\n    >>> md5sum('random text')\n    'd9b9bec3f4cc5482e7c5ef43143e563a'\n    >>> md5sum(u'random text')\n    'd9b9bec3f4cc5482e7c5ef43143e563a'\n    >>> len(md5sum('random text'))\n    32\n    \"\"\"\n    if six.PY3:  # pragma: no cover\n        return hashlib.md5(data.encode('utf-8')).hexdigest()\n    else:  # pragma: no cover\n        return hashlib.md5(data).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a datetime matching the starting point of a specified ISO week in the specified timezone.", "response": "def isoweek_datetime(year, week, timezone='UTC', naive=False):\n    \"\"\"\n    Returns a datetime matching the starting point of a specified ISO week\n    in the specified timezone (default UTC). Returns a naive datetime in\n    UTC if requested (default False).\n\n    >>> isoweek_datetime(2017, 1)\n    datetime.datetime(2017, 1, 2, 0, 0, tzinfo=<UTC>)\n    >>> isoweek_datetime(2017, 1, 'Asia/Kolkata')\n    datetime.datetime(2017, 1, 1, 18, 30, tzinfo=<UTC>)\n    >>> isoweek_datetime(2017, 1, 'Asia/Kolkata', naive=True)\n    datetime.datetime(2017, 1, 1, 18, 30)\n    >>> isoweek_datetime(2008, 1, 'Asia/Kolkata')\n    datetime.datetime(2007, 12, 30, 18, 30, tzinfo=<UTC>)\n    \"\"\"\n    naivedt = datetime.combine(isoweek.Week(year, week).day(0), datetime.min.time())\n    if isinstance(timezone, six.string_types):\n        tz = pytz.timezone(timezone)\n    else:\n        tz = timezone\n    dt = tz.localize(naivedt).astimezone(pytz.UTC)\n    if naive:\n        return dt.replace(tzinfo=None)\n    else:\n        return dt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a UTC datetime matching the midnight for the given datetime.", "response": "def midnight_to_utc(dt, timezone=None, naive=False):\n    \"\"\"\n    Returns a UTC datetime matching the midnight for the given date or datetime.\n\n    >>> from datetime import date\n    >>> midnight_to_utc(datetime(2017, 1, 1))\n    datetime.datetime(2017, 1, 1, 0, 0, tzinfo=<UTC>)\n    >>> midnight_to_utc(pytz.timezone('Asia/Kolkata').localize(datetime(2017, 1, 1)))\n    datetime.datetime(2016, 12, 31, 18, 30, tzinfo=<UTC>)\n    >>> midnight_to_utc(datetime(2017, 1, 1), naive=True)\n    datetime.datetime(2017, 1, 1, 0, 0)\n    >>> midnight_to_utc(pytz.timezone('Asia/Kolkata').localize(datetime(2017, 1, 1)), naive=True)\n    datetime.datetime(2016, 12, 31, 18, 30)\n    >>> midnight_to_utc(date(2017, 1, 1))\n    datetime.datetime(2017, 1, 1, 0, 0, tzinfo=<UTC>)\n    >>> midnight_to_utc(date(2017, 1, 1), naive=True)\n    datetime.datetime(2017, 1, 1, 0, 0)\n    >>> midnight_to_utc(date(2017, 1, 1), timezone='Asia/Kolkata')\n    datetime.datetime(2016, 12, 31, 18, 30, tzinfo=<UTC>)\n    >>> midnight_to_utc(datetime(2017, 1, 1), timezone='Asia/Kolkata')\n    datetime.datetime(2016, 12, 31, 18, 30, tzinfo=<UTC>)\n    >>> midnight_to_utc(pytz.timezone('Asia/Kolkata').localize(datetime(2017, 1, 1)), timezone='UTC')\n    datetime.datetime(2017, 1, 1, 0, 0, tzinfo=<UTC>)\n    \"\"\"\n    if timezone:\n        if isinstance(timezone, six.string_types):\n            tz = pytz.timezone(timezone)\n        else:\n            tz = timezone\n    elif isinstance(dt, datetime) and dt.tzinfo:\n        tz = dt.tzinfo\n    else:\n        tz = pytz.UTC\n\n    utc_dt = tz.localize(datetime.combine(dt, datetime.min.time())).astimezone(pytz.UTC)\n    if naive:\n        return utc_dt.replace(tzinfo=None)\n    return utc_dt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a boolean from any of a range of values.", "response": "def getbool(value):\n    \"\"\"\n    Returns a boolean from any of a range of values. Returns None for\n    unrecognized values. Numbers other than 0 and 1 are considered\n    unrecognized.\n\n    >>> getbool(True)\n    True\n    >>> getbool(1)\n    True\n    >>> getbool('1')\n    True\n    >>> getbool('t')\n    True\n    >>> getbool(2)\n    >>> getbool(0)\n    False\n    >>> getbool(False)\n    False\n    >>> getbool('n')\n    False\n    \"\"\"\n    value = str(value).lower()\n    if value in ['1', 't', 'true', 'y', 'yes']:\n        return True\n    elif value in ['0', 'f', 'false', 'n', 'no']:\n        return False\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef require_one_of(_return=False, **kwargs):\n\n    # Two ways to count number of non-None parameters:\n    #\n    # 1. sum([1 if v is not None else 0 for v in kwargs.values()])\n    #\n    #    Using a list comprehension instead of a generator comprehension as the\n    #    parameter to `sum` is faster on both Python 2 and 3.\n    #\n    # 2. len(kwargs) - kwargs.values().count(None)\n    #\n    #    This is 2x faster than the first method under Python 2.7. Unfortunately,\n    #    it doesn't work in Python 3 because `kwargs.values()` is a view that doesn't\n    #    have a `count` method. It needs to be cast into a tuple/list first, but\n    #    remains faster despite the cast's slowdown. Tuples are faster than lists.\n\n    if six.PY3:  # pragma: no cover\n        count = len(kwargs) - tuple(kwargs.values()).count(None)\n    else:  # pragma: no cover\n        count = len(kwargs) - kwargs.values().count(None)\n\n    if count == 0:\n        raise TypeError(\"One of these parameters is required: \" + ', '.join(kwargs.keys()))\n    elif count != 1:\n        raise TypeError(\"Only one of these parameters is allowed: \" + ', '.join(kwargs.keys()))\n\n    if _return:\n        keys, values = zip(*[(k, 1 if v is not None else 0) for k, v in kwargs.items()])\n        k = keys[values.index(1)]\n        return k, kwargs[k]", "response": "Validator that raises a TypeError if one and only one parameter is not specified."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unicode_http_header(value):\n    if six.PY3:  # pragma: no cover\n        # email.header.decode_header expects strings, not bytes. Your input data may be in bytes.\n        # Since these bytes are almost always ASCII, calling `.decode()` on it without specifying\n        # a charset should work fine.\n        if isinstance(value, six.binary_type):\n            value = value.decode()\n    return u''.join([six.text_type(s, e or 'iso-8859-1') if not isinstance(s, six.text_type) else s\n        for s, e in decode_header(value)])", "response": "r Convert an ASCII HTTP header string into a unicode string with the appropriate encoding applied."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of timezones sorted by offset from UTC.", "response": "def sorted_timezones():\n    \"\"\"\n    Return a list of timezones sorted by offset from UTC.\n    \"\"\"\n    def hourmin(delta):\n        if delta.days < 0:\n            hours, remaining = divmod(86400 - delta.seconds, 3600)\n        else:\n            hours, remaining = divmod(delta.seconds, 3600)\n        minutes, remaining = divmod(remaining, 60)\n        return hours, minutes\n\n    now = datetime.utcnow()\n    # Make a list of country code mappings\n    timezone_country = {}\n    for countrycode in pytz.country_timezones:\n        for timezone in pytz.country_timezones[countrycode]:\n            timezone_country[timezone] = countrycode\n\n    # Make a list of timezones, discarding the US/* and Canada/* zones since they aren't reliable for\n    # DST, and discarding UTC and GMT since timezones in that zone have their own names\n    timezones = [(pytz.timezone(tzname).utcoffset(now, is_dst=False), tzname) for tzname in pytz.common_timezones\n        if not tzname.startswith('US/') and not tzname.startswith('Canada/') and tzname not in ('GMT', 'UTC')]\n    # Sort timezones by offset from UTC and their human-readable name\n    presorted = [(delta, '%s%s - %s%s (%s)' % (\n        (delta.days < 0 and '-') or (delta.days == 0 and delta.seconds == 0 and ' ') or '+',\n        '%02d:%02d' % hourmin(delta),\n        (pytz.country_names[timezone_country[name]] + ': ') if name in timezone_country else '',\n        name.replace('_', ' '),\n        pytz.timezone(name).tzname(now, is_dst=False)), name) for delta, name in timezones]\n    presorted.sort()\n    # Return a list of (timezone, label) with the timezone offset included in the label.\n    return [(name, label) for (delta, label, name) in presorted]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef namespace_from_url(url):\n    parsed = urlparse(url)\n    if parsed.hostname is None or parsed.hostname in ['localhost', 'localhost.localdomain'] or (\n            _ipv4_re.search(parsed.hostname)):\n        return None\n\n    namespace = parsed.hostname.split('.')\n    namespace.reverse()\n    if namespace and not namespace[0]:\n        namespace.pop(0)\n    if namespace and namespace[-1] == 'www':\n        namespace.pop(-1)\n    return type(url)('.'.join(namespace))", "response": "Construct a dotted namespace string from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef base_domain_matches(d1, d2):\n    r1 = tldextract.extract(d1)\n    r2 = tldextract.extract(d2)\n    # r1 and r2 contain subdomain, domain and suffix.\n    # We want to confirm that domain and suffix match.\n    return r1.domain == r2.domain and r1.suffix == r2.suffix", "response": "Check if two domains have the same base domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef MarkdownColumn(name, deferred=False, group=None, **kwargs):\n    return composite(MarkdownComposite,\n        Column(name + '_text', UnicodeText, **kwargs),\n        Column(name + '_html', UnicodeText, **kwargs),\n        deferred=deferred, group=group or name\n    )", "response": "Create a composite column that autogenerates HTML from Markdown text and stores data in db columns named with _html and _text prefixes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting plain dictionaries to MutableDict.", "response": "def coerce(cls, key, value):\n        \"\"\"Convert plain dictionaries to MutableDict.\"\"\"\n\n        if not isinstance(value, MutableDict):\n            if isinstance(value, dict):\n                return MutableDict(value)\n            elif isinstance(value, six.string_types):\n                # Assume JSON string\n                if value:\n                    return MutableDict(simplejson.loads(value, use_decimal=True))\n                else:\n                    return MutableDict()  # Empty value is an empty dict\n\n            # this call will raise ValueError\n            return Mutable.coerce(key, value)\n        else:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef require(self, *namespecs):\n        blacklist = set([n[1:] for n in namespecs if n.startswith('!')])\n        not_blacklist = [n for n in namespecs if not n.startswith('!')]\n        return Bundle(*[bundle for name, version, bundle\n            in self._require_recursive(*not_blacklist) if name not in blacklist])", "response": "Return a bundle of the requested assets and their dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the state manager current state label for the current state", "response": "def _state_invalid(self):\n        \"\"\"\n        If the state is invalid for the transition, return details on what didn't match\n\n        :return: Tuple of (state manager, current state, label for current state)\n        \"\"\"\n        for statemanager, conditions in self.statetransition.transitions.items():\n            current_state = getattr(self.obj, statemanager.propname)\n            if conditions['from'] is None:\n                state_valid = True\n            else:\n                mstate = conditions['from'].get(current_state)\n                state_valid = mstate and mstate(self.obj)\n            if state_valid and conditions['if']:\n                state_valid = all(v(self.obj) for v in conditions['if'])\n            if not state_valid:\n                return statemanager, current_state, statemanager.lenum.get(current_state)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a group of managed states.", "response": "def add_state_group(self, name, *states):\n        \"\"\"\n        Add a group of managed states. Groups can be specified directly in the\n        :class:`~coaster.utils.classes.LabeledEnum`. This method is only useful\n        for grouping a conditional state with existing states. It cannot be\n        used to form a group of groups.\n\n        :param str name: Name of this group\n        :param states: :class:`ManagedState` instances to be grouped together\n        \"\"\"\n        # See `_add_state_internal` for explanation of the following\n        if hasattr(self, name):\n            raise AttributeError(\n                \"State group name %s conflicts with existing attribute in the state manager\" % name)\n        mstate = ManagedStateGroup(name, self, states)\n        self.states[name] = mstate\n        setattr(self, name, mstate)\n        setattr(self, 'is_' + name.lower(), mstate)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a conditional state that combines an existing state with a validator that must also pass.", "response": "def add_conditional_state(self, name, state, validator, class_validator=None, cache_for=None, label=None):\n        \"\"\"\n        Add a conditional state that combines an existing state with a validator\n        that must also pass. The validator receives the object on which the property\n        is present as a parameter.\n\n        :param str name: Name of the new state\n        :param ManagedState state: Existing state that this is based on\n        :param validator: Function that will be called with the host object as a parameter\n        :param class_validator: Function that will be called when the state is queried\n            on the class instead of the instance. Falls back to ``validator`` if not specified.\n            Receives the class as the parameter\n        :param cache_for: Integer or function that indicates how long ``validator``'s\n            result can be cached (not applicable to ``class_validator``). ``None`` implies\n            no cache, ``0`` implies indefinite cache (until invalidated by a transition)\n            and any other integer is the number of seconds for which to cache the assertion\n        :param label: Label for this state (string or 2-tuple)\n\n        TODO: `cache_for`'s implementation is currently pending a test case demonstrating\n        how it will be used.\n        \"\"\"\n        # We'll accept a ManagedState with grouped values, but not a ManagedStateGroup\n        if not isinstance(state, ManagedState):\n            raise TypeError(\"Not a managed state: %s\" % repr(state))\n        elif state.statemanager != self:\n            raise ValueError(\"State %s is not associated with this state manager\" % repr(state))\n        if isinstance(label, tuple) and len(label) == 2:\n            label = NameTitle(*label)\n        self._add_state_internal(name, state.value, label=label,\n            validator=validator, class_validator=class_validator, cache_for=cache_for)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transition(self, from_, to, if_=None, **data):\n        def decorator(f):\n            if isinstance(f, StateTransition):\n                f.add_transition(self, from_, to, if_, data)\n                st = f\n            else:\n                st = StateTransition(f, self, from_, to, if_, data)\n            self.transitions.append(st.name)\n            return st\n\n        return decorator", "response": "Decorator to add a transition to one state to another."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecorate a method that may be called if the given state is currently active. Registers a transition internally, but does not change the state. :param from_: Required state to allow this call (can be a state group) :param if_: Validator(s) that, given the object, must all return True for the call to proceed :param data: Additional metadata, stored on the `StateTransition` object as a :attr:`data` attribute", "response": "def requires(self, from_, if_=None, **data):\n        \"\"\"\n        Decorates a method that may be called if the given state is currently active.\n        Registers a transition internally, but does not change the state.\n\n        :param from_: Required state to allow this call (can be a state group)\n        :param if_: Validator(s) that, given the object, must all return True for the call to proceed\n        :param data: Additional metadata, stored on the `StateTransition` object as a :attr:`data` attribute\n        \"\"\"\n        return self.transition(from_, None, if_, **data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a SQL CHECK constraint string given a column name and a LabeledEnum", "response": "def check_constraint(column, lenum, **kwargs):\n        \"\"\"\n        Returns a SQL CHECK constraint string given a column name and a\n        :class:`~coaster.utils.classes.LabeledEnum`.\n\n        Alembic may not detect the CHECK constraint when autogenerating\n        migrations, so you may need to do this manually using the Python\n        console to extract the SQL string::\n\n            from coaster.sqlalchemy import StateManager\n            from your_app.models import YOUR_ENUM\n\n            print str(StateManager.check_constraint('your_column', YOUR_ENUM).sqltext)\n\n        :param str column: Column name\n        :param LabeledEnum lenum: :class:`~coaster.utils.classes.LabeledEnum` to retrieve valid values from\n        :param kwargs: Additional options passed to CheckConstraint\n        \"\"\"\n        return CheckConstraint(\n            str(column_constructor(column).in_(lenum.keys()).compile(compile_kwargs={'literal_binds': True})),\n            **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the best match for this scalar state.", "response": "def bestmatch(self):\n        \"\"\"\n        Best matching current scalar state (direct or conditional), only\n        applicable when accessed via an instance.\n        \"\"\"\n        if self.obj is not None:\n            for mstate in self.statemanager.all_states_by_value[self.value]:\n                msw = mstate(self.obj, self.cls)  # This returns a wrapper\n                if msw:  # If the wrapper evaluates to True, it's our best match\n                    return msw"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef current(self):\n        if self.obj is not None:\n            return {name: mstate(self.obj, self.cls)\n                for name, mstate in self.statemanager.states.items()\n                if mstate(self.obj, self.cls)}", "response": "Return a dictionary of all states and state groups that are currently active."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transitions(self, current=True):\n        if current and isinstance(self.obj, RoleMixin):\n            proxy = self.obj.current_access()\n        else:\n            proxy = {}\n            current = False  # In case the host object is not a RoleMixin\n        return OrderedDict((name, transition) for name, transition in\n            # Retrieve transitions from the host object to activate the descriptor.\n            ((name, getattr(self.obj, name)) for name in self.statemanager.transitions)\n            if transition.is_available and (name in proxy if current else True))", "response": "Returns the available transitions for the current state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transitions_for(self, roles=None, actor=None, anchors=[]):\n        proxy = self.obj.access_for(roles, actor, anchors)\n        return {name: transition for name, transition in self.transitions(current=False).items()\n            if name in proxy}", "response": "Returns a dictionary of names of all states that are available for the specified roles actor and anchors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive an iterable of instances groups them by state using the state manager s state_by_value property as keys.", "response": "def group(self, items, keep_empty=False):\n        \"\"\"\n        Given an iterable of instances, groups them by state using :class:`ManagedState` instances\n        as dictionary keys. Returns an `OrderedDict` that preserves the order of states from\n        the source :class:`~coaster.utils.classes.LabeledEnum`.\n\n        :param bool keep_empty: If ``True``, empty states are included in the result\n        \"\"\"\n        cls = self.cls if self.cls is not None else type(self.obj)  # Class of the item being managed\n        groups = OrderedDict()\n        for mstate in self.statemanager.states_by_value.values():\n            # Ensure we sort groups using the order of states in the source LabeledEnum.\n            # We'll discard the unused states later.\n            groups[mstate] = []\n        # Now process the items by state\n        for item in items:\n            # Use isinstance instead of `type(item) != cls` to account for subclasses\n            if not isinstance(item, cls):\n                raise TypeError(\"Item %s is not an instance of type %s\" % (repr(item), repr(self.cls)))\n            statevalue = self.statemanager._value(item)\n            mstate = self.statemanager.states_by_value[statevalue]\n            groups[mstate].append(item)\n        if not keep_empty:\n            for key, value in list(groups.items()):\n                if not value:\n                    del groups[key]\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gfm(text):\n    def indent_code(matchobj):\n        syntax = matchobj.group(1)\n        code = matchobj.group(2)\n        if syntax:\n            result = '    :::' + syntax + '\\n'\n        else:\n            result = ''\n        # The last line will be blank since it had the closing \"```\". Discard it\n        # when indenting the lines.\n        return result + '\\n'.join(['    ' + line for line in code.split('\\n')[:-1]])\n\n    use_crlf = text.find('\\r') != -1\n    if use_crlf:\n        text = text.replace('\\r\\n', '\\n')\n\n    # Render GitHub-style ```code blocks``` into Markdown-style 4-space indented blocks\n    text = CODEPATTERN_RE.sub(indent_code, text)\n\n    text, code_blocks = remove_pre_blocks(text)\n    text, inline_blocks = remove_inline_code_blocks(text)\n\n    # Prevent foo_bar_baz from ending up with an italic word in the middle.\n    def italic_callback(matchobj):\n        s = matchobj.group(0)\n        # don't mess with URLs:\n        if 'http:' in s or 'https:' in s:\n            return s\n\n        return s.replace('_', r'\\_')\n\n    # fix italics for code blocks\n    text = ITALICSPATTERN_RE.sub(italic_callback, text)\n\n    # linkify naked URLs\n    # wrap the URL in brackets: http://foo -> [http://foo](http://foo)\n    text = NAKEDURL_RE.sub(r'\\1[\\2](\\2)\\3', text)\n\n    # In very clear cases, let newlines become <br /> tags.\n    def newline_callback(matchobj):\n        if len(matchobj.group(1)) == 1:\n            return matchobj.group(0).rstrip() + '  \\n'\n        else:\n            return matchobj.group(0)\n\n    text = NEWLINE_RE.sub(newline_callback, text)\n\n    # now restore removed code blocks\n    removed_blocks = code_blocks + inline_blocks\n    for removed_block in removed_blocks:\n        text = text.replace('{placeholder}', removed_block, 1)\n\n    if use_crlf:\n        text = text.replace('\\n', '\\r\\n')\n\n    return text", "response": "Render a Markdown text into a list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef markdown(text, html=False, valid_tags=GFM_TAGS):\n    if text is None:\n        return None\n    if html:\n        return Markup(sanitize_html(markdown_convert_html(gfm(text)), valid_tags=valid_tags))\n    else:\n        return Markup(markdown_convert_text(gfm(text)))", "response": "Return Markdown rendered text using GitHub Flavoured Markdown and syntax - highlighting enabled."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rulejoin(class_rule, method_rule):\n    if method_rule.startswith('/'):\n        return method_rule\n    else:\n        return class_rule + ('' if class_rule.endswith('/') or not method_rule else '/') + method_rule", "response": "Join class and method rules. Used internally by the class view decorators and the method view decorators."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nviewing method decorator that checks the URL of the loaded object in ``self.obj`` against the URL in the request (using ``self.obj.url_for(__name__)``). If the URLs do not match, and the request is a ``GET``, it issues a redirect to the correct URL. Usage:: @route('/doc/<document>') class MyModelView(UrlForView, InstanceLoader, ModelView): model = MyModel route_model_map = {'document': 'url_id_name'} @route('') @url_change_check @render_with(json=True) def view(self): return self.obj.current_access() If the decorator is required for all view handlers in the class, use :class:`UrlChangeCheck`.", "response": "def url_change_check(f):\n    \"\"\"\n    View method decorator that checks the URL of the loaded object in\n    ``self.obj`` against the URL in the request (using\n    ``self.obj.url_for(__name__)``). If the URLs do not match,\n    and the request is a ``GET``, it issues a redirect to the correct URL.\n    Usage::\n\n        @route('/doc/<document>')\n        class MyModelView(UrlForView, InstanceLoader, ModelView):\n            model = MyModel\n            route_model_map = {'document': 'url_id_name'}\n\n            @route('')\n            @url_change_check\n            @render_with(json=True)\n            def view(self):\n                return self.obj.current_access()\n\n    If the decorator is required for all view handlers in the class, use\n    :class:`UrlChangeCheck`.\n    \"\"\"\n    @wraps(f)\n    def wrapper(self, *args, **kwargs):\n        if request.method == 'GET' and self.obj is not None:\n            correct_url = self.obj.url_for(f.__name__, _external=True)\n            if correct_url != request.base_url:\n                if request.query_string:\n                    correct_url = correct_url + '?' + request.query_string.decode()\n                return redirect(correct_url)  # TODO: Decide if this should be 302 (default) or 301\n        return f(self, *args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters routes for a given app and class.", "response": "def init_app(self, app, cls, callback=None):\n        \"\"\"\n        Register routes for a given app and :class:`ClassView` class. At the\n        time of this call, we will always be in the view class even if we were\n        originally defined in a base class. :meth:`ClassView.init_app`\n        ensures this. :meth:`init_app` therefore takes the liberty of adding\n        additional attributes to ``self``:\n\n        * :attr:`wrapped_func`: The function wrapped with all decorators added by the class\n        * :attr:`view_func`: The view function registered as a Flask view handler\n        * :attr:`endpoints`: The URL endpoints registered to this view handler\n        \"\"\"\n        def view_func(**view_args):\n            # view_func does not make any reference to variables from init_app to avoid creating\n            # a closure. Instead, the code further below sticks all relevant variables into\n            # view_func's namespace.\n\n            # Instantiate the view class. We depend on its __init__ requiring no parameters\n            viewinst = view_func.view_class()\n            # Declare ourselves (the ViewHandler) as the current view. The wrapper makes\n            # equivalence tests possible, such as ``self.current_handler == self.index``\n            viewinst.current_handler = ViewHandlerWrapper(view_func.view, viewinst, view_func.view_class)\n            # Place view arguments in the instance, in case they are needed outside the dispatch process\n            viewinst.view_args = view_args\n            # Place the view instance on the request stack for :obj:`current_view` to discover\n            _request_ctx_stack.top.current_view = viewinst\n            # Call the view instance's dispatch method. View classes can customise this for\n            # desired behaviour.\n            return viewinst.dispatch_request(view_func.wrapped_func, view_args)\n\n        # Decorate the wrapped view function with the class's desired decorators.\n        # Mixin classes may provide their own decorators, and all of them will be applied.\n        # The oldest defined decorators (from mixins) will be applied first, and the\n        # class's own decorators last. Within the list of decorators, we reverse the list\n        # again, so that a list specified like this:\n        #\n        #     __decorators__ = [first, second]\n        #\n        # Has the same effect as writing this:\n        #\n        #     @first\n        #     @second\n        #     def myview(self):\n        #         pass\n        wrapped_func = self.func\n        for base in reversed(cls.__mro__):\n            if '__decorators__' in base.__dict__:\n                for decorator in reversed(base.__dict__['__decorators__']):\n                    wrapped_func = decorator(wrapped_func)\n                    wrapped_func.__name__ = self.name  # See below\n\n        # Make view_func resemble the underlying view handler method...\n        view_func = update_wrapper(view_func, wrapped_func)\n        # ...but give view_func the name of the method in the class (self.name),\n        # self.name will differ from __name__ only if the view handler method\n        # was defined outside the class and then added to the class with a\n        # different name.\n        view_func.__name__ = self.name\n\n        # Stick `wrapped_func` and `cls` into view_func to avoid creating a closure.\n        view_func.wrapped_func = wrapped_func\n        view_func.view_class = cls\n        view_func.view = self\n\n        # Keep a copy of these functions (we already have self.func)\n        self.wrapped_func = wrapped_func\n        self.view_func = view_func\n\n        for class_rule, class_options in cls.__routes__:\n            for method_rule, method_options in self.routes:\n                use_options = dict(method_options)\n                use_options.update(class_options)\n                endpoint = use_options.pop('endpoint', self.endpoint)\n                self.endpoints.add(endpoint)\n                use_rule = rulejoin(class_rule, method_rule)\n                app.add_url_rule(use_rule, endpoint, view_func, **use_options)\n                if callback:\n                    callback(use_rule, endpoint, view_func, **use_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nindicates whether this view is available in the current context", "response": "def is_available(self):\n        \"\"\"Indicates whether this view is available in the current context\"\"\"\n        if hasattr(self._viewh.wrapped_func, 'is_available'):\n            return self._viewh.wrapped_func.is_available(self._obj)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dispatch_request(self, view, view_args):\n        # Call the :meth:`before_request` method\n        resp = self.before_request()\n        if resp:\n            return self.after_request(make_response(resp))\n        # Call the view handler method, then pass the response to :meth:`after_response`\n        return self.after_request(make_response(view(self, **view_args)))", "response": "Dispatches a request to the specified view and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if any view handler in the class is currently available via its is_available method.", "response": "def is_available(self):\n        \"\"\"\n        Returns `True` if *any* view handler in the class is currently\n        available via its `is_available` method.\n        \"\"\"\n        if self.is_always_available:\n            return True\n        for viewname in self.__views__:\n            if getattr(self, viewname).is_available():\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_route_for(cls, _name, rule, **options):\n        setattr(cls, _name, route(rule, **options)(cls.__get_raw_attr(_name)))", "response": "Add a route for an existing method or view. Useful for modifying routes\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister views on an app.", "response": "def init_app(cls, app, callback=None):\n        \"\"\"\n        Register views on an app. If :attr:`callback` is specified, it will\n        be called after ``app.``:meth:`~flask.Flask.add_url_rule`, with the same\n        parameters.\n        \"\"\"\n        processed = set()\n        cls.__views__ = set()\n        cls.is_always_available = False\n        for base in cls.__mro__:\n            for name, attr in base.__dict__.items():\n                if name in processed:\n                    continue\n                processed.add(name)\n                if isinstance(attr, ViewHandler):\n                    if base != cls:  # Copy ViewHandler instances into subclasses\n                        # TODO: Don't do this during init_app. Use a metaclass\n                        # and do this when the class is defined.\n                        attr = attr.copy_for_subclass()\n                        setattr(cls, name, attr)\n                    attr.__set_name__(cls, name)  # Required for Python < 3.6\n                    cls.__views__.add(name)\n                    attr.init_app(app, cls, callback=callback)\n                    if not hasattr(attr.wrapped_func, 'is_available'):\n                        cls.is_always_available = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndispatches a request to the specified view method and returns the response.", "response": "def dispatch_request(self, view, view_args):\n        \"\"\"\n        View dispatcher that calls :meth:`before_request`, :meth:`loader`,\n        :meth:`after_loader`, the view, and then :meth:`after_request`.\n\n        :param view: View method wrapped in specified decorators.\n        :param dict view_args: View arguments, to be passed on to the view method\n        \"\"\"\n        # Call the :meth:`before_request` method\n        resp = self.before_request()\n        if resp:\n            return self.after_request(make_response(resp))\n        # Load the database model\n        self.obj = self.loader(**view_args)\n        # Trigger pre-view processing of the loaded object\n        resp = self.after_loader()\n        if resp:\n            return self.after_request(make_response(resp))\n        # Call the view handler method, then pass the response to :meth:`after_response`\n        return self.after_request(make_response(view(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef declared_attr_roles(rw=None, call=None, read=None, write=None):\n    def inner(f):\n        @wraps(f)\n        def attr(cls):\n            # Pass f(cls) as a parameter to with_roles.inner to avoid the test for\n            # iterables within with_roles. We have no idea about the use cases for\n            # declared_attr in downstream code. There could be a declared_attr\n            # that returns a list that should be accessible via the proxy.\n            return with_roles(rw=rw, call=call, read=read, write=write)(f(cls))\n        return attr\n    warnings.warn(\"declared_attr_roles is deprecated; use with_roles\", stacklevel=2)\n    return inner", "response": "Decorator for the class that has declared_attr_roles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring the class s roles based on the base classes.", "response": "def __configure_roles(mapper, cls):\n    \"\"\"\n    Run through attributes of the class looking for role decorations from\n    :func:`with_roles` and add them to :attr:`cls.__roles__`\n    \"\"\"\n    # Don't mutate ``__roles__`` in the base class.\n    # The subclass must have its own.\n    # Since classes may specify ``__roles__`` directly without\n    # using :func:`with_roles`, we must preserve existing content.\n    if '__roles__' not in cls.__dict__:\n        # If the following line is confusing, it's because reading an\n        # attribute on an object invokes the Method Resolution Order (MRO)\n        # mechanism to find it on base classes, while writing always writes\n        # to the current object.\n        cls.__roles__ = deepcopy(cls.__roles__)\n\n    # An attribute may be defined more than once in base classes. Only handle the first\n    processed = set()\n\n    # Loop through all attributes in this and base classes, looking for role annotations\n    for base in cls.__mro__:\n        for name, attr in base.__dict__.items():\n            if name in processed or name.startswith('__'):\n                continue\n\n            if isinstance(attr, collections.Hashable) and attr in __cache__:\n                data = __cache__[attr]\n                del __cache__[attr]\n            elif isinstance(attr, InstrumentedAttribute) and attr.property in __cache__:\n                data = __cache__[attr.property]\n                del __cache__[attr.property]\n            elif hasattr(attr, '_coaster_roles'):\n                data = attr._coaster_roles\n            else:\n                data = None\n            if data is not None:\n                for role in data.get('call', []):\n                    cls.__roles__.setdefault(role, {}).setdefault('call', set()).add(name)\n                for role in data.get('read', []):\n                    cls.__roles__.setdefault(role, {}).setdefault('read', set()).add(name)\n                for role in data.get('write', []):\n                    cls.__roles__.setdefault(role, {}).setdefault('write', set()).add(name)\n                processed.add(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_roles(self):\n        return InspectableSet(self.roles_for(actor=current_auth.actor, anchors=current_auth.anchors))", "response": "Return a InspectableSet containing currently\n            available roles on this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef access_for(self, roles=None, actor=None, anchors=[]):\n        if roles is None:\n            roles = self.roles_for(actor=actor, anchors=anchors)\n        elif actor is not None or anchors:\n            raise TypeError('If roles are specified, actor/anchors must not be specified')\n        return RoleAccessProxy(self, roles=roles)", "response": "Returns a proxy object that limits read and write access to attributes\n            based on the actor s roles."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps :meth:`access_for` with :obj:`~coaster.auth.current_auth` to return a proxy for the currently authenticated user.", "response": "def current_access(self):\n        \"\"\"\n        Wraps :meth:`access_for` with :obj:`~coaster.auth.current_auth` to\n        return a proxy for the currently authenticated user.\n        \"\"\"\n        return self.access_for(actor=current_auth.actor, anchors=current_auth.anchors)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_current_url():\n    if current_app.config.get('SERVER_NAME') and (\n            # Check current hostname against server name, ignoring port numbers, if any (split on ':')\n            request.environ['HTTP_HOST'].split(':', 1)[0] != current_app.config['SERVER_NAME'].split(':', 1)[0]):\n        return request.url\n\n    url = url_for(request.endpoint, **request.view_args)\n    query = request.query_string\n    if query:\n        return url + '?' + query.decode()\n    else:\n        return url", "response": "Return the current URL including the query string as a relative path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_next_url(referrer=False, external=False, session=False, default=__marker):\n    if session:\n        next_url = request_session.pop('next', None) or request.args.get('next', '')\n    else:\n        next_url = request.args.get('next', '')\n    if next_url and not external:\n        next_url = __clean_external_url(next_url)\n    if next_url:\n        return next_url\n\n    if default is __marker:\n        usedefault = False\n    else:\n        usedefault = True\n\n    if referrer and request.referrer:\n        if external:\n            return request.referrer\n        else:\n            return __clean_external_url(request.referrer) or (default if usedefault else __index_url())\n    else:\n        return default if usedefault else __index_url()", "response": "Get the next URL to redirect to. Don t return external URLs unless explicitly asked for."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON response with a callback wrapper if asked for.", "response": "def jsonp(*args, **kw):\n    \"\"\"\n    Returns a JSON response with a callback wrapper, if asked for.\n    Consider using CORS instead, as JSONP makes the client app insecure.\n    See the :func:`~coaster.views.decorators.cors` decorator.\n    \"\"\"\n    data = json.dumps(dict(*args, **kw), indent=2)\n    callback = request.args.get('callback', request.args.get('jsonp'))\n    if callback and __jsoncallback_re.search(callback) is not None:\n        data = callback + u'(' + data + u');'\n        mimetype = 'application/javascript'\n    else:\n        mimetype = 'application/json'\n    return Response(data, mimetype=mimetype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an absolute URL retrieve the matching endpoint name or view arguments.", "response": "def endpoint_for(url, method=None, return_rule=False, follow_redirects=True):\n    \"\"\"\n    Given an absolute URL, retrieve the matching endpoint name (or rule) and\n    view arguments. Requires a current request context to determine runtime\n    environment.\n\n    :param str method: HTTP method to use (defaults to GET)\n    :param bool return_rule: Return the URL rule instead of the endpoint name\n    :param bool follow_redirects: Follow redirects to final endpoint\n    :return: Tuple of endpoint name or URL rule or `None`, view arguments\n    \"\"\"\n    parsed_url = urlsplit(url)\n    if not parsed_url.netloc:\n        # We require an absolute URL\n        return None, {}\n\n    # Take the current runtime environment...\n    environ = dict(request.environ)\n    # ...but replace the HTTP host with the URL's host...\n    environ['HTTP_HOST'] = parsed_url.netloc\n    # ...and the path with the URL's path (after discounting the app path, if not hosted at root).\n    environ['PATH_INFO'] = parsed_url.path[len(environ.get('SCRIPT_NAME', '')):]\n    # Create a new request with this environment...\n    url_request = current_app.request_class(environ)\n    # ...and a URL adapter with the new request.\n    url_adapter = current_app.create_url_adapter(url_request)\n\n    # Run three hostname tests, one of which must pass:\n\n    # 1. Does the URL map have host matching enabled? If so, the URL adapter will validate the hostname.\n    if current_app.url_map.host_matching:\n        pass\n\n    # 2. If not, does the domain match? url_adapter.server_name will prefer app.config['SERVER_NAME'],\n    # but if that is not specified, it will take it from the environment.\n    elif parsed_url.netloc == url_adapter.server_name:\n        pass\n\n    # 3. If subdomain matching is enabled, does the subdomain match?\n    elif current_app.subdomain_matching and parsed_url.netloc.endswith('.' + url_adapter.server_name):\n        pass\n\n    # If no test passed, we don't have a matching endpoint.\n    else:\n        return None, {}\n\n    # Now retrieve the endpoint or rule, watching for redirects or resolution failures\n    try:\n        return url_adapter.match(parsed_url.path, method, return_rule=return_rule)\n    except RequestRedirect as r:\n        # A redirect typically implies `/folder` -> `/folder/`\n        # This will not be a redirect response from a view, since the view isn't being called\n        if follow_redirects:\n            return endpoint_for(r.new_url, method=method, return_rule=return_rule, follow_redirects=follow_redirects)\n    except (NotFound, MethodNotAllowed):\n        pass\n    # If we got here, no endpoint was found.\n    return None, {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of permissions for this workflow.", "response": "def permissions(self):\n        \"\"\"\n        Permissions for this workflow. Plays nice with\n        :meth:`coaster.views.load_models` and\n        :class:`coaster.sqlalchemy.PermissionMixin` to determine the available\n        permissions to the current user.\n        \"\"\"\n        perms = set(super(DocumentWorkflow, self).permissions())\n        if g:\n            if hasattr(g, 'permissions'):\n                perms.update(g.permissions or [])\n            if hasattr(self.document, 'permissions'):\n                perms = self.document.permissions(current_auth.actor, perms)\n        return perms"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure the annotations of the class.", "response": "def __configure_annotations(mapper, cls):\n    \"\"\"\n    Run through attributes of the class looking for annotations from\n    :func:`annotation_wrapper` and add them to :attr:`cls.__annotations__`\n    and :attr:`cls.__annotations_by_attr__`\n    \"\"\"\n    annotations = {}\n    annotations_by_attr = {}\n\n    # An attribute may be defined more than once in base classes. Only handle the first\n    processed = set()\n\n    # Loop through all attributes in the class and its base classes, looking for annotations\n    for base in cls.__mro__:\n        for name, attr in base.__dict__.items():\n            if name in processed or name.startswith('__'):\n                continue\n\n            # 'data' is a list of string annotations\n            if isinstance(attr, collections.Hashable) and attr in __cache__:\n                data = __cache__[attr]\n                del __cache__[attr]\n            elif isinstance(attr, InstrumentedAttribute) and attr.property in __cache__:\n                data = __cache__[attr.property]\n                del __cache__[attr.property]\n            elif hasattr(attr, '_coaster_annotations'):\n                data = attr._coaster_annotations\n            else:\n                data = None\n            if data is not None:\n                annotations_by_attr.setdefault(name, []).extend(data)\n                for a in data:\n                    annotations.setdefault(a, []).append(name)\n                processed.add(name)\n\n    # Classes specifying ``__annotations__`` directly isn't supported,\n    # so we don't bother preserving existing content, if any.\n    if annotations:\n        cls.__annotations__ = annotations\n    if annotations_by_attr:\n        cls.__annotations_by_attr__ = annotations_by_attr\n    annotations_configured.send(cls)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef annotation_wrapper(annotation, doc=None):\n    def decorator(attr):\n        __cache__.setdefault(attr, []).append(annotation)\n        # Also mark the annotation on the object itself. This will\n        # fail if the object has a restrictive __slots__, but it's\n        # required for some objects like Column because SQLAlchemy copies\n        # them in subclasses, changing their hash and making them\n        # undiscoverable via the cache.\n        try:\n            if not hasattr(attr, '_coaster_annotations'):\n                setattr(attr, '_coaster_annotations', [])\n            attr._coaster_annotations.append(annotation)\n        except AttributeError:\n            pass\n        return attr\n\n    decorator.__name__ = decorator.name = annotation\n    decorator.__doc__ = doc\n    return decorator", "response": "A decorator that adds an annotation to an attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_auth_attribute(attr, value, actor=False):\n    if attr in ('actor', 'anchors', 'is_anonymous', 'not_anonymous', 'is_authenticated', 'not_authenticated'):\n        raise AttributeError(\"Attribute name %s is reserved by current_auth\" % attr)\n\n    # Invoking current_auth will also create it on the local stack. We can\n    # then proceed to set attributes on it.\n    ca = current_auth._get_current_object()\n    # Since :class:`CurrentAuth` overrides ``__setattr__``, we need to use :class:`object`'s.\n    object.__setattr__(ca, attr, value)\n\n    if attr == 'user':\n        # Special-case 'user' for compatibility with Flask-Login\n        _request_ctx_stack.top.user = value\n        # A user is always an actor\n        actor = True\n\n    if actor:\n        object.__setattr__(ca, 'actor', value)", "response": "Helper function for login managers. Adds authorization attributes to the current_auth object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_attributes(cls, attributes, namespace):\n        config_path = attributes.get('config_path')\n        tokens = {}\n\n        def build_config_key(value_def, config_key):\n            key = value_def.config_key or config_key\n            return '%s.%s' % (config_path, key) if config_path else key\n\n        def build_token(name, value_def):\n            config_key = build_config_key(value_def, name)\n            value_token = ValueToken.from_definition(\n                                            value_def, namespace, config_key)\n            getters.register_value_proxy(namespace, value_token, value_def.help)\n            tokens[name] = value_token\n            return name, build_property(value_token)\n\n        def build_attr(name, attribute):\n            if not isinstance(attribute, ValueTypeDefinition):\n                return name, attribute\n            return build_token(name, attribute)\n\n        attributes = dict(build_attr(*item)\n                          for item in six.iteritems(attributes))\n        attributes['_tokens'] = tokens\n        return attributes", "response": "Build a dictionary of attributes with ValueTokens replaced by a\n            property which returns the config value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cache_as_field(cache_name):\n    def cache_wrapper(func):\n        @functools.wraps(func)\n        def inner_wrapper(self, *args, **kwargs):\n            value = getattr(self, cache_name, UndefToken)\n            if value != UndefToken:\n                return value\n\n            ret = func(self, *args, **kwargs)\n            setattr(self, cache_name, ret)\n            return ret\n        return inner_wrapper\n    return cache_wrapper", "response": "Cache a functions return value as the field cache_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_value(proxy):\n    value = proxy.namespace.get(proxy.config_key, proxy.default)\n    if value is UndefToken:\n        raise errors.ConfigurationError(\"%s is missing value for: %s\" %\n            (proxy.namespace, proxy.config_key))\n\n    try:\n        return proxy.validator(value)\n    except errors.ValidationError as e:\n        raise errors.ConfigurationError(\"%s failed to validate %s: %s\" %\n            (proxy.namespace, proxy.config_key, e))", "response": "Given a value proxy type Retrieve a value from a namespace raising an exception if no value is found raising an exception if the value does not validate."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_reader(validator, reader_namespace=config.DEFAULT):\n    def reader(config_key, default=UndefToken, namespace=None):\n        config_namespace = config.get_namespace(namespace or reader_namespace)\n        return validator(_read_config(config_key, config_namespace, default))\n    return reader", "response": "A factory method for creating a custom config reader from a validation\n    function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_namespaces_from_names(name, all_names):\n    names = configuration_namespaces.keys() if all_names else [name]\n    for name in names:\n        yield get_namespace(name)", "response": "Return a generator which yields namespace objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_namespace(name):\n    if name not in configuration_namespaces:\n        configuration_namespaces[name] = ConfigNamespace(name)\n    return configuration_namespaces[name]", "response": "Return a ConfigNamespace by name creating it if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreload one or all namespaces.", "response": "def reload(name=DEFAULT, all_names=False):\n    \"\"\"Reload one or all :class:`ConfigNamespace`. Reload clears the cache of\n    :mod:`staticconf.schema` and :mod:`staticconf.getters`, allowing them to\n    pickup the latest values in the namespace.\n\n    Defaults to reloading just the DEFAULT namespace.\n\n    :param name: the name of the :class:`ConfigNamespace` to reload\n    :param all_names: If True, reload all namespaces, and ignore `name`\n    \"\"\"\n    for namespace in get_namespaces_from_names(name, all_names):\n        for value_proxy in namespace.get_value_proxies():\n            value_proxy.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(name=DEFAULT, all_names=False):\n    for namespace in get_namespaces_from_names(name, all_names):\n        all(value_proxy.get_value() for value_proxy in namespace.get_value_proxies())", "response": "Validate all registered keys or values in a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare two dictionaries for duplicate keys.", "response": "def has_duplicate_keys(config_data, base_conf, raise_error):\n    \"\"\"Compare two dictionaries for duplicate keys. if raise_error is True\n    then raise on exception, otherwise log return True.\"\"\"\n    duplicate_keys = set(base_conf) & set(config_data)\n    if not duplicate_keys:\n        return\n    msg = \"Duplicate keys in config: %s\" % duplicate_keys\n    if raise_error:\n        raise errors.ConfigurationError(msg)\n    log.info(msg)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a compare_func that can be passed to MTimeComparator.", "response": "def build_compare_func(err_logger=None):\n    \"\"\"Returns a compare_func that can be passed to MTimeComparator.\n\n    The returned compare_func first tries os.path.getmtime(filename),\n    then calls err_logger(filename) if that fails. If err_logger is None,\n    then it does nothing. err_logger is always called within the context of\n    an OSError raised by os.path.getmtime(filename). Information on this\n    error can be retrieved by calling sys.exc_info inside of err_logger.\"\"\"\n    def compare_func(filename):\n        try:\n            return os.path.getmtime(filename)\n        except OSError:\n            if err_logger is not None:\n                err_logger(filename)\n        return -1\n    return compare_func"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreconstructs the nested structure of this object s configuration and return it as a dict.", "response": "def get_config_dict(self):\n        \"\"\"Reconstruct the nested structure of this object's configuration\n        and return it as a dict.\n        \"\"\"\n        config_dict = {}\n        for dotted_key, value in self.get_config_values().items():\n            subkeys = dotted_key.split('.')\n            d = config_dict\n            for key in subkeys:\n                d = d.setdefault(key, value if key == subkeys[-1] else {})\n        return config_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view_help(self):\n        def format_desc(desc):\n            return \"%s (Type: %s, Default: %s)\\n%s\" % (\n                    desc.name,\n                    desc.validator.__name__.replace('validate_', ''),\n                    desc.default,\n                    desc.help or '')\n\n        def format_namespace(key, desc_list):\n            return \"\\nNamespace: %s\\n%s\" % (\n                    key,\n                    '\\n'.join(sorted(format_desc(desc) for desc in desc_list)))\n\n        def namespace_cmp(item):\n            name, _ = item\n            return chr(0) if name == DEFAULT else name\n\n        return '\\n'.join(format_namespace(*desc) for desc in\n                         sorted(six.iteritems(self.descriptions),\n                                key=namespace_cmp))", "response": "Return a help message describing all the statically configured keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reload_if_changed(self, force=False):\n        if (force or self.should_check) and self.file_modified():\n            return self.reload()", "response": "Reloads the configuration of the object if the file has changed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a configuration file into a new namespace.", "response": "def load(\n            cls,\n            filename,\n            namespace,\n            loader_func,\n            min_interval=0,\n            comparators=None,\n    ):\n        \"\"\"Create a new :class:`ConfigurationWatcher` and load the initial\n        configuration by calling `loader_func`.\n\n        :param filename: a filename or list of filenames to monitor for changes\n        :param namespace: the name of a namespace to use when loading\n                          configuration. All config data from `filename` will\n                          end up in a :class:`ConfigNamespace` with this name\n        :param loader_func: a function which accepts two arguments and uses\n                            loader functions from :mod:`staticconf.loader` to\n                            load configuration data into a namespace. The\n                            arguments are `filename` and `namespace`\n        :param min_interval: minimum number of seconds to wait between calls to\n                             :func:`os.path.getmtime` to check if a file has\n                             been modified.\n        :param comparators: a list of classes which support the\n            :class:`IComparator` interface which are used to determine if a config\n            file has been modified. See ConfigurationWatcher::__init__.\n        :returns: a :class:`ConfigFacade`\n        \"\"\"\n        watcher = ConfigurationWatcher(\n            build_loader_callable(loader_func, filename, namespace=namespace),\n            filename,\n            min_interval=min_interval,\n            reloader=ReloadCallbackChain(namespace=namespace),\n            comparators=comparators,\n        )\n        watcher.load_config()\n        return cls(watcher)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_iterable(iterable_type, value):\n    if isinstance(value, six.string_types):\n        msg = \"Invalid iterable of type(%s): %s\"\n        raise ValidationError(msg % (type(value), value))\n\n    try:\n        return iterable_type(value)\n    except TypeError:\n        raise ValidationError(\"Invalid iterable: %s\" % (value))", "response": "Convert the iterable to iterable_type or raise a Configuration\n    exception."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_list_type_validator(item_validator):\n    def validate_list_of_type(value):\n        return [item_validator(item) for item in validate_list(value)]\n    return validate_list_of_type", "response": "Build a function which validates that the value is a list of items\n    which are validated using item_validator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_map_type_validator(item_validator):\n    def validate_mapping(value):\n        return dict(item_validator(item) for item in validate_list(value))\n    return validate_mapping", "response": "Return a function which validates that the value is a mapping of\n    items. The function should return pairs of items that will be\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a value proxy with the namespace and add the help_text.", "response": "def register_value_proxy(namespace, value_proxy, help_text):\n    \"\"\"Register a value proxy with the namespace, and add the help_text.\"\"\"\n    namespace.register_proxy(value_proxy)\n    config.config_help.add(\n        value_proxy.config_key, value_proxy.validator, value_proxy.default,\n        namespace.get_name(), help_text)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_getter(validator, getter_namespace=None):\n    def proxy_register(key_name, default=UndefToken, help=None, namespace=None):\n        name        = namespace or getter_namespace or config.DEFAULT\n        namespace   = config.get_namespace(name)\n        return proxy_factory.build(validator, namespace, key_name, default, help)\n\n    return proxy_register", "response": "Create a getter function for retrieving values from the config cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self, validator, namespace, config_key, default, help):\n        proxy_attrs = validator, namespace, config_key, default\n        proxy_key = repr(proxy_attrs)\n        if proxy_key in self.proxies:\n            return self.proxies[proxy_key]\n\n        value_proxy = proxy.ValueProxy(*proxy_attrs)\n        register_value_proxy(namespace, value_proxy, help)\n        return self.proxies.setdefault(proxy_key, value_proxy)", "response": "Build or retrieve a ValueProxy from the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minimizeSPSA(func, x0, args=(), bounds=None, niter=100, paired=True,\n                 a=1.0, alpha=0.602, c=1.0, gamma=0.101,\n                 disp=False, callback=None):\n    \"\"\"\n    Minimization of an objective function by a simultaneous perturbation\n    stochastic approximation algorithm.\n\n    This algorithm approximates the gradient of the function by finite differences\n    along stochastic directions Deltak. The elements of Deltak are drawn from\n    +- 1 with probability one half. The gradient is approximated from the \n    symmetric difference f(xk + ck*Deltak) - f(xk - ck*Deltak), where the evaluation\n    step size ck is scaled according ck =  c/(k+1)**gamma.\n    The algorithm takes a step of size ak = a/(0.01*niter+k+1)**alpha along the\n    negative gradient.\n    \n\n    See Spall, IEEE, 1998, 34, 817-823 for guidelines about how to choose the algorithm's\n    parameters (a, alpha, c, gamma).\n\n    Parameters\n    ----------\n    func: callable\n        objective function to be minimized:\n        called as `func(x, *args)`,\n        if `paired=True`, then called with keyword argument `seed` additionally\n    x0: array-like\n        initial guess for parameters \n    args: tuple\n        extra arguments to be supplied to func\n    bounds: array-like\n        bounds on the variables\n    niter: int\n        number of iterations after which to terminate the algorithm\n    paired: boolean\n        calculate gradient for same random seeds\n    a: float\n       scaling parameter for step size\n    alpha: float\n        scaling exponent for step size\n    c: float\n       scaling parameter for evaluation step size\n    gamma: float\n        scaling exponent for evaluation step size \n    disp: boolean\n        whether to output status updates during the optimization\n    callback: callable\n        called after each iteration, as callback(xk), where xk are the current parameters\n\n    Returns\n    -------\n    `scipy.optimize.OptimizeResult` object\n    \"\"\"\n    A = 0.01 * niter\n\n    if bounds is not None:\n        bounds = np.asarray(bounds)\n        project = lambda x: np.clip(x, bounds[:, 0], bounds[:, 1])\n\n    if args is not None:\n        # freeze function arguments\n        def funcf(x, **kwargs):\n            return func(x, *args, **kwargs)\n\n    N = len(x0)\n    x = x0\n    for k in range(niter):\n        ak = a/(k+1.0+A)**alpha\n        ck = c/(k+1.0)**gamma\n        Deltak = np.random.choice([-1, 1], size=N)\n        fkwargs = dict()\n        if paired:\n            fkwargs['seed'] = np.random.randint(0, np.iinfo(np.uint32).max)\n        if bounds is None:\n            grad = (funcf(x + ck*Deltak, **fkwargs) - funcf(x - ck*Deltak, **fkwargs)) / (2*ck*Deltak)\n            x -= ak*grad\n        else:\n            # ensure evaluation points are feasible\n            xplus = project(x + ck*Deltak)\n            xminus = project(x - ck*Deltak)\n            grad = (funcf(xplus, **fkwargs) - funcf(xminus, **fkwargs)) / (xplus-xminus)\n            x = project(x - ak*grad)\n        # print 100 status updates if disp=True\n        if disp and (k % (niter//100)) == 0:\n            print(x)\n        if callback is not None:\n            callback(x)\n    message = 'terminated after reaching max number of iterations'\n    return OptimizeResult(fun=funcf(x), x=x, nit=niter, nfev=2*niter, message=message, success=True)", "response": "Minimization of an objective function by a simultaneous perturbation of a single object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding root by bysection search.", "response": "def bisect(func, a, b, xtol=1e-6, errorcontrol=True,\n           testkwargs=dict(), outside='extrapolate',\n           ascending=None,\n           disp=False):\n    \"\"\"Find root by bysection search.\n\n    If the function evaluation is noisy then use `errorcontrol=True` for adaptive\n    sampling of the function during the bisection search.\n\n    Parameters\n    ----------\n    func: callable\n        Function of which the root should be found. If `errorcontrol=True`\n        then the function should be derived from `AverageBase`.\n    a, b: float\n        initial interval\n    xtol: float\n        target tolerance for interval size\n    errorcontrol: boolean\n        if true, assume that function is derived from `AverageBase`.\n    testkwargs: only for `errorcontrol=True`\n        see `AverageBase.test0`\n    outside: ['extrapolate', 'raise']\n        How to handle the case where f(a) and f(b) have same sign,\n        i.e. where the root lies outside of the interval.\n        If 'raise' throws a `BisectException`.\n    ascending: allow passing in directly whether function is ascending or not\n        if ascending=True then it is assumed without check that f(a) < 0 and f(b) > 0\n        if ascending=False then it is assumed without check that f(a) > 0 and f(b) < 0\n\n    Returns\n    -------\n    float, root of function\n    \"\"\"\n    search = True\n    # check whether function is ascending or not\n    if ascending is None:\n        if errorcontrol:\n            testkwargs.update(dict(type_='smaller', force=True))\n            fa = func.test0(a, **testkwargs)\n            fb = func.test0(b, **testkwargs)\n        else:\n            fa = func(a) < 0\n            fb = func(b) < 0\n        if fa and not fb:\n            ascending = True\n        elif fb and not fa:\n            ascending =  False\n        else:\n            if disp:\n                print('Warning: func(a) and func(b) do not have opposing signs -> no search done')\n            if outside == 'raise':\n                raise BisectException()\n            search = False\n\n    # refine interval until it has reached size xtol, except if root outside\n    while (b-a > xtol) and search:\n        mid = (a+b)/2.0\n        if ascending:\n            if ((not errorcontrol) and (func(mid) < 0)) or \\\n                    (errorcontrol and func.test0(mid, **testkwargs)):\n                a = mid \n            else:\n                b = mid\n        else:\n            if ((not errorcontrol) and (func(mid) < 0)) or \\\n                    (errorcontrol and func.test0(mid, **testkwargs)):\n                b = mid \n            else:\n                a = mid\n        if disp:\n            print('bisect bounds', a, b)\n    # interpolate linearly to get zero\n    if errorcontrol:\n        ya, yb = func(a)[0], func(b)[0]\n    else:\n        ya, yb = func(a), func(b)\n    m = (yb-ya) / (b-a)\n    res = a-ya/m\n    if disp:\n        print('bisect final value', res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndocument : selection_set fragment_list", "response": "def p_document_shorthand_with_fragments(self, p):\n        \"\"\"\n        document : selection_set fragment_list\n        \"\"\"\n        p[0] = Document(definitions=[Query(selections=p[1])] + p[2])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an operation definition", "response": "def p_operation_definition1(self, p):\n        \"\"\"\n        operation_definition : operation_type name variable_definitions directives selection_set\n        \"\"\"\n        p[0] = self.operation_cls(p[1])(\n            selections=p[5],\n            name=p[2],\n            variable_definitions=p[3],\n            directives=p[4],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the 2nd parameter of the operation definition", "response": "def p_operation_definition2(self, p):\n        \"\"\"\n        operation_definition : operation_type name variable_definitions selection_set\n        \"\"\"\n        p[0] = self.operation_cls(p[1])(\n            selections=p[4],\n            name=p[2],\n            variable_definitions=p[3],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the 3rd party operation definition.", "response": "def p_operation_definition3(self, p):\n        \"\"\"\n        operation_definition : operation_type name directives selection_set\n        \"\"\"\n        p[0] = self.operation_cls(p[1])(\n            selections=p[4],\n            name=p[2],\n            directives=p[3],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_operation_definition4(self, p):\n        p[0] = self.operation_cls(p[1])(selections=p[3], name=p[2])", "response": "P 3. 4. 2. 2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_operation_definition5(self, p):\n        p[0] = self.operation_cls(p[1])(\n            selections=p[4],\n            variable_definitions=p[2],\n            directives=p[3],\n        )", "response": "Parse the 5 part of the operation definition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_operation_definition6(self, p):\n        p[0] = self.operation_cls(p[1])(\n            selections=p[3],\n            variable_definitions=p[2],\n        )", "response": "P 6 - level operation definition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_operation_definition7(self, p):\n        p[0] = self.operation_cls(p[1])(\n            selections=p[3],\n            directives=p[2],\n        )", "response": "Parse the 7th section of the resource definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfields : alias name arguments directives selection_set", "response": "def p_field_all(self, p):\n        \"\"\"\n        field : alias name arguments directives selection_set\n        \"\"\"\n        p[0] = Field(name=p[2], alias=p[1], arguments=p[3], directives=p[4],\n                     selections=p[5])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the optional 1. 0 field.", "response": "def p_field_optional1_1(self, p):\n        \"\"\"\n        field : name arguments directives selection_set\n        \"\"\"\n        p[0] = Field(name=p[1], arguments=p[2], directives=p[3],\n                     selections=p[5])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_field_optional1_2(self, p):\n        p[0] = Field(name=p[2], alias=p[1], directives=p[3], selections=p[5])", "response": "Parse the optional 1. 2 section of the spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_field_optional1_3(self, p):\n        p[0] = Field(name=p[2], alias=p[1], arguments=p[3], selections=p[4])", "response": "Parse the optional 1. 3. 1. 2 field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the optional 1. 4 section of the log message.", "response": "def p_field_optional1_4(self, p):\n        \"\"\"\n        field : alias name arguments directives\n        \"\"\"\n        p[0] = Field(name=p[2], alias=p[1], arguments=p[3], directives=p[4])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_field_optional2_1(self, p):\n        p[0] = Field(name=p[1], directives=p[2], selections=p[3])", "response": "Parse the optional 2. 1 section of the spec."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfield : name arguments selection_set", "response": "def p_field_optional2_2(self, p):\n        \"\"\"\n        field : name arguments selection_set\n        \"\"\"\n        p[0] = Field(name=p[1], arguments=p[2], selections=p[3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_field_optional2_3(self, p):\n        p[0] = Field(name=p[1], arguments=p[2], directives=p[3])", "response": "Parse the optional 2. 3. 1. 2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse optional 2. 4. 1. 2.", "response": "def p_field_optional2_4(self, p):\n        \"\"\"\n        field : alias name selection_set\n        \"\"\"\n        p[0] = Field(name=p[2], alias=p[1], selections=p[3])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an optional 2. 5 field.", "response": "def p_field_optional2_5(self, p):\n        \"\"\"\n        field : alias name directives\n        \"\"\"\n        p[0] = Field(name=p[2], alias=p[1], directives=p[3])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_field_optional2_6(self, p):\n        p[0] = Field(name=p[2], alias=p[1], arguments=p[3])", "response": "Parse the optional 2. 6. 1. 1 Field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_directive(self, p):\n        arguments = p[3] if len(p) == 4 else None\n        p[0] = Directive(name=p[2], arguments=arguments)", "response": "parse a tag directive"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_const_object_field_list(self, p):\n        obj = p[1].copy()\n        obj.update(p[2])\n        p[0] = obj", "response": "A function to update the const_object_field_list field of a class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort a list of triples by relation name.", "response": "def alphanum_order(triples):\n    \"\"\"\n    Sort a list of triples by relation name.\n\n    Embedded integers are sorted numerically, but otherwise the sorting\n    is alphabetic.\n    \"\"\"\n    return sorted(\n        triples,\n        key=lambda t: [\n            int(t) if t.isdigit() else t\n            for t in re.split(r'([0-9]+)', t.relation or '')\n        ]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize the graph g from top to PENMAN notation.", "response": "def encode(g, top=None, cls=PENMANCodec, **kwargs):\n    \"\"\"\n    Serialize the graph *g* from *top* to PENMAN notation.\n\n    Args:\n        g: the Graph object\n        top: the node identifier for the top of the serialized graph; if\n            unset, the original top of *g* is used\n        cls: serialization codec class\n        kwargs: keyword arguments passed to the constructor of *cls*\n    Returns:\n        the PENMAN-serialized string of the Graph *g*\n    Example:\n\n        >>> encode(Graph([('h', 'instance', 'hi')]))\n        (h / hi)\n    \"\"\"\n    codec = cls(**kwargs)\n    return codec.encode(g, top=top)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(source, triples=False, cls=PENMANCodec, **kwargs):\n    decode = cls(**kwargs).iterdecode\n    if hasattr(source, 'read'):\n        return list(decode(source.read()))\n    else:\n        with open(source) as fh:\n            return list(decode(fh.read()))", "response": "Deserialize a list of PENMAN - encoded graphs from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(graphs, file, triples=False, cls=PENMANCodec, **kwargs):\n    text = dumps(graphs, triples=triples, cls=cls, **kwargs)\n\n    if hasattr(file, 'write'):\n        print(text, file=file)\n    else:\n        with open(file, 'w') as fh:\n            print(text, file=fh)", "response": "Serialize each graph in graphs to PENMAN and write to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing graphs to the PENMAN format.", "response": "def dumps(graphs, triples=False, cls=PENMANCodec, **kwargs):\n    \"\"\"\n    Serialize each graph in *graphs* to the PENMAN format.\n\n    Args:\n        graphs: an iterable of Graph objects\n        triples: if True, write graphs as triples instead of as PENMAN\n    Returns:\n        the string of serialized graphs\n    \"\"\"\n    codec = cls(**kwargs)\n    strings = [codec.encode(g, triples=triples) for g in graphs]\n    return '\\n\\n'.join(strings)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes the graph g from top to PENMAN notation.", "response": "def encode(self, g, top=None, triples=False):\n        \"\"\"\n        Serialize the graph *g* from *top* to PENMAN notation.\n\n        Args:\n            g: the Graph object\n            top: the node identifier for the top of the serialized\n                graph; if unset, the original top of *g* is used\n            triples: if True, serialize as a conjunction of logical triples\n        Returns:\n            the PENMAN-serialized string of the Graph *g*\n        Example:\n\n            >>> codec = PENMANCodec()\n            >>> codec.encode(Graph([('h', 'instance', 'hi')]))\n            (h / hi)\n            >>> codec.encode(Graph([('h', 'instance', 'hi')]),\n            ...                      triples=True)\n            instance(h, hi)\n        \"\"\"\n        if len(g.triples()) == 0:\n            raise EncodeError('Cannot encode empty graph.')\n        if triples:\n            return self._encode_triple_conjunction(g, top=top)\n        else:\n            return self._encode_penman(g, top=top)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_triple(self, lhs, relation, rhs):\n        relation = relation.replace(':', '', 1)  # remove leading :\n\n        if self.is_relation_inverted(relation):  # deinvert\n            source, target, inverted = rhs, lhs, True\n            relation = self.invert_relation(relation)\n        else:\n            source, target, inverted = lhs, rhs, False\n\n        source = _default_cast(source)\n        target = _default_cast(target)\n\n        if relation == '':  # set empty relations to None\n            relation = None\n\n        return Triple(source, relation, target, inverted)", "response": "Process a triple from lhs to rhs and return a new triple."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Graph object from a list of triples.", "response": "def triples_to_graph(self, triples, top=None):\n        \"\"\"\n        Create a Graph from *triples* considering codec configuration.\n\n        The Graph class does not know about information in the codec,\n        so if Graph instantiation depends on special `TYPE_REL` or\n        `TOP_VAR` values, use this function instead of instantiating\n        a Graph object directly. This is also where edge\n        normalization (de-inversion) and value type conversion occur\n        (via handle_triple()).\n\n        Args:\n            triples: an iterable of (lhs, relation, rhs) triples\n            top: node identifier of the top node\n        Returns:\n            a Graph object\n        \"\"\"\n        inferred_top = triples[0][0] if triples else None\n        ts = []\n        for triple in triples:\n            if triple[0] == self.TOP_VAR and triple[1] == self.TOP_REL:\n                inferred_top = triple[2]\n            else:\n                ts.append(self.handle_triple(*triple))\n        top = self.handle_triple(self.TOP_VAR, self.TOP_REL, top).target\n        return Graph(ts, top=top or inferred_top)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the relation is inverted.", "response": "def is_relation_inverted(self, relation):\n        \"\"\"\n        Return True if *relation* is inverted.\n        \"\"\"\n        return (\n            relation in self._deinversions or\n            (relation.endswith('-of') and relation not in self._inversions)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvert or deinvert relation.", "response": "def invert_relation(self, relation):\n        \"\"\"\n        Invert or deinvert *relation*.\n        \"\"\"\n        if self.is_relation_inverted(relation):\n            rel = self._deinversions.get(relation, relation[:-3])\n        else:\n            rel = self._inversions.get(relation, relation + '-of')\n        if rel is None:\n             raise PenmanError(\n                'Cannot (de)invert {}; not allowed'.format(relation)\n            )\n        return rel"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef triples(self, source=None, relation=None, target=None):\n        triplematch = lambda t: (\n            (source is None or source == t.source) and\n            (relation is None or relation == t.relation) and\n            (target is None or target == t.target)\n        )\n        return list(filter(triplematch, self._triples))", "response": "Return a list of triples filtered by their source relation or target."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edges(self, source=None, relation=None, target=None):\n        edgematch = lambda e: (\n            (source is None or source == e.source) and\n            (relation is None or relation == e.relation) and\n            (target is None or target == e.target)\n        )\n        variables = self.variables()\n        edges = [t for t in self._triples if t.target in variables]\n        return list(filter(edgematch, edges))", "response": "Return a list of edges that are in the terminal graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all attributes that are in the current context.", "response": "def attributes(self, source=None, relation=None, target=None):\n        \"\"\"\n        Return attributes filtered by their *source*, *relation*, or *target*.\n\n        Attributes don't include triples where the target is a nonterminal.\n        \"\"\"\n        attrmatch = lambda a: (\n            (source is None or source == a.source) and\n            (relation is None or relation == a.relation) and\n            (target is None or target == a.target)\n        )\n        variables = self.variables()\n        attrs = [t for t in self.triples() if t.target not in variables]\n        return list(filter(attrmatch, attrs))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a mapping of variables to their re - entrancy count.", "response": "def reentrancies(self):\n        \"\"\"\n        Return a mapping of variables to their re-entrancy count.\n\n        A re-entrancy is when more than one edge selects a node as its\n        target. These graphs are rooted, so the top node always has an\n        implicit entrancy. Only nodes with re-entrancies are reported,\n        and the count is only for the entrant edges beyond the first.\n        Also note that these counts are for the interpreted graph, not\n        for the linearized form, so inverted edges are always\n        re-entrant.\n        \"\"\"\n        entrancies = defaultdict(int)\n        entrancies[self.top] += 1  # implicit entrancy to top\n        for t in self.edges():\n            entrancies[t.target] += 1\n        return dict((v, cnt - 1) for v, cnt in entrancies.items() if cnt >= 2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_1d(inp):\n    if isinstance(inp, list):\n        return check_1d(np.array(inp))\n    if isinstance(inp, np.ndarray):\n        if inp.ndim == 1: # input is a vector\n            return inp", "response": "Checks input to be a vector. Converts lists to np. ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck input to be a matrix. Converts lists of lists to np. ndarray.", "response": "def check_2d(inp):\n    \"\"\"\n    Check input to be a matrix. Converts lists of lists to np.ndarray.\n\n    Also allows the input to be a scipy sparse matrix.\n    \n    Parameters\n    ----------\n    inp : obj\n        Input matrix\n\n    Returns\n    -------\n    numpy.ndarray, scipy.sparse or None\n        Input matrix or None\n\n    Examples\n    --------\n    >>> check_2d([[0, 1], [2, 3]])\n    [[0, 1], [2, 3]]\n\n    >>> check_2d('test')\n    None\n\n    \"\"\"\n    if isinstance(inp, list):\n        return check_2d(np.array(inp))\n    if isinstance(inp, (np.ndarray, np.matrixlib.defmatrix.matrix)):\n        if inp.ndim == 2: # input is a dense matrix\n            return inp\n    if sps.issparse(inp):\n        if inp.ndim == 2: # input is a sparse matrix\n            return inp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef graph_to_laplacian(G, normalized=True):\n    try:\n        import networkx as nx\n        if isinstance(G, nx.Graph):\n            if normalized:\n                return nx.normalized_laplacian_matrix(G)\n            else:\n                return nx.laplacian_matrix(G)\n    except ImportError:\n        pass\n    try:\n        import graph_tool.all as gt\n        if isinstance(G, gt.Graph):\n            if normalized:\n                return gt.laplacian_type(G, normalized=True)\n            else:\n                return gt.laplacian(G)\n    except ImportError:\n        pass\n    try:\n        import igraph as ig\n        if isinstance(G, ig.Graph):\n            if normalized:\n                return np.array(G.laplacian(normalized=True))\n            else:\n                return np.array(G.laplacian())\n    except ImportError:\n        pass", "response": "Converts a graph from popular Python packages to Laplacian representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a sparse or dence adjacency matrix to Laplacian.", "response": "def mat_to_laplacian(mat, normalized):\n    \"\"\"\n    Converts a sparse or dence adjacency matrix to Laplacian.\n    \n    Parameters\n    ----------\n    mat : obj\n        Input adjacency matrix. If it is a Laplacian matrix already, return it.\n    normalized : bool\n        Whether to use normalized Laplacian.\n        Normalized and unnormalized Laplacians capture different properties of graphs, e.g. normalized Laplacian spectrum can determine whether a graph is bipartite, but not the number of its edges. We recommend using normalized Laplacian.\n\n    Returns\n    -------\n    obj\n        Laplacian of the input adjacency matrix\n\n    Examples\n    --------\n    >>> mat_to_laplacian(numpy.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]]), False)\n    [[ 2, -1, -1], [-1,  2, -1], [-1, -1,  2]]\n\n    \"\"\"\n    if sps.issparse(mat):\n        if np.all(mat.diagonal()>=0): # Check diagonal\n            if np.all((mat-sps.diags(mat.diagonal())).data <= 0): # Check off-diagonal elements\n                return mat\n    else:\n        if np.all(np.diag(mat)>=0): # Check diagonal\n            if np.all(mat - np.diag(mat) <= 0): # Check off-diagonal elements\n                return mat\n    deg = np.squeeze(np.asarray(mat.sum(axis=1)))\n    if sps.issparse(mat):\n        L = sps.diags(deg) - mat\n    else:\n        L = np.diag(deg) - mat\n    if not normalized:\n        return L\n    with np.errstate(divide='ignore'):\n        sqrt_deg = 1.0 / np.sqrt(deg)\n    sqrt_deg[sqrt_deg==np.inf] = 0\n    if sps.issparse(mat):\n        sqrt_deg_mat = sps.diags(sqrt_deg)\n    else:\n        sqrt_deg_mat = np.diag(sqrt_deg)\n    return sqrt_deg_mat.dot(L).dot(sqrt_deg_mat)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating Laplacian spectrum using lower and upper parts of the eigenspectrum.", "response": "def updown_linear_approx(eigvals_lower, eigvals_upper, nv):\n    \"\"\"\n    Approximates Laplacian spectrum using upper and lower parts of the eigenspectrum.\n    \n    Parameters\n    ----------\n    eigvals_lower : numpy.ndarray\n        Lower part of the spectrum, sorted\n    eigvals_upper : numpy.ndarray\n        Upper part of the spectrum, sorted\n    nv : int\n        Total number of nodes (eigenvalues) in the graph.\n\n    Returns\n    -------\n    numpy.ndarray\n        Vector of approximated eigenvalues\n\n    Examples\n    --------\n    >>> updown_linear_approx([1, 2, 3], [7, 8, 9], 9)\n    array([1,  2,  3,  4,  5,  6,  7,  8,  9])\n\n    \"\"\"\n    nal = len(eigvals_lower)\n    nau = len(eigvals_upper)\n    if nv < nal + nau:\n        raise ValueError('Number of supplied eigenvalues ({0} lower and {1} upper) is higher than number of nodes ({2})!'.format(nal, nau, nv))\n    ret = np.zeros(nv)\n    ret[:nal] = eigvals_lower\n    ret[-nau:] = eigvals_upper\n    ret[nal-1:-nau+1] = np.linspace(eigvals_lower[-1], eigvals_upper[0], nv-nal-nau+2)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eigenvalues_auto(mat, n_eivals='auto'):\n    do_full = True\n    n_lower = 150\n    n_upper = 150\n    nv = mat.shape[0]\n    if n_eivals == 'auto':\n        if mat.shape[0] > 1024:\n            do_full = False\n    if n_eivals == 'full':\n        do_full = True\n    if isinstance(n_eivals, int):\n        n_lower = n_upper = n_eivals\n        do_full = False\n    if isinstance(n_eivals, tuple):\n        n_lower, n_upper = n_eivals\n        do_full = False\n    if do_full and sps.issparse(mat):\n        mat = mat.todense()\n    if sps.issparse(mat):\n        if n_lower == n_upper:\n            tr_eivals = spsl.eigsh(mat, 2*n_lower, which='BE', return_eigenvectors=False)\n            return updown_linear_approx(tr_eivals[:n_upper], tr_eivals[n_upper:], nv)\n        else:\n            lo_eivals = spsl.eigsh(mat, n_lower, which='SM', return_eigenvectors=False)[::-1]\n            up_eivals = spsl.eigsh(mat, n_upper, which='LM', return_eigenvectors=False)\n            return updown_linear_approx(lo_eivals, up_eivals, nv)\n    else:\n        if do_full:\n            return spl.eigvalsh(mat)\n        else:\n            lo_eivals = spl.eigvalsh(mat, eigvals=(0, n_lower-1))\n            up_eivals = spl.eigvalsh(mat, eigvals=(nv-n_upper-1, nv-1))\n            return updown_linear_approx(lo_eivals, up_eivals, nv)", "response": "A function that computes the spectrum of a given Laplacian matrix and returns the vector of approximated eigenvalues."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef netlsd(inp, timescales=np.logspace(-2, 2, 250), kernel='heat', eigenvalues='auto', normalization='empty', normalized_laplacian=True):\n    if kernel not in {'heat', 'wave'}:\n        raise AttributeError('Unirecognized kernel type: expected one of [\\'heat\\', \\'wave\\'], got {0}'.format(kernel))\n    if not isinstance(normalized_laplacian, bool):\n        raise AttributeError('Unknown Laplacian type: expected bool, got {0}'.format(normalized_laplacian))\n    if not isinstance(eigenvalues, (int, tuple, str)):\n        raise AttributeError('Unirecognized requested eigenvalue number: expected type of [\\'str\\', \\'tuple\\', or \\'int\\'], got {0}'.format(type(eigenvalues)))\n    if not isinstance(timescales, np.ndarray):\n        raise AttributeError('Unirecognized timescales data type: expected np.ndarray, got {0}'.format(type(timescales)))\n    if timescales.ndim != 1:\n        raise AttributeError('Unirecognized timescales dimensionality: expected a vector, got {0}-d array'.format(timescales.ndim))\n    if normalization not in {'complete', 'empty', 'none', True, False, None}:\n        if not isinstance(normalization, np.ndarray):\n            raise AttributeError('Unirecognized normalization type: expected one of [\\'complete\\', \\'empty\\', None or np.ndarray], got {0}'.format(normalization))\n        if normalization.ndim != 1:\n            raise AttributeError('Unirecognized normalization dimensionality: expected a vector, got {0}-d array'.format(normalization.ndim))\n        if timescales.shape[0] != normalization.shape[0]:\n            raise AttributeError('Unirecognized normalization dimensionality: expected {0}-length vector, got length {1}'.format(timescales.shape[0], normalization.shape[0]))\n\n    eivals = check_1d(inp)\n    if eivals is None:\n        mat = check_2d(inp)\n        if mat is None:\n            mat = graph_to_laplacian(inp, normalized_laplacian)\n            if mat is None:\n                raise ValueError('Unirecognized input type: expected one of [\\'np.ndarray\\', \\'scipy.sparse\\', \\'networkx.Graph\\',\\' graph_tool.Graph,\\' or \\'igraph.Graph\\'], got {0}'.format(type(inp)))\n        else:\n            mat = mat_to_laplacian(inp, normalized_laplacian)\n        eivals = eigenvalues_auto(mat, eigenvalues)\n    if kernel == 'heat':\n        return _hkt(eivals, timescales, normalization, normalized_laplacian)\n    else:\n        return _wkt(eivals, timescales, normalization, normalized_laplacian)", "response": "This function computes the NetLSD signature from some input time series and kernels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heat(inp, timescales=np.logspace(-2, 2, 250), eigenvalues='auto', normalization='empty', normalized_laplacian=True):\n    return netlsd(inp, timescales, 'heat', eigenvalues, normalization, normalized_laplacian)", "response": "Compute the heat kernel trace from some input time series timescales and normalization."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wave(inp, timescales=np.linspace(0, 2*np.pi, 250), eigenvalues='auto', normalization='empty', normalized_laplacian=True):\n    return netlsd(inp, timescales, 'wave', eigenvalues, normalization, normalized_laplacian)", "response": "This function computes the wave kernel trace from some input time series timescales and normalization."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _hkt(eivals, timescales, normalization, normalized_laplacian):\n    nv = eivals.shape[0]\n    hkt = np.zeros(timescales.shape)\n    for idx, t in enumerate(timescales):\n        hkt[idx] = np.sum(np.exp(-t * eivals))\n    if isinstance(normalization, np.ndarray):\n        return hkt / normalization\n    if normalization == 'empty' or normalization == True:\n        return hkt / nv\n    if normalization == 'complete':\n        if normalized_laplacian:\n            return hkt / (1 + (nv - 1) * np.exp(-timescales))\n        else:\n            return hkt / (1 + nv * np.exp(-nv * timescales))\n    return hkt", "response": "Compute the heat kernel trace from given eigenvalues timescales and normalization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the wave kernel trace from given eigenvalues timescales and normalization.", "response": "def _wkt(eivals, timescales, normalization, normalized_laplacian):\n    \"\"\"\n    Computes wave kernel trace from given eigenvalues, timescales, and normalization.\n\n    For precise definition, please refer to \"NetLSD: Hearing the Shape of a Graph\" by A. Tsitsulin, D. Mottin, P. Karras, A. Bronstein, E. M\u00fcller. Published at KDD'18.\n    \n    Parameters\n    ----------\n    eivals : numpy.ndarray\n        Eigenvalue vector\n    timescales : numpy.ndarray\n        Vector of discrete timesteps for the kernel computation\n    normalization : str or numpy.ndarray\n        Either 'empty', 'complete' or None.\n        If None or any ther value, return unnormalized wave kernel trace.\n        For the details how 'empty' and 'complete' are computed, please refer to the paper.\n        If np.ndarray, they are treated as exact normalization constants\n    normalized_laplacian: bool\n        Defines whether the eigenvalues came from the normalized Laplacian. It only affects 'complete' normalization.\n\n    Returns\n    -------\n    numpy.ndarray\n        Wave kernel trace signature\n\n    \"\"\"\n    nv = eivals.shape[0]\n    wkt = np.zeros(timescales.shape)\n    for idx, t in enumerate(timescales):\n        wkt[idx] = np.sum(np.exp(-1j * t * eivals))\n    if isinstance(normalization, np.ndarray):\n        return hkt / normalization\n    if normalization == 'empty' or normalization == True:\n        return wkt / nv\n    if normalization == 'complete':\n        if normalized_laplacian:\n            return wkt / (1 + (nv - 1) * np.cos(timescales))\n        else:\n            return wkt / (1 + (nv - 1) * np.cos(nv * timescales))\n    return wkt"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves all the elements from the list.", "response": "def clear(self):\n        \"\"\"Remove all the elements from the list.\"\"\"\n        self._len = 0\n        del self._maxes[:]\n        del self._lists[:]\n        del self._keys[:]\n        del self._index[:]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the first occurrence of val from the internal list of the internal list of the internal list of the internal list of the internal list of the internal list.", "response": "def discard(self, val):\n        \"\"\"\n        Remove the first occurrence of *val*.\n\n        If *val* is not a member, does nothing.\n        \"\"\"\n        _maxes = self._maxes\n\n        if not _maxes:\n            return\n\n        key = self._key(val)\n        pos = bisect_left(_maxes, key)\n\n        if pos == len(_maxes):\n            return\n\n        _keys = self._keys\n        _lists = self._lists\n        idx = bisect_left(_keys[pos], key)\n\n        len_keys = len(_keys)\n        len_sublist = len(_keys[pos])\n\n        while True:\n            if _keys[pos][idx] != key:\n                return\n            if _lists[pos][idx] == val:\n                self._delete(pos, idx)\n                return\n            idx += 1\n            if idx == len_sublist:\n                pos += 1\n                if pos == len_keys:\n                    return\n                len_sublist = len(_keys[pos])\n                idx = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove first occurrence of val from the internal list. Raises ValueError if val is not present.", "response": "def remove(self, val):\n        \"\"\"\n        Remove first occurrence of *val*.\n\n        Raises ValueError if *val* is not present.\n        \"\"\"\n        _maxes = self._maxes\n\n        if not _maxes:\n            raise ValueError('{0} not in list'.format(repr(val)))\n\n        key = self._key(val)\n        pos = bisect_left(_maxes, key)\n\n        if pos == len(_maxes):\n            raise ValueError('{0} not in list'.format(repr(val)))\n\n        _keys = self._keys\n        _lists = self._lists\n        idx = bisect_left(_keys[pos], key)\n\n        len_keys = len(_keys)\n        len_sublist = len(_keys[pos])\n\n        while True:\n            if _keys[pos][idx] != key:\n                raise ValueError('{0} not in list'.format(repr(val)))\n            if _lists[pos][idx] == val:\n                self._delete(pos, idx)\n                return\n            idx += 1\n            if idx == len_sublist:\n                pos += 1\n                if pos == len_keys:\n                    raise ValueError('{0} not in list'.format(repr(val)))\n                len_sublist = len(_keys[pos])\n                idx = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _delete(self, pos, idx):\n        _maxes, _lists, _keys, _index = self._maxes, self._lists, self._keys, self._index\n\n        keys_pos = _keys[pos]\n        lists_pos = _lists[pos]\n\n        del keys_pos[idx]\n        del lists_pos[idx]\n        self._len -= 1\n\n        len_keys_pos = len(keys_pos)\n\n        if len_keys_pos > self._half:\n\n            _maxes[pos] = keys_pos[-1]\n\n            if len(_index) > 0:\n                child = self._offset + pos\n                while child > 0:\n                    _index[child] -= 1\n                    child = (child - 1) >> 1\n                _index[0] -= 1\n\n        elif len(_keys) > 1:\n\n            if not pos:\n                pos += 1\n\n            prev = pos - 1\n            _keys[prev].extend(_keys[pos])\n            _lists[prev].extend(_lists[pos])\n            _maxes[prev] = _keys[prev][-1]\n\n            del _keys[pos]\n            del _lists[pos]\n            del _maxes[pos]\n            del _index[:]\n\n            self._expand(prev)\n\n        elif len_keys_pos:\n\n            _maxes[pos] = keys_pos[-1]\n\n        else:\n\n            del _keys[pos]\n            del _lists[pos]\n            del _maxes[pos]\n            del _index[:]", "response": "Delete the item at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an index pair to a single index that corresponds to the position of the value in the sorted list at the given position.", "response": "def _loc(self, pos, idx):\n        \"\"\"Convert an index pair (alpha, beta) into a single index that corresponds to\n        the position of the value in the sorted list.\n\n        Most queries require the index be built. Details of the index are\n        described in self._build_index.\n\n        Indexing requires traversing the tree from a leaf node to the root. The\n        parent of each node is easily computable at (pos - 1) // 2.\n\n        Left-child nodes are always at odd indices and right-child nodes are\n        always at even indices.\n\n        When traversing up from a right-child node, increment the total by the\n        left-child node.\n\n        The final index is the sum from traversal and the index in the sublist.\n\n        For example, using the index from self._build_index:\n\n        _index = 14 5 9 3 2 4 5\n        _offset = 3\n\n        Tree:\n\n                 14\n              5      9\n            3   2  4   5\n\n        Converting index pair (2, 3) into a single index involves iterating like\n        so:\n\n        1. Starting at the leaf node: offset + alpha = 3 + 2 = 5. We identify\n           the node as a left-child node. At such nodes, we simply traverse to\n           the parent.\n\n        2. At node 9, position 2, we recognize the node as a right-child node\n           and accumulate the left-child in our total. Total is now 5 and we\n           traverse to the parent at position 0.\n\n        3. Iteration ends at the root.\n\n        Computing the index is the sum of the total and beta: 5 + 3 = 8.\n        \"\"\"\n        if not pos:\n            return idx\n\n        _index = self._index\n\n        if not len(_index):\n            self._build_index()\n\n        total = 0\n\n        # Increment pos to point in the index to len(self._lists[pos]).\n\n        pos += self._offset\n\n        # Iterate until reaching the root of the index tree at pos = 0.\n\n        while pos:\n\n            # Right-child nodes are at odd indices. At such indices\n            # account the total below the left child node.\n\n            if not (pos & 1):\n                total += _index[pos - 1]\n\n            # Advance pos to the parent node.\n\n            pos = (pos - 1) >> 1\n\n        return total + idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef islice(self, start=None, stop=None, reverse=False):\n        _len = self._len\n\n        if not _len:\n            return iter(())\n\n        start, stop, step = self._slice(slice(start, stop))\n\n        if start >= stop:\n            return iter(())\n\n        _pos = self._pos\n\n        min_pos, min_idx = _pos(start)\n\n        if stop == _len:\n            max_pos = len(self._lists) - 1\n            max_idx = len(self._lists[-1])\n        else:\n            max_pos, max_idx = _pos(stop)\n\n        return self._islice(min_pos, min_idx, max_pos, max_idx, reverse)", "response": "Returns an iterator that slices self from start to stop index inclusive and exclusive respectively."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef irange(self, minimum=None, maximum=None, inclusive=(True, True),\n               reverse=False):\n        \"\"\"\n        Create an iterator of values between `minimum` and `maximum`.\n\n        `inclusive` is a pair of booleans that indicates whether the minimum\n        and maximum ought to be included in the range, respectively. The\n        default is (True, True) such that the range is inclusive of both\n        minimum and maximum.\n\n        Both `minimum` and `maximum` default to `None` which is automatically\n        inclusive of the start and end of the list, respectively.\n\n        When `reverse` is `True` the values are yielded from the iterator in\n        reverse order; `reverse` defaults to `False`.\n        \"\"\"\n        minimum = self._key(minimum) if minimum is not None else None\n        maximum = self._key(maximum) if maximum is not None else None\n        return self.irange_key(\n            min_key=minimum, max_key=maximum,\n            inclusive=inclusive, reverse=reverse,\n        )", "response": "Create an iterator of values between minimum and maximum."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy(self):\n        return self.__class__(self, key=self._key, load=self._load)", "response": "Return a shallow copy of the sorted list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, val):\n        _maxes, _lists, _keys = self._maxes, self._lists, self._keys\n\n        key = self._key(val)\n\n        if not _maxes:\n            _maxes.append(key)\n            _keys.append([key])\n            _lists.append([val])\n            self._len = 1\n            return\n\n        pos = len(_keys) - 1\n\n        if key < _keys[pos][-1]:\n            msg = '{0} not in sort order at index {1}'.format(repr(val), self._len)\n            raise ValueError(msg)\n\n        _maxes[pos] = key\n        _keys[pos].append(key)\n        _lists[pos].append(val)\n        self._len += 1\n        self._expand(pos)", "response": "Append the element val to the list. Raises a ValueError if the element val would violate the sort order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextend the list by appending all elements from the given list. Raises a ValueError if the sort order is violated.", "response": "def extend(self, values):\n        \"\"\"\n        Extend the list by appending all elements from the *values*. Raises a\n        ValueError if the sort order would be violated.\n        \"\"\"\n        _maxes, _keys, _lists, _load = self._maxes, self._keys, self._lists, self._load\n\n        if not isinstance(values, list):\n            values = list(values)\n\n        keys = list(map(self._key, values))\n\n        if any(keys[pos - 1] > keys[pos]\n               for pos in range(1, len(keys))):\n            raise ValueError('given sequence not in sort order')\n\n        offset = 0\n\n        if _maxes:\n            if keys[0] < _keys[-1][-1]:\n                msg = '{0} not in sort order at index {1}'.format(repr(values[0]), self._len)\n                raise ValueError(msg)\n\n            if len(_keys[-1]) < self._half:\n                _lists[-1].extend(values[:_load])\n                _keys[-1].extend(keys[:_load])\n                _maxes[-1] = _keys[-1][-1]\n                offset = _load\n\n        len_keys = len(_keys)\n\n        for idx in range(offset, len(keys), _load):\n            _lists.append(values[idx:(idx + _load)])\n            _keys.append(keys[idx:(idx + _load)])\n            _maxes.append(_keys[-1][-1])\n\n        _index = self._index\n\n        if len_keys == len(_keys):\n            len_index = len(_index)\n            if len_index > 0:\n                len_values = len(values)\n                child = len_index - 1\n                while child:\n                    _index[child] += len_values\n                    child = (child - 1) >> 1\n                _index[0] += len_values\n        else:\n            del _index[:]\n\n        self._len += len(values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove and returns the item at the specified index.", "response": "def pop(self, idx=-1):\n        \"\"\"\n        Remove and return item at *idx* (default last).  Raises IndexError if\n        list is empty or index is out of range.  Negative indices are supported,\n        as for slice indices.\n        \"\"\"\n        if (idx < 0 and -idx > self._len) or (idx >= self._len):\n            raise IndexError('pop index out of range')\n\n        pos, idx = self._pos(idx)\n        val = self._lists[pos][idx]\n        self._delete(pos, idx)\n\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction decorator for methods not implemented in Python 2. 6.", "response": "def not26(func):\n    \"\"\"Function decorator for methods not implemented in Python 2.6.\"\"\"\n\n    @wraps(func)\n    def errfunc(*args, **kwargs):\n        raise NotImplementedError\n\n    if hexversion < 0x02070000:\n        return errfunc\n    else:\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        return self.__class__(self._key, self._load, self._iteritems())", "response": "Return a shallow copy of the sorted dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the entry with the given key from the dictionary. If the key is not in the dictionary a KeyError is raised. If the default is given a KeyError is raised.", "response": "def pop(self, key, default=_NotGiven):\n        \"\"\"\n        If *key* is in the dictionary, remove it and return its value,\n        else return *default*. If *default* is not given and *key* is not in\n        the dictionary, a KeyError is raised.\n        \"\"\"\n        if key in self:\n            self._list_remove(key)\n            return self._pop(key)\n        else:\n            if default is _NotGiven:\n                raise KeyError(key)\n            else:\n                return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the value of the key to the given default value. If the key is not in the dictionary insert it with a value of default. If the key is not in the dictionary insert it with a value of None.", "response": "def setdefault(self, key, default=None):\n        \"\"\"\n        If *key* is in the dictionary, return its value.  If not, insert *key*\n        with a value of *default* and return *default*.  *default* defaults to\n        ``None``.\n        \"\"\"\n        if key in self:\n            return self[key]\n        else:\n            self._setitem(key, default)\n            self._list_add(key)\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the index of the key value in the keyview.", "response": "def index(self, value, start=None, stop=None):\n        \"\"\"\n        Return the smallest *k* such that `keysview[k] == value` and `start <= k\n        < end`.  Raises `KeyError` if *value* is not present.  *stop* defaults\n        to the end of the set.  *start* defaults to the beginning.  Negative\n        indexes are supported, as for slice indices.\n        \"\"\"\n        return self._list.index(value, start, stop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_summary(self):\n        if not self.ignore_self:\n            res = summary.summarize(muppy.get_objects())\n        else:\n            # If the user requested the data required to store summaries to be\n            # ignored in the summaries, we need to identify all objects which\n            # are related to each summary stored.\n            # Thus we build a list of all objects used for summary storage as\n            # well as a dictionary which tells us how often an object is\n            # referenced by the summaries.\n            # During this identification process, more objects are referenced,\n            # namely int objects identifying referenced objects as well as the\n            # correspondind count.\n            # For all these objects it will be checked wether they are\n            # referenced from outside the monitor's scope. If not, they will be\n            # subtracted from the snapshot summary, otherwise they are\n            # included (as this indicates that they are relevant to the\n            # application).\n\n            all_of_them = []  # every single object\n            ref_counter = {}  # how often it is referenced; (id(o), o) pairs\n            def store_info(o):\n                all_of_them.append(o)\n                if id(o) in ref_counter:\n                    ref_counter[id(o)] += 1\n                else:\n                    ref_counter[id(o)] = 1\n\n            # store infos on every single object related to the summaries\n            store_info(self.summaries)\n            for k, v in self.summaries.items():\n                store_info(k)\n                summary._traverse(v, store_info)\n\n            # do the summary\n            res = summary.summarize(muppy.get_objects())\n\n            # remove ids stored in the ref_counter\n            for _id in ref_counter:\n                # referenced in frame, ref_counter, ref_counter.keys()\n                if len(gc.get_referrers(_id)) == (3):\n                    summary._subtract(res, _id)\n            for o in all_of_them:\n                # referenced in frame, summary, all_of_them\n                if len(gc.get_referrers(o)) == (ref_counter[id(o)] + 2):\n                    summary._subtract(res, o)\n\n        return res", "response": "Create a summary for the object which is referenced by the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the diff between two summaries.", "response": "def diff(self, summary1=None, summary2=None):\n        \"\"\"Compute diff between to summaries.\n\n        If no summary is provided, the diff from the last to the current\n        summary is used. If summary1 is provided the diff from summary1\n        to the current summary is used. If summary1 and summary2 are\n        provided, the diff between these two is used.\n\n        \"\"\"\n        res = None\n        if summary2 is None:\n            self.s1 = self.create_summary()\n            if summary1 is None:\n                res = summary.get_diff(self.s0, self.s1)\n            else:\n                res = summary.get_diff(summary1, self.s1)\n            self.s0 = self.s1\n        else:\n            if summary1 is not None:\n                res = summary.get_diff(summary1, summary2)\n            else:\n                raise ValueError(\"You cannot provide summary2 without summary1.\")\n        return summary._sweep(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_diff(self, summary1=None, summary2=None):\n        summary.print_(self.diff(summary1=summary1, summary2=summary2))", "response": "Compute diff between to summaries and print it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_objects(self, ignore=[]):\n        def remove_ignore(objects, ignore=[]):\n            # remove all objects listed in the ignore list\n            res = []\n            for o in objects:\n                if not compat.object_in_list(o, ignore):\n                    res.append(o)\n            return res\n\n        tmp = gc.get_objects()\n        ignore.append(inspect.currentframe()) #PYCHOK change ignore\n        ignore.append(self) #PYCHOK change ignore\n        if hasattr(self, 'o0'): ignore.append(self.o0) #PYCHOK change ignore\n        if hasattr(self, 'o1'): ignore.append(self.o1) #PYCHOK change ignore\n        ignore.append(ignore) #PYCHOK change ignore\n        ignore.append(remove_ignore) #PYCHOK change ignore\n        # this implies that referenced objects are also ignored\n        tmp = remove_ignore(tmp, ignore)\n        res = []\n        for o in tmp:\n            # gc.get_objects returns only container objects, but we also want\n            # the objects referenced by them\n            refs = muppy.get_referents(o)\n            for ref in refs:\n                if not muppy._is_containerobject(ref):\n                    # we already got the container objects, now we only add\n                    # non-container objects\n                    res.append(ref)\n        res.extend(tmp)\n        res = muppy._remove_duplicates(res)\n        if ignore is not None:\n            # repeat to filter out objects which may have been referenced\n            res = remove_ignore(res, ignore)\n        # manual cleanup, see comment above\n        del ignore[:]\n        return res", "response": "Get all currently existing objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_diff(self, ignore=[]):\n        # ignore this and the caller frame\n        ignore.append(inspect.currentframe()) #PYCHOK change ignore\n        self.o1 = self._get_objects(ignore)\n        diff = muppy.get_diff(self.o0, self.o1)\n        self.o0 = self.o1\n        # manual cleanup, see comment above\n        del ignore[:] #PYCHOK change ignore\n        return diff", "response": "Get the diff to the last time the state of objects was measured."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the diff to the last time the state of objects was measured.", "response": "def print_diff(self, ignore=[]):\n        \"\"\"Print the diff to the last time the state of objects was measured.\n\n        keyword arguments\n        ignore -- list of objects to ignore\n        \"\"\"\n        # ignore this and the caller frame\n        ignore.append(inspect.currentframe()) #PYCHOK change ignore\n        diff = self.get_diff(ignore)\n        print(\"Added objects:\")\n        summary.print_(summary.summarize(diff['+']))\n        print(\"Removed objects:\")\n        summary.print_(summary.summarize(diff['-']))\n        # manual cleanup, see comment above\n        del ignore[:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hamming(seq1, seq2, normalized=False):\n\tL = len(seq1)\n\tif L != len(seq2):\n\t\traise ValueError(\"expected two strings of the same length\")\n\tif L == 0:\n\t\treturn 0.0 if normalized else 0  # equal\n\tdist = sum(c1 != c2 for c1, c2 in zip(seq1, seq2))\n\tif normalized:\n\t\treturn dist / float(L)\n\treturn dist", "response": "Compute the Hamming distance between the two sequences seq1 and seq2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the Jaccard distance between the two sequences seq1 and seq2.", "response": "def jaccard(seq1, seq2):\n\t\"\"\"Compute the Jaccard distance between the two sequences `seq1` and `seq2`.\n\tThey should contain hashable items.\n\t\n\tThe return value is a float between 0 and 1, where 0 means equal, and 1 totally different.\n\t\"\"\"\n\tset1, set2 = set(seq1), set(seq2)\n\treturn 1 - len(set1 & set2) / float(len(set1 | set2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sorensen(seq1, seq2):\n\tset1, set2 = set(seq1), set(seq2)\n\treturn 1 - (2 * len(set1 & set2) / float(len(set1) + len(set2)))", "response": "Compute the Sorensen distance between the two sequences seq1 and seq2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lcsubstrings(seq1, seq2, positions=False):\n\tL1, L2 = len(seq1), len(seq2)\n\tms = []\n\tmlen = last = 0\n\tif L1 < L2:\n\t\tseq1, seq2 = seq2, seq1\n\t\tL1, L2 = L2, L1\n\t\n\tcolumn = array('L', range(L2))\n\t\n\tfor i in range(L1):\n\t\tfor j in range(L2):\n\t\t\told = column[j]\n\t\t\tif seq1[i] == seq2[j]:\n\t\t\t\tif i == 0 or j == 0:\n\t\t\t\t\tcolumn[j] = 1\n\t\t\t\telse:\n\t\t\t\t\tcolumn[j] = last + 1\n\t\t\t\tif column[j] > mlen:\n\t\t\t\t\tmlen = column[j]\n\t\t\t\t\tms = [(i, j)]\n\t\t\t\telif column[j] == mlen:\n\t\t\t\t\tms.append((i, j))\n\t\t\telse:\n\t\t\t\tcolumn[j] = 0\n\t\t\tlast = old\n\t\n\tif positions:\n\t\treturn (mlen, tuple((i - mlen + 1, j - mlen + 1) for i, j in ms if ms))\n\treturn set(seq1[i - mlen + 1:i + 1] for i, _ in ms if ms)", "response": "Find the longest common substring in the sequences seq1 and seq2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef background_color(self, node, depth):\n        if self.color_mapping is None:\n            self.color_mapping = {}\n        color = self.color_mapping.get(node.key)\n        if color is None:\n            depth = len(self.color_mapping)\n            red = (depth * 10) % 255\n            green = 200 - ((depth * 5) % 200)\n            blue = (depth * 25) % 200\n            self.color_mapping[node.key] = color = wx.Colour(red, green, blue)\n        return color", "response": "Create a color for each node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetPercentage(self, percent, total):\n        self.percentageView = percent\n        self.total = total", "response": "Set whether to display percentage values and total for doing so."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runprofilerandshow(funcname, profilepath, argv='', *args, **kwargs):\n    '''\n    Run a functions profiler and show it in a GUI visualisation using RunSnakeRun\n    Note: can also use calibration for more exact results\n    '''\n    functionprofiler.runprofile(funcname+'(\\''+argv+'\\')', profilepath, *args, **kwargs)\n    print 'Showing profile (windows should open in the background)'; sys.stdout.flush();\n    functionprofiler.browseprofilegui(profilepath)", "response": "Run a functions profiler and show it in a GUI visualisation using RunSnakeRun\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a call graph Note: be sure to install GraphViz prior to printing the dot graph!", "response": "def callgraph(func):\n    ''' Makes a call graph\n    Note: be sure to install GraphViz prior to printing the dot graph!\n    '''\n    import pycallgraph\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        pycallgraph.start_trace()\n        func(*args, **kwargs)\n        pycallgraph.save_dot('callgraph.log')\n        pycallgraph.make_dot_graph('callgraph.png')\n        #pycallgraph.make_dot_graph('callgraph.jpg', format='jpg', tool='neato')\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a long integer to a byte string.", "response": "def _long2bytes(n, blocksize=0):\r\n    \"\"\"Convert a long integer to a byte string.\r\n\r\n    If optional blocksize is given and greater than zero, pad the front\r\n    of the byte string with binary zeros so that the length is a multiple\r\n    of blocksize.\r\n    \"\"\"\r\n\r\n    # After much testing, this algorithm was deemed to be the fastest.\r\n    s = ''\r\n    pack = struct.pack\r\n    while n > 0:\r\n        ### CHANGED FROM '>I' TO '<I'. (DCG)\r\n        s = pack('<I', n & 0xffffffffL) + s\r\n        ### --------------------------\r\n        n = n >> 32\r\n\r\n    # Strip off leading zeros.\r\n    for i in range(len(s)):\r\n        if s[i] <> '\\000':\r\n            break\r\n    else:\r\n        # Only happens when n == 0.\r\n        s = '\\000'\r\n        i = 0\r\n\r\n    s = s[i:]\r\n\r\n    # Add back some pad bytes. This could be done more efficiently\r\n    # w.r.t. the de-padding being done above, but sigh...\r\n    if blocksize > 0 and len(s) % blocksize:\r\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\r\n\r\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for calling functions F G H and I from Appl. Crypto.", "response": "def XX(func, a, b, c, d, x, s, ac):\r\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\r\n\r\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\r\n    Rotation is separate from addition to prevent recomputation\r\n    (now summed-up in one function).\r\n    \"\"\"\r\n\r\n    res = 0L\r\n    res = res + a + func(b, c, d)\r\n    res = res + x \r\n    res = res + ac\r\n    res = res & 0xffffffffL\r\n    res = _rotateLeft(res, s)\r\n    res = res & 0xffffffffL\r\n    res = res + b\r\n\r\n    return res & 0xffffffffL"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(self):\r\n        \"Initialize the message-digest and set all fields to zero.\"\r\n\r\n        self.length = 0L\r\n        self.input = []\r\n\r\n        # Load magic initialization constants.\r\n        self.A = 0x67452301L\r\n        self.B = 0xefcdab89L\r\n        self.C = 0x98badcfeL\r\n        self.D = 0x10325476L", "response": "Initialize the message - digest and set all fields to zero."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming the input array into a new array of the same size.", "response": "def _transform(self, inp):\r\n        \"\"\"Basic MD5 step transforming the digest based on the input.\r\n\r\n        Note that if the Mysterious Constants are arranged backwards\r\n        in little-endian order and decrypted with the DES they produce\r\n        OCCULT MESSAGES!\r\n        \"\"\"\r\n\r\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\r\n\r\n        # Round 1.\r\n\r\n        S11, S12, S13, S14 = 7, 12, 17, 22\r\n\r\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478L) # 1 \r\n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756L) # 2 \r\n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DBL) # 3 \r\n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEEL) # 4 \r\n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAFL) # 5 \r\n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62AL) # 6 \r\n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613L) # 7 \r\n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501L) # 8 \r\n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8L) # 9 \r\n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AFL) # 10 \r\n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1L) # 11 \r\n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BEL) # 12 \r\n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122L) # 13 \r\n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193L) # 14 \r\n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438EL) # 15 \r\n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821L) # 16 \r\n\r\n        # Round 2.\r\n\r\n        S21, S22, S23, S24 = 5, 9, 14, 20\r\n\r\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562L) # 17 \r\n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340L) # 18 \r\n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51L) # 19 \r\n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AAL) # 20 \r\n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105DL) # 21 \r\n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453L) # 22 \r\n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681L) # 23 \r\n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8L) # 24 \r\n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6L) # 25 \r\n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6L) # 26 \r\n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87L) # 27 \r\n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14EDL) # 28 \r\n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905L) # 29 \r\n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8L) # 30 \r\n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9L) # 31 \r\n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8AL) # 32 \r\n\r\n        # Round 3.\r\n\r\n        S31, S32, S33, S34 = 4, 11, 16, 23\r\n\r\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942L) # 33 \r\n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681L) # 34 \r\n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122L) # 35 \r\n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380CL) # 36 \r\n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44L) # 37 \r\n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9L) # 38 \r\n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60L) # 39 \r\n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70L) # 40 \r\n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6L) # 41 \r\n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FAL) # 42 \r\n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085L) # 43 \r\n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05L) # 44 \r\n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039L) # 45 \r\n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5L) # 46 \r\n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8L) # 47 \r\n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665L) # 48 \r\n\r\n        # Round 4.\r\n\r\n        S41, S42, S43, S44 = 6, 10, 15, 21\r\n\r\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244L) # 49 \r\n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97L) # 50 \r\n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7L) # 51 \r\n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039L) # 52 \r\n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3L) # 53 \r\n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92L) # 54 \r\n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47DL) # 55 \r\n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1L) # 56 \r\n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4FL) # 57 \r\n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0L) # 58 \r\n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314L) # 59 \r\n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1L) # 60 \r\n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82L) # 61 \r\n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235L) # 62 \r\n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BBL) # 63 \r\n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391L) # 64 \r\n\r\n        A = (A + a) & 0xffffffffL\r\n        B = (B + b) & 0xffffffffL\r\n        C = (C + c) & 0xffffffffL\r\n        D = (D + d) & 0xffffffffL\r\n\r\n        self.A, self.B, self.C, self.D = A, B, C, D"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, inBuf):\r\n        leninBuf = long(len(inBuf))\r\n\r\n        # Compute number of bytes mod 64.\r\n        index = (self.count[0] >> 3) & 0x3FL\r\n\r\n        # Update number of bits.\r\n        self.count[0] = self.count[0] + (leninBuf << 3)\r\n        if self.count[0] < (leninBuf << 3):\r\n            self.count[1] = self.count[1] + 1\r\n        self.count[1] = self.count[1] + (leninBuf >> 29)\r\n\r\n        partLen = 64 - index\r\n\r\n        if leninBuf >= partLen:\r\n            self.input[index:] = map(None, inBuf[:partLen])\r\n            self._transform(_bytelist2long(self.input))\r\n            i = partLen\r\n            while i + 63 < leninBuf:\r\n                self._transform(_bytelist2long(map(None, inBuf[i:i+64])))\r\n                i = i + 64\r\n            else:\r\n                self.input = map(None, inBuf[i:leninBuf])\r\n        else:\r\n            i = 0\r\n            self.input = self.input + map(None, inBuf)", "response": "Add to the current message and update the md5 object with the string arg."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nterminates the message - digest computation and return digest.", "response": "def digest(self):\r\n        \"\"\"Terminate the message-digest computation and return digest.\r\n\r\n        Return the digest of the strings passed to the update()\r\n        method so far. This is a 16-byte string which may contain\r\n        non-ASCII characters, including null bytes.\r\n        \"\"\"\r\n\r\n        A = self.A\r\n        B = self.B\r\n        C = self.C\r\n        D = self.D\r\n        input = [] + self.input\r\n        count = [] + self.count\r\n\r\n        index = (self.count[0] >> 3) & 0x3fL\r\n\r\n        if index < 56:\r\n            padLen = 56 - index\r\n        else:\r\n            padLen = 120 - index\r\n\r\n        padding = ['\\200'] + ['\\000'] * 63\r\n        self.update(padding[:padLen])\r\n\r\n        # Append length (before padding).\r\n        bits = _bytelist2long(self.input[:56]) + count\r\n\r\n        self._transform(bits)\r\n\r\n        # Store state in digest.\r\n        digest = _long2bytes(self.A << 96, 16)[:4] + \\\r\n                 _long2bytes(self.B << 64, 16)[4:8] + \\\r\n                 _long2bytes(self.C << 32, 16)[8:12] + \\\r\n                 _long2bytes(self.D, 16)[12:]\r\n\r\n        self.A = A \r\n        self.B = B\r\n        self.C = C\r\n        self.D = D\r\n        self.input = input \r\n        self.count = count \r\n\r\n        return digest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hexdigest(self):\r\n\r\n        d = map(None, self.digest())\r\n        d = map(ord, d)\r\n        d = map(lambda x:\"%02x\" % x, d)\r\n        d = string.join(d, '')\r\n\r\n        return d", "response": "Terminate and return digest in HEX form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning value used to compare size of this node", "response": "def value( self, node, parent=None ):\n        \"\"\"Return value used to compare size of this node\"\"\"\n        # this is the *weighted* size/contribution of the node \n        try:\n            return node['contribution']\n        except KeyError, err:\n            contribution = int(node.get('totsize',0)/float( len(node.get('parents',())) or 1))\n            node['contribution'] = contribution\n            return contribution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label( self, node ):\n        result = []\n        if node.get('type'):\n            result.append( node['type'] )\n        if node.get('name' ):\n            result.append( node['name'] )\n        elif node.get('value') is not None:\n            result.append( unicode(node['value'])[:32])\n        if 'module' in node and not node['module'] in result:\n            result.append( ' in %s'%( node['module'] ))\n        if node.get( 'size' ):\n            result.append( '%s'%( mb( node['size'] )))\n        if node.get( 'totsize' ):\n            result.append( '(%s)'%( mb( node['totsize'] )))\n        parent_count = len( node.get('parents',()))\n        if parent_count > 1:\n            result.append( '/%s refs'%( parent_count ))\n        return \" \".join(result)", "response": "Return textual description of this node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the set of parents for the given node", "response": "def parents( self, node ):\n        \"\"\"Retrieve/calculate the set of parents for the given node\"\"\"\n        if 'index' in node:\n            index = node['index']()\n            parents = list(meliaeloader.children( node, index, 'parents' ))\n            return parents \n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef best_parent( self, node, tree_type=None ):\n        parents = self.parents(node)\n        selected_parent = None\n        if node['type'] == 'type':\n            module = \".\".join( node['name'].split( '.' )[:-1] )\n            if module:\n                for mod in parents:\n                    if mod['type'] == 'module' and mod['name'] == module:\n                        selected_parent = mod \n        if parents and selected_parent is None:\n            parents.sort( key = lambda x: self.value(node, x) )\n            return parents[-1]\n        return selected_parent", "response": "Choose the best parent for a given node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Gooey(f=None,\n          advanced=True,\n          language='english',\n          show_config=True,\n          program_name=None,\n          program_description=None,\n          default_size=(610, 530),\n          required_cols=2,\n          optional_cols=2,\n          dump_build_config=False,\n          monospace_display=False):\n  '''\n  Decorator for client code's main function.\n  Serializes argparse data to JSON for use with the Gooey front end\n  '''\n\n  params = locals()\n\n  def build(payload):\n    def run_gooey(self, args=None, namespace=None):\n      source_path = sys.argv[0]\n      build_spec = config_generator.create_from_parser(self, source_path, payload_name=payload.__name__, **params)\n\n      if dump_build_config:\n        config_path = os.path.join(os.getcwd(), 'gooey_config.json')\n        print( 'Writing Build Config to: {}'.format(config_path))\n        with open(config_path, 'w') as f:\n          f.write(json.dumps(build_spec, indent=2))\n      application.run(build_spec)\n\n    def inner2(*args, **kwargs):\n      ArgumentParser.original_parse_args = ArgumentParser.parse_args\n      ArgumentParser.parse_args = run_gooey\n      return payload(*args, **kwargs)\n\n    inner2.__name__ = payload.__name__\n    return inner2\n\n  def run_without_gooey(func):\n    return lambda: func()\n\n  if IGNORE_COMMAND in sys.argv:\n    sys.argv.remove(IGNORE_COMMAND)\n    if callable(f):\n      return run_without_gooey(f)\n    return run_without_gooey\n\n  if callable(f):\n    return build(f)\n  return build", "response": "A function that returns a Gooey client code s main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a list of wx Component objects from a Json widget list", "response": "def build_components(widget_list):\n  '''\n  :param widget_list: list of dicts containing widget info (name, type, etc..)\n  :return: ComponentList\n\n  Converts the Json widget information into concrete wx Widget types\n  '''\n  required_args, optional_args  = partition(widget_list, is_required)\n  checkbox_args, general_args = partition(map(build_widget, optional_args), is_checkbox)\n\n  required_args = map(build_widget, required_args)\n  optional_args = general_args + checkbox_args\n\n  return ComponentList(required_args, optional_args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _merge_asized(base, other, level=0):\n    ref2key = lambda ref: ref.name.split(':')[0]\n    base.size += other.size\n    base.flat += other.flat\n    if level > 0:\n        base.name = ref2key(base)\n    # Add refs from other to base. Any new refs are appended.\n    base.refs = list(base.refs) # we may need to append items\n    refs = {}\n    for ref in base.refs:\n        refs[ref2key(ref)] = ref\n    for ref in other.refs:\n        key = ref2key(ref)\n        if key in refs:\n            _merge_asized(refs[key], ref, level=level+1)\n        else:\n            # Don't modify existing Asized instances => deepcopy\n            base.refs.append(deepcopy(ref))\n            base.refs[-1].name = key", "response": "Merge two Asized instances into base."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge the snapshot size information of multiple tracked objects at time tref into merged.", "response": "def _merge_objects(tref, merged, obj):\n    \"\"\"\n    Merge the snapshot size information of multiple tracked objects.  The\n    tracked object `obj` is scanned for size information at time `tref`.\n    The sizes are merged into **Asized** instance `merged`.\n    \"\"\"\n    size = None\n    for (timestamp, tsize) in obj.snapshots:\n        if timestamp == tref:\n            size = tsize\n    if size:\n        _merge_asized(merged, size)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the stack - trace to a nice readable format.", "response": "def _format_trace(trace):\n    \"\"\"\n    Convert the (stripped) stack-trace to a nice readable format. The stack\n    trace `trace` is a list of frame records as returned by\n    **inspect.stack** but without the frame objects.\n    Returns a string.\n    \"\"\"\n    lines = []\n    for fname, lineno, func, src, _ in trace:\n        if src:\n            for line in src:\n                lines.append('    '+line.strip()+'\\n')\n        lines.append('  %s:%4d in %s\\n' % (fname, lineno, func))\n    return ''.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_stats(self, fdump):\n        if isinstance(fdump, type('')):\n            fdump = open(fdump, 'rb')\n        self.index = pickle.load(fdump)\n        self.snapshots = pickle.load(fdump)\n        self.sorted = []", "response": "Load the data from a dump file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump_stats(self, fdump, close=True):\n        if self.tracker:\n            self.tracker.stop_periodic_snapshots()\n\n        if isinstance(fdump, type('')):\n            fdump = open(fdump, 'wb')\n        pickle.dump(self.index, fdump, protocol=pickle.HIGHEST_PROTOCOL)\n        pickle.dump(self.snapshots, fdump, protocol=pickle.HIGHEST_PROTOCOL)\n        if close:\n            fdump.close()", "response": "Dump the logged data to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_sort(self):\n        if not self.sorted:\n            # Identify the snapshot that tracked the largest amount of memory.\n            tmax = None\n            maxsize = 0\n            for snapshot in self.snapshots:\n                if snapshot.tracked_total > maxsize:\n                    tmax = snapshot.timestamp\n            for key in list(self.index.keys()):\n                for tobj in self.index[key]:\n                    tobj.classname = key\n                    tobj.size = tobj.get_max_size()\n                    tobj.tsize = tobj.get_size_at_time(tmax)\n                self.sorted.extend(self.index[key])", "response": "Initialize the data to be sorted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting the tracked objects according to the supplied criteria.", "response": "def sort_stats(self, *args):\n        \"\"\"\n        Sort the tracked objects according to the supplied criteria. The\n        argument is a string identifying the basis of a sort (example: 'size'\n        or 'classname'). When more than one key is provided, then additional\n        keys are used as secondary criteria when there is equality in all keys\n        selected before them. For example, ``sort_stats('name', 'size')`` will\n        sort all the entries according to their class name, and resolve all\n        ties (identical class names) by sorting by size.  The criteria are\n        fields in the tracked object instances. Results are stored in the\n        ``self.sorted`` list which is used by ``Stats.print_stats()`` and other\n        methods. The fields available for sorting are:\n\n            'classname'\n                the name with which the class was registered\n            'name'\n                the classname\n            'birth'\n                creation timestamp\n            'death'\n                destruction timestamp\n            'size'\n                the maximum measured size of the object\n            'tsize'\n                the measured size during the largest snapshot\n            'repr'\n                string representation of the object\n\n        Note that sorts on size are in descending order (placing most memory\n        consuming items first), whereas name, repr, and creation time searches\n        are in ascending order (alphabetical).\n\n        The function returns self to allow calling functions on the result::\n\n            stats.sort_stats('size').reverse_order().print_stats()\n        \"\"\"\n\n        criteria = ('classname', 'tsize', 'birth', 'death',\n                    'name', 'repr', 'size')\n\n        if not set(criteria).issuperset(set(args)):\n            raise ValueError(\"Invalid sort criteria\")\n\n        if not args:\n            args = criteria\n\n        def args_to_tuple(obj):\n            keys = []\n            for attr in args:\n                attribute = getattr(obj, attr)\n                if attr in ('tsize', 'size'):\n                    attribute = -attribute\n                keys.append(attribute)\n            return tuple(keys)\n\n        self._init_sort()\n        self.sorted.sort(key=args_to_tuple)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef annotate_snapshot(self, snapshot):\n        if hasattr(snapshot, 'classes'):\n            return\n\n        snapshot.classes = {}\n\n        for classname in list(self.index.keys()):\n            total = 0\n            active = 0\n            merged = Asized(0, 0)\n            for tobj in self.index[classname]:\n                _merge_objects(snapshot.timestamp, merged, tobj)\n                total += tobj.get_size_at_time(snapshot.timestamp)\n                if tobj.birth < snapshot.timestamp and \\\n                    (tobj.death is None or tobj.death > snapshot.timestamp):\n                    active += 1\n\n            try:\n                pct = total * 100.0 / snapshot.total\n            except ZeroDivisionError: # pragma: no cover\n                pct = 0\n            try:\n                avg = total / active\n            except ZeroDivisionError:\n                avg = 0\n\n            snapshot.classes[classname] = dict(sum=total,\n                                               avg=avg,\n                                               pct=pct,\n                                               active=active)\n            snapshot.classes[classname]['merged'] = merged", "response": "Annotate a snapshot with the statistical data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _print_refs(self, refs, total, prefix='    ',\n                    level=1, minsize=0, minpct=0.1):\n        \"\"\"\n        Print individual referents recursively.\n        \"\"\"\n        lrefs = list(refs)\n        lrefs.sort(key=lambda x: x.size)\n        lrefs.reverse()\n        for ref in lrefs:\n            if ref.size > minsize and (ref.size*100.0/total) > minpct:\n                self.stream.write('%-50s %-14s %3d%% [%d]\\n' % (\n                    trunc(prefix+str(ref.name), 50),\n                    pp(ref.size),\n                    int(ref.size*100.0/total),\n                    level\n                ))\n                self._print_refs(ref.refs, total, prefix=prefix+'  ',\n                                 level=level+1)", "response": "Print all the referents in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_object(self, tobj):\n        if tobj.death:\n            self.stream.write('%-32s ( free )   %-35s\\n' % (\n                trunc(tobj.name, 32, left=1), trunc(tobj.repr, 35)))\n        else:\n            self.stream.write('%-32s 0x%08x %-35s\\n' % (\n                trunc(tobj.name, 32, left=1),\n                tobj.id,\n                trunc(tobj.repr, 35)\n            ))\n        if tobj.trace:\n            self.stream.write(_format_trace(tobj.trace))\n        for (timestamp, size) in tobj.snapshots:\n            self.stream.write('  %-30s %s\\n' % (\n                pp_timestamp(timestamp), pp(size.size)\n            ))\n            self._print_refs(size.refs, size.size)\n        if tobj.death is not None:\n            self.stream.write('  %-30s finalize\\n' % (\n                pp_timestamp(tobj.death),\n            ))", "response": "Print the gathered information of the object tobj in human - readable format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite tracked objects to stdout.", "response": "def print_stats(self, clsname=None, limit=1.0):\n        \"\"\"\n        Write tracked objects to stdout.  The output can be filtered and\n        pruned.  Only objects are printed whose classname contain the substring\n        supplied by the `clsname` argument.  The output can be pruned by\n        passing a `limit` value.\n\n        :param clsname: Only print objects whose classname contain the given\n            substring.\n        :param limit: If `limit` is a float smaller than one, only the supplied\n            percentage of the total tracked data is printed. If `limit` is\n            bigger than one, this number of tracked objects are printed.\n            Tracked objects are first filtered, and then pruned (if specified).\n        \"\"\"\n        if self.tracker:\n            self.tracker.stop_periodic_snapshots()\n\n        if not self.sorted:\n            self.sort_stats()\n\n        _sorted = self.sorted\n\n        if clsname:\n            _sorted = [to for to in _sorted if clsname in to.classname]\n\n        if limit < 1.0:\n            limit = max(1, int(len(self.sorted) * limit))\n        _sorted = _sorted[:int(limit)]\n\n        # Emit per-instance data\n        for tobj in _sorted:\n            self.print_object(tobj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints summary for each class in the current instance.", "response": "def print_summary(self):\n        \"\"\"\n        Print per-class summary for each snapshot.\n        \"\"\"\n        # Emit class summaries for each snapshot\n        classlist = self.tracked_classes\n\n        fobj = self.stream\n\n        fobj.write('---- SUMMARY '+'-'*66+'\\n')\n        for snapshot in self.snapshots:\n            self.annotate_snapshot(snapshot)\n            fobj.write('%-35s %11s %12s %12s %5s\\n' % (\n                trunc(snapshot.desc, 35),\n                'active',\n                pp(snapshot.asizeof_total),\n                'average',\n                'pct'\n            ))\n            for classname in classlist:\n                info = snapshot.classes.get(classname)\n                fobj.write('  %-33s %11d %12s %12s %4d%%\\n' % (\n                    trunc(classname, 33),\n                    info['active'],\n                    pp(info['sum']),\n                    pp(info['avg']),\n                    info['pct']\n                ))\n        fobj.write('-'*79+'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint all the referents in the tree.", "response": "def _print_refs(self, fobj, refs, total, level=1, minsize=0, minpct=0.1):\n        \"\"\"\n        Print individual referents recursively.\n        \"\"\"\n        lrefs = list(refs)\n        lrefs.sort(key=lambda x: x.size)\n        lrefs.reverse()\n        if level == 1:\n            fobj.write('<table>\\n')\n        for ref in lrefs:\n            if ref.size > minsize and (ref.size*100.0/total) > minpct:\n                data = dict(level=level,\n                            name=trunc(str(ref.name), 128),\n                            size=pp(ref.size),\n                            pct=ref.size*100.0/total)\n                fobj.write(self.refrow % data)\n                self._print_refs(fobj, ref.refs, total, level=level+1)\n        if level == 1:\n            fobj.write(\"</table>\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_class_details(self, fname, classname):\n        fobj = open(fname, \"w\")\n        fobj.write(self.header % (classname, self.style))\n\n        fobj.write(\"<h1>%s</h1>\\n\" % (classname))\n\n        sizes = [tobj.get_max_size() for tobj in self.index[classname]]\n        total = 0\n        for s in sizes:\n            total += s\n        data = {'cnt': len(self.index[classname]), 'cls': classname}\n        data['avg'] = pp(total / len(sizes))\n        data['max'] = pp(max(sizes))\n        data['min'] = pp(min(sizes))\n        fobj.write(self.class_summary % data)\n\n        fobj.write(self.charts[classname])\n\n        fobj.write(\"<h2>Coalesced Referents per Snapshot</h2>\\n\")\n        for snapshot in self.snapshots:\n            if classname in snapshot.classes:\n                merged = snapshot.classes[classname]['merged']\n                fobj.write(self.class_snapshot % {\n                    'name': snapshot.desc, 'cls':classname, 'total': pp(merged.size)\n                })\n                if merged.refs:\n                    self._print_refs(fobj, merged.refs, merged.size)\n                else:\n                    fobj.write('<p>No per-referent sizes recorded.</p>\\n')\n\n        fobj.write(\"<h2>Instances</h2>\\n\")\n        for tobj in self.index[classname]:\n            fobj.write('<table id=\"tl\" width=\"100%\" rules=\"rows\">\\n')\n            fobj.write('<tr><td id=\"hl\" width=\"140px\">Instance</td><td id=\"hl\">%s at 0x%08x</td></tr>\\n' % (tobj.name, tobj.id))\n            if tobj.repr:\n                fobj.write(\"<tr><td>Representation</td><td>%s&nbsp;</td></tr>\\n\" % tobj.repr)\n            fobj.write(\"<tr><td>Lifetime</td><td>%s - %s</td></tr>\\n\" % (pp_timestamp(tobj.birth), pp_timestamp(tobj.death)))\n            if tobj.trace:\n                trace = \"<pre>%s</pre>\" % (_format_trace(tobj.trace))\n                fobj.write(\"<tr><td>Instantiation</td><td>%s</td></tr>\\n\" % trace)\n            for (timestamp, size) in tobj.snapshots:\n                fobj.write(\"<tr><td>%s</td>\" % pp_timestamp(timestamp))\n                if not size.refs:\n                    fobj.write(\"<td>%s</td></tr>\\n\" % pp(size.size))\n                else:\n                    fobj.write(\"<td>%s\" % pp(size.size))\n                    self._print_refs(fobj, size.refs, size.size)\n                    fobj.write(\"</td></tr>\\n\")\n            fobj.write(\"</table>\\n\")\n\n        fobj.write(self.footer)\n        fobj.close()", "response": "Print detailed statistics and instances for the class classname."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the title page.", "response": "def create_title_page(self, filename, title=''):\n        \"\"\"\n        Output the title page.\n        \"\"\"\n        fobj = open(filename, \"w\")\n        fobj.write(self.header % (title, self.style))\n\n        fobj.write(\"<h1>%s</h1>\\n\" % title)\n        fobj.write(\"<h2>Memory distribution over time</h2>\\n\")\n        fobj.write(self.charts['snapshots'])\n\n        fobj.write(\"<h2>Snapshots statistics</h2>\\n\")\n        fobj.write('<table id=\"nb\">\\n')\n\n        classlist = list(self.index.keys())\n        classlist.sort()\n\n        for snapshot in self.snapshots:\n            fobj.write('<tr><td>\\n')\n            fobj.write('<table id=\"tl\" rules=\"rows\">\\n')\n            fobj.write(\"<h3>%s snapshot at %s</h3>\\n\" % (\n                snapshot.desc or 'Untitled',\n                pp_timestamp(snapshot.timestamp)\n            ))\n\n            data = {}\n            data['sys']      = pp(snapshot.system_total.vsz)\n            data['tracked']  = pp(snapshot.tracked_total)\n            data['asizeof']  = pp(snapshot.asizeof_total)\n            data['overhead'] = pp(getattr(snapshot, 'overhead', 0))\n\n            fobj.write(self.snapshot_summary % data)\n\n            if snapshot.tracked_total:\n                fobj.write(self.snapshot_cls_header)\n                for classname in classlist:\n                    data = snapshot.classes[classname].copy()\n                    data['cls'] = '<a href=\"%s\">%s</a>' % (self.relative_path(self.links[classname]), classname)\n                    data['sum'] = pp(data['sum'])\n                    data['avg'] = pp(data['avg'])\n                    fobj.write(self.snapshot_cls % data)\n            fobj.write('</table>')\n            fobj.write('</td><td>\\n')\n            if snapshot.tracked_total:\n                fobj.write(self.charts[snapshot])\n            fobj.write('</td></tr>\\n')\n\n        fobj.write(\"</table>\\n\")\n        fobj.write(self.footer)\n        fobj.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a chart that depicts the lifetime of the instance registered with the given classname. The output is written to filename.", "response": "def create_lifetime_chart(self, classname, filename=''):\n        \"\"\"\n        Create chart that depicts the lifetime of the instance registered with\n        `classname`. The output is written to `filename`.\n        \"\"\"\n        try:\n            from pylab import figure, title, xlabel, ylabel, plot, savefig\n        except ImportError:\n            return HtmlStats.nopylab_msg % (classname+\" lifetime\")\n\n        cnt = []\n        for tobj in self.index[classname]:\n            cnt.append([tobj.birth, 1])\n            if tobj.death:\n                cnt.append([tobj.death, -1])\n        cnt.sort()\n        for i in range(1, len(cnt)):\n            cnt[i][1] += cnt[i-1][1]\n            #if cnt[i][0] == cnt[i-1][0]:\n            #    del cnt[i-1]\n\n        x = [t for [t,c] in cnt]\n        y = [c for [t,c] in cnt]\n\n        figure()\n        xlabel(\"Execution time [s]\")\n        ylabel(\"Instance #\")\n        title(\"%s instances\" % classname)\n        plot(x, y, 'o')\n        savefig(filename)\n\n        return self.chart_tag % (os.path.basename(filename))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_snapshot_chart(self, filename=''):\n        try:\n            from pylab import figure, title, xlabel, ylabel, plot, fill, legend, savefig\n            import matplotlib.mlab as mlab\n        except ImportError:\n            return self.nopylab_msg % (\"memory allocation\")\n\n        classlist = self.tracked_classes\n\n        times = [snapshot.timestamp for snapshot in self.snapshots]\n        base = [0] * len(self.snapshots)\n        poly_labels = []\n        polys = []\n        for cn in classlist:\n            pct = [snapshot.classes[cn]['pct'] for snapshot in self.snapshots]\n            if max(pct) > 3.0:\n                sz = [float(fp.classes[cn]['sum'])/(1024*1024) for fp in self.snapshots]\n                sz = [sx+sy for sx, sy in zip(base, sz)]\n                xp, yp = mlab.poly_between(times, base, sz)\n                polys.append( ((xp, yp), {'label': cn}) )\n                poly_labels.append(cn)\n                base = sz\n\n        figure()\n        title(\"Snapshot Memory\")\n        xlabel(\"Execution Time [s]\")\n        ylabel(\"Virtual Memory [MiB]\")\n\n        sizes = [float(fp.asizeof_total)/(1024*1024) for fp in self.snapshots]\n        plot(times, sizes, 'r--', label='Total')\n        sizes = [float(fp.tracked_total)/(1024*1024) for fp in self.snapshots]\n        plot(times, sizes, 'b--', label='Tracked total')\n\n        for (args, kwds) in polys:\n            fill(*args, **kwds)\n        legend(loc=2)\n        savefig(filename)\n\n        return self.chart_tag % (self.relative_path(filename))", "response": "Create a chart that depicts the memory allocation over time apportioned to\n            the tracked classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a pie chart that depicts the distribution of the allocated memory for a given snapshot.", "response": "def create_pie_chart(self, snapshot, filename=''):\n        \"\"\"\n        Create a pie chart that depicts the distribution of the allocated memory\n        for a given `snapshot`. The chart is saved to `filename`.\n        \"\"\"\n        try:\n            from pylab import figure, title, pie, axes, savefig\n            from pylab import sum as pylab_sum\n        except ImportError:\n            return self.nopylab_msg % (\"pie_chart\")\n\n        # Don't bother illustrating a pie without pieces.\n        if not snapshot.tracked_total:\n            return ''\n\n        classlist = []\n        sizelist = []\n        for k, v in list(snapshot.classes.items()):\n            if v['pct'] > 3.0:\n                classlist.append(k)\n                sizelist.append(v['sum'])\n        sizelist.insert(0, snapshot.asizeof_total - pylab_sum(sizelist))\n        classlist.insert(0, 'Other')\n        #sizelist = [x*0.01 for x in sizelist]\n\n        title(\"Snapshot (%s) Memory Distribution\" % (snapshot.desc))\n        figure(figsize=(8,8))\n        axes([0.1, 0.1, 0.8, 0.8])\n        pie(sizelist, labels=classlist)\n        savefig(filename, dpi=50)\n\n        return self.chart_tag % (self.relative_path(filename))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate HTML page and additional files in a directory derived from fname.", "response": "def create_html(self, fname, title=\"ClassTracker Statistics\"):\n        \"\"\"\n        Create HTML page `fname` and additional files in a directory derived\n        from `fname`.\n        \"\"\"\n        # Create a folder to store the charts and additional HTML files.\n        self.basedir = os.path.dirname(os.path.abspath(fname))\n        self.filesdir = os.path.splitext(fname)[0] + '_files'\n        if not os.path.isdir(self.filesdir):\n            os.mkdir(self.filesdir)\n        self.filesdir = os.path.abspath(self.filesdir)\n        self.links = {}\n\n        # Annotate all snapshots in advance\n        self.annotate()\n\n        # Create charts. The tags to show the images are returned and stored in\n        # the self.charts dictionary. This allows to return alternative text if\n        # the chart creation framework is not available.\n        self.charts = {}\n        fn = os.path.join(self.filesdir, 'timespace.png')\n        self.charts['snapshots'] = self.create_snapshot_chart(fn)\n\n        for fp, idx in zip(self.snapshots, list(range(len(self.snapshots)))):\n            fn = os.path.join(self.filesdir, 'fp%d.png' % (idx))\n            self.charts[fp] = self.create_pie_chart(fp, fn)\n\n        for cn in list(self.index.keys()):\n            fn = os.path.join(self.filesdir, cn.replace('.', '_')+'-lt.png')\n            self.charts[cn] = self.create_lifetime_chart(cn, fn)\n\n        # Create HTML pages first for each class and then the index page.\n        for cn in list(self.index.keys()):\n            fn = os.path.join(self.filesdir, cn.replace('.', '_')+'.html')\n            self.links[cn]  = fn\n            self.print_class_details(fn, cn)\n\n        self.create_title_page(fname, title=title)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_from(self, parent_path):\n        path_cls = type(parent_path)\n        is_dir = path_cls.is_dir\n        exists = path_cls.exists\n        listdir = parent_path._accessor.listdir\n        return self._select_from(parent_path, is_dir, exists, listdir)", "response": "Iterate over all child paths of parent_path matched by this\n        selector. This can contain parent_path itself."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relative_to(self, *other):\n        # For the purpose of this method, drive and root are considered\n        # separate parts, i.e.:\n        #   Path('c:/').relative_to('c:')  gives Path('/')\n        #   Path('c:/').relative_to('/')   raise ValueError\n        if not other:\n            raise TypeError(\"need at least one argument\")\n        parts = self._parts\n        drv = self._drv\n        root = self._root\n        if root:\n            abs_parts = [drv, root] + parts[1:]\n        else:\n            abs_parts = parts\n        to_drv, to_root, to_parts = self._parse_args(other)\n        if to_root:\n            to_abs_parts = [to_drv, to_root] + to_parts[1:]\n        else:\n            to_abs_parts = to_parts\n        n = len(to_abs_parts)\n        cf = self._flavour.casefold_parts\n        if (root or drv) if n == 0 else cf(abs_parts[:n]) != cf(to_abs_parts):\n            formatted = self._format_parsed_parts(to_drv, to_root, to_parts)\n            raise ValueError(\"{!r} does not start with {!r}\"\n                             .format(str(self), str(formatted)))\n        return self._from_parsed_parts('', root if n == 1 else '',\n                                       abs_parts[n:])", "response": "Return the relative path to another path identified by the passed\n        arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over this subtree and yield all existing files of any kind matching the given pattern.", "response": "def glob(self, pattern):\n        \"\"\"Iterate over this subtree and yield all existing files (of any\n        kind, including directories) matching the given pattern.\n        \"\"\"\n        pattern = self._flavour.casefold(pattern)\n        drv, root, pattern_parts = self._flavour.parse_parts((pattern,))\n        if drv or root:\n            raise NotImplementedError(\"Non-relative patterns are unsupported\")\n        selector = _make_selector(tuple(pattern_parts))\n        for p in selector.select_from(self):\n            yield p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the data to the file in bytes mode.", "response": "def write_bytes(self, data):\n        \"\"\"\n        Open the file in bytes mode, write to it, and close the file.\n        \"\"\"\n        if not isinstance(data, six.binary_type):\n            raise TypeError(\n                'data must be %s, not %s' %\n                (six.binary_type.__class__.__name__, data.__class__.__name__))\n        with self.open(mode='wb') as f:\n            return f.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the data to the log file in text mode.", "response": "def write_text(self, data, encoding=None, errors=None):\n        \"\"\"\n        Open the file in text mode, write to it, and close the file.\n        \"\"\"\n        if not isinstance(data, six.text_type):\n            raise TypeError(\n                'data must be %s, not %s' %\n                (six.text_type.__class__.__name__, data.__class__.__name__))\n        with self.open(mode='w', encoding=encoding, errors=errors) as f:\n            return f.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relpath_posix(recwalk_result, pardir, fromwinpath=False):\n    ''' Helper function to convert all paths to relative posix like paths (to ease comparison) '''\n    return recwalk_result[0], path2unix(os.path.join(os.path.relpath(recwalk_result[0], pardir),recwalk_result[1]), nojoin=True, fromwinpath=fromwinpath)", "response": "Helper function to convert all paths to relative posix like paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts a dict of paths parts and organically to the paths alphabetical order", "response": "def sort_dict_of_paths(d):\n    \"\"\" Sort a dict containing paths parts (ie, paths divided in parts and stored as a list). Top paths will be given precedence over deeper paths. \"\"\"\n    # Find the path that is the deepest, and count the number of parts\n    max_rec = max(len(x) if x else 0 for x in d.values())\n    # Pad other paths with empty parts to fill in, so that all paths will have the same number of parts (necessary to compare correctly, else deeper paths may get precedence over top ones, since the folder name will be compared to filenames!)\n    for key in d.keys():\n        if d[key]:\n            d[key] = ['']*(max_rec-len(d[key])) + d[key]\n    # Sort the dict relatively to the paths alphabetical order\n    d_sort = sorted(d.items(), key=lambda x: x[1])\n    return d_sort"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_group(d, return_only_first=False):\n    ''' Sort a dictionary of relative paths and cluster equal paths together at the same time '''\n    # First, sort the paths in order (this must be a couple: (parent_dir, filename), so that there's no ambiguity because else a file at root will be considered as being after a folder/file since the ordering is done alphabetically without any notion of tree structure).\n    d_sort = sort_dict_of_paths(d)\n    # Pop the first item in the ordered list\n    base_elt = (-1, None)\n    while (base_elt[1] is None and d_sort):\n        base_elt = d_sort.pop(0)\n    # No element, then we just return\n    if base_elt[1] is None:\n        return None\n    # Else, we will now group equivalent files together (remember we are working on multiple directories, so we can have multiple equivalent relative filepaths, but of course the absolute filepaths are different).\n    else:\n        # Init by creating the first group and pushing the first ordered filepath into the first group\n        lst = []\n        lst.append([base_elt])\n        if d_sort:\n            # For each subsequent filepath\n            for elt in d_sort:\n                # If the filepath is not empty (generator died)\n                if elt[1] is not None:\n                    # If the filepath is the same to the latest grouped filepath, we add it to the same group\n                    if elt[1] == base_elt[1]:\n                        lst[-1].append(elt)\n                    # Else the filepath is different: we create a new group, add the filepath to this group, and replace the latest grouped filepath\n                    else:\n                        if return_only_first: break  # break here if we only need the first group\n                        lst.append([elt])\n                        base_elt = elt # replace the latest grouped filepath\n        return lst", "response": "Sort a dictionary of relative paths and cluster equal paths together at the same time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef majority_vote_byte_scan(relfilepath, fileslist, outpath, blocksize=65535, default_char_null=False):\n    '''Takes a list of files in string format representing the same data, and disambiguate by majority vote: for position in string, if the character is not the same accross all entries, we keep the major one. If none, it will be replaced by a null byte (because we can't know if any of the entries are correct about this character).\n    relfilepath is the filename or the relative file path relative to the parent directory (ie, this is the relative path so that we can compare the files from several directories).'''\n    # The idea of replication combined with ECC was a bit inspired by this paper: Friedman, Roy, Yoav Kantor, and Amir Kantor. \"Combining Erasure-Code and Replication Redundancy Schemes for Increased Storage and Repair Efficiency in P2P Storage Systems.\", 2013, Technion, Computer Science Department, Technical Report CS-2013-03\n    # But it is a very well known concept in redundancy engineering, usually called triple-modular redundancy (which is here extended to n-modular since we can supply any number of files we want, not just three).\n    # Preference in case of ambiguity is always given to the file of the first folder.\n\n    fileshandles = []\n    for filepath in fileslist:\n        if filepath:\n            # Already a file handle? Just store it in the fileshandles list\n            if hasattr(filepath, 'read'):\n                fileshandles.append(filepath)\n            # Else it's a string filepath, open the file\n            else:\n                fileshandles.append(open(filepath, 'rb'))\n\n    # Create and open output (merged) file, except if we were already given a file handle\n    if hasattr(outpath, 'write'):\n        outfile = outpath\n    else:\n        outpathfull = os.path.join(outpath, relfilepath)\n        pardir = os.path.dirname(outpathfull)\n        if not os.path.exists(pardir):\n            os.makedirs(pardir)\n        outfile = open(outpathfull, 'wb')\n\n    # Cannot vote if there's not at least 3 files!\n    # In this case, just copy the file from the first folder, verbatim\n    if len(fileshandles) < 3:\n        # If there's at least one input file, then copy it verbatim to the output folder\n        if fileshandles:\n            create_dir_if_not_exist(os.path.dirname(outpathfull))\n            buf = 1\n            while (buf):\n                buf = fileshandles[0].read()\n                outfile.write(buf)\n                outfile.flush()\n        return (1, \"Error with file %s: only %i copies available, cannot vote (need at least 3)! Copied the first file from the first folder, verbatim.\" % (relfilepath, len(fileshandles)))\n\n    errors = []\n    entries = [1]*len(fileshandles)  # init with 0 to start the while loop\n    while (entries.count('') < len(fileshandles)):\n        final_entry = []\n        # Read a block from all input files into memory\n        for i in xrange(len(fileshandles)):\n            entries[i] = fileshandles[i].read(blocksize)\n\n        # End of file for all files, we exit\n        if entries.count('') == len(fileshandles):\n            break\n        # Else if there's only one file, just copy the file's content over\n        elif len(entries) == 1:\n            final_entry = entries[0]\n\n        # Else, do the majority vote\n        else:\n            # Walk along each column (imagine the strings being rows in a matrix, then we pick one column at each iteration = all characters at position i of each string), so that we can compare these characters easily\n            for i in xrange(max(len(entry) for entry in entries)):\n                hist = {} # kind of histogram, we just memorize how many times a character is presented at the position i in each string TODO: use collections.Counter instead of dict()?\n                # Extract the character at position i of each string and compute the histogram at the same time (number of time this character appear among all strings at this position i)\n                for entry in entries:\n                    # Check if we are not beyond the current entry's length\n                    if i < len(entry): # TODO: check this line, this should allow the vote to continue even if some files are shorter than others\n                        # Extract the character and use it to contribute to the histogram\n                        # TODO: add warning message when one file is not of the same size as the others\n                        key = str(ord(entry[i])) # convert to the ascii value to avoid any funky problem with encoding in dict keys\n                        hist[key] = hist.get(key, 0) + 1 # increment histogram for this value. If it does not exists, use 0. (essentially equivalent to hist[key] += 1 but with exception management if key did not already exists)\n                # If there's only one character (it's the same accross all strings at position i), then it's an exact match, we just save the character and we can skip to the next iteration\n                if len(hist) == 1:\n                    final_entry.append(chr(int(hist.iterkeys().next())))\n                    continue\n                # Else, the character is different among different entries, we will pick the major one (mode)\n                elif len(hist) > 1:\n                    # Sort the dict by value (and reverse because we want the most frequent first)\n                    skeys = sorted(hist, key=hist.get, reverse=True)\n                    # Ambiguity! If each entries present a different character (thus the major has only an occurrence of 1), then it's too ambiguous and we just set a null byte to signal that\n                    if hist[skeys[0]] == 1:\n                        if default_char_null:\n                            if default_char_null is True:\n                                final_entry.append(\"\\x00\")\n                            else:\n                                final_entry.append(default_char_null)\n                        else:\n                            # Use the entry of the first file that is still open\n                            first_char = ''\n                            for entry in entries:\n                                # Found the first file that has a character at this position: store it and break loop\n                                if i < len(entry):\n                                    first_char = entry[i]\n                                    break\n                            # Use this character in spite of ambiguity\n                            final_entry.append(first_char)\n                        errors.append(outfile.tell() + i) # Print an error indicating the characters that failed\n                    # Else if there is a tie (at least two characters appear with the same frequency), then we just pick one of them\n                    elif hist[skeys[0]] == hist[skeys[1]]:\n                        final_entry.append(chr(int(skeys[0]))) # TODO: find a way to account for both characters. Maybe return two different strings that will both have to be tested? (eg: maybe one has a tampered hash, both will be tested and if one correction pass the hash then it's ok we found the correct one)\n                    # Else we have a clear major character that appear in more entries than any other character, then we keep this one\n                    else:\n                        final_entry.append(chr(int(skeys[0]))) # alternative one-liner: max(hist.iteritems(), key=operator.itemgetter(1))[0]\n                    continue\n            # Concatenate to a string (this is faster than using a string from the start and concatenating at each iteration because Python strings are immutable so Python has to copy over the whole string, it's in O(n^2)\n            final_entry = ''.join(final_entry)\n            # Commit to output file\n            outfile.write(final_entry)\n            outfile.flush()\n\n    # Errors signaling\n    if errors:\n        error_msg = \"Unrecoverable corruptions (because of ambiguity) in file %s on characters: %s.\" % (relfilepath, [hex(int(x)) for x in errors]) # Signal to user that this file has unrecoverable corruptions (he may try to fix the bits manually or with his own script)\n        return (1, error_msg) # return an error\n    # Close all input files\n    for fh in fileshandles:\n        fh.close()\n    # Close output file\n    if outfile != outpath:  # close only if we were not given a file handle in the first place\n        outfile.flush()\n        outfile.close()\n    return (0, None)", "response": "Takes a list of files in string format representing the same data and disambiguates by majority vote."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef synchronize_files(inputpaths, outpath, database=None, tqdm_bar=None, report_file=None, ptee=None, verbose=False):\n    ''' Main function to synchronize files contents by majority vote\n    The main job of this function is to walk through the input folders and align the files, so that we can compare every files across every folders, one by one.\n    The whole trick here is to align files, so that we don't need to memorize all the files in memory and we compare all equivalent files together: to do that, we ensure that we walk through the input directories in alphabetical order, and we pick the relative filepath at the top of the alphabetical order, this ensures the alignment of files between different folders, without memorizing  the whole trees structures.\n    '''\n    # (Generator) Files Synchronization Algorithm:\n    # Needs a function stable_dir_walking, which will walk through directories recursively but in always the same order on all platforms (same order for files but also for folders), whatever order it is, as long as it is stable.\n    # Until there's no file in any of the input folders to be processed:\n    # - curfiles <- load first file for each folder by using stable_dir_walking on each input folder.\n    # - curfiles_grouped <- group curfiles_ordered:\n    #    * curfiles_ordered <- order curfiles alphabetically (need to separate the relative parent directory and the filename, to account for both without ambiguity)\n    #    * curfiles_grouped <- empty list\n    #    * curfiles_grouped[0] = add first element in curfiles_ordered\n    #    * last_group = 0\n    #    * for every subsequent element nextelt in curfiles_ordered:\n    #        . if nextelt == curfiles_grouped[last_group][0]: add nextelt into curfiles_grouped[last_group] (the latest group in curfiles_grouped)\n    #        . else: create a new group in curfiles_grouped (last_group += 1) and add nextelt into curfiles_grouped[last_group]\n    # At this stage, curfiles_grouped[0] should contain a group of files with the same relative filepath from different input folders, and since we used stable_dir_walking, we are guaranteed that this file is the next to be processed in alphabetical order.\n    # - Majority vote byte-by-byte for each of curfiles_grouped[0], and output winning byte to the output file.\n    # - Update files list alignment: we will now ditch files in curfiles_grouped[0] from curfiles, and replace by the next files respectively from each respective folder. Since we processed in alphabetical (or whatever) order, the next loaded files will match the files in other curfiles_grouped groups that we could not process before.\n    # At this point (after the loop), all input files have been processed in order, without maintaining the whole files list in memory, just one file per input folder.\n\n    # Init files walking generator for each inputpaths\n    recgen = [recwalk(path, sorting=True) for path in inputpaths]\n    curfiles = {}\n    recgen_exhausted = {}\n    recgen_exhausted_count = 0\n    nbpaths = len(inputpaths)\n    retcode = 0\n\n    if not ptee: ptee = sys.stdout\n\n    # Open report file and write header\n    if report_file is not None:\n        rfile = open(report_file, 'wb')\n        r_writer = csv.writer(rfile, delimiter='|', lineterminator='\\n', quotechar='\"')\n        r_header = [\"filepath\"] + [\"dir%i\" % (i+1) for i in xrange(nbpaths)] + [\"hash-correct\", \"error_code\", \"errors\"]\n        r_length = len(r_header)\n        r_writer.writerow(r_header)\n\n    # Initialization: load the first batch of files, one for each folder\n    for i in xrange(len(recgen)):\n        recgen_exhausted[i] = False\n        try:\n            if curfiles.get(i, None) is None:\n                curfiles[i] = relpath_posix(recgen[i].next(), inputpaths[i])[1]\n        except StopIteration:\n            recgen_exhausted[i] = True\n            recgen_exhausted_count += 1\n\n    # Files lists alignment loop\n    while recgen_exhausted_count < nbpaths:\n        errcode = 0\n        errmsg = None\n\n        # Init a new report's row\n        if report_file: r_row = [\"-\"] * r_length\n\n        # -- Group equivalent relative filepaths together\n        #print curfiles # debug\n        curfiles_grouped = sort_group(curfiles, True)\n\n        # -- Extract first group of equivalent filepaths (this allows us to process with the same alphabetical order on all platforms)\n        # Note that the remaining files in other groups will be processed later, because their alphabetical order is higher to the first group, this means that the first group is to be processed now\n        to_process = curfiles_grouped[0]\n        #print to_process # debug\n\n        # -- Byte-by-byte majority vote on the first group of files\n        # Need the relative filepath also (note that there's only one since it's a group of equivalent relative filepaths, only the absolute path is different between files of a same group)\n        relfilepath = path2unix(os.path.join(*to_process[0][1]))\n        if report_file: r_row[0] = relfilepath\n        if verbose: ptee.write(\"- Processing file %s.\" % relfilepath)\n        # Generate output path\n        outpathfull = os.path.join(outpath, relfilepath)\n        create_dir_if_not_exist(os.path.dirname(outpathfull))\n        # Initialize the list of absolute filepaths\n        fileslist = []\n        for elt in to_process:\n            i = elt[0]\n            fileslist.append(os.path.join(inputpaths[i], os.path.join(*elt[1])))\n            if report_file: r_row[i+1] = 'X' # put an X in the report file below each folder that contains this file\n        # If there's only one file, just copy it over\n        if len(to_process) == 1:\n            shutil.copyfile(fileslist[0], outpathfull)\n            id = to_process[0][0]\n            if report_file: r_row[id+1] = 'O'\n        # Else, merge by majority vote\n        else:\n            # Before-merge check using rfigc database, if provided\n            # If one of the files in the input folders is already correct, just copy it over\n            correct_file = None\n            if database:\n                for id, filepath in enumerate(fileslist):\n                    if rfigc.main(\"-i \\\"%s\\\" -d \\\"%s\\\" -m --silent\" % (filepath, database)) == 0:\n                        correct_file = filepath\n                        correct_id = to_process[id][0]\n                        break\n\n            # If one correct file was found, copy it over\n            if correct_file:\n                create_dir_if_not_exist(os.path.dirname(outpathfull))\n                shutil.copyfile(correct_file, outpathfull)\n                if report_file:\n                    r_row[correct_id+1] = \"O\"\n                    r_row[-3] = \"OK\"\n            # Else, we need to do the majority vote merge\n            else:\n                # Do the majority vote merge\n                errcode, errmsg = majority_vote_byte_scan(relfilepath, fileslist, outpath)\n\n        # After-merge/move check using rfigc database, if provided\n        if database:\n            if rfigc.main(\"-i \\\"%s\\\" -d \\\"%s\\\" -m --silent\" % (outpathfull, database)) == 1:\n                errcode = 1\n                r_row[-3] = \"KO\"\n                if not errmsg: errmsg = ''\n                errmsg += \" File could not be totally repaired according to rfigc database.\"\n            else:\n                if report_file:\n                    r_row[-3] = \"OK\"\n                    if errmsg: errmsg += \" But merged file is correct according to rfigc database.\"\n\n        # Display errors if any\n        if errcode:\n            if report_file:\n                r_row[-2] = \"KO\"\n                r_row[-1] = errmsg\n            ptee.write(errmsg)\n            retcode = 1\n        else:\n            if report_file: r_row[-2] = \"OK\"\n\n        # Save current report's row\n        if report_file:\n            r_writer.writerow(r_row)\n\n        # -- Update files lists alignment (ie, retrieve new files but while trying to keep the alignment)\n        for elt in to_process:  # for files of the first group (the ones we processed)\n            i = elt[0]\n            # Walk their respective folders and load up the next file\n            try:\n                if not recgen_exhausted.get(i, False):\n                    curfiles[i] = relpath_posix(recgen[i].next(), inputpaths[i])[1]\n            # If there's no file left in this folder, mark this input folder as exhausted and continue with the others\n            except StopIteration:\n                curfiles[i] = None\n                recgen_exhausted[i] = True\n                recgen_exhausted_count += 1\n        if tqdm_bar: tqdm_bar.update()\n    if tqdm_bar: tqdm_bar.close()\n\n    # Closing report file\n    if report_file:\n        # Write list of directories and legend\n        rfile.write(\"\\n=> Input directories:\")\n        for id, ipath in enumerate(inputpaths):\n            rfile.write(\"\\n\\t- dir%i = %s\" % ((id+1), ipath))\n        rfile.write(\"\\n=> Output directory: %s\" % outpath)\n        rfile.write(\"\\n=> Legend: X=existing/selected for majority vote, O=only used this file, - = not existing, OK = check correct, KO = check incorrect (file was not recovered)\\n\")\n        # Close the report file handle\n        rfile.close()\n\n    return retcode", "response": "This function is used to synchronize files contents by majority. It will recursively walk through the input folders and align the files with the same filepath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a tree of referrers of the root object.", "response": "def get_tree(self):\n        \"\"\"Get a tree of referrers of the root object.\"\"\"\n        self.ignore.append(inspect.currentframe())\n        return self._get_tree(self.root, self.maxdepth)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_tree(self, tree=None):\n        if tree is None:\n            self._print(self.root, '', '')\n        else:\n            self._print(tree, '', '')", "response": "Print referrers tree to console."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes and print a new line of the tree.", "response": "def _print(self, tree, prefix, carryon):\n        \"\"\"Compute and print a new line of the tree.\n\n        This is a recursive function.\n\n        arguments\n        tree -- tree to print\n        prefix -- prefix to the current line to print\n        carryon -- prefix which is used to carry on the vertical lines\n\n        \"\"\"\n        level = prefix.count(self.cross) + prefix.count(self.vline)\n        len_children = 0\n        if isinstance(tree , _Node):\n            len_children = len(tree.children)\n\n        # add vertex\n        prefix += str(tree)\n        # and as many spaces as the vertex is long\n        carryon += self.space * len(str(tree))\n        if (level == self.maxdepth) or (not isinstance(tree, _Node)) or\\\n           (len_children == 0):\n            self.stream.write(prefix+'\\n')\n            return\n        else:\n            # add in between connections\n            prefix += self.hline\n            carryon += self.space\n            # if there is more than one branch, add a cross\n            if len(tree.children) > 1:\n                prefix += self.cross\n                carryon += self.vline\n            prefix += self.hline\n            carryon += self.space\n\n            if len_children > 0:\n                # print the first branch (on the same line)\n                self._print(tree.children[0], prefix, carryon)\n                for b in range(1, len_children):\n                    # the caryon becomes the prefix for all following children\n                    prefix = carryon[:-2] + self.cross + self.hline\n                    # remove the vlines for any children of last branch\n                    if b == (len_children-1):\n                        carryon = carryon[:-2] + 2*self.space\n                    self._print(tree.children[b], prefix, carryon)\n                    # leave a free line before the next branch\n                    if b == (len_children-1):\n                        if len(carryon.strip(' ')) == 0:\n                            return\n                        self.stream.write(carryon[:-2].rstrip()+'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_tree(self, filename, tree=None):\n        old_stream = self.stream\n        self.stream = open(filename, 'w')\n        try:\n            super(FileBrowser, self).print_tree(tree=tree)\n        finally:\n            self.stream.close()\n            self.stream = old_stream", "response": "Print referrers tree to file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate interactive browser window. keyword arguments standalone -- Set to true", "response": "def main(self, standalone=False):\n        \"\"\"Create interactive browser window.\n\n        keyword arguments\n        standalone -- Set to true, if the browser is not attached to other\n        windows\n\n        \"\"\"\n        window = _Tkinter.Tk()\n        sc = _TreeWidget.ScrolledCanvas(window, bg=\"white\",\\\n                                       highlightthickness=0, takefocus=1)\n        sc.frame.pack(expand=1, fill=\"both\")\n        item = _ReferrerTreeItem(window, self.get_tree(), self)\n        node = _TreeNode(sc.canvas, None, item)\n        node.expand()\n        if standalone:\n            window.mainloop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the value of the DropDown option in the order of the argument.", "response": "def getValue(self):\n    '''\n    Returns\n      str(option_string * DropDown Value)\n      e.g.\n      -vvvvv\n    '''\n    dropdown_value = self.widget.GetValue()\n    if not str(dropdown_value).isdigit():\n      return ''\n    arg = str(self.option_string).replace('-', '')\n    repeated_args = arg * int(dropdown_value)\n    return '-' + repeated_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_meter(n, total, elapsed, ncols=None, prefix='',\n         unit=None, unit_scale=False, ascii=False):\n    \"\"\"\n    Return a string-based progress bar given some parameters\n\n    Parameters\n    ----------\n    n  : int\n        Number of finished iterations.\n    total  : int\n        The expected total number of iterations. If None, only basic progress\n        statistics are displayed (no ETA).\n    elapsed  : float\n        Number of seconds passed since start.\n    ncols  : int, optional\n        The width of the entire output message. If sepcified, dynamically\n        resizes the progress meter [default: None]. The fallback meter\n        width is 10.\n    prefix  : str, optional\n        Prefix message (included in total width).\n    unit  : str, optional\n        String that will be used to define the unit of each iteration.\n        [default: \"it\"]\n    unit_scale  : bool, optional\n        If set, the number of iterations will be reduced/scaled\n        automatically and a metric prefix following the\n        International System of Units standard will be added\n        (kilo, mega, etc.). [default: False]\n    ascii  : bool, optional\n        If not set, use unicode (smooth blocks) to fill the meter\n        [default: False]. The fallback is to use ASCII characters (1-9 #).\n\n    Returns\n    -------\n    out  : Formatted meter and stats, ready to display.\n    \"\"\"\n\n    # in case the total is wrong (n is above the total), then\n    # we switch to the mode without showing the total prediction\n    # (since ETA would be wrong anyway)\n    if total and n > total:\n        total = None\n\n    elapsed_str = format_interval(elapsed)\n    if elapsed:\n        if unit_scale:\n            rate = format_sizeof(n / elapsed, suffix='')\n        else:\n            rate = '{0:5.2f}'.format(n / elapsed)\n    else:\n        rate = '?'\n\n    rate_unit = unit if unit else 'it'\n    if not unit:\n        unit = ''\n\n    n_fmt = str(n)\n    total_fmt = str(total)\n    if unit_scale:\n        n_fmt = format_sizeof(n, suffix='')\n        if total:\n            total_fmt = format_sizeof(total, suffix='')\n\n    if total:\n        frac = n / total\n        percentage = frac * 100\n\n        remaining_str = format_interval(elapsed * (total-n) / n) if n else '?'\n\n        l_bar = '{1}{0:.0f}%|'.format(percentage, prefix) if prefix else \\\n                '{0:3.0f}%|'.format(percentage)\n        r_bar = '| {0}/{1}{2} [{3}<{4}, {5} {6}/s]'.format(\n                n_fmt, total_fmt, unit, elapsed_str, remaining_str,\n                rate, rate_unit)\n\n        if ncols == 0:\n            bar = ''\n        else:\n            N_BARS = max(1, ncols - len(l_bar) - len(r_bar)) if ncols \\\n                             else 10\n\n            if ascii:\n                bar_length, frac_bar_length = divmod(\n                    int(frac * N_BARS * 10), 10)\n\n                bar = '#'*bar_length\n                frac_bar = chr(48 + frac_bar_length) if frac_bar_length \\\n                                else ' '\n\n            else:\n                bar_length, frac_bar_length = divmod(int(frac * N_BARS * 8), 8)\n\n                bar = _unich(0x2588)*bar_length\n                frac_bar = _unich(0x2590 - frac_bar_length) \\\n                    if frac_bar_length else ' '\n\n        if bar_length < N_BARS:\n            full_bar = bar + frac_bar + \\\n                ' ' * max(N_BARS - bar_length - 1, 0)  # spacing\n        else:\n            full_bar = bar + \\\n                ' ' * max(N_BARS - bar_length, 0)  # spacing\n\n        return l_bar + full_bar + r_bar\n\n    else:  # no progressbar nor ETA, just progress statistics\n        return '{0}{1} [{2}, {3} {4}/s]'.format(\n            n_fmt, unit, elapsed_str, rate, rate_unit)", "response": "Returns a string - based progress bar given some parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getIcon( data ):\n    import cStringIO\n    stream = cStringIO.StringIO(data)\n    image = wx.ImageFromStream(stream)\n    icon = wx.EmptyIcon()\n    icon.CopyFromBitmap(wx.BitmapFromImage(image))\n    return icon", "response": "Return the data from the resource as a wxIcon"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    logging.basicConfig(level=logging.INFO)\n    app = RunSnakeRunApp(0)\n    app.MainLoop()", "response": "Main loop for the application"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CreateControls(self, config_parser):\n        self.CreateMenuBar()\n        self.SetupToolBar()\n        self.CreateStatusBar()\n        self.leftSplitter = wx.SplitterWindow(\n            self\n        )\n        self.rightSplitter = wx.SplitterWindow(\n            self.leftSplitter\n        )\n        self.listControl = listviews.DataView(\n            self.leftSplitter,\n            columns = PROFILE_VIEW_COLUMNS,\n            name='mainlist',\n        )\n        self.squareMap = squaremap.SquareMap(\n            self.rightSplitter,\n            padding = 6,\n            labels = True,\n            adapter = self.adapter,\n            square_style = True,\n        )\n        self.tabs = wx.Notebook(\n            self.rightSplitter,\n        )\n\n        self.CreateSourceWindow(self.tabs)\n        \n        self.calleeListControl = listviews.DataView(\n            self.tabs,\n            columns = PROFILE_VIEW_COLUMNS,\n            name='callee',\n        )\n        self.allCalleeListControl = listviews.DataView(\n            self.tabs,\n            columns = PROFILE_VIEW_COLUMNS,\n            name='allcallee',\n        )\n        self.allCallerListControl = listviews.DataView(\n            self.tabs,\n            columns = PROFILE_VIEW_COLUMNS,\n            name='allcaller',\n        )\n        self.callerListControl = listviews.DataView(\n            self.tabs,\n            columns = PROFILE_VIEW_COLUMNS,\n            name='caller',\n        )\n        self.ProfileListControls = [\n            self.listControl,\n            self.calleeListControl,\n            self.allCalleeListControl,\n            self.callerListControl,\n            self.allCallerListControl,\n        ]\n        self.tabs.AddPage(self.calleeListControl, _('Callees'), True)\n        self.tabs.AddPage(self.allCalleeListControl, _('All Callees'), False)\n        self.tabs.AddPage(self.callerListControl, _('Callers'), False)\n        self.tabs.AddPage(self.allCallerListControl, _('All Callers'), False)\n        if editor:\n            self.tabs.AddPage(self.sourceCodeControl, _('Source Code'), False)\n        self.rightSplitter.SetSashSize(10)\n        # calculate size as proportional value for initial display...\n        self.LoadState( config_parser )\n        width, height = self.GetSizeTuple()\n        rightsplit = 2 * (height // 3)\n        leftsplit = width // 3\n        self.rightSplitter.SplitHorizontally(self.squareMap, self.tabs,\n                                             rightsplit)\n        self.leftSplitter.SplitVertically(self.listControl, self.rightSplitter,\n                                          leftsplit)\n        squaremap.EVT_SQUARE_HIGHLIGHTED(self.squareMap,\n                                         self.OnSquareHighlightedMap)\n        squaremap.EVT_SQUARE_SELECTED(self.listControl,\n                                      self.OnSquareSelectedList)\n        squaremap.EVT_SQUARE_SELECTED(self.squareMap, self.OnSquareSelectedMap)\n        squaremap.EVT_SQUARE_ACTIVATED(self.squareMap, self.OnNodeActivated)\n        for control in self.ProfileListControls:\n            squaremap.EVT_SQUARE_ACTIVATED(control, self.OnNodeActivated)\n            squaremap.EVT_SQUARE_HIGHLIGHTED(control,\n                                             self.OnSquareHighlightedList)\n        self.moreSquareViewItem.Check(self.squareMap.square_style)", "response": "Create our control classes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate our menu - bar for triggering operations", "response": "def CreateMenuBar(self):\n        \"\"\"Create our menu-bar for triggering operations\"\"\"\n        menubar = wx.MenuBar()\n        menu = wx.Menu()\n        menu.Append(ID_OPEN, _('&Open Profile'), _('Open a cProfile file'))\n        menu.Append(ID_OPEN_MEMORY, _('Open &Memory'), _('Open a Meliae memory-dump file'))\n        menu.AppendSeparator()\n        menu.Append(ID_EXIT, _('&Close'), _('Close this RunSnakeRun window'))\n        menubar.Append(menu, _('&File'))\n        menu = wx.Menu()\n#        self.packageMenuItem = menu.AppendCheckItem(\n#            ID_PACKAGE_VIEW, _('&File View'),\n#            _('View time spent by package/module')\n#        )\n        self.percentageMenuItem = menu.AppendCheckItem(\n            ID_PERCENTAGE_VIEW, _('&Percentage View'),\n            _('View time spent as percent of overall time')\n        )\n        self.rootViewItem = menu.Append(\n            ID_ROOT_VIEW, _('&Root View (Home)'),\n            _('View the root of the tree')\n        )\n        self.backViewItem = menu.Append(\n            ID_BACK_VIEW, _('&Back'), _('Go back in your viewing history')\n        )\n        self.upViewItem = menu.Append(\n            ID_UP_VIEW, _('&Up'),\n            _('Go \"up\" to the parent of this node with the largest cumulative total')\n        )\n        self.moreSquareViewItem = menu.AppendCheckItem(\n            ID_MORE_SQUARE, _('&Hierarchic Squares'),\n            _('Toggle hierarchic squares in the square-map view')\n        )\n\n        # This stuff isn't really all that useful for profiling,\n        # it's more about how to generate graphics to describe profiling...\n        self.deeperViewItem = menu.Append(\n            ID_DEEPER_VIEW, _('&Deeper'), _('View deeper squaremap views')\n        )\n        self.shallowerViewItem = menu.Append(\n            ID_SHALLOWER_VIEW, _('&Shallower'), _('View shallower squaremap views')\n        )\n#        wx.ToolTip.Enable(True)\n        menubar.Append(menu, _('&View'))\n        \n        self.viewTypeMenu =wx.Menu( )\n        menubar.Append(self.viewTypeMenu, _('View &Type'))\n        \n        self.SetMenuBar(menubar)\n\n        wx.EVT_MENU(self, ID_EXIT, lambda evt: self.Close(True))\n        wx.EVT_MENU(self, ID_OPEN, self.OnOpenFile)\n        wx.EVT_MENU(self, ID_OPEN_MEMORY, self.OnOpenMemory)\n        \n        wx.EVT_MENU(self, ID_PERCENTAGE_VIEW, self.OnPercentageView)\n        wx.EVT_MENU(self, ID_UP_VIEW, self.OnUpView)\n        wx.EVT_MENU(self, ID_DEEPER_VIEW, self.OnDeeperView)\n        wx.EVT_MENU(self, ID_SHALLOWER_VIEW, self.OnShallowerView)\n        wx.EVT_MENU(self, ID_ROOT_VIEW, self.OnRootView)\n        wx.EVT_MENU(self, ID_BACK_VIEW, self.OnBackView)\n        wx.EVT_MENU(self, ID_MORE_SQUARE, self.OnMoreSquareToggle)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateSourceWindow(self, tabs):\n        if editor and self.sourceCodeControl is None:\n            self.sourceCodeControl = wx.py.editwindow.EditWindow(\n                self.tabs, -1\n            )\n            self.sourceCodeControl.SetText(u\"\")\n            self.sourceFileShown = None\n            self.sourceCodeControl.setDisplayLineNumbers(True)", "response": "Create our source - view window for tabs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the toolbar for common actions", "response": "def SetupToolBar(self):\n        \"\"\"Create the toolbar for common actions\"\"\"\n        tb = self.CreateToolBar(self.TBFLAGS)\n        tsize = (24, 24)\n        tb.ToolBitmapSize = tsize\n        open_bmp = wx.ArtProvider.GetBitmap(wx.ART_FILE_OPEN, wx.ART_TOOLBAR,\n                                            tsize)\n        tb.AddLabelTool(ID_OPEN, \"Open\", open_bmp, shortHelp=\"Open\",\n                        longHelp=\"Open a (c)Profile trace file\")\n        if not osx:\n            tb.AddSeparator()\n#        self.Bind(wx.EVT_TOOL, self.OnOpenFile, id=ID_OPEN)\n        self.rootViewTool = tb.AddLabelTool(\n            ID_ROOT_VIEW, _(\"Root View\"),\n            wx.ArtProvider.GetBitmap(wx.ART_GO_HOME, wx.ART_TOOLBAR, tsize),\n            shortHelp=_(\"Display the root of the current view tree (home view)\")\n        )\n        self.rootViewTool = tb.AddLabelTool(\n            ID_BACK_VIEW, _(\"Back\"),\n            wx.ArtProvider.GetBitmap(wx.ART_GO_BACK, wx.ART_TOOLBAR, tsize),\n            shortHelp=_(\"Back to the previously activated node in the call tree\")\n        )\n        self.upViewTool = tb.AddLabelTool(\n            ID_UP_VIEW, _(\"Up\"),\n            wx.ArtProvider.GetBitmap(wx.ART_GO_UP, wx.ART_TOOLBAR, tsize),\n            shortHelp=_(\"Go one level up the call tree (highest-percentage parent)\")\n        )\n        if not osx:\n            tb.AddSeparator()\n        # TODO: figure out why the control is sizing the label incorrectly on Linux\n        self.percentageViewTool = wx.CheckBox(tb, -1, _(\"Percent    \"))\n        self.percentageViewTool.SetToolTip(wx.ToolTip(\n            _(\"Toggle display of percentages in list views\")))\n        tb.AddControl(self.percentageViewTool)\n        wx.EVT_CHECKBOX(self.percentageViewTool,\n                        self.percentageViewTool.GetId(), self.OnPercentageView)\n\n        self.viewTypeTool= wx.Choice( tb, -1, choices= getattr(self.loader,'ROOTS',[]) )\n        self.viewTypeTool.SetToolTip(wx.ToolTip(\n            _(\"Switch between different hierarchic views of the data\")))\n        wx.EVT_CHOICE( self.viewTypeTool, self.viewTypeTool.GetId(), self.OnViewTypeTool )\n        tb.AddControl( self.viewTypeTool )\n        tb.Realize()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ConfigureViewTypeChoices( self, event=None ):\n        self.viewTypeTool.SetItems( getattr( self.loader, 'ROOTS', [] ))\n        if self.loader and self.viewType in self.loader.ROOTS:\n            self.viewTypeTool.SetSelection( self.loader.ROOTS.index( self.viewType ))\n            \n        # configure the menu with the available choices...\n        def chooser( typ ):\n            def Callback( event ):\n                if typ != self.viewType:\n                    self.viewType = typ \n                    self.OnRootView( event )\n            return Callback\n        # Clear all previous items\n        for item in self.viewTypeMenu.GetMenuItems():\n            self.viewTypeMenu.DeleteItem( item )\n        if self.loader and self.loader.ROOTS:\n            for root in self.loader.ROOTS:\n                item = wx.MenuItem( \n                    self.viewTypeMenu, -1, root.title(), \n                    _(\"View hierarchy by %(name)s\")%{\n                        'name': root.title(),\n                    },\n                    kind=wx.ITEM_RADIO,\n                )\n                item.SetCheckable( True )\n                self.viewTypeMenu.AppendItem( item )\n                item.Check( root == self.viewType )\n                wx.EVT_MENU( self, item.GetId(), chooser( root ))", "response": "Configure the set of view types in the toolbar and menus"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting to open a new profile file", "response": "def OnOpenFile(self, event):\n        \"\"\"Request to open a new profile file\"\"\"\n        dialog = wx.FileDialog(self, style=wx.OPEN|wx.FD_MULTIPLE)\n        if dialog.ShowModal() == wx.ID_OK:\n            paths = dialog.GetPaths()\n            if self.loader:\n                # we've already got a displayed data-set, open new window...\n                frame = MainFrame()\n                frame.Show(True)\n                frame.load(*paths)\n            else:\n                self.load(*paths)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef OnOpenMemory(self, event):\n        dialog = wx.FileDialog(self, style=wx.OPEN)\n        if dialog.ShowModal() == wx.ID_OK:\n            path = dialog.GetPath()\n            if self.loader:\n                # we've already got a displayed data-set, open new window...\n                frame = MainFrame()\n                frame.Show(True)\n                frame.load_memory(path)\n            else:\n                self.load_memory(path)", "response": "Request to open a new profile file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SetPackageView(self, directoryView):\n        self.directoryView = not self.directoryView\n        self.packageMenuItem.Check(self.directoryView)\n        self.packageViewTool.SetValue(self.directoryView)\n        if self.loader:\n            self.SetModel(self.loader)\n        self.RecordHistory()", "response": "Sets whether to use directory based view"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting whether to display percentage or absolute values", "response": "def SetPercentageView(self, percentageView):\n        \"\"\"Set whether to display percentage or absolute values\"\"\"\n        self.percentageView = percentageView\n        self.percentageMenuItem.Check(self.percentageView)\n        self.percentageViewTool.SetValue(self.percentageView)\n        total = self.adapter.value( self.loader.get_root( self.viewType ) )\n        for control in self.ProfileListControls:\n            control.SetPercentage(self.percentageView, total)\n        self.adapter.SetPercentage(self.percentageView, total)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting to move up the hierarchy to highest - weight parent", "response": "def OnUpView(self, event):\n        \"\"\"Request to move up the hierarchy to highest-weight parent\"\"\"\n        node = self.activated_node\n        parents = []\n        selected_parent = None\n        \n        if node:\n            if hasattr( self.adapter, 'best_parent' ):\n                selected_parent = self.adapter.best_parent( node )\n            else:\n                parents = self.adapter.parents( node )\n            if parents:\n                if not selected_parent:\n                    parents.sort(key = lambda a: self.adapter.value(node, a))\n                    selected_parent = parents[-1]\n                class event:\n                    node = selected_parent\n                self.OnNodeActivated(event)\n            else:\n                self.SetStatusText(_('No parents for the currently selected node: %(node_name)s')\n                                   % dict(node_name=self.adapter.label(node)))\n        else:\n            self.SetStatusText(_('No currently selected node'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbacks view event handler", "response": "def OnBackView(self, event):\n        \"\"\"Request to move backward in the history\"\"\"\n        self.historyIndex -= 1\n        try:\n            self.RestoreHistory(self.history[self.historyIndex])\n        except IndexError, err:\n            self.SetStatusText(_('No further history available'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef OnRootView(self, event):\n        self.adapter, tree, rows = self.RootNode()\n        self.squareMap.SetModel(tree, self.adapter)\n        self.RecordHistory()\n        self.ConfigureViewTypeChoices()", "response": "Reset view to the root of the tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef OnNodeActivated(self, event):\n        self.activated_node = self.selected_node = event.node\n        self.squareMap.SetModel(event.node, self.adapter)\n        self.squareMap.SetSelected( event.node )\n        if editor:\n            if self.SourceShowFile(event.node):\n                if hasattr(event.node,'lineno'):\n                    self.sourceCodeControl.GotoLine(event.node.lineno)\n        self.RecordHistory()", "response": "Double - click or enter on a node in some control..."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow the given file in the source - code view ( attempt it anyway", "response": "def SourceShowFile(self, node):\n        \"\"\"Show the given file in the source-code view (attempt it anyway)\"\"\"\n        filename = self.adapter.filename( node )\n        if filename and self.sourceFileShown != filename:\n            try:\n                data = open(filename).read()\n            except Exception, err:\n                # TODO: load from zips/eggs? What about .pyc issues?\n                return None\n            else:\n                #self.sourceCodeControl.setText(data)\n                self.sourceCodeControl.ClearAll()\n                self.sourceCodeControl.AppendText( data )\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OnSquareSelected(self, event):\n        self.selected_node = event.node\n        self.calleeListControl.integrateRecords(self.adapter.children( event.node) )\n        self.callerListControl.integrateRecords(self.adapter.parents( event.node) )", "response": "Square selected event handler"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef OnMoreSquareToggle( self, event ):\n        self.squareMap.square_style = not self.squareMap.square_style\n        self.squareMap.Refresh()\n        self.moreSquareViewItem.Check(self.squareMap.square_style)", "response": "More Square toggle event handler"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RecordHistory(self):\n        if not self.restoringHistory:\n            record = self.activated_node\n            if self.historyIndex < -1:\n                try:\n                    del self.history[self.historyIndex+1:]\n                except AttributeError, err:\n                    pass\n            if (not self.history) or record != self.history[-1]:\n                self.history.append(record)\n            del self.history[:-200]\n            self.historyIndex = -1", "response": "Record the given node in the history - set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, *filenames):\n        if len(filenames) == 1:\n            if os.path.basename( filenames[0] ) == 'index.coldshot':\n                return self.load_coldshot( os.path.dirname( filenames[0]) )\n            elif os.path.isdir( filenames[0] ):\n                return self.load_coldshot( filenames[0] )\n        try:\n            self.loader = pstatsloader.PStatsLoader(*filenames)\n            self.ConfigureViewTypeChoices()\n            self.SetModel( self.loader )\n            self.viewType = self.loader.ROOTS[0]\n            self.SetTitle(_(\"Run Snake Run: %(filenames)s\")\n                          % {'filenames': ', '.join(filenames)[:120]})\n        except (IOError, OSError, ValueError,MemoryError), err:\n            self.SetStatusText(\n                _('Failure during load of %(filenames)s: %(err)s'\n            ) % dict(\n                filenames=\" \".join([repr(x) for x in filenames]),\n                err=err\n            ))", "response": "Load our dataset (iteratively)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SetModel(self, loader):\n        self.loader = loader\n        self.adapter, tree, rows = self.RootNode()\n        self.listControl.integrateRecords(rows.values())\n        self.activated_node = tree\n        self.squareMap.SetModel(tree, self.adapter)\n        self.RecordHistory()", "response": "Set our overall model and populate sub - controls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn our current root node and appropriate adapter for it", "response": "def RootNode(self):\n        \"\"\"Return our current root node and appropriate adapter for it\"\"\"\n        tree = self.loader.get_root( self.viewType )\n        adapter = self.loader.get_adapter( self.viewType )\n        rows = self.loader.get_rows( self.viewType )\n        \n        adapter.SetPercentage(self.percentageView, adapter.value( tree ))\n        \n        return adapter, tree, rows"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving window state to be restored on the next run...", "response": "def SaveState( self, config_parser ):\n        \"\"\"Retrieve window state to be restored on the next run...\"\"\"\n        if not config_parser.has_section( 'window' ):\n            config_parser.add_section( 'window' )\n        if self.IsMaximized():\n            config_parser.set( 'window', 'maximized', str(True))\n        else:\n            config_parser.set( 'window', 'maximized', str(False))\n        size = self.GetSizeTuple()\n        position = self.GetPositionTuple()\n        config_parser.set( 'window', 'width', str(size[0]) )\n        config_parser.set( 'window', 'height', str(size[1]) )\n        config_parser.set( 'window', 'x', str(position[0]) )\n        config_parser.set( 'window', 'y', str(position[1]) )\n        \n        for control in self.ProfileListControls:\n            control.SaveState( config_parser )\n\n        return config_parser"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef LoadState( self, config_parser ):\n        if not config_parser:\n            return\n        if (\n            not config_parser.has_section( 'window' ) or (\n                config_parser.has_option( 'window','maximized' ) and \n                config_parser.getboolean( 'window', 'maximized' )\n            )\n        ):\n            self.Maximize(True)\n        try:\n            width,height,x,y = [\n                config_parser.getint( 'window',key )\n                for key in ['width','height','x','y']\n            ]\n            self.SetPosition( (x,y))\n            self.SetSize( (width,height))\n        except ConfigParser.NoSectionError, err:\n            # the file isn't written yet, so don't even warn...\n            pass\n        except Exception, err:\n            # this is just convenience, if it breaks in *any* way, ignore it...\n            log.error(\n                \"Unable to load window preferences, ignoring: %s\", traceback.format_exc()\n            )\n\n        try:\n            font_size = config_parser.getint('window', 'font_size')\n        except Exception:\n            pass # use the default, by default\n        else:\n            font = wx.SystemSettings_GetFont(wx.SYS_DEFAULT_GUI_FONT)\n            font.SetPointSize(font_size)\n            for ctrl in self.ProfileListControls:\n                ctrl.SetFont(font)\n        \n        for control in self.ProfileListControls:\n            control.LoadState( config_parser )\n        \n        self.config = config_parser\n        wx.EVT_CLOSE( self, self.OnCloseWindow )", "response": "Load our window state from the given config_parser instance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the edit distance between two sequences and return it.", "response": "def fast_comp(seq1, seq2, transpositions=False):\n\t\"\"\"Compute the distance between the two sequences `seq1` and `seq2` up to a\n\tmaximum of 2 included, and return it. If the edit distance between the two\n\tsequences is higher than that, -1 is returned.\n\t\n\tIf `transpositions` is `True`, transpositions will be taken into account for\n\tthe computation of the distance. This can make a difference, e.g.:\n\n\t\t>>> fast_comp(\"abc\", \"bac\", transpositions=False)\n\t\t2\n\t\t>>> fast_comp(\"abc\", \"bac\", transpositions=True)\n\t\t1\n\t\n\tThis is faster than `levenshtein` by an order of magnitude, but on the\n\tother hand is of limited use.\n\n\tThe algorithm comes from `http://writingarchives.sakura.ne.jp/fastcomp`.\n\tI've added transpositions support to the original code.\n\t\"\"\"\n\treplace, insert, delete = \"r\", \"i\", \"d\"\n\n\tL1, L2  = len(seq1), len(seq2)\n\tif L1 < L2:\n\t\tL1, L2 = L2, L1\n\t\tseq1, seq2 = seq2, seq1\n\n\tldiff = L1 - L2\n\tif ldiff == 0:\n\t\tmodels = (insert+delete, delete+insert, replace+replace)\n\telif ldiff == 1:\n\t\tmodels = (delete+replace, replace+delete)\n\telif ldiff == 2:\n\t\tmodels = (delete+delete,)\n\telse:\n\t\treturn -1\n\n\tres = 3\n\tfor model in models:\n\t\ti = j = c = 0\n\t\twhile (i < L1) and (j < L2):\n\t\t\tif seq1[i] != seq2[j]:\n\t\t\t\tc = c+1\n\t\t\t\tif 2 < c:\n\t\t\t\t\tbreak\n            \n\t\t\t\tif transpositions and ldiff != 2 \\\n            \tand i < L1 - 1 and j < L2 - 1 \\\n            \tand seq1[i+1] == seq2[j] and seq1[i] == seq2[j+1]:\n\t\t\t\t\ti, j = i+2, j+2\n\t\t\t\telse:\n\t\t\t\t\tcmd = model[c-1]\n\t\t\t\t\tif cmd == delete:\n\t\t\t\t\t\ti = i+1\n\t\t\t\t\telif cmd == insert:\n\t\t\t\t\t\tj = j+1\n\t\t\t\t\telse:\n\t\t\t\t\t\tassert cmd == replace\n\t\t\t\t\t\ti,j = i+1, j+1\n\t\t\telse:\n\t\t\t\ti,j = i+1, j+1\n\n\t\tif 2 < c:\n\t\t\tcontinue\n\t\telif i < L1:\n\t\t\tif L1-i <= model[c:].count(delete):\n\t\t\t\tc = c + (L1-i)\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telif j < L2:\n\t\t\tif L2-j <= model[c:].count(insert):\n\t\t\t\tc = c + (L2-j)\n\t\t\telse:\n\t\t\t\tcontinue\n\n\t\tif c < res:\n\t\t\tres = c\n\n\tif res == 3:\n\t\tres = -1\n\treturn res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a path is an actual file that exists'''", "response": "def is_file(dirname):\n    '''Checks if a path is an actual file that exists'''\n    if not os.path.isfile(dirname):\n        msg = \"{0} is not an existing file\".format(dirname)\n        raise argparse.ArgumentTypeError(msg)\n    else:\n        return dirname"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_dir(dirname):\n    '''Checks if a path is an actual directory that exists'''\n    if not os.path.isdir(dirname):\n        msg = \"{0} is not a directory\".format(dirname)\n        raise argparse.ArgumentTypeError(msg)\n    else:\n        return dirname", "response": "Checks if a path is an actual directory that exists'''"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a path is an actual directory that exists or a file", "response": "def is_dir_or_file(dirname):\n    '''Checks if a path is an actual directory that exists or a file'''\n    if not os.path.isdir(dirname) and not os.path.isfile(dirname):\n        msg = \"{0} is not a directory nor a file\".format(dirname)\n        raise argparse.ArgumentTypeError(msg)\n    else:\n        return dirname"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fullpath(relpath):\n    '''Relative path to absolute'''\n    if (type(relpath) is object or type(relpath) is file):\n        relpath = relpath.name\n    return os.path.abspath(os.path.expanduser(relpath))", "response": "Relative path to absolute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recwalk(inputpath, sorting=True):\n    '''Recursively walk through a folder. This provides a mean to flatten out the files restitution (necessary to show a progress bar). This is a generator.'''\n    # If it's only a single file, return this single file\n    if os.path.isfile(inputpath):\n        abs_path = fullpath(inputpath)\n        yield os.path.dirname(abs_path), os.path.basename(abs_path)\n    # Else if it's a folder, walk recursively and return every files\n    else:\n        for dirpath, dirs, files in walk(inputpath):\t\n            if sorting:\n                files.sort()\n                dirs.sort() # sort directories in-place for ordered recursive walking\n            for filename in files:\n                yield (dirpath, filename)", "response": "Recursively walk through a folder and return all files and directories in the folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a path given in any format converts to posix path format fromwinpath forces the input path to be recognized as a Windows path", "response": "def path2unix(path, nojoin=False, fromwinpath=False):\n    '''From a path given in any format, converts to posix path format\n    fromwinpath=True forces the input path to be recognized as a Windows path (useful on Unix machines to unit test Windows paths)'''\n    if fromwinpath:\n        pathparts = list(PureWindowsPath(path).parts)\n    else:\n        pathparts = list(PurePath(path).parts)\n    if nojoin:\n        return pathparts\n    else:\n        return posixpath.join(*pathparts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_next_entry(file, entrymarker=\"\\xFE\\xFF\\xFE\\xFF\\xFE\\xFF\\xFE\\xFF\\xFE\\xFF\", only_coord=True, blocksize=65535):\n    '''Find or read the next ecc entry in a given ecc file.\n    Call this function multiple times with the same file handle to get subsequent markers positions (this is not a generator but it works very similarly, because it will continue reading from the file's current cursor position -- this can be used advantageously if you want to read only a specific entry by seeking before supplying the file handle).\n    This will read any string length between two entrymarkers.\n    The reading is very tolerant, so it will always return any valid entry (but also scrambled entries if any, but the decoding will ensure everything's ok).\n    `file` is a file handle, not the path to the file.'''\n    found = False\n    start = None # start and end vars are the relative position of the starting/ending entrymarkers in the current buffer\n    end = None\n    startcursor = None # startcursor and endcursor are the absolute position of the starting/ending entrymarkers inside the database file\n    endcursor = None\n    buf = 1\n    # Sanity check: cannot screen the file's content if the window is of the same size as the pattern to match (the marker)\n    if blocksize <= len(entrymarker): blocksize = len(entrymarker) + 1\n    # Continue the search as long as we did not find at least one starting marker and one ending marker (or end of file)\n    while (not found and buf):\n        # Read a long block at once, we will readjust the file cursor after\n        buf = file.read(blocksize)\n        # Find the start marker (if not found already)\n        if start is None or start == -1:\n            start = buf.find(entrymarker); # relative position of the starting marker in the currently read string\n            if start >= 0 and not startcursor: # assign startcursor only if it's empty (meaning that we did not find the starting entrymarker, else if found we are only looking for \n                startcursor = file.tell() - len(buf) + start # absolute position of the starting marker in the file\n            if start >= 0: start = start + len(entrymarker)\n        # If we have a starting marker, we try to find a subsequent marker which will be the ending of our entry (if the entry is corrupted we don't care: it won't pass the entry_to_dict() decoding or subsequent steps of decoding and we will just pass to the next ecc entry). This allows to process any valid entry, no matter if previous ones were scrambled.\n        if startcursor is not None and startcursor >= 0:\n            end = buf.find(entrymarker, start)\n            if end < 0 and len(buf) < blocksize: # Special case: we didn't find any ending marker but we reached the end of file, then we are probably in fact just reading the last entry (thus there's no ending marker for this entry)\n                end = len(buf) # It's ok, we have our entry, the ending marker is just the end of file\n            # If we found an ending marker (or if end of file is reached), then we compute the absolute cursor value and put the file reading cursor back in position, just before the next entry (where the ending marker is if any)\n            if end >= 0:\n                endcursor = file.tell() - len(buf) + end\n                # Make sure we are not redetecting the same marker as the start marker\n                if endcursor > startcursor:\n                    file.seek(endcursor)\n                    found = True\n                else:\n                    end = -1\n                    encursor = None\n        #print(\"Start:\", start, startcursor)\n        #print(\"End: \", end, endcursor)\n        # Stop criterion to avoid infinite loop: in the case we could not find any entry in the rest of the file and we reached the EOF, we just quit now\n        if len(buf) < blocksize: break\n        # Did not find the full entry in one buffer? Reinit variables for next iteration, but keep in memory startcursor.\n        if start > 0: start = 0 # reset the start position for the end buf find at next iteration (ie: in the arithmetic operations to compute the absolute endcursor position, the start entrymarker won't be accounted because it was discovered in a previous buffer).\n        if not endcursor: file.seek(file.tell()-len(entrymarker)) # Try to fix edge case where blocksize stops the buffer exactly in the middle of the ending entrymarker. The starting marker should always be ok because it should be quite close (or generally immediately after) the previous entry, but the end depends on the end of the current entry (size of the original file), thus the buffer may miss the ending entrymarker. should offset file.seek(-len(entrymarker)) before searching for ending.\n\n    if found: # if an entry was found, we seek to the beginning of the entry and then either read the entry from file or just return the markers positions (aka the entry bounds)\n        file.seek(startcursor + len(entrymarker))\n        if only_coord:\n            # Return only coordinates of the start and end markers\n            # Note: it is useful to just return the reading positions and not the entry itself because it can get quite huge and may overflow memory, thus we will read each ecc blocks on request using a generator.\n            return [startcursor + len(entrymarker), endcursor]\n        else:\n            # Return the full entry's content\n            return file.read(endcursor - startcursor - len(entrymarker))\n    else:\n        # Nothing found (or no new entry to find, we've already found them all), so we return None\n        return None", "response": "Find or read the next entry in a given ecc file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_if_exist(path):  # pragma: no cover\n    if os.path.exists(path):\n        if os.path.isdir(path):\n            shutil.rmtree(path)\n            return True\n        elif os.path.isfile(path):\n            os.remove(path)\n            return True\n    return False", "response": "Delete a file or a directory recursively if it exists else no exception is raised"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy a file or a directory tree deleting the destination before processing", "response": "def copy_any(src, dst, only_missing=False):  # pragma: no cover\n    \"\"\"Copy a file or a directory tree, deleting the destination before processing\"\"\"\n    if not only_missing:\n        remove_if_exist(dst)\n    if os.path.exists(src):\n        if os.path.isdir(src):\n            if not only_missing:\n                shutil.copytree(src, dst, symlinks=False, ignore=None)\n            else:\n                for dirpath, filepath in recwalk(src):\n                    srcfile = os.path.join(dirpath, filepath)\n                    relpath = os.path.relpath(srcfile, src)\n                    dstfile = os.path.join(dst, relpath)\n                    if not os.path.exists(dstfile):\n                        create_dir_if_not_exist(os.path.dirname(dstfile))\n                        shutil.copyfile(srcfile, dstfile)\n                        shutil.copystat(srcfile, dstfile)\n            return True\n        elif os.path.isfile(src) and (not only_missing or not os.path.exists(dst)):\n            shutil.copyfile(src, dst)\n            shutil.copystat(src, dst)\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_files_by_size(fileslist, multi):  # pragma: no cover\n    ''' Cluster files into the specified number of groups, where each groups total size is as close as possible to each other.\n\n    Pseudo-code (O(n^g) time complexity):\n    Input: number of groups G per cluster, list of files F with respective sizes\n    - Order F by descending size\n    - Until F is empty:\n        - Create a cluster X\n        - A = Pop first item in F\n        - Put A in X[0] (X[0] is thus the first group in cluster X)\n        For g in 1..len(G)-1 :\n            - B = Pop first item in F\n            - Put B in X[g]\n            - group_size := size(B)\n            If group_size != size(A):\n                While group_size < size(A):\n                    - Find next item C in F which size(C) <= size(A) - group_size\n                    - Put C in X[g]\n                    - group_size := group_size + size(C)\n    '''\n    flord = OrderedDict(sorted(fileslist.items(), key=lambda x: x[1], reverse=True))\n    if multi <= 1:\n        fgrouped = {}\n        i = 0\n        for x in flord.keys():\n            i += 1\n            fgrouped[i] = [[x]]\n        return fgrouped\n\n    fgrouped = {}\n    i = 0\n    while flord:\n        i += 1\n        fgrouped[i] = []\n        big_key, big_value = flord.popitem(0)\n        fgrouped[i].append([big_key])\n        for j in xrange(multi-1):\n            cluster = []\n            if not flord: break\n            child_key, child_value = flord.popitem(0)\n            cluster.append(child_key)\n            if child_value == big_value:\n                fgrouped[i].append(cluster)\n                continue\n            else:\n                diff = big_value - child_value\n                for key, value in flord.iteritems():\n                    if value <= diff:\n                        cluster.append(key)\n                        del flord[key]\n                        if value == diff:\n                            break\n                        else:\n                            child_value += value\n                            diff = big_value - child_value\n                fgrouped[i].append(cluster)\n    return fgrouped", "response": "Group files into the specified number of groups where each group has the same size."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a files list with sizes output a list where the files are grouped in nbgroups per cluster.", "response": "def group_files_by_size_fast(fileslist, nbgroups, mode=1):  # pragma: no cover\n    '''Given a files list with sizes, output a list where the files are grouped in nbgroups per cluster.\n\n    Pseudo-code for algorithm in O(n log(g)) (thank's to insertion sort or binary search trees)\n    See for more infos: http://cs.stackexchange.com/questions/44406/fast-algorithm-for-clustering-groups-of-elements-given-their-size-time/44614#44614\n    For each file:\n        - If to-fill list is empty or file.size > first-key(to-fill):\n          * Create cluster c with file in first group g1\n          * Add to-fill[file.size].append([c, g2], [c, g3], ..., [c, gn])\n        - Else:\n          * ksize = first-key(to-fill)\n          * c, g = to-fill[ksize].popitem(0)\n          * Add file to cluster c in group g\n          * nsize = ksize - file.size\n          * if nsize > 0:\n            . to-fill[nsize].append([c, g])\n            . sort to-fill if not an automatic ordering structure\n        '''\n    ftofill = SortedList()\n    ftofill_pointer = {}\n    fgrouped = [] # [] or {}\n    ford = sorted(fileslist.iteritems(), key=lambda x: x[1])\n    last_cid = -1\n    while ford:\n        fname, fsize = ford.pop()\n        #print \"----\\n\"+fname, fsize\n        #if ftofill: print \"beforebranch\", fsize, ftofill[-1]\n        #print ftofill\n        if not ftofill or fsize > ftofill[-1]:\n            last_cid += 1\n            #print \"Branch A: create cluster %i\" % last_cid\n            fgrouped.append([])\n            #fgrouped[last_cid] = []\n            fgrouped[last_cid].append([fname])\n            if mode==0:\n                for g in xrange(nbgroups-1, 0, -1):\n                    fgrouped[last_cid].append([])\n                    if not fsize in ftofill_pointer:\n                        ftofill_pointer[fsize] = []\n                    ftofill_pointer[fsize].append((last_cid, g))\n                    ftofill.add(fsize)\n            else:\n                for g in xrange(1, nbgroups):\n                    try:\n                        fgname, fgsize = ford.pop()\n                        #print \"Added to group %i: %s %i\" % (g, fgname, fgsize)\n                    except IndexError:\n                        break\n                    fgrouped[last_cid].append([fgname])\n                    diff_size = fsize - fgsize\n                    if diff_size > 0:\n                        if not diff_size in ftofill_pointer:\n                            ftofill_pointer[diff_size] = []\n                        ftofill_pointer[diff_size].append((last_cid, g))\n                        ftofill.add(diff_size)\n        else:\n            #print \"Branch B\"\n            ksize = ftofill.pop()\n            c, g = ftofill_pointer[ksize].pop()\n            #print \"Assign to cluster %i group %i\" % (c, g)\n            fgrouped[c][g].append(fname)\n            nsize = ksize - fsize\n            if nsize > 0:\n                if not nsize in ftofill_pointer:\n                    ftofill_pointer[nsize] = []\n                ftofill_pointer[nsize].append((c, g))\n                ftofill.add(nsize)\n    return fgrouped"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_files_by_size_simple(fileslist, nbgroups):  # pragma: no cover\n    ford = sorted(fileslist.iteritems(), key=lambda x: x[1], reverse=True)\n    ford = [[x[0]] for x in ford]\n    return [group for group in grouper(nbgroups, ford)]", "response": "Simple and fast files grouping strategy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grouped_count_sizes(fileslist, fgrouped):  # pragma: no cover\n    '''Compute the total size per group and total number of files. Useful to check that everything is OK.'''\n    fsizes = {}\n    total_files = 0\n    allitems = None\n    if isinstance(fgrouped, dict):\n        allitems = fgrouped.iteritems()\n    elif isinstance(fgrouped, list):\n        allitems = enumerate(fgrouped)\n    for fkey, cluster in allitems:\n        fsizes[fkey] = []\n        for subcluster in cluster:\n            tot = 0\n            if subcluster is not None:\n                for fname in subcluster:\n                    tot += fileslist[fname]\n                    total_files += 1\n            fsizes[fkey].append(tot)\n    return fsizes, total_files", "response": "Compute the total size per group and total number of files. Useful to check that everything is OK."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetOptions(self):\n    values = [c.GetValue()\n              for c in chain(*self.widgets)\n              if c.GetValue() is not None]\n    return ' '.join(values)", "response": "Returns the collective values from all of the available widgets in the current panel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetValue(self):\n    '''\n    Positionals have no associated options_string,\n    so only the supplied arguments are returned.\n    The order is assumed to be the same as the order\n    of declaration in the client code\n\n    Returns\n      \"argument_value\"\n    '''\n    self.AssertInitialization('Positional')\n    if str(self._widget.GetValue()) == EMPTY:\n      return None\n    return self._widget.GetValue()", "response": "Returns the value of the argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of the choice argument", "response": "def GetValue(self):\n    '''\n    Returns\n      \"--option_name argument\"\n    '''\n    self.AssertInitialization('Choice')\n    if self._widget.GetValue() == self._DEFAULT_VALUE:\n      return None\n    return ' '.join(\n      [self._action.option_strings[0] if self._action.option_strings else '',  # get the verbose copy if available\n       self._widget.GetValue()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of the option.", "response": "def GetValue(self):\n    '''\n    General options are key/value style pairs (conceptually).\n    Thus the name of the option, as well as the argument to it\n    are returned\n    e.g.\n      >>> myscript --outfile myfile.txt\n    returns\n      \"--Option Value\"\n    '''\n    self.AssertInitialization('Optional')\n    value = self._widget.GetValue()\n    if not value or len(value) <= 0:\n      return None\n    return ' '.join(\n      [self._action.option_strings[0],  # get the verbose copy if available\n       value])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value of the option.", "response": "def GetValue(self):\n    '''\n    Flag options have no param associated with them.\n    Thus we only need the name of the option.\n    e.g\n      >>> Python -v myscript\n    returns\n      Options name for argument (-v)\n    '''\n    if not self._widget.GetValue() or len(self._widget.GetValue()) <= 0:\n        return None\n    else:\n        return self._action.option_strings[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Update(self, size):\n    '''\n    Custom wrapper calculator to account for the\n    increased size of the _msg widget after being\n    inlined with the wx.CheckBox\n    '''\n    if self._msg is None:\n      return\n    help_msg = self._msg\n    width, height = size\n    content_area = int((width / 3) * .70)\n\n    wiggle_room = range(int(content_area - content_area * .05), int(content_area + content_area * .05))\n    if help_msg.Size[0] not in wiggle_room:\n      self._msg.SetLabel(self._msg.GetLabelText().replace('\\n', ' '))\n      self._msg.Wrap(content_area)", "response": "Update the _msg widget with the given size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the DropDown option.", "response": "def GetValue(self):\n    '''\n    NOTE: Added on plane. Cannot remember exact implementation\n    of counter objects. I believe that they count sequentail\n    pairings of options\n    e.g.\n      -vvvvv\n    But I'm not sure. That's what I'm going with for now.\n\n    Returns\n      str(action.options_string[0]) * DropDown Value\n    '''\n    dropdown_value = self._widget.GetValue()\n    if not str(dropdown_value).isdigit():\n      return None\n    arg = str(self._action.option_strings[0]).replace('-', '')\n    repeated_args = arg * int(dropdown_value)\n    return '-' + repeated_args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the full path to the language file", "response": "def get_path(language):\n  ''' Returns the full path to the language file '''\n  filename = language.lower() + '.json'\n  lang_file_path = os.path.join(_DEFAULT_DIR, filename)\n  if not os.path.exists(lang_file_path):\n    raise IOError('Could not find {} language file'.format(language))\n  return lang_file_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen and return the supplied json file", "response": "def load(filename):\n  ''' Open and return the supplied json file '''\n  global _DICTIONARY\n  try:\n    json_file = filename + '.json'\n    with open(os.path.join(_DEFAULT_DIR, json_file), 'rb') as f:\n      _DICTIONARY = json.load(f)\n  except IOError:\n    raise IOError('Language file not found. Make sure that your ',\n                  'translation file is in the languages directory, ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_repr(obj, clip=None):\n    try:\n        s = repr(obj)\n        if not clip or len(s) <= clip:\n            return s\n        else:\n            return s[:clip-4]+'..'+s[-2:]\n    except:\n        return 'N/A'", "response": "Convert object to string representation yielding the same result a repr and returns N / A instead of raising the exception."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trunc(obj, max, left=0):\n    s = str(obj)\n    s = s.replace('\\n', '|')\n    if len(s) > max:\n        if left:\n            return '...'+s[len(s)-max+3:]\n        else:\n            return s[:(max-3)]+'...'\n    else:\n        return s", "response": "Convert obj to string eliminate newlines and truncate the string to max characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a friendly timestamp represented as a string.", "response": "def pp_timestamp(t):\n    \"\"\"\n    Get a friendly timestamp represented as a string.\n    \"\"\"\n    if t is None:\n        return ''\n    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60\n    return \"%02d:%02d:%05.2f\" % (h, m, s)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_stats(self, stream=None):\n        if not stream: # pragma: no cover\n            stream = sys.stdout\n        self.metadata.sort(key=lambda x: -x.size)\n        stream.write('%-10s %8s %-12s %-46s\\n' % ('id', 'size', 'type', 'representation'))\n        for g in self.metadata:\n            stream.write('0x%08x %8d %-12s %-46s\\n' % (g.id, g.size, trunc(g.type, 12),\n                trunc(g.str, 46)))\n        stream.write('Garbage: %8d collected objects (%s in cycles): %12s\\n' % \\\n            (self.count, self.num_in_cycles, pp(self.total_size)))", "response": "Log annotated garbage objects to console or file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of profiled file names.", "response": "def getFilenameSet(self):\n        \"\"\"\n        Returns a set of profiled file names.\n\n        Note: \"file name\" is used loosely here. See python documentation for\n        co_filename, linecache module and PEP302. It may not be a valid\n        filesystem path.\n        \"\"\"\n        result = set(self.file_dict)\n        # Ignore profiling code. __file__ does not always provide consistent\n        # results with f_code.co_filename (ex: easy_install with zipped egg),\n        # so inspect current frame instead.\n        # XXX: assumes all of pprofile code resides in a single file.\n        result.discard(inspect.currentframe().f_code.co_filename)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef annotate(self, out, filename=None, commandline=None, relative_path=False):\n        file_dict = self.file_dict\n        total_time = self.total_time\n        if commandline is not None:\n            print >> out, 'Command line:', commandline\n        print >> out, 'Total duration: %gs' % total_time\n        if not total_time:\n            return\n        def percent(value, scale):\n            if scale == 0:\n                return 0\n            return value * 100 / float(scale)\n        for name in self._getFileNameList(filename):\n            file_timing = file_dict[name]\n            file_total_time = file_timing.getTotalTime()\n            call_list_by_line = file_timing.getCallListByLine()\n            print >> out, 'File:', name\n            print >> out, 'File duration: %gs (%.2f%%)' % (file_total_time,\n                percent(file_total_time, total_time))\n            print >> out, _ANNOTATE_HEADER\n            print >> out, _ANNOTATE_HORIZONTAL_LINE\n            for lineno, _, _, hits, duration, line in self._iterFile(name,\n                    call_list_by_line):\n                if hits:\n                    time_per_hit = duration / hits\n                else:\n                    time_per_hit = 0\n                print >> out, _ANNOTATE_FORMAT % {\n                    'lineno': lineno,\n                    'hits': hits,\n                    'time': duration,\n                    'time_per_hit': time_per_hit,\n                    'percent': percent(duration, total_time),\n                    'line': line,\n                },\n                for _, _, hits, duration, callee_file, callee_line, \\\n                        callee_name in call_list_by_line.get(lineno, ()):\n                    print >> out, _ANNOTATE_CALL_FORMAT % {\n                        'hits': hits,\n                        'time': duration,\n                        'time_per_hit': duration / hits,\n                        'percent': percent(duration, total_time),\n                        'callee_file': callee_file,\n                        'callee_line': callee_line,\n                        'callee_name': callee_name,\n                    }", "response": "Dump annotated source code with current profiling statistics to out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting data to stdout and file.", "response": "def write(self, data, end=\"\\n\", flush=True):\n        \"\"\" Output data to stdout and/or file \"\"\"\n        if not self.nostdout:\n            self.stdout.write(data+end)\n        if self.file is not None:\n            self.file.write(data+end)\n        if flush:\n            self.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nforces commit changes to the file and stdout", "response": "def flush(self):\n        \"\"\" Force commit changes to the file and stdout \"\"\"\n        if not self.nostdout:\n            self.stdout.flush()\n        if self.file is not None:\n            self.file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the absolute Levenshtein distance between two sequences seq1 and seq2.", "response": "def levenshtein(seq1, seq2, normalized=False, max_dist=-1):\n\t\"\"\"Compute the absolute Levenshtein distance between the two sequences\n\t`seq1` and `seq2`.\n\t\n\tThe Levenshtein distance is the minimum number of edit operations necessary\n\tfor transforming one sequence into the other. The edit operations allowed are:\n\t\n\t\t* deletion:     ABC -> BC, AC, AB\n\t\t* insertion:    ABC -> ABCD, EABC, AEBC..\n\t\t* substitution: ABC -> ABE, ADC, FBC..\n\t\n\tThe `max_dist` parameter controls at which moment we should stop computing the\n\tdistance between the provided sequences. If it is a negative integer, the\n\tdistance will be computed until the sequences are exhausted; otherwise, the\n\tcomputation will stop at the moment the calculated distance is higher than\n\t`max_dist`, and then return -1. For example:\n\t\n\t\t>>> levenshtein(\"abc\", \"abcd\", max_dist=1)  # dist = 1\n\t\t1\n\t\t>>> levenshtein(\"abc\", \"abcde\", max_dist=1) # dist = 2\n\t\t-1\n\t\n\tThis can be a time saver if you're not interested in the exact distance, but\n\tonly need to check if the distance between the given sequences is below a\n\tgiven threshold.\n\t\n\tThe `normalized` parameter is here for backward compatibility; providing\n\tit will result in a call to `nlevenshtein`, which should be used directly\n\tinstead. \n\t\"\"\"\n\tif normalized:\n\t\treturn nlevenshtein(seq1, seq2, method=1)\n\t\t\n\tif seq1 == seq2:\n\t\treturn 0\n\t\n\tlen1, len2 = len(seq1), len(seq2)\n\tif max_dist >= 0 and abs(len1 - len2) > max_dist:\n\t\treturn -1\n\tif len1 == 0:\n\t\treturn len2\n\tif len2 == 0:\n\t\treturn len1\n\tif len1 < len2:\n\t\tlen1, len2 = len2, len1\n\t\tseq1, seq2 = seq2, seq1\n\t\n\tcolumn = array('L', range(len2 + 1))\n\t\n\tfor x in range(1, len1 + 1):\n\t\tcolumn[0] = x\n\t\tlast = x - 1\n\t\tfor y in range(1, len2 + 1):\n\t\t\told = column[y]\n\t\t\tcost = int(seq1[x - 1] != seq2[y - 1])\n\t\t\tcolumn[y] = min(column[y] + 1, column[y - 1] + 1, last + cost)\n\t\t\tlast = old\n\t\tif max_dist >= 0 and min(column) > max_dist:\n\t\t\treturn -1\n\t\n\tif max_dist >= 0 and column[len2] > max_dist:\n\t\t# stay consistent, even if we have the exact distance\n\t\treturn -1\n\treturn column[len2]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the normalized Levenshtein distance between seq1 and seq2.", "response": "def nlevenshtein(seq1, seq2, method=1):\n\t\"\"\"Compute the normalized Levenshtein distance between `seq1` and `seq2`.\n\t\n\tTwo normalization methods are provided. For both of them, the normalized\n\tdistance will be a float between 0 and 1, where 0 means equal and 1\n\tcompletely different. The computation obeys the following patterns:\n\t\n\t\t0.0                       if seq1 == seq2\n\t\t1.0                       if len(seq1) == 0 or len(seq2) == 0\n\t\tedit distance / factor    otherwise\n\t\n\tThe `method` parameter specifies which normalization factor should be used.\n\tIt can have the value 1 or 2, which correspond to the following:\n\t\n\t\t1: the length of the shortest alignment between the sequences\n\t\t   (that is, the length of the longest sequence)\n\t\t2: the length of the longest alignment between the sequences\n\t\n\tWhich normalization factor should be chosen is a matter of taste. The first\n\tone is cheap to compute. The second one is more costly, but it accounts\n\tbetter than the first one for parallelisms of symbols between the sequences.\n\t\t\n\tFor the rationale behind the use of the second method, see:\n\tHeeringa, \"Measuring Dialect Pronunciation Differences using Levenshtein\n\tDistance\", 2004, p. 130 sq, which is available online at:\n\thttp://www.let.rug.nl/~heeringa/dialectology/thesis/thesis.pdf\n\t\"\"\"\n\t\n\tif seq1 == seq2:\n\t\treturn 0.0\n\tlen1, len2 = len(seq1), len(seq2)\n\tif len1 == 0 or len2 == 0:\n\t\treturn 1.0\n\tif len1 < len2: # minimize the arrays size\n\t\tlen1, len2 = len2, len1\n\t\tseq1, seq2 = seq2, seq1\n\t\n\tif method == 1:\n\t\treturn levenshtein(seq1, seq2) / float(len1)\n\tif method != 2:\n\t\traise ValueError(\"expected either 1 or 2 for `method` parameter\")\n\t\n\tcolumn = array('L', range(len2 + 1))\n\tlength = array('L', range(len2 + 1))\n\t\n\tfor x in range(1, len1 + 1):\n\t\n\t\tcolumn[0] = length[0] = x\n\t\tlast = llast = x - 1\n\t\t\n\t\tfor y in range(1, len2 + 1):\n\t\t\n\t\t\t# dist\n\t\t\told = column[y]\n\t\t\tic = column[y - 1] + 1\n\t\t\tdc = column[y] + 1\n\t\t\trc = last + (seq1[x - 1] != seq2[y - 1])\n\t\t\tcolumn[y] = min(ic, dc, rc)\n\t\t\tlast = old\n\t\t\t\n\t\t\t# length\n\t\t\tlold = length[y]\n\t\t\tlic = length[y - 1] + 1 if ic == column[y] else 0\n\t\t\tldc = length[y] + 1 if dc == column[y] else 0\n\t\t\tlrc = llast + 1 if rc == column[y] else 0\n\t\t\tlength[y] = max(ldc, lic, lrc)\n\t\t\tllast = lold\n\t\n\treturn column[y] / float(length[y])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining all parents of a node in our tree", "response": "def parents(self, node):\n        \"\"\"Determine all parents of node in our tree\"\"\"\n        return [\n            parent for parent in\n            getattr( node, 'parents', [] )\n            if getattr(parent, 'tree', self.TREE) == self.TREE\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting string reference to object. Stores a weak reference in a dictionary using the object s id as the key.", "response": "def get_ref(obj):\n    \"\"\"\n    Get string reference to object. Stores a weak reference in a dictionary\n    using the object's id as the key. If the object cannot be weakly\n    referenced (e.g. dictionaries, frame objects), store a strong references\n    in a classic dictionary.\n\n    Returns the object's id as a string.\n    \"\"\"\n    oid = id(obj)\n    try:\n        server.id2ref[oid] = obj\n    except TypeError:\n        server.id2obj[oid] = obj\n    return str(oid)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_obj(ref):\n    oid = int(ref)\n    return server.id2ref.get(oid) or server.id2obj[oid]", "response": "Get object from string reference."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tracker_class(clsname):\n    stats = server.stats\n    if not stats:\n        bottle.redirect('/tracker')\n    stats.annotate()\n    return dict(stats=stats, clsname=clsname)", "response": "Get class instance details."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting reference cycle details.", "response": "def garbage_cycle(index):\n    \"\"\"Get reference cycle details.\"\"\"\n    graph = _compute_garbage_graphs()[int(index)]\n    graph.reduce_to_cycles()\n    objects = graph.metadata\n    objects.sort(key=lambda x: -x.size)\n    return dict(objects=objects, index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_graph(graph, filename):\n    try:\n        rendered = graph.rendered_file\n    except AttributeError:\n        try:\n            graph.render(os.path.join(server.tmpdir, filename), format='png')\n            rendered = filename\n        except OSError:\n            rendered = None\n    graph.rendered_file = rendered\n    return rendered", "response": "Retrieve or render a graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef garbage_graph(index):\n    graph = _compute_garbage_graphs()[int(index)]\n    reduce_graph = bottle.request.GET.get('reduce', '')\n    if reduce_graph:\n        graph = graph.reduce_to_cycles()\n    if not graph:\n        return None\n    filename = 'garbage%so%s.png' % (index, reduce_graph)\n    rendered_file = _get_graph(graph, filename)\n    if rendered_file:\n        bottle.send_file(rendered_file, root=server.tmpdir)\n    else:\n        return None", "response": "Get graph representation of reference cycle."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_profiler(host='localhost', port=8090, tracker=None, stats=None,\n                   debug=False, **kwargs):\n    \"\"\"\n    Start the web server to show profiling data. The function suspends the\n    Python application (the current thread) until the web server is stopped.\n\n    The only way to stop the server is to signal the running thread, e.g. press\n    Ctrl+C in the console. If this isn't feasible for your application use\n    `start_in_background` instead.\n\n    During the execution of the web server, profiling data is (lazily) cached\n    to improve performance. For example, garbage graphs are rendered when the\n    garbage profiling data is requested and are simply retransmitted upon later\n    requests.\n\n    :param host: the host where the server shall run, default is localhost\n    :param port: server listens on the specified port, default is 8090 to allow\n        coexistance with common web applications\n    :param tracker: `ClassTracker` instance, browse profiling data (on-line\n        analysis)\n    :param stats: `Stats` instance, analyze `ClassTracker` profiling dumps\n        (useful for off-line analysis)\n    \"\"\"\n    if tracker and not stats:\n        server.stats = tracker.stats\n    else:\n        server.stats = stats\n    try:\n        server.tmpdir = mkdtemp(prefix='pympler')\n        server.server = PymplerServer(host=host, port=port, **kwargs)\n        bottle.debug(debug)\n        bottle.run(server=server.server)\n    finally:\n        rmtree(server.tmpdir)", "response": "Start the profiling server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _winreg_getShellFolder( name ):\n    k = _winreg.OpenKey(\n        _winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n    )\n    try:\n        # should check that it's valid? How?\n        return _winreg.QueryValueEx( k, name )[0]\n    finally:\n        _winreg.CloseKey( k )", "response": "Get a shell folder by string name from the registry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef appdatadirectory(  ):\n    if shell:\n        # on Win32 and have Win32all extensions, best-case\n        return shell_getShellFolder(shellcon.CSIDL_APPDATA)\n    if _winreg:\n        # on Win32, but no Win32 shell com available, this uses\n        # a direct registry access, likely to fail on Win98/Me\n        return _winreg_getShellFolder( 'AppData' )\n    # okay, what if for some reason _winreg is missing? would we want to allow ctypes?\n    ## default case, look for name in environ...\n    for name in ['APPDATA', 'HOME']:\n        if name in os.environ:\n            return os.path.join( os.environ[name], '.config' )\n    # well, someone's being naughty, see if we can get ~ to expand to a directory...\n    possible = os.path.abspath(os.path.expanduser( '~/.config' ))\n    if os.path.exists( possible ):\n        return possible\n    raise OSError( \"\"\"Unable to determine user's application-data directory, no ${HOME} or ${APPDATA} in environment\"\"\" )", "response": "Attempt to retrieve the current user s application - data directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all known objects excluding frame objects.", "response": "def get_objects(remove_dups=True, include_frames=False):\n    \"\"\"Return a list of all known objects excluding frame objects.\n\n    If (outer) frame objects shall be included, pass `include_frames=True`.  In\n    order to prevent building reference cycles, the current frame object (of\n    the caller of get_objects) is ignored. This will not prevent creating\n    reference cycles if the object list is passed up the call-stack. Therefore,\n    frame objects are not included by default.\n\n    Keyword arguments:\n    remove_dups -- if True, all duplicate objects will be removed.\n    include_frames -- if True, includes frame objects.\n    \"\"\"\n    gc.collect()\n\n    # Do not initialize local variables before calling gc.get_objects or those\n    # will be included in the list. Furthermore, ignore frame objects to\n    # prevent reference cycles.\n    tmp = gc.get_objects()\n    tmp = [o for o in tmp if not isframe(o)]\n\n    res = []\n    for o in tmp:\n        # gc.get_objects returns only container objects, but we also want\n        # the objects referenced by them\n        refs = get_referents(o)\n        for ref in refs:\n            if not _is_containerobject(ref):\n                # we already got the container objects, now we only add\n                # non-container objects\n                res.append(ref)\n    res.extend(tmp)\n    if remove_dups:\n        res = _remove_duplicates(res)\n\n    if include_frames:\n        for sf in stack()[2:]:\n            res.append(sf[0])\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the total size of all elements in objects.", "response": "def get_size(objects):\n    \"\"\"Compute the total size of all elements in objects.\"\"\"\n    res = 0\n    for o in objects:\n        try:\n            res += _getsizeof(o)\n        except AttributeError:\n            print(\"IGNORING: type=%s; o=%s\" % (str(type(o)), str(o)))\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_diff(left, right):\n    res = {'+': [], '-': []}\n\n    def partition(objects):\n        \"\"\"Partition the passed object list.\"\"\"\n        res = {}\n        for o in objects:\n            t = type(o)\n            if type(o) not in res:\n                res[t] = []\n            res[t].append(o)\n        return res\n\n    def get_not_included(foo, bar):\n        \"\"\"Compare objects from foo with objects defined in the values of\n        bar (set of partitions).\n        Returns a list of all objects included in list, but not dict values.\n        \"\"\"\n        res = []\n        for o in foo:\n            if not compat.object_in_list(type(o), bar):\n                res.append(o)\n            elif not compat.object_in_list(o, bar[type(o)]):\n                res.append(o)\n        return res\n\n    # Create partitions of both lists. This will reduce the time required for\n    # the comparison\n    left_objects = partition(left)\n    right_objects = partition(right)\n    # and then do the diff\n    res['+'] = get_not_included(right, left_objects)\n    res['-'] = get_not_included(left, right_objects)\n    return res", "response": "Get the difference of two lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering objects by type minimum size and maximum size.", "response": "def filter(objects, Type=None, min=-1, max=-1): #PYCHOK muppy filter\n    \"\"\"Filter objects.\n\n    The filter can be by type, minimum size, and/or maximum size.\n\n    Keyword arguments:\n    Type -- object type to filter by\n    min -- minimum object size\n    max -- maximum object size\n\n    \"\"\"\n    res = []\n    if min > max:\n        raise ValueError(\"minimum must be smaller than maximum\")\n    if Type is not None:\n        res = [o for o in objects if isinstance(o, Type)]\n    if min > -1:\n        res = [o for o in res if _getsizeof(o) < min]\n    if max > -1:\n        res = [o for o in res if _getsizeof(o) > max]\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all referents of an object up to a certain level.", "response": "def get_referents(object, level=1):\n    \"\"\"Get all referents of an object up to a certain level.\n\n    The referents will not be returned in a specific order and\n    will not contain duplicate objects. Duplicate objects will be removed.\n\n    Keyword arguments:\n    level -- level of indirection to which referents considered.\n\n    This function is recursive.\n\n    \"\"\"\n    res = gc.get_referents(object)\n    level -= 1\n    if level > 0:\n        for o in res:\n            res.extend(get_referents(o, level))\n    res = _remove_duplicates(res)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_usage(function, *args):\n    # The usage of a function is calculated by creating one summary of all\n    # objects before the function is invoked and afterwards. These summaries\n    # are compared and the diff is returned.\n    # This function works in a 2-steps process. Before the actual function is\n    # invoked an empty dummy function is measurement to identify the overhead\n    # involved in the measuring process. This overhead then is subtracted from\n    # the measurement performed on the passed function. The result reflects the\n    # actual usage of a function call.\n    # Also, a measurement is performed twice, allowing the adjustment to\n    # initializing things, e.g. modules\n\n    res = None\n\n    def _get_summaries(function, *args):\n        \"\"\"Get a 2-tuple containing one summary from before, and one summary\n        from after the function has been invoked.\n\n        \"\"\"\n        s_before = summary.summarize(get_objects())\n        function(*args)\n        s_after = summary.summarize(get_objects())\n        return (s_before, s_after)\n\n    def _get_usage(function, *args):\n        \"\"\"Get the usage of a function call.\n        This function is to be used only internally. The 'real' get_usage\n        function is a wrapper around _get_usage, but the workload is done\n        here.\n\n        \"\"\"\n        res = []\n        # init before calling\n        (s_before, s_after) = _get_summaries(function, *args)\n        # ignore all objects used for the measurement\n        ignore = []\n        if s_before != s_after:\n            ignore.append(s_before)\n        for row in s_before:\n            # ignore refs from summary and frame (loop)\n            if len(gc.get_referrers(row)) == 2:\n                ignore.append(row)\n            for item in row:\n                # ignore refs from summary and frame (loop)\n                if len(gc.get_referrers(item)) == 2:\n                    ignore.append(item)\n        for o in ignore:\n            s_after = summary._subtract(s_after, o)\n        res = summary.get_diff(s_before, s_after)\n        return summary._sweep(res)\n\n    # calibrate; twice for initialization\n    def noop(): pass\n    offset = _get_usage(noop)\n    offset = _get_usage(noop)\n    # perform operation twice to handle objects possibly used in\n    # initialisation\n    tmp = _get_usage(function, *args)\n    tmp = _get_usage(function, *args)\n    tmp = summary.get_diff(offset, tmp)\n    tmp = summary._sweep(tmp)\n    if len(tmp) != 0:\n        res = tmp\n    return res", "response": "This function is used only internally. It is used by the real get_usage function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _remove_duplicates(objects):\n    seen = {}\n    result = []\n    for item in objects:\n        marker = id(item)\n        if marker in seen:\n            continue\n        seen[marker] = 1\n        result.append(item)\n    return result", "response": "Remove duplicate objects.\n\n    Inspired by http://www.peterbe.com/plog/uniqifiers-benchmark"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_optionals_without_choices(self, actions):\n    boolean_actions = (\n      _StoreConstAction, _StoreFalseAction,\n      _StoreTrueAction\n    )\n    return [action\n            for action in actions\n            if action.option_strings\n            and not action.choices\n            and not isinstance(action, _CountAction)\n            and not isinstance(action, _HelpAction)\n            and type(action) not in boolean_actions]", "response": "Returns a list of all actions which are not optional but without required choices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all instances of flag type options.", "response": "def get_flag_style_optionals(self, actions):\n    \"\"\"\n    Gets all instances of \"flag\" type options.\n    i.e. options which either store a const, or\n    store boolean style options (e.g. StoreTrue).\n    Types:\n      _StoreTrueAction\n      _StoreFalseAction\n      _StoreConst\n    \"\"\"\n    return [action\n            for action in actions\n            if isinstance(action, _StoreTrueAction)\n            or isinstance(action, _StoreFalseAction)\n            or isinstance(action, _StoreConstAction)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resize_bitmap(parent, _bitmap, target_height):\n  '''\n  Resizes a bitmap to a height of 89 pixels (the\n  size of the top panel), while keeping aspect ratio\n  in tact\n  '''\n  image = wx.ImageFromBitmap(_bitmap)\n  _width, _height = image.GetSize()\n  if _height < target_height:\n    return wx.StaticBitmap(parent, -1, wx.BitmapFromImage(image))\n  ratio = float(_width) / _height\n  image = image.Scale(target_height * ratio, target_height, wx.IMAGE_QUALITY_HIGH)\n  return wx.StaticBitmap(parent, -1, wx.BitmapFromImage(image))", "response": "Resizes a bitmap to a height of 89 pixels while keeping aspect ratio\n in tact\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ilevenshtein(seq1, seqs, max_dist=-1):\n\tfor seq2 in seqs:\n\t\tdist = levenshtein(seq1, seq2, max_dist=max_dist)\n\t\tif dist != -1:\n\t\t\tyield dist, seq2", "response": "Compute the Levenshtein distance between the sequence seq1 and the series\n\tsequences seqs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an iterator over all the sequences in seqs which distance from the seq1 is lower or equal to 2.", "response": "def ifast_comp(seq1, seqs, transpositions=False):\n\t\"\"\"Return an iterator over all the sequences in `seqs` which distance from\n\t`seq1` is lower or equal to 2. The sequences which distance from the\n\treference sequence is higher than that are dropped.\n\t\n\t\t`seq1`: the reference sequence.\n\t\t`seqs`: a series of sequences (can be a generator)\n\t\t`transpositions` has the same sense than in `fast_comp`.\n\t\n\tThe return value is a series of pairs (distance, sequence).\n\t\n\tYou might want to call `sorted()` on the iterator to get the results in a\n\tsignificant order:\n\t\n\t\t>>> g = ifast_comp(\"foo\", [\"fo\", \"bar\", \"foob\", \"foo\", \"foobaz\"])\n\t\t>>> sorted(g)\n\t\t[(0, 'foo'), (1, 'fo'), (1, 'foob')]\n\t\"\"\"\n\tfor seq2 in seqs:\n\t\tdist = fast_comp(seq1, seq2, transpositions)\n\t\tif dist != -1:\n\t\t\tyield dist, seq2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summarize(objects):\n    count = {}\n    total_size = {}\n    for o in objects:\n        otype = _repr(o)\n        if otype in count:\n            count[otype] += 1\n            total_size[otype] += _getsizeof(o)\n        else:\n            count[otype] = 1\n            total_size[otype] = _getsizeof(o)\n    rows = []\n    for otype in count:\n        rows.append([otype, count[otype], total_size[otype]])\n    return rows", "response": "Summarize an objects list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the difference of two summaries.", "response": "def get_diff(left, right):\n    \"\"\"Get the difference of two summaries.\n\n    Subtracts the values of the right summary from the values of the left\n    summary.\n    If similar rows appear on both sides, the are included in the summary with\n    0 for number of elements and total size.\n    If the number of elements of a row of the diff is 0, but the total size is\n    not, it means that objects likely have changed, but not there number, thus\n    resulting in a changed size.\n\n    \"\"\"\n    res = []\n    for row_r in right:\n        found = False\n        for row_l in left:\n            if row_r[0] == row_l[0]:\n                res.append([row_r[0], row_r[1] - row_l[1], row_r[2] - row_l[2]])\n                found = True\n        if not found:\n            res.append(row_r)\n\n    for row_l in left:\n        found = False\n        for row_r in right:\n            if row_l[0] == row_r[0]:\n                found = True\n        if not found:\n            res.append([row_l[0], -row_l[1], -row_l[2]])\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_(rows, limit=15, sort='size', order='descending'):\n    localrows = []\n    for row in rows:\n        localrows.append(list(row))\n    # input validation\n    sortby = ['type', '#', 'size']\n    if sort not in sortby:\n        raise ValueError(\"invalid sort, should be one of\" + str(sortby))\n    orders = ['ascending', 'descending']\n    if order not in orders:\n        raise ValueError(\"invalid order, should be one of\" + str(orders))\n    # sort rows\n    if sortby.index(sort) == 0:\n        if order == \"ascending\":\n            localrows.sort(key=lambda x: _repr(x[0]))\n        elif order == \"descending\":\n            localrows.sort(key=lambda x: _repr(x[0]), reverse=True)\n    else:\n        if order == \"ascending\":\n            localrows.sort(key=lambda x: x[sortby.index(sort)])\n        elif order == \"descending\":\n            localrows.sort(key=lambda x: x[sortby.index(sort)], reverse=True)\n    # limit rows\n    localrows = localrows[0:limit]\n    for row in localrows:\n        row[2] = stringutils.pp(row[2])\n    # print rows\n    localrows.insert(0,[\"types\", \"# objects\", \"total size\"])\n    _print_table(localrows)", "response": "Print the rows as a summary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint a list of lists as a pretty table.", "response": "def _print_table(rows, header=True):\n    \"\"\"Print a list of lists as a pretty table.\n\n    Keyword arguments:\n    header -- if True the first row is treated as a table header\n\n    inspired by http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/267662\n    \"\"\"\n    border = \"=\"\n    # vertical delimiter\n    vdelim = \" | \"\n    # padding nr. of spaces are left around the longest element in the\n    # column\n    padding = 1\n    # may be left,center,right\n    justify = 'right'\n    justify = {'left'   : str.ljust,\n               'center' : str.center,\n               'right'  : str.rjust}[justify.lower()]\n    # calculate column widths (longest item in each col\n    # plus \"padding\" nr of spaces on both sides)\n    cols = zip(*rows)\n    colWidths = [max([len(str(item))+2*padding for item in col]) for col in cols]\n    borderline = vdelim.join([w*border for w in colWidths])\n    for row in rows:\n        print(vdelim.join([justify(str(item),width) for (item,width) in zip(row,colWidths)]))\n        if header:\n            print(borderline)\n            header=False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the string representation of the object o.", "response": "def _repr(o, verbosity=1):\n    \"\"\"Get meaning object representation.\n\n    This function should be used when the simple str(o) output would result in\n    too general data. E.g. \"<type 'instance'\" is less meaningful than\n    \"instance: Foo\".\n\n    Keyword arguments:\n    verbosity -- if True the first row is treated as a table header\n\n    \"\"\"\n    res = \"\"\n\n    t = type(o)\n    if (verbosity == 0) or (t not in representations):\n        res = str(t)\n    else:\n        verbosity -= 1\n        if len(representations[t]) < verbosity:\n            verbosity = len(representations[t]) - 1\n        res = representations[t][verbosity](o)\n\n    res = address.sub('', res)\n    res = type_prefix.sub('', res)\n    res = type_suffix.sub('', res)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntraverse all objects of a summary and call function with each as a parameter.", "response": "def _traverse(summary, function, *args):\n    \"\"\"Traverse all objects of a summary and call function with each as a\n    parameter.\n\n    Using this function, the following objects will be traversed:\n    - the summary\n    - each row\n    - each item of a row\n    \"\"\"\n    function(summary, *args)\n    for row in summary:\n        function(row, *args)\n        for item in row:\n            function(item, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove object o from the summary by subtracting it s size.", "response": "def _subtract(summary, o):\n    \"\"\"Remove object o from the summary by subtracting it's size.\"\"\"\n    found = False\n    row = [_repr(o), 1, _getsizeof(o)]\n    for r in summary:\n        if r[0] == row[0]:\n            (r[1], r[2]) = (r[1] - row[1], r[2] - row[2])\n            found = True\n    if not found:\n        summary.append([row[0], -row[1], -row[2]])\n    return summary"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the file is okay returns False if the file is not okay None if the file is corrupt", "response": "def check_structure(filepath):\n    \"\"\"Returns False if the file is okay, None if file format is unsupported by PIL/PILLOW, or returns an error string if the file is corrupt.\"\"\"\n    #http://stackoverflow.com/questions/1401527/how-do-i-programmatically-check-whether-an-image-png-jpeg-or-gif-is-corrupted/1401565#1401565\n    \n    # Check structure only for images (not supported for other types currently)\n    if filepath.lower().endswith(tuple(img_filter)):\n        try:\n            #try:\n            im = PIL.Image.open(filepath)\n            #except IOError: # File format not supported by PIL, we skip the check_structure - ARG this is also raised if a supported image file is corrupted...\n                #print(\"File: %s: DETECTNOPE\" % filepath)\n                #return None\n            im.verify()\n        # If an error occurred, the structure is corrupted\n        except Exception as e:\n            return str(e)\n        # Else no exception, there's no corruption\n        return False\n    # Else the format does not currently support structure checking, we just return None to signal we didin't check\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates several hashes at a single sweep of the file.", "response": "def generate_hashes(filepath, blocksize=65536):\n    '''Generate several hashes (md5 and sha1) in a single sweep of the file. Using two hashes lowers the probability of collision and false negative (file modified but the hash is the same). Supports big files by streaming blocks by blocks to the hasher automatically. Blocksize can be any multiple of 128.'''\n    # Init hashers\n    hasher_md5 = hashlib.md5()\n    hasher_sha1 = hashlib.sha1()\n    # Read the file blocks by blocks\n    with open(filepath, 'rb') as afile:\n        buf = afile.read(blocksize)\n        while len(buf) > 0:\n            # Compute both hashes at the same time\n            hasher_md5.update(buf)\n            hasher_sha1.update(buf)\n            # Load the next data block from file\n            buf = afile.read(blocksize)\n    return (hasher_md5.hexdigest(), hasher_sha1.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the degree of the polynomial", "response": "def get_degree(self, poly=None):\n        '''Returns the degree of the polynomial'''\n        if not poly:\n            return self.degree\n            #return len(self.coefficients) - 1\n        elif poly and hasattr(\"coefficients\", poly):\n            return len(poly.coefficients) - 1\n        else:\n            while poly and poly[-1] == 0:\n                poly.pop()   # normalize\n            return len(poly)-1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the multiplication between two polynomials only at the specified coefficient", "response": "def mul_at(self, other, k):\n        '''Compute the multiplication between two polynomials only at the specified coefficient (this is a lot cheaper than doing the full polynomial multiplication and then extract only the required coefficient)'''\n        if k > (self.degree + other.degree) or k > self.degree: return 0 # optimization: if the required coefficient is above the maximum coefficient of the resulting polynomial, we can already predict that and just return 0\n\n        term = 0\n\n        for i in _range(min(len(self), len(other))):\n            coef1 = self.coefficients[-(k-i+1)]\n            coef2 = other.coefficients[-(i+1)]\n            if coef1 == 0 or coef2 == 0: continue # log(0) is undefined, skip (and in addition it's a nice optimization)\n            term += coef1 * coef2\n        return term"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmultiplies a polynomial with a scalar", "response": "def scale(self, scalar):\n        '''Multiply a polynomial with a scalar'''\n        return self.__class__([self.coefficients[i] * scalar for i in _range(len(self))])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfasting division by using Extended Synthetic Division", "response": "def _fastdivmod(dividend, divisor):\n        '''Fast polynomial division by using Extended Synthetic Division (aka Horner's method). Also works with non-monic polynomials.\n        A nearly exact same code is explained greatly here: http://research.swtch.com/field and you can also check the Wikipedia article and the Khan Academy video.'''\n        # Note: for RS encoding, you should supply divisor = mprime (not m, you need the padded message)\n        msg_out = list(dividend) # Copy the dividend\n        normalizer = divisor[0] # precomputing for performance\n        for i in _range(len(dividend)-(len(divisor)-1)):\n            msg_out[i] /= normalizer # for general polynomial division (when polynomials are non-monic), the usual way of using synthetic division is to divide the divisor g(x) with its leading coefficient (call it a). In this implementation, this means:we need to compute: coef = msg_out[i] / gen[0]. For more infos, see http://en.wikipedia.org/wiki/Synthetic_division\n            coef = msg_out[i] # precaching\n            if coef != 0: # log(0) is undefined, so we need to avoid that case explicitly (and it's also a good optimization)\n                for j in _range(1, len(divisor)): # in synthetic division, we always skip the first coefficient of the divisior, because it's only used to normalize the dividend coefficient\n                    if divisor[j] != 0: # log(0) is undefined so we need to avoid that case\n                        msg_out[i + j] += -divisor[j] * coef\n\n        # The resulting msg_out contains both the quotient and the remainder, the remainder being the size of the divisor (the remainder has necessarily the same degree as the divisor -- not length but degree == length-1 -- since it's what we couldn't divide from the dividend), so we compute the index where this separation is, and return the quotient and remainder.\n        separator = -(len(divisor)-1)\n        return Polynomial(msg_out[:separator]), Polynomial(msg_out[separator:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfast polynomial division by using Extended Synthetic Division and optimized for GF2int computations.", "response": "def _gffastdivmod(dividend, divisor):\n        '''Fast polynomial division by using Extended Synthetic Division and optimized for GF(2^p) computations (so it is not generic, must be used with GF2int).\n        Transposed from the reedsolomon library: https://github.com/tomerfiliba/reedsolomon\n        BEWARE: it works only for monic divisor polynomial! (which is always the case with Reed-Solomon's generator polynomials)'''\n\n        msg_out = list(dividend) # Copy the dividend list and pad with 0 where the ecc bytes will be computed\n        for i in _range(len(dividend)-(len(divisor)-1)):\n            coef = msg_out[i] # precaching\n            if coef != 0: # log(0) is undefined, so we need to avoid that case explicitly (and it's also a good optimization)\n                for j in _range(1, len(divisor)): # in synthetic division, we always skip the first coefficient of the divisior, because it's only used to normalize the dividend coefficient (which is here useless since the divisor, the generator polynomial, is always monic)\n                    #if divisor[j] != 0: # log(0) is undefined so we need to check that, but it slow things down in fact and it's useless in our case (reed-solomon encoding) since we know that all coefficients in the generator are not 0\n                    msg_out[i + j] ^= divisor[j] * coef # equivalent to the more mathematically correct (but xoring directly is faster): msg_out[i + j] += -divisor[j] * coef\n                    # Note: we could speed things up a bit if we could inline the table lookups, but the Polynomial class is generic, it doesn't know anything about the underlying fields and their operators. Good OOP design, bad for performances in Python because of function calls and the optimizations we can't do (such as precomputing gf_exp[divisor]). That's what is done in reedsolo lib, this is one of the reasons it is faster.\n\n        # The resulting msg_out contains both the quotient and the remainder, the remainder being the size of the divisor (the remainder has necessarily the same degree as the divisor -- not length but degree == length-1 -- since it's what we couldn't divide from the dividend), so we compute the index where this separation is, and return the quotient and remainder.\n        separator = -(len(divisor)-1)\n        return Polynomial(msg_out[:separator]), Polynomial(msg_out[separator:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate(self, x):\n        '''Evaluate this polynomial at value x, returning the result (which is the sum of all evaluations at each term).'''\n        # Holds the sum over each term in the polynomial\n        #c = 0\n\n        # Holds the current power of x. This is multiplied by x after each term\n        # in the polynomial is added up. Initialized to x^0 = 1\n        #p = 1\n\n        #for term in self.coefficients[::-1]:\n        #    c = c + term * p\n        #    p = p * x\n        #return c\n\n        # Faster alternative using Horner's Scheme\n        y = self[0]\n        for i in _range(1, len(self)):\n            y = y * x + self.coefficients[i]\n        return y", "response": "Evaluate this polynomial at value x returning the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the formal derivative of the polynomial", "response": "def derive(self):\n        '''Compute the formal derivative of the polynomial: sum(i*coeff[i] x^(i-1))'''\n        #res = [0] * (len(self)-1) # pre-allocate the list, it will be one item shorter because the constant coefficient (x^0) will be removed\n        #for i in _range(2, len(self)+1): # start at 2 to skip the first coeff which is useless since it's a constant (x^0) so we +1, and because we work in reverse (lower coefficients are on the right) so +1 again\n            #res[-(i-1)] = (i-1) * self[-i] # self[-i] == coeff[i] and i-1 is the x exponent (eg: x^1, x^2, x^3, etc.)\n        #return Polynomial(res)\n\n        # One liner way to do it (also a bit faster too)\n        #return Polynomial( [(i-1) * self[-i] for i in _range(2, len(self)+1)][::-1] )\n        # Another faster version\n        L = len(self)-1\n        return Polynomial( [(L-i) * self[i] for i in _range(0, len(self)-1)] )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngeneralizing feature scaling (useful for variable error correction rate calculation)", "response": "def feature_scaling(x, xmin, xmax, a=0, b=1):\n    '''Generalized feature scaling (useful for variable error correction rate calculation)'''\n    return a + float(x - xmin) * (b - a) / (xmax - xmin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef entry_fields(file, entry_pos, field_delim=\"\\xFF\"):\n    '''From an ecc entry position (a list with starting and ending positions), extract the metadata fields (filename, filesize, ecc for both), and the starting/ending positions of the ecc stream (containing variably encoded blocks of hash and ecc per blocks of the original file's header)'''\n    # Read the the beginning of the ecc entry\n    blocksize = 65535\n    file.seek(entry_pos[0])\n    entry = file.read(blocksize)\n    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).\n    # TODO: do in a while loop in case the filename is really big (bigger than blocksize) - or in case we add intra-ecc for filename\n\n    # Find metadata fields delimiters positions\n    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)\n    first = entry.find(field_delim)\n    second = entry.find(field_delim, first+len(field_delim))\n    third = entry.find(field_delim, second+len(field_delim))\n    fourth = entry.find(field_delim, third+len(field_delim))\n    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.\n\n    # Extract the content of the fields\n    # Metadata fields\n    relfilepath = entry[:first]\n    filesize = entry[first+len(field_delim):second]\n    relfilepath_ecc = entry[second+len(field_delim):third]\n    filesize_ecc = entry[third+len(field_delim):fourth]\n    # Ecc stream field (aka ecc blocks)\n    ecc_field_pos = [entry_pos[0]+fourth+len(field_delim), entry_pos[1]] # return the starting and ending position of the rest of the ecc track, which contains blocks of hash/ecc of the original file's content.\n\n    # Place the cursor at the beginning of the ecc_field\n    file.seek(ecc_field_pos[0])\n\n    # Try to convert to an int, an error may happen\n    try:\n        filesize = int(filesize)\n    except Exception, e:\n        print(\"Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: \")\n        print(e)\n        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc\n\n    # entries = [ {\"message\":, \"ecc\":, \"hash\":}, etc.]\n    return {\"relfilepath\": relfilepath, \"relfilepath_ecc\": relfilepath_ecc, \"filesize\": filesize, \"filesize_ecc\": filesize_ecc, \"ecc_field_pos\": ecc_field_pos}", "response": "Extract the metadata fields from an ecc entry position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stream_entry_assemble(hasher, file, eccfile, entry_fields, max_block_size, header_size, resilience_rates, constantmode=False):\n    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''\n    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block\n    eccfile.seek(entry_fields[\"ecc_field_pos\"][0])\n    curpos = file.tell()\n    ecc_curpos = eccfile.tell()\n    while (ecc_curpos < entry_fields[\"ecc_field_pos\"][1]): # continue reading the input file until we reach the position of the previously detected ending marker\n        # Compute the current rate, depending on where we are inside the input file (headers? later stage?)\n        if curpos < header_size or constantmode: # header stage: constant rate\n            rate = resilience_rates[0]\n        else: # later stage 2 or 3: progressive rate\n            rate = feature_scaling(curpos, header_size, entry_fields[\"filesize\"], resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)\n        # From the rate, compute the ecc parameters\n        ecc_params = compute_ecc_params(max_block_size, rate, hasher)\n        # Extract the message block from input file, given the computed ecc parameters\n        mes = file.read(ecc_params[\"message_size\"])\n        if len(mes) == 0: return # quit if message is empty (reached end-of-file), this is a safeguard if ecc pos ending was miscalculated (we thus only need the starting position to be correct)\n        buf = eccfile.read(ecc_params[\"hash_size\"]+ecc_params[\"ecc_size\"])\n        hash = buf[:ecc_params[\"hash_size\"]]\n        ecc = buf[ecc_params[\"hash_size\"]:]\n\n        yield {\"message\": mes, \"hash\": hash, \"ecc\": ecc, \"rate\": rate, \"ecc_params\": ecc_params, \"curpos\": curpos, \"ecc_curpos\": ecc_curpos}\n        # Prepare for the next iteration of the loop\n        curpos = file.tell()\n        ecc_curpos = eccfile.tell()\n    # Just a quick safe guard against ecc ending marker misdetection\n    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file\n    size = file.tell()\n    if curpos < size: print(\"WARNING: end of ecc track reached but not the end of file! Either the ecc ending marker was misdetected, or either the file hash changed! Some blocks maybe may not have been properly checked!\")", "response": "Assemble a list of each entry from the original file along with the relative hash and ecc for easy processing later."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a stream of hash and ecc blocks of variable encoding rate and size given a file.", "response": "def stream_compute_ecc_hash(ecc_manager, hasher, file, max_block_size, header_size, resilience_rates):\n    '''Generate a stream of hash/ecc blocks, of variable encoding rate and size, given a file.'''\n    curpos = file.tell() # init the reading cursor at the beginning of the file\n    # Find the total size to know when to stop\n    #size = os.fstat(file.fileno()).st_size # old way of doing it, doesn't work with StringIO objects\n    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file\n    size = file.tell()\n    file.seek(0, curpos) # place the reading cursor back at the beginning of the file\n    # Main encoding loop\n    while curpos < size: # Continue encoding while we do not reach the end of the file\n        # Calculating the encoding rate\n        if curpos < header_size: # if we are still reading the file's header, we use a constant rate\n            rate = resilience_rates[0]\n        else: # else we use a progressive rate for the rest of the file the we calculate on-the-fly depending on our current reading cursor position in the file\n            rate = feature_scaling(curpos, header_size, size, resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)\n\n        # Compute the ecc parameters given the calculated rate\n        ecc_params = compute_ecc_params(max_block_size, rate, hasher)\n        #ecc_manager = ECCMan(max_block_size, ecc_params[\"message_size\"]) # not necessary to create an ecc manager anymore, as it is very costly. Now we can specify a value for k on the fly (tables for all possible values of k are pre-generated in the reed-solomon libraries)\n\n        # Compute the ecc and hash for the current message block\n        mes = file.read(ecc_params[\"message_size\"])\n        hash = hasher.hash(mes)\n        ecc = ecc_manager.encode(mes, k=ecc_params[\"message_size\"])\n        #print(\"mes %i (%i) - ecc %i (%i) - hash %i (%i)\" % (len(mes), message_size, len(ecc), ecc_params[\"ecc_size\"], len(hash), ecc_params[\"hash_size\"])) # DEBUGLINE\n\n        # Return the result\n        yield [hash, ecc, ecc_params]\n        # Prepare for next iteration\n        curpos = file.tell()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_ecc_hash_from_string(string, ecc_manager, hasher, max_block_size, resilience_rate):\n    '''Generate a concatenated string of ecc stream of hash/ecc blocks, of constant encoding rate, given a string.\n    NOTE: resilience_rate here is constant, you need to supply only one rate, between 0.0 and 1.0. The encoding rate will then be constant, like in header_ecc.py.'''\n    fpfile = StringIO(string)\n    ecc_stream = ''.join( [str(x[1]) for x in stream_compute_ecc_hash(ecc_manager, hasher, fpfile, max_block_size, len(string), [resilience_rate])] ) # \"hack\" the function by tricking it to always use a constant rate, by setting the header_size=len(relfilepath), and supplying the resilience_rate_intra instead of resilience_rate_s1 (the one for header)\n    return ecc_stream", "response": "Generate a concatenated string of ecc stream of hash blocks of constant encoding rate given a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, field, ecc, enable_erasures=False, erasures_char=\"\\x00\", only_erasures=False, max_block_size=65535):\n    # convert strings to StringIO object so that we can trick our ecc reading functions that normally works only on files\n    fpfile = StringIO(field)\n    fpfile_ecc = StringIO(ecc)\n    fpentry_p = {\"ecc_field_pos\": [0, len(field)]} # create a fake entry_pos so that the ecc reading function works correctly\n    # Prepare variables\n    field_correct = [] # will store each block of the corrected (or already correct) filepath\n    fcorrupted = False # check if field was corrupted\n    fcorrected = True # check if field was corrected (if it was corrupted)\n    errmsg = ''\n    # Decode each block of the filepath\n    for e in stream_entry_assemble(hasher_intra, fpfile, fpfile_ecc, fpentry_p, max_block_size, len(field), [resilience_rate_intra], constantmode=True):\n        # Check if this block of the filepath is OK, if yes then we just copy it over\n        if ecc_manager_intra.check(e[\"message\"], e[\"ecc\"]):\n            field_correct.append(e[\"message\"])\n        else: # Else this block is corrupted, we will try to fix it using the ecc\n            fcorrupted = True\n            # Repair the message block and the ecc\n            try:\n                repaired_block, repaired_ecc = ecc_manager_intra.decode(e[\"message\"], e[\"ecc\"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)\n            except (ReedSolomonError, RSCodecError), exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.\n                repaired_block = None\n                repaired_ecc = None\n                errmsg += \"- Error: metadata field at offset %i: %s\\n\" % (entry_pos[0], exc)\n            # Check if the block was successfully repaired: if yes then we copy the repaired block...\n            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):\n                field_correct.append(repaired_block)\n            else: # ... else it failed, then we copy the original corrupted block and report an error later\n                field_correct.append(e[\"message\"])\n                fcorrected = False\n    # Join all the blocks into one string to build the final filepath\n    if isinstance(field_correct[0], bytearray): field_correct = [str(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str\n    field = ''.join(field_correct)\n    # Report errors\n    return (field, fcorrupted, fcorrected, errmsg)", "response": "Correct an intra - field with its corresponding intra - ecc if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the memory usage of a process or piece of code.", "response": "def memory_usage(proc=-1, interval=.1, timeout=None, run_in_place=False):\n    \"\"\"\n    Return the memory usage of a process or piece of code\n\n    Parameters\n    ----------\n    proc : {int, string, tuple}, optional\n        The process to monitor. Can be given by a PID, by a string\n        containing a filename or by a tuple. The tuple should contain\n        three values (f, args, kw) specifies to run the function\n        f(*args, **kw).  Set to -1 (default) for current process.\n\n    interval : float, optional\n\n    timeout : float, optional\n\n\n    run_in_place : boolean, optional. False by default\n        If False fork the process and retrieve timings from a different\n        process. You shouldn't need to change this unless you are affected\n        by this (http://blog.vene.ro/2012/07/04/on-why-my-memit-fails-on-osx)\n        bug.\n\n    Returns\n    -------\n    mm : list of integers, size less than num\n        memory usage, in KB\n    \"\"\"\n    ret = []\n\n    if timeout is not None:\n        max_iter = timeout / interval\n    elif isinstance(proc, int):\n        # external process and no timeout\n        max_iter = 1\n    else:\n        # for a Python function wait until it finishes\n        max_iter = float('inf')\n\n    if str(proc).endswith('.py'):\n        filename = _find_script(proc)\n        with open(filename) as f:\n            proc = f.read()\n        raise NotImplementedError\n\n    if isinstance(proc, (list, tuple)):\n\n        if len(proc) == 1:\n            f, args, kw = (proc[0], (), {})\n        elif len(proc) == 2:\n            f, args, kw = (proc[0], proc[1], {})\n        elif len(proc) == 3:\n            f, args, kw = (proc[0], proc[1], proc[2])\n        else:\n            raise ValueError\n        try:\n            import multiprocessing\n        except ImportError:\n            print ('WARNING: cannot import module `multiprocessing`. Forcing to'\n                   ' run inplace.')\n            # force inplace\n            run_in_place = True\n        if run_in_place:\n            import threading\n            main_thread = threading.Thread(target=f, args=args, kwargs=kw)\n        else:\n            main_thread = multiprocessing.Process(target=f, args=args, kwargs=kw)\n        i = 0\n        main_thread.start()\n        pid = getattr(main_thread, 'pid', os.getpid())\n        while i < max_iter and main_thread.is_alive():\n            m = _get_memory(pid)\n            ret.append(m)\n            time.sleep(interval)\n            i += 1\n        main_thread.join()\n    else:\n        # external process\n        if proc == -1:\n            proc = os.getpid()\n        if max_iter == -1:\n            max_iter = 1\n        for _ in range(max_iter):\n            ret.append(_get_memory(proc))\n            time.sleep(interval)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the script in the PATH.", "response": "def _find_script(script_name):\n    \"\"\" Find the script.\n\n    If the input is not a file, then $PATH will be searched.\n    \"\"\"\n    if os.path.isfile(script_name):\n        return script_name\n    path = os.getenv('PATH', os.defpath).split(os.pathsep)\n    for dir in path:\n        if dir == '':\n            continue\n        fn = os.path.join(dir, script_name)\n        if os.path.isfile(fn):\n            return fn\n\n    print >> sys.stderr, 'Could not find script {0}'.format(script_name)\n    raise SystemExit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef magic_memit(self, line=''):\n    opts, stmt = self.parse_options(line, 'r:t:i', posix=False, strict=False)\n    repeat = int(getattr(opts, 'r', 1))\n    if repeat < 1:\n        repeat == 1\n    timeout = int(getattr(opts, 't', 0))\n    if timeout <= 0:\n        timeout = None\n    run_in_place = hasattr(opts, 'i')\n\n    mem_usage = memory_usage((_func_exec, (stmt, self.shell.user_ns)), timeout=timeout,\n        run_in_place=run_in_place)\n\n    if mem_usage:\n        print('maximum of %d: %f MB per loop' % (repeat, max(mem_usage)))\n    else:\n        print('ERROR: could not read memory usage, try with a lower interval or more iterations')", "response": "Measure memory usage of a Python statement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a Python function to the line profiling information.", "response": "def add_function(self, func):\n        \"\"\" Record line profiling information for the given Python function.\n        \"\"\"\n        try:\n            # func_code does not exist in Python3\n            code = func.__code__\n        except AttributeError:\n            import warnings\n            warnings.warn(\"Could not extract a code object for the object %r\"\n                          % (func,))\n            return\n        if code not in self.code_map:\n            self.code_map[code] = {}\n            self.functions.append(func)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_function(self, func):\n\n        def f(*args, **kwds):\n            self.enable_by_count()\n            try:\n                result = func(*args, **kwds)\n            finally:\n                self.disable_by_count()\n            return result\n        return f", "response": "Wrap a function to profile it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runctx(self, cmd, globals, locals):\n        self.enable_by_count()\n        try:\n            exec(cmd, globals, locals)\n        finally:\n            self.disable_by_count()\n        return self", "response": "Profile a single executable statement in the given namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprofiles a single function call.", "response": "def runcall(self, func, *args, **kw):\n        \"\"\" Profile a single function call.\n        \"\"\"\n        # XXX where is this used ? can be removed ?\n        self.enable_by_count()\n        try:\n            return func(*args, **kw)\n        finally:\n            self.disable_by_count()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisable the profiler if the number of disable requests matches the number of enable requests.", "response": "def disable_by_count(self):\n        \"\"\" Disable the profiler if the number of disable requests matches the\n        number of enable requests.\n        \"\"\"\n        if self.enable_count > 0:\n            self.enable_count -= 1\n            if self.enable_count == 0:\n                self.disable()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a makefile to find commands and substitute variables. Expects a makefile with only aliases and a line return between each command and each alias. Returns a dict with a list of commands for each alias.", "response": "def parse_makefile_aliases(filepath):\n    '''\n    Parse a makefile to find commands and substitute variables. Expects a\n    makefile with only aliases and a line return between each command.\n\n    Returns a dict, with a list of commands for each alias.\n    '''\n\n    # -- Parsing the Makefile using ConfigParser\n    # Adding a fake section to make the Makefile a valid Ini file\n    ini_str = '[root]\\n'\n    with open(filepath, 'r') as fd:\n        ini_str = ini_str + fd.read().replace('@make ', '')\n    ini_fp = StringIO.StringIO(ini_str)\n    # Parse using ConfigParser\n    config = ConfigParser.RawConfigParser()\n    config.readfp(ini_fp)\n    # Fetch the list of aliases\n    aliases = config.options('root')\n\n    # -- Extracting commands for each alias\n    commands = {}\n    for alias in aliases:\n        # strip the first line return, and then split by any line return\n        commands[alias] = config.get('root', alias).lstrip('\\n').split('\\n')\n\n    # -- Commands substitution\n    # Loop until all aliases are substituted by their commands:\n    # Check each command of each alias, and if there is one command that is to\n    # be substituted by an alias, try to do it right away. If this is not\n    # possible because this alias itself points to other aliases , then stop\n    # and put the current alias back in the queue to be processed again later.\n\n    # Create the queue of aliases to process\n    aliases_todo = commands.keys()\n    # Create the dict that will hold the full commands\n    commands_new = {}\n    # Loop until we have processed all aliases\n    while aliases_todo:\n        # Pick the first alias in the queue\n        alias = aliases_todo.pop(0)\n        # Create a new entry in the resulting dict\n        commands_new[alias] = []\n        # For each command of this alias\n        for cmd in commands[alias]:\n            # Ignore self-referencing (alias points to itself)\n            if cmd == alias:\n                pass\n            # Substitute full command\n            elif cmd in aliases and cmd in commands_new:\n                # Append all the commands referenced by the alias\n                commands_new[alias].extend(commands_new[cmd])\n            # Delay substituting another alias, waiting for the other alias to\n            # be substituted first\n            elif cmd in aliases and cmd not in commands_new:\n                # Delete the current entry to avoid other aliases\n                # to reference this one wrongly (as it is empty)\n                del commands_new[alias]\n                aliases_todo.append(alias)\n                break\n            # Full command (no aliases)\n            else:\n                commands_new[alias].append(cmd)\n    commands = commands_new\n    del commands_new\n\n    # -- Prepending prefix to avoid conflicts with standard setup.py commands\n    # for alias in commands.keys():\n    #     commands['make_'+alias] = commands[alias]\n    #     del commands[alias]\n\n    return commands"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhacks run function which installs the trace.", "response": "def __run(self):\n    \"\"\"Hacked run function, which installs the trace.\"\"\"\n    sys.settrace(self.globaltrace)\n    self.__run_backup()\n    self.run = self.__run_backup"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef codepoint_included(self, codepoint):\n        if self.codepoints == None:\n            return True\n        for cp in self.codepoints:\n            mismatch = False\n            for i in range(len(cp)):\n                if (cp[i] is not None) and (cp[i] != codepoint[i]):\n                    mismatch = True\n                    break\n            if not mismatch:\n                return True\n        return False", "response": "Check if codepoint is included in the current codepoints."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprofiling method used to profile matching codepoints and events.", "response": "def profile(self, frame, event, arg): #PYCHOK arg requ. to match signature\n        \"\"\"Profiling method used to profile matching codepoints and events.\"\"\"\n        if (self.events == None) or (event in self.events):\n            frame_info = inspect.getframeinfo(frame)\n            cp = (frame_info[0], frame_info[2], frame_info[1])\n            if self.codepoint_included(cp):\n                objects = muppy.get_objects()\n                size = muppy.get_size(objects)\n                if cp not in self.memories:\n                    self.memories[cp] = [0,0,0,0]\n                    self.memories[cp][0] = 1\n                    self.memories[cp][1] = size\n                    self.memories[cp][2] = size\n                else:\n                    self.memories[cp][0] += 1\n                    if self.memories[cp][1] > size:\n                        self.memories[cp][1] = size\n                    if self.memories[cp][2] < size:\n                        self.memories[cp][2] = size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runprofile(mainfunction, output, timeout = 0, calibrate=False):\n    '''\n    Run the functions profiler and save the result\n    If timeout is greater than 0, the profile will automatically stops after timeout seconds\n    '''\n    if noprofiler == True:\n            print('ERROR: profiler and/or pstats library missing ! Please install it (probably package named python-profile) before running a profiling !')\n            return False\n    # This is the main function for profiling\n    def _profile():\n        profile.run(mainfunction, output)\n    print('=> RUNNING FUNCTIONS PROFILER\\n\\n'); sys.stdout.flush();\n    # Calibrate the profiler (only use this if the profiler produces some funny stuff, but calibration can also produce even more funny stuff with the latest cProfile of Python v2.7! So you should only enable calibration if necessary)\n    if calibrate:\n        print('Calibrating the profiler...'); sys.stdout.flush();\n        cval = calibrateprofile()\n        print('Calibration found value : %s' % cval); sys.stdout.flush();\n    print('Initializing the profiler...'); sys.stdout.flush();\n    # Run in timeout mode (if the function cannot ends by itself, this is the best mode: the function must ends for the profile to be saved)\n    if timeout > 0:\n        pthread = KThread(target=_profile) # we open the function with the profiler, in a special killable thread (see below why)\n        print('Will now run the profiling and terminate it in %s seconds. Results will be saved in %s' % (str(timeout), str(output))); sys.stdout.flush();\n        print('\\nCountdown:'); sys.stdout.flush();\n        for i in range(0,5):\n                print(str(5-i))\n                sys.stdout.flush()\n                time.sleep(1)\n        print('0\\nStarting to profile...'); sys.stdout.flush();\n        pthread.start() # starting the thread\n        time.sleep(float(timeout)) # after this amount of seconds, the thread gets killed and the profiler will end its job\n        print('\\n\\nFinishing the profile and saving to the file %s' % str(output)); sys.stdout.flush();\n        pthread.kill() # we must end the main function in order for the profiler to output its results (if we didn't launch a thread and just closed the process, it would have done no result)\n    # Run in full length mode (we run the function until it ends)\n    else:\n        print(\"Running the profiler, please wait until the process terminates by itself (if you forcequit before, the profile won't be saved)\")\n        _profile()\n    print('=> Functions Profile done !')\n    return True", "response": "Runs profiling and saves the result in a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalibrate the profiler and return the final value", "response": "def calibrateprofile():\n    '''\n    Calibrate the profiler (necessary to have non negative and more exact values)\n    '''\n    pr = profile.Profile()\n    calib = []\n    crepeat = 10\n    for i in range(crepeat):\n            calib.append(pr.calibrate(10000))\n    final = sum(calib) / crepeat\n    profile.Profile.bias = final # Apply computed bias to all Profile instances created hereafter\n    return final"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a profile log and print the result on screen", "response": "def parseprofile(profilelog, out):\n    '''\n    Parse a profile log and print the result on screen\n    '''\n    file = open(out, 'w') # opening the output file\n    print('Opening the profile in %s...' % profilelog)\n    p = pstats.Stats(profilelog, stream=file) # parsing the profile with pstats, and output everything to the file\n\n    print('Generating the stats, please wait...')\n    file.write(\"=== All stats:\\n\")\n    p.strip_dirs().sort_stats(-1).print_stats()\n    file.write(\"=== Cumulative time:\\n\")\n    p.sort_stats('cumulative').print_stats(100)\n    file.write(\"=== Time:\\n\")\n    p.sort_stats('time').print_stats(100)\n    file.write(\"=== Time + cumulative time:\\n\")\n    p.sort_stats('time', 'cum').print_stats(.5, 'init')\n    file.write(\"=== Callees:\\n\")\n    p.print_callees()\n    file.write(\"=== Callers:\\n\")\n    p.print_callers()\n    #p.print_callers(.5, 'init')\n    #p.add('fooprof')\n    file.close()\n    print('Stats generated and saved to %s.' % out)\n    print('Everything is done. Exiting')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef browseprofile(profilelog):\n    '''\n    Browse interactively a profile log in console\n    '''\n    print('Starting the pstats profile browser...\\n')\n    try:\n            browser = ProfileBrowser(profilelog)\n            print >> browser.stream, \"Welcome to the profile statistics browser. Type help to get started.\"\n            browser.cmdloop()\n            print >> browser.stream, \"Goodbye.\"\n    except KeyboardInterrupt:\n            pass", "response": "Browse profile log in console"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef browseprofilegui(profilelog):\n    '''\n    Browse interactively a profile log in GUI using RunSnakeRun and SquareMap\n    '''\n    from runsnakerun import runsnake # runsnakerun needs wxPython lib, if it's not available then we can pass if we don't want a GUI. RunSnakeRun is only used for GUI visualisation, not for profiling (and you can still use pstats for console browsing)\n    app = runsnake.RunSnakeRunApp(0)\n    app.OnInit(profilelog)\n    #app.OnInit()\n    app.MainLoop()", "response": "Browse a profile log in GUI using RunSnakeRun and SquareMap\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of primes < n", "response": "def rwh_primes1(n):\n    # http://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188\n    ''' Returns  a list of primes < n '''\n    sieve = [True] * (n/2)\n    for i in _range(3,int(n**0.5)+1,2):\n        if sieve[i/2]:\n            sieve[i*i/2::i] = [False] * ((n-i*i-1)/(2*i)+1)\n    return [2] + [2*i+1 for i in _range(1,n/2) if sieve[i]]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_prime_polynomials(generator=2, c_exp=8, fast_primes=False, single=False):\n    '''Compute the list of prime polynomials for the given generator and galois field characteristic exponent.'''\n    # fast_primes will output less results but will be significantly faster.\n    # single will output the first prime polynomial found, so if all you want is to just find one prime polynomial to generate the LUT for Reed-Solomon to work, then just use that.\n\n    # A prime polynomial (necessarily irreducible) is necessary to reduce the multiplications in the Galois Field, so as to avoid overflows.\n    # Why do we need a \"prime polynomial\"? Can't we just reduce modulo 255 (for GF(2^8) for example)? Because we need the values to be unique.\n    # For example: if the generator (alpha) = 2 and c_exp = 8 (GF(2^8) == GF(256)), then the generated Galois Field (0, 1, \u03b1, \u03b1^1, \u03b1^2, ..., \u03b1^(p-1)) will be galois field it becomes 0, 1, 2, 4, 8, 16, etc. However, upon reaching 128, the next value will be doubled (ie, next power of 2), which will give 256. Then we must reduce, because we have overflowed above the maximum value of 255. But, if we modulo 255, this will generate 256 == 1. Then 2, 4, 8, 16, etc. giving us a repeating pattern of numbers. This is very bad, as it's then not anymore a bijection (ie, a non-zero value doesn't have a unique index). That's why we can't just modulo 255, but we need another number above 255, which is called the prime polynomial.\n    # Why so much hassle? Because we are using precomputed look-up tables for multiplication: instead of multiplying a*b, we precompute alpha^a, alpha^b and alpha^(a+b), so that we can just use our lookup table at alpha^(a+b) and get our result. But just like in our original field we had 0,1,2,...,p-1 distinct unique values, in our \"LUT\" field using alpha we must have unique distinct values (we don't care that they are different from the original field as long as they are unique and distinct). That's why we need to avoid duplicated values, and to avoid duplicated values we need to use a prime irreducible polynomial.\n\n    # Here is implemented a bruteforce approach to find all these prime polynomials, by generating every possible prime polynomials (ie, every integers between field_charac+1 and field_charac*2), and then we build the whole Galois Field, and we reject the candidate prime polynomial if it duplicates even one value or if it generates a value above field_charac (ie, cause an overflow).\n    # Note that this algorithm is slow if the field is too big (above 12), because it's an exhaustive search algorithm. There are probabilistic approaches, and almost surely prime approaches, but there is no determistic polynomial time algorithm to find irreducible monic polynomials. More info can be found at: http://people.mpi-inf.mpg.de/~csaha/lectures/lec9.pdf\n    # Another faster algorithm may be found at Adleman, Leonard M., and Hendrik W. Lenstra. \"Finding irreducible polynomials over finite fields.\" Proceedings of the eighteenth annual ACM symposium on Theory of computing. ACM, 1986.\n\n    # Prepare the finite field characteristic (2^p - 1), this also represent the maximum possible value in this field\n    root_charac = 2 # we're in GF(2)\n    field_charac = int(root_charac**c_exp - 1)\n    field_charac_next = int(root_charac**(c_exp+1) - 1)\n\n    prim_candidates = []\n    if fast_primes:\n        prim_candidates = rwh_primes1(field_charac_next) # generate maybe prime polynomials and check later if they really are irreducible\n        prim_candidates = [x for x in prim_candidates if x > field_charac] # filter out too small primes\n    else:\n        prim_candidates = _range(field_charac+2, field_charac_next, root_charac) # try each possible prime polynomial, but skip even numbers (because divisible by 2 so necessarily not irreducible)\n\n    # Start of the main loop\n    correct_primes = []\n    for prim in prim_candidates: # try potential candidates primitive irreducible polys\n        seen = bytearray(field_charac+1) # memory variable to indicate if a value was already generated in the field (value at index x is set to 1) or not (set to 0 by default)\n        conflict = False # flag to know if there was at least one conflict\n\n        # Second loop, build the whole Galois Field\n        x = GF2int(1)\n        for i in _range(field_charac):\n            # Compute the next value in the field (ie, the next power of alpha/generator)\n            x = x.multiply(generator, prim, field_charac+1)\n\n            # Rejection criterion: if the value overflowed (above field_charac) or is a duplicate of a previously generated power of alpha, then we reject this polynomial (not prime)\n            if x > field_charac or seen[x] == 1:\n                conflict = True\n                break\n            # Else we flag this value as seen (to maybe detect future duplicates), and we continue onto the next power of alpha\n            else:\n                seen[x] = 1\n\n        # End of the second loop: if there's no conflict (no overflow nor duplicated value), this is a prime polynomial!\n        if not conflict: \n            correct_primes.append(prim)\n            if single: return prim\n\n    # Return the list of all prime polynomials\n    return correct_primes", "response": "Compute the list of prime polynomials for the given generator and galois field characteristic exponent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_lut(generator=3, prim=0x11b, c_exp=8):\n    '''Precompute the logarithm and anti-log (look-up) tables for faster computation later, using the provided primitive polynomial.\n    These tables are used for multiplication/division since addition/substraction are simple XOR operations inside GF of characteristic 2.\n    The basic idea is quite simple: since b**(log_b(x), log_b(y)) == x * y given any number b (the base or generator of the logarithm), then we can use any number b to precompute logarithm and anti-log (exponentiation) tables to use for multiplying two numbers x and y.\n    That's why when we use a different base/generator number, the log and anti-log tables are drastically different, but the resulting computations are the same given any such tables.\n    For more infos, see https://en.wikipedia.org/wiki/Finite_field_arithmetic#Implementation_tricks\n    '''\n    # generator is the generator number (the \"increment\" that will be used to walk through the field by multiplication, this must be a prime number). This is basically the base of the logarithm/anti-log tables. Also often noted \"alpha\" in academic books.\n    # prim is the primitive/prime (binary) polynomial and must be irreducible (it can't represented as the product of two smaller polynomials). It's a polynomial in the binary sense: each bit is a coefficient, but in fact it's an integer between 0 and 255, and not a list of gf values. For more infos: http://research.swtch.com/field\n    # note that the choice of generator or prime polynomial doesn't matter very much: any two finite fields of size p^n have identical structure, even if they give the individual elements different names (ie, the coefficients of the codeword will be different, but the final result will be the same: you can always correct as many errors/erasures with any choice for those parameters). That's why it makes sense to refer to all the finite fields, and all decoders based on Reed-Solomon, of size p^n as one concept: GF(p^n). It can however impact sensibly the speed (because some parameters will generate sparser tables).\n\n    global GF2int_exptable, GF2int_logtable, GF2_charac, GF2_c_exp\n    GF2_charac = int(2**c_exp - 1)\n    GF2_c_exp = int(c_exp)\n    exptable = [-1] * (GF2_charac+1) # anti-log (exponential) table. The first two elements will always be [GF2int(1), generator]\n    logtable = [-1] * (GF2_charac+1) # log table, log[0] is impossible and thus unused\n\n    # Construct the anti-log table\n    # It's basically the cumulative product of 1 by the generator number, on and on and on until you have walked through the whole field.\n    # That's why exptable is always dense (all entries are filled), but logtable may be sparse (lots of empty values, because multiple logtable's entries point to the same exptable's entry).\n    g = GF2int(1)\n    for i in range(GF2_charac+1): # note that the last item of exptable will always be equal to the first item in the table, because g^p==g^0 because of the modulo p (we're in a finite field!).\n        exptable[i] = g # compute anti-log for this value and store it in a table\n        #logtable[g] = i # compute logtable at the same time as exptable (but log[1] will always be equal to g^255, which may be weird when compared to lists of logtables online but this is equivalent)\n        g = g.multiply(generator, prim, GF2_charac+1) # equivalent to: g = generator**(i+1)\n\n    # Construct the log table\n    # Ignore the last element of the field because fields wrap back around.\n    # The log of 1 can have two values: either g^0 (the exact value change depending on parameters) or it could be 255 (g^255=1) because of the wraparound\n    # Note that either way, this does not change anything any output later (ie, the ecc symbols will be the same either way).\n    for i, x in enumerate(exptable[:-1]):\n        logtable[x] = i\n\n    # Optimization: convert to integer arrays\n    GF2int_exptable = array.array('i', exptable)\n    GF2int_logtable = array.array('i', logtable)\n    return GF2int_exptable, GF2int_logtable", "response": "Initialize the logarithm and anti - log tables for fast computation later using the provided primitive polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a Galois Field s number into a nice polynomial", "response": "def _to_binpoly(x):\n        '''Convert a Galois Field's number into a nice polynomial'''\n        if x <= 0: return \"0\"\n        b = 1 # init to 2^0 = 1\n        c = [] # stores the degrees of each term of the polynomials\n        i = 0 # counter for b = 2^i\n        while x > 0:\n            b = (1 << i) # generate a number power of 2: 2^0, 2^1, 2^2, ..., 2^i. Equivalent to b = 2^i\n            if x & b : # then check if x is divisible by the power of 2. Equivalent to x % 2^i == 0\n                # If yes, then...\n                c.append(i) # append this power (i, the exponent, gives us the coefficient)\n                x ^= b # and compute the remainder of x / b\n            i = i+1 # increment to compute the next power of 2\n        return \" + \".join([\"x^%i\" % y for y in c[::-1]])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loads( source ):\n    source = source.strip()\n    assert source.startswith( '{' )\n    assert source.endswith( '}' )\n    source = source[1:-1]\n    result = {}\n    for match in attr.finditer( source ):\n        key = match.group('key')\n        if match.group( 'list' ) is not None:\n            value = [ \n                int(x) \n                for x in match.group( 'list' ).strip().replace(',',' ').split() \n            ]\n        elif match.group( 'int' ) is not None:\n            value = int( match.group( 'int' ))\n        elif match.group( 'string' ) is not None:\n            def deescape( match ):\n                return unichr( int( match.group(0)[2:], 16 ))\n            value = match.group('string').decode( 'utf-8' )\n            value = escape.sub( \n                deescape,\n                value,\n            )\n            value = simple_escape.sub(\n                lambda x: x.group(1),\n                value,\n            )\n        else:\n            raise RuntimeError( \"Matched something we don't know how to process:\", match.groupdict() )\n        result[key] = value\n    return result", "response": "Load json structure from meliae from source\n    \n    Supports only the required structures to support loading meliae memory dumps\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode a given string or list of values with reed - solomon encoding. Returns a list of values or a string.", "response": "def encode(self, message, poly=False, k=None, return_string=True):\n        '''Encode a given string or list of values (between 0 and gf2_charac)\n        with reed-solomon encoding. Returns a list of values (or a string if return_string is true)\n        with the k message bytes and n-k parity bytes at the end.\n        \n        If a message is < k bytes long, it is assumed to be padded at the front\n        with null bytes.\n\n        The sequence returned is always n bytes long.\n\n        If poly is not False, returns the encoded Polynomial object instead of\n        the polynomial translated back to a string (useful for debugging)\n        '''\n        n = self.n\n        if not k: k = self.k\n\n        if len(message)>k:\n            raise ValueError(\"Message length is max %d. Message was %d\" % (k,\n                len(message)))\n\n        # If we were given a string, convert to a list (important to support fields above 2^8)\n        if isinstance(message, _str):\n            message = [ord(x) for x in message]\n\n        # Encode message as a polynomial:\n        m = Polynomial([GF2int(x) for x in message])\n\n        # Shift polynomial up by n-k by multiplying by x^(n-k) to reserve the first n-k coefficients for the ecc. This effectively pad the message with \\0 bytes for the lower coefficients (where the ecc will be placed).\n        mprime = m * Polynomial([GF2int(1)] + [GF2int(0)]*(n-k))\n\n        # mprime = q*g + b for some q\n        # so let's find b, the code word (ecc block):\n        b = mprime % self.g[k]\n\n        # Subtract out b, so now c = q*g, which is a way of xoring mprime and code word b, which is a way of just saying that we append the polynomial ecc code to the original message (we replace the padded 0 bytes of mprime with the code word)\n        c = mprime - b\n        # Since c is a multiple of g, it has (at least) n-k roots: \u03b1^1 through\n        # \u03b1^(n-k)\n\n        if not poly:\n            # Turn the polynomial c back into a string\n            ret = self._list_rjust(c.coefficients, n, 0) # rjust is useful for the nostrip feature\n            if return_string and self.gf2_charac < 256:\n                ret = self._list2str(ret)\n        else:\n            ret = c\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify the codeword is valid by testing that the codeword r divides g returns True or False", "response": "def check(self, r, k=None):\n        '''Verifies the codeword is valid by testing that the codeword (message+ecc) as a polynomial code divides g\n        returns True/False\n        '''\n        n = self.n\n        if not k: k = self.k\n        #h = self.h[k]\n        g = self.g[k]\n\n        # If we were given a string, convert to a list (important to support fields above 2^8)\n        if isinstance(r, _str):\n            r = [ord(x) for x in r]\n\n        # Turn r into a polynomial\n        c = Polynomial([GF2int(x) for x in r])\n\n        # This works too, but takes longer. Both checks are just as valid.\n        #return (c*h)%gtimesh == Polynomial(x0=0)\n\n        # Since all codewords are multiples of g, checking that codeword c divides g\n        # suffices for validating a codeword.\n        return c % g == Polynomial(x0=0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfasting check if there s any error in a message + ecc.", "response": "def check_fast(self, r, k=None):\n        '''Fast check if there's any error in a message+ecc. Can be used before decoding, in addition to hashes to detect if the message was tampered, or after decoding to check that the message was fully recovered.\n        returns True/False\n        '''\n        n = self.n\n        if not k: k = self.k\n        #h = self.h[k]\n        g = self.g[k]\n\n        # If we were given a string, convert to a list (important to support fields above 2^8)\n        if isinstance(r, _str):\n            r = [ord(x) for x in r]\n\n        # Turn r into a polynomial\n        r = Polynomial([GF2int(x) for x in r])\n\n        # Compute the syndromes:\n        sz = self._syndromes(r, k=k)\n\n        # Checking that the syndrome is all 0 is sufficient to check if there are no more any errors in the decoded message\n        #return all(int(x) == 0 for x in sz)\n        return sz.coefficients.count(GF2int(0)) == len(sz)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(self, r, nostrip=False, k=None, erasures_pos=None, only_erasures=False, return_string=True):\n        '''Given a received string or byte array or list r of values between\n        0 and gf2_charac, attempts to decode it. If it's a valid codeword, or\n        if there are no more than (n-k)/2 errors, the repaired message is returned.\n\n        A message always has k bytes, if a message contained less it is left\n        padded with null bytes. When decoded, these leading null bytes are\n        stripped, but that can cause problems if decoding binary data. When\n        nostrip is True, messages returned are always k bytes long. This is\n        useful to make sure no data is lost when decoding binary data.\n        \n        Theoretically, we have R(x) = C(x) + E(x) + V(x), where R is the received message, C is the correct message without errors nor erasures, E are the errors and V the erasures. Thus the goal is to compute E and V from R, so that we can compute: C(x) = R(x) - E(x) - V(x), and then we have our original message! The main problem of decoding is to solve the so-called Key Equation, here we use Berlekamp-Massey.\n        \n        When stated in the language of spectral estimation, consists of a Fourier transform (syndrome computer), followed by a spectral analysis (Berlekamp-Massey or Euclidian algorithm), followed by an inverse Fourier transform (Chien search).\n        (see Blahut, \"Algebraic Codes for Data Transmission\", 2003, chapter 7.6 Decoding in Time Domain).\n        '''\n        n = self.n\n        if not k: k = self.k\n\n        # If we were given a string, convert to a list (important to support fields above 2^8)\n        if isinstance(r, _str):\n            r = [ord(x) for x in r]\n\n        # Turn r into a polynomial\n        rp = Polynomial([GF2int(x) for x in r])\n\n        if erasures_pos:\n            # Convert string positions to coefficients positions for the algebra to work (see _find_erasures_locator(), ecc characters represent the first coefficients while the message is put last, so it's exactly the reverse of the string positions where the message is first and the ecc is last, thus it's just like if you read the message+ecc string in reverse)\n            erasures_pos = [len(r)-1-x for x in erasures_pos]\n            # Set erasures characters to null bytes\n            # Note that you can just leave the original characters as they are, you don't need to set erased characters to null bytes for the decoding to work, but note that it won't help either (ie, fake erasures, meaning characters that were detected as erasures but actually aren't, will still \"consume\" one ecc symbol, even if you don't set them to null byte, this is because the syndrome is limited to n-k and thus you can't decode above this bound without a clever trick).\n            # Example string containing a fake erasure: \"hello sam\" -> \"ooooo sam\" with erasures_pos = [0, 1, 2, 3, 4]. Here in fact the last erasure is fake because the original character also was \"o\" so if we detect \"o\" as an erasure, we will end up with one fake erasure. But setting it to null byte or not, it will still use up one ecc symbol, it will always be counted as a real erasure. If you're below the n-k bound, then the doceding will be ok. If you're above, then you can't do anything, the decoding won't work. Maybe todo: try to find a clever list decoding algorithm to account for fake erasures....\n            # Note: commented out so that the resulting omega (error evaluator polynomial) is the same as the erasure evaluator polynomial when decoding the same number of errors or erasures (ie, decoding 3 erasures only will give the same result as 3 errors only, with of course the errors/erasures on the same characters).\n            #for erasure in erasures_pos:\n                #rp[erasure] = GF2int(0)\n\n        # Compute the syndromes:\n        sz = self._syndromes(rp, k=k)\n\n        if sz.coefficients.count(GF2int(0)) == len(sz): # the code is already valid, there's nothing to do\n            # The last n-k bytes are parity\n            ret = r[:-(n-k)]\n            ecc = r[-(n-k):]\n            if not nostrip:\n                ret = self._list_lstrip(r[:-(n-k)], 0)\n            if return_string and self.gf2_charac < 256:\n                ret = self._list2str(ret)\n                ecc = self._list2str(ecc) \n            return ret, ecc\n\n        # Erasures locator polynomial computation\n        erasures_loc = None\n        erasures_eval = None\n        erasures_count = 0\n        if erasures_pos:\n            erasures_count = len(erasures_pos)\n            # Compute the erasure locator polynomial\n            erasures_loc = self._find_erasures_locator(erasures_pos)\n            # Compute the erasure evaluator polynomial\n            erasures_eval = self._find_error_evaluator(sz, erasures_loc, k=k)\n\n        if only_erasures:\n            sigma = erasures_loc\n            omega = erasures_eval\n        else:\n            # Find the error locator polynomial and error evaluator polynomial\n            # using the Berlekamp-Massey algorithm\n            # if erasures were supplied, BM will generate the errata (errors-and-erasures) locator and evaluator polynomials\n            sigma, omega = self._berlekamp_massey(sz, k=k, erasures_loc=erasures_loc, erasures_eval=erasures_eval, erasures_count=erasures_count)\n            omega = self._find_error_evaluator(sz, sigma, k=k) # we want to make sure that omega is correct (we know that sigma is always correct, but omega not really)\n\n        # Now use Chien's procedure to find the error locations\n        # j is an array of integers representing the positions of the errors, 0\n        # being the rightmost byte\n        # X is a corresponding array of GF(2^8) values where X_i = alpha^(j_i)\n        X, j = self._chien_search(sigma)\n\n        # Sanity check: Cannot guarantee correct decoding of more than n-k errata (Singleton Bound, n-k being the minimum distance), and we cannot even check if it's correct (the syndrome will always be all 0 if we try to decode above the bound), thus it's better to just return the input as-is.\n        if len(j) > n-k:\n            ret = r[:-(n-k)]\n            ecc = r[-(n-k):]\n            if not nostrip:\n                ret = self._list_lstrip(r[:-(n-k)], 0)\n            if return_string and self.gf2_charac < 256:\n                ret = self._list2str(ret)\n                ecc = self._list2str(ecc) \n            return ret, ecc\n\n        # And finally, find the error magnitudes with Forney's Formula\n        # Y is an array of GF(2^8) values corresponding to the error magnitude\n        # at the position given by the j array\n        Y = self._forney(omega, X)\n\n        # Put the error and locations together to form the error polynomial\n        # Note that an alternative would be to compute the error-spectrum polynomial E(x) which satisfies E(x)*Sigma(x) = 0 (mod x^n - 1) = Omega(x)(x^n - 1) -- see Blahut, Algebraic codes for data transmission\n        Elist = [GF2int(0)] * self.gf2_charac\n        if len(Y) >= len(j): # failsafe: if the number of erratas is higher than the number of coefficients in the magnitude polynomial, we failed!\n            for i in _range(self.gf2_charac): # FIXME? is this really necessary to go to self.gf2_charac? len(rp) wouldn't be just enough? (since the goal is anyway to substract E to rp)\n                if i in j:\n                    Elist[i] = Y[j.index(i)]\n            E = Polynomial( Elist[::-1] ) # reverse the list because we used the coefficient degrees (j) instead of the error positions\n        else:\n            E = Polynomial()\n\n        # And we get our real codeword!\n        c = rp - E # Remember what we wrote above: R(x) = C(x) + E(x), so here to get back the original codeword C(x) = R(x) - E(x) ! (V(x) the erasures are here is included inside E(x))\n\n        if len(c) > len(r): c = rp # failsafe: in case the correction went totally wrong (we repaired padded null bytes instead of the message! thus we end up with a longer message than what we should have), then we just return the uncorrected message. Note: we compare the length of c with r on purpose, that's not an error: if we compare with rp, if the first few characters were erased (null bytes) in r, then in rp the Polynomial will automatically skip them, thus the length will always be smaller in that case.\n\n        # Split the polynomial into two parts: the corrected message and the corrected ecc\n        ret = c.coefficients[:-(n-k)]\n        ecc = c.coefficients[-(n-k):]\n\n        if nostrip:\n            # Polynomial objects don't store leading 0 coefficients, so we\n            # actually need to pad this to k bytes\n            ret = self._list_rjust(ret, k, 0)\n\n        if return_string and self.gf2_charac < 256: # automatically disable return_string if the field is above 255 (chr would fail, so it's up to the user to define the mapping)\n            # Form it back into a string \n            ret = self._list2str(ret)\n            ecc = self._list2str(ecc)\n\n        return ret, ecc", "response": "Given a received string or byte array or list r attempts to decode it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _list_lstrip(self, L, val=0):\n        '''Left strip the specified value'''\n        for i in _range(len(L)):\n            if L[i] != val:\n                return L[i:]", "response": "Left strip the specified value from the specified list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nleaves pad with the specified value to obtain a list of the specified width", "response": "def _list_rjust(self, L, width, fillchar=0):\n        '''Left pad with the specified value to obtain a list of the specified width (length)'''\n        length = max(0, width - len(L))\n        return [fillchar]*length + L"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _syndromes(self, r, k=None):\n        '''Given the received codeword r in the form of a Polynomial object,\n        computes the syndromes and returns the syndrome polynomial.\n        Mathematically, it's essentially equivalent to a Fourrier Transform (Chien search being the inverse).\n        '''\n        n = self.n\n        if not k: k = self.k\n        # Note the + [GF2int(0)] : we add a 0 coefficient for the lowest degree (the constant). This effectively shifts the syndrome, and will shift every computations depending on the syndromes (such as the errors locator polynomial, errors evaluator polynomial, etc. but not the errors positions).\n        # This is not necessary as anyway syndromes are defined such as there are only non-zero coefficients (the only 0 is the shift of the constant here) and subsequent computations will/must account for the shift by skipping the first iteration (eg, the often seen range(1, n-k+1)), but you can also avoid prepending the 0 coeff and adapt every subsequent computations to start from 0 instead of 1.\n        return Polynomial( [r.evaluate( GF2int(self.generator)**(l+self.fcr) ) for l in _range(n-k-1, -1, -1)] + [GF2int(0)], keep_zero=True )", "response": "Given the received codeword r computes the syndromes and returns the syndrome polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the erasures locator polynomial from the erasures positions.", "response": "def _find_erasures_locator(self, erasures_pos):\n        '''Compute the erasures locator polynomial from the erasures positions (the positions must be relative to the x coefficient, eg: \"hello worldxxxxxxxxx\" is tampered to \"h_ll_ worldxxxxxxxxx\" with xxxxxxxxx being the ecc of length n-k=9, here the string positions are [1, 4], but the coefficients are reversed since the ecc characters are placed as the first coefficients of the polynomial, thus the coefficients of the erased characters are n-1 - [1, 4] = [18, 15] = erasures_loc to be specified as an argument.'''\n        # See: http://ocw.usu.edu/Electrical_and_Computer_Engineering/Error_Control_Coding/lecture7.pdf and Blahut, Richard E. \"Transform techniques for error control codes.\" IBM Journal of Research and development 23.3 (1979): 299-315. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.600&rep=rep1&type=pdf and also a MatLab implementation here: http://www.mathworks.com/matlabcentral/fileexchange/23567-reed-solomon-errors-and-erasures-decoder/content//RS_E_E_DEC.m\n        erasures_loc = Polynomial([GF2int(1)]) # just to init because we will multiply, so it must be 1 so that the multiplication starts correctly without nulling any term\n        # erasures_loc is very simple to compute: erasures_loc = prod(1 - x*alpha[j]**i) for i in erasures_pos and where alpha is the alpha chosen to evaluate polynomials (here in this library it's gf(3)). To generate c*x where c is a constant, we simply generate a Polynomial([c, 0]) where 0 is the constant and c is positionned to be the coefficient for x^1. See https://en.wikipedia.org/wiki/Forney_algorithm#Erasures\n        for i in erasures_pos:\n            erasures_loc = erasures_loc * (Polynomial([GF2int(1)]) - Polynomial([GF2int(self.generator)**i, 0]))\n        return erasures_loc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _berlekamp_massey(self, s, k=None, erasures_loc=None, erasures_eval=None, erasures_count=0):\n        '''Computes and returns the errata (errors+erasures) locator polynomial (sigma) and the\n        error evaluator polynomial (omega) at the same time.\n        If the erasures locator is specified, we will return an errors-and-erasures locator polynomial and an errors-and-erasures evaluator polynomial, else it will compute only errors. With erasures in addition to errors, it can simultaneously decode up to v+2e <= (n-k) where v is the number of erasures and e the number of errors.\n        Mathematically speaking, this is equivalent to a spectral analysis (see Blahut, \"Algebraic Codes for Data Transmission\", 2003, chapter 7.6 Decoding in Time Domain).\n        The parameter s is the syndrome polynomial (syndromes encoded in a\n        generator function) as returned by _syndromes.\n\n        Notes:\n        The error polynomial:\n        E(x) = E_0 + E_1 x + ... + E_(n-1) x^(n-1)\n\n        j_1, j_2, ..., j_s are the error positions. (There are at most s\n        errors)\n\n        Error location X_i is defined: X_i = \u03b1^(j_i)\n        that is, the power of \u03b1 (alpha) corresponding to the error location\n\n        Error magnitude Y_i is defined: E_(j_i)\n        that is, the coefficient in the error polynomial at position j_i\n\n        Error locator polynomial:\n        sigma(z) = Product( 1 - X_i * z, i=1..s )\n        roots are the reciprocals of the error locations\n        ( 1/X_1, 1/X_2, ...)\n\n        Error evaluator polynomial omega(z) is here computed at the same time as sigma, but it can also be constructed afterwards using the syndrome and sigma (see _find_error_evaluator() method).\n\n        It can be seen that the algorithm tries to iteratively solve for the error locator polynomial by\n        solving one equation after another and updating the error locator polynomial. If it turns out that it\n        cannot solve the equation at some step, then it computes the error and weights it by the last\n        non-zero discriminant found, and delays the weighted result to increase the polynomial degree\n        by 1. Ref: \"Reed Solomon Decoder: TMS320C64x Implementation\" by Jagadeesh Sankaran, December 2000, Application Report SPRA686\n\n        The best paper I found describing the BM algorithm for errata (errors-and-erasures) evaluator computation is in \"Algebraic Codes for Data Transmission\", Richard E. Blahut, 2003.\n        '''\n        # For errors-and-erasures decoding, see: \"Algebraic Codes for Data Transmission\", Richard E. Blahut, 2003 and (but it's less complete): Blahut, Richard E. \"Transform techniques for error control codes.\" IBM Journal of Research and development 23.3 (1979): 299-315. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.600&rep=rep1&type=pdf and also a MatLab implementation here: http://www.mathworks.com/matlabcentral/fileexchange/23567-reed-solomon-errors-and-erasures-decoder/content//RS_E_E_DEC.m\n        # also see: Blahut, Richard E. \"A universal Reed-Solomon decoder.\" IBM Journal of Research and Development 28.2 (1984): 150-158. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.2084&rep=rep1&type=pdf\n        # and another good alternative book with concrete programming examples: Jiang, Yuan. A practical guide to error-control coding using Matlab. Artech House, 2010.\n        n = self.n\n        if not k: k = self.k\n\n        # Initialize, depending on if we include erasures or not:\n        if erasures_loc:\n            sigma = [ Polynomial(erasures_loc.coefficients) ] # copy erasures_loc by creating a new Polynomial, so that we initialize the errata locator polynomial with the erasures locator polynomial.\n            B = [ Polynomial(erasures_loc.coefficients) ]\n            omega = [ Polynomial(erasures_eval.coefficients) ] # to compute omega (the evaluator polynomial) at the same time, we also need to initialize it with the partial erasures evaluator polynomial\n            A = [ Polynomial(erasures_eval.coefficients) ] # TODO: fix the initial value of the evaluator support polynomial, because currently the final omega is not correct (it contains higher order terms that should be removed by the end of BM)\n        else:\n            sigma = [ Polynomial([GF2int(1)]) ] # error locator polynomial. Also called Lambda in other notations.\n            B = [ Polynomial([GF2int(1)]) ] # this is the error locator support/secondary polynomial, which is a funky way to say that it's just a temporary variable that will help us construct sigma, the error locator polynomial\n            omega = [ Polynomial([GF2int(1)]) ] # error evaluator polynomial. We don't need to initialize it with erasures_loc, it will still work, because Delta is computed using sigma, which itself is correctly initialized with erasures if needed.\n            A = [ Polynomial([GF2int(0)]) ] # this is the error evaluator support/secondary polynomial, to help us construct omega\n        L = [ 0 ] # update flag: necessary variable to check when updating is necessary and to check bounds (to avoid wrongly eliminating the higher order terms). For more infos, see https://www.cs.duke.edu/courses/spring11/cps296.3/decoding_rs.pdf\n        M = [ 0 ] # optional variable to check bounds (so that we do not mistakenly overwrite the higher order terms). This is not necessary, it's only an additional safe check. For more infos, see the presentation decoding_rs.pdf by Andrew Brown in the doc folder.\n\n        # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients.\n        # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome (that we do not use in this lib but it may be implemented in the future), which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors.\n        synd_shift = 0\n        if len(s) > (n-k): synd_shift = len(s) - (n-k)\n\n        # Polynomial constants:\n        ONE = Polynomial(z0=GF2int(1))\n        ZERO = Polynomial(z0=GF2int(0))\n        Z = Polynomial(z1=GF2int(1)) # used to shift polynomials, simply multiply your poly * Z to shift\n\n        # Precaching\n        s2 = ONE + s\n\n        # Iteratively compute the polynomials n-k-erasures_count times. The last ones will be correct (since the algorithm refines the error/errata locator polynomial iteratively depending on the discrepancy, which is kind of a difference-from-correctness measure).\n        for l in _range(0, n-k-erasures_count): # skip the first erasures_count iterations because we already computed the partial errata locator polynomial (by initializing with the erasures locator polynomial)\n            K = erasures_count+l+synd_shift # skip the FIRST erasures_count iterations (not the last iterations, that's very important!)\n\n            # Goal for each iteration: Compute sigma[l+1] and omega[l+1] such that\n            # (1 + s)*sigma[l] == omega[l] in mod z^(K)\n\n            # For this particular loop iteration, we have sigma[l] and omega[l],\n            # and are computing sigma[l+1] and omega[l+1]\n\n            # First find Delta, the non-zero coefficient of z^(K) in\n            # (1 + s) * sigma[l]\n            # Note that adding 1 to the syndrome s is not really necessary, you can do as well without.\n            # This delta is valid for l (this iteration) only\n            Delta = ( s2 * sigma[l] ).get_coefficient(K) # Delta is also known as the Discrepancy, and is always a scalar (not a polynomial).\n            # Make it a polynomial of degree 0, just for ease of computation with polynomials sigma and omega.\n            Delta = Polynomial(x0=Delta)\n\n            # Can now compute sigma[l+1] and omega[l+1] from\n            # sigma[l], omega[l], B[l], A[l], and Delta\n            sigma.append( sigma[l] - Delta * Z * B[l] )\n            omega.append( omega[l] - Delta * Z * A[l] )\n\n            # Now compute the next support polynomials B and A\n            # There are two ways to do this\n            # This is based on a messy case analysis on the degrees of the four polynomials sigma, omega, A and B in order to minimize the degrees of A and B. For more infos, see https://www.cs.duke.edu/courses/spring10/cps296.3/decoding_rs_scribe.pdf\n            # In fact it ensures that the degree of the final polynomials aren't too large.\n            if Delta == ZERO or 2*L[l] > K+erasures_count \\\n                or (2*L[l] == K+erasures_count and M[l] == 0):\n            #if Delta == ZERO or len(sigma[l+1]) <= len(sigma[l]): # another way to compute when to update, and it doesn't require to maintain the update flag L\n                # Rule A\n                B.append( Z * B[l] )\n                A.append( Z * A[l] )\n                L.append( L[l] )\n                M.append( M[l] )\n\n            elif (Delta != ZERO and 2*L[l] < K+erasures_count) \\\n                or (2*L[l] == K+erasures_count and M[l] != 0):\n            # elif Delta != ZERO and len(sigma[l+1]) > len(sigma[l]): # another way to compute when to update, and it doesn't require to maintain the update flag L\n                # Rule B\n                B.append( sigma[l] // Delta )\n                A.append( omega[l] // Delta )\n                L.append( K - L[l] ) # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erasures_count` (and indeed in a previous draft of this function, if you forgot to do `- erasures_count` it would lead to correcting only 2*(errors+erasures) <= (n-k) instead of 2*errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erasures_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures.\n                M.append( 1 - M[l] )\n\n            else:\n                raise Exception(\"Code shouldn't have gotten here\")\n\n        # Hack to fix the simultaneous computation of omega, the errata evaluator polynomial: because A (the errata evaluator support polynomial) is not correctly initialized (I could not find any info in academic papers). So at the end, we get the correct errata evaluator polynomial omega + some higher order terms that should not be present, but since we know that sigma is always correct and the maximum degree should be the same as omega, we can fix omega by truncating too high order terms.\n        if omega[-1].degree > sigma[-1].degree: omega[-1] = Polynomial(omega[-1].coefficients[-(sigma[-1].degree+1):])\n\n        # Debuglines, uncomment to show the result of every iterations\n        #print \"SIGMA BM\"\n        #for i,x in enumerate(sigma):\n            #print i, \":\", x\n\n        # Return the last result of the iterations (since BM compute iteratively, the last iteration being correct - it may already be before, but we're not sure)\n        return sigma[-1], omega[-1]", "response": "Computes and returns the errata locator polynomial and error evaluator polynomial at the same time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _berlekamp_massey_fast(self, s, k=None, erasures_loc=None, erasures_eval=None, erasures_count=0):\n        '''Faster implementation of errata (errors-and-erasures) Berlekamp-Massey.\n        Returns the error locator polynomial (sigma) and the\n        error evaluator polynomial (omega) with a faster implementation.\n        '''\n        n = self.n\n        if not k: k = self.k\n\n        # Initialize, depending on if we include erasures or not:\n        if erasures_loc:\n            sigma = Polynomial(erasures_loc.coefficients) # copy erasures_loc by creating a new Polynomial, so that we initialize the errata locator polynomial with the erasures locator polynomial.\n            sigmaprev = Polynomial(sigma.coefficients)\n            B = Polynomial(sigma.coefficients)\n            omega = Polynomial(erasures_eval.coefficients) # to compute omega (the evaluator polynomial) at the same time, we also need to initialize it with the partial erasures evaluator polynomial\n            omegaprev = Polynomial(omega.coefficients)\n            A = Polynomial(omega.coefficients) # TODO: fix the initial value of the evaluator support polynomial, because currently the final omega is not correct (it contains higher order terms that should be removed by the end of BM)\n        else:\n            sigma = sigmaprev = Polynomial([GF2int(1)]) # error locator polynomial. Also called Lambda in other notations.\n            sigmaprev = Polynomial([GF2int(1)]) # we need the previous iteration to compute the next value of the support polynomials\n            B = Polynomial([GF2int(1)]) # this is the error locator support/secondary polynomial, which is a funky way to say that it's just a temporary variable that will help us construct sigma, the error locator polynomial\n            omega = omegaprev = Polynomial([GF2int(1)]) # error evaluator polynomial. We don't need to initialize it with erasures_loc, it will still work, because Delta is computed using sigma, which itself is correctly initialized with erasures if needed.\n            omegaprev = Polynomial([GF2int(1)])\n            A = Polynomial([GF2int(0)]) # this is the error evaluator support/secondary polynomial, to help us construct omega\n        L = 0 # update flag: necessary variable to check when updating is necessary and to check bounds (to avoid wrongly eliminating the higher order terms). For more infos, see https://www.cs.duke.edu/courses/spring11/cps296.3/decoding_rs.pdf\n        #M = 0 # optional variable to check bounds (so that we do not mistakenly overwrite the higher order terms). This is not necessary, it's only an additional safe check. For more infos, see the presentation decoding_rs.pdf by Andrew Brown in the doc folder.\n\n        # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients.\n        # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome (that we do not use in this lib but it may be implemented in the future), which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors.\n        synd_shift = 0\n        if len(s) > (n-k): synd_shift = len(s) - (n-k)\n\n        # Polynomial constants:\n        ONE = Polynomial([GF2int(1)])\n        ZERO = GF2int(0)\n        Z = Polynomial([GF2int(1), GF2int(0)]) # used to shift polynomials, simply multiply your poly * Z to shift\n\n        # Precaching\n        s2 = ONE+s\n\n        # Iteratively compute the polynomials n-k-erasures_count times. The last ones will be correct (since the algorithm refines the error/errata locator polynomial iteratively depending on the discrepancy, which is kind of a difference-from-correctness measure).\n        for l in _range(n-k-erasures_count): # skip the first erasures_count iterations because we already computed the partial errata locator polynomial (by initializing with the erasures locator polynomial)\n            K = erasures_count+l+synd_shift # skip the FIRST erasures_count iterations (not the last iterations, that's very important!)\n\n            # Goal for each iteration: Compute sigma[l+1] and omega[l+1] such that\n            # (1 + s)*sigma[l] == omega[l] in mod z^(K)\n\n            # For this particular loop iteration, we have sigma[l] and omega[l],\n            # and are computing sigma[l+1] and omega[l+1]\n\n            # First find Delta, the non-zero coefficient of z^(K) in\n            # (1 + s) * sigma[l]\n            # Note that adding 1 to the syndrome s is not really necessary, you can do as well without.\n            # This delta is valid for l (this iteration) only\n            Delta = s2.mul_at(sigma, K) # Delta is also known as the Discrepancy, and is always a scalar (not a polynomial). We just need one coefficient at a specific degree, so we can optimize by computing only the polynomial multiplication at this term, and skip the others.\n\n            # Can now compute sigma[l+1] and omega[l+1] from\n            # sigma[l], omega[l], B[l], A[l], and Delta\n            sigmaprev = sigma\n            omegaprev = omega\n            sigma = sigma - (Z * B).scale(Delta)\n            omega = omega - (Z * A).scale(Delta)\n\n            # Now compute the next support polynomials B and A\n            # There are two ways to do this\n            # This is based on a messy case analysis on the degrees of the four polynomials sigma, omega, A and B in order to minimize the degrees of A and B. For more infos, see https://www.cs.duke.edu/courses/spring10/cps296.3/decoding_rs_scribe.pdf\n            # In fact it ensures that the degree of the final polynomials aren't too large.\n            if Delta == ZERO or 2*L > K+erasures_count:\n                #or (2*L == K+erasures_count and M == 0):\n            #if Delta == ZERO or len(sigma) <= len(sigmaprev): # another way to compute when to update, and it doesn't require to maintain the update flag L\n                # Rule A\n                B = Z * B\n                A = Z * A\n                #L = L\n                #M = M\n\n            else:\n            #elif (Delta != ZERO and 2*L < K+erasures_count) \\\n            #    or (2*L == K+erasures_count and M != 0):\n            # elif Delta != ZERO and len(sigma) > len(sigmaprev): # another way to compute when to update, and it doesn't require to maintain the update flag L\n                # Rule B\n                B = sigmaprev.scale(Delta.inverse())\n                A = omegaprev.scale(Delta.inverse())\n                L = K - L # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erasures_count` (and indeed in a previous draft of this function, if you forgot to do `- erasures_count` it would lead to correcting only 2*(errors+erasures) <= (n-k) instead of 2*errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erasures_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures.\n                #M = 1 - M\n\n            #else:\n            #    raise Exception(\"Code shouldn't have gotten here\")\n\n        # Hack to fix the simultaneous computation of omega, the errata evaluator polynomial: because A (the errata evaluator support polynomial) is not correctly initialized (I could not find any info in academic papers). So at the end, we get the correct errata evaluator polynomial omega + some higher order terms that should not be present, but since we know that sigma is always correct and the maximum degree should be the same as omega, we can fix omega by truncating too high order terms.\n        if omega.degree > sigma.degree: omega = Polynomial(omega.coefficients[-(sigma.degree+1):])\n\n        # Return the last result of the iterations (since BM compute iteratively, the last iteration being correct - it may already be before, but we're not sure)\n        return sigma, omega", "response": "Faster implementation of errata ( errors - and - erasures ) Berlekamp - Massey."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_error_evaluator(self, synd, sigma, k=None):\n        '''Compute the error (or erasures if you supply sigma=erasures locator polynomial) evaluator polynomial Omega from the syndrome and the error/erasures/errata locator Sigma. Omega is already computed at the same time as Sigma inside the Berlekamp-Massey implemented above, but in case you modify Sigma, you can recompute Omega afterwards using this method, or just ensure that Omega computed by BM is correct given Sigma (as long as syndrome and sigma are correct, omega will be correct).'''\n        n = self.n\n        if not k: k = self.k\n\n        # Omega(x) = [ (1 + Synd(x)) * Error_loc(x) ] mod x^(n-k+1)\n        # NOTE: I don't know why we do 1+Synd(x) here, from docs it seems just Synd(x) is enough (and in practice if you remove the \"ONE +\" it will still decode correcty) as advised by Blahut in Algebraic Codes for Data Transmission, but it seems it's an implementation detail here.\n        #ONE = Polynomial([GF2int(1)])\n        #return ((ONE + synd) * sigma) % Polynomial([GF2int(1)] + [GF2int(0)] * (n-k+1)) # NOT CORRECT: in practice it works flawlessly with this implementation (primitive polynomial = 3), but if you use another primitive like in reedsolo lib, it doesn't work! Thus, I guess that adding ONE is not correct for the general case.\n        return (synd * sigma) % Polynomial([GF2int(1)] + [GF2int(0)] * (n-k+1))", "response": "Compute the error or erasures evaluator polynomial Omega from the syndrome and the error / errata locator polynomial Omega from the syndrome and the error / erasures locator polynomial Sigma."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the error evaluator polynomial Omega from the syndrome and the error / errata locator polynomial Omega.", "response": "def _find_error_evaluator_fast(self, synd, sigma, k=None):\n        '''Compute the error (or erasures if you supply sigma=erasures locator polynomial) evaluator polynomial Omega from the syndrome and the error/erasures/errata locator Sigma. Omega is already computed at the same time as Sigma inside the Berlekamp-Massey implemented above, but in case you modify Sigma, you can recompute Omega afterwards using this method, or just ensure that Omega computed by BM is correct given Sigma (as long as syndrome and sigma are correct, omega will be correct).'''\n        n = self.n\n        if not k: k = self.k\n\n        # Omega(x) = [ Synd(x) * Error_loc(x) ] mod x^(n-k+1) -- From Blahut, Algebraic codes for data transmission, 2003\n        return (synd * sigma)._gffastmod(Polynomial([GF2int(1)] + [GF2int(0)] * (n-k+1)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _chien_search(self, sigma):\n        '''Recall the definition of sigma, it has s roots. To find them, this\n        function evaluates sigma at all 2^(c_exp-1) (ie: 255 for GF(2^8)) non-zero points to find the roots\n        The inverse of the roots are X_i, the error locations\n\n        Returns a list X of error locations, and a corresponding list j of\n        error positions (the discrete log of the corresponding X value) The\n        lists are up to s elements large.\n        \n        This is essentially an inverse Fourrier transform.\n\n        Important technical math note: This implementation is not actually\n        Chien's search. Chien's search is a way to evaluate the polynomial\n        such that each evaluation only takes constant time. This here simply\n        does 255 evaluations straight up, which is much less efficient.\n        Said differently, we simply do a bruteforce search by trial substitution to find the zeros of this polynomial, which identifies the error locations.\n        '''\n        # TODO: find a more efficient algorithm, this is the slowest part of the whole decoding process (~2.5 ms, while any other part is only ~400microsec). Could try the Pruned FFT from \"Simple Algorithms for BCH Decoding\", by Jonathan Hong and Martin Vetterli, IEEE Transactions on Communications, Vol.43, No.8, August 1995\n        X = []\n        j = []\n        p = GF2int(self.generator)\n        # Try for each possible location\n        for l in _range(1, self.gf2_charac+1): # range 1:256 is important: if you use range 0:255, if the last byte of the ecc symbols is corrupted, it won't be correctable! You need to use the range 1,256 to include this last byte.\n            #l = (i+self.fcr)\n            # These evaluations could be more efficient, but oh well\n            if sigma.evaluate( p**l ) == 0: # If it's 0, then bingo! It's an error location\n                # Compute the error location polynomial X (will be directly used to compute the errors magnitudes inside the Forney algorithm)\n                X.append( p**(-l) )\n                # Compute the coefficient position (not the error position, it's actually the reverse: we compute the degree of the term where the error is located. To get the error position, just compute n-1-j).\n                # This is different than the notes, I think the notes were in error\n                # Notes said j values were just l, when it's actually 255-l\n                j.append(self.gf2_charac - l)\n\n        # Sanity check: the number of errors/errata positions found should be exactly the same as the length of the errata locator polynomial\n        errs_nb = len(sigma) - 1 # compute the exact number of errors/errata that this error locator should find\n        if len(j) != errs_nb:\n            raise RSCodecError(\"Too many (or few) errors found by Chien Search for the errata locator polynomial!\")\n\n        return X, j", "response": "Recall the definition of sigma it has s roots. To find them it has s roots. To find them it has s roots. To find them it has s roots and the error positions of the error locations and j of the error positions of the roots."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _chien_search_fast(self, sigma):\n        '''Real chien search, we reuse the previous polynomial evaluation and just multiply by a constant polynomial. This should be faster, but it seems it's just the same speed as the other bruteforce version. However, it should easily be parallelizable.'''\n        # TODO: doesn't work when fcr is different than 1 (X values are incorrectly \"shifted\"...)\n        # TODO: try to mix this approach with the optimized walk on only interesting values, implemented in _chien_search_faster()\n        X = []\n        j = []\n        p = GF2int(self.generator)\n        if not hasattr(self, 'const_poly'): self.const_poly = [GF2int(self.generator)**(i+self.fcr) for i in _range(self.gf2_charac, -1, -1)] # constant polynomial that will allow us to update the previous polynomial evaluation to get the next one\n        const_poly = self.const_poly # caching for more efficiency since it never changes\n        ev_poly, ev = sigma.evaluate_array( p**1 ) # compute the first polynomial evaluation\n        # Try for each possible location\n        for l in _range(1, self.gf2_charac+1): # range 1:256 is important: if you use range 0:255, if the last byte of the ecc symbols is corrupted, it won't be correctable! You need to use the range 1,256 to include this last byte.\n            #l = (i+self.fcr)\n\n            # Check if it's a root for the polynomial\n            if ev == 0: # If it's 0, then bingo! It's an error location\n                # Compute the error location polynomial X (will be directly used to compute the errors magnitudes inside the Forney algorithm)\n                X.append( p**(-l) )\n                # Compute the coefficient position (not the error position, it's actually the reverse: we compute the degree of the term where the error is located. To get the error position, just compute n-1-j).\n                # This is different than the notes, I think the notes were in error\n                # Notes said j values were just l, when it's actually 255-l\n                j.append(self.gf2_charac - l)\n\n            # Update the polynomial evaluation for the next iteration\n            # we simply multiply each term[k] with alpha^k (where here alpha = p = GF2int(generator)).\n            # For more info, see the presentation by Andrew Brown, or this one: http://web.ntpu.edu.tw/~yshan/BCH_decoding.pdf\n            # TODO: parallelize this loop\n            for i in _range(1, len(ev_poly)+1): # TODO: maybe the fcr != 1 fix should be put here?\n                ev_poly[-i] *= const_poly[-i]\n            # Compute the new evaluation by just summing\n            ev = sum(ev_poly)\n\n        return X, j", "response": "Real chien search for the next set of ecc symbols."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the error magnitudes for a given polynomial X and k.", "response": "def _old_forney(self, omega, X, k=None):\n        '''Computes the error magnitudes (only works with errors or erasures under t = floor((n-k)/2), not with erasures above (n-k)//2)'''\n        # XXX Is floor division okay here? Should this be ceiling?\n        if not k: k = self.k\n        t = (self.n - k) // 2\n\n        Y = []\n\n        for l, Xl in enumerate(X):\n\n            # Compute the sequence product and multiply its inverse in\n            prod = GF2int(1) # just to init the product (1 is the neutral term for multiplication)\n            Xl_inv = Xl.inverse()\n            for ji in _range(t): # do not change to _range(len(X)) as can be seen in some papers, it won't give the correct result! (sometimes yes, but not always)\n                if ji == l:\n                    continue\n                if ji < len(X):\n                    Xj = X[ji]\n                else: # if above the maximum degree of the polynomial, then all coefficients above are just 0 (that's logical...)\n                    Xj = GF2int(0)\n                prod = prod * (Xl - Xj)\n                #if (ji != l):\n                #    prod = prod * (GF2int(1) - X[ji]*(Xl.inverse()))\n\n            # Compute Yl\n            Yl = Xl**t * omega.evaluate(Xl_inv) * Xl_inv * prod.inverse()\n\n            Y.append(Yl)\n        return Y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _forney(self, omega, X):\n        '''Computes the error magnitudes. Works also with erasures and errors+erasures beyond the (n-k)//2 bound, here the bound is 2*e+v <= (n-k-1) with e the number of errors and v the number of erasures.'''\n        # XXX Is floor division okay here? Should this be ceiling?\n\n        Y = [] # the final result, the error/erasures polynomial (contain the values that we should minus on the received message to get the repaired message)\n        Xlength = len(X)\n        for l, Xl in enumerate(X):\n\n            Xl_inv = Xl.inverse()\n\n            # Compute the formal derivative of the error locator polynomial (see Blahut, Algebraic codes for data transmission, pp 196-197).\n            # the formal derivative of the errata locator is used as the denominator of the Forney Algorithm, which simply says that the ith error value is given by error_evaluator(gf_inverse(Xi)) / error_locator_derivative(gf_inverse(Xi)). See Blahut, Algebraic codes for data transmission, pp 196-197.\n            sigma_prime_tmp = [1 - Xl_inv * X[j] for j in _range(Xlength) if j != l] # TODO? maybe a faster way would be to precompute sigma_prime = sigma[len(sigma) & 1:len(sigma):2] and then just do sigma_prime.evaluate(X[j]) ? (like in reedsolo.py)\n            # compute the product\n            sigma_prime = 1\n            for coef in sigma_prime_tmp:\n                sigma_prime = sigma_prime * coef\n            # equivalent to: sigma_prime = functools.reduce(mul, sigma_prime, 1)\n\n            # Compute Yl\n            # This is a more faithful translation of the theoretical equation contrary to the old forney method. Here it is exactly copy/pasted from the included presentation decoding_rs.pdf: Yl = omega(Xl.inverse()) / prod(1 - Xj*Xl.inverse()) for j in len(X) (in the paper it's for j in s, but it's useless when len(X) < s because we compute neutral terms 1 for nothing, and wrong when correcting more than s erasures or erasures+errors since it prevents computing all required terms).\n            # Thus here this method works with erasures too because firstly we fixed the equation to be like the theoretical one (don't know why it was modified in _old_forney(), if it's an optimization, it doesn't enhance anything), and secondly because we removed the product bound on s, which prevented computing errors and erasures above the s=(n-k)//2 bound.\n            # The best resource I have found for the correct equation is https://en.wikipedia.org/wiki/Forney_algorithm -- note that in the article, fcr is defined as c.\n            Yl = - (Xl**(1-self.fcr)  * omega.evaluate(Xl_inv) / sigma_prime) # sigma_prime is the denominator of the Forney algorithm\n\n            Y.append(Yl)\n        return Y", "response": "Compute the error magnitudes. Works also with erasures and errors + erasures beyond the 2 bound here the bound is 2 * e + v < = n - k"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n        try:\n            p = Popen(['/bin/ps', '-p%s' % self.pid, '-o', 'rss,vsz'],\n                      stdout=PIPE, stderr=PIPE)\n        except OSError: # pragma: no cover\n            pass\n        else:\n            s = p.communicate()[0].split()\n            if p.returncode == 0 and len(s) >= 2: # pragma: no branch\n                self.vsz = int(s[-1]) * 1024\n                self.rss = int(s[-2]) * 1024\n                return True\n        return False", "response": "Update the virtual and resident size of the current process via ps. Returns True if it was\n        successful."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the internal state of the object.", "response": "def update(self):\n        \"\"\"\n        Get virtual size of current process by reading the process' stat file.\n        This should work for Linux.\n        \"\"\"\n        try:\n            stat = open('/proc/self/stat')\n            status = open('/proc/self/status')\n        except IOError: # pragma: no cover\n            return False\n        else:\n            stats = stat.read().split()\n            self.vsz = int( stats[22] )\n            self.rss = int( stats[23] ) * self.pagesize\n            self.pagefaults = int( stats[11] )\n\n            for entry in status.readlines():\n                key, value = entry.split(':')\n                size_in_bytes = lambda x: int(x.split()[0]) * 1024\n                if key == 'VmData':\n                    self.data_segment = size_in_bytes(value)\n                elif key == 'VmExe':\n                    self.code_segment = size_in_bytes(value)\n                elif key == 'VmLib':\n                    self.shared_segment = size_in_bytes(value)\n                elif key == 'VmStk':\n                    self.stack_segment = size_in_bytes(value)\n                key = self.key_map.get(key)\n                if key:\n                    self.os_specific.append((key, value.strip()))\n\n\n            stat.close()\n            status.close()\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateControls(self):\n        wx.EVT_LIST_COL_CLICK(self, self.GetId(), self.OnReorder)\n        wx.EVT_LIST_ITEM_SELECTED(self, self.GetId(), self.OnNodeSelected)\n        wx.EVT_MOTION(self, self.OnMouseMove)\n        wx.EVT_LIST_ITEM_ACTIVATED(self, self.GetId(), self.OnNodeActivated)\n        self.CreateColumns()", "response": "Create our sub - controls"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateColumns( self ):\n        self.SetItemCount(0)\n        # clear any current columns...\n        for i in range( self.GetColumnCount())[::-1]:\n            self.DeleteColumn( i )\n        # now create\n        for i, column in enumerate(self.columns):\n            column.index = i\n            self.InsertColumn(i, column.name)\n            if not windows or column.targetWidth is None:\n                self.SetColumnWidth(i, wx.LIST_AUTOSIZE)\n            else:\n                self.SetColumnWidth(i, column.targetWidth)", "response": "Create our column definitions from current self. columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the columns to a set of values other than the originals and recreates column controls", "response": "def SetColumns( self, columns, sortOrder=None ):\n        \"\"\"Set columns to a set of values other than the originals and recreates column controls\"\"\"\n        self.columns = columns \n        self.sortOrder = [(x.defaultOrder,x) for x in self.columns if x.sortDefault]\n        self.CreateColumns()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef OnNodeActivated(self, event):\n        try:\n            node = self.sorted[event.GetIndex()]\n        except IndexError, err:\n            log.warn(_('Invalid index in node activated: %(index)s'),\n                     index=event.GetIndex())\n        else:\n            wx.PostEvent(\n                self,\n                squaremap.SquareActivationEvent(node=node, point=None,\n                                                map=None)\n            )", "response": "Event handler for node activated event"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef OnNodeSelected(self, event):\n        try:\n            node = self.sorted[event.GetIndex()]\n        except IndexError, err:\n            log.warn(_('Invalid index in node selected: %(index)s'),\n                     index=event.GetIndex())\n        else:\n            if node is not self.selected_node:\n                wx.PostEvent(\n                    self,\n                    squaremap.SquareSelectionEvent(node=node, point=None,\n                                                   map=None)\n                )", "response": "Event handler for node selection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting this node to indicated status", "response": "def SetIndicated(self, node):\n        \"\"\"Set this node to indicated status\"\"\"\n        self.indicated_node = node\n        self.indicated = self.NodeToIndex(node)\n        self.Refresh(False)\n        return self.indicated"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SetSelected(self, node):\n        self.selected_node = node\n        index = self.NodeToIndex(node)\n        if index != -1:\n            self.Focus(index)\n            self.Select(index, True)\n        return index", "response": "Set our selected node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a new record is reordered", "response": "def OnReorder(self, event):\n        \"\"\"Given a request to reorder, tell us to reorder\"\"\"\n        column = self.columns[event.GetColumn()]\n        return self.ReorderByColumn( column )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreorder the set of records by a column.", "response": "def ReorderByColumn( self, column ):\n        \"\"\"Reorder the set of records by column\"\"\"\n        # TODO: store current selection and re-select after sorting...\n        single_column = self.SetNewOrder( column )\n        self.reorder( single_column = True )\n        self.Refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SetNewOrder( self, column ):\n        if column.sortOn:\n            # multiple sorts for the click...\n            columns = [self.columnByAttribute(attr) for attr in column.sortOn]\n            diff = [(a, b) for a, b in zip(self.sortOrder, columns)\n                    if b is not a[1]]\n            if not diff:\n                self.sortOrder[0] = (not self.sortOrder[0][0], column)\n            else:\n                self.sortOrder = [\n                    (c.defaultOrder, c) for c in columns\n                ] + [(a, b) for (a, b) in self.sortOrder if b not in columns]\n            return False\n        else:\n            if column is self.sortOrder[0][1]:\n                # reverse current major order\n                self.sortOrder[0] = (not self.sortOrder[0][0], column)\n            else:\n                self.sortOrder = [(column.defaultOrder, column)] + [\n                    (a, b)\n                    for (a, b) in self.sortOrder if b is not column\n                ]\n            return True", "response": "Sets new sorting order based on column."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reorder(self, single_column=False):\n        if single_column:\n            columns = self.sortOrder[:1]\n        else:\n            columns = self.sortOrder\n        for ascending,column in columns[::-1]:\n            # Python 2.2+ guarantees stable sort, so sort by each column in reverse \n            # order will order by the assigned columns \n            self.sorted.sort( key=column.get, reverse=(not ascending))", "response": "Force a reorder of the displayed items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef integrateRecords(self, functions):\n        self.SetItemCount(len(functions))\n        self.sorted = functions[:]\n        self.reorder()\n        self.Refresh()", "response": "Integrate records from the loader"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OnGetItemAttr(self, item):\n        if self.indicated > -1 and item == self.indicated:\n            return self.indicated_attribute\n        return None", "response": "Called when the user has requested the attribute of the item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef OnGetItemText(self, item, col):\n        # TODO: need to format for rjust and the like...\n        try:\n            column = self.columns[col]\n            value = column.get(self.sorted[item])\n        except IndexError, err:\n            return None\n        else:\n            if value is None:\n                return u''\n            if column.percentPossible and self.percentageView and self.total:\n                value = value / float(self.total) * 100.00\n            if column.format:\n                try:\n                    return column.format % (value,)\n                except Exception, err:\n                    log.warn('Column %s could not format %r value: %r',\n                        column.name, type(value), value\n                    )\n                    value = column.get(self.sorted[item] )\n                    if isinstance(value,(unicode,str)):\n                        return value\n                    return unicode(value)\n            else:\n                if isinstance(value,(unicode,str)):\n                    return value\n                return unicode(value)", "response": "Retrieve text for the item and column respectively"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes the input data with reed - solomon error correction in 223 byte blocks and outputs each block along with 32 parity bytes to a new file by .", "response": "def encode(input, output_filename):\n    \"\"\"Encodes the input data with reed-solomon error correction in 223 byte\n    blocks, and outputs each block along with 32 parity bytes to a new file by\n    the given filename.\n\n    input is a file-like object\n\n    The outputted image will be in png format, and will be 255 by x pixels with\n    one color channel. X is the number of 255 byte blocks from the input. Each\n    block of data will be one row, therefore, the data can be recovered if no\n    more than 16 pixels per row are altered.\n    \"\"\"\n    coder = rs.RSCoder(255,223)\n\n    output = []\n\n    while True:\n        block = input.read(223)\n        if not block: break\n        code = coder.encode_fast(block)\n        output.append(code)\n        sys.stderr.write(\".\")\n\n    sys.stderr.write(\"\\n\")\n\n    out = Image.new(\"L\", (rowstride,len(output)))\n    out.putdata(\"\".join(output))\n    out.save(output_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef redirect(url, code=303):\n    scriptname = request.environ.get('SCRIPT_NAME', '').rstrip('/') + '/'\n    location = urljoin(request.url, urljoin(scriptname, url))\n    raise HTTPResponse(\"\", status=code, header=dict(Location=location))", "response": "Abort execution and causes a 303 redirect"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_date(ims):\n    try:\n        ts = email.utils.parsedate_tz(ims)\n        return time.mktime(ts[:8] + (0,)) - (ts[9] or 0) - time.timezone\n    except (TypeError, ValueError, IndexError):\n        return None", "response": "Parse rfc1123 rfc850 and asctime timestamps and return UTC epoch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(app=None, server=WSGIRefServer, host='127.0.0.1', port=8080,\n        interval=1, reloader=False, **kargs):\n    \"\"\" Runs bottle as a web server. \"\"\"\n    app = app if app else default_app()\n    quiet = bool(kargs.get('quiet', False))\n    # Instantiate server, if it is a class instead of an instance\n    if isinstance(server, type):\n        server = server(host=host, port=port, **kargs)\n    if not isinstance(server, ServerAdapter):\n        raise RuntimeError(\"Server must be a subclass of WSGIAdapter\")\n    if not quiet and isinstance(server, ServerAdapter): # pragma: no cover\n        if not reloader or os.environ.get('BOTTLE_CHILD') == 'true':\n            print(\"Bottle server starting up (using %s)...\" % repr(server))\n            print(\"Listening on http://%s:%d/\" % (server.host, server.port))\n            print(\"Use Ctrl-C to quit.\")\n            print()\n        else:\n            print(\"Bottle auto reloader starting up...\")\n    try:\n        if reloader and interval:\n            reloader_run(server, app, interval)\n        else:\n            server.run(app)\n    except KeyboardInterrupt:\n        if not quiet: # pragma: no cover\n            print(\"Shutting Down...\")", "response": "Runs bottle as a web server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a template string.", "response": "def template(tpl, template_adapter=SimpleTemplate, **kwargs):\n    '''\n    Get a rendered template as a string iterator.\n    You can use a name, a filename or a template string as first parameter.\n    '''\n    if tpl not in TEMPLATES or DEBUG:\n        settings = kwargs.get('template_settings',{})\n        lookup = kwargs.get('template_lookup', TEMPLATE_PATH)\n        if isinstance(tpl, template_adapter):\n            TEMPLATES[tpl] = tpl\n            if settings: TEMPLATES[tpl].prepare(settings)\n        elif \"\\n\" in tpl or \"{\" in tpl or \"%\" in tpl or '$' in tpl:\n            TEMPLATES[tpl] = template_adapter(source=tpl, lookup=lookup, settings=settings)\n        else:\n            TEMPLATES[tpl] = template_adapter(name=tpl, lookup=lookup, settings=settings)\n    if not TEMPLATES[tpl]:\n        abort(500, 'Template (%s) not found' % tpl)\n    kwargs['abort'] = abort\n    kwargs['request'] = request\n    kwargs['response'] = response\n    return TEMPLATES[tpl].render(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of ( type value ) tokens.", "response": "def tokens(self):\n        \"\"\" Return a list of (type, value) tokens. \"\"\"\n        if not self._tokens:\n            self._tokens = list(self.tokenise(self.route))\n        return self._tokens"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenise(cls, route):\n        ''' Split a string into an iterator of (type, value) tokens. '''\n        match = None\n        for match in cls.syntax.finditer(route):\n            pre, name, rex = match.groups()\n            if pre: yield ('TXT', pre.replace('\\\\:',':'))\n            if rex and name: yield ('VAR', (rex, name))\n            elif name: yield ('VAR', (cls.default, name))\n            elif rex: yield ('ANON', rex)\n        if not match:\n            yield ('TXT', route.replace('\\\\:',':'))\n        elif match.end() < len(route):\n            yield ('TXT', route[match.end():].replace('\\\\:',':'))", "response": "Split a string into an iterator of ( type value ) tokens."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a regexp pattern with named groups", "response": "def group_re(self):\n        ''' Return a regexp pattern with named groups '''\n        out = ''\n        for token, data in self.tokens():\n            if   token == 'TXT':  out += re.escape(data)\n            elif token == 'VAR':  out += '(?P<%s>%s)' % (data[1], data[0])\n            elif token == 'ANON': out += '(?:%s)' % data\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a format string with named fields.", "response": "def format_str(self):\n        ''' Return a format string with named fields. '''\n        if self.static:\n            return self.route.replace('%','%%')\n        out, i = '', 0\n        for token, value in self.tokens():\n            if token == 'TXT': out += value.replace('%','%%')\n            elif token == 'ANON': out += '%%(anon%d)s' % i; i+=1\n            elif token == 'VAR': out += '%%(%s)s' % value[1]\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn true if the route contains dynamic parts", "response": "def is_dynamic(self):\n        ''' Return true if the route contains dynamic parts '''\n        if not self._static:\n            for token, value in self.tokens():\n                if token != 'TXT':\n                    return True\n        self._static = True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmatch an URL and returns a tuple of handler target groups", "response": "def match(self, uri):\n        ''' Matches an URL and returns a (handler, target) tuple '''\n        if uri in self.static:\n            return self.static[uri], {}\n        for combined, subroutes in self.dynamic:\n            match = combined.match(uri)\n            if not match: continue\n            target, groups = subroutes[match.lastindex - 1]\n            groups = groups.match(uri).groupdict() if groups else {}\n            return target, groups\n        return None, {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, route_name, **args):\n        ''' Builds an URL out of a named route and some parameters.'''\n        try:\n            return self.named[route_name] % args\n        except KeyError:\n            raise RouteBuildError(\"No route found with name '%s'.\" % route_name)", "response": "Builds an URL out of a named route and some parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a new output filter. Whenever bottle hits a handler output matching ftype func is applied to it.", "response": "def add_filter(self, ftype, func):\n        ''' Register a new output filter. Whenever bottle hits a handler output\n            matching `ftype`, `func` is applyed to it. '''\n        if not isinstance(ftype, type):\n            raise TypeError(\"Expected type object, got %s\" % type(ftype))\n        self.castfilter = [(t, f) for (t, f) in self.castfilter if t != ftype]\n        self.castfilter.append((ftype, func))\n        self.castfilter.sort()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds a callback bound to a path and a specific HTTP method.", "response": "def match_url(self, path, method='GET'):\n        \"\"\" Find a callback bound to a path and a specific HTTP method.\n            Return (callback, param) tuple or (None, {}).\n            method: HEAD falls back to GET. HEAD and GET fall back to ALL.\n        \"\"\"\n        path = path.strip().lstrip('/')\n        handler, param = self.routes.match(method + ';' + path)\n        if handler: return handler, param\n        if method == 'HEAD':\n            handler, param = self.routes.match('GET;' + path)\n            if handler: return handler, param\n        handler, param = self.routes.match('ANY;' + path)\n        if handler: return handler, param\n        return None, {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_url(self, routename, **kargs):\n        return '/' + self.routes.build(routename, **kargs).split(';', 1)[1]", "response": "Return a string that matches a named route"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbinding a new WSGI enviroment and clear out all previously computed content.", "response": "def bind(self, environ, app=None):\n        \"\"\" Bind a new WSGI enviroment and clear out all previously computed\n            attributes.\n            \n            This is done automatically for the global `bottle.request`\n            instance on every request.\n        \"\"\"\n        if isinstance(environ, Request): # Recycle already parsed content\n            for key in self.__dict__: #TODO: Test this\n                setattr(self, key, getattr(environ, key))\n            self.app = app\n            return\n        self._GET = self._POST = self._GETPOST = self._COOKIES = None\n        self._body = self._header = None\n        self.environ = environ\n        self.app = app\n        # These attributes are used anyway, so it is ok to compute them here\n        self.path = '/' + environ.get('PATH_INFO', '/').lstrip('/')\n        self.method = environ.get('REQUEST_METHOD', 'GET').upper()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshift some levels of PATH_INFO into SCRIPT_NAME and return the moved part. count defaults to 1", "response": "def path_shift(self, count=1):\n        ''' Shift some levels of PATH_INFO into SCRIPT_NAME and return the\n            moved part. count defaults to 1'''\n        #/a/b/  /c/d  --> 'a','b'  'c','d'\n        if count == 0: return ''\n        pathlist = self.path.strip('/').split('/')\n        scriptlist = self.environ.get('SCRIPT_NAME','/').strip('/').split('/')\n        if pathlist and pathlist[0] == '': pathlist = []\n        if scriptlist and scriptlist[0] == '': scriptlist = []\n        if count > 0 and count <= len(pathlist):\n            moved = pathlist[:count]\n            scriptlist = scriptlist + moved\n            pathlist = pathlist[count:]\n        elif count < 0 and count >= -len(scriptlist):\n            moved = scriptlist[count:]\n            pathlist = moved + pathlist\n            scriptlist = scriptlist[:count]\n        else:\n            empty = 'SCRIPT_NAME' if count < 0 else 'PATH_INFO'\n            raise AssertionError(\"Cannot shift. Nothing left from %s\" % empty)\n        self['PATH_INFO'] = self.path =  '/' + '/'.join(pathlist) \\\n                          + ('/' if self.path.endswith('/') and pathlist else '')\n        self['SCRIPT_NAME'] = '/' + '/'.join(scriptlist)\n        return '/'.join(moved)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the full URL as requested by the client.", "response": "def url(self):\n        \"\"\" Full URL as requested by the client (computed).\n\n            This value is constructed out of different environment variables\n            and includes scheme, host, port, scriptname, path and query string. \n        \"\"\"\n        scheme = self.environ.get('wsgi.url_scheme', 'http')\n        host   = self.environ.get('HTTP_X_FORWARDED_HOST', self.environ.get('HTTP_HOST', None))\n        if not host:\n            host = self.environ.get('SERVER_NAME')\n            port = self.environ.get('SERVER_PORT', '80')\n            if scheme + port not in ('https443', 'http80'):\n                host += ':' + port\n        parts = (scheme, host, urlquote(self.fullpath), self.query_string, '')\n        return urlunsplit(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef POST(self):\n        if self._POST is None:\n            save_env = dict() # Build a save environment for cgi\n            for key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):\n                if key in self.environ:\n                    save_env[key] = self.environ[key]\n            save_env['QUERY_STRING'] = '' # Without this, sys.argv is called!\n            if TextIOWrapper:\n                fb = TextIOWrapper(self.body, encoding='ISO-8859-1')\n            else:\n                fb = self.body\n            data = cgi.FieldStorage(fp=fb, environ=save_env)\n            self._POST = MultiDict()\n            for item in data.list:\n                self._POST[item.name] = item if item.filename else item.value\n        return self._POST", "response": "The HTTP POST body parsed into a MultiDict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef params(self):\n        if self._GETPOST is None:\n            self._GETPOST = MultiDict(self.GET)\n            self._GETPOST.update(dict(self.POST))\n        return self._GETPOST", "response": "A combined MultiDict with POST and GET parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cookie(self, *args):\n        value = self.COOKIES.get(*args)\n        sec = self.app.config['securecookie.key']\n        dec = cookie_decode(value, sec)\n        return dec or value", "response": "Return the ( decoded ) value of a cookie."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self):\n        ''' Returns a copy of self '''\n        copy = Response(self.app)\n        copy.status = self.status\n        copy.headers = self.headers.copy()\n        copy.content_type = self.content_type\n        return copy", "response": "Returns a copy of self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a wsgi conform list of header value pairs.", "response": "def wsgiheader(self):\n        ''' Returns a wsgi conform list of header/value pairs. '''\n        for c in list(self.COOKIES.values()):\n            if c.OutputString() not in self.headers.getall('Set-Cookie'):\n                self.headers.append('Set-Cookie', c.OutputString())\n        return list(self.headers.iterallitems())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_cookie(self, key, value, **kargs):\n        if not isinstance(value, str):\n            sec = self.app.config['securecookie.key']\n            value = cookie_encode(value, sec).decode('ascii') #2to3 hack\n        self.COOKIES[key] = value\n        for k, v in kargs.items():\n            self.COOKIES[key][k.replace('_', '-')] = v", "response": "Add a new cookie with various options."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the source file and returns the AST of the source file.", "response": "def parse_source_file(file_name):\n  \"\"\"\n  Parses the AST of Python file for lines containing\n  references to the argparse module.\n\n  returns the collection of ast objects found.\n\n  Example client code:\n\n    1. parser = ArgumentParser(desc=\"My help Message\")\n    2. parser.add_argument('filename', help=\"Name of the file to load\")\n    3. parser.add_argument('-f', '--format', help='Format of output \\nOptions: ['md', 'html']\n    4. args = parser.parse_args()\n\n  Variables:\n    * nodes \t\t\t\t\t\t\t\t\tPrimary syntax tree object\n    *\targparse_assignments   \tThe assignment of the ArgumentParser (line 1 in example code)\n    * add_arg_assignments     Calls to add_argument() (lines 2-3 in example code)\n    * parser_var_name\t\t\t\t\tThe instance variable of the ArgumentParser (line 1 in example code)\n    * ast_source\t\t\t\t\t\t\tThe curated collection of all parser related nodes in the client code\n  \"\"\"\n\n  nodes = ast.parse(_openfile(file_name))\n\n  module_imports = get_nodes_by_instance_type(nodes, _ast.Import)\n  specific_imports = get_nodes_by_instance_type(nodes, _ast.ImportFrom)\n\n  assignment_objs = get_nodes_by_instance_type(nodes, _ast.Assign)\n  call_objects = get_nodes_by_instance_type(nodes, _ast.Call)\n\n  argparse_assignments = get_nodes_by_containing_attr(assignment_objs, 'ArgumentParser')\n  add_arg_assignments  = get_nodes_by_containing_attr(call_objects, 'add_argument')\n  parse_args_assignment = get_nodes_by_containing_attr(call_objects, 'parse_args')\n\n  ast_argparse_source = chain(\n    module_imports,\n    specific_imports,\n    argparse_assignments,\n    add_arg_assignments\n    # parse_args_assignment\n  )\n  return ast_argparse_source"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the memory usage of a process or piece of code.", "response": "def memory_usage(proc=-1, interval=.1, timeout=None, timestamps=False,\n                 include_children=False, max_usage=False, retval=False,\n                 stream=None):\n    \"\"\"\n    Return the memory usage of a process or piece of code\n\n    Parameters\n    ----------\n    proc : {int, string, tuple, subprocess.Popen}, optional\n        The process to monitor. Can be given by an integer/string\n        representing a PID, by a Popen object or by a tuple\n        representing a Python function. The tuple contains three\n        values (f, args, kw) and specifies to run the function\n        f(*args, **kw).\n        Set to -1 (default) for current process.\n\n    interval : float, optional\n        Interval at which measurements are collected.\n\n    timeout : float, optional\n        Maximum amount of time (in seconds) to wait before returning.\n\n    max_usage : bool, optional\n        Only return the maximum memory usage (default False)\n\n    retval : bool, optional\n        For profiling python functions. Save the return value of the profiled\n        function. Return value of memory_usage becomes a tuple:\n        (mem_usage, retval)\n\n    timestamps : bool, optional\n        if True, timestamps of memory usage measurement are collected as well.\n\n    stream : File\n        if stream is a File opened with write access, then results are written\n        to this file instead of stored in memory and returned at the end of\n        the subprocess. Useful for long-running processes.\n        Implies timestamps=True.\n\n    Returns\n    -------\n    mem_usage : list of floating-poing values\n        memory usage, in MiB. It's length is always < timeout / interval\n        if max_usage is given, returns the two elements maximum memory and\n        number of measurements effectuated\n    ret : return value of the profiled function\n        Only returned if retval is set to True\n    \"\"\"\n    if stream is not None:\n        timestamps = True\n\n    if not max_usage:\n        ret = []\n    else:\n        ret = -1\n\n    if timeout is not None:\n        max_iter = int(timeout / interval)\n    elif isinstance(proc, int):\n        # external process and no timeout\n        max_iter = 1\n    else:\n        # for a Python function wait until it finishes\n        max_iter = float('inf')\n\n    if hasattr(proc, '__call__'):\n        proc = (proc, (), {})\n    if isinstance(proc, (list, tuple)):\n        if len(proc) == 1:\n            f, args, kw = (proc[0], (), {})\n        elif len(proc) == 2:\n            f, args, kw = (proc[0], proc[1], {})\n        elif len(proc) == 3:\n            f, args, kw = (proc[0], proc[1], proc[2])\n        else:\n            raise ValueError\n\n        while True:\n            child_conn, parent_conn = Pipe()  # this will store MemTimer's results\n            p = MemTimer(os.getpid(), interval, child_conn, timestamps=timestamps,\n                      max_usage=max_usage, include_children=include_children)\n            p.start()\n            parent_conn.recv()  # wait until we start getting memory\n            returned = f(*args, **kw)\n            parent_conn.send(0)  # finish timing\n            ret = parent_conn.recv()\n            n_measurements = parent_conn.recv()\n            if retval:\n                ret = ret, returned\n            p.join(5 * interval)\n            if n_measurements > 4 or interval < 1e-6:\n                break\n            interval /= 10.\n    elif isinstance(proc, subprocess.Popen):\n        # external process, launched from Python\n        line_count = 0\n        while True:\n            if not max_usage:\n                mem_usage = _get_memory(proc.pid, timestamps=timestamps,\n                                        include_children=include_children)\n                if stream is not None:\n                    stream.write(\"MEM {0:.6f} {1:.4f}\\n\".format(*mem_usage))\n                else:\n                    ret.append(mem_usage)\n            else:\n                ret = max([ret,\n                           _get_memory(proc.pid,\n                                       include_children=include_children)])\n            time.sleep(interval)\n            line_count += 1\n            # flush every 50 lines. Make 'tail -f' usable on profile file\n            if line_count > 50:\n                line_count = 0\n                if stream is not None:\n                    stream.flush()\n            if timeout is not None:\n                max_iter -= 1\n                if max_iter == 0:\n                    break\n            if proc.poll() is not None:\n                break\n    else:\n        # external process\n        if max_iter == -1:\n            max_iter = 1\n        counter = 0\n        while counter < max_iter:\n            counter += 1\n            if not max_usage:\n                mem_usage = _get_memory(proc, timestamps=timestamps,\n                                        include_children=include_children)\n                if stream is not None:\n                    stream.write(\"MEM {0:.6f} {1:.4f}\\n\".format(*mem_usage))\n                else:\n                    ret.append(mem_usage)\n            else:\n                ret = max([ret,\n                           _get_memory(proc, include_children=include_children)\n                           ])\n\n            time.sleep(interval)\n            # Flush every 50 lines.\n            if counter % 50 == 0 and stream is not None:\n                stream.flush()\n    if stream:\n        return None\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_script(script_name):\n    if os.path.isfile(script_name):\n        return script_name\n    path = os.getenv('PATH', os.defpath).split(os.pathsep)\n    for folder in path:\n        if not folder:\n            continue\n        fn = os.path.join(folder, script_name)\n        if os.path.isfile(fn):\n            return fn\n\n    sys.stderr.write('Could not find script {0}\\n'.format(script_name))\n    raise SystemExit(1)", "response": "Find the script in the system path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef magic_mprun(self, parameter_s=''):\n    try:\n        from StringIO import StringIO\n    except ImportError:  # Python 3.x\n        from io import StringIO\n\n    # Local imports to avoid hard dependency.\n    from distutils.version import LooseVersion\n    import IPython\n    ipython_version = LooseVersion(IPython.__version__)\n    if ipython_version < '0.11':\n        from IPython.genutils import page\n        from IPython.ipstruct import Struct\n        from IPython.ipapi import UsageError\n    else:\n        from IPython.core.page import page\n        from IPython.utils.ipstruct import Struct\n        from IPython.core.error import UsageError\n\n    # Escape quote markers.\n    opts_def = Struct(T=[''], f=[])\n    parameter_s = parameter_s.replace('\"', r'\\\"').replace(\"'\", r\"\\'\")\n    opts, arg_str = self.parse_options(parameter_s, 'rf:T:c', list_all=True)\n    opts.merge(opts_def)\n    global_ns = self.shell.user_global_ns\n    local_ns = self.shell.user_ns\n\n    # Get the requested functions.\n    funcs = []\n    for name in opts.f:\n        try:\n            funcs.append(eval(name, global_ns, local_ns))\n        except Exception as e:\n            raise UsageError('Could not find function %r.\\n%s: %s' % (name,\n                             e.__class__.__name__, e))\n\n    include_children = 'c' in opts\n    profile = LineProfiler(include_children=include_children)\n    for func in funcs:\n        profile(func)\n\n    # Add the profiler to the builtins for @profile.\n    try:\n        import builtins\n    except ImportError:  # Python 3x\n        import __builtin__ as builtins\n\n    if 'profile' in builtins.__dict__:\n        had_profile = True\n        old_profile = builtins.__dict__['profile']\n    else:\n        had_profile = False\n        old_profile = None\n    builtins.__dict__['profile'] = profile\n\n    try:\n        try:\n            profile.runctx(arg_str, global_ns, local_ns)\n            message = ''\n        except SystemExit:\n            message = \"*** SystemExit exception caught in code being profiled.\"\n        except KeyboardInterrupt:\n            message = (\"*** KeyboardInterrupt exception caught in code being \"\n                       \"profiled.\")\n    finally:\n        if had_profile:\n            builtins.__dict__['profile'] = old_profile\n\n    # Trap text output.\n    stdout_trap = StringIO()\n    show_results(profile, stdout_trap)\n    output = stdout_trap.getvalue()\n    output = output.rstrip()\n\n    if ipython_version < '0.11':\n        page(output, screen_lines=self.shell.rc.screen_length)\n    else:\n        page(output)\n    print(message,)\n\n    text_file = opts.T[0]\n    if text_file:\n        with open(text_file, 'w') as pfile:\n            pfile.write(output)\n        print('\\n*** Profile printout saved to text file %s. %s' % (text_file,\n                                                                    message))\n\n    return_value = None\n    if 'r' in opts:\n        return_value = profile\n\n    return return_value", "response": "Execute a magic statement under the line - by - line memory profiler module."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmeasures memory usage of a Python statement.", "response": "def magic_memit(self, line=''):\n    \"\"\"Measure memory usage of a Python statement\n\n    Usage, in line mode:\n      %memit [-r<R>t<T>i<I>] statement\n\n    Options:\n    -r<R>: repeat the loop iteration <R> times and take the best result.\n    Default: 1\n\n    -t<T>: timeout after <T> seconds. Default: None\n\n    -i<I>: Get time information at an interval of I times per second.\n        Defaults to 0.1 so that there is ten measurements per second.\n\n    -c: If present, add the memory usage of any children process to the report.\n\n    Examples\n    --------\n    ::\n\n      In [1]: import numpy as np\n\n      In [2]: %memit np.zeros(1e7)\n      maximum of 1: 76.402344 MiB per loop\n\n      In [3]: %memit np.ones(1e6)\n      maximum of 1: 7.820312 MiB per loop\n\n      In [4]: %memit -r 10 np.empty(1e8)\n      maximum of 10: 0.101562 MiB per loop\n\n    \"\"\"\n    opts, stmt = self.parse_options(line, 'r:t:i:c', posix=False, strict=False)\n    repeat = int(getattr(opts, 'r', 1))\n    if repeat < 1:\n        repeat == 1\n    timeout = int(getattr(opts, 't', 0))\n    if timeout <= 0:\n        timeout = None\n    interval = float(getattr(opts, 'i', 0.1))\n    include_children = 'c' in opts\n\n    # I've noticed we get less noisier measurements if we run\n    # a garbage collection first\n    import gc\n    gc.collect()\n\n    mem_usage = 0\n    counter = 0\n    baseline = memory_usage()[0]\n    while counter < repeat:\n        counter += 1\n        tmp = memory_usage((_func_exec, (stmt, self.shell.user_ns)),\n                           timeout=timeout, interval=interval, max_usage=True,\n                           include_children=include_children)\n        mem_usage = max(mem_usage, tmp[0])\n\n    if mem_usage:\n        print('peak memory: %.02f MiB, increment: %.02f MiB' %\n              (mem_usage, mem_usage - baseline))\n    else:\n        print('ERROR: could not read memory usage, try with a lower interval '\n              'or more iterations')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timestamp(self, name=\"<block>\"):\n        # Make a fake function\n        func = lambda x: x\n        func.__module__ = \"\"\n        func.__name__ = name\n        self.add_function(func)\n        timestamps = []\n        self.functions[func].append(timestamps)\n        # A new object is required each time, since there can be several\n        # nested context managers.\n        return _TimeStamperCM(timestamps)", "response": "Returns a context manager for timestamping a block of code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrap_function(self, func):\n        def f(*args, **kwds):\n            # Start time\n            timestamps = [_get_memory(os.getpid(), timestamps=True)]\n            self.functions[func].append(timestamps)\n            try:\n                result = func(*args, **kwds)\n            finally:\n                # end time\n                timestamps.append(_get_memory(os.getpid(), timestamps=True))\n            return result\n        return f", "response": "Wrap a function to timestamp it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a Python function to the line profiling information.", "response": "def add_function(self, func):\n        \"\"\" Record line profiling information for the given Python function.\n        \"\"\"\n        try:\n            # func_code does not exist in Python3\n            code = func.__code__\n        except AttributeError:\n            warnings.warn(\"Could not extract a code object for the object %r\"\n                          % func)\n        else:\n            self.add_code(code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, cmd):\n        # TODO: can this be removed ?\n        import __main__\n        main_dict = __main__.__dict__\n        return self.runctx(cmd, main_dict, main_dict)", "response": "Profile a single executable statement in the main namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trace_memory_usage(self, frame, event, arg):\n        if (event in ('call', 'line', 'return')\n                and frame.f_code in self.code_map):\n            if event != 'call':\n                # \"call\" event just saves the lineno but not the memory\n                mem = _get_memory(-1, include_children=self.include_children)\n                # if there is already a measurement for that line get the max\n                old_mem = self.code_map[frame.f_code].get(self.prevline, 0)\n                self.code_map[frame.f_code][self.prevline] = max(mem, old_mem)\n            self.prevline = frame.f_lineno\n\n        if self._original_trace_function is not None:\n            (self._original_trace_function)(frame, event, arg)\n\n        return self.trace_memory_usage", "response": "Callback for sys. settrace"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_root( self, key ):\n        if key not in self.roots:\n            function = getattr( self, 'load_%s'%(key,) )()\n            self.roots[key] = function\n        return self.roots[key]", "response": "Retrieve a given declared root by root - type - key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the set of rows for the type - key", "response": "def get_rows( self, key ):\n        \"\"\"Get the set of rows for the type-key\"\"\"\n        if key not in self.roots:\n            self.get_root( key )\n        if key == 'location':\n            return self.location_rows \n        else:\n            return self.rows"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a squaremap - compatible model from a pstats class", "response": "def load( self, stats ):\n        \"\"\"Build a squaremap-compatible model from a pstats class\"\"\"\n        rows = self.rows\n        for func, raw in stats.iteritems():\n            try:\n                rows[func] = row = PStatRow( func,raw )\n            except ValueError, err:\n                log.info( 'Null row: %s', func )\n        for row in rows.itervalues():\n            row.weave( rows )\n        return self.find_root( rows )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_root( self, rows ):\n        maxes = sorted( rows.values(), key = lambda x: x.cumulative )\n        if not maxes:\n            raise RuntimeError( \"\"\"Null results!\"\"\" )\n        root = maxes[-1]\n        roots = [root]\n        for key,value in rows.items():\n            if not value.parents:\n                log.debug( 'Found node root: %s', value )\n                if value not in roots:\n                    roots.append( value )\n        if len(roots) > 1:\n            root = PStatGroup(\n                directory='*',\n                filename='*',\n                name=_(\"<profiling run>\"),\n                children= roots,\n            )\n            root.finalize()\n            self.rows[ root.key ] = root\n        self.roots['functions'] = root\n        return root", "response": "Attempt to find a reasonable root node from a list of rows."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_location( self ):\n        directories = {}\n        files = {}\n        root = PStatLocation( '/', 'PYTHONPATH' )\n        self.location_rows = self.rows.copy()\n        for child in self.rows.values():\n            current = directories.get( child.directory )\n            directory, filename = child.directory, child.filename\n            if current is None:\n                if directory == '':\n                    current = root\n                else:\n                    current = PStatLocation( directory, '' )\n                    self.location_rows[ current.key ] = current\n                directories[ directory ] = current\n            if filename == '~':\n                filename = '<built-in>'\n            file_current = files.get( (directory,filename) )\n            if file_current is None:\n                file_current = PStatLocation( directory, filename )\n                self.location_rows[ file_current.key ] = file_current\n                files[ (directory,filename) ] = file_current\n                current.children.append( file_current )\n            file_current.children.append( child )\n        # now link the directories...\n        for key,value in directories.items():\n            if value is root:\n                continue\n            found = False\n            while key:\n                new_key,rest = os.path.split( key )\n                if new_key == key:\n                    break\n                key = new_key\n                parent = directories.get( key )\n                if parent:\n                    if value is not parent:\n                        parent.children.append( value )\n                        found = True\n                        break\n            if not found:\n                root.children.append( value )\n        # lastly, finalize all of the directory records...\n        root.finalize()\n        return root", "response": "Build a squaremap - compatible model for location - based hierarchy of directories and files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef finalize( self, already_done=None ):\n        if already_done is None:\n            already_done = {}\n        if already_done.has_key( self ):\n            return True\n        already_done[self] = True\n        self.filter_children()\n        children = self.children\n        for child in children:\n            if hasattr( child, 'finalize' ):\n                child.finalize( already_done)\n            child.parents.append( self )\n        self.calculate_totals( self.children, self.local_children )", "response": "Finalize our values taken from our children"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_totals( self, children, local_children=None ):\n        for field,local_field in (('recursive','calls'),('cumulative','local')):\n            values = []\n            for child in children:\n                if isinstance( child, PStatGroup ) or not self.LOCAL_ONLY:\n                    values.append( getattr( child, field, 0 ) )\n                elif isinstance( child, PStatRow ) and self.LOCAL_ONLY:\n                    values.append( getattr( child, local_field, 0 ) )\n            value = sum( values )\n            setattr( self, field, value )\n        if self.recursive:\n            self.cumulativePer = self.cumulative/float(self.recursive)\n        else:\n            self.recursive = 0\n        if local_children:\n            for field in ('local','calls'):\n                value = sum([ getattr( child, field, 0 ) for child in children] )\n                setattr( self, field, value )\n            if self.calls:\n                self.localPer = self.local / self.calls\n        else:\n            self.local = 0\n            self.calls = 0\n            self.localPer = 0", "response": "Calculate our cumulative totals from children and local children"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering our children into regular and local children sets", "response": "def filter_children( self ):\n        \"\"\"Filter our children into regular and local children sets\"\"\"\n        real_children = []\n        for child in self.children:\n            if child.name == '<module>':\n                self.local_children.append( child )\n            else:\n                real_children.append( child )\n        self.children = real_children"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn set of two boxes where first is the fraction given", "response": "def split_box( fraction, x,y, w,h ):\n    \"\"\"Return set of two boxes where first is the fraction given\"\"\"\n    if w >= h:\n        new_w = int(w*fraction)\n        if new_w:\n            return (x,y,new_w,h),(x+new_w,y,w-new_w,h)\n        else:\n            return None,None\n    else:\n        new_h = int(h*fraction)\n        if new_h:\n            return (x,y,w,new_h),(x,y+new_h,w,h-new_h)\n        else:\n            return None,None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_by_value( total, nodes, headdivisor=2.0 ):\n    head_sum,tail_sum = 0,0\n    divider = 0\n    for node in nodes[::-1]:\n        if head_sum < total/headdivisor:\n            head_sum += node[0]\n            divider -= 1\n        else:\n            break\n    return (head_sum,nodes[divider:]),(total-head_sum,nodes[:divider])", "response": "Produce a tuple of head and tail for a binary partition of the total number of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef findNode(class_, hot_map, targetNode, parentNode=None):\n        ''' Find the target node in the hot_map. '''\n        for index, (rect, node, children) in enumerate(hot_map):\n            if node == targetNode:\n                return parentNode, hot_map, index\n            result = class_.findNode(children, targetNode, node)\n            if result:\n                return result\n        return None", "response": "Find the target node in the hot_map."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef firstChild(hot_map, index):\n        ''' Return the first child of the node indicated by index. '''\n        children = hot_map[index][2]\n        if children:\n            return children[0][1]\n        else:\n            return hot_map[index][1]", "response": "Return the first child of the node indicated by index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nextChild(hotmap, index):\n        ''' Return the next sibling of the node indicated by index. '''\n        nextChildIndex = min(index + 1, len(hotmap) - 1)\n        return hotmap[nextChildIndex][1]", "response": "Return the next sibling of the node indicated by index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the very last node in the hot map.", "response": "def lastNode(class_, hot_map):\n        ''' Return the very last node (recursively) in the hot map. '''\n        children = hot_map[-1][2]\n        if children:\n            return class_.lastNode(children)\n        else:\n            return hot_map[-1][1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling mouse - move event by selecting a given element", "response": "def OnMouse( self, event ):\n        \"\"\"Handle mouse-move event by selecting a given element\"\"\"\n        node = HotMapNavigator.findNodeAtPosition(self.hot_map, event.GetPosition())\n        self.SetHighlight( node, event.GetPosition() )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OnClickRelease( self, event ):\n        node = HotMapNavigator.findNodeAtPosition(self.hot_map, event.GetPosition())\n        self.SetSelected( node, event.GetPosition() )", "response": "Release over a given square in the map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef OnDoubleClick(self, event):\n        node = HotMapNavigator.findNodeAtPosition(self.hot_map, event.GetPosition())\n        if node:\n            wx.PostEvent( self, SquareActivationEvent( node=node, point=event.GetPosition(), map=self ) )", "response": "Double click on a given square in the map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SetSelected( self, node, point=None, propagate=True ):\n        if node == self.selectedNode:\n            return\n        self.selectedNode = node\n        self.UpdateDrawing()\n        if node:\n            wx.PostEvent( self, SquareSelectionEvent( node=node, point=point, map=self ) )", "response": "Set the given node selected in the square - map"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetHighlight( self, node, point=None, propagate=True ):\n        if node == self.highlightedNode:\n            return\n        self.highlightedNode = node\n        # TODO: restrict refresh to the squares for previous node and new node...\n        self.UpdateDrawing()\n        if node and propagate:\n            wx.PostEvent( self, SquareHighlightEvent( node=node, point=point, map=self ) )", "response": "Set the currently - highlighted node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets our model object.", "response": "def SetModel( self, model, adapter=None ):\n        \"\"\"Set our model object (root of the tree)\"\"\"\n        self.model = model\n        if adapter is not None:\n            self.adapter = adapter\n        self.UpdateDrawing()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Draw(self, dc):\n        ''' Draw the tree map on the device context. '''\n        self.hot_map = []\n        dc.BeginDrawing()\n        brush = wx.Brush( self.BackgroundColour  )\n        dc.SetBackground( brush )\n        dc.Clear()\n        if self.model:\n            self.max_depth_seen = 0\n            font = self.FontForLabels(dc)\n            dc.SetFont(font)\n            self._em_size_ = dc.GetFullTextExtent( 'm', font )[0]\n            w, h = dc.GetSize()\n            self.DrawBox( dc, self.model, 0,0,w,h, hot_map = self.hot_map )\n        dc.EndDrawing()", "response": "Draw the tree map on the device context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the default GUI font scaled for printing if necessary.", "response": "def FontForLabels(self, dc):\n        ''' Return the default GUI font, scaled for printing if necessary. '''\n        font = wx.SystemSettings_GetFont(wx.SYS_DEFAULT_GUI_FONT)\n        scale = dc.GetPPI()[0] / wx.ScreenDC().GetPPI()[0]\n        font.SetPointSize(scale*font.GetPointSize())\n        return font"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef BrushForNode( self, node, depth=0 ):\n        if node == self.selectedNode:\n            color = wx.SystemSettings_GetColour(wx.SYS_COLOUR_HIGHLIGHT)\n        elif node == self.highlightedNode:\n            color = wx.Colour( red=0, green=255, blue=0 )\n        else:\n            color = self.adapter.background_color(node, depth)\n            if not color:\n                red = (depth * 10)%255\n                green = 255-((depth * 5)%255)\n                blue = (depth * 25)%255\n                color = wx.Colour( red, green, blue )\n        return wx.Brush( color  )", "response": "Create brush to use to display the given node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef PenForNode( self, node, depth=0 ):\n        if node == self.selectedNode:\n            return self.SELECTED_PEN\n        return self.DEFAULT_PEN", "response": "Determine the pen to use to display the given node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the text foreground color to use to display the label of the given node", "response": "def TextForegroundForNode(self, node, depth=0):\n        \"\"\"Determine the text foreground color to use to display the label of\n           the given node\"\"\"\n        if node == self.selectedNode:\n            fg_color = wx.SystemSettings_GetColour(wx.SYS_COLOUR_HIGHLIGHTTEXT)\n        else:\n            fg_color = self.adapter.foreground_color(node, depth)\n            if not fg_color:\n                fg_color = wx.SystemSettings_GetColour(wx.SYS_COLOUR_WINDOWTEXT)\n        return fg_color"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw a model - node s box and all children nodes.", "response": "def DrawBox( self, dc, node, x,y,w,h, hot_map, depth=0 ):\n        \"\"\"Draw a model-node's box and all children nodes\"\"\"\n        log.debug( 'Draw: %s to (%s,%s,%s,%s) depth %s',\n            node, x,y,w,h, depth,\n        )\n        if self.max_depth and depth > self.max_depth:\n            return\n        self.max_depth_seen = max( (self.max_depth_seen,depth))\n        dc.SetBrush( self.BrushForNode( node, depth ) )\n        dc.SetPen( self.PenForNode( node, depth ) )\n        # drawing offset by margin within the square...\n        dx,dy,dw,dh = x+self.margin,y+self.margin,w-(self.margin*2),h-(self.margin*2)\n        if sys.platform == 'darwin':\n            # Macs don't like drawing small rounded rects...\n            if w < self.padding*2 or h < self.padding*2:\n                dc.DrawRectangle( dx,dy,dw,dh )\n            else:\n                dc.DrawRoundedRectangle( dx,dy,dw,dh, self.padding )\n        else:\n            dc.DrawRoundedRectangle( dx,dy,dw,dh, self.padding*3 )\n#        self.DrawIconAndLabel(dc, node, x, y, w, h, depth)\n        children_hot_map = []\n        hot_map.append( (wx.Rect( int(x),int(y),int(w),int(h)), node, children_hot_map ) )\n        x += self.padding\n        y += self.padding\n        w -= self.padding*2\n        h -= self.padding*2\n        \n\n        empty = self.adapter.empty( node )\n        icon_drawn = False\n        if self.max_depth and depth == self.max_depth:\n            self.DrawIconAndLabel(dc, node, x, y, w, h, depth)\n            icon_drawn = True\n        elif empty:\n            # is a fraction of the space which is empty...\n            log.debug( '  empty space fraction: %s', empty )\n            new_h = h * (1.0-empty)\n            self.DrawIconAndLabel(dc, node, x, y, w, h-new_h, depth)\n            icon_drawn = True\n            y += (h-new_h)\n            h = new_h\n\n        if w >self.padding*2 and h> self.padding*2:\n            children = self.adapter.children( node )\n            if children:\n                log.debug( '  children: %s', children )\n                self.LayoutChildren( dc, children, node, x,y,w,h, children_hot_map, depth+1 )\n            else:\n                log.debug( '  no children' )\n                if not icon_drawn:\n                    self.DrawIconAndLabel(dc, node, x, y, w, h, depth)\n        else:\n            log.debug( '  not enough space: children skipped' )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef DrawIconAndLabel(self, dc, node, x, y, w, h, depth):\n        ''' Draw the icon, if any, and the label, if any, of the node. '''\n        if w-2 < self._em_size_//2 or h-2 < self._em_size_ //2:\n            return\n        dc.SetClippingRegion(x+1, y+1, w-2, h-2) # Don't draw outside the box\n        try:\n            icon = self.adapter.icon(node, node==self.selectedNode)\n            if icon and h >= icon.GetHeight() and w >= icon.GetWidth():\n                iconWidth = icon.GetWidth() + 2\n                dc.DrawIcon(icon, x+2, y+2)\n            else:\n                iconWidth = 0\n            if self.labels and h >= dc.GetTextExtent('ABC')[1]:\n                dc.SetTextForeground(self.TextForegroundForNode(node, depth))\n                dc.DrawText(self.adapter.label(node), x + iconWidth + 2, y+2)\n        finally:\n            dc.DestroyClippingRegion()", "response": "Draw the icon and label of the selected node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overall( self, node ):\n        return sum( [self.value(value,node) for value in self.children(node)] )", "response": "Calculate overall size of the node including children and empty space"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef children_sum( self, children,node ):\n        return sum( [self.value(value,node) for value in children] )", "response": "Calculate children s total sum"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating empty space as a fraction of total space", "response": "def empty( self, node ):\n        \"\"\"Calculate empty space as a fraction of total space\"\"\"\n        overall = self.overall( node )\n        if overall:\n            return (overall - self.children_sum( self.children(node), node))/float(overall)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the internal counter of the internal counter of the internal counter.", "response": "def update(self, n=1):\n        \"\"\"\n        Manually update the progress bar, useful for streams such as reading files (set init(total=filesize) and then in the reading loop, use update(len(current_buffer)) )\n\n        Parameters\n        ----------\n        n  : int\n            Increment to add to the internal counter of iterations.\n        \"\"\"\n        if n < 1:\n            n = 1\n        self.n += n\n\n        delta_it = self.n - self.last_print_n\n        if delta >= self.miniters:\n            # We check the counter first, to reduce the overhead of time.time()\n            cur_t = time.time()\n            if cur_t - self.last_print_t >= self.mininterval:\n                self.sp.print_status(format_meter(self.n, self.total, cur_t-self.start_t, self.ncols, self.prefix, self.unit, self.unit_format, self.ascii))\n                if self.dynamic_miniters: self.miniters = max(self.miniters, delta_it)\n                self.last_print_n = self.n\n                self.last_print_t = cur_t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall this method to force print the last progress bar update based on the latest n value", "response": "def close(self):\n        \"\"\"\n        Call this method to force print the last progress bar update based on the latest n value\n        \"\"\"\n        if self.leave:\n            if self.last_print_n < self.n:\n                cur_t = time.time()\n                self.sp.print_status(format_meter(self.n, self.total, cur_t-self.start_t, self.ncols, self.prefix, self.unit, self.unit_format, self.ascii))\n            self.file.write('\\n')\n        else:\n            self.sp.print_status('')\n            self.file.write('\\r')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, value):\n        if value not in self._set:\n            self._set.add(value)\n            self._list.add(value)", "response": "Add the element value to the set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self):\n        return self.__class__(key=self._key, load=self._load, _set=set(self._set))", "response": "Create a shallow copy of the sorted set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new set with elements in the set that are not in the given iterable.", "response": "def difference(self, *iterables):\n        \"\"\"\n        Return a new set with elements in the set that are not in the\n        *iterables*.\n        \"\"\"\n        diff = self._set.difference(*iterables)\n        new_set = self.__class__(key=self._key, load=self._load, _set=diff)\n        return new_set"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intersection(self, *iterables):\n        comb = self._set.intersection(*iterables)\n        new_set = self.__class__(key=self._key, load=self._load, _set=comb)\n        return new_set", "response": "Return a new set with elements common to the set and all iterables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new set with elements in either self or that but not both.", "response": "def symmetric_difference(self, that):\n        \"\"\"\n        Return a new set with elements in either *self* or *that* but not both.\n        \"\"\"\n        diff = self._set.symmetric_difference(that)\n        new_set = self.__class__(key=self._key, load=self._load, _set=diff)\n        return new_set"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cookie_decode(data, key):\n    ''' Verify and decode an encoded string. Return an object or None'''\n    if isinstance(data, unicode): data = data.encode('ascii') #2to3 hack\n    if cookie_is_encoded(data):\n        sig, msg = data.split(u'?'.encode('ascii'),1) #2to3 hack\n        if sig[1:] == base64.b64encode(hmac.new(key, msg).digest()):\n           return pickle.loads(base64.b64decode(msg))\n    return None", "response": "Verify and decode an encoded string. Return an object or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cookie_is_encoded(data):\n    ''' Verify and decode an encoded string. Return an object or None'''\n    return bool(data.startswith(u'!'.encode('ascii')) and u'?'.encode('ascii') in data)", "response": "Verify and decode an encoded string. Return an object or None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tonativefunc(enc='utf-8'):\n    ''' Returns a function that turns everything into 'native' strings using enc '''\n    if sys.version_info >= (3,0,0):\n        return lambda x: x.decode(enc) if isinstance(x, bytes) else str(x)\n    return lambda x: x.encode(enc) if isinstance(x, unicode) else str(x)", "response": "Returns a function that turns everything into native strings using enc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, *a, **ka):\n        route = a[0] if a and isinstance(a[0], Route) else Route(*a, **ka)\n        self.routes.append(route)\n        if route.name:\n            self.named[route.name] = route.format_str()\n        if route.static:\n            self.static[route.route] = route.target\n            return\n        gpatt = route.group_re()\n        fpatt = route.flat_re()\n        try:\n            gregexp = re.compile('^(%s)$' % gpatt) if '(?P' in gpatt else None\n            combined = '%s|(^%s$)' % (self.dynamic[-1][0].pattern, fpatt)\n            self.dynamic[-1] = (re.compile(combined), self.dynamic[-1][1])\n            self.dynamic[-1][1].append((route.target, gregexp))\n        except (AssertionError, IndexError), e: # AssertionError: Too many groups\n            self.dynamic.append((re.compile('(^%s$)'%fpatt),[(route.target, gregexp)]))\n        except re.error, e:\n            raise RouteSyntaxError(\"Could not add Route: %s (%s)\" % (route, e))", "response": "Adds a route - > target pair or a Route object to the Router."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmounting a Bottle application to a specific URL prefix.", "response": "def mount(self, app, script_path):\n        ''' Mount a Bottle application to a specific URL prefix '''\n        if not isinstance(app, Bottle):\n            raise TypeError('Only Bottle instances are supported for now.')\n        script_path = '/'.join(filter(None, script_path.split('/')))\n        path_depth = script_path.count('/') + 1\n        if not script_path:\n            raise TypeError('Empty script_path. Perhaps you want a merge()?')\n        for other in self.mounts:\n            if other.startswith(script_path):\n                raise TypeError('Conflict with existing mount: %s' % other)\n        @self.route('/%s/:#.*#' % script_path, method=\"ANY\")\n        def mountpoint():\n            request.path_shift(path_depth)\n            return app.handle(request.path, request.method)\n        self.mounts[script_path] = app"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the handler bound to the specified url and method and return the result as a new HTTP response object.", "response": "def handle(self, url, method):\n        \"\"\" Execute the handler bound to the specified url and method and return\n        its output. If catchall is true, exceptions are catched and returned as\n        HTTPError(500) objects. \"\"\"\n        if not self.serve:\n            return HTTPError(503, \"Server stopped\")\n\n        handler, args = self.match_url(url, method)\n        if not handler:\n            return HTTPError(404, \"Not found:\" + url)\n\n        try:\n            return handler(**args)\n        except HTTPResponse, e:\n            return e\n        except Exception, e:\n            if isinstance(e, (KeyboardInterrupt, SystemExit, MemoryError))\\\n            or not self.catchall:\n                raise\n            return HTTPError(500, 'Unhandled exception', e, format_exc(10))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to convert the output of a WSGI request into something WSGI compatible and set correct HTTP headers when possible.", "response": "def _cast(self, out, request, response, peek=None):\n        \"\"\" Try to convert the parameter into something WSGI compatible and set\n        correct HTTP headers when possible.\n        Support: False, str, unicode, dict, HTTPResponse, HTTPError, file-like,\n        iterable of strings and iterable of unicodes\n        \"\"\"\n        # Filtered types (recursive, because they may return anything)\n        for testtype, filterfunc in self.castfilter:\n            if isinstance(out, testtype):\n                return self._cast(filterfunc(out), request, response)\n\n        # Empty output is done here\n        if not out:\n            response.headers['Content-Length'] = 0\n            return []\n        # Join lists of byte or unicode strings. Mixed lists are NOT supported\n        if isinstance(out, list) and isinstance(out[0], (StringType, unicode)):\n            out = out[0][0:0].join(out) # b'abc'[0:0] -> b''\n        # Encode unicode strings\n        if isinstance(out, unicode):\n            out = out.encode(response.charset)\n        # Byte Strings are just returned\n        if isinstance(out, StringType):\n            response.headers['Content-Length'] = str(len(out))\n            return [out]\n        # HTTPError or HTTPException (recursive, because they may wrap anything)\n        if isinstance(out, HTTPError):\n            out.apply(response)\n            return self._cast(self.error_handler.get(out.status, repr)(out), request, response)\n        if isinstance(out, HTTPResponse):\n            out.apply(response)\n            return self._cast(out.output, request, response)\n\n        # Cast Files into iterables\n        if hasattr(out, 'read') and 'wsgi.file_wrapper' in request.environ:\n            out = request.environ.get('wsgi.file_wrapper',\n            lambda x, y: iter(lambda: x.read(y), ''))(out, 1024*64)\n\n        # Handle Iterables. We peek into them to detect their inner type.\n        try:\n            out = iter(out)\n            first = out.next()\n            while not first:\n                first = out.next()\n        except StopIteration:\n            return self._cast('', request, response)\n        except HTTPResponse, e:\n            first = e\n        except Exception, e:\n            first = HTTPError(500, 'Unhandled exception', e, format_exc(10))\n            if isinstance(e, (KeyboardInterrupt, SystemExit, MemoryError))\\\n            or not self.catchall:\n                raise\n        # These are the inner types allowed in iterator or generator objects.\n        if isinstance(first, HTTPResponse):\n            return self._cast(first, request, response)\n        if isinstance(first, StringType):\n            return itertools.chain([first], out)\n        if isinstance(first, unicode):\n            return itertools.imap(lambda x: x.encode(response.charset),\n                                  itertools.chain([first], out))\n        return self._cast(HTTPError(500, 'Unsupported response type: %s'\\\n                                         % type(first)), request, response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a HeaderDict filled with request headers.", "response": "def header(self):\n        ''' :class:`HeaderDict` filled with request headers.\n\n            HeaderDict keys are case insensitive str.title()d\n        '''\n        if self._header is None:\n            self._header = HeaderDict()\n            for key, value in self.environ.iteritems():\n                if key.startswith('HTTP_'):\n                    key = key[5:].replace('_','-').title()\n                    self._header[key] = value\n        return self._header"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef COOKIES(self):\n        if self._COOKIES is None:\n            raw_dict = SimpleCookie(self.environ.get('HTTP_COOKIE',''))\n            self._COOKIES = {}\n            for cookie in raw_dict.itervalues():\n                self._COOKIES[cookie.key] = cookie.value\n        return self._COOKIES", "response": "Returns a dictionary of cookie information parsed into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_cookie(self, key, value, **kargs):\n        if not isinstance(value, basestring):\n            sec = self.app.config['securecookie.key']\n            value = cookie_encode(value, sec).decode('ascii') #2to3 hack\n        self.COOKIES[key] = value\n        for k, v in kargs.iteritems():\n            self.COOKIES[key][k.replace('_', '-')] = v", "response": "Add a new cookie with various options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tamper_file_at(path, pos=0, replace_str=None):\n    if not replace_str:\n        replace_str = \"\\x00\"\n    try:\n        with open(path, \"r+b\") as fh:\n            if pos < 0: # if negative, we calculate the position backward from the end of file\n                fsize = os.fstat(fh.fileno()).st_size\n                pos = fsize + pos\n            fh.seek(pos)\n            fh.write(replace_str)\n    except IOError:\n        return False\n    finally:\n        try:\n            fh.close()\n        except Exception:\n            pass\n    return True", "response": "Tamper a file at the given position and using the given string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tamper_dir(inputpath, *args, **kwargs):\n    silent = kwargs.get('silent', False)\n    if 'silent' in kwargs: del kwargs['silent']\n\n    filescount = 0\n    for _ in tqdm(recwalk(inputpath), desc='Precomputing', disable=silent):\n        filescount += 1\n\n    files_tampered = 0\n    tamper_count = 0\n    total_size = 0\n    for dirname, filepath in tqdm(recwalk(inputpath), total=filescount, leave=True, desc='Tamper file n.', disable=silent):\n        tcount, tsize = tamper_file(os.path.join(dirname, filepath), *args, **kwargs)\n        if tcount > 0:\n            tamper_count += tcount\n            files_tampered += 1\n        total_size += tsize\n    return [files_tampered, filescount, tamper_count, total_size]", "response": "Randomly tamper the files content in a directory tree recursively."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving current stack trace as formatted string.", "response": "def _save_trace(self):\n        \"\"\"\n        Save current stack trace as formatted string.\n        \"\"\"\n        stack_trace = stack()\n        try:\n            self.trace = []\n            for frm in stack_trace[5:]: # eliminate our own overhead\n                self.trace.insert(0, frm[1:])\n        finally:\n            del stack_trace"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstore timestamp and current size for later evaluation.", "response": "def track_size(self, ts, sizer):\n        \"\"\"\n        Store timestamp and current size for later evaluation.\n        The 'sizer' is a stateful sizing facility that excludes other tracked\n        objects.\n        \"\"\"\n        obj = self.ref()\n        self.snapshots.append(\n            (ts, sizer.asized(obj, detail=self._resolution_level))\n        )\n        if obj is not None:\n            self.repr = safe_repr(obj, clip=128)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_size_at_time(self, timestamp):\n        size = 0\n        for (t, s) in self.snapshots:\n            if t == timestamp:\n                size = s.size\n        return size", "response": "Get the size of the object at a specific time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        self.stop = False\n        while not self.stop:\n            self.tracker.create_snapshot()\n            sleep(self.interval)", "response": "Loop until a stop signal is set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the total size of the process in bytes.", "response": "def total(self):\n        \"\"\"\n        Return the total (virtual) size of the process in bytes. If process\n        information is not available, get the best number available, even if it\n        is a poor approximation of reality.\n        \"\"\"\n        if self.system_total.available:\n            return self.system_total.vsz\n        elif self.asizeof_total: # pragma: no cover\n            return self.asizeof_total\n        else: # pragma: no cover\n            return self.tracked_total"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning timestamped label for this snapshot or a raw timestamp.", "response": "def label(self):\n        \"\"\"Return timestamped label for this snapshot, or a raw timestamp.\"\"\"\n        if not self.desc:\n            return \"%.3fs\" % self.timestamp\n        return \"%s (%.3fs)\" % (self.desc, self.timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninjecting constructor for tracked classes. Call the actual constructor of the object and track the object. Attach to the object before calling the constructor to track the object with the parameters of the most specialized class.", "response": "def _tracker(self, _observer_, _self_, *args, **kwds):\n        \"\"\"\n        Injected constructor for tracked classes.\n        Call the actual constructor of the object and track the object.\n        Attach to the object before calling the constructor to track the object with\n        the parameters of the most specialized class.\n        \"\"\"\n        self.track_object(_self_,\n                          name=_observer_.name,\n                          resolution_level=_observer_.detail,\n                          keep=_observer_.keep,\n                          trace=_observer_.trace)\n        _observer_.init(_self_, *args, **kwds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninjects a constructor into the class.", "response": "def _inject_constructor(self, cls, func, name, resolution_level, keep,\n                            trace):\n        \"\"\"\n        Modifying Methods in Place - after the recipe 15.7 in the Python\n        Cookbook by Ken Seehof. The original constructors may be restored\n        later.\n        \"\"\"\n        try:\n            constructor = cls.__init__\n        except AttributeError:\n            def constructor(self, *_args, **_kwargs):\n                pass\n\n        # Possible name clash between keyword arguments of the tracked class'\n        # constructor and the curried arguments of the injected constructor.\n        # Therefore, the additional argument has a 'magic' name to make it less\n        # likely that an argument name clash occurs.\n        self._observers[cls] = _ClassObserver(constructor,\n                                              name,\n                                              resolution_level,\n                                              keep,\n                                              trace)\n        cls.__init__ = instancemethod(\n            lambda *args, **kwds: func(self._observers[cls], *args, **kwds),\n            None,\n            cls\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying settings of a tracked class", "response": "def _track_modify(self, cls, name, detail, keep, trace):\n        \"\"\"\n        Modify settings of a tracked class\n        \"\"\"\n        self._observers[cls].modify(name, detail, keep, trace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _restore_constructor(self, cls):\n        cls.__init__ = self._observers[cls].init\n        del self._observers[cls]", "response": "Restores the original constructor lose track of class.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges tracking options for the already tracked object instance.", "response": "def track_change(self, instance, resolution_level=0):\n        \"\"\"\n        Change tracking options for the already tracked object 'instance'.\n        If instance is not tracked, a KeyError will be raised.\n        \"\"\"\n        tobj = self.objects[id(instance)]\n        tobj.set_resolution_level(resolution_level)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef track_object(self, instance, name=None, resolution_level=0, keep=False, trace=False):\n\n        # Check if object is already tracked. This happens if track_object is\n        # called multiple times for the same object or if an object inherits\n        # from multiple tracked classes. In the latter case, the most\n        # specialized class wins.  To detect id recycling, the weak reference\n        # is checked. If it is 'None' a tracked object is dead and another one\n        # takes the same 'id'.\n        if id(instance) in self.objects and \\\n            self.objects[id(instance)].ref() is not None:\n            return\n\n        tobj = TrackedObject(instance, resolution_level=resolution_level, trace=trace)\n\n        if name is None:\n            name = instance.__class__.__name__\n        if not name in self.index:\n            self.index[name] = []\n        self.index[name].append(tobj)\n        self.objects[id(instance)] = tobj\n\n        if keep:\n            self._keepalive.append(instance)", "response": "Track an object and sample size and lifetime information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef track_class(self, cls, name=None, resolution_level=0, keep=False, trace=False):\n        if not isclass(cls):\n            raise TypeError(\"only class objects can be tracked\")\n        if name is None:\n            name = cls.__module__ + '.' + cls.__name__\n        if self._is_tracked(cls):\n            self._track_modify(cls, name, resolution_level, keep, trace)\n        else:\n            self._inject_constructor(cls, self._tracker, name, resolution_level, keep, trace)", "response": "Track all objects of the given class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detach_all_classes(self):\n        classes = list(self._observers.keys())\n        for cls in classes:\n            self.detach_class(cls)", "response": "Detach from all tracked classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetaches from all tracked classes and objects and removes the original constructor and cleans up the tracking lists.", "response": "def detach_all(self):\n        \"\"\"\n        Detach from all tracked classes and objects.\n        Restore the original constructors and cleanse the tracking lists.\n        \"\"\"\n        self.detach_all_classes()\n        self.objects.clear()\n        self.index.clear()\n        self._keepalive[:] = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a thread which takes snapshots periodically.", "response": "def start_periodic_snapshots(self, interval=1.0):\n        \"\"\"\n        Start a thread which takes snapshots periodically. The `interval` specifies\n        the time in seconds the thread waits between taking snapshots. The thread is\n        started as a daemon allowing the program to exit. If periodic snapshots are\n        already active, the interval is updated.\n        \"\"\"\n        if not self._periodic_thread:\n            self._periodic_thread = PeriodicThread(self, interval, name='BackgroundMonitor')\n            self._periodic_thread.setDaemon(True)\n            self._periodic_thread.start()\n        else:\n            self._periodic_thread.interval = interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop_periodic_snapshots(self):\n        if self._periodic_thread and self._periodic_thread.isAlive():\n            self._periodic_thread.stop = True\n            self._periodic_thread.join()\n            self._periodic_thread = None", "response": "Stop the periodic snapshots of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a snapshot of the current state of the class tracker.", "response": "def create_snapshot(self, description='', compute_total=False):\n        \"\"\"\n        Collect current per instance statistics and saves total amount of\n        memory associated with the Python process.\n\n        If `compute_total` is `True`, the total consumption of all objects\n        known to *asizeof* is computed. The latter might be very slow if many\n        objects are mapped into memory at the time the snapshot is taken.\n        Therefore, `compute_total` is set to `False` by default.\n\n        The overhead of the `ClassTracker` structure is also computed.\n\n        Snapshots can be taken asynchronously. The function is protected with a\n        lock to prevent race conditions.\n        \"\"\"\n\n        try:\n            # TODO: It is not clear what happens when memory is allocated or\n            # released while this function is executed but it will likely lead\n            # to inconsistencies. Either pause all other threads or don't size\n            # individual objects in asynchronous mode.\n            self.snapshot_lock.acquire()\n\n            timestamp = _get_time()\n\n            sizer = asizeof.Asizer()\n            objs = [tobj.ref() for tobj in list(self.objects.values())]\n            sizer.exclude_refs(*objs)\n\n            # The objects need to be sized in a deterministic order. Sort the\n            # objects by its creation date which should at least work for non-parallel\n            # execution. The \"proper\" fix would be to handle shared data separately.\n            tracked_objects = list(self.objects.values())\n            tracked_objects.sort(key=lambda x: x.birth)\n            for tobj in tracked_objects:\n                tobj.track_size(timestamp, sizer)\n\n            snapshot = Snapshot()\n\n            snapshot.timestamp = timestamp\n            snapshot.tracked_total = sizer.total\n            if compute_total:\n                snapshot.asizeof_total = asizeof.asizeof(all=True, code=True)\n            snapshot.system_total = pympler.process.ProcessMemoryInfo()\n            snapshot.desc = str(description)\n\n            # Compute overhead of all structures, use sizer to exclude tracked objects(!)\n            snapshot.overhead = 0\n            if snapshot.tracked_total:\n                snapshot.overhead = sizer.asizeof(self)\n                if snapshot.asizeof_total:\n                    snapshot.asizeof_total -= snapshot.overhead\n\n            self.snapshots.append(snapshot)\n\n        finally:\n            self.snapshot_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_required(action):\n  '''_actions which are positional or possessing the `required` flag '''\n  return not action.option_strings and not isinstance(action, _SubParsersAction) or action.required == True", "response": "Return True if the action is required."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the metadata fields from a raw ecc entry.", "response": "def entry_fields(entry, field_delim=\"\\xFF\"):\n    '''From a raw ecc entry (a string), extract the metadata fields (filename, filesize, ecc for both), and the rest being blocks of hash and ecc per blocks of the original file's header'''\n    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).\n\n    # Find metadata fields delimiters positions\n    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)\n    first = entry.find(field_delim)\n    second = entry.find(field_delim, first+len(field_delim))\n    third = entry.find(field_delim, second+len(field_delim))\n    fourth = entry.find(field_delim, third+len(field_delim))\n    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.\n\n    # Extract the content of the fields\n    # Metadata fields\n    relfilepath = entry[:first]\n    filesize = entry[first+len(field_delim):second]\n    relfilepath_ecc = entry[second+len(field_delim):third]\n    filesize_ecc = entry[third+len(field_delim):fourth]\n    # Ecc stream field (aka ecc blocks)\n    ecc_field = entry[fourth+len(field_delim):]\n\n    # Try to convert to an int, an error may happen\n    try:\n        filesize = int(filesize)\n    except Exception, e:\n        print(\"Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: \")\n        print(e)\n        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc\n\n    # entries = [ {\"message\":, \"ecc\":, \"hash\":}, etc.]\n    #print(entry)\n    #print(len(entry))\n    return {\"relfilepath\": relfilepath, \"relfilepath_ecc\": relfilepath_ecc, \"filesize\": filesize, \"filesize_ecc\": filesize_ecc, \"ecc_field\": ecc_field}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef entry_assemble(entry_fields, ecc_params, header_size, filepath, fileheader=None):\n    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''\n    # Extract the header from the file\n    if fileheader is None:\n        with open(filepath, 'rb') as file: # filepath is the absolute path to the original file (the one with maybe corruptions, NOT the output repaired file!)\n            # Compute the size of the buffer to read: either header_size if possible, but if the file is smaller than that then we will read the whole file.\n            if entry_fields[\"filesize\"] > 0 and entry_fields[\"filesize\"] < header_size:\n                fileheader = file.read(entry_fields[\"filesize\"])\n            else:\n                fileheader = file.read(header_size)\n\n    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block\n    entry_asm = []\n    for i, j in itertools.izip(xrange(0, len(fileheader), ecc_params[\"message_size\"]), xrange(0, len(entry_fields[\"ecc_field\"]), ecc_params[\"hash_size\"] + ecc_params[\"ecc_size\"])):\n        # Extract each fields from each block\n        mes = fileheader[i:i+ecc_params[\"message_size\"]]\n        hash = entry_fields[\"ecc_field\"][j:j+ecc_params[\"hash_size\"]]\n        ecc = entry_fields[\"ecc_field\"][j+ecc_params[\"hash_size\"]:j+ecc_params[\"hash_size\"]+ecc_params[\"ecc_size\"]]\n        entry_asm.append({\"message\": mes, \"hash\": hash, \"ecc\": ecc})\n\n    # Return a list of fields for each block\n    return entry_asm", "response": "Assemble a list of each block from the original file along with the relative hash and ecc for easy processing later."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_ecc_hash(ecc_manager, hasher, buf, max_block_size, rate, message_size=None, as_string=False):\n    '''Split a string in blocks given max_block_size and compute the hash and ecc for each block, and then return a nice list with both for easy processing.'''\n    result = []\n    # If required parameters were not provided, we compute them\n    if not message_size:\n        ecc_params = compute_ecc_params(max_block_size, rate, hasher)\n        message_size = ecc_params[\"message_size\"]\n    # Split the buffer string in blocks (necessary for Reed-Solomon encoding because it's limited to 255 characters max)\n    for i in xrange(0, len(buf), message_size):\n        # Compute the message block\n        mes = buf[i:i+message_size]\n        # Compute the ecc\n        ecc = ecc_manager.encode(mes)\n        # Compute the hash\n        hash = hasher.hash(mes)\n        #crc = zlib.crc32(mes) # DEPRECATED: CRC is not resilient enough\n        #print(\"mes %i (%i) - ecc %i (%i) - hash %i (%i)\" % (len(mes), message_size, len(ecc), ecc_params[\"ecc_size\"], len(hash), ecc_params[\"hash_size\"])) # DEBUGLINE\n\n        # Return the result (either in string for easy writing into a file, or in a list for easy post-processing)\n        if as_string:\n            result.append(\"%s%s\" % (str(hash),str(ecc)))\n        else:\n            result.append([hash, ecc])\n    return result", "response": "Split a string in blocks given max_block_size and compute the hash and ecc for each block and then return a nice list with both for easy writing into a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncorrecting an intra - field with its corresponding intra - ecc if necessary", "response": "def ecc_correct_intra(ecc_manager_intra, ecc_params_intra, field, ecc, enable_erasures=False, erasures_char=\"\\x00\", only_erasures=False):\n    \"\"\" Correct an intra-field with its corresponding intra-ecc if necessary \"\"\"\n    fentry_fields = {\"ecc_field\": ecc}\n    field_correct = [] # will store each block of the corrected (or already correct) filepath\n    fcorrupted = False # check if field was corrupted\n    fcorrected = True # check if field was corrected (if it was corrupted)\n    errmsg = ''\n    # Decode each block of the filepath\n    for e in entry_assemble(fentry_fields, ecc_params_intra, len(field), '', field):\n        # Check if this block of the filepath is OK, if yes then we just copy it over\n        if ecc_manager_intra.check(e[\"message\"], e[\"ecc\"]):\n            field_correct.append(e[\"message\"])\n        else: # Else this block is corrupted, we will try to fix it using the ecc\n            fcorrupted = True\n            # Repair the message block and the ecc\n            try:\n                repaired_block, repaired_ecc = ecc_manager_intra.decode(e[\"message\"], e[\"ecc\"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)\n            except (ReedSolomonError, RSCodecError), exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.\n                repaired_block = None\n                repaired_ecc = None\n                errmsg += \"- Error: metadata field at offset %i: %s\\n\" % (entry_pos[0], exc)\n            # Check if the block was successfully repaired: if yes then we copy the repaired block...\n            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):\n                field_correct.append(repaired_block)\n            else: # ... else it failed, then we copy the original corrupted block and report an error later\n                field_correct.append(e[\"message\"])\n                fcorrected = False\n    # Join all the blocks into one string to build the final filepath\n    if isinstance(field_correct[0], bytearray): field_correct = [str(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str\n    field = ''.join(field_correct)\n    # Report errors\n    return (field, fcorrupted, fcorrected, errmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eliminate_leafs(self, graph):\n        result = []\n        idset = set([id(x) for x in graph])\n        for n in graph:\n            refset = set([id(x) for x in get_referents(n)])\n            if refset.intersection(idset):\n                result.append(n)\n        return result", "response": "Eliminate leaf objects - that are objects not referencing any other\n        objects in the list graph. Returns the list of objects without the leafs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _reduce_to_cycles(self):\n        cycles = self.objects[:]\n        cnt = 0\n        while cnt != len(cycles):\n            cnt = len(cycles)\n            cycles = self._eliminate_leafs(cycles)\n        self.objects = cycles\n        return len(self.objects)", "response": "Reduce the set of objects to only those\n        that build cycles. Return the number of objects involved in reference\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing the set of objects to only those those that build cycles. Return the reduced graph.", "response": "def reduce_to_cycles(self):\n        \"\"\"\n        Iteratively eliminate leafs to reduce the set of objects to only those\n        that build cycles. Return the reduced graph. If there are no cycles,\n        None is returned.\n        \"\"\"\n        if not self._reduced:\n            reduced = copy(self)\n            reduced.objects = self.objects[:]\n            reduced.metadata = []\n            reduced.edges = []\n            self.num_in_cycles = reduced._reduce_to_cycles()\n            reduced.num_in_cycles = self.num_in_cycles\n            if self.num_in_cycles:\n                reduced._get_edges()\n                reduced._annotate_objects()\n                for meta in reduced.metadata:\n                    meta.cycle = True\n            else:\n                reduced = None\n            self._reduced = reduced\n        return self._reduced"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_edges(self):\n        idset = set([id(x) for x in self.objects])\n        self.edges = set([])\n        for n in self.objects:\n            refset = set([id(x) for x in get_referents(n)])\n            for ref in refset.intersection(idset):\n                label = ''\n                members = None\n                if isinstance(n, dict):\n                    members = n.items()\n                if not members:\n                    members = named_refs(n)\n                for (k, v) in members:\n                    if id(v) == ref:\n                        label = k\n                        break\n                self.edges.add(_Edge(id(n), ref, label))", "response": "Compute the edges for the reference graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nannotates the objects belonging to separate graphs with their respective groups.", "response": "def _annotate_groups(self):\n        \"\"\"\n        Annotate the objects belonging to separate (non-connected) graphs with\n        individual indices.\n        \"\"\"\n        g = {}\n        for x in self.metadata:\n            g[x.id] = x\n\n        idx = 0\n        for x in self.metadata:\n            if not hasattr(x, 'group'):\n                x.group = idx\n                idx += 1\n            neighbors = set()\n            for e in self.edges:\n                if e.src == x.id:\n                    neighbors.add(e.dst)\n                if e.dst == x.id:\n                    neighbors.add(e.src)\n            for nb in neighbors:\n                g[nb].group = min(x.group, getattr(g[nb], 'group', idx))\n\n        # Assign the edges to the respective groups. Both \"ends\" of the edge\n        # should share the same group so just use the first object's group.\n        for e in self.edges:\n            e.group = g[e.src].group\n\n        self._max_group = idx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the group is non - empty. Otherwise returns False.", "response": "def _filter_group(self, group):\n        \"\"\"\n        Eliminate all objects but those which belong to `group`.\n        ``self.objects``, ``self.metadata`` and ``self.edges`` are modified.\n        Returns `True` if the group is non-empty. Otherwise returns `False`.\n        \"\"\"\n        self.metadata = [x for x in self.metadata if x.group == group]\n        group_set = set([x.id for x in self.metadata])\n        self.objects = [obj for obj in self.objects if id(obj) in group_set]\n        self.count = len(self.metadata)\n        if self.metadata == []:\n            return False\n\n        self.edges = [e for e in self.edges if e.group == group]\n\n        del self._max_group\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting the reference graph into sub - graphs.", "response": "def split(self):\n        \"\"\"\n        Split the graph into sub-graphs. Only connected objects belong to the\n        same graph. `split` yields copies of the Graph object. Shallow copies\n        are used that only replicate the meta-information, but share the same\n        object list ``self.objects``.\n\n        >>> from pympler.refgraph import ReferenceGraph\n        >>> a = 42\n        >>> b = 'spam'\n        >>> c = {a: b}\n        >>> t = (1,2,3)\n        >>> rg = ReferenceGraph([a,b,c,t])\n        >>> for subgraph in rg.split():\n        ...   print subgraph.index\n        0\n        1\n        \"\"\"\n        self._annotate_groups()\n        index = 0\n\n        for group in range(self._max_group):\n            subgraph = copy(self)\n            subgraph.metadata = self.metadata[:]\n            subgraph.edges = self.edges.copy()\n\n            if subgraph._filter_group(group):\n                subgraph.total_size = sum([x.size for x in subgraph.metadata])\n                subgraph.index = index\n                index += 1\n                yield subgraph"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_and_sort(self):\n        graphs = list(self.split())\n        graphs.sort(key=lambda x: -len(x.metadata))\n        for index, graph in enumerate(graphs):\n            graph.index = index\n        return graphs", "response": "Split the graphs into sub graphs and sort the graphs by the number of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _annotate_objects(self):\n        self.metadata = []\n        sizer = Asizer()\n        sizes = sizer.asizesof(*self.objects)\n        self.total_size = sizer.total\n        for obj, sz in zip(self.objects, sizes):\n            md = _MetaObject()\n            md.size = sz\n            md.id = id(obj)\n            try:\n                md.type = obj.__class__.__name__\n            except (AttributeError, ReferenceError): # pragma: no cover\n                md.type = type(obj).__name__\n            md.str = safe_repr(obj, clip=128)\n            self.metadata.append(md)", "response": "Extract meta - data describing the stored objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representation of the object with graphviz.", "response": "def _get_graphviz_data(self):\n        \"\"\"\n        Emit a graph representing the connections between the objects described\n        within the metadata list. The text representation can be transformed to\n        a graph with graphviz. Returns a string.\n        \"\"\"\n        s = []\n        header = '// Process this file with graphviz\\n'\n        s.append( header)\n        s.append('digraph G {\\n')\n        s.append('    node [shape=box];\\n')\n        for md in self.metadata:\n            label = trunc(md.str, 48).replace('\"', \"'\")\n            extra = ''\n            if md.type == 'instancemethod':\n                extra = ', color=red'\n            elif md.type == 'frame':\n                extra = ', color=orange'\n            s.append('    \"X%s\" [ label = \"%s\\\\n%s\" %s ];\\n' % \\\n                (hex(md.id)[1:], label, md.type, extra))\n        for e in self.edges:\n            extra = ''\n            if e.label == '__dict__':\n                extra = ',weight=100'\n            s.append('    X%s -> X%s [label=\"%s\"%s];\\n' % \\\n                (hex(e.src)[1:], hex(e.dst)[1:], e.label, extra))\n\n        s.append('}\\n')\n        return \"\".join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, filename, cmd='dot', format='ps', unflatten=False):\n        if self.objects == []:\n            return False\n\n        data = self._get_graphviz_data()\n\n        options = ('-Nfontsize=10',\n                   '-Efontsize=10',\n                   '-Nstyle=filled',\n                   '-Nfillcolor=#E5EDB8',\n                   '-Ncolor=#CCCCCC')\n        cmdline = (cmd, '-T%s' % format, '-o', filename) + options\n\n        if unflatten:\n            p1 = Popen(('unflatten', '-l7'), stdin=PIPE, stdout=PIPE,\n                **popen_flags)\n            p2 = Popen(cmdline, stdin=p1.stdout, **popen_flags)\n            p1.communicate(encode4pipe(data))\n            p2.communicate()\n            return p2.returncode == 0\n        else:\n            p = Popen(cmdline, stdin=PIPE, **popen_flags)\n            p.communicate(encode4pipe(data))\n            return p.returncode == 0", "response": "Render the object to filename using graphviz."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_graph(self, filename):\n        f = open(filename, 'w')\n        f.write(self._get_graphviz_data())\n        f.close()", "response": "Write raw graph data to a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the parsed result in the form of a tree of Frame objects where the first element is the name of the frame and the second is the time of the frame.", "response": "def root_frame(self):\n        \"\"\"\n        Returns the parsed results in the form of a tree of Frame objects\n        \"\"\"\n        if not hasattr(self, '_root_frame'):\n            self._root_frame = Frame()\n\n            # define a recursive function that builds the hierarchy of frames given the\n            # stack of frame identifiers\n            def frame_for_stack(stack):\n                if len(stack) == 0:\n                    return self._root_frame\n\n                parent = frame_for_stack(stack[:-1])\n                frame_name = stack[-1]\n\n                if not frame_name in parent.children_dict:\n                    parent.add_child(Frame(frame_name, parent))\n\n                return parent.children_dict[frame_name]\n\n            for stack, self_time in self.stack_self_time.items():\n                frame_for_stack(stack).self_time = self_time\n\n        return self._root_frame"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_trace():\n    global call_dict\n    global call_stack\n    global func_count\n    global func_count_max\n    global func_time\n    global func_time_max\n    global call_stack_timer\n\n    call_dict = {}\n\n    # current call stack\n    call_stack = ['__main__']\n\n    # counters for each function\n    func_count = {}\n    func_count_max = 0\n\n    # accumative time per function\n    func_time = {}\n    func_time_max = 0\n\n    # keeps track of the start time of each call on the stack\n    call_stack_timer = []", "response": "Resets all collected statistics. This is run automatically by the module load_module and reset_trace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the file_name is in the lib directory.", "response": "def is_module_stdlib(file_name):\n    \"\"\"Returns True if the file_name is in the lib directory.\"\"\"\n    # TODO: Move these calls away from this function so it doesn't have to run\n    # every time.\n    lib_path = sysconfig.get_python_lib()\n    path = os.path.split(lib_path)\n    if path[1] == 'site-packages':\n        lib_path = path[0]\n    return file_name.lower().startswith(lib_path.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_trace(reset=True, filter_func=None, time_filter_func=None):\n    global trace_filter\n    global time_filter\n    if reset:\n        reset_trace()\n\n    if filter_func:\n        trace_filter = filter_func\n    else:\n        trace_filter = GlobbingFilter(exclude=['pycallgraph.*'])\n\n    if time_filter_func:\n        time_filter = time_filter_func\n    else:\n        time_filter = GlobbingFilter()\n\n    sys.settrace(tracer)", "response": "Starts a trace for the current call graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tracer(frame, event, arg):\n    global func_count_max\n    global func_count\n    global trace_filter\n    global time_filter\n    global call_stack\n    global func_time\n    global func_time_max\n\n    if event == 'call':\n        keep = True\n        code = frame.f_code\n\n        # Stores all the parts of a human readable name of the current call.\n        full_name_list = []\n\n        # Work out the module name\n        module = inspect.getmodule(code)\n        if module:\n            module_name = module.__name__\n            module_path = module.__file__\n            if not settings['include_stdlib'] \\\n                and is_module_stdlib(module_path):\n                keep = False\n            if module_name == '__main__':\n                module_name = ''\n        else:\n            module_name = ''\n        if module_name:\n            full_name_list.append(module_name)\n\n        # Work out the class name.\n        try:\n            class_name = frame.f_locals['self'].__class__.__name__\n            full_name_list.append(class_name)\n        except (KeyError, AttributeError):\n            class_name = ''\n\n        # Work out the current function or method\n        func_name = code.co_name\n        if func_name == '?':\n            func_name = '__main__'\n        full_name_list.append(func_name)\n\n        # Create a readable representation of the current call\n        full_name = '.'.join(full_name_list)\n\n        # Load the trace filter, if any. 'keep' determines if we should ignore\n        # this call\n        if keep and trace_filter:\n            keep = trace_filter(call_stack, module_name, class_name,\n                func_name, full_name)\n\n        # Store the call information\n        if keep:\n\n            if call_stack:\n                fr = call_stack[-1]\n            else:\n                fr = None\n            if fr not in call_dict:\n                call_dict[fr] = {}\n            if full_name not in call_dict[fr]:\n                call_dict[fr][full_name] = 0\n            call_dict[fr][full_name] += 1\n\n            if full_name not in func_count:\n                func_count[full_name] = 0\n            func_count[full_name] += 1\n            if func_count[full_name] > func_count_max:\n                func_count_max = func_count[full_name]\n\n            call_stack.append(full_name)\n            call_stack_timer.append(time.time())\n\n        else:\n            call_stack.append('')\n            call_stack_timer.append(None)\n\n    if event == 'return':\n        if call_stack:\n            full_name = call_stack.pop(-1)\n            if call_stack_timer:\n                t = call_stack_timer.pop(-1)\n            else:\n                t = None\n            if t and time_filter(stack=call_stack, full_name=full_name):\n                if full_name not in func_time:\n                    func_time[full_name] = 0\n                call_time = (time.time() - t)\n                func_time[full_name] += call_time\n                if func_time[full_name] > func_time_max:\n                    func_time_max = func_time[full_name]\n\n    return tracer", "response": "This is the internal function that is called every time a call is made. It is called every time a call is made. It keeps track of relationships between calls between calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string containing a DOT file.", "response": "def get_dot(stop=True):\n    \"\"\"Returns a string containing a DOT file. Setting stop to True will cause\n    the trace to stop.\n    \"\"\"\n    defaults = []\n    nodes    = []\n    edges    = []\n\n\n    # define default attributes\n    for comp, comp_attr in graph_attributes.items():\n        attr = ', '.join( '%s = \"%s\"' % (attr, val)\n                         for attr, val in comp_attr.items() )\n        defaults.append( '\\t%(comp)s [ %(attr)s ];\\n' % locals() )\n\n    # define nodes\n    for func, hits in func_count.items():\n        calls_frac, total_time_frac, total_time = _frac_calculation(func, hits)\n        col = settings['node_colour'](calls_frac, total_time_frac)\n        attribs = ['%s=\"%s\"' % a for a in settings['node_attributes'].items()]\n        node_str = '\"%s\" [%s];' % (func, ', '.join(attribs))\n        nodes.append( node_str % locals() )\n\n    # define edges\n    for fr_key, fr_val in call_dict.items():\n        if not fr_key: continue\n        for to_key, to_val in fr_val.items():\n            calls_frac, total_time_frac, totla_time = \\\n                _frac_calculation(to_key, to_val)\n            col = settings['edge_colour'](calls_frac, total_time_frac)\n            edge = '[ color = \"%s\", label=\"%s\" ]' % (col, to_val)\n            edges.append('\"%s\"->\"%s\" %s;' % (fr_key, to_key, edge))\n\n    defaults = '\\n\\t'.join( defaults )\n    nodes    = '\\n\\t'.join( nodes )\n    edges    = '\\n\\t'.join( edges )\n\n    dot_fmt = (\"digraph G {\\n\"\n               \"\t%(defaults)s\\n\\n\"\n               \"\t%(nodes)s\\n\\n\"\n               \"\t%(edges)s\\n}\\n\"\n              )\n    return dot_fmt % locals()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_gdf(stop=True):\n    ret = ['nodedef>name VARCHAR, label VARCHAR, hits INTEGER, ' + \\\n            'calls_frac DOUBLE, total_time_frac DOUBLE, ' + \\\n            'total_time DOUBLE, color VARCHAR, width DOUBLE']\n    for func, hits in func_count.items():\n        calls_frac, total_time_frac, total_time = _frac_calculation(func, hits)\n        col = settings['node_colour'](calls_frac, total_time_frac)\n        color = ','.join([str(round(float(c) * 255)) for c in col.split()])\n        ret.append('%s,%s,%s,%s,%s,%s,\\'%s\\',%s' % (func, func, hits, \\\n                    calls_frac, total_time_frac, total_time, color, \\\n                    math.log(hits * 10)))\n    ret.append('edgedef>node1 VARCHAR, node2 VARCHAR, color VARCHAR')\n    for fr_key, fr_val in call_dict.items():\n        if fr_key == '':\n            continue\n        for to_key, to_val in fr_val.items():\n            calls_frac, total_time_frac, total_time = \\\n                _frac_calculation(to_key, to_val)\n            col = settings['edge_colour'](calls_frac, total_time_frac)\n            color = ','.join([str(round(float(c) * 255)) for c in col.split()])\n            ret.append('%s,%s,\\'%s\\'' % (fr_key, to_key, color))\n    ret = '\\n'.join(ret)\n    return ret", "response": "Returns a string containing a GDF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a graph from the dot language file specified by filename.", "response": "def make_dot_graph(filename, format='png', tool='dot', stop=True):\n    \"\"\"Creates a graph using a Graphviz tool that supports the dot language. It\n    will output into a file specified by filename with the format specified.\n    Setting stop to True will stop the current trace.\n    \"\"\"\n    if stop:\n        stop_trace()\n\n    dot_data = get_dot()\n\n    # normalize filename\n    regex_user_expand = re.compile('\\A~')\n    if regex_user_expand.match(filename):\n        filename = os.path.expanduser(filename)\n    else:\n        filename = os.path.expandvars(filename)  # expand, just in case\n\n    if format == 'dot':\n        f = open(filename, 'w')\n        f.write(dot_data)\n        f.close()\n\n    else:\n        # create a temporary file to be used for the dot data\n        fd, tempname = tempfile.mkstemp()\n        with os.fdopen(fd, 'w') as f:\n            f.write(dot_data)\n\n        cmd = '%(tool)s -T%(format)s -o%(filename)s %(tempname)s' % locals()\n        try:\n            ret = os.system(cmd)\n            if ret:\n                raise PyCallGraphException( \\\n                    'The command \"%(cmd)s\" failed with error ' \\\n                    'code %(ret)i.' % locals())\n        finally:\n            os.unlink(tempname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_gdf_graph(filename, stop=True):\n    if stop:\n        stop_trace()\n\n    try:\n        f = open(filename, 'w')\n        f.write(get_gdf())\n    finally:\n        if f: f.close()", "response": "Create a graph in simple GDF format suitable for feeding into Gephi"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshim to run 32 - bit on 64 - bit mac as a sub - process", "response": "def macshim():\n    \"\"\"Shim to run 32-bit on 64-bit mac as a sub-process\"\"\"\n    import subprocess, sys\n    subprocess.call([\n        sys.argv[0] + '32'\n    ]+sys.argv[1:], \n        env={\"VERSIONER_PYTHON_PREFER_32_BIT\":\"yes\"}\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef file_supports_color(file_obj):\n    plat = sys.platform\n    supported_platform = plat != 'Pocket PC' and (plat != 'win32' or\n                                                  'ANSICON' in os.environ)\n\n    is_a_tty = hasattr(file_obj, 'isatty') and file_obj.isatty()\n    if not supported_platform or not is_a_tty:\n        return False\n    return True", "response": "Returns True if the running system s terminal supports color and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the ecc parameters for the ECCMan object.", "response": "def compute_ecc_params(max_block_size, rate, hasher):\n    '''Compute the ecc parameters (size of the message, size of the hash, size of the ecc). This is an helper function to easily compute the parameters from a resilience rate to instanciate an ECCMan object.'''\n    #message_size = max_block_size - int(round(max_block_size * rate * 2, 0)) # old way to compute, wasn't really correct because we applied the rate on the total message+ecc size, when we should apply the rate to the message size only (that is not known beforehand, but we want the ecc size (k) = 2*rate*message_size or in other words that k + k * 2 * rate = n)\n    message_size = int(round(float(max_block_size) / (1 + 2*rate), 0))\n    ecc_size = max_block_size - message_size\n    hash_size = len(hasher) # 32 when we use MD5\n    return {\"message_size\": message_size, \"ecc_size\": ecc_size, \"hash_size\": hash_size}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_reedsolomon_parameters(message, mesecc_orig, gen_list=[2, 3, 5], c_exp=8):\n    '''Use an exhaustive search to automatically find the correct parameters for the ReedSolomon codec from a sample message and its encoded RS code.\n    Arguments: message is the sample message, eg, \"hello world\" ; mesecc_orig is the message variable encoded with RS block appended at the end.\n    '''\n    # Description: this is basically an exhaustive search where we will try every possible RS parameter, then try to encode the sample message, and see if the resulting RS code is close to the supplied code.\n    # All variables except the Galois Field's exponent are automatically generated and searched.\n    # To compare with the supplied RS code, we compute the Hamming distance, so that even if the RS code is tampered, we can still find the closest set of RS parameters to decode this message.\n    # The goal is to provide users a function so that they can use the \"hello world\" sample string in generated ECC files to recover their RS parameters in case they forget them. But users can use any sample message: for example, if they have an untampered file and its relative ecc track, they can use the ecc track as the mesecc_orig and their original file as the sample message.\n\n    from .reedsolomon import reedsolo as reedsolop # need to import another time the reedsolo library for detect_reedsolomon_parameters to work (because we need to reinit all the tables, and they are declared module-wide, so this would conflict with decoding)\n\n    # Init the variables\n    n = len(mesecc_orig)\n    k = len(message)\n    field_charac = int((2**c_exp) - 1)\n    maxval1 = max([ord(x) if isinstance(x, basestring) else x for x in message ])\n    maxval2 = max([ord(x) if isinstance(x, basestring) else x for x in mesecc_orig])\n    maxval = max([maxval1, maxval2])\n    if (maxval > field_charac):\n        raise ValueError(\"The specified field's exponent is wrong, the message contains values (%i) above the field's cardinality (%i)!\" % (maxval, field_charac))\n\n    # Prepare the variable that will store the result\n    best_match = {\"hscore\": -1, \"params\": [{\"gen_nb\": 0, \"prim\": 0, \"fcr\": 0}]}\n\n    # Exhaustively search by generating every combination of values for the RS parameters and test the Hamming distance\n    for gen_nb in gen_list:\n        prim_list = reedsolop.find_prime_polys(generator=gen_nb, c_exp=c_exp, fast_primes=False, single=False)\n        for prim in prim_list:\n            reedsolop.init_tables(prim)\n            for fcr in xrange(field_charac):\n                #g = reedsolop.rs_generator_poly_all(n, fcr=fcr, generator=gen_nb)\n                # Generate a RS code from the sample message using the current combination of RS parameters\n                mesecc = reedsolop.rs_encode_msg(message, n-k, fcr=fcr)\n                # Compute the Hamming distance\n                h = hamming(mesecc, mesecc_orig)\n                # If the Hamming distance is lower than the previous best match (or if it's the first try), save this set of parameters\n                if best_match[\"hscore\"] == -1 or h <= best_match[\"hscore\"]:\n                    # If the distance is strictly lower than for the previous match, then we replace the previous match with the current one\n                    if best_match[\"hscore\"] == -1 or h < best_match[\"hscore\"]:\n                        best_match[\"hscore\"] = h\n                        best_match[\"params\"] = [{\"gen_nb\": gen_nb, \"prim\": prim, \"fcr\": fcr}]\n                    # Else there is an ambiguity: the Hamming distance is the same as for the previous best match, so we keep the previous set of parameters but we append the current set\n                    elif h == best_match[\"hscore\"]:\n                        best_match[\"params\"].append({\"gen_nb\": gen_nb, \"prim\": prim, \"fcr\": fcr})\n                    # If Hamming distance is 0, then we have found a perfect match (the current set of parameters allow to generate the exact same RS code from the sample message), so we stop here\n                    if h == 0: break\n\n    # Printing the results to the user\n    if best_match[\"hscore\"] >= 0 and best_match[\"hscore\"] < len(mesecc_orig):\n        perfect_match_str = \" (0=perfect match)\" if best_match[\"hscore\"]==0 else \"\"\n        result = ''\n        result += \"Found closest set of parameters, with Hamming distance %i%s:\\n\" % (best_match[\"hscore\"], perfect_match_str)\n        for param in best_match[\"params\"]:\n            result += \"gen_nb=%s prim=%s(%s) fcr=%s\\n\" % (param[\"gen_nb\"], param[\"prim\"], hex(param[\"prim\"]), param[\"fcr\"])\n        return result\n    else:\n        return \"Parameters could not be automatically detected...\"", "response": "Detects the correct parameters for the ReedSolomon codec from a sample message and its encoded RS code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self, message, k=None):\n        '''Encode one message block (up to 255) into an ecc'''\n        if not k: k = self.k\n        message, _ = self.pad(message, k=k)\n        if self.algo == 1:\n            mesecc = self.ecc_manager.encode(message, k=k)\n        elif self.algo == 2:\n            mesecc = self.ecc_manager.encode_fast(message, k=k)\n        elif self.algo == 3 or self.algo == 4:\n            mesecc = rs_encode_msg(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])\n            #mesecc = rs_encode_msg_precomp(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])\n\n        ecc = mesecc[len(message):]\n        return ecc", "response": "Encode one message block into an ecc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrepairing a message and its ecc also given the message and its ecc", "response": "def decode(self, message, ecc, k=None, enable_erasures=False, erasures_char=\"\\x00\", only_erasures=False):\n        '''Repair a message and its ecc also, given the message and its ecc (both can be corrupted, we will still try to fix both of them)'''\n        if not k: k = self.k\n\n        # Optimization, use bytearray\n        if isinstance(message, _str):\n            message = bytearray([ord(x) for x in message])\n            ecc = bytearray([ord(x) for x in ecc])\n\n        # Detect erasures positions and replace with null bytes (replacing erasures with null bytes is necessary for correct syndrome computation)\n        # Note that this must be done before padding, else we risk counting the padded null bytes as erasures!\n        erasures_pos = None\n        if enable_erasures:\n            # Concatenate to find erasures in the whole codeword\n            mesecc = message + ecc\n            # Convert char to a int (because we use a bytearray)\n            if isinstance(erasures_char, _str): erasures_char = ord(erasures_char)\n            # Find the positions of the erased characters\n            erasures_pos = [i for i in xrange(len(mesecc)) if mesecc[i] == erasures_char]\n            # Failing case: no erasures could be found and we want to only correct erasures, then we return the message as-is\n            if only_erasures and not erasures_pos: return message, ecc\n\n        # Pad with null bytes if necessary\n        message, pad = self.pad(message, k=k)\n        ecc, _ = self.rpad(ecc, k=k) # fill ecc with null bytes if too small (maybe the field delimiters were misdetected and this truncated the ecc? But we maybe still can correct if the truncation is less than the resilience rate)\n\n        # If the message was left padded, then we need to update the positions of the erasures\n        if erasures_pos and pad:\n            len_pad = len(pad)\n            erasures_pos = [x+len_pad for x in erasures_pos]\n\n        # Decoding\n        if self.algo == 1:\n            msg_repaired, ecc_repaired = self.ecc_manager.decode(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures) # Avoid automatic stripping because we are working with binary streams, thus we should manually strip padding only when we know we padded\n        elif self.algo == 2:\n            msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)\n        elif self.algo == 3:\n            #msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)\n            msg_repaired, ecc_repaired = reedsolo.rs_correct_msg_nofsynd(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)\n            msg_repaired = bytearray(msg_repaired)\n            ecc_repaired = bytearray(ecc_repaired)\n        elif self.algo == 4:\n            msg_repaired, ecc_repaired = reedsolo.rs_correct_msg(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)\n            msg_repaired = bytearray(msg_repaired)\n            ecc_repaired = bytearray(ecc_repaired)\n\n        if pad: # Strip the null bytes if we padded the message before decoding\n            msg_repaired = msg_repaired[len(pad):len(msg_repaired)]\n        return msg_repaired, ecc_repaired"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pad(self, message, k=None):\n        '''Automatically left pad with null bytes a message if too small, or leave unchanged if not necessary. This allows to keep track of padding and strip the null bytes after decoding reliably with binary data. Equivalent to shortening (shortened reed-solomon code).'''\n        if not k: k = self.k\n        pad = None\n        if len(message) < k:\n            #pad = \"\\x00\" * (k-len(message))\n            pad = bytearray(k-len(message))\n            message = pad + message\n        return [message, pad]", "response": "Automatically left pad with null bytes a message if too small or leave unchanged if not necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rpad(self, ecc, k=None):\n        '''Automatically right pad with null bytes an ecc to fill for missing bytes if too small, or leave unchanged if not necessary. This can be used as a workaround for field delimiter misdetection. Equivalent to puncturing (punctured reed-solomon code).'''\n        if not k: k = self.k\n        pad = None\n        if len(ecc) < self.n-k:\n            print(\"Warning: the ecc field may have been truncated (entrymarker or field_delim misdetection?).\")\n            #pad = \"\\x00\" * (self.n-k-len(ecc))\n            pad = bytearray(self.n-k-len(ecc))\n            ecc = ecc + pad\n        return [ecc, pad]", "response": "Automatically right pad with null bytes an ecc to fill for missing bytes or leave unchanged if not necessary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(self, message, ecc, k=None):\n        '''Check if there's any error in a message+ecc. Can be used before decoding, in addition to hashes to detect if the message was tampered, or after decoding to check that the message was fully recovered.'''\n        if not k: k = self.k\n        message, _ = self.pad(message, k=k)\n        ecc, _ = self.rpad(ecc, k=k)\n        if self.algo == 1 or self.algo == 2:\n            return self.ecc_manager.check_fast(message + ecc, k=k)\n        elif self.algo == 3 or self.algo == 4:\n            return reedsolo.rs_check(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb)", "response": "Check if there s any error in a message + ecc. Can be used before decoding and addition to hashes to detect if the message was fully recovered."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides a description for each algorithm available useful to print in ecc file.", "response": "def description(self):\n        '''Provide a description for each algorithm available, useful to print in ecc file'''\n        if 0 < self.algo <= 3:\n            return \"Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) with generator=%s, prime poly=%s and first consecutive root=%s.\" % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)\n        elif self.algo == 4:\n            return \"Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) under US FAA ADSB UAT RS FEC standard with generator=%s, prime poly=%s and first consecutive root=%s.\" % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)\n        else:\n            return \"No description for this ECC algorithm.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef profile(fn=None, skip=0, filename=None, immediate=False, dirs=False,\n            sort=None, entries=40,\n            profiler=('cProfile', 'profile', 'hotshot')):\n    \"\"\"Mark `fn` for profiling.\n\n    If `skip` is > 0, first `skip` calls to `fn` will not be profiled.\n\n    If `immediate` is False, profiling results will be printed to\n    sys.stdout on program termination.  Otherwise results will be printed\n    after each call.\n\n    If `dirs` is False only the name of the file will be printed.\n    Otherwise the full path is used.\n\n    `sort` can be a list of sort keys (defaulting to ['cumulative',\n    'time', 'calls']).  The following ones are recognized::\n\n        'calls'      -- call count\n        'cumulative' -- cumulative time\n        'file'       -- file name\n        'line'       -- line number\n        'module'     -- file name\n        'name'       -- function name\n        'nfl'        -- name/file/line\n        'pcalls'     -- call count\n        'stdname'    -- standard name\n        'time'       -- internal time\n\n    `entries` limits the output to the first N entries.\n\n    `profiler` can be used to select the preferred profiler, or specify a\n    sequence of them, in order of preference.  The default is ('cProfile'.\n    'profile', 'hotshot').\n\n    If `filename` is specified, the profile stats will be stored in the\n    named file.  You can load them pstats.Stats(filename).\n\n    Usage::\n\n        def fn(...):\n            ...\n        fn = profile(fn, skip=1)\n\n    If you are using Python 2.4, you should be able to use the decorator\n    syntax::\n\n        @profile(skip=3)\n        def fn(...):\n            ...\n\n    or just ::\n\n        @profile\n        def fn(...):\n            ...\n\n    \"\"\"\n    if fn is None: # @profile() syntax -- we are a decorator maker\n        def decorator(fn):\n            return profile(fn, skip=skip, filename=filename,\n                           immediate=immediate, dirs=dirs,\n                           sort=sort, entries=entries,\n                           profiler=profiler)\n        return decorator\n    # @profile syntax -- we are a decorator.\n    if isinstance(profiler, str):\n        profiler = [profiler]\n    for p in profiler:\n        if p in AVAILABLE_PROFILERS:\n            profiler_class = AVAILABLE_PROFILERS[p]\n            break\n    else:\n        raise ValueError('only these profilers are available: %s'\n                             % ', '.join(AVAILABLE_PROFILERS))\n    fp = profiler_class(fn, skip=skip, filename=filename,\n                        immediate=immediate, dirs=dirs,\n                        sort=sort, entries=entries)\n    # fp = HotShotFuncProfile(fn, skip=skip, filename=filename, ...)\n         # or HotShotFuncProfile\n    # We cannot return fp or fp.__call__ directly as that would break method\n    # definitions, instead we need to return a plain function.\n    def new_fn(*args, **kw):\n        return fp(*args, **kw)\n    new_fn.__doc__ = fn.__doc__\n    new_fn.__name__ = fn.__name__\n    new_fn.__dict__ = fn.__dict__\n    new_fn.__module__ = fn.__module__\n    return new_fn", "response": "Profile a file or file with the specified number of entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coverage(fn):\n    fp = TraceFuncCoverage(fn) # or HotShotFuncCoverage\n    # We cannot return fp or fp.__call__ directly as that would break method\n    # definitions, instead we need to return a plain function.\n    def new_fn(*args, **kw):\n        return fp(*args, **kw)\n    new_fn.__doc__ = fn.__doc__\n    new_fn.__name__ = fn.__name__\n    new_fn.__dict__ = fn.__dict__\n    new_fn.__module__ = fn.__module__\n    return new_fn", "response": "Mark a function as line coverage analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmarks fn for line coverage analysis.", "response": "def coverage_with_hotshot(fn):\n    \"\"\"Mark `fn` for line coverage analysis.\n\n    Uses the 'hotshot' module for fast coverage analysis.\n\n    BUG: Produces inaccurate results.\n\n    See the docstring of `coverage` for usage examples.\n    \"\"\"\n    fp = HotShotFuncCoverage(fn)\n    # We cannot return fp or fp.__call__ directly as that would break method\n    # definitions, instead we need to return a plain function.\n    def new_fn(*args, **kw):\n        return fp(*args, **kw)\n    new_fn.__doc__ = fn.__doc__\n    new_fn.__name__ = fn.__name__\n    new_fn.__dict__ = fn.__dict__\n    new_fn.__module__ = fn.__module__\n    return new_fn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timecall(fn=None, immediate=True, timer=time.time):\n    if fn is None: # @timecall() syntax -- we are a decorator maker\n        def decorator(fn):\n            return timecall(fn, immediate=immediate, timer=timer)\n        return decorator\n    # @timecall syntax -- we are a decorator.\n    fp = FuncTimer(fn, immediate=immediate, timer=timer)\n    # We cannot return fp or fp.__call__ directly as that would break method\n    # definitions, instead we need to return a plain function.\n    def new_fn(*args, **kw):\n        return fp(*args, **kw)\n    new_fn.__doc__ = fn.__doc__\n    new_fn.__name__ = fn.__name__\n    new_fn.__dict__ = fn.__dict__\n    new_fn.__module__ = fn.__module__\n    return new_fn", "response": "Wrap a function to print its execution time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_stats(self):\n        funcname = self.fn.__name__\n        filename = self.fn.__code__.co_filename\n        lineno = self.fn.__code__.co_firstlineno\n        print(\"\")\n        print(\"*** PROFILER RESULTS ***\")\n        print(\"%s (%s:%s)\" % (funcname, filename, lineno))\n        if self.skipped:\n            skipped = \"(%d calls not profiled)\" % self.skipped\n        else:\n            skipped = \"\"\n        print(\"function called %d times%s\" % (self.ncalls, skipped))\n        print(\"\")\n        stats = self.stats\n        if self.filename:\n            stats.dump_stats(self.filename)\n        if not self.dirs:\n            stats.strip_dirs()\n        stats.sort_stats(*self.sort)\n        stats.print_stats(self.entries)", "response": "Print profile information to sys. stdout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset accumulated profiler statistics.", "response": "def reset_stats(self):\n        \"\"\"Reset accumulated profiler statistics.\"\"\"\n        # Note: not using self.Profile, since pstats.Stats() fails then\n        self.stats = pstats.Stats(Profile())\n        self.ncalls = 0\n        self.skipped = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef atexit(self):\n        funcname = self.fn.__name__\n        filename = self.fn.__code__.co_filename\n        lineno = self.fn.__code__.co_firstlineno\n        print(\"\")\n        print(\"*** COVERAGE RESULTS ***\")\n        print(\"%s (%s:%s)\" % (funcname, filename, lineno))\n        print(\"function called %d times\" % self.ncalls)\n        print(\"\")\n        fs = FuncSource(self.fn)\n        for (filename, lineno), count in self.tracer.counts.items():\n            if filename != fs.filename:\n                continue\n            fs.mark(lineno, count)\n        print(fs)\n        never_executed = fs.count_never_executed()\n        if never_executed:\n            print(\"%d lines were not executed.\" % never_executed)", "response": "Stop profiling and print profile information to sys. stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_source_lines(self):\n        strs = trace.find_strings(self.filename)\n        lines = trace.find_lines_from_code(self.fn.__code__, strs)\n        self.firstcodelineno = sys.maxint\n        for lineno in lines:\n            self.firstcodelineno = min(self.firstcodelineno, lineno)\n            self.sourcelines.setdefault(lineno, 0)\n        if self.firstcodelineno == sys.maxint:\n            self.firstcodelineno = self.firstlineno", "response": "Find all executable source lines in fn as executed 0 times."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mark(self, lineno, count=1):\n        self.sourcelines[lineno] = self.sourcelines.get(lineno, 0) + count", "response": "Mark a given source line as executed count times."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount statements that were never executed.", "response": "def count_never_executed(self):\n        \"\"\"Count statements that were never executed.\"\"\"\n        lineno = self.firstlineno\n        counter = 0\n        for line in self.source:\n            if self.sourcelines.get(lineno) == 0:\n                if not self.blank_rx.match(line):\n                    counter += 1\n            lineno += 1\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the element val to the list.", "response": "def add(self, val):\n        \"\"\"Add the element *val* to the list.\"\"\"\n        _maxes, _lists = self._maxes, self._lists\n\n        if _maxes:\n            pos = bisect_right(_maxes, val)\n\n            if pos == len(_maxes):\n                pos -= 1\n                _maxes[pos] = val\n                _lists[pos].append(val)\n            else:\n                insort(_lists[pos], val)\n\n            self._expand(pos)\n        else:\n            _maxes.append(val)\n            _lists.append([val])\n\n        self._len += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving first occurrence of val. Raises ValueError if val is not present.", "response": "def remove(self, val):\n        \"\"\"\n        Remove first occurrence of *val*.\n\n        Raises ValueError if *val* is not present.\n        \"\"\"\n        _maxes = self._maxes\n\n        if not _maxes:\n            raise ValueError('{0} not in list'.format(repr(val)))\n\n        pos = bisect_left(_maxes, val)\n\n        if pos == len(_maxes):\n            raise ValueError('{0} not in list'.format(repr(val)))\n\n        _lists = self._lists\n        idx = bisect_left(_lists[pos], val)\n        if _lists[pos][idx] == val:\n            self._delete(pos, idx)\n        else:\n            raise ValueError('{0} not in list'.format(repr(val)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _delete(self, pos, idx):\n        _maxes, _lists, _index = self._maxes, self._lists, self._index\n\n        lists_pos = _lists[pos]\n\n        del lists_pos[idx]\n        self._len -= 1\n\n        len_lists_pos = len(lists_pos)\n\n        if len_lists_pos > self._half:\n\n            _maxes[pos] = lists_pos[-1]\n\n            if _index:\n                child = self._offset + pos\n                while child > 0:\n                    _index[child] -= 1\n                    child = (child - 1) >> 1\n                _index[0] -= 1\n\n        elif len(_lists) > 1:\n\n            if not pos:\n                pos += 1\n\n            prev = pos - 1\n            _lists[prev].extend(_lists[pos])\n            _maxes[prev] = _lists[prev][-1]\n\n            del _maxes[pos]\n            del _lists[pos]\n            del _index[:]\n\n            self._expand(prev)\n\n        elif len_lists_pos:\n\n            _maxes[pos] = lists_pos[-1]\n\n        else:\n\n            del _maxes[pos]\n            del _lists[pos]\n            del _index[:]", "response": "Delete the item at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextending the list by appending all elements from the given list. Raises a ValueError if the sort order is violated.", "response": "def extend(self, values):\n        \"\"\"\n        Extend the list by appending all elements from the *values*. Raises a\n        ValueError if the sort order would be violated.\n        \"\"\"\n        _maxes, _lists, _load = self._maxes, self._lists, self._load\n\n        if not isinstance(values, list):\n            values = list(values)\n\n        if any(values[pos - 1] > values[pos]\n               for pos in range(1, len(values))):\n            raise ValueError('given sequence not in sort order')\n\n        offset = 0\n\n        if _maxes:\n            if values[0] < _lists[-1][-1]:\n                msg = '{0} not in sort order at index {1}'.format(repr(values[0]), self._len)\n                raise ValueError(msg)\n\n            if len(_lists[-1]) < self._half:\n                _lists[-1].extend(values[:_load])\n                _maxes[-1] = _lists[-1][-1]\n                offset = _load\n\n        len_lists = len(_lists)\n\n        for idx in range(offset, len(values), _load):\n            _lists.append(values[idx:(idx + _load)])\n            _maxes.append(_lists[-1][-1])\n\n        _index = self._index\n\n        if len_lists == len(_lists):\n            len_index = len(_index)\n            if len_index > 0:\n                len_values = len(values)\n                child = len_index - 1\n                while child:\n                    _index[child] += len_values\n                    child = (child - 1) >> 1\n                _index[0] += len_values\n        else:\n            del _index[:]\n\n        self._len += len(values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninserting the element val into the list at idx. Raises a ValueError if the element val is not in the sort order.", "response": "def insert(self, idx, val):\n        \"\"\"\n        Insert the element *val* into the list at *idx*. Raises a ValueError if\n        the *val* at *idx* would violate the sort order.\n        \"\"\"\n        _maxes, _lists, _len = self._maxes, self._lists, self._len\n\n        if idx < 0:\n            idx += _len\n        if idx < 0:\n            idx = 0\n        if idx > _len:\n            idx = _len\n\n        if not _maxes:\n            # The idx must be zero by the inequalities above.\n            _maxes.append(val)\n            _lists.append([val])\n            self._len = 1\n            return\n\n        if not idx:\n            if val > _lists[0][0]:\n                msg = '{0} not in sort order at index {1}'.format(repr(val), 0)\n                raise ValueError(msg)\n            else:\n                _lists[0].insert(0, val)\n                self._expand(0)\n                self._len += 1\n                return\n\n        if idx == _len:\n            pos = len(_lists) - 1\n            if _lists[pos][-1] > val:\n                msg = '{0} not in sort order at index {1}'.format(repr(val), _len)\n                raise ValueError(msg)\n            else:\n                _lists[pos].append(val)\n                _maxes[pos] = _lists[pos][-1]\n                self._expand(pos)\n                self._len += 1\n                return\n\n        pos, idx = self._pos(idx)\n        idx_before = idx - 1\n        if idx_before < 0:\n            pos_before = pos - 1\n            idx_before = len(_lists[pos_before]) - 1\n        else:\n            pos_before = pos\n\n        before = _lists[pos_before][idx_before]\n        if before <= val <= _lists[pos][idx]:\n            _lists[pos].insert(idx, val)\n            self._expand(pos)\n            self._len += 1\n        else:\n            msg = '{0} not in sort order at index {1}'.format(repr(val), idx)\n            raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the list by adding all elements from iterable.", "response": "def update(self, iterable):\n        \"\"\"Update the list by adding all elements from *iterable*.\"\"\"\n        _maxes, _lists, _keys = self._maxes, self._lists, self._keys\n        values = sorted(iterable, key=self._key)\n\n        if _maxes:\n            if len(values) * 4 >= self._len:\n                values.extend(chain.from_iterable(_lists))\n                values.sort(key=self._key)\n                self._clear()\n            else:\n                _add = self.add\n                for val in values:\n                    _add(val)\n                return\n\n        _load, _index = self._load, self._index\n        _lists.extend(values[pos:(pos + _load)]\n                      for pos in range(0, len(values), _load))\n        _keys.extend(list(map(self._key, _list)) for _list in _lists)\n        _maxes.extend(sublist[-1] for sublist in _keys)\n        self._len = len(values)\n        del _index[:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the username of the current object.", "response": "def set_username_ibm(self, username_ibm):\n        \"\"\"\n        Parameters\n        ----------\n        username_ibm : str\n\n        Raises\n        ------\n        Exception\n            If mode is not `ibm`\n        \"\"\"\n        if self.get_mode() == \"ibm\":\n            self.__username_ibm = username_ibm\n        else:\n            raise Exception(\n                \"Mode is {}, whereas it must be `ibm`\".format(\n                    self.get_moder()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the password of the current locale.", "response": "def set_password_ibm(self, password_ibm):\n        \"\"\"\n        Parameters\n        ----------\n        password_ibm : str\n\n        Raises\n        ------\n        Exception\n            If mode is not `ibm`\n        \"\"\"\n        if self.get_mode() == \"ibm\":\n            self.__password_ibm = password_ibm\n        else:\n            raise Exception(\n                \"Mode is {}, whereas it must be `ibm`\".format(self.get_mode()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all audio files in the source directory.", "response": "def _list_audio_files(self, sub_dir=\"\"):\n        \"\"\"\n        Parameters\n        ----------\n        sub_dir : one of `needed_directories`, optional\n            Default is \"\", which means it'll look through all of subdirs.\n\n        Returns\n        -------\n        audio_files : [str]\n            A list whose elements are basenames of the present audiofiles whose\n            formats are `wav`\n        \"\"\"\n        audio_files = list()\n        for possibly_audio_file in os.listdir(\"{}/{}\".format(self.src_dir,\n                                                             sub_dir)):\n            file_format = ''.join(possibly_audio_file.split('.')[-1])\n            if file_format.lower() == \"wav\":\n                audio_files.append(possibly_audio_file)\n        return audio_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_audio_channels(self, audio_abs_path):\n        channel_num = int(\n            subprocess.check_output(\n                (\"\"\"sox --i {} | grep \"{}\" | awk -F \" : \" '{{print $2}}'\"\"\"\n                 ).format(audio_abs_path, \"Channels\"),\n                shell=True, universal_newlines=True).rstrip())\n        return channel_num", "response": "Get the number of audio channels from the audio file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_audio_sample_rate(self, audio_abs_path):\n        sample_rate = int(\n           subprocess.check_output(\n               (\"\"\"sox --i {} | grep \"{}\" | awk -F \" : \" '{{print $2}}'\"\"\"\n                ).format(audio_abs_path, \"Sample Rate\"),\n               shell=True, universal_newlines=True).rstrip())\n        return sample_rate", "response": "Get the audio sample rate from the audio file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the audio sample bit from the cache file.", "response": "def _get_audio_sample_bit(self, audio_abs_path):\n        \"\"\"\n        Parameters\n        ----------\n        audio_abs_path : str\n\n        Returns\n        -------\n        sample_bit : int\n        \"\"\"\n        sample_bit = int(\n           subprocess.check_output(\n               (\"\"\"sox --i {} | grep \"{}\" | awk -F \" : \" '{{print $2}}' | \"\"\"\n                \"\"\"grep -oh \"^[^-]*\" \"\"\").format(audio_abs_path, \"Precision\"),\n               shell=True, universal_newlines=True).rstrip())\n        return sample_bit"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the total number of seconds until the audio file has been downloaded.", "response": "def _get_audio_duration_seconds(self, audio_abs_path):\n        \"\"\"\n        Parameters\n        ----------\n        audio_abs_path : str\n\n        Returns\n        -------\n        total_seconds : int\n        \"\"\"\n        HHMMSS_duration = subprocess.check_output(\n            (\"\"\"sox --i {} | grep \"{}\" | awk -F \" : \" '{{print $2}}' | \"\"\"\n             \"\"\"grep -oh \"^[^=]*\" \"\"\").format(\n                audio_abs_path, \"Duration\"),\n            shell=True, universal_newlines=True).rstrip()\n        total_seconds = sum(\n            [float(x) * 60 ** (2 - i)\n             for i, x in enumerate(HHMMSS_duration.split(\":\"))])\n        return total_seconds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the audio bit rate for the given audio file.", "response": "def _get_audio_bit_rate(self, audio_abs_path):\n        \"\"\"\n        Parameters\n        -----------\n        audio_abs_path : str\n\n        Returns\n        -------\n        bit_rate : int\n        \"\"\"\n        bit_Rate_formatted = subprocess.check_output(\n            \"\"\"sox --i {} | grep \"{}\" | awk -F \" : \" '{{print $2}}'\"\"\".format(\n                audio_abs_path, \"Bit Rate\"),\n            shell=True, universal_newlines=True).rstrip()\n        bit_rate = (lambda x:\n                    int(x[:-1]) * 10 ** 3 if x[-1].lower() == \"k\" else\n                    int(x[:-1]) * 10 ** 6 if x[-1].lower() == \"m\" else\n                    int(x[:-1]) * 10 ** 9 if x[-1].lower() == \"g\" else\n                    int(x))(bit_Rate_formatted)\n        return bit_rate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _seconds_to_HHMMSS(seconds):\n        less_than_second = seconds - floor(seconds)\n        minutes, seconds = divmod(floor(seconds), 60)\n        hours, minutes = divmod(minutes, 60)\n        return \"{}H{}M{}S.{}\".format(hours, minutes, seconds, less_than_second)", "response": "Returns a string which is the hour minute second representation of the intput seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the audio segments from the audio file.", "response": "def _audio_segment_extractor(self, audio_abs_path, segment_abs_path,\n                                 starting_second, duration):\n        \"\"\"\n        Parameters\n        -----------\n        audio_abs_path : str\n        segment_abs_path : str\n        starting_second : int\n        duration : int\n        \"\"\"\n\n        subprocess.Popen([\"sox\",  str(audio_abs_path), str(segment_abs_path),\n                          \"trim\", str(starting_second), str(duration)],\n                         universal_newlines=True).communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_audio_by_duration(self, audio_abs_path,\n                                 results_abs_path, duration_seconds):\n        \"\"\"\n        Calculates the length of each segment and passes it to\n        self._audio_segment_extractor\n\n        Parameters\n        ----------\n        audio_abs_path : str\n        results_abs_path : str\n            A place for adding digits needs to be added prior the the format\n            decleration i.e. name%03.wav. Here, we've added `*` at staging\n            step, which we'll replace.\n        duration_seconds : int\n        \"\"\"\n        total_seconds = self._get_audio_duration_seconds(audio_abs_path)\n        current_segment = 0\n        while current_segment <= total_seconds // duration_seconds + 1:\n            if current_segment + duration_seconds > total_seconds:\n                ending_second = total_seconds\n            else:\n                ending_second = current_segment + duration_seconds\n            self._audio_segment_extractor(\n                audio_abs_path,\n                results_abs_path.replace(\"*\", \"{:03d}\".format(\n                    current_segment)),\n                starting_second=current_segment, duration=(ending_second -\n                                                           current_segment))\n            current_segment += 1", "response": "Splits the audio file into individual segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the audio file into individual items and splits them into individual items.", "response": "def _split_audio_by_size(self, audio_abs_path, results_abs_path,\n                             chunk_size):\n        \"\"\"\n        Calculates the duration of the name.wav in order for all splits have\n        the size of chunk_size except possibly the last split (which will be\n        smaller) and then passes the duration to `split_audio_by_duration`\n\n        Parameters\n        ----------\n        audio_abs_path : str\n        results_abs_path : str\n            A place for adding digits needs to be added prior the the format\n            decleration i.e. name%03.wav\n        chunk_size : int\n            Should be in bytes\n        \"\"\"\n        sample_rate = self._get_audio_sample_rate(audio_abs_path)\n        sample_bit = self._get_audio_sample_bit(audio_abs_path)\n        channel_num = self._get_audio_channels(audio_abs_path)\n        duration = 8 * chunk_size / reduce(lambda x, y: int(x) * int(y),\n                                           [sample_rate, sample_bit,\n                                            channel_num])\n        self._split_audio_by_duration(audio_abs_path, results_abs_path,\n                                      duration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves the audio file if the format is `wav` to `filtered` directory. Parameters ---------- basename : str A basename of `/home/random-guy/some-audio-file.wav` is `some-audio-file.wav`", "response": "def _filtering_step(self, basename):\n        \"\"\"\n        Moves the audio file if the format is `wav` to `filtered` directory.\n\n        Parameters\n        ----------\n        basename : str\n            A basename of `/home/random-guy/some-audio-file.wav` is\n            `some-audio-file.wav`\n        \"\"\"\n        name = ''.join(basename.split('.')[:-1])\n        # May cause problems if wav is not less than 9 channels.\n        if basename.split('.')[-1] == \"wav\":\n            if self.get_verbosity():\n                print(\"Found wave! Copying to {}/filtered/{}\".format(\n                    self.src_dir, basename))\n            subprocess.Popen([\"cp\", \"{}/{}.wav\".format(self.src_dir, name),\n                              \"{}/filtered/{}.wav\".format(self.src_dir, name)],\n                             universal_newlines=True).communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck the size of audio file, splits it if it's needed to manage api limit and then moves to `staged` directory while appending `*` to the end of the filename for self.split_audio_by_duration to replace it by a number. Parameters ---------- basename : str A basename of `/home/random-guy/some-audio-file.wav` is `some-audio-file.wav`", "response": "def _staging_step(self, basename):\n        \"\"\"\n        Checks the size of audio file, splits it if it's needed to manage api\n        limit and then moves to `staged` directory while appending `*` to\n        the end of the filename for self.split_audio_by_duration to replace\n        it by a number.\n\n        Parameters\n        ----------\n        basename : str\n            A basename of `/home/random-guy/some-audio-file.wav` is\n            `some-audio-file.wav`\n        \"\"\"\n        name = ''.join(basename.split('.')[:-1])\n\n        if self.get_mode() == \"ibm\":\n            # Checks the file size. It's better to use 95% of the allocated\n            # size per file since the upper limit is not always respected.\n            total_size = os.path.getsize(\"{}/filtered/{}.wav\".format(\n                self.src_dir, name))\n            if total_size >= self.ibm_api_limit_bytes:\n                if self.get_verbosity():\n                    print((\"{}'s size over API limit ({}). Splitting\").format(\n                        name, self.ibm_api_limit_bytes))\n                self._split_audio_by_size(\n                    \"{}/filtered/{}.wav\".format(self.src_dir, name),\n                    \"{}/staging/{}*.wav\".format(self.src_dir, name),\n                    self.ibm_api_limit_bytes * 95 / 100)\n            else:\n                if self.get_verbosity():\n                    print(\"{}'s size is fine. Moving to staging dir'\".format(\n                        name))\n                subprocess.Popen((\n                    \"mv {}/filtered/{}.wav {}/staging/{}000.wav\").format(\n                                    self.src_dir, name, self.src_dir, name),\n                                 shell=True,\n                                 universal_newlines=True).communicate()\n\n        elif self.get_mode() == \"cmu\":\n            if self.get_verbosity():\n                print(\"Converting {} to a readable wav\".format(basename))\n            ffmpeg = os.path.basename(find_executable(\"ffmpeg\") or\n                                      find_executable(\"avconv\"))\n            if ffmpeg is None:\n                raise Exception((\"Either ffmpeg or avconv is needed. \"\n                                 \"Neither is installed or accessible\"))\n            try:\n                # ffmpeg log levels:\n                # https://ffmpeg.org/ffmpeg.html#Generic-options\n                ffmpeg_log_level = \"8\"  # fatal errors.\n                if self.get_verbosity():\n                    ffmpeg_log_level = \"32\"  # info `default for ffmpeg`\n                subprocess.check_call([\n                    str(ffmpeg), \"-y\", \"-i\", \"{}/filtered/{}.wav\".format(\n                        self.src_dir, str(name)), \"-acodec\", \"pcm_s16le\",\n                    \"-ac\", \"1\", \"-ar\", \"16000\", \"{}/staging/{}000.wav\".format(\n                        self.src_dir, name),\n                    \"-v\", ffmpeg_log_level], universal_newlines=True)\n            except subprocess.CalledProcessError as e:\n                print(e)\n            if os.path.exists(\"{}/staging/{}000.wav\".format(\n                    self.src_dir, name)):\n                if self.get_verbosity():\n                    print((\"{}/filtered/{} was converted to \"\n                           \"{}/staging/{}000.wav Now removing the copy of \"\n                           \"{} in filtered sub directory\").format(\n                                self.src_dir, basename,\n                                self.src_dir, name, basename))\n                subprocess.Popen([\n                    \"rm\", \"{}/filtered/{}\".format(self.src_dir, basename)],\n                                    universal_newlines=True).communicate()\n            else:\n                raise Exception(\"Something went wrong with ffmpeg conversion!\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares and stages the audio files.", "response": "def _prepare_audio(self, basename, replace_already_indexed=False):\n        \"\"\"\n        Prepares and stages the audio file to be indexed.\n\n        Parameters\n        ----------\n        basename : str, None\n            A basename of `/home/random-guy/some-audio-file.wav` is\n            `some-audio-file.wav`\n            If basename is `None`, it'll prepare all the audio files.\n        \"\"\"\n        if basename is not None:\n            if basename in self.get_timestamps():\n                if self.get_verbosity():\n                    print(\"File specified was already indexed. Reindexing...\")\n                del self.__timestamps[basename]\n            self._filtering_step(basename)\n            self._staging_step(basename)\n        else:\n            for audio_basename in self._list_audio_files():\n                if audio_basename in self.__timestamps:\n                    if replace_already_indexed:\n                        if self.get_verbosity():\n                            print(\"Already indexed {}. Reindexing...\".format(\n                                audio_basename))\n                        del self.__timestamps[audio_basename]\n                    else:\n                        if self.get_verbosity():\n                            print(\"Already indexed {}. Skipping...\".format(\n                                audio_basename))\n                        continue\n                self._filtering_step(audio_basename)\n                self._staging_step(audio_basename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nindex audio with pocketsphinx. Beware that the output would not be sufficiently accurate. Use this only if you don't want to upload your files to IBM. Parameters ----------- basename : str, optional A specific basename to be indexed and is placed in src_dir E.g. `audio.wav`. If `None` is selected, all the valid audio files would be indexed. Default is `None`. Raises ------ OSError If the output of pocketsphinx command results in an error.", "response": "def _index_audio_cmu(self, basename=None, replace_already_indexed=False):\n        \"\"\"\n        Indexes audio with pocketsphinx. Beware that the output would not be\n        sufficiently accurate. Use this only if you don't want to upload your\n        files to IBM.\n\n        Parameters\n        -----------\n        basename : str, optional\n            A specific basename to be indexed and is placed in src_dir\n            E.g. `audio.wav`.\n\n            If `None` is selected, all the valid audio files would be indexed.\n            Default is `None`.\n\n        Raises\n        ------\n        OSError\n            If the output of pocketsphinx command results in an error.\n        \"\"\"\n        self._prepare_audio(basename=basename,\n                            replace_already_indexed=replace_already_indexed)\n\n        for staging_audio_basename in self._list_audio_files(\n                sub_dir=\"staging\"):\n            original_audio_name = ''.join(\n                staging_audio_basename.split('.')[:-1])[:-3]\n            pocketsphinx_command = ''.join([\n                \"pocketsphinx_continuous\", \"-infile\",\n                str(\"{}/staging/{}\".format(\n                    self.src_dir, staging_audio_basename)),\n                \"-time\", \"yes\", \"-logfn\", \"/dev/null\"])\n            try:\n                if self.get_verbosity():\n                    print(\"Now indexing {}\".format(staging_audio_basename))\n                output = subprocess.check_output([\n                    \"pocketsphinx_continuous\", \"-infile\",\n                    str(\"{}/staging/{}\".format(\n                        self.src_dir, staging_audio_basename)),\n                    \"-time\", \"yes\", \"-logfn\", \"/dev/null\"\n                ], universal_newlines=True).split('\\n')\n                str_timestamps_with_sil_conf = list(map(\n                    lambda x: x.split(\" \"), filter(None, output[1:])))\n                # Timestamps are putted in a list of a single element. To match\n                # Watson's output.\n                self.__timestamps_unregulated[\n                    original_audio_name + \".wav\"] = [(\n                        self._timestamp_extractor_cmu(\n                            staging_audio_basename,\n                            str_timestamps_with_sil_conf))]\n                if self.get_verbosity():\n                    print(\"Done indexing {}\".format(staging_audio_basename))\n            except OSError as e:\n                if self.get_verbosity():\n                    print(e, \"The command was: {}\".format(\n                        pocketsphinx_command))\n                self.__errors[(time(), staging_audio_basename)] = e\n        self._timestamp_regulator()\n\n        if self.get_verbosity():\n            print(\"Finished indexing procedure\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _timestamp_extractor_cmu(self, staging_audio_basename,\n                                 str_timestamps_with_sil_conf):\n        \"\"\"\n        Parameters\n        ----------\n        str_timestamps_with_sil_conf : [[str, str, str, str]]\n            Of the form [[word, starting_sec, ending_sec, confidence]]\n\n        Returns\n        -------\n        timestamps : [[str, float, float]]\n        \"\"\"\n        filter_untimed = filter(lambda x: len(x) == 4,\n                                str_timestamps_with_sil_conf)\n        if filter_untimed != str_timestamps_with_sil_conf:\n            self.__errors[\n                (time(), staging_audio_basename)\n            ] = str_timestamps_with_sil_conf\n        str_timestamps = [\n            str_timestamp[:-1]\n            for str_timestamp in filter_untimed\n            if not any([letter in {\"<\", \">\", \"/\"}\n                        for letter in ''.join(str_timestamp)])]\n        timestamps = list([\n            _WordBlock(\n                word=re.findall(\"^[^\\(]+\", x[0])[0],\n                start=round(float(x[1]), 2),\n                end=round(float(x[2]), 2)\n            ) for x in str_timestamps])\n        return timestamps", "response": "Extract the CMU timestamps from the given string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _index_audio_ibm(self, basename=None, replace_already_indexed=False,\n                         continuous=True, model=\"en-US_BroadbandModel\",\n                         word_confidence=True, word_alternatives_threshold=0.9,\n                         profanity_filter_for_US_results=False):\n        \"\"\"\n        Implements a search-suitable interface for Watson speech API.\n\n        Some explaination of the parameters here have been taken from [1]_\n\n        Parameters\n        ----------\n        basename : str, optional\n            A specific basename to be indexed and is placed in src_dir\n            e.g `audio.wav`.\n\n            If `None` is selected, all the valid audio files would be indexed.\n            Default is `None`.\n\n        replace_already_indexed : bool\n            `True`, To reindex some audio file that's already in the\n             timestamps.\n\n             Default is `False`.\n\n        continuous : bool\n            Indicates whether multiple final results that represent consecutive\n            phrases separated by long pauses are returned.\n            If true, such phrases are returned; if false (the default),\n            recognition ends after the first end-of-speech (EOS) incident is\n            detected.\n\n            Default is `True`.\n        model :  {\n                    'ar-AR_BroadbandModel',\n                    'en-UK_BroadbandModel'\n                    'en-UK_NarrowbandModel',\n                    'en-US_BroadbandModel', (the default)\n                    'en-US_NarrowbandModel',\n                    'es-ES_BroadbandModel',\n                    'es-ES_NarrowbandModel',\n                    'fr-FR_BroadbandModel',\n                    'ja-JP_BroadbandModel',\n                    'ja-JP_NarrowbandModel',\n                    'pt-BR_BroadbandModel',\n                    'pt-BR_NarrowbandModel',\n                    'zh-CN_BroadbandModel',\n                    'zh-CN_NarrowbandModel'\n                 }\n            The identifier of the model to be used for the recognition\n\n            Default is 'en-US_BroadbandModel'\n        word_confidence : bool\n            Indicates whether a confidence measure in the range of 0 to 1 is\n            returned for each word.\n\n            The default is True. (It's False in the original)\n        word_alternatives_threshold : numeric\n            A confidence value that is the lower bound for identifying a\n            hypothesis as a possible word alternative (also known as\n            \"Confusion Networks\"). An alternative word is considered if its\n            confidence is greater than or equal to the threshold. Specify a\n            probability between 0 and 1 inclusive.\n\n            Default is `0.9`.\n        profanity_filter_for_US_results : bool\n            Indicates whether profanity filtering is performed on the\n            transcript. If true, the service filters profanity from all output\n            by replacing inappropriate words with a series of asterisks.\n\n            If false, the service returns results with no censoring. Applies\n            to US English transcription only.\n\n            Default is `False`.\n\n        References\n        ----------\n        .. [1] : https://ibm.com/watson/developercloud/speech-to-text/api/v1/\n        \"\"\"\n        params = {'continuous': continuous,\n                  'model': model,\n                  'word_alternatives_threshold': word_alternatives_threshold,\n                  'word_confidence': word_confidence,\n                  'timestamps': True,\n                  'inactivity_timeout': str(-1),\n                  'profanity_filter': profanity_filter_for_US_results}\n\n        self._prepare_audio(basename=basename,\n                            replace_already_indexed=replace_already_indexed)\n\n        for staging_audio_basename in self._list_audio_files(\n                sub_dir=\"staging\"):\n            original_audio_name = ''.join(\n                staging_audio_basename.split('.')[:-1])[:-3]\n            with open(\"{}/staging/{}\".format(\n                    self.src_dir, staging_audio_basename), \"rb\") as f:\n                if self.get_verbosity():\n                    print(\"Uploading {}...\".format(staging_audio_basename))\n                response = requests.post(\n                    url=(\"https://stream.watsonplatform.net/\"\n                         \"speech-to-text/api/v1/recognize\"),\n                    auth=(self.get_username_ibm(), self.get_password_ibm()),\n                    headers={'content-type': 'audio/wav'},\n                    data=f.read(),\n                    params=params)\n                if self.get_verbosity():\n                    print(\"Indexing {}...\".format(staging_audio_basename))\n                self.__timestamps_unregulated[\n                    original_audio_name + \".wav\"].append(\n                        self._timestamp_extractor_ibm(\n                            staging_audio_basename, json.loads(response.text)))\n                if self.get_verbosity():\n                    print(\"Done indexing {}\".format(staging_audio_basename))\n        self._timestamp_regulator()\n\n        if self.get_verbosity():\n            print(\"Indexing procedure finished\")", "response": "This method is used to index the audio files in src_dir_wav."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the timestamps of the original audio file.", "response": "def _timestamp_extractor_ibm(self, staging_audio_basename, audio_json):\n        \"\"\"\n        Parameters\n        ----------\n        audio_json : {str: [{str: [{str: str or nuneric}]}]}\n            Refer to Watson Speech API refrence [1]_\n\n        Returns\n        -------\n        [[str, float, float]]\n            A list whose members are lists. Each member list has three\n            elements. First one is a word. Second is the starting second and\n            the third is the ending second of that word in the original\n            audio file.\n        \"\"\"\n        try:\n            timestamps_of_sentences = [\n                audio_json['results'][i]['alternatives'][0]['timestamps']\n                for i in range(len(audio_json['results']))]\n            return [\n                _WordBlock(\n                    word=word_block[0],\n                    start=round(float(word_block[1]), 2),\n                    end=round(float(word_block[2]), 2)\n                ) for sentence_block in timestamps_of_sentences\n                for word_block in sentence_block]\n        except KeyError:\n            self.__errors[(time(), staging_audio_basename)] = audio_json\n            if self.get_verbosity():\n                print(audio_json)\n                print(\"The resulting request from Watson was unintelligible.\")\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the correct indexer function based on the mode. If mode is `ibm`, _indexer_audio_ibm is called which is an interface for Watson. Note that some of the explaination of _indexer_audio_ibm's arguments is from [1]_ If mode is `cmu`, _indexer_audio_cmu is called which is an interface for PocketSphinx Beware that the output would not be sufficiently accurate. Use this only if you don't want to upload your files to IBM. Parameters ---------- mode : {\"ibm\", \"cmu\"} basename : str, optional A specific basename to be indexed and is placed in src_dir e.g `audio.wav`. If `None` is selected, all the valid audio files would be indexed. Default is `None`. replace_already_indexed : bool `True`, To reindex some audio file that's already in the timestamps. Default is `False`. continuous : bool Valid Only if mode is `ibm` Indicates whether multiple final results that represent consecutive phrases separated by long pauses are returned. If true, such phrases are returned; if false (the default), recognition ends after the first end-of-speech (EOS) incident is detected. Default is `True`. model : { 'ar-AR_BroadbandModel', 'en-UK_BroadbandModel' 'en-UK_NarrowbandModel', 'en-US_BroadbandModel', (the default) 'en-US_NarrowbandModel', 'es-ES_BroadbandModel', 'es-ES_NarrowbandModel', 'fr-FR_BroadbandModel', 'ja-JP_BroadbandModel', 'ja-JP_NarrowbandModel', 'pt-BR_BroadbandModel', 'pt-BR_NarrowbandModel', 'zh-CN_BroadbandModel', 'zh-CN_NarrowbandModel' } Valid Only if mode is `ibm` The identifier of the model to be used for the recognition Default is 'en-US_BroadbandModel' word_confidence : bool Valid Only if mode is `ibm` Indicates whether a confidence measure in the range of 0 to 1 is returned for each word. The default is True. (It's False in the original) word_alternatives_threshold : numeric Valid Only if mode is `ibm` A confidence value that is the lower bound for identifying a hypothesis as a possible word alternative (also known as \"Confusion Networks\"). An alternative word is considered if its confidence is greater than or equal to the threshold. Specify a probability between 0 and 1 inclusive. Default is `0.9`. profanity_filter_for_US_results : bool Valid Only if mode is `ibm` Indicates whether profanity filtering is performed on the transcript. If true, the service filters profanity from all output by replacing inappropriate words with a series of asterisks. If false, the service returns results with no censoring. Applies to US English transcription only. Default is `False`. Raises ------ OSError Valid only if mode is `cmu`. If the output of pocketsphinx command results in an error. References ---------- .. [1] : https://ibm.com/watson/developercloud/speech-to-text/api/v1/ Else if mode is `cmu`, then _index_audio_cmu would be called:", "response": "def index_audio(self, *args, **kwargs):\n        \"\"\"\n        Calls the correct indexer function based on the mode.\n\n        If mode is `ibm`, _indexer_audio_ibm is called which is an interface\n        for Watson. Note that some of the explaination of _indexer_audio_ibm's\n        arguments is from [1]_\n\n        If mode is `cmu`, _indexer_audio_cmu is called which is an interface\n        for PocketSphinx Beware that the output would not be sufficiently\n        accurate. Use this only if you don't want to upload your files to IBM.\n\n        Parameters\n        ----------\n        mode : {\"ibm\", \"cmu\"}\n\n        basename : str, optional\n\n            A specific basename to be indexed and is placed in src_dir\n            e.g `audio.wav`.\n\n            If `None` is selected, all the valid audio files would be indexed.\n            Default is `None`.\n\n        replace_already_indexed : bool\n\n            `True`, To reindex some audio file that's already in the\n            timestamps.\n\n            Default is `False`.\n\n        continuous : bool\n\n            Valid Only if mode is `ibm`\n\n            Indicates whether multiple final results that represent consecutive\n            phrases separated by long pauses are returned.\n            If true, such phrases are returned; if false (the default),\n            recognition ends after the first end-of-speech (EOS) incident is\n            detected.\n\n            Default is `True`.\n\n        model :  {\n                    'ar-AR_BroadbandModel',\n                    'en-UK_BroadbandModel'\n                    'en-UK_NarrowbandModel',\n                    'en-US_BroadbandModel', (the default)\n                    'en-US_NarrowbandModel',\n                    'es-ES_BroadbandModel',\n                    'es-ES_NarrowbandModel',\n                    'fr-FR_BroadbandModel',\n                    'ja-JP_BroadbandModel',\n                    'ja-JP_NarrowbandModel',\n                    'pt-BR_BroadbandModel',\n                    'pt-BR_NarrowbandModel',\n                    'zh-CN_BroadbandModel',\n                    'zh-CN_NarrowbandModel'\n                 }\n\n            Valid Only if mode is `ibm`\n\n            The identifier of the model to be used for the recognition\n\n            Default is 'en-US_BroadbandModel'\n\n        word_confidence : bool\n\n            Valid Only if mode is `ibm`\n\n            Indicates whether a confidence measure in the range of 0 to 1 is\n            returned for each word.\n\n            The default is True. (It's False in the original)\n\n        word_alternatives_threshold : numeric\n\n            Valid Only if mode is `ibm`\n\n            A confidence value that is the lower bound for identifying a\n            hypothesis as a possible word alternative (also known as\n            \"Confusion Networks\"). An alternative word is considered if its\n            confidence is greater than or equal to the threshold. Specify a\n            probability between 0 and 1 inclusive.\n\n            Default is `0.9`.\n\n        profanity_filter_for_US_results : bool\n\n            Valid Only if mode is `ibm`\n\n            Indicates whether profanity filtering is performed on the\n            transcript. If true, the service filters profanity from all output\n            by replacing inappropriate words with a series of asterisks.\n\n            If false, the service returns results with no censoring. Applies\n            to US English transcription only.\n\n            Default is `False`.\n\n        Raises\n        ------\n        OSError\n\n            Valid only if mode is `cmu`.\n\n            If the output of pocketsphinx command results in an error.\n\n        References\n        ----------\n        .. [1] : https://ibm.com/watson/developercloud/speech-to-text/api/v1/\n\n        Else if mode is `cmu`, then _index_audio_cmu would be called:\n        \"\"\"\n        with _Subdirectory_Managing_Decorator(\n                self.src_dir, self._needed_directories):\n            if self.get_mode() == \"ibm\":\n                self._index_audio_ibm(*args, **kwargs)\n            elif self.get_mode() == \"cmu\":\n                self._index_audio_cmu(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a dictionary whose keys are audio file basenames and whose values are a list of word blocks from unregulated timestamps and updates the main timestamp attribute. After all done, purges unregulated ones. In case the audio file was large enough to be splitted, it adds seconds to correct timing and in case the timestamp was manually loaded, it leaves it alone. Note that the difference between self.__timestamps and self.__timestamps_unregulated is that in the regulated version, right after the word, a list of word blocks must appear. However in the unregulated version, after a word, a list of individual splits containing word blocks would appear!", "response": "def _timestamp_regulator(self):\n        \"\"\"\n        Makes a dictionary whose keys are audio file basenames and whose\n        values are a list of word blocks from unregulated timestamps and\n        updates the main timestamp attribute. After all done, purges\n        unregulated ones.\n        In case the audio file was large enough to be splitted, it adds seconds\n        to correct timing and in case the timestamp was manually loaded, it\n        leaves it alone.\n\n        Note that the difference between self.__timestamps and\n        self.__timestamps_unregulated is that in the regulated version,\n        right after the word, a list of word blocks must appear. However in the\n        unregulated version, after a word, a list of individual splits\n        containing word blocks would appear!\n        \"\"\"\n        unified_timestamps = _PrettyDefaultDict(list)\n        staged_files = self._list_audio_files(sub_dir=\"staging\")\n        for timestamp_basename in self.__timestamps_unregulated:\n            if len(self.__timestamps_unregulated[timestamp_basename]) > 1:\n                # File has been splitted\n                timestamp_name = ''.join(timestamp_basename.split('.')[:-1])\n                staged_splitted_files_of_timestamp = list(\n                    filter(lambda staged_file: (\n                        timestamp_name == staged_file[:-3] and\n                        all([(x in set(map(str, range(10))))\n                             for x in staged_file[-3:]])), staged_files))\n                if len(staged_splitted_files_of_timestamp) == 0:\n                    self.__errors[(time(), timestamp_basename)] = {\n                        \"reason\": \"Missing staged file\",\n                        \"current_staged_files\": staged_files}\n                    continue\n                staged_splitted_files_of_timestamp.sort()\n                unified_timestamp = list()\n                for staging_digits, splitted_file in enumerate(\n                        self.__timestamps_unregulated[timestamp_basename]):\n                    prev_splits_sec = 0\n                    if int(staging_digits) != 0:\n                        prev_splits_sec = self._get_audio_duration_seconds(\n                            \"{}/staging/{}{:03d}\".format(\n                                self.src_dir, timestamp_name,\n                                staging_digits - 1))\n                    for word_block in splitted_file:\n                        unified_timestamp.append(\n                            _WordBlock(\n                                word=word_block.word,\n                                start=round(word_block.start +\n                                            prev_splits_sec, 2),\n                                end=round(word_block.end +\n                                          prev_splits_sec, 2)))\n                unified_timestamps[\n                    str(timestamp_basename)] += unified_timestamp\n            else:\n                unified_timestamps[\n                    timestamp_basename] += self.__timestamps_unregulated[\n                        timestamp_basename][0]\n\n        self.__timestamps.update(unified_timestamps)\n        self.__timestamps_unregulated = _PrettyDefaultDict(list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the corrected timestamps to a file.", "response": "def save_indexed_audio(self, indexed_audio_file_abs_path):\n        \"\"\"\n        Writes the corrected timestamps to a file. Timestamps are a python\n        dictionary.\n\n        Parameters\n        ----------\n        indexed_audio_file_abs_path : str\n        \"\"\"\n        with open(indexed_audio_file_abs_path, \"wb\") as f:\n            pickle.dump(self.get_timestamps(), f, pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_indexed_audio(self, indexed_audio_file_abs_path):\n        with open(indexed_audio_file_abs_path, \"rb\") as f:\n            self.__timestamps = pickle.load(f)", "response": "Loads the indexed audio file into the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_subsequence_of(self, sub, sup):\n        return bool(re.search(\".*\".join(sub), sup))", "response": "Returns True if the subsequence of the supsequence of the current set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _partial_search_validator(self, sub, sup, anagram=False,\n                                  subsequence=False, supersequence=False):\n        \"\"\"\n        It's responsible for validating the partial results of `search` method.\n        If it returns True, the search would return its result. Else, search\n        method would discard what it found and look for others.\n\n        First, checks to see if all elements of `sub` is in `sup` with at least\n        the same frequency and then checks to see if every element `sub`\n        appears in `sup` with the same order (index-wise).\n        If advanced control sturctures are specified, the containment condition\n        won't be checked.\n        The code for index checking is from [1]_.\n\n        Parameters\n        ----------\n        sub : list\n        sup : list\n        anagram : bool, optional\n            Default is `False`\n        subsequence : bool, optional\n            Default is `False`\n        supersequence : bool, optional\n            Default is `False`\n\n        Returns\n        -------\n        bool\n\n        References\n        ----------\n        .. [1] : `\n   https://stackoverflow.com/questions/35964155/checking-if-list-is-a-sublist`\n        \"\"\"\n        def get_all_in(one, another):\n            for element in one:\n                if element in another:\n                    yield element\n\n        def containment_check(sub, sup):\n            return (set(Counter(sub).keys()).issubset(\n                set(Counter(sup).keys())))\n\n        def containment_freq_check(sub, sup):\n            return (all([Counter(sub)[element] <= Counter(sup)[element]\n                         for element in Counter(sub)]))\n\n        def extra_freq_check(sub, sup, list_of_tups):\n            # Would be used for matching anagrams, subsequences etc.\n            return (len(list_of_tups) > 0 and\n                    all([Counter(sub)[tup[0]] <= Counter(sup)[tup[1]]\n                         for tup in list_of_tups]))\n\n        # Regarding containment checking while having extra conditions,\n        # there's no good way to map each anagram or subseuqnece etc. that was\n        # found to the query word, without making it more complicated than\n        # it already is, because a query word can be anagram/subsequence etc.\n        # to multiple words of the timestamps yet finding the one with the\n        # right index would be the problem.\n        # Therefore we just approximate the solution by just counting\n        # the elements.\n        if len(sub) > len(sup):\n            return False\n\n        for pred, func in set([(anagram, self._is_anagram_of),\n                               (subsequence, self._is_subsequence_of),\n                               (supersequence, self._is_supersequence_of)]):\n            if pred:\n                pred_seive = [(sub_key, sup_key)\n                              for sub_key in set(Counter(sub).keys())\n                              for sup_key in set(Counter(sup).keys())\n                              if func(sub_key, sup_key)]\n                if not extra_freq_check(sub, sup, pred_seive):\n                    return False\n\n        if (\n                not any([anagram, subsequence, supersequence]) and\n                (not containment_check(sub, sup) or\n                 not containment_freq_check(sub, sup))\n        ):\n                return False\n\n        for x1, x2 in zip(get_all_in(sup, sub), get_all_in(sub, sup)):\n            if x1 != x2:\n                return False\n\n        return True", "response": "This method is responsible for validating the partial results of a list of elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a generator that searches for the given query within the source directory.", "response": "def search_gen(self, query, audio_basename=None, case_sensitive=False,\n                   subsequence=False, supersequence=False, timing_error=0.0,\n                   anagram=False, missing_word_tolerance=0):\n        \"\"\"\n        A generator that searches for the `query` within the audiofiles of the\n        src_dir.\n\n        Parameters\n        ----------\n        query : str\n            A string that'll be searched. It'll be splitted on spaces and then\n            each word gets sequentially searched.\n        audio_basename : str, optional\n            Search only within the given audio_basename.\n\n            Default is `None`\n        case_sensitive : bool, optional\n            Default is `False`\n        subsequence : bool, optional\n            `True` if it's not needed for the exact word be detected and larger\n            strings that contain the given one are fine.\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        supersequence : bool, optional\n            `True` if it's not needed for the exact word be detected and\n            smaller strings that are contained within the given one are fine.\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        anagram : bool, optional\n            `True` if it's acceptable for a complete permutation of the word to\n            be found. e.g. \"abcde\" would be acceptable for \"edbac\".\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        timing_error : None or float, optional\n            Sometimes other words (almost always very small) would be detected\n            between the words of the `query`. This parameter defines the\n            timing difference/tolerance of the search.\n\n            Default is 0.0 i.e. No timing error is tolerated.\n        missing_word_tolerance : int, optional\n            The number of words that can be missed within the result.\n            For example, if the query is \"Some random text\" and the tolerance\n            value is `1`, then \"Some text\" would be a valid response.\n            Note that the first and last words cannot be missed. Also,\n            there'll be an error if the value is more than the number of\n            available words. For the example above, any value more than 1\n            would have given an error (since there's only one word i.e.\n            \"random\" that can be missed)\n\n            Default is 0.\n\n        Yields\n        ------\n        {\"File Name\": str, \"Query\": `query`, \"Result\": (float, float)}\n            The result of the search is returned as a tuple which is the value\n            of the \"Result\" key. The first element of the tuple is the\n            starting second of `query` and the last element is the ending\n            second of `query`\n\n        Raises\n        ------\n        AssertionError\n            If `missing_word_tolerance` value is more than the total number of\n            words in the query minus 2 (since the first and the last word\n            cannot be removed)\n        \"\"\"\n        def case_sensitivity_handler(case_sensitive=case_sensitive):\n\n            def get_query_words(query, case_sensitive=case_sensitive):\n                query_words = list(\n                    filter(None, ''.join(\n                        filter(lambda char: char in (ascii_letters + \" \"),\n                               list(query))).split(\" \")))\n                if case_sensitive:\n                    return query_words\n                return [q.lower() for q in query_words]\n\n            def get_timestamps(case_sensitive=case_sensitive):\n                timestamps = self.get_timestamps().copy()\n                if not case_sensitive:\n                    return {\n                        audio_basename: [\n                            _WordBlock(word=word_block.word.lower(),\n                                       start=word_block.start,\n                                       end=word_block.end)\n                            for word_block in timestamps[audio_basename]]\n                        for audio_basename in timestamps}\n                return timestamps\n\n            return locals()\n\n        query_words = case_sensitivity_handler()[\"get_query_words\"](query)\n        timestamps = case_sensitivity_handler()[\"get_timestamps\"]()\n\n        assert abs(missing_word_tolerance -\n                   (len(query_words) - 2)) >= 0, (\n            \"The number of words that can be missing must be less than \"\n            \"the total number of words within the query minus the first and \"\n            \"the last word.\"\n        )\n\n        for audio_filename in (\n                (lambda: (timestamps.keys() if audio_basename is None else\n                          [audio_basename]))()):\n            result = list()\n            missed_words_so_far = 0\n            query_cursor = 0\n            try:\n                for word_block in timestamps[audio_filename]:\n                    if (\n                            # When the query is identical\n                            (word_block.word == query_words[query_cursor]) or\n                            # When the query is a subsequence of what's\n                            # available\n                            (subsequence and\n                             self._is_subsequence_of(query_words[query_cursor],\n                                                     word_block.word)) or\n                            # When the query is a supersequence of what's\n                            # available\n                            (supersequence and self._is_supersequence_of(\n                                query_words[query_cursor], word_block.word)) or\n                            # When query is a permutation of what's available.\n                            (anagram and self._is_anagram_of(\n                                query_words[query_cursor], word_block.word))\n                    ):\n                        result.append(word_block)\n\n                        if timing_error is not None:\n                            try:\n                                if round(result[-1].start -\n                                         result[-2].end, 4) > timing_error:\n                                    result = list()\n                                    query_cursor = 0\n                            except IndexError:\n                                pass\n\n                        if self._partial_search_validator(\n                                query_words, [x.word for x in result],\n                                anagram=anagram,\n                                subsequence=subsequence,\n                                supersequence=supersequence):\n                            yield {\n                                \"File Name\": audio_filename,\n                                \"Query\": query,\n                                \"Result\": tuple([result[0].start,\n                                                 result[-1].end])}\n                            result = list()\n                            query_cursor = 0\n\n                        else:\n                            query_cursor += 1\n\n                    elif missed_words_so_far > missing_word_tolerance:\n                        result = list()\n                        query_cursor = 0\n\n                    elif (missing_word_tolerance > 0) and (len(result) > 0):\n                        result.append(word_block)\n                        missed_words_so_far += 1\n\n            except KeyError:\n                # This is needed for the case where no timestamp is present.\n                pass\n\n            except IndexError:\n                # This is needed when multiple timestamps are present, and\n                # advanced control structures like `missed_word_tolerance` are\n                # non-zero. In that case, it can search to the end of the first\n                # timestamp looking to complete its partial result and since\n                # there are no more `word_block`s left, it returns an error.\n                # `continue` should be used to reset the partial result and\n                # move to the next timestamp.\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_all(self, queries, audio_basename=None, case_sensitive=False,\n                   subsequence=False, supersequence=False, timing_error=0.0,\n                   anagram=False, missing_word_tolerance=0):\n        \"\"\"\n        Returns a dictionary of all results of all of the queries for all of\n        the audio files.\n        All the specified parameters work per query.\n\n        Parameters\n        ----------\n        queries : [str] or str\n            A list of the strings that'll be searched. If type of queries is\n            `str`, it'll be insterted into a list within the body of the\n            method.\n        audio_basename : str, optional\n            Search only within the given audio_basename.\n\n            Default is `None`.\n        case_sensitive : bool\n            Default is `False`\n        subsequence : bool, optional\n            `True` if it's not needed for the exact word be detected and larger\n            strings that contain the given one are fine.\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        supersequence : bool, optional\n            `True` if it's not needed for the exact word be detected and\n            smaller strings that are contained within the given one are fine.\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        anagram : bool, optional\n            `True` if it's acceptable for a complete permutation of the word to\n            be found. e.g. \"abcde\" would be acceptable for \"edbac\".\n\n            If the query is a sentences with multiple words, it'll be\n            considered for each word, not the whole sentence.\n\n            Default is `False`.\n        timing_error : None or float, optional\n            Sometimes other words (almost always very small) would be detected\n            between the words of the `query`. This parameter defines the\n            timing difference/tolerance of the search.\n\n            Default is 0.0 i.e. No timing error is tolerated.\n        missing_word_tolerance : int, optional\n            The number of words that can be missed within the result.\n            For example, if the query is \"Some random text\" and the tolerance\n            value is `1`, then \"Some text\" would be a valid response.\n            Note that the first and last words cannot be missed. Also,\n            there'll be an error if the value is more than the number of\n            available words. For the example above, any value more than 1\n            would have given an error (since there's only one word i.e.\n            \"random\" that can be missed)\n\n            Default is 0.\n\n        Returns\n        -------\n        search_results : {str: {str: [(float, float)]}}\n            A dictionary whose keys are queries and whose values are\n            dictionaries whose keys are all the audiofiles in which the query\n            is present and whose values are a list whose elements are 2-tuples\n            whose first element is the starting second of the query and whose\n            values are the ending second. e.g.\n            {\"apple\": {\"fruits.wav\" : [(1.1, 1.12)]}}\n\n        Raises\n        ------\n        TypeError\n            if `queries` is neither a list nor a str\n        \"\"\"\n\n        search_gen_rest_of_kwargs = {\n            \"audio_basename\": audio_basename,\n            \"case_sensitive\": case_sensitive,\n            \"subsequence\": subsequence,\n            \"supersequence\": supersequence,\n            \"timing_error\": timing_error,\n            \"anagram\": anagram,\n            \"missing_word_tolerance\": missing_word_tolerance}\n\n        if not isinstance(queries, (list, str)):\n            raise TypeError(\"Invalid query type.\")\n        if type(queries) is not list:\n            queries = [queries]\n        search_results = _PrettyDefaultDict(lambda: _PrettyDefaultDict(list))\n        for query in queries:\n            search_gen = self.search_gen(query=query,\n                                         **search_gen_rest_of_kwargs)\n            for search_result in search_gen:\n                search_results[query][\n                    search_result[\"File Name\"]].append(search_result[\"Result\"])\n        return search_results", "response": "This method searches all of the audio files for all of the specified words."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_regexp(self, pattern, audio_basename=None):\n\n        def indexes_in_transcript_to_start_end_second(index_tup,\n                                                      audio_basename):\n            \"\"\"\n            Calculates the word block index by having the beginning and ending\n            index of the matched result from the transcription\n\n            Parameters\n            ----------\n            index_tup : (int, tup)\n                index_tup is of the form tuple(index_start, index_end)\n            audio_basename : str\n\n            Retrun\n            ------\n            [float, float]\n                The time of the output of the matched result. Derived from two\n                separate word blocks belonging to the beginning and the end of\n                the index_start and index_end.\n            \"\"\"\n            space_indexes = [i for i, x in enumerate(\n                transcription[audio_basename]) if x == \" \"]\n            space_indexes.sort(reverse=True)\n            index_start, index_end = index_tup\n            # re.finditer returns the ending index by one more\n            index_end -= 1\n            while transcription[audio_basename][index_start] == \" \":\n                index_start += 1\n            while transcription[audio_basename][index_end] == \" \":\n                index_end -= 1\n            block_number_start = 0\n            block_number_end = len(space_indexes)\n            for block_cursor, space_index in enumerate(space_indexes):\n                if index_start > space_index:\n                    block_number_start = (len(space_indexes) - block_cursor)\n                    break\n            for block_cursor, space_index in enumerate(space_indexes):\n                if index_end > space_index:\n                    block_number_end = (len(space_indexes) - block_cursor)\n                    break\n            return (timestamps[audio_basename][block_number_start].start,\n                    timestamps[audio_basename][block_number_end].end)\n\n        timestamps = self.get_timestamps()\n        if audio_basename is not None:\n            timestamps = {audio_basename: timestamps[audio_basename]}\n        transcription = {\n            audio_basename: ' '.join(\n                [word_block.word for word_block in timestamps[audio_basename]]\n            ) for audio_basename in timestamps}\n        match_map = map(\n            lambda audio_basename: tuple((\n                audio_basename,\n                re.finditer(pattern, transcription[audio_basename]))),\n            transcription.keys())\n        search_results = _PrettyDefaultDict(lambda: _PrettyDefaultDict(list))\n        for audio_basename, match_iter in match_map:\n            for match in match_iter:\n                search_results[match.group()][audio_basename].append(\n                    tuple(indexes_in_transcript_to_start_end_second(\n                        match.span(), audio_basename)))\n        return search_results", "response": "Search the word_blocks of the given pattern and returns a dictionary of the word_blocks that match the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rwh_primes1(n):\n    # http://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188\n    ''' Returns  a list of primes < n '''\n    sieve = [True] * (n/2)\n    for i in xrange(3,int(n**0.5)+1,2):\n        if sieve[i/2]:\n            sieve[i*i/2::i] = [False] * ((n-i*i-1)/(2*i)+1)\n    return [2] + [2*i+1 for i in xrange(1,n/2) if sieve[i]]", "response": "Returns a list of primes < n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the list of prime polynomials for the given generator and galois field characteristic exponent.", "response": "def find_prime_polys(generator=2, c_exp=8, fast_primes=False, single=False):\n    '''Compute the list of prime polynomials for the given generator and galois field characteristic exponent.'''\n    # fast_primes will output less results but will be significantly faster.\n    # single will output the first prime polynomial found, so if all you want is to just find one prime polynomial to generate the LUT for Reed-Solomon to work, then just use that.\n\n    # A prime polynomial (necessarily irreducible) is necessary to reduce the multiplications in the Galois Field, so as to avoid overflows.\n    # Why do we need a \"prime polynomial\"? Can't we just reduce modulo 255 (for GF(2^8) for example)? Because we need the values to be unique.\n    # For example: if the generator (alpha) = 2 and c_exp = 8 (GF(2^8) == GF(256)), then the generated Galois Field (0, 1, \u03b1, \u03b1^1, \u03b1^2, ..., \u03b1^(p-1)) will be galois field it becomes 0, 1, 2, 4, 8, 16, etc. However, upon reaching 128, the next value will be doubled (ie, next power of 2), which will give 256. Then we must reduce, because we have overflowed above the maximum value of 255. But, if we modulo 255, this will generate 256 == 1. Then 2, 4, 8, 16, etc. giving us a repeating pattern of numbers. This is very bad, as it's then not anymore a bijection (ie, a non-zero value doesn't have a unique index). That's why we can't just modulo 255, but we need another number above 255, which is called the prime polynomial.\n    # Why so much hassle? Because we are using precomputed look-up tables for multiplication: instead of multiplying a*b, we precompute alpha^a, alpha^b and alpha^(a+b), so that we can just use our lookup table at alpha^(a+b) and get our result. But just like in our original field we had 0,1,2,...,p-1 distinct unique values, in our \"LUT\" field using alpha we must have unique distinct values (we don't care that they are different from the original field as long as they are unique and distinct). That's why we need to avoid duplicated values, and to avoid duplicated values we need to use a prime irreducible polynomial.\n\n    # Here is implemented a bruteforce approach to find all these prime polynomials, by generating every possible prime polynomials (ie, every integers between field_charac+1 and field_charac*2), and then we build the whole Galois Field, and we reject the candidate prime polynomial if it duplicates even one value or if it generates a value above field_charac (ie, cause an overflow).\n    # Note that this algorithm is slow if the field is too big (above 12), because it's an exhaustive search algorithm. There are probabilistic approaches, and almost surely prime approaches, but there is no determistic polynomial time algorithm to find irreducible monic polynomials. More info can be found at: http://people.mpi-inf.mpg.de/~csaha/lectures/lec9.pdf\n    # Another faster algorithm may be found at Adleman, Leonard M., and Hendrik W. Lenstra. \"Finding irreducible polynomials over finite fields.\" Proceedings of the eighteenth annual ACM symposium on Theory of computing. ACM, 1986.\n\n    # Prepare the finite field characteristic (2^p - 1), this also represent the maximum possible value in this field\n    root_charac = 2 # we're in GF(2)\n    field_charac = int(root_charac**c_exp - 1)\n    field_charac_next = int(root_charac**(c_exp+1) - 1)\n\n    prim_candidates = []\n    if fast_primes:\n        prim_candidates = rwh_primes1(field_charac_next) # generate maybe prime polynomials and check later if they really are irreducible\n        prim_candidates = [x for x in prim_candidates if x > field_charac] # filter out too small primes\n    else:\n        prim_candidates = xrange(field_charac+2, field_charac_next, root_charac) # try each possible prime polynomial, but skip even numbers (because divisible by 2 so necessarily not irreducible)\n\n    # Start of the main loop\n    correct_primes = []\n    for prim in prim_candidates: # try potential candidates primitive irreducible polys\n        seen = bytearray(field_charac+1) # memory variable to indicate if a value was already generated in the field (value at index x is set to 1) or not (set to 0 by default)\n        conflict = False # flag to know if there was at least one conflict\n\n        # Second loop, build the whole Galois Field\n        x = 1\n        for i in xrange(field_charac):\n            # Compute the next value in the field (ie, the next power of alpha/generator)\n            x = gf_mult_noLUT(x, generator, prim, field_charac+1)\n\n            # Rejection criterion: if the value overflowed (above field_charac) or is a duplicate of a previously generated power of alpha, then we reject this polynomial (not prime)\n            if x > field_charac or seen[x] == 1:\n                conflict = True\n                break\n            # Else we flag this value as seen (to maybe detect future duplicates), and we continue onto the next power of alpha\n            else:\n                seen[x] = 1\n\n        # End of the second loop: if there's no conflict (no overflow nor duplicated value), this is a prime polynomial!\n        if not conflict: \n            correct_primes.append(prim)\n            if single: return prim\n\n    # Return the list of all prime polynomials\n    return correct_primes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gf_poly_mul(p, q):\n    '''Multiply two polynomials, inside Galois Field (but the procedure is generic). Optimized function by precomputation of log.'''\n    # Pre-allocate the result array\n    r = bytearray(len(p) + len(q) - 1)\n    # Precompute the logarithm of p\n    lp = [gf_log[p[i]] for i in xrange(len(p))]\n    # Compute the polynomial multiplication (just like the outer product of two vectors, we multiply each coefficients of p with all coefficients of q)\n    for j in xrange(len(q)):\n        qj = q[j] # optimization: load the coefficient once\n        if qj != 0: # log(0) is undefined, we need to check that\n            lq = gf_log[qj] # Optimization: precache the logarithm of the current coefficient of q\n            for i in xrange(len(p)):\n                if p[i] != 0: # log(0) is undefined, need to check that...\n                    r[i + j] ^= gf_exp[lp[i] + lq] # equivalent to: r[i + j] = gf_add(r[i+j], gf_mul(p[i], q[j]))\n    return r", "response": "Multiply two polynomials inside Galois Field. Optimized function by precomputation of log."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmultiply two polynomials inside Galois Field", "response": "def gf_poly_mul_simple(p, q): # simple equivalent way of multiplying two polynomials without precomputation, but thus it's slower\n    '''Multiply two polynomials, inside Galois Field'''\n    # Pre-allocate the result array\n    r = bytearray(len(p) + len(q) - 1)\n    # Compute the polynomial multiplication (just like the outer product of two vectors, we multiply each coefficients of p with all coefficients of q)\n    for j in xrange(len(q)):\n        for i in xrange(len(p)):\n            r[i + j] ^= gf_mul(p[i], q[j]) # equivalent to: r[i + j] = gf_add(r[i+j], gf_mul(p[i], q[j])) -- you can see it's your usual polynomial multiplication\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfast polynomial division by using Extended Synthetic Division and optimized for GF ( 2^p computations.", "response": "def gf_poly_div(dividend, divisor):\n    '''Fast polynomial division by using Extended Synthetic Division and optimized for GF(2^p) computations (doesn't work with standard polynomials outside of this galois field).'''\n    # CAUTION: this function expects polynomials to follow the opposite convention at decoding: the terms must go from the biggest to lowest degree (while most other functions here expect a list from lowest to biggest degree). eg: 1 + 2x + 5x^2 = [5, 2, 1], NOT [1, 2, 5]\n\n    msg_out = bytearray(dividend) # Copy the dividend list and pad with 0 where the ecc bytes will be computed\n    #normalizer = divisor[0] # precomputing for performance\n    for i in xrange(len(dividend) - (len(divisor)-1)):\n        #msg_out[i] /= normalizer # for general polynomial division (when polynomials are non-monic), the usual way of using synthetic division is to divide the divisor g(x) with its leading coefficient (call it a). In this implementation, this means:we need to compute: coef = msg_out[i] / gen[0]. For more infos, see http://en.wikipedia.org/wiki/Synthetic_division\n        coef = msg_out[i] # precaching\n        if coef != 0: # log(0) is undefined, so we need to avoid that case explicitly (and it's also a good optimization). In fact if you remove it, it should still work because gf_mul() will take care of the condition. But it's still a good practice to put the condition here.\n            for j in xrange(1, len(divisor)): # in synthetic division, we always skip the first coefficient of the divisior, because it's only used to normalize the dividend coefficient\n                if divisor[j] != 0: # log(0) is undefined\n                    msg_out[i + j] ^= gf_mul(divisor[j], coef) # equivalent to the more mathematically correct (but xoring directly is faster): msg_out[i + j] += -divisor[j] * coef\n\n    # The resulting msg_out contains both the quotient and the remainder, the remainder being the size of the divisor (the remainder has necessarily the same degree as the divisor -- not length but degree == length-1 -- since it's what we couldn't divide from the dividend), so we compute the index where this separation is, and return the quotient and remainder.\n    separator = -(len(divisor)-1)\n    return msg_out[:separator], msg_out[separator:]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gf_poly_square(poly):\n    '''Linear time implementation of polynomial squaring. For details, see paper: \"A fast software implementation for arithmetic operations in GF (2n)\". De Win, E., Bosselaers, A., Vandenberghe, S., De Gersem, P., & Vandewalle, J. (1996, January). In Advances in Cryptology - Asiacrypt'96 (pp. 65-76). Springer Berlin Heidelberg.'''\n    length = len(poly)\n    out = bytearray(2*length - 1)\n    for i in xrange(length-1):\n        p = poly[i]\n        k = 2*i\n        if p != 0:\n            #out[k] = gf_exp[(2*gf_log[p]) % field_charac] # not necessary to modulo (2^r)-1 since gf_exp is duplicated up to 510.\n            out[k] = gf_exp[2*gf_log[p]]\n        #else: # not necessary since the output is already initialized to an array of 0\n            #out[k] = 0\n    out[2*length-2] = gf_exp[2*gf_log[poly[length-1]]]\n    if out[0] == 0: out[0] = 2*poly[1] - 1\n    return out", "response": "Linear time implementation of polynomial squaring."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate a polynomial in GF ( 2^p given the value for x. This is based on Horner s scheme for maximum efficiency.", "response": "def gf_poly_eval(poly, x):\n    '''Evaluates a polynomial in GF(2^p) given the value for x. This is based on Horner's scheme for maximum efficiency.'''\n    y = poly[0]\n    for i in xrange(1, len(poly)):\n        y = gf_mul(y, x) ^ poly[i]\n    return y"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates an irreducible generator polynomial", "response": "def rs_generator_poly(nsym, fcr=0, generator=2):\n    '''Generate an irreducible generator polynomial (necessary to encode a message into Reed-Solomon)'''\n    g = bytearray([1])\n    for i in xrange(nsym):\n        g = gf_poly_mul(g, [1, gf_pow(generator, i+fcr)])\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates all irreducible generator polynomials up to max_nsym.", "response": "def rs_generator_poly_all(max_nsym, fcr=0, generator=2):\n    '''Generate all irreducible generator polynomials up to max_nsym (usually you can use n, the length of the message+ecc). Very useful to reduce processing time if you want to encode using variable schemes and nsym rates.'''\n    g_all = {}\n    g_all[0] = g_all[1] = [1]\n    for nsym in xrange(max_nsym):\n        g_all[nsym] = rs_generator_poly(nsym, fcr, generator)\n    return g_all"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the syndromes polynomial for a given codeword msg.", "response": "def rs_calc_syndromes(msg, nsym, fcr=0, generator=2):\n    '''Given the received codeword msg and the number of error correcting symbols (nsym), computes the syndromes polynomial.\n    Mathematically, it's essentially equivalent to a Fourrier Transform (Chien search being the inverse).\n    '''\n    # Note the \"[0] +\" : we add a 0 coefficient for the lowest degree (the constant). This effectively shifts the syndrome, and will shift every computations depending on the syndromes (such as the errors locator polynomial, errors evaluator polynomial, etc. but not the errors positions).\n    # This is not necessary as anyway syndromes are defined such as there are only non-zero coefficients (the only 0 is the shift of the constant here) and subsequent computations will/must account for the shift by skipping the first iteration (eg, the often seen range(1, n-k+1)), but you can also avoid prepending the 0 coeff and adapt every subsequent computations to start from 0 instead of 1.\n    return [0] + [gf_poly_eval(msg, gf_pow(generator, i+fcr)) for i in xrange(nsym)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rs_correct_errata(msg_in, synd, err_pos, fcr=0, generator=2): # err_pos is a list of the positions of the errors/erasures/errata\n    '''Forney algorithm, computes the values (error magnitude) to correct the input message.'''\n    global field_charac\n    msg = bytearray(msg_in)\n    # calculate errata locator polynomial to correct both errors and erasures (by combining the errors positions given by the error locator polynomial found by BM with the erasures positions given by caller)\n    coef_pos = [len(msg) - 1 - p for p in err_pos] # need to convert the positions to coefficients degrees for the errata locator algo to work (eg: instead of [0, 1, 2] it will become [len(msg)-1, len(msg)-2, len(msg) -3])\n    err_loc = rs_find_errata_locator(coef_pos, generator)\n    # calculate errata evaluator polynomial (often called Omega or Gamma in academic papers)\n    err_eval = rs_find_error_evaluator(synd[::-1], err_loc, len(err_loc)-1)[::-1]\n\n    # Second part of Chien search to get the error location polynomial X from the error positions in err_pos (the roots of the error locator polynomial, ie, where it evaluates to 0)\n    X = [] # will store the position of the errors\n    for i in xrange(len(coef_pos)):\n        l = field_charac - coef_pos[i]\n        X.append( gf_pow(generator, -l) )\n\n    # Forney algorithm: compute the magnitudes\n    E = bytearray(len(msg)) # will store the values that need to be corrected (substracted) to the message containing errors. This is sometimes called the error magnitude polynomial.\n    Xlength = len(X)\n    for i, Xi in enumerate(X):\n\n        Xi_inv = gf_inverse(Xi)\n\n        # Compute the formal derivative of the error locator polynomial (see Blahut, Algebraic codes for data transmission, pp 196-197).\n        # the formal derivative of the errata locator is used as the denominator of the Forney Algorithm, which simply says that the ith error value is given by error_evaluator(gf_inverse(Xi)) / error_locator_derivative(gf_inverse(Xi)). See Blahut, Algebraic codes for data transmission, pp 196-197.\n        err_loc_prime_tmp = []\n        for j in xrange(Xlength):\n            if j != i:\n                err_loc_prime_tmp.append( gf_sub(1, gf_mul(Xi_inv, X[j])) )\n        # compute the product, which is the denominator of the Forney algorithm (errata locator derivative)\n        err_loc_prime = 1\n        for coef in err_loc_prime_tmp:\n            err_loc_prime = gf_mul(err_loc_prime, coef)\n        # equivalent to: err_loc_prime = functools.reduce(gf_mul, err_loc_prime_tmp, 1)\n\n        # Compute y (evaluation of the errata evaluator polynomial)\n        # This is a more faithful translation of the theoretical equation contrary to the old forney method. Here it is exactly copy/pasted from the included presentation decoding_rs.pdf: Yl = omega(Xl.inverse()) / prod(1 - Xj*Xl.inverse()) for j in len(X) (in the paper it's for j in s, but it's useless when len(X) < s because we compute neutral terms 1 for nothing, and wrong when correcting more than s erasures or erasures+errors since it prevents computing all required terms).\n        # Thus here this method works with erasures too because firstly we fixed the equation to be like the theoretical one (don't know why it was modified in _old_forney(), if it's an optimization, it doesn't enhance anything), and secondly because we removed the product bound on s, which prevented computing errors and erasures above the s=(n-k)//2 bound.\n        y = gf_poly_eval(err_eval[::-1], Xi_inv) # numerator of the Forney algorithm (errata evaluator evaluated)\n        y = gf_mul(gf_pow(Xi, 1-fcr), y) # adjust to fcr parameter\n        \n        # Compute the magnitude\n        magnitude = gf_div(y, err_loc_prime) # magnitude value of the error, calculated by the Forney algorithm (an equation in fact): dividing the errata evaluator with the errata locator derivative gives us the errata magnitude (ie, value to repair) the ith symbol\n        E[err_pos[i]] = magnitude # store the magnitude for this error into the magnitude polynomial\n\n    # Apply the correction of values to get our message corrected! (note that the ecc bytes also gets corrected!)\n    # (this isn't the Forney algorithm, we just apply the result of decoding here)\n    msg = gf_poly_add(msg, E) # equivalent to Ci = Ri - Ei where Ci is the correct message, Ri the received (senseword) message, and Ei the errata magnitudes (minus is replaced by XOR since it's equivalent in GF(2^p)). So in fact here we substract from the received message the errors magnitude, which logically corrects the value to what it should be.\n    return msg", "response": "Forney algorithm computes the values to correct the input message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rs_find_error_locator(synd, nsym, erase_loc=None, erase_count=0):\n    '''Find error/errata locator and evaluator polynomials with Berlekamp-Massey algorithm'''\n    # The idea is that BM will iteratively estimate the error locator polynomial.\n    # To do this, it will compute a Discrepancy term called Delta, which will tell us if the error locator polynomial needs an update or not\n    # (hence why it's called discrepancy: it tells us when we are getting off board from the correct value).\n\n    # Init the polynomials\n    if erase_loc: # if the erasure locator polynomial is supplied, we init with its value, so that we include erasures in the final locator polynomial\n        err_loc = bytearray(erase_loc)\n        old_loc = bytearray(erase_loc)\n    else:\n        err_loc = bytearray([1]) # This is the main variable we want to fill, also called Sigma in other notations or more formally the errors/errata locator polynomial.\n        old_loc = bytearray([1]) # BM is an iterative algorithm, and we need the errata locator polynomial of the previous iteration in order to update other necessary variables.\n    #L = 0 # update flag variable, not needed here because we use an alternative equivalent way of checking if update is needed (but using the flag could potentially be faster depending on if using length(list) is taking linear time in your language, here in Python it's constant so it's as fast.\n\n    # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients.\n    # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome, which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors.\n    synd_shift = 0\n    if len(synd) > nsym: synd_shift = len(synd) - nsym\n\n    for i in xrange(nsym-erase_count): # generally: nsym-erase_count == len(synd), except when you input a partial erase_loc and using the full syndrome instead of the Forney syndrome, in which case nsym-erase_count is more correct (len(synd) will fail badly with IndexError).\n        if erase_loc: # if an erasures locator polynomial was provided to init the errors locator polynomial, then we must skip the FIRST erase_count iterations (not the last iterations, this is very important!)\n            K = erase_count+i+synd_shift\n        else: # if erasures locator is not provided, then either there's no erasures to account or we use the Forney syndromes, so we don't need to use erase_count nor erase_loc (the erasures have been trimmed out of the Forney syndromes).\n            K = i+synd_shift\n\n        # Compute the discrepancy Delta\n        # Here is the close-to-the-books operation to compute the discrepancy Delta: it's a simple polynomial multiplication of error locator with the syndromes, and then we get the Kth element.\n        #delta = gf_poly_mul(err_loc[::-1], synd)[K] # theoretically it should be gf_poly_add(synd[::-1], [1])[::-1] instead of just synd, but it seems it's not absolutely necessary to correctly decode.\n        # But this can be optimized: since we only need the Kth element, we don't need to compute the polynomial multiplication for any other element but the Kth. Thus to optimize, we compute the polymul only at the item we need, skipping the rest (avoiding a nested loop, thus we are linear time instead of quadratic).\n        # This optimization is actually described in several figures of the book \"Algebraic codes for data transmission\", Blahut, Richard E., 2003, Cambridge university press.\n        delta = synd[K]\n        for j in xrange(1, len(err_loc)):\n            delta ^= gf_mul(err_loc[-(j+1)], synd[K - j]) # delta is also called discrepancy. Here we do a partial polynomial multiplication (ie, we compute the polynomial multiplication only for the term of degree K). Should be equivalent to brownanrs.polynomial.mul_at().\n        #print \"delta\", K, delta, list(gf_poly_mul(err_loc[::-1], synd)) # debugline\n\n        # Shift polynomials to compute the next degree\n        old_loc = old_loc + bytearray([0])\n\n        # Iteratively estimate the errata locator and evaluator polynomials\n        if delta != 0: # Update only if there's a discrepancy\n            if len(old_loc) > len(err_loc): # Rule B (rule A is implicitly defined because rule A just says that we skip any modification for this iteration)\n            #if 2*L <= K+erase_count: # equivalent to len(old_loc) > len(err_loc), as long as L is correctly computed\n                # Computing errata locator polynomial Sigma\n                new_loc = gf_poly_scale(old_loc, delta)\n                old_loc = gf_poly_scale(err_loc, gf_inverse(delta)) # effectively we are doing err_loc * 1/delta = err_loc // delta\n                err_loc = new_loc\n                # Update the update flag\n                #L = K - L # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erase_count` (and indeed in a previous draft of this function, if you forgot to do `- erase_count` it would lead to correcting only 2*(errors+erasures) <= (n-k) instead of 2*errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erase_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures.\n\n            # Update with the discrepancy\n            err_loc = gf_poly_add(err_loc, gf_poly_scale(old_loc, delta))\n\n    # Check if the result is correct, that there's not too many errors to correct\n    err_loc = list(itertools.dropwhile(lambda x: x == 0, err_loc)) # drop leading 0s, else errs will not be of the correct size\n    errs = len(err_loc) - 1\n    if (errs-erase_count) * 2 + erase_count > nsym:\n        raise ReedSolomonError(\"Too many errors to correct\")\n\n    return err_loc", "response": "Find error and errata locator polynomials for a syndrometric symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the error locator polynomial from the erasures and errors positions.", "response": "def rs_find_errata_locator(e_pos, generator=2):\n    '''Compute the erasures/errors/errata locator polynomial from the erasures/errors/errata positions (the positions must be relative to the x coefficient, eg: \"hello worldxxxxxxxxx\" is tampered to \"h_ll_ worldxxxxxxxxx\" with xxxxxxxxx being the ecc of length n-k=9, here the string positions are [1, 4], but the coefficients are reversed since the ecc characters are placed as the first coefficients of the polynomial, thus the coefficients of the erased characters are n-1 - [1, 4] = [18, 15] = erasures_loc to be specified as an argument.'''\n    # See: http://ocw.usu.edu/Electrical_and_Computer_Engineering/Error_Control_Coding/lecture7.pdf and Blahut, Richard E. \"Transform techniques for error control codes.\" IBM Journal of Research and development 23.3 (1979): 299-315. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.600&rep=rep1&type=pdf and also a MatLab implementation here: http://www.mathworks.com/matlabcentral/fileexchange/23567-reed-solomon-errors-and-erasures-decoder/content//RS_E_E_DEC.m\n    e_loc = [1] # just to init because we will multiply, so it must be 1 so that the multiplication starts correctly without nulling any term\n    # erasures_loc is very simple to compute: erasures_loc = prod(1 - x*alpha**i) for i in erasures_pos and where alpha is the alpha chosen to evaluate polynomials (here in this library it's gf(3)). To generate c*x where c is a constant, we simply generate a Polynomial([c, 0]) where 0 is the constant and c is positionned to be the coefficient for x^1.\n    for i in e_pos:\n        e_loc = gf_poly_mul( e_loc, gf_poly_add([1], [gf_pow(generator, i), 0]) )\n    return e_loc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the error evaluator polynomial Omega from the syndrome and the error locator polynomial.", "response": "def rs_find_error_evaluator(synd, err_loc, nsym):\n    '''Compute the error (or erasures if you supply sigma=erasures locator polynomial, or errata) evaluator polynomial Omega from the syndrome and the error/erasures/errata locator Sigma. Omega is already computed at the same time as Sigma inside the Berlekamp-Massey implemented above, but in case you modify Sigma, you can recompute Omega afterwards using this method, or just ensure that Omega computed by BM is correct given Sigma.'''\n    # Omega(x) = [ Synd(x) * Error_loc(x) ] mod x^(n-k+1)\n    _, remainder = gf_poly_div( gf_poly_mul(synd, err_loc), ([1] + [0]*(nsym+1)) ) # first multiply syndromes * errata_locator, then do a polynomial division to truncate the polynomial to the required length\n\n    # Faster way that is equivalent\n    #remainder = gf_poly_mul(synd, err_loc) # first multiply the syndromes with the errata locator polynomial\n    #remainder = remainder[len(remainder)-(nsym+1):] # then divide by a polynomial of the length we want, which is equivalent to slicing the list (which represents the polynomial)\n\n    return remainder"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rs_find_errors(err_loc, nmess, generator=2):\n    '''Find the roots (ie, where evaluation = zero) of error polynomial by bruteforce trial, this is a sort of Chien's search (but less efficient, Chien's search is a way to evaluate the polynomial such that each evaluation only takes constant time).'''\n    # nmess = length of whole codeword (message + ecc symbols)\n    errs = len(err_loc) - 1\n    err_pos = []\n    for i in xrange(nmess): # normally we should try all 2^8 possible values, but here we optimize to just check the interesting symbols\n        if gf_poly_eval(err_loc, gf_pow(generator, i)) == 0: # It's a 0? Bingo, it's a root of the error locator polynomial, in other terms this is the location of an error\n            err_pos.append(nmess - 1 - i)\n    # Sanity check: the number of errors/errata positions found should be exactly the same as the length of the errata locator polynomial\n    if len(err_pos) != errs:\n        # TODO: to decode messages+ecc with length n > 255, we may try to use a bruteforce approach: the correct positions ARE in the final array j, but the problem is because we are above the Galois Field's range, there is a wraparound so that for example if j should be [0, 1, 2, 3], we will also get [255, 256, 257, 258] (because 258 % 255 == 3, same for the other values), so we can't discriminate. The issue is that fixing any errs_nb errors among those will always give a correct output message (in the sense that the syndrome will be all 0), so we may not even be able to check if that's correct or not, so I'm not sure the bruteforce approach may even be possible.\n        raise ReedSolomonError(\"Too many (or few) errors found by Chien Search for the errata locator polynomial!\")\n    return err_pos", "response": "Find the roots of the error locator polynomial by Chien s search."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncorrects a message using the Chien Search algorithm.", "response": "def rs_correct_msg(msg_in, nsym, fcr=0, generator=2, erase_pos=None, only_erasures=False):\n    '''Reed-Solomon main decoding function'''\n    global field_charac\n    if len(msg_in) > field_charac:\n        # Note that it is in fact possible to encode/decode messages that are longer than field_charac, but because this will be above the field, this will generate more error positions during Chien Search than it should, because this will generate duplicate values, which should normally be prevented thank's to the prime polynomial reduction (eg, because it can't discriminate between error at position 1 or 256, both being exactly equal under galois field 2^8). So it's really not advised to do it, but it's possible (but then you're not guaranted to be able to correct any error/erasure on symbols with a position above the length of field_charac -- if you really need a bigger message without chunking, then you should better enlarge c_exp so that you get a bigger field).\n        raise ValueError(\"Message is too long (%i when max is %i)\" % (len(msg_in), field_charac))\n\n    msg_out = bytearray(msg_in)     # copy of message\n    # erasures: set them to null bytes for easier decoding (but this is not necessary, they will be corrected anyway, but debugging will be easier with null bytes because the error locator polynomial values will only depend on the errors locations, not their values)\n    if erase_pos is None:\n        erase_pos = []\n    else:\n        for e_pos in erase_pos:\n            msg_out[e_pos] = 0\n    # check if there are too many erasures to correct (beyond the Singleton bound)\n    if len(erase_pos) > nsym: raise ReedSolomonError(\"Too many erasures to correct\")\n    # prepare the syndrome polynomial using only errors (ie: errors = characters that were either replaced by null byte or changed to another character, but we don't know their positions)\n    synd = rs_calc_syndromes(msg_out, nsym, fcr, generator)\n    # check if there's any error/erasure in the input codeword. If not (all syndromes coefficients are 0), then just return the codeword as-is.\n    if max(synd) == 0:\n        return msg_out[:-nsym], msg_out[-nsym:]  # no errors\n    \n    # Find errors locations\n    if only_erasures:\n        err_pos = []\n    else:\n        # compute the Forney syndromes, which hide the erasures from the original syndrome (so that BM will just have to deal with errors, not erasures)\n        fsynd = rs_forney_syndromes(synd, erase_pos, len(msg_out), generator)\n        # compute the error locator polynomial using Berlekamp-Massey\n        err_loc = rs_find_error_locator(fsynd, nsym, erase_count=len(erase_pos))\n        # locate the message errors using Chien search (or bruteforce search)\n        err_pos = rs_find_errors(err_loc[::-1], len(msg_out), generator)\n        if err_pos is None:\n            raise ReedSolomonError(\"Could not locate error\")\n\n    # Find errors values and apply them to correct the message\n    # compute errata evaluator and errata magnitude polynomials, then correct errors and erasures\n    msg_out = rs_correct_errata(msg_out, synd, (erase_pos + err_pos), fcr, generator) # note that we here use the original syndrome, not the forney syndrome (because we will correct both errors and erasures, so we need the full syndrome)\n    # check if the final message is fully repaired\n    synd = rs_calc_syndromes(msg_out, nsym, fcr, generator)\n    if max(synd) > 0:\n        raise ReedSolomonError(\"Could not correct message\")\n    # return the successfully decoded message\n    return msg_out[:-nsym], msg_out[-nsym:]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rs_correct_msg_nofsynd(msg_in, nsym, fcr=0, generator=2, erase_pos=None, only_erasures=False):\n    '''Reed-Solomon main decoding function, without using the modified Forney syndromes'''\n    global field_charac\n    if len(msg_in) > field_charac:\n        raise ValueError(\"Message is too long (%i when max is %i)\" % (len(msg_in), field_charac))\n\n    msg_out = bytearray(msg_in)     # copy of message\n    # erasures: set them to null bytes for easier decoding (but this is not necessary, they will be corrected anyway, but debugging will be easier with null bytes because the error locator polynomial values will only depend on the errors locations, not their values)\n    if erase_pos is None:\n        erase_pos = []\n    else:\n        for e_pos in erase_pos:\n            msg_out[e_pos] = 0\n    # check if there are too many erasures\n    if len(erase_pos) > nsym: raise ReedSolomonError(\"Too many erasures to correct\")\n    # prepare the syndrome polynomial using only errors (ie: errors = characters that were either replaced by null byte or changed to another character, but we don't know their positions)\n    synd = rs_calc_syndromes(msg_out, nsym, fcr, generator)\n    # check if there's any error/erasure in the input codeword. If not (all syndromes coefficients are 0), then just return the codeword as-is.\n    if max(synd) == 0:\n        return msg_out[:-nsym], msg_out[-nsym:]  # no errors\n\n    # prepare erasures locator and evaluator polynomials\n    erase_loc = None\n    #erase_eval = None\n    erase_count = 0\n    if erase_pos:\n        erase_count = len(erase_pos)\n        erase_pos_reversed = [len(msg_out)-1-eras for eras in erase_pos]\n        erase_loc = rs_find_errata_locator(erase_pos_reversed, generator=generator)\n        #erase_eval = rs_find_error_evaluator(synd[::-1], erase_loc, len(erase_loc)-1)\n\n    # prepare errors/errata locator polynomial\n    if only_erasures:\n        err_loc = erase_loc[::-1]\n        #err_eval = erase_eval[::-1]\n    else:\n        err_loc = rs_find_error_locator(synd, nsym, erase_loc=erase_loc, erase_count=erase_count)\n        err_loc = err_loc[::-1]\n        #err_eval = rs_find_error_evaluator(synd[::-1], err_loc[::-1], len(err_loc)-1)[::-1] # find error/errata evaluator polynomial (not really necessary since we already compute it at the same time as the error locator poly in BM)\n\n    # locate the message errors\n    err_pos = rs_find_errors(err_loc, len(msg_out), generator) # find the roots of the errata locator polynomial (ie: the positions of the errors/errata)\n    if err_pos is None:\n        raise ReedSolomonError(\"Could not locate error\")\n\n    # compute errata evaluator and errata magnitude polynomials, then correct errors and erasures\n    msg_out = rs_correct_errata(msg_out, synd, err_pos, fcr=fcr, generator=generator)\n    # check if the final message is fully repaired\n    synd = rs_calc_syndromes(msg_out, nsym, fcr, generator)\n    if max(synd) > 0:\n        raise ReedSolomonError(\"Could not correct message\")\n    # return the successfully decoded message\n    return msg_out[:-nsym], msg_out[-nsym:]", "response": "This function is used to correct the message using the modified Forney syndromes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rs_check(msg, nsym, fcr=0, generator=2):\n    '''Returns true if the message + ecc has no error of false otherwise (may not always catch a wrong decoding or a wrong message, particularly if there are too many errors -- above the Singleton bound --, but it usually does)'''\n    return ( max(rs_calc_syndromes(msg, nsym, fcr, generator)) == 0 )", "response": "Returns true if the message + ecc has no error of false otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self, data):\n        '''Encode a message (ie, add the ecc symbols) using Reed-Solomon, whatever the length of the message because we use chunking'''\n        if isinstance(data, str):\n            data = bytearray(data, \"latin-1\")\n        chunk_size = self.nsize - self.nsym\n        enc = bytearray()\n        for i in xrange(0, len(data), chunk_size):\n            chunk = data[i:i+chunk_size]\n            enc.extend(rs_encode_msg(chunk, self.nsym, fcr=self.fcr, generator=self.generator))\n        return enc", "response": "Encode a message using Reed - Solomon"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode(self, data, erase_pos=None, only_erasures=False):\n        '''Repair a message, whatever its size is, by using chunking'''\n        # erase_pos is a list of positions where you know (or greatly suspect at least) there is an erasure (ie, wrong character but you know it's at this position). Just input the list of all positions you know there are errors, and this method will automatically split the erasures positions to attach to the corresponding data chunk.\n        if isinstance(data, str):\n            data = bytearray(data, \"latin-1\")\n        dec = bytearray()\n        for i in xrange(0, len(data), self.nsize):\n            # Split the long message in a chunk\n            chunk = data[i:i+self.nsize]\n            # Extract the erasures for this chunk\n            e_pos = []\n            if erase_pos:\n                # First extract the erasures for this chunk (all erasures below the maximum chunk length)\n                e_pos = [x for x in erase_pos if x <= self.nsize]\n                # Then remove the extract erasures from the big list and also decrement all subsequent positions values by nsize (the current chunk's size) so as to prepare the correct alignment for the next iteration\n                erase_pos = [x - (self.nsize+1) for x in erase_pos if x > self.nsize]\n            # Decode/repair this chunk!\n            dec.extend(rs_correct_msg(chunk, self.nsym, fcr=self.fcr, generator=self.generator, erase_pos=e_pos, only_erasures=only_erasures)[0])\n        return dec", "response": "Decode a message from a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recurse( record, index, stop_types=STOP_TYPES,already_seen=None, type_group=False ):\n    if already_seen is None:\n        already_seen = set()\n    if record['address'] not in already_seen:\n        already_seen.add(record['address'])\n        if 'refs' in record:\n            for child in children( record, index, stop_types=stop_types ):\n                if child['address'] not in already_seen:\n                    for descendant in recurse( \n                        child, index, stop_types, \n                        already_seen=already_seen, type_group=type_group,\n                    ):\n                        yield descendant\n        yield record", "response": "Recursive generator for the tree traversal of a dictionary record."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all loops within the index and replace with loop records", "response": "def find_loops( record, index, stop_types = STOP_TYPES, open=None, seen = None ):\n    \"\"\"Find all loops within the index and replace with loop records\"\"\"\n    if open is None:\n        open = []\n    if seen is None:\n        seen = set()\n    for child in children( record, index, stop_types = stop_types ):\n        if child['type'] in stop_types or child['type'] == LOOP_TYPE:\n            continue\n        if child['address'] in open:\n            # loop has been found \n            start = open.index( child['address'] )\n            new = frozenset( open[start:] )\n            if new not in seen:\n                seen.add(new)\n                yield new\n        elif child['address'] in seen:\n            continue \n        else:\n            seen.add( child['address'])\n            open.append( child['address'] )\n            for loop in find_loops( child, index, stop_types=stop_types, open=open, seen=seen ):\n                yield loop \n            open.pop( -1 )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef promote_loops( loops, index, shared ):\n    for loop in loops:\n        loop = list(loop)\n        members = [index[addr] for addr in loop]\n        external_parents = list(set([\n            addr for addr in sum([shared.get(addr,[]) for addr in loop],[])\n            if addr not in loop \n        ]))\n        if external_parents:\n            if len(external_parents) == 1:\n                # potentially a loop that's been looped...\n                parent = index.get( external_parents[0] )\n                if parent['type'] == LOOP_TYPE:\n                    continue \n            # we haven't already been looped...\n            loop_addr = new_address( index )\n            shared[loop_addr] = external_parents\n            loop_record = index[loop_addr] = {\n                'address': loop_addr,\n                'refs': loop,\n                'parents': external_parents,\n                'type': LOOP_TYPE,\n                'size': 0,\n            }\n            for member in members:\n                # member's references must *not* point to loop...\n                member['refs'] = [\n                    ref for ref in member['refs']\n                    if ref not in loop \n                ]\n                # member's parents are *just* the loop\n                member['parents'][:] = [loop_addr]\n            # each referent to loop holds a single reference to the loop rather than many to children\n            for parent in external_parents:\n                parent = index[parent]\n                for member in members:\n                    rewrite_references( parent['refs'], member['address'], None )\n                parent['refs'].append( loop_addr )", "response": "Turn loops into objects that can be processed normally"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve children records for given record", "response": "def children( record, index, key='refs', stop_types=STOP_TYPES ):\n    \"\"\"Retrieve children records for given record\"\"\"\n    result = []\n    for ref in record.get( key,[]):\n        try:\n            record = index[ref]\n        except KeyError, err:\n            #print 'No record for %s address %s in %s'%(key, ref, record['address'] )\n            pass # happens when an unreachable references a reachable that has been compressed out...\n        else:\n            if record['type'] not in stop_types:\n                result.append(  record  )\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef children_types( record, index, key='refs', stop_types=STOP_TYPES ):\n    types = {}\n    for child in children( record, index, key, stop_types=stop_types ):\n        types.setdefault(child['type'],[]).append( child )\n    return types", "response": "Produce dictionary mapping type - key to instances for all children"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a recursive - cost hierarchy based on the overall record and the shared dictionary.", "response": "def recurse_module( overall_record, index, shared, stop_types=STOP_TYPES, already_seen=None, min_size=0 ):\n    \"\"\"Creates a has-a recursive-cost hierarchy\n    \n    Mutates objects in-place to produce a hierarchy of memory usage based on \n    reference-holding cost assignment\n    \"\"\"\n    for record in recurse( \n        overall_record, index, \n        stop_types=stop_types, \n        already_seen=already_seen, \n        type_group=True,\n    ):\n        # anything with a totsize we've already processed...\n        if record.get('totsize') is not None:\n            continue \n        rinfo = record \n        rinfo['module'] = overall_record.get('name',NON_MODULE_REFS )\n        if not record['refs']:\n            rinfo['rsize'] = 0\n            rinfo['children'] = []\n        else:\n            # TODO: provide a flag to coalesce based on e.g. type at each level or throughout...\n            rinfo['children'] = rinfo_children = list ( children( record, index, stop_types=stop_types ) )\n            rinfo['rsize'] = sum([\n                (\n                    child.get('totsize',0.0)/float(len(shared.get( child['address'], [])) or 1)\n                )\n                for child in rinfo_children\n            ], 0.0 )\n        rinfo['totsize'] = record['size'] + rinfo['rsize']\n    \n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rewrite_refs( targets, old,new, index, key='refs', single_ref=False ):\n    for parent in targets:\n        if not isinstance( parent, dict ):\n            try:\n                parent = index[parent]\n            except KeyError, err:\n                continue \n        rewrite_references( parent[key], old, new, single_ref=single_ref )", "response": "Rewrite key in all targets to replace old with new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rewrite_references( sequence, old, new, single_ref=False ):\n    old,new = as_id(old),as_id(new)\n    to_delete = []\n    for i,n in enumerate(sequence):\n        if n == old:\n            if new is None:\n                to_delete.append( i )\n            else:\n                sequence[i] = new \n                if single_ref:\n                    new = None\n        elif n == new and single_ref:\n            new = None\n    if to_delete:\n        to_delete.reverse()\n        for i in to_delete:\n            del sequence[i]\n    return sequence", "response": "Rewrite parents to point to new in old\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a sub - set of children who are simple in the sense of group_children", "response": "def simple( child, shared, parent ):\n    \"\"\"Return sub-set of children who are \"simple\" in the sense of group_children\"\"\"\n    return (\n        not child.get('refs',())\n        and (\n            not shared.get(child['address'])\n        or \n            shared.get(child['address']) == [parent['address']]\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngroups children of a single object into a single tree of objects with long children - lists", "response": "def group_children( index, shared, min_kids=10, stop_types=STOP_TYPES, delete_children=True ):\n    \"\"\"Collect like-type children into sub-groups of objects for objects with long children-lists\n    \n    Only group if:\n    \n        * there are more than X children of type Y\n        * children are \"simple\"\n            * individual children have no children themselves\n            * individual children have no other parents...\n    \"\"\"\n    to_compress = []\n    \n    for to_simplify in list(iterindex( index )):\n        if not isinstance( to_simplify, dict ):\n            continue\n        for typ,kids in children_types( to_simplify, index, stop_types=stop_types ).items():\n            kids = [k for k in kids if k and simple(k,shared, to_simplify)]\n            if len(kids) >= min_kids:\n                # we can group and compress out...\n                to_compress.append( (to_simplify,typ,kids))\n    \n    for to_simplify,typ,kids in to_compress:\n        typ_address = new_address(index)\n        kid_addresses = [k['address'] for k in kids]\n        index[typ_address] = {\n            'address': typ_address,\n            'type': MANY_TYPE,\n            'name': typ,\n            'size': sum( [k.get('size',0) for k in kids], 0),\n            'parents': [to_simplify['address']],\n        }\n        \n        shared[typ_address] = index[typ_address]['parents']\n        to_simplify['refs'][:] = [typ_address]\n        \n        if delete_children:\n            for address in kid_addresses:\n                try:\n                    del index[address]\n                except KeyError, err: \n                    pass # already compressed out\n                try:\n                    del shared[address]\n                except KeyError, err:\n                    pass # already compressed out\n            index[typ_address]['refs'] = []\n        else:\n            index[typ_address]['refs'] = kid_addresses"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simplify_dicts( index, shared, simplify_dicts=SIMPLIFY_DICTS, always_compress=ALWAYS_COMPRESS_DICTS ):\n    \n    # things which will have their dictionaries compressed out\n    \n    to_delete = set()\n    \n    for to_simplify in iterindex(index):\n        if to_simplify['address'] in to_delete:\n            continue \n        if to_simplify['type'] in simplify_dicts and not 'compressed' in to_simplify:\n            refs = to_simplify['refs']\n            for ref in refs:\n                child = index.get( ref )\n                if child is not None and child['type'] == 'dict':\n                    child_referrers = child['parents'][:]\n                    if len(child_referrers) == 1 or to_simplify['type'] in always_compress:\n                        \n                        to_simplify['compressed'] = True\n                        to_simplify['refs'] = child['refs']\n                        to_simplify['size'] += child['size']\n                        \n                        # rewrite anything *else* that was pointing to child to point to us...\n                        while to_simplify['address'] in child_referrers:\n                            child_referrers.remove( to_simplify['address'] )\n                        if child_referrers:\n                            rewrite_refs( \n                                child_referrers, \n                                child['address'],\n                                to_simplify['address'], \n                                index, single_ref=True\n                            )\n                        \n                        # now rewrite grandchildren to point to root obj instead of dict\n                        for grandchild in child['refs']:\n                            grandchild = index[grandchild]\n                            parent_set = grandchild['parents']\n                            if parent_set:\n                                rewrite_references( \n                                    parent_set, \n                                    child,\n                                    to_simplify,\n                                    single_ref = True,\n                                )\n                            assert parent_set\n                        to_delete.add( child['address'] )\n    for item in to_delete:\n        del index[item]\n        del shared[item]\n    \n    return index", "response": "Eliminate noise dictionary records from the overall index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_reachable( modules, index, shared, stop_types=STOP_TYPES ):\n    reachable = set()\n    already_seen = set()\n    for module in modules:\n        for child in recurse( module, index, stop_types=stop_types, already_seen=already_seen):\n            reachable.add( child['address'] )\n    return reachable", "response": "Find all reachable objects from given root nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deparent_unreachable( reachable, shared ):\n    for id,shares in shared.iteritems():\n        if id in reachable: # child is reachable\n            filtered = [\n                x \n                for x in shares \n                if x in reachable # only those parents which are reachable\n            ]\n            if len(filtered) != len(shares):\n                shares[:] = filtered", "response": "Eliminate all parent - links from unreachable objects from shared"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bind_parents( index, shared ):\n    for v in iterindex( index ):\n        v['parents'] = shared.get( v['address'], [] )", "response": "Bind parents on all items in index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding appropriate root objects from which to recurse the hierarchies", "response": "def find_roots( disconnected, index, shared ):\n    \"\"\"Find appropriate \"root\" objects from which to recurse the hierarchies\n    \n    Will generate a synthetic root for anything which doesn't have any parents...\n    \"\"\"\n    log.warn( '%s disconnected objects in %s total objects', len(disconnected), len(index))\n    natural_roots = [x for x in disconnected if x.get('refs') and not x.get('parents')]\n    log.warn( '%s objects with no parents at all' ,len(natural_roots))\n    for natural_root in natural_roots:\n        recurse_module(\n            natural_root, index, shared\n        )\n        yield natural_root\n    rest = [x for x in disconnected if x.get( 'totsize' ) is None]\n    un_found = {\n        'type': 'module',\n        'name': '<disconnected objects>',\n        'children': rest,\n        'parents': [ ],\n        'size': 0,\n        'totsize': sum([x['size'] for x in rest],0),\n        'address': new_address( index ),\n    }\n    index[un_found['address']] = un_found\n    yield un_found"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the given root by type - key", "response": "def get_root( self, key ):\n        \"\"\"Retrieve the given root by type-key\"\"\"\n        if key not in self.roots:\n            root,self.rows = load( self.filename, include_interpreter = self.include_interpreter )\n            self.roots[key] = root\n        return self.roots[key]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record_action(self, method_name, *args, **kwargs):\n        self.actions.append((method_name, args, kwargs))", "response": "Record the method - calling action."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef play_actions(self, target):\n        for method_name, args, kwargs in self.actions:\n            method = getattr(target, method_name)\n            method(*args, **kwargs)", "response": "Play record actions on the target object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef route(self, host, rule, **options):\n        def decorator(func):\n            endpoint = \"{func.__module__}:{func.__name__}\".format(func=func)\n            self.record_action(\"add_url_rule\", host, rule, endpoint, **options)\n            return func\n        return decorator", "response": "The decorator to register wrapped function as the brownant app."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the input value into bytes type.", "response": "def to_bytes_safe(text, encoding=\"utf-8\"):\n    \"\"\"Convert the input value into bytes type.\n\n    If the input value is string type and could be encode as UTF-8 bytes, the\n    encoded value will be returned. Otherwise, the encoding has failed, the\n    origin value will be returned as well.\n\n    :param text: the input value which could be string or bytes.\n    :param encoding: the expected encoding be used while converting the string\n                     input into bytes.\n    :rtype: :class:`~__builtin__.bytes`\n    \"\"\"\n    if not isinstance(text, (bytes, text_type)):\n        raise TypeError(\"must be string type\")\n\n    if isinstance(text, text_type):\n        return text.encode(encoding)\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_attr(self, obj, name):\n        attr_name = self.attr_names[name]\n        return getattr(obj, attr_name)", "response": "Get the attribute of the target object with the configured attribute\n        name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_url_rule(self, host, rule_string, endpoint, **options):\n        rule = Rule(rule_string, host=host, endpoint=endpoint, **options)\n        self.url_map.add(rule)", "response": "Add a url rule to the app instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_url(self, url_string):\n        url = urllib.parse.urlparse(url_string)\n        url = self.validate_url(url)\n        url_adapter = self.url_map.bind(server_name=url.hostname,\n                                        url_scheme=url.scheme,\n                                        path_info=url.path)\n        query_args = url_decode(url.query)\n        return url, url_adapter, query_args", "response": "Parse the URL string with the url_map and return the url adapter and query_args."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_url(self, url):\n        # fix up the non-ascii path\n        url_path = to_bytes_safe(url.path)\n        url_path = urllib.parse.quote(url_path, safe=b\"/%\")\n\n        # fix up the non-ascii query\n        url_query = to_bytes_safe(url.query)\n        url_query = urllib.parse.quote(url_query, safe=b\"?=&\")\n\n        url = urllib.parse.ParseResult(url.scheme, url.netloc, url_path,\n                                       url.params, url_query, url.fragment)\n\n        # validate the components of URL\n        has_hostname = url.hostname is not None and len(url.hostname) > 0\n        has_http_scheme = url.scheme in (\"http\", \"https\")\n        has_path = not len(url.path) or url.path.startswith(\"/\")\n\n        if not (has_hostname and has_http_scheme and has_path):\n            raise NotSupported(\"invalid url: %s\" % repr(url))\n\n        return url", "response": "Validate the :class:`~urllib.parse.ParseResult` object.\n\n        This method will make sure the :meth:`~brownant.app.BrownAnt.parse_url`\n        could work as expected even meet a unexpected URL string.\n\n        :param url: the parsed url.\n        :type url: :class:`~urllib.parse.ParseResult`"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch_url(self, url_string):\n        url, url_adapter, query_args = self.parse_url(url_string)\n\n        try:\n            endpoint, kwargs = url_adapter.match()\n        except NotFound:\n            raise NotSupported(url_string)\n        except RequestRedirect as e:\n            new_url = \"{0.new_url}?{1}\".format(e, url_encode(query_args))\n            return self.dispatch_url(new_url)\n\n        try:\n            handler = import_string(endpoint)\n            request = Request(url=url, args=query_args)\n            return handler(request, **kwargs)\n        except RequestRedirect as e:\n            return self.dispatch_url(e.new_url)", "response": "Dispatch the URL string to the target endpoint function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmount a supported site to this app instance.", "response": "def mount_site(self, site):\n        \"\"\"Mount a supported site to this app instance.\n\n        :param site: the site instance be mounted.\n        \"\"\"\n        if isinstance(site, string_types):\n            site = import_string(site)\n        site.play_actions(target=self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ping(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"ping\"], *args, **kwargs)", "response": "Ping Server\n\n        Respond without doing anything.\n        This endpoint is used to check that the service is up.\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef githubWebHookConsumer(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"githubWebHookConsumer\"], *args, **kwargs)", "response": "Consume GitHub WebHook events"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef badge(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"badge\"], *args, **kwargs)", "response": "Badge is a simple wrapper for the badge method"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npost a comment on a given GitHub Issue or Pull Request For a given Issue or Pull Request of a repository, this will write a new message. This method takes input: ``v1/create-comment.json#`` This method is ``experimental``", "response": "def createComment(self, *args, **kwargs):\n        \"\"\"\n        Post a comment on a given GitHub Issue or Pull Request\n\n        For a given Issue or Pull Request of a repository, this will write a new message.\n\n        This method takes input: ``v1/create-comment.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"createComment\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    print()\n    print(\"-~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~-\")\n    print(lorem_gotham_title().center(50))\n    print(\"-~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~--~*~-\")\n    print()\n    poem = lorem_gotham()\n    for n in range(16):\n        if n in (4, 8, 12):\n            print()\n        print(next(poem))\n    print()", "response": "I provide a command - line interface for this module\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def listWorkerTypes(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"listWorkerTypes\"], *args, **kwargs)", "response": "This method returns a list of worker types known to be managed by this client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning an instance Request an instance of a worker type This method takes input: ``v1/run-instance-request.json#`` This method is ``experimental``", "response": "async def runInstance(self, *args, **kwargs):\n        \"\"\"\n        Run an instance\n\n        Request an instance of a worker type\n\n        This method takes input: ``v1/run-instance-request.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"runInstance\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def workerTypeStats(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"workerTypeStats\"], *args, **kwargs)", "response": "This method returns the resource stats for a workerType"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlook up the resource health for a workerType Return a view of the health of a given worker type This method gives output: ``v1/health.json#`` This method is ``experimental``", "response": "async def workerTypeHealth(self, *args, **kwargs):\n        \"\"\"\n        Look up the resource health for a workerType\n\n        Return a view of the health of a given worker type\n\n        This method gives output: ``v1/health.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"workerTypeHealth\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking up the most recent errors of a workerType Return a list of the most recent errors encountered by a worker type This method gives output: ``v1/errors.json#`` This method is ``experimental``", "response": "async def workerTypeErrors(self, *args, **kwargs):\n        \"\"\"\n        Look up the most recent errors of a workerType\n\n        Return a list of the most recent errors encountered by a worker type\n\n        This method gives output: ``v1/errors.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"workerTypeErrors\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def workerTypeState(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"workerTypeState\"], *args, **kwargs)", "response": "This method returns the state of a given worker type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring a KeyPair for a given worker type exists. Idempotently ensure that a KeyPair for a given worker type exists.", "response": "async def ensureKeyPair(self, *args, **kwargs):\n        \"\"\"\n        Ensure a KeyPair for a given worker type exists\n\n        Idempotently ensure that a keypair of a given name exists\n\n        This method takes input: ``v1/create-key-pair.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"ensureKeyPair\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a KeyPair from the cluster", "response": "async def removeKeyPair(self, *args, **kwargs):\n        \"\"\"\n        Ensure a KeyPair for a given worker type does not exist\n\n        Ensure that a keypair of a given name does not exist.\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"removeKeyPair\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def terminateInstance(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"terminateInstance\"], *args, **kwargs)", "response": "Terminate an instance in a specified region"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrequesting prices for EC2 Return a list of possible prices for EC2 This method takes input: ``v1/prices-request.json#`` This method gives output: ``v1/prices.json#`` This method is ``experimental``", "response": "async def getSpecificPrices(self, *args, **kwargs):\n        \"\"\"\n        Request prices for EC2\n\n        Return a list of possible prices for EC2\n\n        This method takes input: ``v1/prices-request.json#``\n\n        This method gives output: ``v1/prices.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"getSpecificPrices\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def getHealth(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getHealth\"], *args, **kwargs)", "response": "Get health of the EC2 account"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def getRecentErrors(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getRecentErrors\"], *args, **kwargs)", "response": "This method returns a list of recent errors encountered in the provisioner across all worker types"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list of regions managed by this ec2 - manager", "response": "async def regions(self, *args, **kwargs):\n        \"\"\"\n        See the list of regions managed by this ec2-manager\n\n        This method is only for debugging the ec2-manager\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"regions\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def amiUsage(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"amiUsage\"], *args, **kwargs)", "response": "This method returns the list of AMIs and their usage by returning a list of objects in the form of a list of strings"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsee the current EBS volume usage list Lists current EBS volume usage by returning a list of objects that are uniquely defined by {region, volumetype, state} in the form: { region: string, volumetype: string, state: string, totalcount: integer, totalgb: integer, touched: timestamp (last time that information was updated), } This method is ``experimental``", "response": "async def ebsUsage(self, *args, **kwargs):\n        \"\"\"\n        See the current EBS volume usage list\n\n        Lists current EBS volume usage by returning a list of objects\n        that are uniquely defined by {region, volumetype, state} in the form:\n        {\n        region: string,\n          volumetype: string,\n          state: string,\n          totalcount: integer,\n          totalgb: integer,\n          touched: timestamp (last time that information was updated),\n        }\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"ebsUsage\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def sqsStats(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"sqsStats\"], *args, **kwargs)", "response": "This method returns statistics on the sqs queues"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npurge the SQS queues", "response": "async def purgeQueues(self, *args, **kwargs):\n        \"\"\"\n        Purge the SQS queues\n\n        This method is only for debugging the ec2-manager\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"purgeQueues\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotating_cube(degree_change=3, frame_rate=3):\n    degrees = 0\n    while True:\n        t1 = time.time()\n\n        with Frame() as frame:\n            oval_width = frame.width\n            oval_height = frame.height / 3.0\n            cube_height = int(oval_height * 2)\n\n            (p1_x, p1_y) = ellipse_point(degrees, oval_width, oval_height)\n            (p2_x, p2_y) = ellipse_point(degrees + 90, oval_width, oval_height)\n            (p3_x, p3_y) = ellipse_point(degrees + 180, oval_width, oval_height)\n            (p4_x, p4_y) = ellipse_point(degrees + 270, oval_width, oval_height)\n            degrees = (degrees + degree_change) % 360\n\n            # connect square thing at top\n            frame.line(p1_x, p1_y, p2_x, p2_y)\n            frame.line(p2_x, p2_y, p3_x, p3_y)\n            frame.line(p3_x, p3_y, p4_x, p4_y)\n            frame.line(p4_x, p4_y, p1_x, p1_y)\n\n            # connect top to bottom\n            frame.line(p1_x, p1_y, p1_x, p1_y + cube_height)\n            frame.line(p2_x, p2_y, p2_x, p2_y + cube_height)\n            frame.line(p3_x, p3_y, p3_x, p3_y + cube_height)\n            frame.line(p4_x, p4_y, p4_x, p4_y + cube_height)\n\n            # connect square thing at bottom\n            frame.line(p1_x, p1_y + cube_height, p2_x, p2_y + cube_height)\n            frame.line(p2_x, p2_y + cube_height, p3_x, p3_y + cube_height)\n            frame.line(p3_x, p3_y + cube_height, p4_x, p4_y + cube_height)\n            frame.line(p4_x, p4_y + cube_height, p1_x, p1_y + cube_height)\n\n        elapsed = (time.time() - t1)\n        time.sleep(abs(1.0 / frame_rate - elapsed))", "response": "Rotating a single cube program."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line(self, x0, y0, x1, y1, c='*'):\n        steep = abs(y1 - y0) > abs(x1 - x0)\n        if steep:\n            (x0, y0) = (y0, x0)\n            (x1, y1) = (y1, x1)\n        if x0 > x1:\n            (x0, x1) = (x1, x0)\n            (y0, y1) = (y1, y0)\n        deltax = x1 - x0\n        deltay = abs(y1 - y0)\n        error = deltax / 2\n        y = y0\n        if y0 < y1:\n            ystep = 1\n        else:\n            ystep = -1\n        for x in range(x0, x1 - 1):\n            if steep:\n                self[y, x] = c\n            else:\n                self[x, y] = c\n            error = error - deltay\n            if error < 0:\n                y = y + ystep\n                error = error + deltax", "response": "r Draws a line in the region of the area defined by x0 y0 x1 y1."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminateWorkerType(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"terminateWorkerType\"], *args, **kwargs)", "response": "Terminate all resources from a worker type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrequest prices for EC2 Return a list of possible prices for EC2 This method gives output: ``v1/prices.json#`` This method is ``experimental``", "response": "def getPrices(self, *args, **kwargs):\n        \"\"\"\n        Request prices for EC2\n\n        Return a list of possible prices for EC2\n\n        This method gives output: ``v1/prices.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"getPrices\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef allState(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"allState\"], *args, **kwargs)", "response": "This method returns the entire internal state of the ec2 - manager"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Task Definition This end - point will return the task - definition.", "response": "def task(self, *args, **kwargs):\n        \"\"\"\n        Get Task Definition\n\n        This end-point will return the task-definition. Notice that the task\n        definition may have been modified by queue, if an optional property is\n        not specified the queue may provide a default value.\n\n        This method gives output: ``v1/task.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"task\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef defineTask(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"defineTask\"], *args, **kwargs)", "response": "This is only present for legacy use."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nschedules a task for execution.", "response": "def scheduleTask(self, *args, **kwargs):\n        \"\"\"\n        Schedule Defined Task\n\n        scheduleTask will schedule a task to be executed, even if it has\n        unresolved dependencies. A task would otherwise only be scheduled if\n        its dependencies were resolved.\n\n        This is useful if you have defined a task that depends on itself or on\n        some other task that has not been resolved, but you wish the task to be\n        scheduled immediately.\n\n        This will announce the task as pending and workers will be allowed to\n        claim it and resolve the task.\n\n        **Note** this operation is **idempotent** and will not fail or complain\n        if called with a `taskId` that is already scheduled, or even resolved.\n        To reschedule a task previously resolved, use `rerunTask`.\n\n        This method gives output: ``v1/task-status-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"scheduleTask\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrerun a previously resolved task", "response": "def rerunTask(self, *args, **kwargs):\n        \"\"\"\n        Rerun a Resolved Task\n\n        This method _reruns_ a previously resolved task, even if it was\n        _completed_. This is useful if your task completes unsuccessfully, and\n        you just want to run it from scratch again. This will also reset the\n        number of `retries` allowed.\n\n        This method is deprecated in favour of creating a new task with the same\n        task definition (but with a new taskId).\n\n        Remember that `retries` in the task status counts the number of runs that\n        the queue have started because the worker stopped responding, for example\n        because a spot node died.\n\n        **Remark** this operation is idempotent, if you try to rerun a task that\n        is not either `failed` or `completed`, this operation will just return\n        the current task status.\n\n        This method gives output: ``v1/task-status-response.json#``\n\n        This method is ``deprecated``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"rerunTask\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncancels a scheduled task", "response": "def cancelTask(self, *args, **kwargs):\n        \"\"\"\n        Cancel Task\n\n        This method will cancel a task that is either `unscheduled`, `pending` or\n        `running`. It will resolve the current run as `exception` with\n        `reasonResolved` set to `canceled`. If the task isn't scheduled yet, ie.\n        it doesn't have any runs, an initial run will be added and resolved as\n        described above. Hence, after canceling a task, it cannot be scheduled\n        with `queue.scheduleTask`, but a new run can be created with\n        `queue.rerun`. These semantics is equivalent to calling\n        `queue.scheduleTask` immediately followed by `queue.cancelTask`.\n\n        **Remark** this operation is idempotent, if you try to cancel a task that\n        isn't `unscheduled`, `pending` or `running`, this operation will just\n        return the current task status.\n\n        This method gives output: ``v1/task-status-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"cancelTask\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreporting Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed Report Run Failed", "response": "def reportFailed(self, *args, **kwargs):\n        \"\"\"\n        Report Run Failed\n\n        Report a run failed, resolving the run as `failed`. Use this to resolve\n        a run that failed because the task specific code behaved unexpectedly.\n        For example the task exited non-zero, or didn't produce expected output.\n\n        Do not use this if the task couldn't be run because if malformed\n        payload, or other unexpected condition. In these cases we have a task\n        exception, which should be reported with `reportException`.\n\n        This method gives output: ``v1/task-status-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"reportFailed\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reportException(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"reportException\"], *args, **kwargs)", "response": "Report a run as an exception."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createArtifact(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"createArtifact\"], *args, **kwargs)", "response": "This method creates an artifact for a specific run of a task. This method is used by the Task s createArtifact method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef completeArtifact(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"completeArtifact\"], *args, **kwargs)", "response": "This endpoint finalises an artifact upload and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of all active provisioners Get all active provisioners. The term \"provisioner\" is taken broadly to mean anything with a provisionerId. This does not necessarily mean there is an associated service performing any provisioning activity. The response is paged. If this end-point returns a `continuationToken`, you should call the end-point again with the `continuationToken` as a query-string option. By default this end-point will list up to 1000 provisioners in a single page. You may limit this with the query-string parameter `limit`. This method gives output: ``v1/list-provisioners-response.json#`` This method is ``experimental``", "response": "def listProvisioners(self, *args, **kwargs):\n        \"\"\"\n        Get a list of all active provisioners\n\n        Get all active provisioners.\n\n        The term \"provisioner\" is taken broadly to mean anything with a provisionerId.\n        This does not necessarily mean there is an associated service performing any\n        provisioning activity.\n\n        The response is paged. If this end-point returns a `continuationToken`, you\n        should call the end-point again with the `continuationToken` as a query-string\n        option. By default this end-point will list up to 1000 provisioners in a single\n        page. You may limit this with the query-string parameter `limit`.\n\n        This method gives output: ``v1/list-provisioners-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"listProvisioners\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getProvisioner(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"getProvisioner\"], *args, **kwargs)", "response": "Get an active provisioning"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeclaring a provisioner. This is a simple API call to update a provisioner with some details about it.", "response": "def declareProvisioner(self, *args, **kwargs):\n        \"\"\"\n        Update a provisioner\n\n        Declare a provisioner, supplying some details about it.\n\n        `declareProvisioner` allows updating one or more properties of a provisioner as long as the required scopes are\n        possessed. For example, a request to update the `aws-provisioner-v1`\n        provisioner with a body `{description: 'This provisioner is great'}` would require you to have the scope\n        `queue:declare-provisioner:aws-provisioner-v1#description`.\n\n        The term \"provisioner\" is taken broadly to mean anything with a provisionerId.\n        This does not necessarily mean there is an associated service performing any\n        provisioning activity.\n\n        This method takes input: ``v1/update-provisioner-request.json#``\n\n        This method gives output: ``v1/provisioner-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"declareProvisioner\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget Number of Pending Tasks", "response": "def pendingTasks(self, *args, **kwargs):\n        \"\"\"\n        Get Number of Pending Tasks\n\n        Get an approximate number of pending tasks for the given `provisionerId`\n        and `workerType`.\n\n        The underlying Azure Storage Queues only promises to give us an estimate.\n        Furthermore, we cache the result in memory for 20 seconds. So consumers\n        should be no means expect this to be an accurate number.\n        It is, however, a solid estimate of the number of pending tasks.\n\n        This method gives output: ``v1/pending-tasks-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"pendingTasks\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nquarantines a worker Quarantine a worker This method takes input: ``v1/quarantine-worker-request.json#`` This method gives output: ``v1/worker-response.json#`` This method is ``experimental``", "response": "def quarantineWorker(self, *args, **kwargs):\n        \"\"\"\n        Quarantine a worker\n\n        Quarantine a worker\n\n        This method takes input: ``v1/quarantine-worker-request.json#``\n\n        This method gives output: ``v1/worker-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"quarantineWorker\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef declareWorker(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"declareWorker\"], *args, **kwargs)", "response": "This method allows you to declare a worker in the worker manager. It is intended to be used by the worker manager to update one or more properties of a worker."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def findTask(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"findTask\"], *args, **kwargs)", "response": "Find Indexed Task\n        Find Indexed Task\n        Find Indexed Task\n        Find Indexed Task\n        Find Indexed Task\n        Find Indexed Task\nTTtl Find Indexed Task\nTTtl Find Indexed Task\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def listNamespaces(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"listNamespaces\"], *args, **kwargs)", "response": "This method returns the list of namespaces immediately under a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting Tasks List the tasks immediately under a given namespace. This endpoint lists up to 1000 tasks. If more tasks are present, a `continuationToken` will be returned, which can be given in the next request. For the initial request, the payload should be an empty JSON object. **Remark**, this end-point is designed for humans browsing for tasks, not services, as that makes little sense. This method gives output: ``v1/list-tasks-response.json#`` This method is ``stable``", "response": "async def listTasks(self, *args, **kwargs):\n        \"\"\"\n        List Tasks\n\n        List the tasks immediately under a given namespace.\n\n        This endpoint\n        lists up to 1000 tasks. If more tasks are present, a\n        `continuationToken` will be returned, which can be given in the next\n        request. For the initial request, the payload should be an empty JSON\n        object.\n\n        **Remark**, this end-point is designed for humans browsing for tasks, not\n        services, as that makes little sense.\n\n        This method gives output: ``v1/list-tasks-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"listTasks\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def insertTask(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"insertTask\"], *args, **kwargs)", "response": "Insert a task into the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def findArtifactFromTask(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"findArtifactFromTask\"], *args, **kwargs)", "response": "Find an artifact from a task by its index path and redirect to the artifact on the most recent run with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def allPurgeRequests(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"allPurgeRequests\"], *args, **kwargs)", "response": "All Purge Requests This endpoint returns a list of all open purge requests for the specified master and provisioner."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npurging Requests for a particular workerType", "response": "async def purgeRequests(self, *args, **kwargs):\n        \"\"\"\n        Open Purge Requests for a provisionerId/workerType pair\n\n        List of caches that need to be purged if they are from before\n        a certain time. This is safe to be used in automation from\n        workers.\n\n        This method gives output: ``v1/purge-cache-request-list.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"purgeRequests\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, text):\n        escape_parts = re.compile('\\x01?\\x1b\\\\[([0-9;]*)m\\x02?')\n        chunks = escape_parts.split(text)\n        i = 0\n        for chunk in chunks:\n            if chunk != '':\n                if i % 2 == 0:\n                    self.stream.write(chunk)\n                else:\n                    c = chunk.split(';')\n                    r = Magic.rdisplay(c)\n                    self.display(**r) #see caveat 0\n                self.flush()\n            i += 1", "response": "Parses the text and prints proper output to the terminal\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getch(self):\n        return NotImplemented\n        fno = stdout.fileno()\n        mode = self.termios.tcgetattr(fno)\n        try:\n            self.tty.setraw(fno, self.termios.TCSANOW)\n            ch = self.read(1)\n        finally:\n            self.termios.tcsetattr(fno, self.termios.TCSANOW, mode)\n        return ch", "response": "Return a single character from the terminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays the codes using ANSI escapes", "response": "def display(self, codes=[], fg=None, bg=None):\n        \"\"\"Displays the codes using ANSI escapes\n        \"\"\"\n        codes, fg, bg = Magic.displayformat(codes, fg, bg)\n        self.stream.write(Magic.display(codes, fg, bg))\n        self.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves the term to a new location", "response": "def move(self, place, distance = 1):\n        \"\"\"see doc in Term class\"\"\"\n        for d in range(distance):\n            self.stream.write(self._get_cap('move '+place))\n        self.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self, scope = 'screen'):\n        if scope == 'line':\n            self.clear('beginning of line')\n            self.clear('end of line')\n        else: self.stream.write(self._get_cap('clear '+scope))\n        self.flush()", "response": "see doc in Term class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_size(self):\n        self.curses.setupterm()\n        return self.curses.tigetnum('cols'), self.curses.tigetnum('lines')", "response": "return the number of columns and lines"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display(self, codes=[], fg=None, bg=None):\n        codes, fg, bg = Magic.displayformat(codes, fg, bg)\n        color = 0\n        for c in codes:\n            try:\n                f = getattr(self, '_display_' + c)\n                out = f()\n                if out: color |= out\n            except AttributeError:\n                pass\n        cfg, cfgi, cbg, cbgi = self._split_attributes(\n                          self._get_console_info()['attributes'])\n        if self.reverse_input:\n            cfg, cbg = (cbg // 0x10), (cfg * 0x10)\n            cfgi, cbgi = (cbgi // 0x10), (cfgi * 0x10)\n        if fg != None:\n            color |= self.FG[fg]\n            self.real_fg = self.FG[fg]\n        else: color |= cfg\n        if bg != None:\n            color |= self.BG[bg]\n        else: color |= cbg\n        color |= (cfgi | cbgi)\n        fg, fgi, bg, bgi = self._split_attributes(color)\n        if self.dim_output:\n            # intense black\n            fg = 0\n            fgi = self.FG_INTENSITY\n        if self.reverse_output:\n            fg, bg = (bg // 0x10), (fg * 0x10)\n            fgi, bgi = (bgi // 0x10), (fgi * 0x10)\n            self.reverse_input = True\n        if self.hidden_output:\n            fg = (bg // 0x10)\n            fgi = (bgi // 0x10)\n        self._set_attributes(fg | fgi | bg | bgi)", "response": "Displays the codes using Windows kernel calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsees doc in Term class", "response": "def get_size(self):\n        \"\"\"see doc in Term class\"\"\"\n        attr = self._get_console_info()\n        cols = attr['window']['right'] - attr['window']['left'] + 1\n        lines = attr['window']['bottom'] - attr['window']['top'] + 1\n        return cols, lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the attributes into foreground background and background intensity.", "response": "def _split_attributes(self, attrs):\n        \"\"\"Spilt attribute code\n        \n        Takes an attribute code and returns a tuple containing\n        foreground (fg), foreground intensity (fgi), background (bg), and\n        background intensity (bgi)\n        \n        Attributes can be joined using ``fg | fgi | bg | bgi``\n        \"\"\"\n        fg = attrs & self.FG_ALL\n        fgi = attrs & self.FG_INTENSITY\n        bg = attrs & self.BG_ALL\n        bgi = attrs & self.BG_INTENSITY\n        return fg, fgi, bg, bgi"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves the term to a specific location", "response": "def move(self, place, distance = 1):\n        \"\"\"see doc in Term class\"\"\"\n        x, y = self._get_position()\n        if place == 'up':\n            y -= distance\n        elif place == 'down':\n            for i in range(distance): print\n            nx, ny = self._get_position()\n            y = ny\n            self.move('beginning of line')\n        elif place == 'left':\n            x -= distance\n        elif place == 'right':\n            x += distance\n        elif place == 'beginning of line':\n            x = 0\n        elif place == 'beginning of screen':\n            x = 0\n            y = self._get_console_info()['window']['top']\n        else:\n            raise ValueError(\"invalid place to move\")\n        self._set_position((x, y))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsees doc in Term class According to http://support.microsoft.com/kb/99261 the best way to clear the console is to write out empty spaces", "response": "def clear(self, scope = 'screen'):\n        \"\"\"see doc in Term class\n        \n        According to http://support.microsoft.com/kb/99261 the best way\n        to clear the console is to write out empty spaces\n        \"\"\"\n        #TODO: clear attributes too\n        if scope == 'screen':\n            bos = (0, self._get_console_info()['window']['top'])\n            cols, lines = self.get_size()\n            length = cols * lines\n            self._clear_console(length, bos)\n            self.move('beginning of screen')\n        elif scope == ' beginning of line':\n            pass\n        elif scope == 'end of line':\n            curx, cury = self._get_position()\n            cols, lines = self.get_size()\n            coord = (curx, cury)\n            length = cols - curx\n            self._clear_console(length, coord)\n        elif scope == 'end of screen':\n            curx, cury = self._get_position()\n            coord = (curx, cury)\n            cols, lines = self.get_size()\n            length = (lines - cury) * cols - curx\n            self._clear_console(length, coord)\n        elif scope == 'line':\n            curx, cury = self._get_position()\n            coord = (0, cury)\n            cols, lines = self.get_size()\n            self._clear_console(cols, coord)\n            self._set_position((curx, cury))\n        elif scope == 'left':\n            self.move('left')\n            self.write(' ')\n        elif scope == 'right':\n            self.write(' ')\n            self.move('left')\n        else:\n            raise ValueError(\"invalid scope to clear\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_title(self):\n        #TODO: unicode support\n        strbuffer = self.ctypes.create_string_buffer(1024)\n        size = self.ctypes.c_short(1024)\n        #unicode versions are (Get|Set)ConsolTitleW\n        self.ctypes.windll.kernel32.GetConsoleTitleA(strbuffer, size)\n        return strbuffer.value", "response": "Get the title of the current console entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the coord of the current log entry.", "response": "def _get_coord(self, coord):\n        \"\"\" It's a hack, see fixcoord in pyreadline's console.py (revision \n        1289)\n        \"\"\"\n        x, y = coord\n        return self.ctypes.c_int(y << 16 | x)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake sure all arguments are valid", "response": "def displayformat(codes=[], fg=None, bg=None):\n        \"\"\"Makes sure all arguments are valid\"\"\"\n        if isinstance(codes, basestring):\n            codes = [codes]\n        else:\n            codes = list(codes)\n        for code in codes:\n            if code not in Magic.DISPLAY.keys():\n                raise ValueError(\"'%s' not a valid display value\" % code)\n        for color in (fg, bg):\n            if color != None:\n                if color not in Magic.COLORS.keys():\n                    raise ValueError(\"'%s' not a valid color\" % color)\n        return [codes, fg, bg]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a list of codes and generates dict", "response": "def rdisplay(codes):\n        \"\"\"Reads a list of codes and generates dict\n        \n        >>> Magic.rdisplay([])\n        {}\n        >>> result = Magic.rdisplay([1,2,34,46])\n        >>> sorted(result.keys())\n        ['bg', 'codes', 'fg']\n        >>> sorted(result['codes'])\n        ['bright', 'dim']\n        >>> result['bg']\n        'cyan'\n        >>> result['fg']\n        'blue'\n        \"\"\"\n        dcodes = []\n        fg = bg = None\n        for code in codes:\n            code = int(code)\n            offset = code // 10\n            decimal = code % 10\n            if offset == 3 and decimal in Magic.COLORS.values(): fg = decimal\n            elif offset == 4 and decimal in Magic.COLORS.values(): bg = decimal\n            elif code in Magic.DISPLAY.values(): dcodes.append(code)\n            else: pass # drop unhandled values\n        r = {}\n        if len(codes): r['codes'] = [Magic.rDISPLAY[c] for c in dcodes]\n        if fg != None: r['fg'] = Magic.rCOLORS[fg]\n        if bg != None: r['bg'] = Magic.rCOLORS[bg]\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists Clients with the same prefix", "response": "def listClients(self, *args, **kwargs):\n        \"\"\"\n        List Clients\n\n        Get a list of all clients.  With `prefix`, only clients for which\n        it is a prefix of the clientId are returned.\n\n        By default this end-point will try to return up to 1000 clients in one\n        request. But it **may return less, even none**.\n        It may also return a `continuationToken` even though there are no more\n        results. However, you can only be sure to have seen all results if you\n        keep calling `listClients` with the last `continuationToken` until you\n        get a result without a `continuationToken`.\n\n        This method gives output: ``v1/list-clients-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"listClients\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets information about a single client", "response": "def client(self, *args, **kwargs):\n        \"\"\"\n        Get Client\n\n        Get information about a single client.\n\n        This method gives output: ``v1/get-client-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"client\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate Client Create a new client and get the `accessToken` for this client. You should store the `accessToken` from this API call as there is no other way to retrieve it. If you loose the `accessToken` you can call `resetAccessToken` to reset it, and a new `accessToken` will be returned, but you cannot retrieve the current `accessToken`. If a client with the same `clientId` already exists this operation will fail. Use `updateClient` if you wish to update an existing client. The caller's scopes must satisfy `scopes`. This method takes input: ``v1/create-client-request.json#`` This method gives output: ``v1/create-client-response.json#`` This method is ``stable``", "response": "def createClient(self, *args, **kwargs):\n        \"\"\"\n        Create Client\n\n        Create a new client and get the `accessToken` for this client.\n        You should store the `accessToken` from this API call as there is no\n        other way to retrieve it.\n\n        If you loose the `accessToken` you can call `resetAccessToken` to reset\n        it, and a new `accessToken` will be returned, but you cannot retrieve the\n        current `accessToken`.\n\n        If a client with the same `clientId` already exists this operation will\n        fail. Use `updateClient` if you wish to update an existing client.\n\n        The caller's scopes must satisfy `scopes`.\n\n        This method takes input: ``v1/create-client-request.json#``\n\n        This method gives output: ``v1/create-client-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"createClient\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating Client Update an exisiting client. The `clientId` and `accessToken` cannot be updated, but `scopes` can be modified. The caller's scopes must satisfy all scopes being added to the client in the update operation. If no scopes are given in the request, the client's scopes remain unchanged This method takes input: ``v1/create-client-request.json#`` This method gives output: ``v1/get-client-response.json#`` This method is ``stable``", "response": "def updateClient(self, *args, **kwargs):\n        \"\"\"\n        Update Client\n\n        Update an exisiting client. The `clientId` and `accessToken` cannot be\n        updated, but `scopes` can be modified.  The caller's scopes must\n        satisfy all scopes being added to the client in the update operation.\n        If no scopes are given in the request, the client's scopes remain\n        unchanged\n\n        This method takes input: ``v1/create-client-request.json#``\n\n        This method gives output: ``v1/get-client-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"updateClient\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling a client that was disabled with disableClient.", "response": "def enableClient(self, *args, **kwargs):\n        \"\"\"\n        Enable Client\n\n        Enable a client that was disabled with `disableClient`.  If the client\n        is already enabled, this does nothing.\n\n        This is typically used by identity providers to re-enable clients that\n        had been disabled when the corresponding identity's scopes changed.\n\n        This method gives output: ``v1/get-client-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"enableClient\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disableClient(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"disableClient\"], *args, **kwargs)", "response": "Disable Client\n\n        Disable a client.  If the client is already disabled, this does nothing.\n\n        This is typically used by identity providers to disable clients when the\n        corresponding identity's scopes no longer satisfy the client's scopes.\n\n        This method gives output: ``v1/get-client-response.json#``\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deleteClient(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"deleteClient\"], *args, **kwargs)", "response": "Delete Client\n\n        Delete a client, please note that any roles related to this client must\n        be deleted independently.\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listRoles(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"listRoles\"], *args, **kwargs)", "response": "This method returns a list of all roles in the system"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting Role IDs If no limit is given, the roleIds of all roles are returned. Since this list may become long, callers can use the `limit` and `continuationToken` query arguments to page through the responses. This method gives output: ``v1/list-role-ids-response.json#`` This method is ``stable``", "response": "def listRoleIds(self, *args, **kwargs):\n        \"\"\"\n        List Role IDs\n\n        If no limit is given, the roleIds of all roles are returned. Since this\n        list may become long, callers can use the `limit` and `continuationToken`\n        query arguments to page through the responses.\n\n        This method gives output: ``v1/list-role-ids-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"listRoleIds\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listRoles2(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"listRoles2\"], *args, **kwargs)", "response": "This is the base API endpoint for listing the roles in the second level of the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new role", "response": "def createRole(self, *args, **kwargs):\n        \"\"\"\n        Create Role\n\n        Create a new role.\n\n        The caller's scopes must satisfy the new role's scopes.\n\n        If there already exists a role with the same `roleId` this operation\n        will fail. Use `updateRole` to modify an existing role.\n\n        Creation of a role that will generate an infinite expansion will result\n        in an error response.\n\n        This method takes input: ``v1/create-role-request.json#``\n\n        This method gives output: ``v1/get-role-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"createRole\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateRole(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"updateRole\"], *args, **kwargs)", "response": "This is a private API call to update a role. This is a private API call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deleteRole(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"deleteRole\"], *args, **kwargs)", "response": "Delete Role\n\n        Delete a role. This operation will succeed regardless of whether or not\n        the role exists.\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expandScopesGet(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"expandScopesGet\"], *args, **kwargs)", "response": "This is a convenience method that returns an expanded copy of the given scopeset with implied by any\n        roles included."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexpanding Scopes Return an expanded copy of the given scopeset, with scopes implied by any roles included. This method takes input: ``v1/scopeset.json#`` This method gives output: ``v1/scopeset.json#`` This method is ``stable``", "response": "def expandScopes(self, *args, **kwargs):\n        \"\"\"\n        Expand Scopes\n\n        Return an expanded copy of the given scopeset, with scopes implied by any\n        roles included.\n\n        This method takes input: ``v1/scopeset.json#``\n\n        This method gives output: ``v1/scopeset.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"expandScopes\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef currentScopes(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"currentScopes\"], *args, **kwargs)", "response": "Get the expanded scopes available in the request"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Temporary Read/Write Credentials S3 Get temporary AWS credentials for `read-write` or `read-only` access to a given `bucket` and `prefix` within that bucket. The `level` parameter can be `read-write` or `read-only` and determines which type of credentials are returned. Please note that the `level` parameter is required in the scope guarding access. The bucket name must not contain `.`, as recommended by Amazon. This method can only allow access to a whitelisted set of buckets. To add a bucket to that whitelist, contact the Taskcluster team, who will add it to the appropriate IAM policy. If the bucket is in a different AWS account, you will also need to add a bucket policy allowing access from the Taskcluster account. That policy should look like this: ``` { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"allow-taskcluster-auth-to-delegate-access\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::692406183521:root\" }, \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\", \"s3:GetBucketLocation\" ], \"Resource\": [ \"arn:aws:s3:::<bucket>\", \"arn:aws:s3:::<bucket>/*\" ] } ] } ``` The credentials are set to expire after an hour, but this behavior is subject to change. Hence, you should always read the `expires` property from the response, if you intend to maintain active credentials in your application. Please note that your `prefix` may not start with slash `/`. Such a prefix is allowed on S3, but we forbid it here to discourage bad behavior. Also note that if your `prefix` doesn't end in a slash `/`, the STS credentials may allow access to unexpected keys, as S3 does not treat slashes specially. For example, a prefix of `my-folder` will allow access to `my-folder/file.txt` as expected, but also to `my-folder.txt`, which may not be intended. Finally, note that the `PutObjectAcl` call is not allowed. Passing a canned ACL other than `private` to `PutObject` is treated as a `PutObjectAcl` call, and will result in an access-denied error from AWS. This limitation is due to a security flaw in Amazon S3 which might otherwise allow indefinite access to uploaded objects. **EC2 metadata compatibility**, if the querystring parameter `?format=iam-role-compat` is given, the response will be compatible with the JSON exposed by the EC2 metadata service. This aims to ease compatibility for libraries and tools built to auto-refresh credentials. For details on the format returned by EC2 metadata service see: [EC2 User Guide](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials). This method gives output: ``v1/aws-s3-credentials-response.json#`` This method is ``stable``", "response": "def awsS3Credentials(self, *args, **kwargs):\n        \"\"\"\n        Get Temporary Read/Write Credentials S3\n\n        Get temporary AWS credentials for `read-write` or `read-only` access to\n        a given `bucket` and `prefix` within that bucket.\n        The `level` parameter can be `read-write` or `read-only` and determines\n        which type of credentials are returned. Please note that the `level`\n        parameter is required in the scope guarding access.  The bucket name must\n        not contain `.`, as recommended by Amazon.\n\n        This method can only allow access to a whitelisted set of buckets.  To add\n        a bucket to that whitelist, contact the Taskcluster team, who will add it to\n        the appropriate IAM policy.  If the bucket is in a different AWS account, you\n        will also need to add a bucket policy allowing access from the Taskcluster\n        account.  That policy should look like this:\n\n        ```\n        {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Sid\": \"allow-taskcluster-auth-to-delegate-access\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                \"AWS\": \"arn:aws:iam::692406183521:root\"\n              },\n              \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:GetBucketLocation\"\n              ],\n              \"Resource\": [\n                \"arn:aws:s3:::<bucket>\",\n                \"arn:aws:s3:::<bucket>/*\"\n              ]\n            }\n          ]\n        }\n        ```\n\n        The credentials are set to expire after an hour, but this behavior is\n        subject to change. Hence, you should always read the `expires` property\n        from the response, if you intend to maintain active credentials in your\n        application.\n\n        Please note that your `prefix` may not start with slash `/`. Such a prefix\n        is allowed on S3, but we forbid it here to discourage bad behavior.\n\n        Also note that if your `prefix` doesn't end in a slash `/`, the STS\n        credentials may allow access to unexpected keys, as S3 does not treat\n        slashes specially.  For example, a prefix of `my-folder` will allow\n        access to `my-folder/file.txt` as expected, but also to `my-folder.txt`,\n        which may not be intended.\n\n        Finally, note that the `PutObjectAcl` call is not allowed.  Passing a canned\n        ACL other than `private` to `PutObject` is treated as a `PutObjectAcl` call, and\n        will result in an access-denied error from AWS.  This limitation is due to a\n        security flaw in Amazon S3 which might otherwise allow indefinite access to\n        uploaded objects.\n\n        **EC2 metadata compatibility**, if the querystring parameter\n        `?format=iam-role-compat` is given, the response will be compatible\n        with the JSON exposed by the EC2 metadata service. This aims to ease\n        compatibility for libraries and tools built to auto-refresh credentials.\n        For details on the format returned by EC2 metadata service see:\n        [EC2 User Guide](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials).\n\n        This method gives output: ``v1/aws-s3-credentials-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"awsS3Credentials\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef azureTables(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"azureTables\"], *args, **kwargs)", "response": "This method returns a list of all tables in an Account managed by Auth\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets Shared-Access-Signature for Azure Table Get a shared access signature (SAS) string for use with a specific Azure Table Storage table. The `level` parameter can be `read-write` or `read-only` and determines which type of credentials are returned. If level is read-write, it will create the table if it doesn't already exist. This method gives output: ``v1/azure-table-access-response.json#`` This method is ``stable``", "response": "def azureTableSAS(self, *args, **kwargs):\n        \"\"\"\n        Get Shared-Access-Signature for Azure Table\n\n        Get a shared access signature (SAS) string for use with a specific Azure\n        Table Storage table.\n\n        The `level` parameter can be `read-write` or `read-only` and determines\n        which type of credentials are returned.  If level is read-write, it will create the\n        table if it doesn't already exist.\n\n        This method gives output: ``v1/azure-table-access-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"azureTableSAS\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef azureContainers(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"azureContainers\"], *args, **kwargs)", "response": "This method returns a list of all containers in an Account managed by Auth\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Shared-Access-Signature for Azure Container Get a shared access signature (SAS) string for use with a specific Azure Blob Storage container. The `level` parameter can be `read-write` or `read-only` and determines which type of credentials are returned. If level is read-write, it will create the container if it doesn't already exist. This method gives output: ``v1/azure-container-response.json#`` This method is ``stable``", "response": "def azureContainerSAS(self, *args, **kwargs):\n        \"\"\"\n        Get Shared-Access-Signature for Azure Container\n\n        Get a shared access signature (SAS) string for use with a specific Azure\n        Blob Storage container.\n\n        The `level` parameter can be `read-write` or `read-only` and determines\n        which type of credentials are returned.  If level is read-write, it will create the\n        container if it doesn't already exist.\n\n        This method gives output: ``v1/azure-container-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"azureContainerSAS\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sentryDSN(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"sentryDSN\"], *args, **kwargs)", "response": "Sentry DSN for a Sentry Project"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef statsumToken(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"statsumToken\"], *args, **kwargs)", "response": "This method returns a Statsum Token for Statsum Project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a client token for the Websocktunnel service Get a temporary token suitable for use connecting to a [websocktunnel](https://github.com/taskcluster/websocktunnel) server. The resulting token will only be accepted by servers with a matching audience value. Reaching such a server is the callers responsibility. In general, a server URL or set of URLs should be provided to the caller as configuration along with the audience value. The token is valid for a limited time (on the scale of hours). Callers should refresh it before expiration. This method gives output: ``v1/websocktunnel-token-response.json#`` This method is ``stable``", "response": "def websocktunnelToken(self, *args, **kwargs):\n        \"\"\"\n        Get a client token for the Websocktunnel service\n\n        Get a temporary token suitable for use connecting to a\n        [websocktunnel](https://github.com/taskcluster/websocktunnel) server.\n\n        The resulting token will only be accepted by servers with a matching audience\n        value.  Reaching such a server is the callers responsibility.  In general,\n        a server URL or set of URLs should be provided to the caller as configuration\n        along with the audience value.\n\n        The token is valid for a limited time (on the scale of hours). Callers should\n        refresh it before expiration.\n\n        This method gives output: ``v1/websocktunnel-token-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"websocktunnelToken\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef h1(title, line=OVERLINE):\n    width = utils.term.width\n    printy(bold(title.center(width)).as_utf8)\n    printy(bold((line * width)[:width]).as_utf8)", "response": "Prints bold text with line beneath it spanning width of terminal\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_color(color):\n    if isinstance(color, basestring):\n        color = grapefruit.Color.NewFromHtml(color)\n    if isinstance(color, int):\n        (r, g, b) = xterm256.xterm_to_rgb(color)\n    elif hasattr(color, 'rgb'):\n        (r, g, b) = [int(c * 255.0) for c in color.rgb]\n    else:\n        (r, g, b) = color\n    assert isinstance(r, int) and 0 <= r <= 255\n    assert isinstance(g, int) and 0 <= g <= 255\n    assert isinstance(b, int) and 0 <= b <= 255\n    return (r, g, b)", "response": "r Converts a color into an r g b tuple"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint a section of the terminal", "response": "def section(title, bar=OVERLINE, strm=sys.stdout):\n    \"\"\"Helper function for testing demo routines\n    \"\"\"\n    width = utils.term.width\n    printy(bold(title.center(width)))\n    printy(bold((bar * width)[:width]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef workerTypeUpdated(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'worker-type-updated',\n            'name': 'workerTypeUpdated',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'http://schemas.taskcluster.net/aws-provisioner/v1/worker-type-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "This is a simple workerTypeUpdated message. It is a simple function that returns a message that is published to the master."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef workerTypeRemoved(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'worker-type-removed',\n            'name': 'workerTypeRemoved',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'http://schemas.taskcluster.net/aws-provisioner/v1/worker-type-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "This method is used to publish a WorkerType Removed Message to this base class. It is used to remove a WorkerType from a Taskcluster topic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef jobs(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'jobs',\n            'name': 'jobs',\n            'routingKey': [\n                {\n                    'multipleWords': False,\n                    'name': 'destination',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'project',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/pulse-job.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "This is an alias for the jobs method in Treeherder. It is used to create and publish a new Job Messages to Treeherder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a set of temporary credentials for the given clientId and accessToken.", "response": "def createTemporaryCredentials(clientId, accessToken, start, expiry, scopes, name=None):\n    \"\"\" Create a set of temporary credentials\n\n    Callers should not apply any clock skew; clock drift is accounted for by\n    auth service.\n\n    clientId: the issuing clientId\n    accessToken: the issuer's accessToken\n    start: start time of credentials (datetime.datetime)\n    expiry: expiration time of credentials, (datetime.datetime)\n    scopes: list of scopes granted\n    name: credential name (optional)\n\n    Returns a dictionary in the form:\n        { 'clientId': str, 'accessToken: str, 'certificate': str}\n    \"\"\"\n\n    for scope in scopes:\n        if not isinstance(scope, six.string_types):\n            raise exceptions.TaskclusterFailure('Scope must be string')\n\n    # Credentials can only be valid for 31 days.  I hope that\n    # this is validated on the server somehow...\n\n    if expiry - start > datetime.timedelta(days=31):\n        raise exceptions.TaskclusterFailure('Only 31 days allowed')\n\n    # We multiply times by 1000 because the auth service is JS and as a result\n    # uses milliseconds instead of seconds\n    cert = dict(\n        version=1,\n        scopes=scopes,\n        start=calendar.timegm(start.utctimetuple()) * 1000,\n        expiry=calendar.timegm(expiry.utctimetuple()) * 1000,\n        seed=utils.slugId().encode('ascii') + utils.slugId().encode('ascii'),\n    )\n\n    # if this is a named temporary credential, include the issuer in the certificate\n    if name:\n        cert['issuer'] = utils.toStr(clientId)\n\n    sig = ['version:' + utils.toStr(cert['version'])]\n    if name:\n        sig.extend([\n            'clientId:' + utils.toStr(name),\n            'issuer:' + utils.toStr(clientId),\n        ])\n    sig.extend([\n        'seed:' + utils.toStr(cert['seed']),\n        'start:' + utils.toStr(cert['start']),\n        'expiry:' + utils.toStr(cert['expiry']),\n        'scopes:'\n    ] + scopes)\n    sigStr = '\\n'.join(sig).encode()\n\n    if isinstance(accessToken, six.text_type):\n        accessToken = accessToken.encode()\n    sig = hmac.new(accessToken, sigStr, hashlib.sha256).digest()\n\n    cert['signature'] = utils.encodeStringForB64Header(sig)\n\n    newToken = hmac.new(accessToken, cert['seed'], hashlib.sha256).digest()\n    newToken = utils.makeB64UrlSafe(utils.encodeStringForB64Header(newToken)).replace(b'=', b'')\n\n    return {\n        'clientId': name or clientId,\n        'accessToken': newToken,\n        'certificate': utils.dumpJson(cert),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes an ext for Hawk authentication", "response": "def makeHawkExt(self):\n        \"\"\" Make an 'ext' for Hawk authentication \"\"\"\n        o = self.options\n        c = o.get('credentials', {})\n        if c.get('clientId') and c.get('accessToken'):\n            ext = {}\n            cert = c.get('certificate')\n            if cert:\n                if six.PY3 and isinstance(cert, six.binary_type):\n                    cert = cert.decode()\n                if isinstance(cert, six.string_types):\n                    cert = json.loads(cert)\n                ext['certificate'] = cert\n\n            if 'authorizedScopes' in o:\n                ext['authorizedScopes'] = o['authorizedScopes']\n\n            # .encode('base64') inserts a newline, which hawk doesn't\n            # like but doesn't strip itself\n            return utils.makeB64UrlSafe(utils.encodeStringForB64Header(utils.dumpJson(ext)).strip())\n        else:\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a signed URL.", "response": "def buildSignedUrl(self, methodName, *args, **kwargs):\n        \"\"\" Build a signed URL.  This URL contains the credentials needed to access\n        a resource.\"\"\"\n\n        if 'expiration' in kwargs:\n            expiration = kwargs['expiration']\n            del kwargs['expiration']\n        else:\n            expiration = self.options['signedUrlExpiration']\n\n        expiration = int(time.time() + expiration)  # Mainly so that we throw if it's not a number\n\n        requestUrl = self.buildUrl(methodName, *args, **kwargs)\n\n        if not self._hasCredentials():\n            raise exceptions.TaskclusterAuthFailure('Invalid Hawk Credentials')\n\n        clientId = utils.toStr(self.options['credentials']['clientId'])\n        accessToken = utils.toStr(self.options['credentials']['accessToken'])\n\n        def genBewit():\n            # We need to fix the output of get_bewit.  It returns a url-safe base64\n            # encoded string, which contains a list of tokens separated by '\\'.\n            # The first one is the clientId, the second is an int, the third is\n            # url-safe base64 encoded MAC, the fourth is the ext param.\n            # The problem is that the nested url-safe base64 encoded MAC must be\n            # base64 (i.e. not url safe) or server-side will complain.\n\n            # id + '\\\\' + exp + '\\\\' + mac + '\\\\' + options.ext;\n            resource = mohawk.base.Resource(\n                credentials={\n                    'id': clientId,\n                    'key': accessToken,\n                    'algorithm': 'sha256',\n                },\n                method='GET',\n                ext=utils.toStr(self.makeHawkExt()),\n                url=requestUrl,\n                timestamp=expiration,\n                nonce='',\n                # content='',\n                # content_type='',\n            )\n            bewit = mohawk.bewit.get_bewit(resource)\n            return bewit.rstrip('=')\n\n        bewit = genBewit()\n\n        if not bewit:\n            raise exceptions.TaskclusterFailure('Did not receive a bewit')\n\n        u = urllib.parse.urlparse(requestUrl)\n\n        qs = u.query\n        if qs:\n            qs += '&'\n        qs += 'bewit=%s' % bewit\n\n        return urllib.parse.urlunparse((\n            u.scheme,\n            u.netloc,\n            u.path,\n            u.params,\n            qs,\n            u.fragment,\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing a URL for the given route on this service based on the rootUrl", "response": "def _constructUrl(self, route):\n        \"\"\"Construct a URL for the given route on this service, based on the\n        rootUrl\"\"\"\n        return liburls.api(\n            self.options['rootUrl'],\n            self.serviceName,\n            self.apiVersion,\n            route.rstrip('/'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _processArgs(self, entry, *_args, **_kwargs):\n\n        # We need the args to be a list so we can mutate them\n        args = list(_args)\n        kwargs = copy.deepcopy(_kwargs)\n\n        reqArgs = entry['args']\n        routeParams = {}\n\n        query = {}\n        payload = None\n        kwApiArgs = {}\n\n        paginationHandler = None\n        paginationLimit = None\n\n        # There are three formats for calling methods:\n        #   1. method(v1, v1, payload)\n        #   2. method(payload, k1=v1, k2=v2)\n        #   3. method(payload=payload, query=query, params={k1: v1, k2: v2})\n        if len(kwargs) == 0:\n            if 'input' in entry and len(args) == len(reqArgs) + 1:\n                payload = args.pop()\n            if len(args) != len(reqArgs):\n                log.debug(args)\n                log.debug(reqArgs)\n                raise exceptions.TaskclusterFailure('Incorrect number of positional arguments')\n            log.debug('Using method(v1, v2, payload) calling convention')\n        else:\n            # We're considering kwargs which are the api route parameters to be\n            # called 'flat' because they're top level keys.  We're special\n            # casing calls which have only api-arg kwargs and possibly a payload\n            # value and handling them directly.\n            isFlatKwargs = True\n            if len(kwargs) == len(reqArgs):\n                for arg in reqArgs:\n                    if not kwargs.get(arg, False):\n                        isFlatKwargs = False\n                        break\n                if 'input' in entry and len(args) != 1:\n                    isFlatKwargs = False\n                if 'input' not in entry and len(args) != 0:\n                    isFlatKwargs = False\n                else:\n                    pass  # We're using payload=, query= and param=\n            else:\n                isFlatKwargs = False\n\n            # Now we're going to handle the two types of kwargs.  The first is\n            # 'flat' ones, which are where the api params\n            if isFlatKwargs:\n                if 'input' in entry:\n                    payload = args.pop()\n                kwApiArgs = kwargs\n                log.debug('Using method(payload, k1=v1, k2=v2) calling convention')\n                warnings.warn(\n                    \"The method(payload, k1=v1, k2=v2) calling convention will soon be deprecated\",\n                    PendingDeprecationWarning\n                )\n            else:\n                kwApiArgs = kwargs.get('params', {})\n                payload = kwargs.get('payload', None)\n                query = kwargs.get('query', {})\n                paginationHandler = kwargs.get('paginationHandler', None)\n                paginationLimit = kwargs.get('paginationLimit', None)\n                log.debug('Using method(payload=payload, query=query, params={k1: v1, k2: v2}) calling convention')\n\n        if 'input' in entry and isinstance(payload, type(None)):\n            raise exceptions.TaskclusterFailure('Payload is required')\n\n        # These all need to be rendered down to a string, let's just check that\n        # they are up front and fail fast\n        for arg in args:\n            if not isinstance(arg, six.string_types) and not isinstance(arg, int):\n                raise exceptions.TaskclusterFailure(\n                    'Positional arg \"%s\" to %s is not a string or int' % (arg, entry['name']))\n\n        for name, arg in six.iteritems(kwApiArgs):\n            if not isinstance(arg, six.string_types) and not isinstance(arg, int):\n                raise exceptions.TaskclusterFailure(\n                    'KW arg \"%s: %s\" to %s is not a string or int' % (name, arg, entry['name']))\n\n        if len(args) > 0 and len(kwApiArgs) > 0:\n            raise exceptions.TaskclusterFailure('Specify either positional or key word arguments')\n\n        # We know for sure that if we don't give enough arguments that the call\n        # should fail.  We don't yet know if we should fail because of two many\n        # arguments because we might be overwriting positional ones with kw ones\n        if len(reqArgs) > len(args) + len(kwApiArgs):\n            raise exceptions.TaskclusterFailure(\n                '%s takes %d args, only %d were given' % (\n                    entry['name'], len(reqArgs), len(args) + len(kwApiArgs)))\n\n        # We also need to error out when we have more positional args than required\n        # because we'll need to go through the lists of provided and required args\n        # at the same time.  Not disqualifying early means we'll get IndexErrors if\n        # there are more positional arguments than required\n        if len(args) > len(reqArgs):\n            raise exceptions.TaskclusterFailure('%s called with too many positional args',\n                                                entry['name'])\n\n        i = 0\n        for arg in args:\n            log.debug('Found a positional argument: %s', arg)\n            routeParams[reqArgs[i]] = arg\n            i += 1\n\n        log.debug('After processing positional arguments, we have: %s', routeParams)\n\n        routeParams.update(kwApiArgs)\n\n        log.debug('After keyword arguments, we have: %s', routeParams)\n\n        if len(reqArgs) != len(routeParams):\n            errMsg = '%s takes %s args, %s given' % (\n                entry['name'],\n                ','.join(reqArgs),\n                routeParams.keys())\n            log.error(errMsg)\n            raise exceptions.TaskclusterFailure(errMsg)\n\n        for reqArg in reqArgs:\n            if reqArg not in routeParams:\n                errMsg = '%s requires a \"%s\" argument which was not provided' % (\n                    entry['name'], reqArg)\n                log.error(errMsg)\n                raise exceptions.TaskclusterFailure(errMsg)\n\n        return routeParams, payload, query, paginationHandler, paginationLimit", "response": "This method takes a single entry and returns what the internal state of the object is."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a route like \"/task/<taskId > artifacts and a mapping like {\"taskId\": 12345 } return a string like \"/task/<taskId > artifacts", "response": "def _subArgsInRoute(self, entry, args):\n        \"\"\" Given a route like \"/task/<taskId>/artifacts\" and a mapping like\n        {\"taskId\": \"12345\"}, return a string like \"/task/12345/artifacts\"\n        \"\"\"\n\n        route = entry['route']\n\n        for arg, val in six.iteritems(args):\n            toReplace = \"<%s>\" % arg\n            if toReplace not in route:\n                raise exceptions.TaskclusterFailure(\n                    'Arg %s not found in route for %s' % (arg, entry['name']))\n            val = urllib.parse.quote(str(val).encode(\"utf-8\"), '')\n            route = route.replace(\"<%s>\" % arg, val)\n\n        return route.lstrip('/')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _hasCredentials(self):\n        cred = self.options.get('credentials')\n        return (\n            cred and\n            'clientId' in cred and\n            'accessToken' in cred and\n            cred['clientId'] and\n            cred['accessToken']\n        )", "response": "Return True if credentials is given"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _makeHttpRequest(self, method, route, payload):\n\n        url = self._constructUrl(route)\n        log.debug('Full URL used is: %s', url)\n\n        hawkExt = self.makeHawkExt()\n\n        # Serialize payload if given\n        if payload is not None:\n            payload = utils.dumpJson(payload)\n\n        # Do a loop of retries\n        retry = -1  # we plus first in the loop, and attempt 1 is retry 0\n        retries = self.options['maxRetries']\n        while retry < retries:\n            retry += 1\n            # if this isn't the first retry then we sleep\n            if retry > 0:\n                time.sleep(utils.calculateSleepTime(retry))\n            # Construct header\n            if self._hasCredentials():\n                sender = mohawk.Sender(\n                    credentials={\n                        'id': self.options['credentials']['clientId'],\n                        'key': self.options['credentials']['accessToken'],\n                        'algorithm': 'sha256',\n                    },\n                    ext=hawkExt if hawkExt else {},\n                    url=url,\n                    content=payload if payload else '',\n                    content_type='application/json' if payload else '',\n                    method=method,\n                )\n\n                headers = {'Authorization': sender.request_header}\n            else:\n                log.debug('Not using hawk!')\n                headers = {}\n            if payload:\n                # Set header for JSON if payload is given, note that we serialize\n                # outside this loop.\n                headers['Content-Type'] = 'application/json'\n\n            log.debug('Making attempt %d', retry)\n            try:\n                response = utils.makeSingleHttpRequest(method, url, payload, headers)\n            except requests.exceptions.RequestException as rerr:\n                if retry < retries:\n                    log.warn('Retrying because of: %s' % rerr)\n                    continue\n                # raise a connection exception\n                raise exceptions.TaskclusterConnectionError(\n                    \"Failed to establish connection\",\n                    superExc=rerr\n                )\n\n            # Handle non 2xx status code and retry if possible\n            status = response.status_code\n            if status == 204:\n                return None\n\n            # Catch retryable errors and go to the beginning of the loop\n            # to do the retry\n            if 500 <= status and status < 600 and retry < retries:\n                log.warn('Retrying because of a %s status code' % status)\n                continue\n\n            # Throw errors for non-retryable errors\n            if status < 200 or status >= 300:\n                data = {}\n                try:\n                    data = response.json()\n                except Exception:\n                    pass  # Ignore JSON errors in error messages\n                # Find error message\n                message = \"Unknown Server Error\"\n                if isinstance(data, dict):\n                    message = data.get('message')\n                else:\n                    if status == 401:\n                        message = \"Authentication Error\"\n                    elif status == 500:\n                        message = \"Internal Server Error\"\n                # Raise TaskclusterAuthFailure if this is an auth issue\n                if status == 401:\n                    raise exceptions.TaskclusterAuthFailure(\n                        message,\n                        status_code=status,\n                        body=data,\n                        superExc=None\n                    )\n                # Raise TaskclusterRestFailure for all other issues\n                raise exceptions.TaskclusterRestFailure(\n                    message,\n                    status_code=status,\n                    body=data,\n                    superExc=None\n                )\n\n            # Try to load JSON\n            try:\n                return response.json()\n            except ValueError:\n                return {\"response\": response}\n\n        # This code-path should be unreachable\n        assert False, \"Error from last retry should have been raised!\"", "response": "This method is used to make an HTTP request for the Taskcluster API endpoint. It will attempt to make a single HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _makeHttpRequest(self, method, route, payload):\n\n        url = self._constructUrl(route)\n        log.debug('Full URL used is: %s', url)\n\n        hawkExt = self.makeHawkExt()\n\n        # Serialize payload if given\n        if payload is not None:\n            payload = utils.dumpJson(payload)\n\n        # Do a loop of retries\n        retry = -1  # we plus first in the loop, and attempt 1 is retry 0\n        retries = self.options['maxRetries']\n        while retry < retries:\n            retry += 1\n            # if this isn't the first retry then we sleep\n            if retry > 0:\n                snooze = float(retry * retry) / 10.0\n                log.info('Sleeping %0.2f seconds for exponential backoff', snooze)\n                await asyncio.sleep(utils.calculateSleepTime(retry))\n            # Construct header\n            if self._hasCredentials():\n                sender = mohawk.Sender(\n                    credentials={\n                        'id': self.options['credentials']['clientId'],\n                        'key': self.options['credentials']['accessToken'],\n                        'algorithm': 'sha256',\n                    },\n                    ext=hawkExt if hawkExt else {},\n                    url=url,\n                    content=payload if payload else '',\n                    content_type='application/json' if payload else '',\n                    method=method,\n                )\n\n                headers = {'Authorization': sender.request_header}\n            else:\n                log.debug('Not using hawk!')\n                headers = {}\n            if payload:\n                # Set header for JSON if payload is given, note that we serialize\n                # outside this loop.\n                headers['Content-Type'] = 'application/json'\n\n            log.debug('Making attempt %d', retry)\n            try:\n                response = await asyncutils.makeSingleHttpRequest(\n                    method, url, payload, headers, session=self.session\n                )\n            except aiohttp.ClientError as rerr:\n                if retry < retries:\n                    log.warn('Retrying because of: %s' % rerr)\n                    continue\n                # raise a connection exception\n                raise exceptions.TaskclusterConnectionError(\n                    \"Failed to establish connection\",\n                    superExc=rerr\n                )\n\n            status = response.status\n            if status == 204:\n                return None\n\n            # Catch retryable errors and go to the beginning of the loop\n            # to do the retry\n            if 500 <= status and status < 600 and retry < retries:\n                log.warn('Retrying because of a %s status code' % status)\n                continue\n\n            # Throw errors for non-retryable errors\n            if status < 200 or status >= 300:\n                # Parse messages from errors\n                data = {}\n                try:\n                    data = await response.json()\n                except Exception:\n                    pass  # Ignore JSON errors in error messages\n                # Find error message\n                message = \"Unknown Server Error\"\n                if isinstance(data, dict):\n                    message = data.get('message')\n                else:\n                    if status == 401:\n                        message = \"Authentication Error\"\n                    elif status == 500:\n                        message = \"Internal Server Error\"\n                    else:\n                        message = \"Unknown Server Error %s\\n%s\" % (str(status), str(data)[:1024])\n                # Raise TaskclusterAuthFailure if this is an auth issue\n                if status == 401:\n                    raise exceptions.TaskclusterAuthFailure(\n                        message,\n                        status_code=status,\n                        body=data,\n                        superExc=None\n                    )\n                # Raise TaskclusterRestFailure for all other issues\n                raise exceptions.TaskclusterRestFailure(\n                    message,\n                    status_code=status,\n                    body=data,\n                    superExc=None\n                )\n\n            # Try to load JSON\n            try:\n                await response.release()\n                return await response.json()\n            except (ValueError, aiohttp.client_exceptions.ContentTypeError):\n                return {\"response\": response}\n\n        # This code-path should be unreachable\n        assert False, \"Error from last retry should have been raised!\"", "response": "This method is used to make an HTTP request for the API endpoint. It will attempt to make a new HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists worker types with details Return a list of worker types, including some summary information about current capacity for each. While this list includes all defined worker types, there may be running EC2 instances for deleted worker types that are not included here. The list is unordered. This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/list-worker-types-summaries-response.json#`` This method is ``stable``", "response": "def listWorkerTypeSummaries(self, *args, **kwargs):\n        \"\"\"\n        List worker types with details\n\n        Return a list of worker types, including some summary information about\n        current capacity for each.  While this list includes all defined worker types,\n        there may be running EC2 instances for deleted worker types that are not\n        included here.  The list is unordered.\n\n        This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/list-worker-types-summaries-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"listWorkerTypeSummaries\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Worker Type Last Modified Time This method is provided to allow workers to see when they were last modified. The value provided through UserData can be compared against this value to see if changes have been made If the worker type definition has not been changed, the date should be identical as it is the same stored value. This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/get-worker-type-last-modified.json#`` This method is ``stable``", "response": "def workerTypeLastModified(self, *args, **kwargs):\n        \"\"\"\n        Get Worker Type Last Modified Time\n\n        This method is provided to allow workers to see when they were\n        last modified.  The value provided through UserData can be\n        compared against this value to see if changes have been made\n        If the worker type definition has not been changed, the date\n        should be identical as it is the same stored value.\n\n        This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/get-worker-type-last-modified.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"workerTypeLastModified\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef removeWorkerType(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"removeWorkerType\"], *args, **kwargs)", "response": "This method deletes a worker type definition from the storage table. This method is only applicable to background workers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getSecret(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"getSecret\"], *args, **kwargs)", "response": "Get a Secret from storage"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef instanceStarted(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"instanceStarted\"], *args, **kwargs)", "response": "Report an instance starting by giving its id as well\n        as its security token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting AWS State for a given worker type", "response": "def state(self, *args, **kwargs):\n        \"\"\"\n        Get AWS State for a worker type\n\n        Return the state of a given workertype as stored by the provisioner.\n        This state is stored as three lists: 1 for running instances, 1 for\n        pending requests.  The `summary` property contains an updated summary\n        similar to that returned from `listWorkerTypeSummaries`.\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"state\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def set(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"set\"], *args, **kwargs)", "response": "Set Secret\n\n        Set the secret associated with some key.  If the secret already exists, it is\n        updated instead.\n\n        This method takes input: ``v1/secret.json#``\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a secret from a key set", "response": "async def remove(self, *args, **kwargs):\n        \"\"\"\n        Delete Secret\n\n        Delete the secret associated with some key.\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"remove\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a specific key from the secret store", "response": "async def get(self, *args, **kwargs):\n        \"\"\"\n        Read Secret\n\n        Read the secret associated with some key.  If the secret has recently\n        expired, the response code 410 is returned.  If the caller lacks the\n        scope necessary to get the secret, the call will fail with a 403 code\n        regardless of whether the secret exists.\n\n        This method gives output: ``v1/secret.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"get\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting input from the command line and validates it.", "response": "def input_object(prompt_text, cast = None, default = None,\n                 prompt_ext = ': ', castarg = [], castkwarg = {}):\n    \"\"\"Gets input from the command line and validates it.\n    \n    prompt_text\n        A string. Used to prompt the user. Do not include a trailing\n        space.\n        \n    prompt_ext\n        Added on to the prompt at the end. At the moment this must not\n        include any control stuff because it is send directly to\n        raw_input\n        \n    cast\n        This can be any callable object (class, function, type, etc). It\n        simply calls the cast with the given arguements and returns the \n        result. If a ValueError is raised, it\n        will output an error message and prompt the user again.\n\n        Because some builtin python objects don't do casting in the way\n        that we might like you can easily write a wrapper function that\n        looks and the input and returns the appropriate object or exception.\n        Look in the cast submodule for examples.\n        \n        If cast is None, then it will do nothing (and you will have a string)\n        \n    default\n        function returns this value if the user types nothing in. This is\n        can be used to cancel the input so-to-speek\n        \n    castarg, castkwarg\n        list and dictionary. Extra arguments passed on to the cast.\n    \"\"\"\n    while True:\n        stdout.write(prompt_text)\n        value = stdout.raw_input(prompt_ext)\n        if value == '': return default\n        try:\n            if cast != None: value = cast(value, *castarg, **castkwarg)\n        except ValueError as details:\n            if cast in NICE_INPUT_ERRORS: # see comment above this constant\n                stderr.write(ERROR_MESSAGE % (NICE_INPUT_ERRORS[cast] % details))\n            else: stderr.write(ERROR_MESSAGE % (DEFAULT_INPUT_ERRORS % str(details)))\n            continue\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(question, values, default=None, list_values = False, ignorecase = True ):\n    values = list(values)\n    for i in range(len(values)):\n        if not isinstance(values[i], dict):\n            values[i] = {'values': [values[i]]}\n    try:\n        import readline, rlcomplete\n        wordlist = [ str(v) for value in values\n                    for v in value['values']]\n        completer = rlcomplete.ListCompleter(wordlist, ignorecase)\n        readline.parse_and_bind(\"tab: complete\")\n        readline.set_completer(completer.complete)\n    except ImportError:\n        pass\n    valuelist = []\n    for item in values:\n        entry = ( display('bright', item.get('fg'), item.get('bg')) +\n            str(item['values'][0]) + display(['default']) )\n        if str(item['values'][0]) == str(default): entry = '['+entry+']'\n        if list_values: entry += ' : ' + item['desc']\n        valuelist.append(entry)\n    if list_values: question += os.linesep + os.linesep.join(valuelist) + os.linesep\n    else: question += ' (' + '/'.join(valuelist) + ')'\n    return input_object(question, cast = query_cast, default=default,\n                 castarg=[values,ignorecase])", "response": "This function is used to query the user for a specific language of the language of the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getHookStatus(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"getHookStatus\"], *args, **kwargs)", "response": "This endpoint returns the current status of the hook."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a hook This endpoint will create a new hook. The caller's credentials must include the role that will be used to create the task. That role must satisfy task.scopes as well as the necessary scopes to add the task to the queue. This method takes input: ``v1/create-hook-request.json#`` This method gives output: ``v1/hook-definition.json#`` This method is ``stable``", "response": "def createHook(self, *args, **kwargs):\n        \"\"\"\n        Create a hook\n\n        This endpoint will create a new hook.\n\n        The caller's credentials must include the role that will be used to\n        create the task.  That role must satisfy task.scopes as well as the\n        necessary scopes to add the task to the queue.\n\n        This method takes input: ``v1/create-hook-request.json#``\n\n        This method gives output: ``v1/hook-definition.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"createHook\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef listLastFires(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"listLastFires\"], *args, **kwargs)", "response": "This endpoint returns information about the last few times this hook has been fired"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntask Defined Messages When a task is created or just defined a message is posted to this exchange. This message exchange is mainly useful when tasks are scheduled by a scheduler that uses `defineTask` as this does not make the task `pending`. Thus, no `taskPending` message is published. Please, note that messages are also published on this exchange if defined using `createTask`. This exchange outputs: ``v1/task-defined-message.json#``This exchange takes the following keys: * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required) * taskId: `taskId` for the task this message concerns (required) * runId: `runId` of latest run for the task, `_` if no run is exists for the task. * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. * provisionerId: `provisionerId` this task is targeted at. (required) * workerType: `workerType` this task must run on. (required) * schedulerId: `schedulerId` this task was created by. (required) * taskGroupId: `taskGroupId` this task was created in. (required) * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.", "response": "def taskDefined(self, *args, **kwargs):\n        \"\"\"\n        Task Defined Messages\n\n        When a task is created or just defined a message is posted to this\n        exchange.\n\n        This message exchange is mainly useful when tasks are scheduled by a\n        scheduler that uses `defineTask` as this does not make the task\n        `pending`. Thus, no `taskPending` message is published.\n        Please, note that messages are also published on this exchange if defined\n        using `createTask`.\n\n        This exchange outputs: ``v1/task-defined-message.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskId: `taskId` for the task this message concerns (required)\n\n         * runId: `runId` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task.\n\n         * provisionerId: `provisionerId` this task is targeted at. (required)\n\n         * workerType: `workerType` this task must run on. (required)\n\n         * schedulerId: `schedulerId` this task was created by. (required)\n\n         * taskGroupId: `taskGroupId` this task was created in. (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-defined',\n            'name': 'taskDefined',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-defined-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntask Pending Messages When a task becomes `pending` a message is posted to this exchange. This is useful for workers who doesn't want to constantly poll the queue for new tasks. The queue will also be authority for task states and claims. But using this exchange workers should be able to distribute work efficiently and they would be able to reduce their polling interval significantly without affecting general responsiveness. This exchange outputs: ``v1/task-pending-message.json#``This exchange takes the following keys: * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required) * taskId: `taskId` for the task this message concerns (required) * runId: `runId` of latest run for the task, `_` if no run is exists for the task. (required) * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. * provisionerId: `provisionerId` this task is targeted at. (required) * workerType: `workerType` this task must run on. (required) * schedulerId: `schedulerId` this task was created by. (required) * taskGroupId: `taskGroupId` this task was created in. (required) * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.", "response": "def taskPending(self, *args, **kwargs):\n        \"\"\"\n        Task Pending Messages\n\n        When a task becomes `pending` a message is posted to this exchange.\n\n        This is useful for workers who doesn't want to constantly poll the queue\n        for new tasks. The queue will also be authority for task states and\n        claims. But using this exchange workers should be able to distribute work\n        efficiently and they would be able to reduce their polling interval\n        significantly without affecting general responsiveness.\n\n        This exchange outputs: ``v1/task-pending-message.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskId: `taskId` for the task this message concerns (required)\n\n         * runId: `runId` of latest run for the task, `_` if no run is exists for the task. (required)\n\n         * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task.\n\n         * provisionerId: `provisionerId` this task is targeted at. (required)\n\n         * workerType: `workerType` this task must run on. (required)\n\n         * schedulerId: `schedulerId` this task was created by. (required)\n\n         * taskGroupId: `taskGroupId` this task was created in. (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-pending',\n            'name': 'taskPending',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-pending-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef taskRunning(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'task-running',\n            'name': 'taskRunning',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-running-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "This method is used to create the Task Running Messages in the base class. It is called by the worker to create the task running message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef artifactCreated(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'artifact-created',\n            'name': 'artifactCreated',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/artifact-created-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "The createArtifact method is called when the queue is created. This method is called when the queue is created and the message is published to the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntask Completed Messages When a task is successfully completed by a worker a message is posted this exchange. This message is routed using the `runId`, `workerGroup` and `workerId` that completed the task. But information about additional runs is also available from the task status structure. This exchange outputs: ``v1/task-completed-message.json#``This exchange takes the following keys: * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required) * taskId: `taskId` for the task this message concerns (required) * runId: `runId` of latest run for the task, `_` if no run is exists for the task. (required) * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. (required) * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. (required) * provisionerId: `provisionerId` this task is targeted at. (required) * workerType: `workerType` this task must run on. (required) * schedulerId: `schedulerId` this task was created by. (required) * taskGroupId: `taskGroupId` this task was created in. (required) * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.", "response": "def taskCompleted(self, *args, **kwargs):\n        \"\"\"\n        Task Completed Messages\n\n        When a task is successfully completed by a worker a message is posted\n        this exchange.\n        This message is routed using the `runId`, `workerGroup` and `workerId`\n        that completed the task. But information about additional runs is also\n        available from the task status structure.\n\n        This exchange outputs: ``v1/task-completed-message.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskId: `taskId` for the task this message concerns (required)\n\n         * runId: `runId` of latest run for the task, `_` if no run is exists for the task. (required)\n\n         * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. (required)\n\n         * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. (required)\n\n         * provisionerId: `provisionerId` this task is targeted at. (required)\n\n         * workerType: `workerType` this task must run on. (required)\n\n         * schedulerId: `schedulerId` this task was created by. (required)\n\n         * taskGroupId: `taskGroupId` this task was created in. (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-completed',\n            'name': 'taskCompleted',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-completed-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntasks Failed Messages When a task ran, but failed to complete successfully a message is posted to this exchange. This is same as worker ran task-specific code, but the task specific code exited non-zero. This exchange outputs: ``v1/task-failed-message.json#``This exchange takes the following keys: * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required) * taskId: `taskId` for the task this message concerns (required) * runId: `runId` of latest run for the task, `_` if no run is exists for the task. * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. * provisionerId: `provisionerId` this task is targeted at. (required) * workerType: `workerType` this task must run on. (required) * schedulerId: `schedulerId` this task was created by. (required) * taskGroupId: `taskGroupId` this task was created in. (required) * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.", "response": "def taskFailed(self, *args, **kwargs):\n        \"\"\"\n        Task Failed Messages\n\n        When a task ran, but failed to complete successfully a message is posted\n        to this exchange. This is same as worker ran task-specific code, but the\n        task specific code exited non-zero.\n\n        This exchange outputs: ``v1/task-failed-message.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskId: `taskId` for the task this message concerns (required)\n\n         * runId: `runId` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task.\n\n         * provisionerId: `provisionerId` this task is targeted at. (required)\n\n         * workerType: `workerType` this task must run on. (required)\n\n         * schedulerId: `schedulerId` this task was created by. (required)\n\n         * taskGroupId: `taskGroupId` this task was created in. (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-failed',\n            'name': 'taskFailed',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-failed-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntask Exception Messages Whenever Taskcluster fails to run a message is posted to this exchange. This happens if the task isn't completed before its `deadl\u00ecne`, all retries failed (i.e. workers stopped responding), the task was canceled by another entity, or the task carried a malformed payload. The specific _reason_ is evident from that task status structure, refer to the `reasonResolved` property for the last run. This exchange outputs: ``v1/task-exception-message.json#``This exchange takes the following keys: * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required) * taskId: `taskId` for the task this message concerns (required) * runId: `runId` of latest run for the task, `_` if no run is exists for the task. * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task. * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task. * provisionerId: `provisionerId` this task is targeted at. (required) * workerType: `workerType` this task must run on. (required) * schedulerId: `schedulerId` this task was created by. (required) * taskGroupId: `taskGroupId` this task was created in. (required) * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.", "response": "def taskException(self, *args, **kwargs):\n        \"\"\"\n        Task Exception Messages\n\n        Whenever Taskcluster fails to run a message is posted to this exchange.\n        This happens if the task isn't completed before its `deadl\u00ecne`,\n        all retries failed (i.e. workers stopped responding), the task was\n        canceled by another entity, or the task carried a malformed payload.\n\n        The specific _reason_ is evident from that task status structure, refer\n        to the `reasonResolved` property for the last run.\n\n        This exchange outputs: ``v1/task-exception-message.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskId: `taskId` for the task this message concerns (required)\n\n         * runId: `runId` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerGroup: `workerGroup` of latest run for the task, `_` if no run is exists for the task.\n\n         * workerId: `workerId` of latest run for the task, `_` if no run is exists for the task.\n\n         * provisionerId: `provisionerId` this task is targeted at. (required)\n\n         * workerType: `workerType` this task must run on. (required)\n\n         * schedulerId: `schedulerId` this task was created by. (required)\n\n         * taskGroupId: `taskGroupId` this task was created in. (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-exception',\n            'name': 'taskException',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'runId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerGroup',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'provisionerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'workerType',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-exception-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntask Group Resolved Messages", "response": "def taskGroupResolved(self, *args, **kwargs):\n        \"\"\"\n        Task Group Resolved Messages\n\n        A message is published on task-group-resolved whenever all submitted\n        tasks (whether scheduled or unscheduled) for a given task group have\n        been resolved, regardless of whether they resolved as successful or\n        not. A task group may be resolved multiple times, since new tasks may\n        be submitted against an already resolved task group.\n\n        This exchange outputs: ``v1/task-group-resolved.json#``This exchange takes the following keys:\n\n         * routingKeyKind: Identifier for the routing-key kind. This is always `'primary'` for the formalized routing key. (required)\n\n         * taskGroupId: `taskGroupId` for the task-group this message concerns (required)\n\n         * schedulerId: `schedulerId` for the task-group this message concerns (required)\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified.\n        \"\"\"\n\n        ref = {\n            'exchange': 'task-group-resolved',\n            'name': 'taskGroupResolved',\n            'routingKey': [\n                {\n                    'constant': 'primary',\n                    'multipleWords': False,\n                    'name': 'routingKeyKind',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'taskGroupId',\n                },\n                {\n                    'multipleWords': False,\n                    'name': 'schedulerId',\n                },\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/task-group-resolved.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matchuserhome(prefix):\n        if not prefix.startswith('~'):\n            raise ValueError(\"prefix must start with ~\")\n        try: import pwd\n        except ImportError:\n            try: import winpwd as pwd\n            except ImportError: return []\n        return ['~' + u[0] for u in pwd.getpwall() if u[0].startswith(prefix[1:])]", "response": "To find matches that start with prefix. This function returns a list of possible matches in form of [ ~user ~user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef completelist(self, text):\n        path = os.path.expanduser(text)\n        if len(path) == 0 or path[0] != os.path.sep:\n            path = os.path.join(os.getcwd(), path)\n        if text == '~':\n            dpath = dtext = ''\n            bpath = '~'\n            files = ['~/']\n        elif text.startswith('~') and text.find('/', 1) < 0:\n            return self.matchuserhome(text)\n        else:\n            dtext = os.path.dirname(text)\n            dpath = os.path.dirname(path)\n            bpath = os.path.basename(path)\n            files = os.listdir(dpath)\n        if bpath =='':\n            matches = [self.buildpath(text, f) for f in files if not f.startswith('.')]\n        else:\n            matches = [self.buildpath(dtext, f) for f in files if f.startswith(bpath)]\n        if len(matches) == 0 and os.path.basename(path)=='..':\n            files = os.listdir(path)\n            matches = [os.path.join(text, f) for f in files]\n        return matches", "response": "Return a list of potential matches for completion\n        \n        n. b. you want to complete to a file in the current working directory that starts with ~"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def status(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"status\"], *args, **kwargs)", "response": "Get task status\n\n        Get task status structure from `taskId`\n\n        This method gives output: ``v1/task-status-response.json#``\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def listTaskGroup(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"listTaskGroup\"], *args, **kwargs)", "response": "List Task Group\n\n        List tasks sharing the same `taskGroupId`.\n\n        As a task-group may contain an unbounded number of tasks, this end-point\n        may return a `continuationToken`. To continue listing tasks you must call\n        the `listTaskGroup` again with the `continuationToken` as the\n        query-string option `continuationToken`.\n\n        By default this end-point will try to return up to 1000 members in one\n        request. But it **may return less**, even if more tasks are available.\n        It may also return a `continuationToken` even though there are no more\n        results. However, you can only be sure to have seen all results if you\n        keep calling `listTaskGroup` with the last `continuationToken` until you\n        get a result without a `continuationToken`.\n\n        If you are not interested in listing all the members at once, you may\n        use the query-string option `limit` to return fewer.\n\n        This method gives output: ``v1/list-task-group-response.json#``\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist Dependent Tasks This method returns a list of tasks that depend on the given taskId.", "response": "async def listDependentTasks(self, *args, **kwargs):\n        \"\"\"\n        List Dependent Tasks\n\n        List tasks that depend on the given `taskId`.\n\n        As many tasks from different task-groups may dependent on a single tasks,\n        this end-point may return a `continuationToken`. To continue listing\n        tasks you must call `listDependentTasks` again with the\n        `continuationToken` as the query-string option `continuationToken`.\n\n        By default this end-point will try to return up to 1000 tasks in one\n        request. But it **may return less**, even if more tasks are available.\n        It may also return a `continuationToken` even though there are no more\n        results. However, you can only be sure to have seen all results if you\n        keep calling `listDependentTasks` with the last `continuationToken` until\n        you get a result without a `continuationToken`.\n\n        If you are not interested in listing all the tasks at once, you may\n        use the query-string option `limit` to return fewer.\n\n        This method gives output: ``v1/list-dependent-tasks-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"listDependentTasks\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def createTask(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"createTask\"], *args, **kwargs)", "response": "Create a new task in the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclaiming pending tasks for the given provisionerId and workerType queue.", "response": "async def claimWork(self, *args, **kwargs):\n        \"\"\"\n        Claim Work\n\n        Claim pending task(s) for the given `provisionerId`/`workerType` queue.\n\n        If any work is available (even if fewer than the requested number of\n        tasks, this will return immediately. Otherwise, it will block for tens of\n        seconds waiting for work.  If no work appears, it will return an emtpy\n        list of tasks.  Callers should sleep a short while (to avoid denial of\n        service in an error condition) and call the endpoint again.  This is a\n        simple implementation of \"long polling\".\n\n        This method takes input: ``v1/claim-work-request.json#``\n\n        This method gives output: ``v1/claim-work-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"claimWork\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def reclaimTask(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"reclaimTask\"], *args, **kwargs)", "response": "Reclaim a task from a specific runId."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def getArtifact(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getArtifact\"], *args, **kwargs)", "response": "Get an artifact from a specific run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget Artifacts from Run", "response": "async def listArtifacts(self, *args, **kwargs):\n        \"\"\"\n        Get Artifacts from Run\n\n        Returns a list of artifacts and associated meta-data for a given run.\n\n        As a task may have many artifacts paging may be necessary. If this\n        end-point returns a `continuationToken`, you should call the end-point\n        again with the `continuationToken` as the query-string option:\n        `continuationToken`.\n\n        By default this end-point will list up-to 1000 artifacts in a single page\n        you may limit this with the query-string parameter `limit`.\n\n        This method gives output: ``v1/list-artifacts-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"listArtifacts\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def getWorkerType(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getWorkerType\"], *args, **kwargs)", "response": "This method returns a worker - type from a provisioner."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a worker-type Declare a workerType, supplying some details about it. `declareWorkerType` allows updating one or more properties of a worker-type as long as the required scopes are possessed. For example, a request to update the `gecko-b-1-w2008` worker-type within the `aws-provisioner-v1` provisioner with a body `{description: 'This worker type is great'}` would require you to have the scope `queue:declare-worker-type:aws-provisioner-v1/gecko-b-1-w2008#description`. This method takes input: ``v1/update-workertype-request.json#`` This method gives output: ``v1/workertype-response.json#`` This method is ``experimental``", "response": "async def declareWorkerType(self, *args, **kwargs):\n        \"\"\"\n        Update a worker-type\n\n        Declare a workerType, supplying some details about it.\n\n        `declareWorkerType` allows updating one or more properties of a worker-type as long as the required scopes are\n        possessed. For example, a request to update the `gecko-b-1-w2008` worker-type within the `aws-provisioner-v1`\n        provisioner with a body `{description: 'This worker type is great'}` would require you to have the scope\n        `queue:declare-worker-type:aws-provisioner-v1/gecko-b-1-w2008#description`.\n\n        This method takes input: ``v1/update-workertype-request.json#``\n\n        This method gives output: ``v1/workertype-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"declareWorkerType\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def listWorkers(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"listWorkers\"], *args, **kwargs)", "response": "This method returns a list of all active workers of a workerType."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def getWorker(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getWorker\"], *args, **kwargs)", "response": "Get a worker from a worker - type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef file(value, **kwarg):\n    #a bit weird, but I don't want to hard code default values\n    try:\n        f = open(value, **kwarg)\n    except IOError as e:\n        raise ValueError(\"unable to open %s : %s\" % (path.abspath(value), e))\n    return f", "response": "returns a file object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculateSleepTime(attempt):\n    if attempt <= 0:\n        return 0\n\n    # We subtract one to get exponents: 1, 2, 3, 4, 5, ..\n    delay = float(2 ** (attempt - 1)) * float(DELAY_FACTOR)\n    # Apply randomization factor\n    delay = delay * (RANDOMIZATION_FACTOR * (random.random() * 2 - 1) + 1)\n    # Always limit with a maximum delay\n    return min(delay, MAX_DELAY)", "response": "Calculate sleep time based on the attempt number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromNow(offset, dateObj=None):\n\n    # We want to handle past dates as well as future\n    future = True\n    offset = offset.lstrip()\n    if offset.startswith('-'):\n        future = False\n        offset = offset[1:].lstrip()\n    if offset.startswith('+'):\n        offset = offset[1:].lstrip()\n\n    # Parse offset\n    m = r.match(offset)\n    if m is None:\n        raise ValueError(\"offset string: '%s' does not parse\" % offset)\n\n    # In order to calculate years and months we need to calculate how many days\n    # to offset the offset by, since timedelta only goes as high as weeks\n    days = 0\n    hours = 0\n    minutes = 0\n    seconds = 0\n    if m.group('years'):\n        years = int(m.group('years'))\n        days += 365 * years\n    if m.group('months'):\n        months = int(m.group('months'))\n        days += 30 * months\n    days += int(m.group('days') or 0)\n    hours += int(m.group('hours') or 0)\n    minutes += int(m.group('minutes') or 0)\n    seconds += int(m.group('seconds') or 0)\n\n    # Offset datetime from utc\n    delta = datetime.timedelta(\n        weeks=int(m.group('weeks') or 0),\n        days=days,\n        hours=hours,\n        minutes=minutes,\n        seconds=seconds,\n    )\n\n    if not dateObj:\n        dateObj = datetime.datetime.utcnow()\n\n    return dateObj + delta if future else dateObj - delta", "response": "Generate a datetime. datetime instance which is offset using a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumpJson(obj, **kwargs):\n    def handleDateAndBinaryForJs(x):\n        if six.PY3 and isinstance(x, six.binary_type):\n            x = x.decode()\n        if isinstance(x, datetime.datetime) or isinstance(x, datetime.date):\n            return stringDate(x)\n        else:\n            return x\n    d = json.dumps(obj, separators=(',', ':'), default=handleDateAndBinaryForJs, **kwargs)\n    assert '\\n' not in d\n    return d", "response": "Match JS s JSON. stringify."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a base64 string URL Safe", "response": "def makeB64UrlSafe(b64str):\n    \"\"\" Make a base64 string URL Safe \"\"\"\n    if isinstance(b64str, six.text_type):\n        b64str = b64str.encode()\n    # see RFC 4648, sec. 5\n    return b64str.replace(b'+', b'-').replace(b'/', b'_')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encodeStringForB64Header(s):\n    if isinstance(s, six.text_type):\n        s = s.encode()\n    return base64.encodestring(s).strip().replace(b'\\n', b'')", "response": "Encodes a string for use in a base64 - encoded HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stableSlugId():\n    _cache = {}\n\n    def closure(name):\n        if name not in _cache:\n            _cache[name] = slugId()\n        return _cache[name]\n\n    return closure", "response": "Returns a closure which can be used to generate stable slugIds for a single node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if all of the required scope sets in the list of assumed scopes are satisfied.", "response": "def scopeMatch(assumedScopes, requiredScopeSets):\n    \"\"\"\n        Take a list of a assumed scopes, and a list of required scope sets on\n        disjunctive normal form, and check if any of the required scope sets are\n        satisfied.\n\n        Example:\n\n            requiredScopeSets = [\n                [\"scopeA\", \"scopeB\"],\n                [\"scopeC\"]\n            ]\n\n        In this case assumed_scopes must contain, either:\n        \"scopeA\" AND \"scopeB\", OR just \"scopeC\".\n    \"\"\"\n    for scopeSet in requiredScopeSets:\n        for requiredScope in scopeSet:\n            for scope in assumedScopes:\n                if scope == requiredScope:\n                    # requiredScope satisifed, no need to check more scopes\n                    break\n                if scope.endswith(\"*\") and requiredScope.startswith(scope[:-1]):\n                    # requiredScope satisifed, no need to check more scopes\n                    break\n            else:\n                # requiredScope not satisfied, stop checking scopeSet\n                break\n        else:\n            # scopeSet satisfied, so we're happy\n            return True\n    # none of the requiredScopeSets were satisfied\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makeHttpRequest(method, url, payload, headers, retries=MAX_RETRIES, session=None):\n    retry = -1\n    response = None\n    while retry < retries:\n        retry += 1\n        # if this isn't the first retry then we sleep\n        if retry > 0:\n            snooze = float(retry * retry) / 10.0\n            log.info('Sleeping %0.2f seconds for exponential backoff', snooze)\n            time.sleep(snooze)\n\n        # Seek payload to start, if it is a file\n        if hasattr(payload, 'seek'):\n            payload.seek(0)\n\n        log.debug('Making attempt %d', retry)\n        try:\n            response = makeSingleHttpRequest(method, url, payload, headers, session)\n        except requests.exceptions.RequestException as rerr:\n            if retry < retries:\n                log.warn('Retrying because of: %s' % rerr)\n                continue\n            # raise a connection exception\n            raise rerr\n        # Handle non 2xx status code and retry if possible\n        try:\n            response.raise_for_status()\n        except requests.exceptions.RequestException:\n            pass\n        status = response.status_code\n        if 500 <= status and status < 600 and retry < retries:\n            if retry < retries:\n                log.warn('Retrying because of: %d status' % status)\n                continue\n            else:\n                raise exceptions.TaskclusterRestFailure(\"Unknown Server Error\", superExc=None)\n        return response\n\n    # This code-path should be unreachable\n    assert False, \"Error from last retry should have been raised!\"", "response": "Make an HTTP request and retry it until success."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if certificate is expired", "response": "def isExpired(certificate):\n    \"\"\" Check if certificate is expired \"\"\"\n    if isinstance(certificate, six.string_types):\n        certificate = json.loads(certificate)\n    expiry = certificate.get('expiry', 0)\n    return expiry < int(time.time() * 1000) + 20 * 60"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches root URL and credentials from the standard TASKCLUSTER_\u2026 environment variables and return them in a format suitable for passing to a client constructor.", "response": "def optionsFromEnvironment(defaults=None):\n    \"\"\"Fetch root URL and credentials from the standard TASKCLUSTER_\u2026\n    environment variables and return them in a format suitable for passing to a\n    client constructor.\"\"\"\n    options = defaults or {}\n    credentials = options.get('credentials', {})\n\n    rootUrl = os.environ.get('TASKCLUSTER_ROOT_URL')\n    if rootUrl:\n        options['rootUrl'] = rootUrl\n\n    clientId = os.environ.get('TASKCLUSTER_CLIENT_ID')\n    if clientId:\n        credentials['clientId'] = clientId\n\n    accessToken = os.environ.get('TASKCLUSTER_ACCESS_TOKEN')\n    if accessToken:\n        credentials['accessToken'] = accessToken\n\n    certificate = os.environ.get('TASKCLUSTER_CERTIFICATE')\n    if certificate:\n        credentials['certificate'] = certificate\n\n    if credentials:\n        options['credentials'] = credentials\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xterm_to_rgb(xcolor):\n    assert 0 <= xcolor <= 255\n    if xcolor < 16:\n        # basic colors\n        return BASIC16[xcolor]\n    elif 16 <= xcolor <= 231:\n        # color cube\n        xcolor -= 16\n        return (CUBE_STEPS[xcolor // 36 % 6],\n                CUBE_STEPS[xcolor // 6 % 6],\n                CUBE_STEPS[xcolor % 6])\n    elif 232 <= xcolor <= 255:\n        # gray tone\n        c = 8 + (xcolor - 232) * 0x0A\n        return (c, c, c)", "response": "Convert an xterm Color ID to an RGB value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rgb_to_xterm(r, g, b):\n    if r < 5 and g < 5 and b < 5:\n        return 16\n    best_match = 0\n    smallest_distance = 10000000000\n    for c in range(16, 256):\n        d = (COLOR_TABLE[c][0] - r) ** 2 + \\\n            (COLOR_TABLE[c][1] - g) ** 2 + \\\n            (COLOR_TABLE[c][2] - b) ** 2\n        if d < smallest_distance:\n            smallest_distance = d\n            best_match = c\n    return best_match", "response": "Quantize RGB values to an xterm 256 - color ID\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_speedup():\n    import os\n    import ctypes\n    from os.path import join, dirname, getmtime, exists, expanduser\n    # library = join(dirname(__file__), '_xterm256.so')\n    library = expanduser('~/.xterm256.so')\n    sauce = join(dirname(__file__), '_xterm256.c')\n    if not exists(library) or getmtime(sauce) > getmtime(library):\n        build = \"gcc -fPIC -shared -o %s %s\" % (library, sauce)\n        if (os.system(build + \" >/dev/null 2>&1\") != 0):\n            raise OSError(\"GCC error\")\n    xterm256_c = ctypes.cdll.LoadLibrary(library)\n    xterm256_c.init()\n    def xterm_to_rgb(xcolor):\n        res = xterm256_c.xterm_to_rgb_i(xcolor)\n        return ((res >> 16) & 0xFF, (res >> 8) & 0xFF, res & 0xFF)\n    return (xterm256_c.rgb_to_xterm, xterm_to_rgb)", "response": "Try to compile the C version of this module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    import optparse\n    parser = optparse.OptionParser()\n    parser.add_option(\n        \"-w\", \"--width\", dest=\"width\", type=\"int\", default=None,\n        help=(\"Width of printed image in characters.  Default: %default\"))\n    (options, args) = parser.parse_args(args=sys.argv[1:])\n    for imgpath in args:\n        for line in Image(imgpath, options.width):\n            printy(line)", "response": "Main function for the catulous - image command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce(self, colors):\n        need_reset = False\n        line = []\n        for color, items in itertools.groupby(colors):\n            if color is None:\n                if need_reset:\n                    line.append(\"\\x1b[49m\")\n                    need_reset = False\n                line.append(self.pad * len(list(items)))\n            elif color == \"EOL\":\n                if need_reset:\n                    line.append(\"\\x1b[49m\")\n                    need_reset = False\n                    yield \"\".join(line)\n                else:\n                    line.pop()\n                    yield \"\".join(line)\n                line = []\n            else:\n                need_reset = True\n                line.append(\"\\x1b[48;5;%dm%s\" % (\n                    color, self.pad * len(list(items))))", "response": "Reduces the color codes into optimized text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(self):\n        (width, height) = self.img.size\n        bgcolor = utils.term.bgcolor\n        self.img.load()\n        for y in range(height):\n            for x in range(width):\n                rgba = self.img.getpixel((x, y))\n                if len(rgba) == 4 and rgba[3] == 0:\n                    yield None\n                elif len(rgba) == 3 or rgba[3] == 255:\n                    yield xterm256.rgb_to_xterm(*rgba[:3])\n                else:\n                    color = grapefruit.Color.NewFromRgb(\n                        *[c / 255.0 for c in rgba])\n                    rgba = grapefruit.Color.AlphaBlend(color, bgcolor).rgb\n                    yield xterm256.rgb_to_xterm(\n                        *[int(c * 255.0) for c in rgba])\n            yield \"EOL\"", "response": "Yields xterm color codes for each pixel in image\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget Taskcluster credentials given a suitable `access_token` Given an OIDC `access_token` from a trusted OpenID provider, return a set of Taskcluster credentials for use on behalf of the identified user. This method is typically not called with a Taskcluster client library and does not accept Hawk credentials. The `access_token` should be given in an `Authorization` header: ``` Authorization: Bearer abc.xyz ``` The `access_token` is first verified against the named :provider, then passed to the provider's APIBuilder to retrieve a user profile. That profile is then used to generate Taskcluster credentials appropriate to the user. Note that the resulting credentials may or may not include a `certificate` property. Callers should be prepared for either alternative. The given credentials will expire in a relatively short time. Callers should monitor this expiration and refresh the credentials if necessary, by calling this endpoint again, if they have expired. This method gives output: ``v1/oidc-credentials-response.json#`` This method is ``experimental``", "response": "def oidcCredentials(self, *args, **kwargs):\n        \"\"\"\n        Get Taskcluster credentials given a suitable `access_token`\n\n        Given an OIDC `access_token` from a trusted OpenID provider, return a\n        set of Taskcluster credentials for use on behalf of the identified\n        user.\n\n        This method is typically not called with a Taskcluster client library\n        and does not accept Hawk credentials. The `access_token` should be\n        given in an `Authorization` header:\n        ```\n        Authorization: Bearer abc.xyz\n        ```\n\n        The `access_token` is first verified against the named\n        :provider, then passed to the provider's APIBuilder to retrieve a user\n        profile. That profile is then used to generate Taskcluster credentials\n        appropriate to the user. Note that the resulting credentials may or may\n        not include a `certificate` property. Callers should be prepared for either\n        alternative.\n\n        The given credentials will expire in a relatively short time. Callers should\n        monitor this expiration and refresh the credentials if necessary, by calling\n        this endpoint again, if they have expired.\n\n        This method gives output: ``v1/oidc-credentials-response.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"oidcCredentials\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist of Builds A paginated list of builds that have been run in Taskcluster. Can be filtered on various git-specific fields. This method gives output: ``v1/build-list.json#`` This method is ``experimental``", "response": "async def builds(self, *args, **kwargs):\n        \"\"\"\n        List of Builds\n\n        A paginated list of builds that have been run in\n        Taskcluster. Can be filtered on various git-specific\n        fields.\n\n        This method gives output: ``v1/build-list.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"builds\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def repository(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"repository\"], *args, **kwargs)", "response": "Get Repository Info\n\n        Returns any repository metadata that is\n        useful within Taskcluster related services.\n\n        This method gives output: ``v1/repository.json#``\n\n        This method is ``experimental``"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a status for a given changeset.", "response": "async def createStatus(self, *args, **kwargs):\n        \"\"\"\n        Post a status against a given changeset\n\n        For a given changeset (SHA) of a repository, this will attach a \"commit status\"\n        on github. These statuses are links displayed next to each revision.\n        The status is either OK (green check) or FAILURE (red cross),\n        made of a custom title and link.\n\n        This method takes input: ``v1/create-status.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"createStatus\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def email(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"email\"], *args, **kwargs)", "response": "Send an email to a specific address"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def pulse(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"pulse\"], *args, **kwargs)", "response": "This method is used to publish a message on a pulse with the given routingKey."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def resetAccessToken(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"resetAccessToken\"], *args, **kwargs)", "response": "Reset an existing accessToken"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def role(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"role\"], *args, **kwargs)", "response": "Get information about a single role"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def azureAccounts(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"azureAccounts\"], *args, **kwargs)", "response": "This method returns a list of all Azure accounts managed by Taskcluster Auth."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nauthenticates Hawk Request Validate the request signature given on input and return list of scopes that the authenticating client has. This method is used by other services that wish rely on Taskcluster credentials for authentication. This way we can use Hawk without having the secret credentials leave this service. This method takes input: ``v1/authenticate-hawk-request.json#`` This method gives output: ``v1/authenticate-hawk-response.json#`` This method is ``stable``", "response": "async def authenticateHawk(self, *args, **kwargs):\n        \"\"\"\n        Authenticate Hawk Request\n\n        Validate the request signature given on input and return list of scopes\n        that the authenticating client has.\n\n        This method is used by other services that wish rely on Taskcluster\n        credentials for authentication. This way we can use Hawk without having\n        the secret credentials leave this service.\n\n        This method takes input: ``v1/authenticate-hawk-request.json#``\n\n        This method gives output: ``v1/authenticate-hawk-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"authenticateHawk\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist hook groups This endpoint will return a list of all hook groups with at least one hook. This method gives output: ``v1/list-hook-groups-response.json#`` This method is ``stable``", "response": "async def listHookGroups(self, *args, **kwargs):\n        \"\"\"\n        List hook groups\n\n        This endpoint will return a list of all hook groups with at least one hook.\n\n        This method gives output: ``v1/list-hook-groups-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"listHookGroups\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def listHooks(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"listHooks\"], *args, **kwargs)", "response": "This endpoint returns a list of all the hook definitions within a given hook group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def hook(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"hook\"], *args, **kwargs)", "response": "Get the hook definition for the given hookGroupId and hookId"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def updateHook(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"updateHook\"], *args, **kwargs)", "response": "This endpoint updates a hook in the specified hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a hook This endpoint will remove a hook definition. This method is ``stable``", "response": "async def removeHook(self, *args, **kwargs):\n        \"\"\"\n        Delete a hook\n\n        This endpoint will remove a hook definition.\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"removeHook\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def triggerHook(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"triggerHook\"], *args, **kwargs)", "response": "Trigger a hook that creates a new task from a hook definition."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def getTriggerToken(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getTriggerToken\"], *args, **kwargs)", "response": "This method returns a unique secret token for the specified hook."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset a trigger token for a given hook", "response": "async def resetTriggerToken(self, *args, **kwargs):\n        \"\"\"\n        Reset a trigger token\n\n        Reset the token for triggering a given hook. This invalidates token that\n        may have been issued via getTriggerToken with a new token.\n\n        This method gives output: ``v1/trigger-token-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"resetTriggerToken\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntriggering a hook with a valid token.", "response": "async def triggerHookWithToken(self, *args, **kwargs):\n        \"\"\"\n        Trigger a hook with a token\n\n        This endpoint triggers a defined hook with a valid token.\n\n        The HTTP payload must match the hooks `triggerSchema`.  If it does, it is\n        provided as the `payload` property of the JSON-e context used to render the\n        task template.\n\n        This method takes input: ``v1/trigger-hook.json#``\n\n        This method gives output: ``v1/trigger-hook-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"triggerHookWithToken\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates new Worker Type Create a worker type. A worker type contains all the configuration needed for the provisioner to manage the instances. Each worker type knows which regions and which instance types are allowed for that worker type. Remember that Capacity is the number of concurrent tasks that can be run on a given EC2 resource and that Utility is the relative performance rate between different instance types. There is no way to configure different regions to have different sets of instance types so ensure that all instance types are available in all regions. This function is idempotent. Once a worker type is in the provisioner, a back ground process will begin creating instances for it based on its capacity bounds and its pending task count from the Queue. It is the worker's responsibility to shut itself down. The provisioner has a limit (currently 96hours) for all instances to prevent zombie instances from running indefinitely. The provisioner will ensure that all instances created are tagged with aws resource tags containing the provisioner id and the worker type. If provided, the secrets in the global, region and instance type sections are available using the secrets api. If specified, the scopes provided will be used to generate a set of temporary credentials available with the other secrets. This method takes input: ``http://schemas.taskcluster.net/aws-provisioner/v1/create-worker-type-request.json#`` This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/get-worker-type-response.json#`` This method is ``stable``", "response": "async def createWorkerType(self, *args, **kwargs):\n        \"\"\"\n        Create new Worker Type\n\n        Create a worker type.  A worker type contains all the configuration\n        needed for the provisioner to manage the instances.  Each worker type\n        knows which regions and which instance types are allowed for that\n        worker type.  Remember that Capacity is the number of concurrent tasks\n        that can be run on a given EC2 resource and that Utility is the relative\n        performance rate between different instance types.  There is no way to\n        configure different regions to have different sets of instance types\n        so ensure that all instance types are available in all regions.\n        This function is idempotent.\n\n        Once a worker type is in the provisioner, a back ground process will\n        begin creating instances for it based on its capacity bounds and its\n        pending task count from the Queue.  It is the worker's responsibility\n        to shut itself down.  The provisioner has a limit (currently 96hours)\n        for all instances to prevent zombie instances from running indefinitely.\n\n        The provisioner will ensure that all instances created are tagged with\n        aws resource tags containing the provisioner id and the worker type.\n\n        If provided, the secrets in the global, region and instance type sections\n        are available using the secrets api.  If specified, the scopes provided\n        will be used to generate a set of temporary credentials available with\n        the other secrets.\n\n        This method takes input: ``http://schemas.taskcluster.net/aws-provisioner/v1/create-worker-type-request.json#``\n\n        This method gives output: ``http://schemas.taskcluster.net/aws-provisioner/v1/get-worker-type-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"createWorkerType\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def updateWorkerType(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"updateWorkerType\"], *args, **kwargs)", "response": "This method is used to update the worker type definition for a specific worker type. This method is used to update the worker type definition for a specific worker type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def workerType(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"workerType\"], *args, **kwargs)", "response": "This method returns a copy of the worker type definition. This method is deprecated in favor of the new worker type definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def createSecret(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"createSecret\"], *args, **kwargs)", "response": "Create a new Secret in the secret storage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a Secret from storage", "response": "async def removeSecret(self, *args, **kwargs):\n        \"\"\"\n        Remove a Secret\n\n        Remove a secret.  After this call, a call to `getSecret` with the given\n        token will return no information.\n\n        It is very important that the consumer of a\n        secret delete the secret from storage before handing over control\n        to untrusted processes to prevent credential and/or secret leakage.\n\n        This method is ``stable``\n        \"\"\"\n\n        return await self._makeApiCall(self.funcinfo[\"removeSecret\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def getLaunchSpecs(self, *args, **kwargs):\n\n        return await self._makeApiCall(self.funcinfo[\"getLaunchSpecs\"], *args, **kwargs)", "response": "This method returns all possible launch specifications for this worker type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def makeHttpRequest(method, url, payload, headers, retries=utils.MAX_RETRIES, session=None):\n    retry = -1\n    response = None\n    implicit = False\n    if session is None:\n        implicit = True\n        session = aiohttp.ClientSession()\n\n    def cleanup():\n        if implicit:\n            loop = asyncio.get_event_loop()\n            loop.run_until_complete(session.close())\n\n    try:\n        while True:\n            retry += 1\n            # if this isn't the first retry then we sleep\n            if retry > 0:\n                snooze = float(retry * retry) / 10.0\n                log.info('Sleeping %0.2f seconds for exponential backoff', snooze)\n                await asyncio.sleep(snooze)\n\n            # Seek payload to start, if it is a file\n            if hasattr(payload, 'seek'):\n                payload.seek(0)\n\n            log.debug('Making attempt %d', retry)\n            try:\n                with async_timeout.timeout(60):\n                    response = await makeSingleHttpRequest(method, url, payload, headers, session)\n            except aiohttp.ClientError as rerr:\n                if retry < retries:\n                    log.warn('Retrying because of: %s' % rerr)\n                    continue\n                # raise a connection exception\n                raise rerr\n            except ValueError as rerr:\n                log.warn('ValueError from aiohttp: redirect to non-http or https')\n                raise rerr\n            except RuntimeError as rerr:\n                log.warn('RuntimeError from aiohttp: session closed')\n                raise rerr\n            # Handle non 2xx status code and retry if possible\n            status = response.status\n            if 500 <= status and status < 600 and retry < retries:\n                if retry < retries:\n                    log.warn('Retrying because of: %d status' % status)\n                    continue\n                else:\n                    raise exceptions.TaskclusterRestFailure(\"Unknown Server Error\", superExc=None)\n            return response\n    finally:\n        cleanup()\n    # This code-path should be unreachable\n    assert False, \"Error from last retry should have been raised!\"", "response": "Make an HTTP request and retry it until success."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resolve_font(name):\n    if os.path.exists(name):\n        return os.path.abspath(name)\n    fonts = get_font_files()\n    if name in fonts:\n        return fonts[name]\n    raise FontNotFound(\"Can't find %r :'(  Try adding it to ~/.fonts\" % name)", "response": "Turns font names into absolute filenames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_font_files():\n    roots = [\n        '/usr/share/fonts/truetype',     # where ubuntu puts fonts\n        '/usr/share/fonts',              # where fedora puts fonts\n        os.path.expanduser('~/.fonts'),  # custom user fonts\n        os.path.abspath(os.path.join(os.path.dirname(__file__), 'fonts')),\n    ]\n    result = {}\n    for root in roots:\n        for path, dirs, names in os.walk(root):\n            for name in names:\n                if name.endswith(('.ttf', '.otf')):\n                    result[name[:-4]] = os.path.join(path, name)\n    return result", "response": "Returns a list of all font files we could find"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    import optparse\n    parser = optparse.OptionParser()\n    parser.add_option(\n        \"-l\", \"--list\", dest=\"list\", action=\"store_true\", default=False,\n        help=(\"List available fonts\"))\n    parser.add_option(\n        \"-S\", \"--skew\", dest=\"skew\", type=\"int\", default=None,\n        help=(\"Apply skew effect (measured in pixels) to make it look \"\n              \"extra cool.  For example, Fabulous' logo logo is skewed \"\n              \"by 5 pixels.  Default: %default\"))\n    parser.add_option(\n        \"-C\", \"--color\", dest=\"color\", default=\"#0099ff\",\n        help=(\"Color of your text.  This can be specified as you would \"\n              \"using HTML/CSS.  Default: %default\"))\n    parser.add_option(\n        \"-B\", \"--term-color\", dest=\"term_color\", default=None,\n        help=(\"If you terminal background isn't black, please change \"\n              \"this value to the proper background so semi-transparent \"\n              \"pixels will blend properly.\"))\n    parser.add_option(\n        \"-F\", \"--font\", dest=\"font\", default='NotoSans-Bold',\n        help=(\"Name of font file, or absolute path to one. Use the --list \"\n              \"flag to see what fonts are available. Fabulous bundles the \"\n              \"NotoSans-Bold and NotoEmoji-Regular fonts, which are guaranteed \"\n              \"to work. Default: %default\"))\n    parser.add_option(\n        \"-Z\", \"--size\", dest=\"fsize\", type=\"int\", default=23,\n        help=(\"Size of font in points.  Default: %default\"))\n    parser.add_option(\n        \"-s\", \"--shadow\", dest=\"shadow\", action=\"store_true\", default=False,\n        help=(\"Size of font in points.  Default: %default\"))\n    (options, args) = parser.parse_args(args=sys.argv[1:])\n    if options.list:\n        print(\"\\n\".join(sorted(get_font_files())))\n        return\n    if options.term_color:\n        utils.term.bgcolor = options.term_color\n    text = \" \".join(args)\n    if not isinstance(text, unicode):\n        text = text.decode('utf-8')\n    for line in text.split(\"\\n\"):\n        fab_text = Text(line, skew=options.skew, color=options.color,\n                        font=options.font, fsize=options.fsize,\n                        shadow=options.shadow)\n        for chunk in fab_text:\n            printy(chunk)", "response": "Main function for the fabulous - text command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef purgeCache(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"purgeCache\"], *args, **kwargs)", "response": "Purge Worker Cache\n\n        Publish a purge-cache message to purge caches named `cacheName` with\n        `provisionerId` and `workerType` in the routing-key. Workers should\n        be listening for this message and purge caches when they see it.\n\n        This method takes input: ``v1/purge-cache-request.json#``\n\n        This method is ``stable``"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_by_step(raw):\n    # make sure it is unicode\n    delta = ord(raw[1]) - ord(raw[0])\n\n    for i in range(2, len(raw)):\n        if ord(raw[i]) - ord(raw[i-1]) != delta:\n            return False\n\n    return True", "response": "If the password is alphabet step by step."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the password is common used.", "response": "def is_common_password(raw, freq=0):\n    \"\"\"If the password is common used.\n\n    10k top passwords: https://xato.net/passwords/more-top-worst-passwords/\n    \"\"\"\n    frequent = WORDS.get(raw, 0)\n    if freq:\n        return frequent > freq\n    return bool(frequent)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the safety level of a password.", "response": "def check(raw, length=8, freq=0, min_types=3, level=STRONG):\n    \"\"\"Check the safety level of the password.\n\n    :param raw: raw text password.\n    :param length: minimal length of the password.\n    :param freq: minimum frequency.\n    :param min_types: minimum character family.\n    :param level: minimum level to validate a password.\n    \"\"\"\n    raw = to_unicode(raw)\n    if level > STRONG:\n        level = STRONG\n\n    if len(raw) < length:\n        return Strength(False, 'terrible', 'password is too short')\n\n    if is_asdf(raw) or is_by_step(raw):\n        return Strength(False, 'simple', 'password has a pattern')\n\n    if is_common_password(raw, freq=freq):\n        return Strength(False, 'simple', 'password is too common')\n\n    types = 0\n\n    if LOWER.search(raw):\n        types += 1\n\n    if UPPER.search(raw):\n        types += 1\n\n    if NUMBER.search(raw):\n        types += 1\n\n    if MARKS.search(raw):\n        types += 1\n\n    if types < 2:\n        return Strength(level <= SIMPLE, 'simple', 'password is too simple')\n\n    if types < min_types:\n        return Strength(level <= MEDIUM, 'medium',\n                        'password is good enough, but not strong')\n\n    return Strength(True, 'strong', 'password is perfect')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_request(self, url, method=\"get\", data=None, extra_headers=None):\n        attempts = 0\n\n        while attempts < 1:\n            # Authenticate first if not authenticated already\n            if not self._is_authenticated:\n                self._authenticate()\n            # Make the request and check for authentication errors\n            # This allows us to catch session timeouts for long standing connections\n            try:\n                return self._send_request(url, method, data, extra_headers)\n            except HTTPError as e:\n                if e.response.status_code == 403:\n                    logger.info(\"Authenticated session against NetMRI timed out. Retrying.\")\n                    self._is_authenticated = False\n                    attempts += 1\n                else:\n                    # re-raise other HTTP errors\n                    raise", "response": "Prepares the request and retries if issues are detected."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a given request and returns a json object containing the response", "response": "def _send_request(self, url, method=\"get\", data=None, extra_headers=None):\n        \"\"\"Performs a given request and returns a json object\n\n        Args:\n            url (str): URL of the request\n            method (str): Any of \"get\", \"post\", \"delete\"\n            data (any): Possible extra data to send with the request\n            extra_headers (dict): Possible extra headers to send along in the request\n        Returns:\n            dict\n        \"\"\"\n        headers = {'Content-type': 'application/json'}\n        if isinstance(extra_headers, dict):\n            headers.update(extra_headers)\n\n        if not data or \"password\" not in data:\n            logger.debug(\"Sending {method} request to {url} with data {data}\".format(\n                method=method.upper(), url=url, data=data)\n            )\n        r = self.session.request(method, url, headers=headers, data=data)\n        r.raise_for_status()\n        return r.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_api_version(self):\n        url = \"{base_url}/api/server_info\".format(base_url=self._base_url())\n        server_info = self._make_request(url=url, method=\"get\")\n        return server_info[\"latest_api_version\"]", "response": "Fetches the most recent API version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _authenticate(self):\n        url = \"{base_url}/api/authenticate\".format(base_url=self._base_url())\n        data = json.dumps({'username': self.username, \"password\": self.password})\n        # Bypass authentication check in make_request by using _send_request\n        logger.debug(\"Authenticating against NetMRI\")\n        self._send_request(url, method=\"post\", data=data)\n        self._is_authenticated = True", "response": "Perform an authentication against NetMRI"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the controller name for the object s type", "response": "def _controller_name(self, objtype):\n        \"\"\"Determines the controller name for the object's type\n\n        Args:\n            objtype (str): The object type\n\n        Returns:\n            A string with the controller name\n        \"\"\"\n        # would be better to use inflect.pluralize here, but would add a dependency\n        if objtype.endswith('y'):\n            return objtype[:-1] + 'ies'\n\n        if objtype[-1] in 'sx' or objtype[-2:] in ['sh', 'ch']:\n            return objtype + 'es'\n\n        if objtype.endswith('an'):\n            return objtype[:-2] + 'en'\n\n        return objtype + 's'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the URL for the specified object.", "response": "def _object_url(self, objtype, objid):\n        \"\"\"Generate the URL for the specified object\n\n        Args:\n            objtype (str): The object's type\n            objid (int): The objects ID\n\n        Returns:\n            A string containing the URL of the object\n        \"\"\"\n        return \"{base_url}/api/{api_version}/{controller}/{obj_id}\".format(\n            base_url=self._base_url(),\n            api_version=self.api_version,\n            controller=self._controller_name(objtype),\n            obj_id=objid\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the URL for the requested method", "response": "def _method_url(self, method_name):\n        \"\"\"Generate the URL for the requested method\n\n        Args:\n            method_name (str): Name of the method\n\n        Returns:\n            A string containing the URL of the method\n        \"\"\"\n        return \"{base_url}/api/{api}/{method}\".format(\n            base_url=self._base_url(),\n            api=self.api_version,\n            method=method_name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef api_request(self, method_name, params):\n        url = self._method_url(method_name)\n        data = json.dumps(params)\n        return self._make_request(url=url, method=\"post\", data=data)", "response": "Execute an arbitrary method."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries for a specific resource by ID.", "response": "def show(self, objtype, objid):\n        \"\"\"Query for a specific resource by ID\n\n        Args:\n            objtype (str): object type, e.g. 'device', 'interface'\n            objid (int): object ID (DeviceID, etc.)\n        Returns:\n            A dict with that object\n        Raises:\n            requests.exceptions.HTTPError\n        \"\"\"\n        url = self._object_url(objtype, int(objid))\n        return self._make_request(url, method=\"get\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef irc(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"irc\"], *args, **kwargs)", "response": "Post IRC Message\n\n        Post a message on IRC to a specific channel or user, or a specific user\n        on a specific channel.\n\n        Success of this API method does not imply the message was successfully\n        posted. This API method merely inserts the IRC message into a queue\n        that will be processed by a background process.\n        This allows us to re-send the message in face of connection issues.\n\n        However, if the user isn't online the message will be dropped without\n        error. We maybe improve this behavior in the future. For now just keep\n        in mind that IRC is a best-effort service.\n\n        This method takes input: ``v1/irc-request.json#``\n\n        This method is ``experimental``"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addDenylistAddress(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"addDenylistAddress\"], *args, **kwargs)", "response": "This is the base method for adding a new denylist address to the given address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete Denylisted Address Delete the specified address from the notification denylist. This method takes input: ``v1/notification-address.json#`` This method is ``experimental``", "response": "def deleteDenylistAddress(self, *args, **kwargs):\n        \"\"\"\n        Delete Denylisted Address\n\n        Delete the specified address from the notification denylist.\n\n        This method takes input: ``v1/notification-address.json#``\n\n        This method is ``experimental``\n        \"\"\"\n\n        return self._makeApiCall(self.funcinfo[\"deleteDenylistAddress\"], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, *args, **kwargs):\n\n        return self._makeApiCall(self.funcinfo[\"list\"], *args, **kwargs)", "response": "This method returns a list of all the denylisted addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clientCreated(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'client-created',\n            'name': 'clientCreated',\n            'routingKey': [\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/client-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "This is the client Created Messages\n            Message that a new client has been created. This is a simple function that returns a new message that has been created."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef roleUpdated(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'role-updated',\n            'name': 'roleUpdated',\n            'routingKey': [\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/role-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "Role Updated Messages\n            Role Updated Messages\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef roleDeleted(self, *args, **kwargs):\n\n        ref = {\n            'exchange': 'role-deleted',\n            'name': 'roleDeleted',\n            'routingKey': [\n                {\n                    'multipleWords': True,\n                    'name': 'reserved',\n                },\n            ],\n            'schema': 'v1/role-message.json#',\n        }\n        return self._makeTopicExchange(ref, *args, **kwargs)", "response": "Role Deleted Messages\n\n        Message that a new role has been deleted.\n\n        This exchange outputs: ``v1/role-message.json#``This exchange takes the following keys:\n\n         * reserved: Space reserved for future routing-key entries, you should always match this entry with `#`. As automatically done by our tooling, if not specified."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dimensions(self):\n        try:\n            call = fcntl.ioctl(self.termfd, termios.TIOCGWINSZ, \"\\000\" * 8)\n        except IOError:\n            return (79, 40)\n        else:\n            height, width = struct.unpack(\"hhhh\", call)[:2]\n            return (width, height)", "response": "Returns terminal dimensions as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sp_msg(cmd, pipe=None, data=None):\n    msg = [SP_HEADER, cmd]\n    if pipe is not None:\n        msg.append(pipe)\n    if data is not None:\n        msg.append(data)\n    return msg", "response": "Produces skypipe protocol multipart message"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stream_skypipe_output(endpoint, name=None):\n    name = name or ''\n    socket = ctx.socket(zmq.DEALER)\n    socket.connect(endpoint)\n    try:\n        socket.send_multipart(sp_msg(SP_CMD_LISTEN, name))\n\n        while True:\n            msg = socket.recv_multipart()\n            try:\n                data = parse_skypipe_data_stream(msg, name)\n                if data:\n                    yield data\n            except EOFError:\n                raise StopIteration()\n\n    finally:\n        socket.send_multipart(sp_msg(SP_CMD_UNLISTEN, name))\n        socket.close()", "response": "Generator for reading skypipe data from the given endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_skypipe_data_stream(msg, for_pipe):\n    header = str(msg.pop(0))\n    command = str(msg.pop(0))\n    pipe_name = str(msg.pop(0))\n    data = str(msg.pop(0))\n    if header != SP_HEADER: return\n    if pipe_name != for_pipe: return\n    if command != SP_CMD_DATA: return\n    if data == SP_DATA_EOF:\n        raise EOFError()\n    else:\n        return data", "response": "May return data from skypipe message or raises EOFError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a context manager for streaming data into skypipe", "response": "def skypipe_input_stream(endpoint, name=None):\n    \"\"\"Returns a context manager for streaming data into skypipe\"\"\"\n    name = name or ''\n    class context_manager(object):\n        def __enter__(self):\n            self.socket = ctx.socket(zmq.DEALER)\n            self.socket.connect(endpoint)\n            return self\n\n        def send(self, data):\n            data_msg = sp_msg(SP_CMD_DATA, name, data)\n            self.socket.send_multipart(data_msg)\n\n        def __exit__(self, *args, **kwargs):\n            eof_msg = sp_msg(SP_CMD_DATA, name, SP_DATA_EOF)\n            self.socket.send_multipart(eof_msg)\n            self.socket.close()\n\n    return context_manager()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stream_stdin_lines():\n    stdin = os.fdopen(sys.stdin.fileno(), 'r', 0)\n    while True:\n        line = stdin.readline()\n        if line:\n            yield line\n        else:\n            break", "response": "Generator for unbuffered lines reading from STDIN"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the skypipe client", "response": "def run(endpoint, name=None):\n    \"\"\"Runs the skypipe client\"\"\"\n    try:\n        if os.isatty(0):\n            # output mode\n            for data in stream_skypipe_output(endpoint, name):\n                sys.stdout.write(data)\n                sys.stdout.flush()\n\n        else:\n            # input mode\n            with skypipe_input_stream(endpoint, name) as stream:\n                for line in stream_stdin_lines():\n                    stream.send(line)\n\n    except KeyboardInterrupt:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates the time inversion of the time range.", "response": "def validate_time_inversion(self):\n        \"\"\"\n        Check time inversion of the time range.\n\n        :raises ValueError:\n            If |attr_start_datetime| is\n            bigger than |attr_end_datetime|.\n        :raises TypeError:\n            Any one of |attr_start_datetime| and |attr_end_datetime|,\n            or both is inappropriate datetime value.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange(\"2015-03-22T10:10:00+0900\", \"2015-03-22T10:00:00+0900\")\n                try:\n                    time_range.validate_time_inversion()\n                except ValueError:\n                    print \"time inversion\"\n        :Output:\n            .. parsed-literal::\n\n                time inversion\n        \"\"\"\n\n        if not self.is_set():\n            # for python2/3 compatibility\n            raise TypeError\n\n        if self.start_datetime > self.end_datetime:\n            raise ValueError(\n                \"time inversion found: {:s} > {:s}\".format(\n                    str(self.start_datetime), str(self.end_datetime)\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning |str| formatted with |attr_start_datetime| as formatted with |attr_start_time_format|.", "response": "def get_start_time_str(self):\n        \"\"\"\n        :return:\n            |attr_start_datetime| as |str| formatted with\n            |attr_start_time_format|.\n            Return |NaT| if the invalid value or the invalid format.\n        :rtype: str\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange(\"2015-03-22T10:00:00+0900\", \"2015-03-22T10:10:00+0900\")\n                print(time_range.get_start_time_str())\n                time_range.start_time_format = \"%Y/%m/%d %H:%M:%S\"\n                print(time_range.get_start_time_str())\n        :Output:\n            .. parsed-literal::\n\n                2015-03-22T10:00:00+0900\n                2015/03/22 10:00:00\n        \"\"\"\n\n        try:\n            return self.start_datetime.strftime(self.start_time_format)\n        except AttributeError:\n            return self.NOT_A_TIME_STR"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_end_time_str(self):\n\n        try:\n            return self.end_datetime.strftime(self.end_time_format)\n        except AttributeError:\n            return self.NOT_A_TIME_STR", "response": "Return |str| formatted with |attr_end_datetime|."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the start time of the time range.", "response": "def set_start_datetime(self, value, timezone=None):\n        \"\"\"\n        Set the start time of the time range.\n\n        :param value: |param_start_datetime|\n        :type value: |datetime|/|str|\n        :raises ValueError: If the value is invalid as a |datetime| value.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange()\n                print(time_range)\n                time_range.set_start_datetime(\"2015-03-22T10:00:00+0900\")\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                NaT - NaT\n                2015-03-22T10:00:00+0900 - NaT\n        \"\"\"\n\n        if value is None:\n            self.__start_datetime = None\n            return\n\n        try:\n            self.__start_datetime = typepy.type.DateTime(\n                value, strict_level=typepy.StrictLevel.MIN, timezone=timezone\n            ).convert()\n        except typepy.TypeConversionError as e:\n            raise ValueError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the end time of the time range.", "response": "def set_end_datetime(self, value, timezone=None):\n        \"\"\"\n        Set the end time of the time range.\n\n        :param datetime.datetime/str value: |param_end_datetime|\n        :raises ValueError: If the value is invalid as a |datetime| value.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange()\n                print(time_range)\n                time_range.set_end_datetime(\"2015-03-22T10:10:00+0900\")\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                NaT - NaT\n                NaT - 2015-03-22T10:10:00+0900\n        \"\"\"\n\n        if value is None:\n            self.__end_datetime = None\n            return\n\n        try:\n            self.__end_datetime = typepy.type.DateTime(\n                value, strict_level=typepy.StrictLevel.MIN, timezone=timezone\n            ).convert()\n        except typepy.TypeConversionError as e:\n            raise ValueError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets start and end times for the current object.", "response": "def set_time_range(self, start, end):\n        \"\"\"\n        :param datetime.datetime/str start: |param_start_datetime|\n        :param datetime.datetime/str end: |param_end_datetime|\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange()\n                print(time_range)\n                time_range.set_time_range(\"2015-03-22T10:00:00+0900\", \"2015-03-22T10:10:00+0900\")\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                NaT - NaT\n                2015-03-22T10:00:00+0900 - 2015-03-22T10:10:00+0900\n        \"\"\"\n\n        self.set_start_datetime(start)\n        self.set_end_datetime(end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator object that yields the current time in the sequence of ISO - 8601 ISO 8601 entries.", "response": "def range(self, step):\n        \"\"\"\n        Return an iterator object.\n\n        :param step: Step of iteration.\n        :type step: |timedelta|/dateutil.relativedelta.relativedelta\n        :return: iterator\n        :rtype: iterator\n\n        :Sample Code:\n            .. code:: python\n\n                import datetime\n                from datetimerange import DateTimeRange\n\n                time_range = DateTimeRange(\"2015-01-01T00:00:00+0900\", \"2015-01-04T00:00:00+0900\")\n                for value in time_range.range(datetime.timedelta(days=1)):\n                    print(value)\n        :Output:\n            .. parsed-literal::\n\n                2015-01-01 00:00:00+09:00\n                2015-01-02 00:00:00+09:00\n                2015-01-03 00:00:00+09:00\n                2015-01-04 00:00:00+09:00\n        \"\"\"\n\n        if self.__compare_timedelta(step, 0) == 0:\n            raise ValueError(\"step must be not zero\")\n\n        is_inversion = False\n        try:\n            self.validate_time_inversion()\n        except ValueError:\n            is_inversion = True\n\n        if not is_inversion:\n            if self.__compare_timedelta(step, seconds=0) < 0:\n                raise ValueError(\"invalid step: expect greater than 0, actual={}\".format(step))\n        else:\n            if self.__compare_timedelta(step, seconds=0) > 0:\n                raise ValueError(\"invalid step: expect less than 0, actual={}\".format(step))\n\n        current_datetime = self.start_datetime\n        while current_datetime <= self.end_datetime:\n            yield current_datetime\n            current_datetime = current_datetime + step"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef intersection(self, x):\n\n        self.validate_time_inversion()\n        x.validate_time_inversion()\n\n        if any([x.start_datetime in self, self.start_datetime in x]):\n            start_datetime = max(self.start_datetime, x.start_datetime)\n            end_datetime = min(self.end_datetime, x.end_datetime)\n        else:\n            start_datetime = None\n            end_datetime = None\n\n        return DateTimeRange(\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            start_time_format=self.start_time_format,\n            end_time_format=self.end_time_format,\n        )", "response": "Return a new time range that overlaps the input and the current time range."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntruncate the entire time of the current object.", "response": "def truncate(self, percentage):\n        \"\"\"\n        Truncate ``percentage`` / 2 [%] of whole time from first and last time.\n\n        :param float percentage: Percentage of truncate.\n\n        :Sample Code:\n            .. code:: python\n\n                from datetimerange import DateTimeRange\n                time_range = DateTimeRange(\n                    \"2015-03-22T10:00:00+0900\", \"2015-03-22T10:10:00+0900\")\n                time_range.is_output_elapse = True\n                print(time_range)\n                time_range.truncate(10)\n                print(time_range)\n        :Output:\n            .. parsed-literal::\n\n                2015-03-22T10:00:00+0900 - 2015-03-22T10:10:00+0900 (0:10:00)\n                2015-03-22T10:00:30+0900 - 2015-03-22T10:09:30+0900 (0:09:00)\n        \"\"\"\n\n        self.validate_time_inversion()\n\n        if percentage < 0:\n            raise ValueError(\"discard_percent must be greater or equal to zero: \" + str(percentage))\n\n        if percentage == 0:\n            return\n\n        discard_time = self.timedelta // int(100) * int(percentage / 2)\n\n        self.__start_datetime += discard_time\n        self.__end_datetime -= discard_time"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for(text, finish=None, io=None):\n    if finish:\n        finish.set()\n        time.sleep(0.1) # threads, sigh\n    if not io:\n        io = sys.stdout\n    finish = threading.Event()\n    io.write(text)\n    def _wait():\n        while not finish.is_set():\n            io.write('.')\n            io.flush()\n            finish.wait(timeout=1)\n        io.write('\\n')\n    threading.Thread(target=_wait).start()\n    return finish", "response": "Displays dots until the event is set"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup_endpoint(cli):\n    url = '/applications/{0}/environment'.format(APPNAME)\n    environ = cli.user.get(url).item\n    port = environ['DOTCLOUD_SATELLITE_ZMQ_PORT']\n    host = socket.gethostbyname(environ['DOTCLOUD_SATELLITE_ZMQ_HOST'])\n    return \"tcp://{0}:{1}\".format(host, port)", "response": "Looks up the application endpoint from dotcloud"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_dotcloud_account(cli):\n    client = RESTClient(endpoint=cli.client.endpoint)\n    client.authenticator = NullAuth()\n    urlmap = client.get('/auth/discovery').item\n    username = cli.prompt('dotCloud email')\n    password = cli.prompt('Password', noecho=True)\n    credential = {'token_url': urlmap.get('token'),\n        'key': CLIENT_KEY, 'secret': CLIENT_SECRET}\n    try:\n        token = cli.authorize_client(urlmap.get('token'), credential, username, password)\n    except Exception as e:\n        cli.die('Username and password do not match. Try again.')\n    token['url'] = credential['token_url']\n    config = GlobalConfig()\n    config.data = {'token': token}\n    config.save()\n    cli.global_config = GlobalConfig()  # reload\n    cli.setup_auth()\n    cli.get_keys()", "response": "Sets up the user and password for dotcloud"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup(cli):\n    if not cli.global_config.loaded:\n        setup_dotcloud_account(cli)\n    discover_satellite(cli)\n    cli.success(\"Skypipe is ready for action\")", "response": "Setup skypipe for the current project"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking to make sure a satellite exists and returns endpoint", "response": "def discover_satellite(cli, deploy=True, timeout=5):\n    \"\"\"Looks to make sure a satellite exists, returns endpoint\n\n    First makes sure we have dotcloud account credentials. Then it looks\n    up the environment for the satellite app. This will contain host and\n    port to construct an endpoint. However, if app doesn't exist, or\n    endpoint does not check out, we call `launch_satellite` to deploy,\n    which calls `discover_satellite` again when finished. Ultimately we\n    return a working endpoint. If deploy is False it will not try to\n    deploy.\n    \"\"\"\n    if not cli.global_config.loaded:\n        cli.die(\"Please setup skypipe by running `skypipe --setup`\")\n\n    try:\n        endpoint = lookup_endpoint(cli)\n        ok = client.check_skypipe_endpoint(endpoint, timeout)\n        if ok:\n            return endpoint\n        else:\n            return launch_satellite(cli) if deploy else None\n    except (RESTAPIError, KeyError):\n        return launch_satellite(cli) if deploy else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlaunching a new satellite app over any existing app", "response": "def launch_satellite(cli):\n    \"\"\"Deploys a new satellite app over any existing app\"\"\"\n\n    cli.info(\"Launching skypipe satellite:\")\n\n    finish = wait_for(\"    Pushing to dotCloud\")\n\n    # destroy any existing satellite\n    destroy_satellite(cli)\n\n    # create new satellite app\n    url = '/applications'\n    try:\n        cli.user.post(url, {\n            'name': APPNAME,\n            'flavor': 'sandbox'\n            })\n    except RESTAPIError as e:\n        if e.code == 409:\n            cli.die('Application \"{0}\" already exists.'.format(APPNAME))\n        else:\n            cli.die('Creating application \"{0}\" failed: {1}'.format(APPNAME, e))\n    class args: application = APPNAME\n    #cli._connect(args)\n\n    # push satellite code\n    protocol = 'rsync'\n    url = '/applications/{0}/push-endpoints{1}'.format(APPNAME, '')\n    endpoint = cli._select_endpoint(cli.user.get(url).items, protocol)\n    class args: path = satellite_path\n    cli.push_with_rsync(args, endpoint)\n\n    # tell dotcloud to deploy, then wait for it to finish\n    revision = None\n    clean = False\n    url = '/applications/{0}/deployments'.format(APPNAME)\n    response = cli.user.post(url, {'revision': revision, 'clean': clean})\n    deploy_trace_id = response.trace_id\n    deploy_id = response.item['deploy_id']\n\n\n    original_stdout = sys.stdout\n\n    finish = wait_for(\"    Waiting for deployment\", finish, original_stdout)\n\n    try:\n        sys.stdout = StringIO()\n        res = cli._stream_deploy_logs(APPNAME, deploy_id,\n                deploy_trace_id=deploy_trace_id, follow=True)\n        if res != 0:\n            return res\n    except KeyboardInterrupt:\n        cli.error('You\\'ve closed your log stream with Ctrl-C, ' \\\n            'but the deployment is still running in the background.')\n        cli.error('If you aborted because of an error ' \\\n            '(e.g. the deployment got stuck), please e-mail\\n' \\\n            'support@dotcloud.com and mention this trace ID: {0}'\n            .format(deploy_trace_id))\n        cli.error('If you want to continue following your deployment, ' \\\n                'try:\\n{0}'.format(\n                    cli._fmt_deploy_logs_command(deploy_id)))\n        cli.die()\n    except RuntimeError:\n        # workaround for a bug in the current dotcloud client code\n        pass\n    finally:\n        sys.stdout = original_stdout\n\n    finish = wait_for(\"    Satellite coming online\", finish)\n\n    endpoint = lookup_endpoint(cli)\n    ok = client.check_skypipe_endpoint(endpoint, 120)\n   \n    finish.set()\n    time.sleep(0.1) # sigh, threads\n\n    if ok:\n        return endpoint\n    else:\n        cli.die(\"Satellite failed to come online\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a byte string representation of the given string.", "response": "def bytes_(s, encoding='utf-8', errors='strict'):\n    \"\"\" If ``s`` is an instance of ``text_type``, return\n    ``s.encode(encoding, errors)``, otherwise return ``s``\"\"\"\n    if isinstance(s, text_type):  # pragma: no cover\n        return s.encode(encoding, errors)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall the pg_dump command to create a db", "response": "def pg_backup(self, pg_dump_exe='pg_dump', exclude_schema=None):\n        \"\"\"Call the pg_dump command to create a db backup\n\n        Parameters\n        ----------\n        pg_dump_exe: str\n            the pg_dump command path\n        exclude_schema: str[]\n            list of schemas to be skipped\n        \"\"\"\n\n        command = [\n            pg_dump_exe, '-Fc', '-f', self.file,\n            'service={}'.format(self.pg_service)\n            ]\n        if exclude_schema:\n            command.append(' '.join(\"--exclude-schema={}\".format(schema) for schema in exclude_schema))\n\n        subprocess.check_output(command, stderr=subprocess.STDOUT)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling the pg_restore command to restore a db", "response": "def pg_restore(self, pg_restore_exe='pg_restore', exclude_schema=None):\n        \"\"\"Call the pg_restore command to restore a db backup\n\n        Parameters\n        ----------\n        pg_restore_exe: str\n            the pg_restore command path\n        \"\"\"\n\n        command = [\n            pg_restore_exe, '-d',\n            'service={}'.format(self.pg_service),\n            '--no-owner'\n            ]\n\n        if exclude_schema:\n            exclude_schema_available = False\n            try:\n                pg_version = subprocess.check_output(['pg_restore','--version'])\n                pg_version = str(pg_version).replace('\\\\n', '').replace(\"'\", '').split(' ')[-1]\n                exclude_schema_available = LooseVersion(pg_version) >= LooseVersion(\"10.0\")\n            except subprocess.CalledProcessError as e:\n                print(\"*** Could not get pg_restore version:\\n\", e.stderr)\n            if exclude_schema_available:\n                command.append(' '.join(\"--exclude-schema={}\".format(schema) for schema in exclude_schema))\n        command.append(self.file)\n\n        try:\n            subprocess.check_output(command)\n        except subprocess.CalledProcessError as e:\n            print(\"*** pg_restore failed:\\n\", command, '\\n', e.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exists_table_upgrades(self):\n\n        query = \"\"\"\n            SELECT EXISTS (\n            SELECT 1\n            FROM   information_schema.tables\n            WHERE  table_schema = '{}'\n            AND    table_name = '{}'\n            );\n        \"\"\".format(self.upgrades_table[:self.upgrades_table.index('.')],\n                   self.upgrades_table[self.upgrades_table.index('.')+1:])\n\n        self.cursor.execute(query)\n        return self.cursor.fetchone()[0]", "response": "Return if the upgrades table exists Returns False if the upgrades table don t exists Returns True if the upgrades table exists False if the upgrades table doesn t exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_delta_files(self):\n        files = [(d, f) for d in self.dirs for f in listdir(d) if isfile(join(d, f))]\n\n        deltas = OrderedDict()\n        for d, f in files:\n            file_ = join(d, f)\n\n            if not Delta.is_valid_delta_name(file_):\n                continue\n\n            delta = Delta(file_)\n\n            if d not in deltas:\n                deltas[d] = []\n            deltas[d].append(delta)\n\n        # sort delta objects in each bucket\n        for d in deltas:\n            deltas[d].sort(key=lambda x: (x.get_version(), x.get_priority(), x.get_name()))\n\n        return deltas", "response": "Search for delta files and return a dict of Delta objects keyed by directory names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __run_delta_sql(self, delta):\n\n        self.__run_sql_file(delta.get_file())\n        self.__update_upgrades_table(delta)", "response": "Execute the delta sql file on the database"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __run_delta_py(self, delta):\n\n        self.__run_py_file(delta.get_file(), delta.get_name())\n        self.__update_upgrades_table(delta)", "response": "Execute the delta py file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __run_pre_all(self):\n\n        # if the list of delta dirs is [delta1, delta2] the pre scripts of delta2 are\n        # executed before the pre scripts of delta1\n\n        for d in reversed(self.dirs):\n            pre_all_py_path = os.path.join(d, 'pre-all.py')\n            if os.path.isfile(pre_all_py_path):\n                print('     Applying pre-all.py...', end=' ')\n                self.__run_py_file(pre_all_py_path, 'pre-all')\n                print('OK')\n\n            pre_all_sql_path = os.path.join(d, 'pre-all.sql')\n            if os.path.isfile(pre_all_sql_path):\n                print('     Applying pre-all.sql...', end=' ')\n                self.__run_sql_file(pre_all_sql_path)\n                print('OK')", "response": "Execute the pre - all. py and pre - all. sql files if they exist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the post - all. py and post - all. sql files if they exist.", "response": "def __run_post_all(self):\n        \"\"\"Execute the post-all.py and post-all.sql files if they exist\"\"\"\n\n        # if the list of delta dirs is [delta1, delta2] the post scripts of delta1 are\n        # executed before the post scripts of delta2\n\n        for d in self.dirs:\n            post_all_py_path = os.path.join(d, 'post-all.py')\n            if os.path.isfile(post_all_py_path):\n                print('     Applying post-all.py...', end=' ')\n                self.__run_py_file(post_all_py_path, 'post-all')\n                print('OK')\n\n            post_all_sql_path = os.path.join(d, 'post-all.sql')\n            if os.path.isfile(post_all_sql_path):\n                print('     Applying post-all.sql...', end=' ')\n                self.__run_sql_file(post_all_sql_path)\n                print('OK')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __run_sql_file(self, filepath):\n\n        with open(filepath, 'r') as delta_file:\n            sql = delta_file.read()\n            if self.variables:\n                self.cursor.execute(sql, self.variables)\n            else:\n                self.cursor.execute(sql)\n            self.connection.commit()", "response": "Execute the sql file at the passed path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the python file at the passed path and return the ID of the object that was executed", "response": "def __run_py_file(self, filepath, module_name):\n        \"\"\"Execute the python file at the passed path\n\n        Parameters\n        ----------\n        filepath: str\n            the path of the file to execute\n        module_name: str\n            the name of the python module\n            \"\"\"\n\n        # Import the module\n        spec = importlib.util.spec_from_file_location(module_name, filepath)\n        delta_py = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(delta_py)\n\n        # Get the python file's directory path\n        # Note: we add a separator for backward compatibility, as existing DeltaPy subclasses\n        # may assume that delta_dir ends with a separator\n        dir_ = dirname(filepath) + os.sep\n\n        # Search for subclasses of DeltaPy\n        for name in dir(delta_py):\n            obj = getattr(delta_py, name)\n            if inspect.isclass(obj) and not obj == DeltaPy and issubclass(\n                    obj, DeltaPy):\n\n                delta_py_inst = obj(\n                    self.current_db_version(), dir_, self.dirs, self.pg_service,\n                    self.upgrades_table, variables=self.variables)\n                delta_py_inst.run()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting info about found delta files and about already made upgrades", "response": "def show_info(self):\n        \"\"\"Print info about found delta files and about already made upgrades\"\"\"\n\n        deltas = self.__get_delta_files()\n\n        table = [['Version', 'Name', 'Type', 'Status']]\n\n        for dir_ in deltas:\n            print('delta files in dir: ', dir_)\n\n            for delta in deltas[dir_]:\n                line = [delta.get_version(), delta.get_name()]\n                if delta.get_type() == DeltaType.PRE_PYTHON:\n                    line.append('pre py')\n                elif delta.get_type() == DeltaType.PRE_SQL:\n                    line.append('pre sql')\n                elif delta.get_type() == DeltaType.PYTHON:\n                    line.append('delta py')\n                elif delta.get_type() == DeltaType.SQL:\n                    line.append('delta sql')\n                elif delta.get_type() == DeltaType.POST_PYTHON:\n                    line.append('post py')\n                elif delta.get_type() == DeltaType.POST_SQL:\n                    line.append('post sql')\n\n                if self.__is_applied(delta):\n                    line.append('Applied')\n                else:\n                    line.append('Pending')\n\n                table.append(line)\n\n        self.__print_table(table)\n\n        print('')\n        print('Applied upgrades in database')\n\n        query = \"\"\"SELECT\n                version,\n                description,\n                type,\n                installed_by,\n                installed_on,\n                success\n                FROM {}\n                \"\"\".format(self.upgrades_table)\n\n        self.cursor.execute(query)\n        records = self.cursor.fetchall()\n\n        table = [['Version', 'Name', 'Type', 'Installed by', 'Installed on',\n                  'Status']]\n\n        for i in records:\n            line = [str(i[0]), str(i[1])]\n            delta_type = i[2]\n            if delta_type == 0:\n                line.append('baseline')\n            elif delta_type == DeltaType.PRE_PYTHON:\n                line.append('pre py')\n            elif delta_type == DeltaType.PRE_SQL:\n                line.append('pre sql')\n            elif delta_type == DeltaType.PYTHON:\n                line.append('delta py')\n            elif delta_type == DeltaType.SQL:\n                line.append('delta sql')\n            elif delta_type == DeltaType.POST_PYTHON:\n                line.append('post py')\n            elif delta_type == DeltaType.POST_SQL:\n                line.append('post sql')\n\n            line.append(str(i[3]))\n            line.append(str(i[4]))\n\n            success = str(i[5])\n            if success == 'True':\n                line.append('Success')\n            else:\n                line.append('Failed')\n\n            table.append(line)\n\n        self.__print_table(table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a list in tabular format Taxonomy", "response": "def __print_table(table):\n        \"\"\"Print a list in tabular format\n        Based on https://stackoverflow.com/a/8356620\"\"\"\n\n        col_width = [max(len(x) for x in col) for col in zip(*table)]\n        print(\"| \" + \" | \".join(\"{:{}}\".format(x, col_width[i])\n                                for i, x in enumerate(table[0])) + \" |\")\n        print(\"| \" + \" | \".join(\"{:{}}\".format('-' * col_width[i], col_width[i])\n                                for i, x in enumerate(table[0])) + \" |\")\n        for line in table[1:]:\n            print(\"| \" + \" | \".join(\"{:{}}\".format(x, col_width[i])\n                                    for i, x in enumerate(line)) + \" |\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __is_applied(self, delta):\n\n        query = \"\"\"\n        SELECT id FROM {}\n        WHERE version = '{}'\n            AND checksum = '{}'\n            AND success = 'TRUE'\n        \"\"\".format(\n            self.upgrades_table, delta.get_version(), delta.get_checksum())\n\n        self.cursor.execute(query)\n        if not self.cursor.fetchone():\n            return False\n        else:\n            return True", "response": "Verifies if the delta file is already applied on the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __update_upgrades_table(self, delta):\n\n        query = \"\"\"\n        INSERT INTO {} (\n            --id,\n            version,\n            description,\n            type,\n            script,\n            checksum,\n            installed_by,\n            --installed_on,\n            execution_time,\n            success\n        ) VALUES(\n            '{}',\n            '{}',\n            {},\n            '{}',\n            '{}',\n            '{}',\n            1,\n            TRUE\n        ) \"\"\".format(\n            self.upgrades_table, delta.get_version(), delta.get_name(),\n            delta.get_type(), delta.get_file(), delta.get_checksum(),\n            self.__get_dbuser())\n\n        self.cursor.execute(query)\n        self.connection.commit()", "response": "Adds a new record to the upgrades table about the applied delta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the upgrades table", "response": "def create_upgrades_table(self):\n        \"\"\"Create the upgrades information table\"\"\"\n\n        query = \"\"\"CREATE TABLE IF NOT EXISTS {}\n                (\n                id serial NOT NULL,\n                version character varying(50),\n                description character varying(200) NOT NULL,\n                type integer NOT NULL,\n                script character varying(1000) NOT NULL,\n                checksum character varying(32) NOT NULL,\n                installed_by character varying(100) NOT NULL,\n                installed_on timestamp without time zone NOT NULL DEFAULT now(),\n                execution_time integer NOT NULL,\n                success boolean NOT NULL,\n                PRIMARY KEY (id)\n                )\n        \"\"\".format(self.upgrades_table)\n\n        self.cursor.execute(query)\n        self.connection.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_baseline(self, version):\n        pattern = re.compile(r\"^\\d+\\.\\d+\\.\\d+$\")\n        if not re.match(pattern, version):\n            raise ValueError('Wrong version format')\n\n        query = \"\"\"\n                INSERT INTO {} (\n                    version,\n                    description,\n                    type,\n                    script,\n                    checksum,\n                    installed_by,\n                    execution_time,\n                    success\n                ) VALUES(\n                    '{}',\n                    '{}',\n                    {},\n                    '{}',\n                    '{}',\n                    '{}',\n                    1,\n                    TRUE\n                ) \"\"\".format(self.upgrades_table, version, 'baseline', 0,\n                             '', '', self.__get_dbuser())\n        self.cursor.execute(query)\n        self.connection.commit()", "response": "Set the baseline into the creation information table\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_valid_delta_name(file):\n        filename = basename(file)\n        pattern = re.compile(Delta.FILENAME_PATTERN)\n        if re.match(pattern, filename):\n            return True\n        return False", "response": "Return if a file has a valid delta file name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_checksum(self):\n        with open(self.file, 'rb') as f:\n            cs = md5(f.read()).hexdigest()\n        return cs", "response": "Return the md5 checksum of the delta file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the type of the delta file.", "response": "def get_type(self):\n        \"\"\"Return the type of the delta file.\n\n        Returns\n        -------\n        type: int\n        \"\"\"\n\n        ext = self.match.group(5)\n\n        if ext == 'pre.py':\n            return DeltaType.PRE_PYTHON\n        elif ext == 'pre.sql':\n            return DeltaType.PRE_SQL\n        elif ext == 'py':\n            return DeltaType.PYTHON\n        elif ext == 'sql':\n            return DeltaType.SQL\n        elif ext == 'post.py':\n            return DeltaType.POST_PYTHON\n        elif ext == 'post.sql':\n            return DeltaType.POST_SQL"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the priority of the file in the current language.", "response": "def get_priority(self) -> int:\n        \"\"\"\n        Rerturns the priority of the file from 1 (pre) to 3 (post)\n        :return: the priority\n        \"\"\"\n        dtype = self.get_type()\n        if dtype & DeltaType.PRE:\n            return 1\n        elif dtype & DeltaType.POST:\n            return 3\n        else:\n            return 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun all the checks functions.", "response": "def run_checks(self):\n        \"\"\"Run all the checks functions.\n\n            Returns\n            -------\n            bool\n                True if all the checks are true\n                False otherwise\n            dict\n                Dictionary of lists of differences\n        \"\"\"\n\n        result = True\n        differences_dict = {}\n\n        if 'tables' not in self.ignore_list:\n            tmp_result, differences_dict['tables'] = self.check_tables()\n            result = False if not tmp_result else result\n        if 'columns' not in self.ignore_list:\n            tmp_result, differences_dict['columns'] = self.check_columns(\n                'views' not in self.ignore_list)\n            result = False if not tmp_result else result\n        if 'constraints' not in self.ignore_list:\n            tmp_result, differences_dict['constraints'] = \\\n                self.check_constraints()\n            result = False if not tmp_result else result\n        if 'views' not in self.ignore_list:\n            tmp_result, differences_dict['views'] = self.check_views()\n            result = False if not tmp_result else result\n        if 'sequences' not in self.ignore_list:\n            tmp_result, differences_dict['sequences'] = self.check_sequences()\n            result = False if not tmp_result else result\n        if 'indexes' not in self.ignore_list:\n            tmp_result, differences_dict['indexes'] = self.check_indexes()\n            result = False if not tmp_result else result\n        if 'triggers' not in self.ignore_list:\n            tmp_result, differences_dict['triggers'] = self.check_triggers()\n            result = False if not tmp_result else result\n        if 'functions' not in self.ignore_list:\n            tmp_result, differences_dict['functions'] = self.check_functions()\n            result = False if not tmp_result else result\n        if 'rules' not in self.ignore_list:\n            tmp_result, differences_dict['rules'] = self.check_rules()\n            result = False if not tmp_result else result\n        if self.verbose_level == 0:\n            differences_dict = None\n        return result, differences_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_columns(self, check_views=True):\n        if check_views:\n            query = \"\"\"WITH table_list AS (\n                SELECT table_schema, table_name\n                FROM information_schema.tables\n                WHERE table_schema NOT IN {}\n                    AND table_schema NOT LIKE 'pg\\_%'\n                ORDER BY table_schema,table_name\n                )\n                SELECT isc.table_schema, isc.table_name, column_name,\n                    column_default, is_nullable, data_type,\n                    character_maximum_length::text, numeric_precision::text,\n                    numeric_precision_radix::text, datetime_precision::text\n                FROM information_schema.columns isc,\n                table_list tl\n                WHERE isc.table_schema = tl.table_schema\n                    AND isc.table_name = tl.table_name\n                ORDER BY isc.table_schema, isc.table_name, column_name\n                \"\"\".format(self.exclude_schema)\n\n        else:\n            query = \"\"\"WITH table_list AS (\n                SELECT table_schema, table_name\n                FROM information_schema.tables\n                WHERE table_schema NOT IN {}\n                    AND table_schema NOT LIKE 'pg\\_%'\n                    AND table_type NOT LIKE 'VIEW'\n                ORDER BY table_schema,table_name\n                )\n                SELECT isc.table_schema, isc.table_name, column_name,\n                    column_default, is_nullable, data_type,\n                    character_maximum_length::text, numeric_precision::text,\n                    numeric_precision_radix::text, datetime_precision::text\n                FROM information_schema.columns isc,\n                table_list tl\n                WHERE isc.table_schema = tl.table_schema\n                    AND isc.table_name = tl.table_name\n                ORDER BY isc.table_schema, isc.table_name, column_name\n                \"\"\".format(self.exclude_schema)\n\n        return self.__check_equals(query)", "response": "Check if the columns in all tables and views are equals."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the rules are equal.", "response": "def check_rules(self):\n        \"\"\"Check if the rules are equals.\n\n            Returns\n            -------\n            bool\n                True if the rules are the same\n                False otherwise\n            list\n                A list with the differences\n        \"\"\"\n        query = \"\"\"\n        select n.nspname as rule_schema,\n        c.relname as rule_table,\n        case r.ev_type\n            when '1' then 'SELECT'\n            when '2' then 'UPDATE'\n            when '3' then 'INSERT'\n            when '4' then 'DELETE'\n            else 'UNKNOWN'\n        end as rule_event\n        from pg_rewrite r\n        join pg_class c on r.ev_class = c.oid\n        left join pg_namespace n on n.oid = c.relnamespace\n        left join pg_description d on r.oid = d.objoid\n        WHERE n.nspname NOT IN {excl}\n            AND r.rulename != '_RETURN'\n            AND n.nspname NOT LIKE 'pg\\_%'\n        ORDER BY n.nspname, c.relname, rule_event\n        \"\"\".format(excl=self.exclude_schema)\n\n        return self.__check_equals(query)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __check_equals(self, query):\n        self.cur1.execute(query)\n        records1 = self.cur1.fetchall()\n\n        self.cur2.execute(query)\n        records2 = self.cur2.fetchall()\n\n        result = True\n        differences = []\n\n        d = difflib.Differ()\n        records1 = [str(x) for x in records1]\n        records2 = [str(x) for x in records2]\n\n        for line in d.compare(records1, records2):\n            if line[0] in ('-', '+'):\n                result = False\n                if self.verbose_level == 1:\n                    differences.append(line[0:79])\n                elif self.verbose_level == 2:\n                    differences.append(line)\n\n        return result, differences", "response": "Check if the query results on the two databases are equals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ask_for_confirmation(prompt=None, resp=False):\n    global input\n    if prompt is None:\n        prompt = 'Confirm'\n\n    if resp:\n        prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')\n    else:\n        prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')\n\n    while True:\n        # Fix for Python2. In python3 raw_input() is now input()\n        try:\n            input = raw_input\n        except NameError:\n            pass\n        ans = input(prompt)\n        if not ans:\n            return resp\n        if ans not in ['y', 'Y', 'n', 'N']:\n            print('please enter y or n.')\n            continue\n        if ans == 'y' or ans == 'Y':\n            return True\n        if ans == 'n' or ans == 'N':\n            return False", "response": "Prompts the user for confirmation from the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_target(self, request, controller_args, controller_kwargs):\n        self.request = request\n        super(AuthDecorator, self).handle_target(request, controller_args, controller_kwargs)\n        del self.request", "response": "This is called by the base class to set the request attribute for the current request and then call the base class s handle_target method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, uri, query=None, **kwargs):\n        return self.fetch('get', uri, query, **kwargs)", "response": "make a GET request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a POST request", "response": "def post(self, uri, body=None, **kwargs):\n        \"\"\"make a POST request\"\"\"\n        return self.fetch('post', uri, kwargs.pop(\"query\", {}), body, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a DELETE request", "response": "def delete(self, uri, query=None, **kwargs):\n        \"\"\"make a DELETE request\"\"\"\n        return self.fetch('delete', uri, query, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping method that all the top level methods use to actually get the request", "response": "def fetch(self, method, uri, query=None, body=None, **kwargs):\n        \"\"\"\n        wrapper method that all the top level methods (get, post, etc.) use to actually\n        make the request\n        \"\"\"\n        if not query: query = {}\n        fetch_url = self.get_fetch_url(uri, query)\n\n        args = [fetch_url]\n\n        kwargs.setdefault(\"timeout\", self.timeout)\n        kwargs[\"headers\"] = self.get_fetch_headers(method, kwargs.get(\"headers\", {}))\n\n        if body:\n            if self.is_json(kwargs[\"headers\"]):\n                kwargs['json'] = self.get_fetch_body(body)\n            else:\n                kwargs['data'] = self.get_fetch_body(body)\n\n        res = self.get_fetch_request(method, *args, **kwargs)\n        #res = requests.request(method, *args, **kwargs)\n        res = self.get_fetch_response(res)\n        self.response = res\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fetch_headers(self, method, headers):\n        all_headers = self.headers.copy()\n        if headers:\n            all_headers.update(headers)\n        return Headers(all_headers)", "response": "merge headers with global class headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_fetch_request(self, method, fetch_url, *args, **kwargs):\n        return requests.request(method, fetch_url, *args, **kwargs)", "response": "This method is handy if you want to modify the request right before passing it\n        to requests. request"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fetch_response(self, res):\n        res.code = res.status_code\n        res.headers = Headers(res.headers)\n        res._body = None\n        res.body = ''\n        body = res.content\n        if body:\n            if self.is_json(res.headers):\n                res._body = res.json()\n            else:\n                res._body = body\n\n            res.body = String(body, res.encoding)\n\n        return res", "response": "get the response object from the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns true if content_type is a json content type", "response": "def is_json(self, headers):\n        \"\"\"return true if content_type is a json content type\"\"\"\n        ret = False\n        ct = headers.get(\"content-type\", \"\").lower()\n        if ct:\n            ret = ct.lower().rfind(\"json\") >= 0\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the description of this endpoint", "response": "def desc(self):\n        \"\"\"return the description of this endpoint\"\"\"\n        doc = None\n        def visit_FunctionDef(node):\n            \"\"\" https://docs.python.org/2/library/ast.html#ast.NodeVisitor.visit \"\"\"\n            if node.name != self.method_name:\n                return\n\n            doc = ast.get_docstring(node)\n            raise StopIteration(doc if doc else \"\")\n\n        target = self.controller.controller_class\n        try:\n            node_iter = ast.NodeVisitor()\n            node_iter.visit_FunctionDef = visit_FunctionDef\n            node_iter.visit(ast.parse(inspect.getsource(target)))\n\n        except StopIteration as e:\n            doc = str(e)\n\n        if not doc: doc = \"\"\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef params(self):\n        ret = {}\n        for rd in self.decorators:\n            args = rd.args\n            kwargs = rd.kwargs\n            if param in rd:\n                is_required =  kwargs.get('required', 'default' not in kwargs)\n                ret[args[0]] = {'required': is_required, 'other_names': args[1:], 'options': kwargs}\n\n        return ret", "response": "return information about the params that the given http option takes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the decorators of all the option methods in the class", "response": "def decorators(self):\n        \"\"\"Get all the decorators of all the option methods in the class\n\n        http://stackoverflow.com/questions/5910703/ specifically, I used this\n        answer http://stackoverflow.com/a/9580006\n        \"\"\"\n        res = collections.defaultdict(list)\n        mmap = {}\n\n        def get_val(na, default=None):\n            ret = None\n            if isinstance(na, ast.Num):\n                repr_n = repr(na.n)\n                val = na.n\n                vtype = float if '.' in repr_n else int\n                ret = vtype(val)\n\n            elif isinstance(na, ast.Str):\n                ret = str(na.s)\n\n            elif isinstance(na, ast.Name):\n                # http://stackoverflow.com/questions/12700893/\n                ret = getattr(builtins, na.id, None)\n                if not ret:\n                    ret = na.id\n                    if ret == 'True':\n                        ret = True\n                    elif ret == 'False':\n                        ret = False\n\n            elif isinstance(na, ast.Dict):\n                if na.keys:\n                    ret = {get_val(na_[0]): get_val(na_[1]) for na_ in zip(na.keys, na.values)}\n                else:\n                    ret = {}\n\n            elif isinstance(na, (ast.List, ast.Tuple)):\n                if na.elts:\n                    ret = [get_val(na_) for na_ in na.elts]\n                else:\n                    ret = []\n\n                if isinstance(na, ast.Tuple):\n                    ret = tuple(ret)\n\n            else:\n                ret = default\n\n            return ret\n\n        def is_super(childnode, parentnode):\n            \"\"\"returns true if child node has a super() call to parent node\"\"\"\n            ret = False\n            for n in childnode.body:\n                if not isinstance(n, ast.Expr): continue\n\n                try:\n                    func = n.value.func\n                    func_name = func.attr\n                    if func_name == parentnode.name:\n                        ret = isinstance(func.value, ast.Call)\n                        break\n\n                except AttributeError as e:\n                    ret = False\n\n            return ret\n\n        def visit_FunctionDef(node):\n            \"\"\" https://docs.python.org/2/library/ast.html#ast.NodeVisitor.visit \"\"\"\n\n            add_decs = True\n            if node.name in res:\n                add_decs = is_super(mmap[node.name], node)\n\n            mmap[node.name] = node\n\n            if add_decs:\n                for n in node.decorator_list:\n                    d = {}\n                    name = ''\n                    args = []\n                    kwargs = {}\n                    if isinstance(n, ast.Call):\n                        name = n.func.attr if isinstance(n.func, ast.Attribute) else n.func.id\n                        for an in n.args:\n                            args.append(get_val(an))\n\n                        for an in n.keywords:\n                            kwargs[an.arg] = get_val(an.value)\n\n                    else:\n                        name = n.attr if isinstance(n, ast.Attribute) else n.id\n\n                    d = {\n                        \"name\": name,\n                        \"args\": args,\n                        \"kwargs\": kwargs\n                    }\n                    m = self.module\n                    decor = getattr(m, name, None)\n                    if decor:\n                        d[\"decorator\"] = decor\n\n                    #res[node.name].append((name, args, kwargs))\n                    res[node.name].append(self.decorator_class(**d))\n\n        node_iter = ast.NodeVisitor()\n        node_iter.visit_FunctionDef = visit_FunctionDef\n        for target_cls in inspect.getmro(self.controller_class):\n            if target_cls == Controller: break\n            node_iter.visit(ast.parse(inspect.getsource(target_cls)))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef desc(self):\n        doc = inspect.getdoc(self.controller_class)\n        if not doc: doc = ''\n        return doc", "response": "return the description of this endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of all the http methods that this class supports", "response": "def methods(self):\n        \"\"\"\n        return the supported http method options that this class supports\n        return what http method options this endpoint supports (eg, POST, GET)\n\n        http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html\n\n        :returns: dict, each http method (eg, GET, POST) will have a key with the value\n            being every method from the controller that can satisfy the http method\n        \"\"\"\n        ret = {}\n        method_regex = re.compile(r\"^[A-Z][A-Z0-9]+(_|$)\")\n        controller_methods = inspect.getmembers(self.controller_class)\n        for controller_method_name, controller_method in controller_methods:\n            if controller_method_name.startswith('_'): continue\n\n            if method_regex.match(controller_method_name):\n                method = self.method_class(\n                    controller_method_name,\n                    controller_method,\n                    controller=self\n                )\n                ret.setdefault(method.name, [])\n                ret[method.name].append(method)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a call object that has endpoints understandable request and response tuples", "response": "def create_call(self, raw_request, **kwargs):\n        \"\"\"create a call object that has endpoints understandable request and response\n        instances\"\"\"\n        req = self.create_request(raw_request, **kwargs)\n        res = self.create_response(**kwargs)\n        rou = self.create_router(**kwargs)\n        c = self.call_class(req, res, rou)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef target(self, request, key, limit, ttl):\n        ret = True\n        if key:\n            #backend = self.create_backend()\n            #method = getattr(backend, \"normalize_limit\", None)\n            #if method:\n            #    limit = method(request, limit)\n            #method = getattr(backend, \"normalize_ttl\", None)\n            #if method:\n            #    ttl = method(request, ttl)\n            #ret = backend.target(request, key, limit, ttl)\n            ret = super(RateLimitDecorator, self).target(request, key, limit, ttl)\n        else:\n            logger.warn(\"No ratelimit key found for {}\".format(request.path))\n\n        return ret", "response": "this will only run the request if the key has a value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decorate(self, func, limit=0, ttl=0, *anoop, **kwnoop):\n        self.limit = int(limit)\n        self.ttl = int(ttl)\n        return super(RateLimitDecorator, self).decorate(func, target=None, *anoop, **kwnoop)", "response": "decorate with limit and ttl"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes limit and ttl required", "response": "def decorate(self, func, limit, ttl, *anoop, **kwnoop):\n        \"\"\"make limit and ttl required\"\"\"\n        return super(ratelimit, self).decorate(func, limit, ttl, *anoop, **kwnoop)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(cls, s):\n        b = ByteString(s)\n        be = base64.b64encode(b).strip()\n        return String(be)", "response": "converts a plain text string to base64 encoded string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(cls, s):\n        b = ByteString(s)\n        bd = base64.b64decode(b)\n        return String(bd)", "response": "decodes a base64 string to plain text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_type(cls, val):\n        mt = \"\"\n        index = val.rfind(\".\")\n        if index == -1:\n            val = \"fake.{}\".format(val)\n        elif index == 0:\n            val = \"fake{}\".format(val)\n\n        mt = mimetypes.guess_type(val)[0]\n        if mt is None:\n            mt = \"\"\n\n        return mt", "response": "return the mimetype from the given string value\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sort(self, a, b):\n        '''\n        sort the headers according to rfc 2616 so when __iter__ is called, the accept media types are\n        in order from most preferred to least preferred\n        '''\n        ret = 0\n\n        # first we check q, higher values win:\n        if a[1] != b[1]:\n            ret = cmp(a[1], b[1])\n        else:\n            found = False\n            for i in range(2):\n                ai = a[0][i]\n                bi = b[0][i]\n                if ai == '*':\n                    if bi != '*':\n                        ret = -1\n                        found = True\n                        break\n                    else:\n                        # both *, more verbose params win\n                        ret = cmp(len(a[2]), len(b[2]))\n                        found = True\n                        break\n                elif bi == '*':\n                    ret = 1\n                    found = True\n                    break\n\n            if not found:\n                ret = cmp(len(a[2]), len(b[2]))\n\n        return ret", "response": "sort the accept headers according to rfc 2616 so when __iter__ is called the accept media types are sorted according to rfc 2616 so when __iter__ is called the accept media types are sorted by most preferred to least preferred."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating all the accept media types that match media_type and yield all matching things", "response": "def filter(self, media_type, **params):\n        \"\"\"\n        iterate all the accept media types that match media_type\n\n        media_type -- string -- the media type to filter by\n        **params -- dict -- further filter by key: val\n\n        return -- generator -- yields all matching media type info things\n        \"\"\"\n        mtype, msubtype = self._split_media_type(media_type)\n        for x in self.__iter__():\n            # all the params have to match to make the media type valid\n            matched = True\n            for k, v in params.items():\n                if x[2].get(k, None) != v:\n                    matched = False\n                    break\n\n            if matched:\n                if x[0][0] == '*':\n                    if x[0][1] == '*':\n                        yield x\n\n                    elif x[0][1] == msubtype:\n                        yield x\n\n                elif mtype == '*':\n                    if msubtype == '*':\n                        yield x\n\n                    elif x[0][1] == msubtype:\n                        yield x\n\n                elif x[0][0] == mtype:\n                    if msubtype == '*':\n                        yield x\n\n                    elif x[0][1] == '*':\n                        yield x\n\n                    elif x[0][1] == msubtype:\n                        yield x"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_request(self, raw_request, **kwargs):\n        r = self.request_class()\n        for k, v in raw_request.items():\n            if k.startswith('HTTP_'):\n                r.set_header(k[5:], v)\n            else:\n                r.environ[k] = v\n\n        r.method = raw_request['REQUEST_METHOD']\n        r.path = raw_request['PATH_INFO']\n        r.query = raw_request['QUERY_STRING']\n\n        # handle headers not prefixed with http\n        for k, t in {'CONTENT_TYPE': None, 'CONTENT_LENGTH': int}.items():\n            v = r.environ.pop(k, None)\n            if v:\n                r.set_header(k, t(v) if t else v)\n\n        if 'wsgi.input' in raw_request:\n\n            if \"CONTENT_LENGTH\" in raw_request and int(r.get_header(\"CONTENT_LENGTH\", 0)) <= 0:\n                r.body_kwargs = {}\n\n            else:\n                if r.get_header('transfer-encoding', \"\").lower().startswith('chunked'):\n                    raise IOError(\"Server does not support chunked requests\")\n\n                else:\n                    r.body_input = raw_request['wsgi.input']\n\n        else:\n            r.body_kwargs = {}\n\n        return r", "response": "create a new request object from a WSGI request object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef application(self, v):\n        self._application = v\n        self.backend.set_app(v)", "response": "allow overriding of the application factory"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_module_path():\n    master_modname = __name__.split(\".\", 1)[0]\n    master_module = sys.modules[master_modname]\n    #return os.path.dirname(os.path.realpath(os.path.join(inspect.getsourcefile(endpoints), \"..\")))\n    path = os.path.dirname(inspect.getsourcefile(master_module))\n    return path", "response": "find where the master module is located"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_string_name(self, k):\n        k = String(k, \"iso-8859-1\")\n        klower = k.lower().replace('_', '-')\n        bits = klower.split('-')\n        return \"-\".join((bit.title() for bit in bits))", "response": "converts things like FOO_BAR to Foo - Bar which is the normal form"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uri(self):\n        uristring = self.path\n        if self.query:\n            uristring += \"?{}\".format(self.query)\n        if self.fragment:\n            uristring += \"#{}\".format(self.fragment)\n\n        return uristring", "response": "return the uri which is everything but base"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_query(cls, query):\n        if not query: return {}\n\n        d = {}\n        # https://docs.python.org/2/library/urlparse.html\n        for k, kv in urlparse.parse_qs(query, True, strict_parsing=True).items():\n            #k = k.rstrip(\"[]\") # strip out php type array designated variables\n            if len(kv) > 1:\n                d[k] = kv\n            else:\n                d[k] = kv[0]\n\n        return d", "response": "return name = val&name2 = val2 strings into dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _normalize_params(self, *paths, **query_kwargs):\n        kwargs = {}\n\n        if paths:\n            fragment = paths[-1]\n            if fragment:\n                if fragment.startswith(\"#\"):\n                    kwargs[\"fragment\"] = fragment\n                    paths.pop(-1)\n\n            kwargs[\"path\"] = \"/\".join(self.normalize_paths(*paths))\n\n        kwargs[\"query_kwargs\"] = query_kwargs\n        return kwargs", "response": "this handles the special case of the helper methods"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_hostname_from_port(cls, hostname):\n        bits = hostname.split(\":\", 2)\n        p = None\n        d = bits[0]\n        if len(bits) == 2:\n            p = int(bits[1])\n\n        return d, p", "response": "given a hostname and port return a tuple ( hostname port"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef controller(self, *paths, **query_kwargs):\n        kwargs = self._normalize_params(*paths, **query_kwargs)\n        if self.controller_path:\n            if \"path\" in kwargs:\n                paths = self.normalize_paths(self.controller_path, kwargs[\"path\"])\n                kwargs[\"path\"] = \"/\".join(paths)\n            else:\n                kwargs[\"path\"] = self.controller_path\n        return self.create(self.root, **kwargs)", "response": "create a new url object using the controller path as the base path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef base(self, *paths, **query_kwargs):\n        kwargs = self._normalize_params(*paths, **query_kwargs)\n        if self.path:\n            if \"path\" in kwargs:\n                paths = self.normalize_paths(self.path, kwargs[\"path\"])\n                kwargs[\"path\"] = \"/\".join(paths)\n            else:\n                kwargs[\"path\"] = self.path\n        return self.create(self.root, **kwargs)", "response": "create a new url object using the current base path as a base"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef host(self, *paths, **query_kwargs):\n        kwargs = self._normalize_params(*paths, **query_kwargs)\n        return self.create(self.root, **kwargs)", "response": "create a new url object using the host"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept_encoding(self):\n        # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Charset\n        ret = \"\"\n        accept_encoding = self.get_header(\"Accept-Charset\", \"\")\n        if accept_encoding:\n            bits = re.split(r\"\\s+\", accept_encoding)\n            bits = bits[0].split(\";\")\n            ret = bits[0]\n        return ret", "response": "The encoding the client requested the response to use"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encoding(self):\n        encoding = None\n        ct = self.get_header('content-type')\n        if ct:\n            ah = AcceptHeader(ct)\n            if ah.media_types:\n                encoding = ah.media_types[0][2].get(\"charset\", None)\n\n        return encoding", "response": "the character encoding of the request"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef access_token(self):\n        access_token = self.get_auth_bearer()\n        if not access_token:\n            access_token = self.query_kwargs.get('access_token', '')\n            if not access_token:\n                access_token = self.body_kwargs.get('access_token', '')\n\n        return access_token", "response": "return an Oauth 2. 0 Bearer access token if it can be found"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying and get Oauth 2. 0 client id and secret from the request body or query string", "response": "def client_tokens(self):\n        \"\"\"try and get Oauth 2.0 client id and secret first from basic auth header,\n        then from GET or POST parameters\n\n        return -- tuple -- client_id, client_secret\n        \"\"\"\n        client_id, client_secret = self.get_auth_basic()\n        if not client_id and not client_secret:\n            client_id = self.query_kwargs.get('client_id', '')\n            client_secret = self.query_kwargs.get('client_secret', '')\n            if not client_id and not client_secret:\n                client_id = self.body_kwargs.get('client_id', '')\n                client_secret = self.body_kwargs.get('client_secret', '')\n\n        return client_id, client_secret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ips(self):\n        r = []\n        names = ['X_FORWARDED_FOR', 'CLIENT_IP', 'X_REAL_IP', 'X_FORWARDED', \n               'X_CLUSTER_CLIENT_IP', 'FORWARDED_FOR', 'FORWARDED', 'VIA',\n               'REMOTE_ADDR']\n\n        for name in names:\n            vs = self.get_header(name, '')\n            if vs:\n                r.extend(map(lambda v: v.strip(), vs.split(',')))\n\n            vs = self.environ.get(name, '')\n            if vs:\n                r.extend(map(lambda v: v.strip(), vs.split(',')))\n\n        return r", "response": "return all the possible ips of this request this will include public and private ips"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ip(self):\n        r = ''\n\n        # this was compiled from here:\n        # https://github.com/un33k/django-ipware\n        # http://www.ietf.org/rfc/rfc3330.txt (IPv4)\n        # http://www.ietf.org/rfc/rfc5156.txt (IPv6)\n        # https://en.wikipedia.org/wiki/Reserved_IP_addresses\n        format_regex = re.compile(r'\\s')\n        ip_regex = re.compile(r'^(?:{})'.format(r'|'.join([\n            r'0\\.', # reserved for 'self-identification'\n            r'10\\.', # class A\n            r'169\\.254', # link local block\n            r'172\\.(?:1[6-9]|2[0-9]|3[0-1])\\.', # class B\n            r'192\\.0\\.2\\.', # documentation/examples\n            r'192\\.168', # class C\n            r'255\\.{3}', # broadcast address\n            r'2001\\:db8', # documentation/examples\n            r'fc00\\:', # private\n            r'fe80\\:', # link local unicast\n            r'ff00\\:', # multicast\n            r'127\\.', # localhost\n            r'\\:\\:1' # localhost\n        ])))\n\n        ips = self.ips\n        for ip in ips:\n            if not format_regex.search(ip) and not ip_regex.match(ip):\n                r = ip\n                break\n\n        return r", "response": "return the public ip address"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the full request url as an Url instance", "response": "def url(self):\n        \"\"\"return the full request url as an Url() instance\"\"\"\n        scheme = self.scheme\n        host = self.host\n        path = self.path\n        query = self.query\n        port = self.port\n\n        # normalize the port\n        host_domain, host_port = Url.split_hostname_from_port(host)\n        if host_port:\n            port = host_port\n\n        controller_path = \"\"\n        if self.controller_info:\n            controller_path = self.controller_info.get(\"path\", \"\")\n\n        u = Url(\n            scheme=scheme,\n            hostname=host,\n            path=path,\n            query=query,\n            port=port,\n            controller_path=controller_path,\n        )\n        return u"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying part of a url", "response": "def query(self):\n        \"\"\"query_string part of a url (eg, http://host.com/path?query=string)\"\"\"\n        self._query = query = \"\"\n\n        query_kwargs = self.query_kwargs\n        if query_kwargs: query = urlencode(query_kwargs, doseq=True)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_kwargs(self):\n        self._query_kwargs = query_kwargs = {}\n        query = self.query\n        if query: query_kwargs = self._parse_query_str(query)\n        return query_kwargs", "response": "Returns a dictionary of keyword arguments for the query"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef body(self):\n        body = None\n        if self.body_input:\n            body = self.body_input.read(int(self.get_header('content-length', -1)))\n\n        return body", "response": "return the raw version of the body"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef body_kwargs(self):\n        body_kwargs = {}\n        ct = self.get_header(\"content-type\")\n        if ct:\n            ct = ct.lower()\n            if ct.rfind(\"json\") >= 0:\n                body = self.body\n                if body:\n                    body_kwargs = json.loads(body)\n\n            else:\n                if self.body_input:\n                    body = RequestBody(\n                        fp=self.body_input,\n                        headers=self.headers,\n                        environ=self.environ\n                        #environ=self.raw_request\n                    )\n                    body_kwargs = dict(body)\n\n                else:\n                    body = self.body\n                    if body:\n                        body_kwargs = self._parse_query_str(body)\n\n        return body_kwargs", "response": "return the kwargs for the request body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncombine GET and POST params to be passed to the controller", "response": "def kwargs(self):\n        \"\"\"combine GET and POST params to be passed to the controller\"\"\"\n        kwargs = dict(self.query_kwargs)\n        kwargs.update(self.body_kwargs)\n\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef version(self, content_type=\"*/*\"):\n        v = \"\"\n        accept_header = self.get_header('accept', \"\")\n        if accept_header:\n            a = AcceptHeader(accept_header)\n            for mt in a.filter(content_type):\n                v = mt[2].get(\"version\", \"\")\n                if v: break\n\n        return v", "response": "get the version of the post object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the bearer token in the authorization header if it exists", "response": "def get_auth_bearer(self):\n        \"\"\"return the bearer token in the authorization header if it exists\"\"\"\n        access_token = ''\n        auth_header = self.get_header('authorization')\n        if auth_header:\n            m = re.search(r\"^Bearer\\s+(\\S+)$\", auth_header, re.I)\n            if m: access_token = m.group(1)\n\n        return access_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_auth_basic(self):\n        username = ''\n        password = ''\n        auth_header = self.get_header('authorization')\n        if auth_header:\n            m = re.search(r\"^Basic\\s+(\\S+)$\", auth_header, re.I)\n            if m:\n                auth_str = Base64.decode(m.group(1))\n                username, password = auth_str.split(':', 1)\n\n        return username, password", "response": "return the username and password of a basic auth header if it exists"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_body(self, b):\n        if b is None: return ''\n\n        if self.is_json():\n            # TODO ???\n            # I don't like this, if we have a content type but it isn't one\n            # of the supported ones we were returning the exception, which threw\n            # Jarid off, but now it just returns a string, which is not best either\n            # my thought is we could have a body_type_subtype method that would \n            # make it possible to easily handle custom types\n            # eg, \"application/json\" would become: self.body_application_json(b, is_error)\n            b = json.dumps(b, cls=ResponseBody)\n\n        else:\n            # just return a string representation of body if no content type\n            b = String(b, self.encoding)\n\n        return b", "response": "return the body as a string formatted to the appropriate content type\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_target_params(self, request, controller_args, controller_kwargs):\n        return [], dict(\n            request=request,\n            controller_args=controller_args, \n            controller_kwargs=controller_kwargs\n        )", "response": "this method is used to normalize the target params for the target method"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decorate(self, func, target, *anoop, **kwnoop):\n        if target:\n            self.target = target\n\n        def decorated(decorated_self, *args, **kwargs):\n            self.handle_target(\n                request=decorated_self.request,\n                controller_args=args,\n                controller_kwargs=kwargs\n            )\n            return func(decorated_self, *args, **kwargs)\n\n        return decorated", "response": "decorate the passed in func calling target when func is called\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize the flags to make sure needed values are there", "response": "def normalize_flags(self, flags):\n        \"\"\"normalize the flags to make sure needed values are there\n\n        after this method is called self.flags is available\n\n        :param flags: the flags that will be normalized\n        \"\"\"\n        flags['type'] = flags.get('type', None)\n        paction = flags.get('action', 'store')\n        if paction == 'store_false':\n            flags['default'] = True \n            flags['type'] = bool\n\n        elif paction == 'store_true':\n            flags['default'] = False\n            flags['type'] = bool\n\n        prequired = False if 'default' in flags else flags.get('required', True)\n\n        flags[\"action\"] = paction\n        flags[\"required\"] = prequired\n        self.flags = flags"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_type(self, names):\n        self.name = names[0]\n        self.is_kwarg = False\n        self.is_arg = False\n        self.names = []\n\n        try:\n            # http://stackoverflow.com/a/16488383/5006 uses ask forgiveness because\n            # of py2/3 differences of integer check\n            self.index = int(self.name)\n            self.name = \"\"\n            self.is_arg = True\n\n        except ValueError:\n            self.is_kwarg = True\n            self.names = names", "response": "Decide if this param is an arg or a kwarg and set appropriate internal flags"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_param(self, slf, args, kwargs):\n        if self.is_kwarg:\n            kwargs = self.normalize_kwarg(slf.request, kwargs)\n        else:\n            args = self.normalize_arg(slf.request, args)\n        return slf, args, kwargs", "response": "this function will try to find the param and\n            put it in args if it has a default and stuff"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_kwarg(self, request, names, required, default, kwargs):\n        val = default\n        found_name = ''\n        for name in names:\n            if name in kwargs:\n                val = kwargs[name]\n                found_name = name\n                break\n\n        if not found_name and required:\n            raise ValueError(\"required param {} does not exist\".format(self.name))\n\n        return found_name, val", "response": "actually try to retrieve names key from params dict\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_val(self, request, val):\n        flags = self.flags\n        paction = flags['action']\n        ptype = flags['type']\n        pchoices = flags.get('choices', None)\n        allow_empty = flags.get('allow_empty', False)\n        min_size = flags.get('min_size', None)\n        max_size = flags.get('max_size', None)\n        regex = flags.get('regex', None)\n\n        if paction in set(['store_list']):\n            if isinstance(val, list) and len(val) > 1:\n                raise ValueError(\"too many values for param\")\n\n            if isinstance(val, basestring):\n                val = list(val.split(','))\n\n            else:\n                val = list(val)\n\n        elif paction in set(['append', 'append_list']):\n            if not isinstance(val, list):\n                val = [val]\n\n            if paction == 'append_list':\n                vs = []\n                for v in val:\n                    if isinstance(v, basestring):\n                        vs.extend(v.split(','))\n                    else:\n                        vs.append(v)\n\n                val = vs\n\n        else:\n            if paction not in set(['store', 'store_false', 'store_true']):\n                raise RuntimeError('unknown param action {}'.format(paction))\n\n        if regex:\n            failed = False\n            if isinstance(regex, basestring):\n                if not re.search(regex, val): failed = True\n            else:\n                if not regex.search(val): failed = True\n\n            if failed:\n                raise ValueError(\"param failed regex check\")\n\n        if ptype:\n            if isinstance(val, list) and ptype != list:\n                val = list(map(ptype, val))\n\n            else:\n                if isinstance(ptype, type):\n                    if issubclass(ptype, bool):\n                        if val in set(['true', 'True', '1']):\n                            val = True\n                        elif val in set(['false', 'False', '0']):\n                            val = False\n                        else:\n                            val = ptype(val)\n\n                    elif issubclass(ptype, str):\n                        charset = request.encoding\n                        if is_py2:\n                            val = ptype(ByteString(val, charset))\n                        else:\n                            val = ptype(String(val, charset))\n\n#                         if charset and isinstance(val, unicode):\n#                             val = val.encode(charset)\n#                         else:\n#                             val = ptype(val)\n\n                    else:\n                        val = ptype(val)\n\n                else:\n                    val = ptype(val)\n\n        if pchoices:\n            if isinstance(val, list) and ptype != list:\n                for v in val:\n                    if v not in pchoices:\n                        raise ValueError(\"param value {} not in choices {}\".format(v, pchoices))\n\n            else:\n                if val not in pchoices:\n                    raise ValueError(\"param value {} not in choices {}\".format(val, pchoices))\n\n        # at some point this if statement is just going to be too ridiculous\n        # FieldStorage check is because of this bug https://bugs.python.org/issue19097\n        if not isinstance(val, cgi.FieldStorage):\n            if not allow_empty and val is not False and not val:\n                if 'default' not in flags:\n                    raise ValueError(\"param was empty\")\n\n        if min_size is not None:\n            failed = False\n            if isinstance(val, (int, float)):\n                if val < min_size: failed = True\n            else:\n                if len(val) < min_size: failed = True\n\n            if failed:\n                raise ValueError(\"param was smaller than {}\".format(min_size))\n\n        if max_size is not None:\n            failed = False\n            if isinstance(val, (int, float)):\n                if val > max_size: failed = True\n            else:\n                if len(val) > max_size: failed = True\n\n            if failed:\n                raise ValueError(\"param was bigger than {}\".format(max_size))\n\n        return val", "response": "This will take the value and make sure it meets expectations"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, path=\"\", headers=None, query=None, timeout=0, **kwargs):\n        ret = None\n        ws_url = self.get_fetch_url(path, query)\n        ws_headers = self.get_fetch_headers(\"GET\", headers)\n        ws_headers = ['{}: {}'.format(h[0], h[1]) for h in ws_headers.items() if h[1]]\n        timeout = self.get_timeout(timeout=timeout, **kwargs)\n\n        self.set_trace(kwargs.pop(\"trace\", False))\n        #pout.v(websocket_url, websocket_headers, self.query_kwargs, self.headers)\n\n        try:\n            logger.debug(\"{} connecting to {}\".format(self.client_id, ws_url))\n            self.ws = websocket.create_connection(\n                ws_url,\n                header=ws_headers,\n                timeout=timeout,\n                sslopt={'cert_reqs':ssl.CERT_NONE},\n            )\n\n            ret = self.recv_callback(callback=lambda r: r.uuid == \"CONNECT\")\n            if ret.code >= 400:\n                raise IOError(\"Failed to connect with code {}\".format(ret.code))\n\n#             self.headers = headers\n#             self.query_kwargs = query_kwargs\n\n        except websocket.WebSocketTimeoutException:\n            raise IOError(\"Failed to connect within {} seconds\".format(timeout))\n\n        except websocket.WebSocketException as e:\n            raise IOError(\"Failed to connect with error: {}\".format(e))\n\n        except socket.error as e:\n            # this is an IOError, I just wanted to be aware of that, most common\n            # problem is: [Errno 111] Connection refused\n            raise\n\n        return ret", "response": "Connect to the most common ACS - related API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch(self, method, path, query=None, body=None, timeout=0, **kwargs):\n        ret = None\n        if not query: query = {}\n        if not body: body = {}\n        query.update(body) # body takes precedence\n        body = query\n\n        self.send_count += 1\n        payload = self.get_fetch_request(method, path, body)\n        attempts = 1\n        max_attempts = self.attempts\n        success = False\n\n        while not success:\n            kwargs['timeout'] = timeout\n            try:\n                try:\n                    if not self.connected: self.connect(path)\n                    with self.wstimeout(**kwargs) as timeout:\n                        kwargs['timeout'] = timeout\n\n                        logger.debug('{} send {} attempt {}/{} with timeout {}'.format(\n                            self.client_id,\n                            payload.uuid,\n                            attempts,\n                            max_attempts,\n                            timeout\n                        ))\n\n                        sent_bits = self.ws.send(payload.payload)\n                        logger.debug('{} sent {} bytes'.format(self.client_id, sent_bits))\n                        if sent_bits:\n                            ret = self.fetch_response(payload, **kwargs)\n                            if ret:\n                                success = True\n\n                except websocket.WebSocketConnectionClosedException as e:\n                    self.ws.shutdown()\n                    raise IOError(\"connection is not open but reported it was open: {}\".format(e))\n\n            except (IOError, TypeError) as e:\n                logger.debug('{} error on send attempt {}: {}'.format(self.client_id, attempts, e))\n                success = False\n\n            finally:\n                if not success:\n                    attempts += 1\n                    if attempts > max_attempts:\n                        raise\n\n                    else:\n                        timeout *= 2\n                        if (attempts / max_attempts) > 0.50:\n                            logger.debug(\n                                \"{} closing and re-opening connection for next attempt\".format(self.client_id)\n                            )\n                            self.close()\n\n        return ret", "response": "send a message to the server"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_response(self, req_payload, **kwargs):\n        if req_payload.uuid:\n            uuids = set([req_payload.uuid, \"CONNECT\"])\n            def callback(res_payload):\n                #pout.v(req_payload, res_payload)\n                #ret = req_payload.uuid == res_payload.uuid or res_payload.uuid == \"CONNECT\"\n                ret = res_payload.uuid in uuids\n                if ret:\n                    logger.debug('{} received {} response for {}'.format(\n                        self.client_id,\n                        res_payload.code,\n                        res_payload.uuid,\n                    ))\n                return ret\n\n            res_payload = self.recv_callback(callback, **kwargs)\n\n        return res_payload", "response": "fetch_response - send a response to the server"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nping the server and check the status of the user s agent.", "response": "def ping(self, timeout=0, **kwargs):\n        \"\"\"THIS DOES NOT WORK, UWSGI DOES NOT RESPOND TO PINGS\"\"\"\n\n        # http://stackoverflow.com/a/2257449/5006\n        def rand_id(size=8, chars=string.ascii_uppercase + string.digits):\n            return ''.join(random.choice(chars) for _ in range(size))\n\n        payload = rand_id()\n        self.ws.ping(payload)\n        opcode, data = self.recv_raw(timeout, [websocket.ABNF.OPCODE_PONG], **kwargs)\n        if data != payload:\n            raise IOError(\"Pinged server but did not receive correct pong\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreceiving messages and validate them with the callback", "response": "def recv_callback(self, callback, **kwargs):\n        \"\"\"receive messages and validate them with the callback, if the callback \n        returns True then the message is valid and will be returned, if False then\n        this will try and receive another message until timeout is 0\"\"\"\n        payload = None\n        timeout = self.get_timeout(**kwargs)\n        full_timeout = timeout\n        while timeout > 0.0:\n            kwargs['timeout'] = timeout\n            start = time.time()\n            payload = self.recv(**kwargs)\n            if callback(payload):\n                break\n            payload = None\n            stop = time.time()\n            elapsed = stop - start\n            timeout -= elapsed\n\n        if not payload:\n            raise IOError(\"recv_callback timed out in {}\".format(full_timeout))\n\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new controller instance for the request and response.", "response": "def create_controller(self):\n        \"\"\"Create a controller to handle the request\n\n        :returns: Controller, this Controller instance should be able to handle\n            the request\n        \"\"\"\n        body = None\n        req = self.request\n        res = self.response\n        rou = self.router\n        con = None\n\n        controller_info = {}\n        try:\n            controller_info = rou.find(req, res)\n\n        except IOError as e:\n            logger.warning(str(e), exc_info=True)\n            raise CallError(\n                408,\n                \"The client went away before the request body was retrieved.\"\n            )\n\n        except (ImportError, AttributeError, TypeError) as e:\n            exc_info = sys.exc_info()\n            logger.warning(str(e), exc_info=exc_info)\n            raise CallError(\n                404,\n                \"{} not found because of {} \\\"{}\\\" on {}:{}\".format(\n                    req.path,\n                    exc_info[0].__name__,\n                    str(e),\n                    os.path.basename(exc_info[2].tb_frame.f_code.co_filename),\n                    exc_info[2].tb_lineno\n                )\n            )\n\n        else:\n            con = controller_info['class_instance']\n\n        return con"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle(self):\n        body = None\n        req = self.request\n        res = self.response\n        rou = self.router\n        con = None\n        start = time.time()\n\n        try:\n            con = self.create_controller()\n            con.call = self\n            self.controller = con\n            if not self.quiet:\n                con.log_start(start)\n\n            # the controller handle method will manipulate self.response, it first\n            # tries to find a handle_HTTP_METHOD method, if it can't find that it\n            # will default to the handle method (which is implemented on Controller).\n            # method arguments are passed in so child classes can add decorators\n            # just like the HTTP_METHOD that will actually handle the request\n            controller_args, controller_kwargs = con.find_method_params()\n            controller_method = getattr(con, \"handle_{}\".format(req.method), None)\n            if not controller_method:\n                controller_method = getattr(con, \"handle\")\n\n            if not self.quiet:\n                logger.debug(\"Using handle method: {}.{}\".format(\n                    con.__class__.__name__,\n                    controller_method.__name__\n                ))\n            controller_method(*controller_args, **controller_kwargs)\n\n        except Exception as e:\n            self.handle_error(e) # this will manipulate self.response\n\n        finally:\n            if res.code == 204:\n                res.headers.pop('Content-Type', None)\n                res.body = None # just to be sure since body could've been \"\"\n\n            if con:\n                if not self.quiet:\n                    con.log_stop(start)\n\n        return res", "response": "Called from the interface to actually handle the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_error(self, e, **kwargs):\n        req = self.request\n        res = self.response\n        con = self.controller\n\n        if isinstance(e, CallStop):\n            logger.info(str(e), exc_info=True)\n            res.code = e.code\n            res.add_headers(e.headers)\n            res.body = e.body\n\n        elif isinstance(e, Redirect):\n            logger.info(str(e), exc_info=True)\n            res.code = e.code\n            res.add_headers(e.headers)\n            res.body = None\n\n        elif isinstance(e, (AccessDenied, CallError)):\n            logger.warning(str(e), exc_info=True)\n            res.code = e.code\n            res.add_headers(e.headers)\n            res.body = e\n\n        elif isinstance(e, NotImplementedError):\n            logger.warning(str(e), exc_info=True)\n            res.code = 501\n            res.body = e\n\n        elif isinstance(e, TypeError):\n            e_msg = unicode(e)\n            if e_msg.startswith(req.method) and 'argument' in e_msg:\n                logger.debug(e_msg, exc_info=True)\n                logger.warning(\n                    \" \".join([\n                        \"Either the path arguments ({} args) or the keyword arguments\",\n                        \"({} args) for {}.{} do not match the {} handling method's\",\n                        \"definition\"\n                    ]).format(\n                        len(req.controller_info[\"method_args\"]),\n                        len(req.controller_info[\"method_kwargs\"]),\n                        req.controller_info['module_name'],\n                        req.controller_info['class_name'],\n                        req.method\n                    )\n                )\n                res.code = 405\n\n            else:\n                logger.exception(e)\n                res.code = 500\n\n            res.body = e\n\n        else:\n            logger.exception(e)\n            res.code = 500\n            res.body = e\n\n        if con:\n            error_method = getattr(con, \"handle_{}_error\".format(req.method), None)\n            if not error_method:\n                error_method = getattr(con, \"handle_error\")\n\n            logger.debug(\"Using error method: {}.{}\".format(\n                con.__class__.__name__,\n                error_method.__name__\n            ))\n            error_method(e, **kwargs)", "response": "This method will set the response body and code to the error code of the current resource and set the response body to the error code of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef module_names(self):\n        controller_prefix = self.controller_prefix\n        _module_name_cache = self._module_name_cache\n        if controller_prefix in _module_name_cache:\n            return _module_name_cache[controller_prefix]\n\n        module = self.get_module(controller_prefix)\n\n        if hasattr(module, \"__path__\"):\n            # path attr exists so this is a package\n            modules = self.find_modules(module.__path__[0], controller_prefix)\n\n        else:\n            # we have a lonely .py file\n            modules = set([controller_prefix])\n\n        _module_name_cache.setdefault(controller_prefix, {})\n        _module_name_cache[controller_prefix] = modules\n\n        return modules", "response": "get all the modules in the controller_prefix\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef modules(self):\n        for modname in self.module_names:\n            module = importlib.import_module(modname)\n            yield module", "response": "Returns an iterator of the actual modules not just their names\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_modules(self, path, prefix):\n\n        modules = set([prefix])\n\n        # https://docs.python.org/2/library/pkgutil.html#pkgutil.iter_modules\n        for module_info in pkgutil.iter_modules([path]):\n            # we want to ignore any \"private\" modules\n            if module_info[1].startswith('_'): continue\n\n            module_prefix = \".\".join([prefix, module_info[1]])\n            if module_info[2]:\n                # module is a package\n                submodules = self.find_modules(os.path.join(path, module_info[1]), module_prefix)\n                modules.update(submodules)\n            else:\n                modules.add(module_prefix)\n\n        return modules", "response": "recursive method that will find all the submodules of the given module\n            at the given path with prefix with path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_module_name(self, path_args):\n        controller_prefix = self.controller_prefix\n        cset = self.module_names\n        module_name = controller_prefix\n        mod_name = module_name\n        while path_args:\n            mod_name += \".\" + path_args[0]\n            if mod_name in cset:\n                module_name = mod_name\n                path_args.pop(0)\n            else:\n                break\n\n        return module_name, path_args", "response": "returns the module_name and remaining path args."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_class(self, module, class_name):\n        # let's get the class\n        class_object = getattr(module, class_name, None)\n        if not class_object or not issubclass(class_object, Controller):\n            class_object = None\n\n        return class_object", "response": "try and get the class_name from the module and make sure it is a valid\n        controller"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef OPTIONS(self, *args, **kwargs):\n        if not self.cors:\n            raise CallError(405)\n\n        req = self.request\n\n        origin = req.get_header('origin')\n        if not origin:\n            raise CallError(400, 'Need Origin header') \n        call_headers = [\n            ('Access-Control-Request-Headers', 'Access-Control-Allow-Headers'),\n            ('Access-Control-Request-Method', 'Access-Control-Allow-Methods')\n        ]\n        for req_header, res_header in call_headers:\n            v = req.get_header(req_header)\n            if v:\n                self.response.set_header(res_header, v)\n            else:\n                raise CallError(400, 'Need {} header'.format(req_header))\n\n        other_headers = {\n            'Access-Control-Allow-Credentials': 'true',\n            'Access-Control-Max-Age': 3600\n        }\n        self.response.add_headers(other_headers)", "response": "Handles CORS requests for this controller."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the request and returns the response", "response": "def handle(self, *controller_args, **controller_kwargs):\n        \"\"\"handles the request and returns the response\n\n        This should set any response information directly onto self.response\n\n        this method has the same signature as the request handling methods\n        (eg, GET, POST) so subclasses can override this method and add decorators\n\n        :param *controller_args: tuple, the path arguments that will be passed to\n            the request handling method (eg, GET, POST)\n        :param **controller_kwargs: dict, the query and body params merged together\n        \"\"\"\n        req = self.request\n        res = self.response\n        res.set_header('Content-Type', \"{};charset={}\".format(\n            self.content_type,\n            self.encoding\n        ))\n\n        encoding = req.accept_encoding\n        res.encoding = encoding if encoding else self.encoding\n\n        res_method_name = \"\"\n        controller_methods = self.find_methods()\n        #controller_args, controller_kwargs = self.find_method_params()\n        for controller_method_name, controller_method in controller_methods:\n            try:\n                logger.debug(\"Attempting to handle request with {}.{}.{}\".format(\n                    req.controller_info['module_name'],\n                    req.controller_info['class_name'],\n                    controller_method_name\n                ))\n                res.body = controller_method(\n                    *controller_args,\n                    **controller_kwargs\n                )\n                res_method_name = controller_method_name\n                break\n\n            except VersionError as e:\n                logger.debug(\"Request {}.{}.{} failed version check [{} not in {}]\".format(\n                    req.controller_info['module_name'],\n                    req.controller_info['class_name'],\n                    controller_method_name,\n                    e.request_version,\n                    e.versions\n                ))\n\n            except RouteError:\n                logger.debug(\"Request {}.{}.{} failed routing check\".format(\n                    req.controller_info['module_name'],\n                    req.controller_info['class_name'],\n                    controller_method_name\n                ))\n\n        if not res_method_name:\n            # https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1\n            # An origin server SHOULD return the status code 405 (Method Not Allowed)\n            # if the method is known by the origin server but not allowed for the\n            # requested resource\n            raise CallError(405, \"Could not find a method to satisfy {}\".format(\n                req.path\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_methods(self):\n        methods = []\n        req = self.request\n        method_name = req.method.upper()\n        method_names = set()\n\n        members = inspect.getmembers(self)\n        for member_name, member in members:\n            if member_name.startswith(method_name):\n                if member:\n                    methods.append((member_name, member))\n                    method_names.add(member_name)\n\n        if len(methods) == 0:\n            # https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1\n            # and 501 (Not Implemented) if the method is unrecognized or not\n            # implemented by the origin server\n            logger.warning(\"No methods to handle {} found\".format(method_name), exc_info=True)\n            raise CallError(501, \"{} {} not implemented\".format(req.method, req.path))\n\n        elif len(methods) > 1 and method_name in method_names:\n            raise ValueError(\n                \" \".join([\n                    \"A multi method {} request should not have any methods named {}.\",\n                    \"Instead, all {} methods should use use an appropriate decorator\",\n                    \"like @route or @version and have a unique name starting with {}_\"\n                ]).format(\n                    method_name,\n                    method_name,\n                    method_name,\n                    method_name\n                )\n            )\n\n        return methods", "response": "Find the methods that could satisfy this request. This will go through and find any methods that start with the request. method GET or POST. This will find any methods that start with the request. method GET or POST. This will find any methods that start with the request. method GET or POST."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the method params", "response": "def find_method_params(self):\n        \"\"\"Return the method params\n\n        :returns: tuple (args, kwargs) that will be passed as *args, **kwargs\n        \"\"\"\n        req = self.request\n        args = req.controller_info[\"method_args\"]\n        kwargs = req.controller_info[\"method_kwargs\"]\n        return args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_start(self, start):\n        if not logger.isEnabledFor(logging.INFO): return\n\n        try:\n            req = self.request\n\n            logger.info(\"REQUEST {} {}?{}\".format(req.method, req.path, req.query))\n            logger.info(datetime.datetime.strftime(datetime.datetime.utcnow(), \"DATE %Y-%m-%dT%H:%M:%S.%f\"))\n\n            ip = req.ip\n            if ip:\n                logger.info(\"\\tIP ADDRESS: {}\".format(ip))\n\n            if 'authorization' in req.headers:\n                logger.info('AUTH {}'.format(req.headers['authorization']))\n\n            ignore_hs = set([\n                'accept-language',\n                'accept-encoding',\n                'connection',\n                'authorization',\n                'host',\n                'x-forwarded-for'\n            ])\n            hs = [\"Request Headers...\"]\n            for k, v in req.headers.items():\n                if k not in ignore_hs:\n                    hs.append(\"\\t{}: {}\".format(k, v))\n\n            logger.info(os.linesep.join(hs))\n\n        except Exception as e:\n            logger.warn(e, exc_info=True)", "response": "log all the headers and stuff at the start of the request"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log_stop(self, start):\n        if not logger.isEnabledFor(logging.INFO): return\n\n        stop = time.time()\n        get_elapsed = lambda start, stop, multiplier, rnd: round(abs(stop - start) * float(multiplier), rnd)\n        elapsed = get_elapsed(start, stop, 1000.00, 1)\n        total = \"%0.1f ms\" % (elapsed)\n        logger.info(\"RESPONSE {} {} in {}\".format(self.response.code, self.response.status, total))", "response": "log a summary line on how the request went"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_lane_from_yaml(path):\n    # Open\n    with open(path, 'rb') as yaml_definition:\n        definition = yaml.load(yaml_definition)\n\n    # Validate schema\n    try:\n        validate_schema(definition)\n    except SchemaError as exc:\n        raise LaneSchemaError(**exc.__dict__)\n\n    def build(lb_def, branch=False):\n        \"\"\"Function to recursively build the `sparklanes.Lane` object from a YAML definition\"\"\"\n        init_kwargs = {k: lb_def[k] for k in (a for a in ('run_parallel', 'name') if a in lb_def)}\n        lane_or_branch = Lane(**init_kwargs) if not branch else Branch(**init_kwargs)\n\n        for task in lb_def['tasks']:\n            if 'branch' in task:\n                branch_def = task['branch']\n                lane_or_branch.add(build(branch_def, True))\n            else:\n                sep = task['class'].rfind('.')\n                if sep == -1:\n                    raise LaneImportError('Class must include its parent module')\n                mdl = task['class'][:sep]\n                cls_ = task['class'][sep + 1:]\n\n                try:\n                    cls = getattr(import_module(mdl), cls_)\n                except ImportError:\n                    raise LaneImportError('Could not find module %s' % mdl)\n                except AttributeError:\n                    raise LaneImportError('Could not find class %s' % cls_)\n\n                args = task['args'] if 'args' in task else []\n                args = [args] if not isinstance(args, list) else args\n                kwargs = task['kwargs'] if 'kwargs' in task else {}\n                lane_or_branch.add(cls, *args, **kwargs)\n\n        return lane_or_branch\n\n    return build(definition['lane'])", "response": "Builds a Sparklanes. Lane object from a YAML file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a class is a task and if it has been decorated with Sparklanes. Task and if it has the supplied args and kwargs match the signature of the task s entry method.", "response": "def __validate_task(self, cls, entry_mtd_name, args, kwargs):\n        \"\"\"Checks if a class is a task, i.e. if it has been decorated with `sparklanes.Task`, and if\n        the supplied args/kwargs match the signature of the task's entry method.\n\n        Parameters\n        ----------\n        cls : LaneTask\n        entry_mtd_name : str\n            Name of the method, which is called when the task is run\n        args : list\n        kwargs : dict\n        \"\"\"\n        if not isclass(cls) or not issubclass(cls, LaneTask):\n            raise TypeError('Tried to add non-Task `%s` to a Lane. Are you sure the task was '\n                            'decorated with `sparklanes.Task`?' % str(cls))\n\n        validate_params(cls, entry_mtd_name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, cls_or_branch, *args, **kwargs):\n        if isinstance(cls_or_branch, Branch):\n            self.tasks.append(cls_or_branch)  # Add branch with already validated tasks\n        else:\n            # Validate\n            self.__validate_task(cls_or_branch, '__init__', args, kwargs)\n            # Append\n            self.tasks.append({'cls_or_branch': cls_or_branch, 'args': args, 'kwargs': kwargs})\n\n        return self", "response": "Adds a task or branch to the lane."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the tasks in the lane in the order in which they have been added.", "response": "def run(self):\n        \"\"\"Executes the tasks in the lane in the order in which they have been added, unless\n        `self.run_parallel` is True, then a thread is spawned for each task and executed in\n        parallel (note that task threads are still spawned in the order in which they were added).\n        \"\"\"\n        logger = make_default_logger(INTERNAL_LOGGER_NAME)\n        logger.info('\\n%s\\nExecuting `%s`\\n%s\\n', '-'*80, self.name, '-'*80)\n        logger.info('\\n%s', str(self))\n\n        threads = []\n\n        if not self.tasks:\n            raise LaneExecutionError('No tasks to execute!')\n\n        for task_def_or_branch in self.tasks:\n            if isinstance(task_def_or_branch, Branch):\n                task_def_or_branch.run()\n            elif isinstance(task_def_or_branch['cls_or_branch'], Branch):  # Nested Branch\n                task_def_or_branch['cls_or_branch'].run()\n            else:\n                task = task_def_or_branch['cls_or_branch'](*task_def_or_branch['args'],\n                                                           **task_def_or_branch['kwargs'])\n                if self.run_parallel:\n                    threads.append(LaneTaskThread(task))\n                else:\n                    task()\n\n        if threads:\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n        logger.info('\\n%s\\nFinished executing `%s`\\n%s', '-'*80, self.name, '-'*80)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the historical tank data.", "response": "def get_historical_data(nmr_problems):\n    \"\"\"Get the historical tank data.\n\n    Args:\n        nmr_problems (int): the number of problems\n\n    Returns:\n        tuple: (observations, nmr_tanks_ground_truth)\n    \"\"\"\n    observations = np.tile(np.array([[10, 256, 202, 97]]), (nmr_problems, 1))\n    nmr_tanks_ground_truth = np.ones((nmr_problems,)) * 276\n    return observations, nmr_tanks_ground_truth"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_simulated_data(nmr_problems):\n    # The number of tanks we observe per problem\n    nmr_observed_tanks = 10\n\n    # Generate some maximum number of tanks. Basically the ground truth of the estimation problem.\n    nmr_tanks_ground_truth = normal(nmr_problems, 1, mean=250, std=30, ctype='uint')\n\n    # Generate some random tank observations\n    observations = uniform(nmr_problems, nmr_observed_tanks, low=0, high=nmr_tanks_ground_truth, ctype='uint')\n\n    return observations, nmr_tanks_ground_truth", "response": "Simulate some data. This returns the simulated tank observations and the corresponding ground truth maximum number of tanks."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nestimate and return the upper triangular elements of the Hessian of the given function at the given parameters.", "response": "def estimate_hessian(objective_func, parameters,\n                     lower_bounds=None, upper_bounds=None,\n                     step_ratio=2, nmr_steps=5,\n                     max_step_sizes=None,\n                     data=None, cl_runtime_info=None):\n    \"\"\"Estimate and return the upper triangular elements of the Hessian of the given function at the given parameters.\n\n    This calculates the Hessian using central difference (using a 2nd order Taylor expansion) with a Richardson\n    extrapolation over the proposed sequence of steps. If enough steps are given, we apply a Wynn epsilon extrapolation\n    on top of the Richardson extrapolated results. If more steps are left, we return the estimate with the lowest error,\n    taking into account outliers using a median filter.\n\n    The Hessian is evaluated at the steps:\n\n    .. math::\n        \\quad  ((f(x + d_j e_j + d_k e_k) - f(x + d_j e_j - d_k e_k)) -\n                (f(x - d_j e_j + d_k e_k) - f(x - d_j e_j - d_k e_k)) /\n                (4 d_j d_k)\n\n    where :math:`e_j` is a vector where element :math:`j` is one and the rest are zero\n    and :math:`d_j` is a scalar spacing :math:`steps_j`.\n\n    Steps are generated according to an exponentially diminishing ratio, defined as:\n\n        steps = max_step * step_ratio**-i, i = 0,1,..,nmr_steps-1.\n\n    Where the maximum step can be provided. For example, a maximum step of 2 with a step ratio of 2, computed for\n    4 steps gives: [2.0, 1.0, 0.5, 0.25]. If lower and upper bounds are given, we use as maximum step size the largest\n    step size that fits between the Hessian point and the boundaries.\n\n    The steps define the order of the estimation, with 2 steps resulting in a O(h^2) estimate, 3 steps resulting in a\n    O(h^4) estimate and 4 or more steps resulting in a O(h^6) derivative estimate.\n\n    Args:\n        objective_func (mot.lib.cl_function.CLFunction): The function we want to differentiate.\n            A CL function with the signature:\n\n            .. code-block:: c\n\n                double <func_name>(local const mot_float_type* const x, void* data);\n\n            The objective function has the same signature as the minimization function in MOT. For the numerical\n            hessian, the ``objective_list`` parameter is ignored.\n\n        parameters (ndarray): The parameters at which to evaluate the gradient. A (d, p) matrix with d problems,\n            and p parameters\n        lower_bounds (list or None): a list of length (p,) for p parameters with the lower bounds.\n            Each element of the list can be a scalar or a vector (of the same length as the number\n            of problem instances). To disable bounds for this parameter use -np.inf.\n        upper_bounds (list or None): a list of length (p,) for p parameters with the upper bounds.\n            Each element of the list can be a scalar or a vector (of the same length as the number\n            of problem instances). To disable bounds for this parameter use np.inf.\n        step_ratio (float): the ratio at which the steps diminish.\n        nmr_steps (int): the number of steps we will generate. We will calculate the derivative for each of these\n            step sizes and extrapolate the best step size from among them. The minimum number of steps is 1.\n        max_step_sizes (float or ndarray or None): the maximum step size, or the maximum step size per parameter.\n            If None is given, we use 0.1 for all parameters. If a float is given, we use that for all parameters.\n            If a list is given, it should be of the same length as the number of parameters.\n        data (mot.lib.kernel_data.KernelData): the user provided data for the ``void* data`` pointer.\n        cl_runtime_info (mot.configuration.CLRuntimeInfo): the runtime information\n\n    Returns:\n        ndarray: per problem instance a vector with the upper triangular elements of the Hessian matrix.\n            This array can hold NaN's, for elements where the Hessian failed to approximate.\n    \"\"\"\n    if len(parameters.shape) == 1:\n        parameters = parameters[None, :]\n\n    nmr_voxels = parameters.shape[0]\n    nmr_params = parameters.shape[1]\n    nmr_derivatives = nmr_params * (nmr_params + 1) // 2\n\n    initial_step = _get_initial_step(parameters, lower_bounds, upper_bounds, max_step_sizes)\n\n    kernel_data = {\n        'parameters': Array(parameters, ctype='mot_float_type'),\n        'initial_step': Array(initial_step, ctype='float'),\n        'derivatives': Zeros((nmr_voxels, nmr_derivatives), 'double'),\n        'errors': Zeros((nmr_voxels, nmr_derivatives), 'double'),\n        'x_tmp': LocalMemory('mot_float_type', nmr_params),\n        'data': data,\n        'scratch': LocalMemory('double', nmr_steps + (nmr_steps - 1) + nmr_steps)\n    }\n\n    hessian_kernel = SimpleCLFunction.from_string('''\n        void _numdiff_hessian(\n                global mot_float_type* parameters,\n                global float* initial_step,\n                global double* derivatives,\n                global double* errors,\n                local mot_float_type* x_tmp,\n                void* data,\n                local double* scratch){\n\n            if(get_local_id(0) == 0){\n                for(uint i = 0; i < ''' + str(nmr_params) + '''; i++){\n                    x_tmp[i] = parameters[i];\n                }\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            double f_x_input = ''' + objective_func.get_cl_function_name() + '''(x_tmp, data);\n\n            // upper triangle loop\n            uint coord_ind = 0;\n            for(int i = 0; i < ''' + str(nmr_params) + '''; i++){\n                for(int j = i; j < ''' + str(nmr_params) + '''; j++){\n                    _numdiff_hessian_element(\n                        data, x_tmp, f_x_input, i, j, initial_step, \n                        derivatives + coord_ind, errors + coord_ind, scratch);\n\n                    coord_ind++;\n                }\n            }\n        }\n    ''', dependencies=[objective_func,\n                       _get_numdiff_hessian_element_func(objective_func, nmr_steps, step_ratio)])\n\n    hessian_kernel.evaluate(kernel_data, nmr_voxels, use_local_reduction=True, cl_runtime_info=cl_runtime_info)\n\n    return kernel_data['derivatives'].get_data()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a function to compute one element of the Hessian matrix.", "response": "def _get_numdiff_hessian_element_func(objective_func, nmr_steps, step_ratio):\n    \"\"\"Return a function to compute one element of the Hessian matrix.\"\"\"\n    return SimpleCLFunction.from_string('''\n        /**\n         * Compute the Hessian using (possibly) multiple steps with various interpolations. \n         */ \n        void _numdiff_hessian_element(\n                void* data, local mot_float_type* x_tmp, mot_float_type f_x_input,\n                uint px, uint py, global float* initial_step, global double* derivative, \n                global double* error, local double* scratch){\n\n            const uint nmr_steps = ''' + str(nmr_steps) + ''';\n            uint nmr_steps_remaining = nmr_steps;\n\n            local double* scratch_ind = scratch;\n            local double* steps = scratch_ind;      scratch_ind += nmr_steps; \n            local double* errors = scratch_ind;     scratch_ind += nmr_steps - 1;\n            local double* steps_tmp = scratch_ind;  scratch_ind += nmr_steps;\n            \n            if(get_local_id(0) == 0){\n                for(int i = 0; i < nmr_steps - 1; i++){\n                    errors[i] = 0;\n                }\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            _numdiff_hessian_steps(data, x_tmp, f_x_input, px, py, steps, initial_step);\n\n            if(nmr_steps_remaining > 1){\n                nmr_steps_remaining = _numdiff_hessian_richardson_extrapolation(steps); \n                barrier(CLK_LOCAL_MEM_FENCE);\n            }\n            \n            if(nmr_steps_remaining >= 3){\n                nmr_steps_remaining = _numdiff_wynn_extrapolation(steps, errors, nmr_steps_remaining);\n                barrier(CLK_LOCAL_MEM_FENCE);                \n            }\n            \n            if(nmr_steps_remaining > 1){\n                _numdiff_find_best_step(steps, errors, steps_tmp, nmr_steps_remaining);\n                barrier(CLK_LOCAL_MEM_FENCE);\n            }\n            \n            if(get_local_id(0) == 0){\n                *derivative = steps[0];\n                *error = errors[0];\n            }\n        }\n    ''', dependencies=[\n        _get_numdiff_hessian_steps_func(objective_func, nmr_steps, step_ratio),\n        _get_numdiff_hessian_richardson_extrapolation_func(nmr_steps, step_ratio),\n        _get_numdiff_wynn_extrapolation_func(),\n        _get_numdiff_find_best_step_func()\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_numdiff_hessian_steps_func(objective_func, nmr_steps, step_ratio):\n    return SimpleCLFunction.from_string('''\n        /**\n         * Compute one element of the Hessian for a number of steps.\n         * \n         * This uses the initial steps in the data structure, indexed by the parameters to change (px, py).\n         *\n         * Args:\n         *  data: the data container\n         *  x_tmp: the array with the input parameters, needs to be writable, although it will return\n         *         the same values.\n         *  f_x_input: the objective function value at the original set of parameters  \n         *  px: the index of the first parameter to perturbate\n         *  py: the index of the second parameter to perturbate\n         *  steps: storage location for the output steps\n         *  initial_step: the initial steps, array of same length as x_temp\n         */\n        void _numdiff_hessian_steps(void* data, local mot_float_type* x_tmp, \n                                    mot_float_type f_x_input,\n                                    uint px, uint py, \n                                    local double* steps, \n                                    global float* initial_step){\n\n            double step_x;\n            double step_y;\n            double tmp;\n            bool is_first_workitem = get_local_id(0) == 0;\n\n            if(px == py){\n                for(uint step_ind = 0; step_ind < ''' + str(nmr_steps) + '''; step_ind++){\n                    step_x = initial_step[px] / pown(''' + str(float(step_ratio)) + ''', step_ind);\n\n                    tmp = (\n                          _numdiff_hessian_eval_step_mono(data, x_tmp, px, 2 * step_x)\n                        + _numdiff_hessian_eval_step_mono(data, x_tmp, px, -2 * step_x)\n                        - 2 * f_x_input\n                    ) / (4 * step_x * step_x);    \n\n                    if(is_first_workitem){\n                        steps[step_ind] = tmp;\n                    }\n                }\n            }\n            else{\n                for(uint step_ind = 0; step_ind < ''' + str(nmr_steps) + '''; step_ind++){\n                    step_x = initial_step[px] / pown(''' + str(float(step_ratio)) + ''', step_ind);\n                    step_y = initial_step[py] / pown(''' + str(float(step_ratio)) + ''', step_ind);\n\n                    tmp = (\n                          _numdiff_hessian_eval_step_bi(data, x_tmp, px, step_x, py, step_y)\n                        - _numdiff_hessian_eval_step_bi(data, x_tmp, px, step_x, py, -step_y)\n                        - _numdiff_hessian_eval_step_bi(data, x_tmp, px, -step_x, py, step_y)\n                        + _numdiff_hessian_eval_step_bi(data, x_tmp, px, -step_x, py, -step_y)\n                    ) / (4 * step_x * step_y);\n\n                    if(is_first_workitem){\n                        steps[step_ind] = tmp;\n                    }                       \n                }\n            }\n        }\n    ''', dependencies=[SimpleCLFunction.from_string('''\n        /**\n         * Evaluate the model with a perturbation in one dimensions.\n         *\n         * Args:\n         *  data: the data container\n         *  x_tmp: the array with the input parameters, needs to be writable, although it will return\n         *         the same values.\n         *  perturb_dim0: the index (into the x_tmp parameters) of the parameter to perturbate\n         *  perturb_0: the added perturbation of the index corresponding to ``perturb_dim_0``\n         *\n         * Returns:\n         *  the function evaluated at the parameters plus their perturbation.\n         */\n        double _numdiff_hessian_eval_step_mono(\n                void* data, local mot_float_type* x_tmp, \n                uint perturb_dim_0, mot_float_type perturb_0){\n\n            mot_float_type old_0;\n            double return_val;\n\n            if(get_local_id(0) == 0){\n                old_0 = x_tmp[perturb_dim_0];\n                x_tmp[perturb_dim_0] += perturb_0;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            return_val = ''' + objective_func.get_cl_function_name() + '''(x_tmp, data);\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            if(get_local_id(0) == 0){\n                x_tmp[perturb_dim_0] = old_0;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            return return_val;\n        }\n    '''), SimpleCLFunction.from_string('''\n        /**\n         * Evaluate the model with a perturbation in two dimensions.\n         *\n         * Args:\n         *  data: the data container\n         *  x_tmp: the array with the input parameters, needs to be writable, although it will return\n         *         the same values.\n         *  perturb_dim_0: the index (into the x_tmp parameters) of the first parameter to perturbate\n         *  perturb_0: the added perturbation of the index corresponding to ``perturb_dim_0``\n         *  perturb_dim_1: the index (into the x_tmp parameters) of the second parameter to perturbate\n         *  perturb_1: the added perturbation of the index corresponding to ``perturb_dim_1``\n         *\n         * Returns:\n         *  the function evaluated at the parameters plus their perturbation.\n         */\n        double _numdiff_hessian_eval_step_bi(\n                void* data, local mot_float_type* x_tmp, \n                uint perturb_dim_0, mot_float_type perturb_0,\n                uint perturb_dim_1, mot_float_type perturb_1){\n\n            mot_float_type old_0;\n            mot_float_type old_1;\n            double return_val;\n\n            if(get_local_id(0) == 0){\n                old_0 = x_tmp[perturb_dim_0];\n                old_1 = x_tmp[perturb_dim_1];\n\n                x_tmp[perturb_dim_0] += perturb_0;\n                x_tmp[perturb_dim_1] += perturb_1;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            return_val = ''' + objective_func.get_cl_function_name() + '''(x_tmp, data);\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            if(get_local_id(0) == 0){\n                x_tmp[perturb_dim_0] = old_0;\n                x_tmp[perturb_dim_1] = old_1;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            return return_val;\n        }\n    ''')])", "response": "Get a function to compute the multiple step sizes for a single element of the Hessian."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an initial step size to use for every parameter. This chooses the step sizes based on the maximum step size and the lower and upper bounds. Args: parameters (ndarray): The parameters at which to evaluate the gradient. A (d, p) matrix with d problems, p parameters and n samples. lower_bounds (list): lower bounds upper_bounds (list): upper bounds max_step_sizes (list or None): the maximum step size, or the maximum step size per parameter. Defaults to 0.1 Returns: ndarray: for every problem instance the vector with the initial step size for each parameter.", "response": "def _get_initial_step(parameters, lower_bounds, upper_bounds, max_step_sizes):\n    \"\"\"Get an initial step size to use for every parameter.\n\n    This chooses the step sizes based on the maximum step size and the lower and upper bounds.\n\n    Args:\n        parameters (ndarray): The parameters at which to evaluate the gradient. A (d, p) matrix with d problems,\n            p parameters and n samples.\n        lower_bounds (list): lower bounds\n        upper_bounds (list): upper bounds\n        max_step_sizes (list or None): the maximum step size, or the maximum step size per parameter. Defaults to 0.1\n\n    Returns:\n        ndarray: for every problem instance the vector with the initial step size for each parameter.\n    \"\"\"\n    nmr_params = parameters.shape[1]\n\n    initial_step = np.zeros_like(parameters)\n\n    if max_step_sizes is None:\n        max_step_sizes = 0.1\n    if isinstance(max_step_sizes, Number):\n        max_step_sizes = [max_step_sizes] * nmr_params\n    max_step_sizes = np.array(max_step_sizes)\n\n    for ind in range(parameters.shape[1]):\n        minimum_allowed_step = np.minimum(np.abs(parameters[:, ind] - lower_bounds[ind]),\n                                          np.abs(upper_bounds[ind] - parameters[:, ind]))\n        initial_step[:, ind] = np.minimum(minimum_allowed_step, max_step_sizes[ind])\n\n    return initial_step / 2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(self):\n        self._old_config = {k: v for k, v in _config.items()}\n        self._apply()", "response": "Apply the current action to the current runtime configuration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the current configuration to the previous state.", "response": "def unapply(self):\n        \"\"\"Reset the current configuration to the previous state.\"\"\"\n        for key, value in self._old_config.items():\n            _config[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassigns an attribute reference to all subsequent tasks.", "response": "def cache(self, name, val, overwrite=True):\n        \"\"\"Assigns an attribute reference to all subsequent tasks. For example, if a task caches a\n        DataFrame `df` using `self.cache('some_df', df)`, all tasks that follow can access the\n        DataFrame using `self.some_df`. Note that manually assigned attributes that share the same\n        name have precedence over cached attributes.\n\n        Parameters\n        ----------\n        name : str\n            Name of the attribute\n        val\n            Attribute value\n        overwrite : bool\n            Indicates if the attribute shall be overwritten, or not (if `False`, and\n            a cached attribute with the given name already exists, `sparklanes.errors.CacheError`\n            will be thrown).\n        \"\"\"\n        if name in TaskCache.cached and not overwrite:\n            raise CacheError('Object with name `%s` already in cache.' % name)\n        TaskCache.cached[name] = val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n        self.exc = None\n        try:\n            self.task()\n        except BaseException:\n            self.exc = sys.exc_info()", "response": "Overwrites threading. Thread. run to allow handling of exceptions thrown by threads\n            from within the main app."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef join(self, timeout=None):\n        Thread.join(self, timeout=timeout)\n        if self.exc:\n            msg = \"Thread '%s' threw an exception `%s`: %s\" \\\n                  % (self.getName(), self.exc[0].__name__, self.exc[1])\n            new_exc = LaneExecutionError(msg)\n\n            if PY3:\n                raise new_exc.with_traceback(self.exc[2])  # pylint: disable=no-member\n            else:\n                raise (new_exc.__class__, new_exc, self.exc[2])", "response": "Overwrites threading. Thread. join to allow handling of exceptions thrown by threads\n            from within the main app."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mock_decorator(*args, **kwargs):\n    def _called_decorator(dec_func):\n        @wraps(dec_func)\n        def _decorator(*args, **kwargs):\n            return dec_func()\n        return _decorator\n    return _called_decorator", "response": "Mocks a function to be called by the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmock all modules starting with one of the mock_modules names.", "response": "def import_mock(name, *args, **kwargs):\n    \"\"\"Mock all modules starting with one of the mock_modules names.\"\"\"\n    if any(name.startswith(s) for s in mock_modules):\n        return MockModule()\n    return orig_import(name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_cl_function(cl_function, kernel_data, nmr_instances, use_local_reduction=False, cl_runtime_info=None):\n    cl_runtime_info = cl_runtime_info or CLRuntimeInfo()\n    cl_environments = cl_runtime_info.cl_environments\n\n    for param in cl_function.get_parameters():\n        if param.name not in kernel_data:\n            names = [param.name for param in cl_function.get_parameters()]\n            missing_names = [name for name in names if name not in kernel_data]\n            raise ValueError('Some parameters are missing an input value, '\n                             'required parameters are: {}, missing inputs are: {}'.format(names, missing_names))\n\n    if cl_function.get_return_type() != 'void':\n        kernel_data['_results'] = Zeros((nmr_instances,), cl_function.get_return_type())\n\n    workers = []\n    for ind, cl_environment in enumerate(cl_environments):\n        worker = _ProcedureWorker(cl_environment, cl_runtime_info.compile_flags,\n                                  cl_function, kernel_data, cl_runtime_info.double_precision, use_local_reduction)\n        workers.append(worker)\n\n    def enqueue_batch(batch_size, offset):\n        items_per_worker = [batch_size // len(cl_environments) for _ in range(len(cl_environments) - 1)]\n        items_per_worker.append(batch_size - sum(items_per_worker))\n\n        for ind, worker in enumerate(workers):\n            worker.calculate(offset, offset + items_per_worker[ind])\n            offset += items_per_worker[ind]\n            worker.cl_queue.flush()\n\n        for worker in workers:\n            worker.cl_queue.finish()\n\n        return offset\n\n    total_offset = 0\n    for batch_start, batch_end in split_in_batches(nmr_instances, 1e4 * len(workers)):\n        total_offset = enqueue_batch(batch_end - batch_start, total_offset)\n\n    if cl_function.get_return_type() != 'void':\n        return kernel_data['_results'].get_data()", "response": "This function will run the given function on the given set of data instances and then execute the given function on the given set of data instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_string(cls, cl_function, dependencies=()):\n        return_type, function_name, parameter_list, body = split_cl_function(cl_function)\n        return SimpleCLFunction(return_type, function_name, parameter_list, body, dependencies=dependencies)", "response": "Parse the given string into a SimpleCLFunction object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_parameter_signatures(self):\n        declarations = []\n        for p in self.get_parameters():\n            new_p = p.get_renamed(p.name.replace('.', '_'))\n            declarations.append(new_p.get_declaration())\n        return declarations", "response": "Get the list of signatures of the parameters for the CL function declaration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the CL code for all the dependencies.", "response": "def _get_cl_dependency_code(self):\n        \"\"\"Get the CL code for all the CL code for all the dependencies.\n\n        Returns:\n            str: The CL code with the actual code.\n        \"\"\"\n        code = ''\n        for d in self._dependencies:\n            code += d.get_cl_code() + \"\\n\"\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of kernel arguments for loading the kernel data elements into the kernel.", "response": "def _get_kernel_arguments(self):\n        \"\"\"Get the list of kernel arguments for loading the kernel data elements into the kernel.\n\n        This will use the sorted keys for looping through the kernel input items.\n\n        Returns:\n            list of str: the list of parameter definitions\n        \"\"\"\n        declarations = []\n        for name, data in self._kernel_data.items():\n            declarations.extend(data.get_kernel_parameters('_' + name))\n        return declarations"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_scalar_arg_dtypes(self):\n        dtypes = []\n        for name, data in self._kernel_data.items():\n            dtypes.extend(data.get_scalar_arg_dtypes())\n        return dtypes", "response": "Get the location and types of the input scalars."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and initializes a new `SparkContext` (the old one will be stopped). Argument signature is copied from `pyspark.SparkContext <https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext>`_.", "response": "def set_sc(cls, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None,\n               batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None,\n               profiler_cls=BasicProfiler):\n        \"\"\"Creates and initializes a new `SparkContext` (the old one will be stopped).\n        Argument signature is copied from `pyspark.SparkContext\n        <https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext>`_.\n        \"\"\"\n        if cls.sc is not None:\n            cls.sc.stop()\n        cls.sc = SparkContext(master, appName, sparkHome, pyFiles, environment, batchSize,\n                              serializer,\n                              conf, gateway, jsc, profiler_cls)\n        cls.__init_spark()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_spark(cls, master=None, appName=None, conf=None, hive_support=False):\n        sess = SparkSession.builder\n        if master:\n            sess.master(master)\n        if appName:\n            sess.appName(appName)\n        if conf:\n            sess.config(conf=conf)\n        if hive_support:\n            sess.enableHiveSupport()\n\n        cls.spark = sess.getOrCreate()", "response": "Creates and initializes a new SparkSession."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _package_and_submit(args):\n    args = _parse_and_validate_args(args)\n\n    logging.debug(args)\n    dist = __make_tmp_dir()\n    try:\n        __package_dependencies(dist_dir=dist, additional_reqs=args['requirements'],\n                               silent=args['silent'])\n        __package_app(tasks_pkg=args['package'],\n                      dist_dir=dist,\n                      custom_main=args['main'],\n                      extra_data=args['extra_data'])\n        __run_spark_submit(lane_yaml=args['yaml'],\n                           dist_dir=dist,\n                           spark_home=args['spark_home'],\n                           spark_args=args['spark_args'],\n                           silent=args['silent'])\n\n    except Exception as exc:\n        __clean_up(dist)\n        raise exc\n    __clean_up(dist)", "response": "Packages and submits a job to Spark."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing and validate the given arguments.", "response": "def _parse_and_validate_args(args):\n    \"\"\"\n    Parse and validate arguments. During validation, it is checked whether the given\n    files/directories exist, while also converting relative paths to absolute ones.\n\n    Parameters\n    ----------\n    args (List): Command-line arguments\n    \"\"\"\n    class ExtendAction(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            if getattr(namespace, self.dest, None) is None:\n                setattr(namespace, self.dest, [])\n            getattr(namespace, self.dest).extend(values)\n\n    parser = argparse.ArgumentParser(description='Submitting a lane to spark.')\n    parser.add_argument('-y', '--yaml', type=str, required=True,\n                        help='Path to the yaml definition file.')\n    parser.add_argument('-p', '--package', type=str, required=True,\n                        help='Path to the python package containing your tasks.')\n    parser.add_argument('-r', '--requirements', type=str, required=False,\n                        help='Path to a `requirements.txt` specifying any additional dependencies '\n                             'of your tasks.')\n    parser.add_argument('-e', '--extra-data', nargs='*', required=False, action=ExtendAction,\n                        help='Path to any additional files or directories that should be packaged '\n                             'and sent to Spark.')\n    parser.add_argument('-m', '--main', type=str, required=False,\n                        help='Path to a custom main python file')\n    parser.add_argument('-d', '--spark-home', type=str, required=False,\n                        help='Custom path to the directory containing your Spark installation. If '\n                             'none is given, sparklanes will try to use the `spark-submit` command '\n                             'from your PATH')\n    parser.add_argument('-s', '--spark-args', nargs='*', required=False,\n                        help='Any additional arguments that should be sent to Spark via '\n                             'spark-submit. '\n                             '(e.g. `--spark-args executor-memory=20G total-executor-cores=100`)')\n    parser.add_argument('--silent', help='If set, no output will be sent to console',\n                        action='store_true')\n    args = parser.parse_args(args).__dict__\n\n    # Check/fix files/dirs\n    for param in ('package', 'spark_home'):\n        args[param] = __validate_and_fix_path(args[param], check_dir=True)\n    for param in ('yaml', 'requirements', 'main'):\n        args[param] = __validate_and_fix_path(args[param], check_file=True)\n    if args['extra_data']:\n        for i in range(len(args['extra_data'])):\n            args['extra_data'][i] = __validate_and_fix_path(args['extra_data'][i],\n                                                            check_file=True, check_dir=True)\n\n    # Check if python package\n    if not os.path.isfile(os.path.join(args['package'], '__init__.py')):\n        raise SystemExit('Could not confirm `%s` is a python package. Make sure it contains an '\n                         '`__init__.py`.')\n\n    # Check/fix spark args\n    if args['spark_args']:\n        args['spark_args'] = __validate_and_fix_spark_args(args['spark_args'])\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a file or directory exists and converts relative paths to absolute ones", "response": "def __validate_and_fix_path(path, check_file=False, check_dir=False):\n    \"\"\"Check if a file/directory exists and converts relative paths to absolute ones\"\"\"\n    # pylint: disable=superfluous-parens\n    if path is None:\n        return path\n    else:\n        if not (os.path.isfile(path) if check_file else False) \\\n                and not (os.path.isdir(path) if check_dir else False):\n            raise SystemExit('Path `%s` does not exist' % path)\n        if not os.path.isabs(path):\n            path = os.path.abspath(os.path.join(os.path.abspath(os.curdir), path))\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __validate_and_fix_spark_args(spark_args):\n    pattern = re.compile(r'[\\w\\-_]+=.+')\n    fixed_args = []\n    for arg in spark_args:\n        if arg not in SPARK_SUBMIT_FLAGS:\n            if not pattern.match(arg):\n                raise SystemExit('Spark argument `%s` does not seem to be in the correct format '\n                                 '`ARG_NAME=ARG_VAL`, and is also not recognized to be one of the'\n                                 'valid spark-submit flags (%s).' % (arg, str(SPARK_SUBMIT_FLAGS)))\n            eq_pos = arg.find('=')\n            fixed_args.append('--' + arg[:eq_pos])\n            fixed_args.append(arg[eq_pos + 1:])\n        else:\n            fixed_args.append('--' + arg)\n\n    return fixed_args", "response": "Validate and fix spark arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npackaging the app s dependencies from pip and packages them to spark", "response": "def __package_dependencies(dist_dir, additional_reqs, silent):\n    \"\"\"\n    Installs the app's dependencies from pip and packages them (as zip), to be submitted to spark.\n\n    Parameters\n    ----------\n    dist_dir (str): Path to directory where the packaged libs shall be located\n    additional_reqs (str): Path to a requirements.txt, containing any of the app's additional\n        requirements\n    silent (bool): Flag indicating whether pip output should be printed to console\n    \"\"\"\n    logging.info('Packaging dependencies')\n    libs_dir = os.path.join(dist_dir, 'libs')\n    if not os.path.isdir(libs_dir):\n        os.mkdir(libs_dir)\n\n    # Get requirements\n    req_txt = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'requirements-submit.txt')\n    with open(req_txt, 'r') as req:\n        requirements = req.read().splitlines()\n    if additional_reqs:\n        with open(additional_reqs, 'r') as req:\n            for row in req:\n                requirements.append(row)\n\n    # Remove duplicates\n    requirements = list(set(requirements))\n\n    # Install\n    devnull = open(os.devnull, 'w')\n    outp = {'stderr': STDOUT, 'stdout': devnull} if silent else {}\n    for pkg in requirements:\n        cmd = ['pip', 'install', pkg, '-t', libs_dir]\n        logging.debug('Calling `%s`', str(cmd))\n        call(cmd, **outp)\n    devnull.close()\n\n    # Package\n    shutil.make_archive(libs_dir, 'zip', libs_dir, './')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __package_app(tasks_pkg, dist_dir, custom_main=None, extra_data=None):\n    logging.info('Packaging application')\n\n    # Package tasks\n    tasks_dir_splits = os.path.split(os.path.realpath(tasks_pkg))\n    shutil.make_archive(os.path.join(dist_dir, 'tasks'),\n                        'zip',\n                        tasks_dir_splits[0],\n                        tasks_dir_splits[1])\n\n    # Package main.py\n    if custom_main is None:\n        from . import _main\n        main_path = _main.__file__\n        if main_path[-3:] == 'pyc':\n            main_path = main_path[:-1]\n        shutil.copy(os.path.realpath(main_path),\n                    os.path.join(dist_dir, 'main.py'))\n    else:\n        shutil.copy(os.path.realpath(custom_main),\n                    os.path.join(dist_dir, 'main.py'))\n\n    # Package _framework\n    shutil.make_archive(os.path.join(dist_dir, '_framework'),\n                        'zip',\n                        os.path.join(os.path.dirname(os.path.realpath(__file__)), '..', '..'),\n                        './sparklanes/')\n\n    # Package extra data\n    if extra_data:\n        for dat in extra_data:\n            real_path = os.path.realpath(dat)\n            target = os.path.join(dist_dir, os.path.split(real_path)[1])\n            if os.path.isfile(real_path):\n                shutil.copy(real_path, target)\n            elif os.path.isdir(real_path):\n                shutil.copytree(real_path, target)\n            else:\n                raise IOError('File `%s` not found at `%s`.' % (dat, real_path))", "response": "Packages the tasks_pkg to dist_dir. Also copies the main. py file to spark."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __run_spark_submit(lane_yaml, dist_dir, spark_home, spark_args, silent):\n    # spark-submit binary\n    cmd = ['spark-submit' if spark_home is None else os.path.join(spark_home, 'bin/spark-submit')]\n\n    # Supplied spark arguments\n    if spark_args:\n        cmd += spark_args\n\n    # Packaged App & lane\n    cmd += ['--py-files', 'libs.zip,_framework.zip,tasks.zip', 'main.py']\n    cmd += ['--lane', lane_yaml]\n\n    logging.info('Submitting to Spark')\n    logging.debug(str(cmd))\n\n    # Submit\n    devnull = open(os.devnull, 'w')\n    outp = {'stderr': STDOUT, 'stdout': devnull} if silent else {}\n    call(cmd, cwd=dist_dir, env=MY_ENV, **outp)\n    devnull.close()", "response": "Submits the packaged application to Spark using a spark - submit subprocess"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds include guards to the given string.", "response": "def add_include_guards(cl_str, guard_name=None):\n    \"\"\"Add include guards to the given string.\n\n    If you are including the same body of CL code multiple times in a Kernel, it is important to add include\n    guards (https://en.wikipedia.org/wiki/Include_guard) around them to prevent the kernel from registering the function\n    twice.\n\n    Args:\n        cl_str (str): the piece of CL code as a string to which we add the include guards\n        guard_name (str): the name of the C pre-processor guard. If not given we use the MD5 hash of the\n            given cl string.\n\n    Returns:\n        str: the same string but then with include guards around them.\n    \"\"\"\n    if not guard_name:\n        guard_name = 'GUARD_' + hashlib.md5(cl_str.encode('utf-8')).hexdigest()\n\n    return '''\n        # ifndef {guard_name}\n        # define {guard_name}\n        {func_str}\n        # endif // {guard_name}\n    '''.format(func_str=cl_str, guard_name=guard_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the numpy datatype of the given CL data type string.", "response": "def ctype_to_dtype(cl_type, mot_float_type='float'):\n    \"\"\"Get the numpy dtype of the given cl_type string.\n\n    Args:\n        cl_type (str): the CL data type to match, for example 'float' or 'float4'.\n        mot_float_type (str): the C name of the ``mot_float_type``. The dtype will be looked up recursively.\n\n    Returns:\n        dtype: the numpy datatype\n    \"\"\"\n    if is_vector_ctype(cl_type):\n        raw_type, vector_length = split_vector_ctype(cl_type)\n\n        if raw_type == 'mot_float_type':\n            if is_vector_ctype(mot_float_type):\n                raw_type, _ = split_vector_ctype(mot_float_type)\n            else:\n                raw_type = mot_float_type\n\n        vector_type = raw_type + str(vector_length)\n        return getattr(cl_array.vec, vector_type)\n    else:\n        if cl_type == 'mot_float_type':\n            cl_type = mot_float_type\n\n        data_types = [\n            ('char', np.int8),\n            ('uchar', np.uint8),\n            ('short', np.int16),\n            ('ushort', np.uint16),\n            ('int', np.int32),\n            ('uint', np.uint32),\n            ('long', np.int64),\n            ('ulong', np.uint64),\n            ('float', np.float32),\n            ('double', np.float64),\n        ]\n        for ctype, dtype in data_types:\n            if ctype == cl_type:\n                return dtype"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_data_to_dtype(data, data_type, mot_float_type='float'):\n    scalar_dtype = ctype_to_dtype(data_type, mot_float_type)\n\n    if isinstance(data, numbers.Number):\n        data = scalar_dtype(data)\n\n    if is_vector_ctype(data_type):\n        shape = data.shape\n        dtype = ctype_to_dtype(data_type, mot_float_type)\n        ve = np.zeros(shape[:-1], dtype=dtype)\n\n        if len(shape) == 1:\n            for vector_ind in range(shape[0]):\n                ve[0][vector_ind] = data[vector_ind]\n        elif len(shape) == 2:\n            for i in range(data.shape[0]):\n                for vector_ind in range(data.shape[1]):\n                    ve[i][vector_ind] = data[i, vector_ind]\n        elif len(shape) == 3:\n            for i in range(data.shape[0]):\n                for j in range(data.shape[1]):\n                    for vector_ind in range(data.shape[2]):\n                        ve[i, j][vector_ind] = data[i, j, vector_ind]\n\n        return np.require(ve, requirements=['C', 'A', 'O'])\n    return np.require(data, scalar_dtype, ['C', 'A', 'O'])", "response": "Convert the given input data to the correct numpy type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_vector_ctype(ctype):\n    if not is_vector_ctype(ctype):\n        raise ValueError('The given ctype is not a vector type.')\n    for vector_length in [2, 3, 4, 8, 16]:\n        if ctype.endswith(str(vector_length)):\n            vector_str_len = len(str(vector_length))\n            return ctype[:-vector_str_len], int(ctype[-vector_str_len:])", "response": "Split a vector ctype into a raw ctype and the vector length."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef device_type_from_string(cl_device_type_str):\n    cl_device_type_str = cl_device_type_str.upper()\n    if hasattr(cl.device_type, cl_device_type_str):\n        return getattr(cl.device_type, cl_device_type_str)\n    return None", "response": "Converts a string like gpu to a pyopencl device type string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the model floating point type definition.", "response": "def get_float_type_def(double_precision, include_complex=True):\n    \"\"\"Get the model floating point type definition.\n\n    Args:\n        double_precision (boolean): if True we will use the double type for the mot_float_type type.\n            Else, we will use the single precision float type for the mot_float_type type.\n        include_complex (boolean): if we include support for complex numbers\n\n    Returns:\n        str: defines the mot_float_type types, the epsilon and the MIN and MAX values.\n    \"\"\"\n    if include_complex:\n        with open(os.path.abspath(resource_filename('mot', 'data/opencl/complex.h')), 'r') as f:\n            complex_number_support = f.read()\n    else:\n        complex_number_support = ''\n\n    scipy_constants = '''\n        #define MACHEP DBL_EPSILON\n        #define MAXLOG log(DBL_MAX)\n        #define LANCZOS_G 6.024680040776729583740234375 /* taken from Scipy */\n        #define EULER 0.577215664901532860606512090082402431 /* Euler constant, from Scipy */\n    '''\n\n    if double_precision:\n        return '''\n            #if __OPENCL_VERSION__ <= CL_VERSION_1_1\n                #pragma OPENCL EXTENSION cl_khr_fp64 : enable\n            #endif\n\n            #define PYOPENCL_DEFINE_CDOUBLE\n\n            typedef double mot_float_type;\n            typedef double2 mot_float_type2;\n            typedef double4 mot_float_type4;\n            typedef double8 mot_float_type8;\n            typedef double16 mot_float_type16;\n\n            #define MOT_EPSILON DBL_EPSILON\n            #define MOT_MIN DBL_MIN\n            #define MOT_MAX DBL_MAX\n        ''' + scipy_constants + complex_number_support\n    else:\n        return '''\n            #if __OPENCL_VERSION__ <= CL_VERSION_1_1\n                #pragma OPENCL EXTENSION cl_khr_fp64 : enable\n            #endif\n\n            typedef float mot_float_type;\n            typedef float2 mot_float_type2;\n            typedef float4 mot_float_type4;\n            typedef float8 mot_float_type8;\n            typedef float16 mot_float_type16;\n\n            #define MOT_EPSILON FLT_EPSILON\n            #define MOT_MIN FLT_MIN\n            #define MOT_MAX FLT_MAX\n        ''' + scipy_constants + complex_number_support"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef topological_sort(data):\n\n    def check_self_dependencies(input_data):\n        \"\"\"Check if there are self dependencies within a node.\n\n        Self dependencies are for example: ``{'a': ('a',)}``.\n\n        Args:\n            input_data (dict): the input data. Of a structure similar to {key: (list of values), ...}.\n\n        Raises:\n            ValueError: if there are indeed self dependencies\n        \"\"\"\n        for k, v in input_data.items():\n            if k in v:\n                raise ValueError('Self-dependency, {} depends on itself.'.format(k))\n\n    def prepare_input_data(input_data):\n        \"\"\"Prepares the input data by making sets of the dependencies. This automatically removes redundant items.\n\n        Args:\n            input_data (dict): the input data. Of a structure similar to {key: (list of values), ...}.\n\n        Returns:\n            dict: a copy of the input dict but with sets instead of lists for the dependencies.\n        \"\"\"\n        return {k: set(v) for k, v in input_data.items()}\n\n    def find_items_without_dependencies(input_data):\n        \"\"\"This searches the dependencies of all the items for items that have no dependencies.\n\n        For example, suppose the input is: ``{'a': ('b',)}``, then ``a`` depends on ``b`` and ``b`` depends on nothing.\n        This class returns ``(b,)`` in this example.\n\n        Args:\n            input_data (dict): the input data. Of a structure similar to {key: (list of values), ...}.\n\n        Returns:\n            list: the list of items without any dependency.\n        \"\"\"\n        return list(reduce(set.union, input_data.values()) - set(input_data.keys()))\n\n    def add_empty_dependencies(data):\n        items_without_dependencies = find_items_without_dependencies(data)\n        data.update({item: set() for item in items_without_dependencies})\n\n    def get_sorted(input_data):\n        data = input_data\n        while True:\n            ordered = set(item for item, dep in data.items() if len(dep) == 0)\n            if not ordered:\n                break\n            yield ordered\n            data = {item: (dep - ordered) for item, dep in data.items() if item not in ordered}\n\n        if len(data) != 0:\n            raise ValueError('Cyclic dependencies exist '\n                             'among these items: {}'.format(', '.join(repr(x) for x in data.items())))\n\n    check_self_dependencies(data)\n\n    if not len(data):\n        return []\n\n    data_copy = prepare_input_data(data)\n    add_empty_dependencies(data_copy)\n\n    result = []\n    for d in get_sorted(data_copy):\n        try:\n            d = sorted(d)\n        except TypeError:\n            d = list(d)\n\n        result.extend(d)\n    return result", "response": "Topological sort the given dictionary structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntests if the given value is a scalar.", "response": "def is_scalar(value):\n    \"\"\"Test if the given value is a scalar.\n\n    This function also works with memory mapped array values, in contrast to the numpy is_scalar method.\n\n    Args:\n        value: the value to test for being a scalar value\n\n    Returns:\n        boolean: if the given value is a scalar or not\n    \"\"\"\n    return np.isscalar(value) or (isinstance(value, np.ndarray) and (len(np.squeeze(value).shape) == 0))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if all elements in the given value are equal to each other.", "response": "def all_elements_equal(value):\n    \"\"\"Checks if all elements in the given value are equal to each other.\n\n    If the input is a single value the result is trivial. If not, we compare all the values to see\n    if they are exactly the same.\n\n    Args:\n        value (ndarray or number): a numpy array or a single number.\n\n    Returns:\n        bool: true if all elements are equal to each other, false otherwise\n    \"\"\"\n    if is_scalar(value):\n        return True\n    return np.array(value == value.flatten()[0]).all()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a single value out of the given value.", "response": "def get_single_value(value):\n    \"\"\"Get a single value out of the given value.\n\n    This is meant to be used after a call to :func:`all_elements_equal` that returned True. With this\n    function we return a single number from the input value.\n\n    Args:\n        value (ndarray or number): a numpy array or a single number.\n\n    Returns:\n        number: a single number from the input\n\n    Raises:\n        ValueError: if not all elements are equal\n    \"\"\"\n    if not all_elements_equal(value):\n        raise ValueError('Not all values are equal to each other.')\n\n    if is_scalar(value):\n        return value\n    return value.item(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisables all logging temporarily. A context manager that will prevent any logging messages triggered during the body from being processed. Args: highest_level: the maximum logging level that is being blocked", "response": "def all_logging_disabled(highest_level=logging.CRITICAL):\n    \"\"\"Disable all logging temporarily.\n\n    A context manager that will prevent any logging messages triggered during the body from being processed.\n\n    Args:\n        highest_level: the maximum logging level that is being blocked\n    \"\"\"\n    previous_level = logging.root.manager.disable\n    logging.disable(highest_level)\n    try:\n        yield\n    finally:\n        logging.disable(previous_level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the total number of elements into batches of the specified maximum size.", "response": "def split_in_batches(nmr_elements, max_batch_size):\n    \"\"\"Split the total number of elements into batches of the specified maximum size.\n\n    Examples::\n        split_in_batches(30, 8) -> [(0, 8), (8, 15), (16, 23), (24, 29)]\n\n        for batch_start, batch_end in split_in_batches(2000, 100):\n            array[batch_start:batch_end]\n\n    Yields:\n        tuple: the start and end point of the next batch\n    \"\"\"\n    offset = 0\n    elements_left = nmr_elements\n    while elements_left > 0:\n        next_batch = (offset, offset + min(elements_left, max_batch_size))\n        yield next_batch\n\n        batch_size = min(elements_left, max_batch_size)\n        elements_left -= batch_size\n        offset += batch_size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming a covariance matrix into a correlations matrix.", "response": "def covariance_to_correlations(covariance):\n    \"\"\"Transform a covariance matrix into a correlations matrix.\n\n    This can be seen as dividing a covariance matrix by the outer product of the diagonal.\n\n    As post processing we replace the infinities and the NaNs with zeros and clip the result to [-1, 1].\n\n    Args:\n        covariance (ndarray): a matrix of shape (n, p, p) with for n problems the covariance matrix of shape (p, p).\n\n    Returns:\n        ndarray: the correlations matrix\n    \"\"\"\n    diagonal_ind = np.arange(covariance.shape[1])\n    diagonal_els = covariance[:, diagonal_ind, diagonal_ind]\n    result = covariance / np.sqrt(diagonal_els[:, :, None] * diagonal_els[:, None, :])\n    result[np.isinf(result)] = 0\n    return np.clip(np.nan_to_num(result), -1, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multiprocess_mapping(func, iterable):\n    if os.name == 'nt':  # In Windows there is no fork.\n        return list(map(func, iterable))\n    try:\n        p = multiprocessing.Pool()\n        return_data = list(p.imap(func, iterable))\n        p.close()\n        p.join()\n        return return_data\n    except OSError:\n        return list(map(func, iterable))", "response": "Multiprocess mapping the given function on the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cl_function(cl_code, dependencies=()):\n    from mot.lib.cl_function import SimpleCLFunction\n\n    def separate_cl_functions(input_str):\n        \"\"\"Separate all the OpenCL functions.\n\n        This creates a list of strings, with for each function found the OpenCL code.\n\n        Args:\n            input_str (str): the string containing one or more functions.\n\n        Returns:\n            list: a list of strings, with one string per found CL function.\n        \"\"\"\n        class Semantics:\n\n            def __init__(self):\n                self._functions = []\n\n            def result(self, ast):\n                return self._functions\n\n            def arglist(self, ast):\n                return '({})'.format(', '.join(ast))\n\n            def function(self, ast):\n                def join(items):\n                    result = ''\n                    for item in items:\n                        if isinstance(item, str):\n                            result += item\n                        else:\n                            result += join(item)\n                    return result\n\n                self._functions.append(join(ast).strip())\n                return ast\n\n        return _extract_cl_functions_parser.parse(input_str, semantics=Semantics())\n\n    functions = separate_cl_functions(cl_code)\n    return SimpleCLFunction.from_string(functions[-1], dependencies=list(dependencies or []) + [\n        SimpleCLFunction.from_string(s) for s in functions[:-1]])", "response": "Parse the given OpenCL string to a single SimpleCLFunction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits a CL function into a return type function name parameter list and body.", "response": "def split_cl_function(cl_str):\n    \"\"\"Split an CL function into a return type, function name, parameters list and the body.\n\n    Args:\n        cl_str (str): the CL code to parse and plit into components\n\n    Returns:\n        tuple: string elements for the return type, function name, parameter list and the body\n    \"\"\"\n    class Semantics:\n\n        def __init__(self):\n            self._return_type = ''\n            self._function_name = ''\n            self._parameter_list = []\n            self._cl_body = ''\n\n        def result(self, ast):\n            return self._return_type, self._function_name, self._parameter_list, self._cl_body\n\n        def address_space(self, ast):\n            self._return_type = ast.strip() + ' '\n            return ast\n\n        def data_type(self, ast):\n            self._return_type += ''.join(ast).strip()\n            return ast\n\n        def function_name(self, ast):\n            self._function_name = ast.strip()\n            return ast\n\n        def arglist(self, ast):\n            if ast != '()':\n                self._parameter_list = ast\n            return ast\n\n        def body(self, ast):\n            def join(items):\n                result = ''\n                for item in items:\n                    if isinstance(item, str):\n                        result += item\n                    else:\n                        result += join(item)\n                return result\n\n            self._cl_body = join(ast).strip()[1:-1]\n            return ast\n\n    return _split_cl_function_parser.parse(cl_str, semantics=Semantics())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_default_logger(name=INTERNAL_LOGGER_NAME, level=logging.INFO,\n                        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s'):\n    \"\"\"Create a logger with the default configuration\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    if not logger.handlers:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setLevel(level)\n        formatter = logging.Formatter(fmt)\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n    return logger", "response": "Create a logger with the default configuration"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the device associated with this environment is a GPU.", "response": "def is_gpu(self):\n        \"\"\"Check if the device associated with this environment is a GPU.\n\n        Returns:\n            boolean: True if the device is an GPU, false otherwise.\n        \"\"\"\n        return self._device.get_info(cl.device_info.TYPE) == cl.device_type.GPU"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_cpu(self):\n        return self._device.get_info(cl.device_info.TYPE) == cl.device_type.CPU", "response": "Check if the device associated with this environment is a CPU."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef single_device(cl_device_type='GPU', platform=None, fallback_to_any_device_type=False):\n        if isinstance(cl_device_type, str):\n            cl_device_type = device_type_from_string(cl_device_type)\n\n        device = None\n\n        if platform is None:\n            platforms = cl.get_platforms()\n        else:\n            platforms = [platform]\n\n        for platform in platforms:\n            devices = platform.get_devices(device_type=cl_device_type)\n\n            for dev in devices:\n                if device_supports_double(dev):\n                    try:\n                        env = CLEnvironment(platform, dev)\n                        return [env]\n                    except cl.RuntimeError:\n                        pass\n\n        if not device:\n            if fallback_to_any_device_type:\n                return cl.get_platforms()[0].get_devices()\n            else:\n                raise ValueError('No devices of the specified type ({}) found.'.format(\n                    cl.device_type.to_string(cl_device_type)))\n\n        raise ValueError('No suitable OpenCL device found.')", "response": "Get a list containing a single environment for a given OpenCL device type on the given platform."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_devices(cl_device_type=None, platform=None):\n        if isinstance(cl_device_type, str):\n            cl_device_type = device_type_from_string(cl_device_type)\n\n        runtime_list = []\n\n        if platform is None:\n            platforms = cl.get_platforms()\n        else:\n            platforms = [platform]\n\n        for platform in platforms:\n            if cl_device_type:\n                devices = platform.get_devices(device_type=cl_device_type)\n            else:\n                devices = platform.get_devices()\n\n            for device in devices:\n                if device_supports_double(device):\n                    env = CLEnvironment(platform, device)\n                    runtime_list.append(env)\n\n        return runtime_list", "response": "Get multiple device environments optionally only of the indicated type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of device environments that is suitable for use in MOT. Basically this gets the total list of devices using all_devices() and applies a filter on it. This filter does the following: 1) if the 'AMD Accelerated Parallel Processing' is available remove all environments using the 'Clover' platform. More things may be implemented in the future. Args: preferred_device_type (str): the preferred device type, one of 'CPU', 'GPU' or 'APU'. If no devices of this type can be found, we will use any other device available. Returns: list of CLEnvironment: List with the CL device environments.", "response": "def smart_device_selection(preferred_device_type=None):\n        \"\"\"Get a list of device environments that is suitable for use in MOT.\n\n        Basically this gets the total list of devices using all_devices() and applies a filter on it.\n\n        This filter does the following:\n            1) if the 'AMD Accelerated Parallel Processing' is available remove all environments using the 'Clover'\n                platform.\n\n        More things may be implemented in the future.\n\n        Args:\n            preferred_device_type (str): the preferred device type, one of 'CPU', 'GPU' or 'APU'.\n                If no devices of this type can be found, we will use any other device available.\n\n        Returns:\n            list of CLEnvironment: List with the CL device environments.\n        \"\"\"\n        cl_environments = CLEnvironmentFactory.all_devices(cl_device_type=preferred_device_type)\n        platform_names = [env.platform.name for env in cl_environments]\n        has_amd_pro_platform = any('AMD Accelerated Parallel Processing' in name for name in platform_names)\n\n        if has_amd_pro_platform:\n            return list(filter(lambda env: 'Clover' not in env.platform.name, cl_environments))\n\n        if preferred_device_type is not None and not len(cl_environments):\n            return CLEnvironmentFactory.all_devices()\n        \n        return cl_environments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multivariate_ess(samples, batch_size_generator=None):\n    samples_generator = _get_sample_generator(samples)\n    return np.array(multiprocess_mapping(_MultivariateESSMultiProcessing(batch_size_generator), samples_generator()))", "response": "r Estimates the multivariate Effective Sample Size for the samples of every problem."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef univariate_ess(samples, method='standard_error', **kwargs):\n    samples_generator = _get_sample_generator(samples)\n    return np.array(multiprocess_mapping(_UnivariateESSMultiProcessing(method, **kwargs), samples_generator()))", "response": "r Estimates the univariate Effective Sample Size for the samples of every problem."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a sample generator from the given polymorphic input.", "response": "def _get_sample_generator(samples):\n    \"\"\"Get a sample generator from the given polymorphic input.\n\n    Args:\n        samples (ndarray, dict or generator): either an matrix of shape (d, p, n) with d problems, p parameters and\n            n samples, or a dictionary with for every parameter a matrix with shape (d, n) or, finally,\n            a generator function that yields sample arrays of shape (p, n).\n\n    Returns:\n        generator: a generator that yields a matrix of size (p, n) for every problem in the input.\n    \"\"\"\n    if isinstance(samples, Mapping):\n        def samples_generator():\n            for ind in range(samples[list(samples.keys())[0]].shape[0]):\n                yield np.array([samples[s][ind, :] for s in sorted(samples)])\n    elif isinstance(samples, np.ndarray):\n        def samples_generator():\n            for ind in range(samples.shape[0]):\n                yield samples[ind]\n    else:\n        samples_generator = samples\n    return samples_generator"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_univariate_ess_standard_error(chain, batch_size_generator=None, compute_method=None):\n    sigma = (monte_carlo_standard_error(chain, batch_size_generator=batch_size_generator,\n                                        compute_method=compute_method) ** 2 * len(chain))\n    lambda_ = np.var(chain, dtype=np.float64)\n    return len(chain) * (lambda_ / sigma)", "response": "r Estimates the univariate ESS using the standard error method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimum_multivariate_ess(nmr_params, alpha=0.05, epsilon=0.05):\n    tmp = 2.0 / nmr_params\n    log_min_ess = tmp * np.log(2) + np.log(np.pi) - tmp * (np.log(nmr_params) + gammaln(nmr_params / 2)) \\\n                  + np.log(chi2.ppf(1 - alpha, nmr_params)) - 2 * np.log(epsilon)\n    return int(round(np.exp(log_min_ess)))", "response": "r Calculates the minimum multivariate Effective Sample Size for a given set of free parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multivariate_ess_precision(nmr_params, multi_variate_ess, alpha=0.05):\n    tmp = 2.0 / nmr_params\n    log_min_ess = tmp * np.log(2) + np.log(np.pi) - tmp * (np.log(nmr_params) + gammaln(nmr_params / 2)) \\\n                  + np.log(chi2.ppf(1 - alpha, nmr_params)) - np.log(multi_variate_ess)\n    return np.sqrt(np.exp(log_min_ess))", "response": "r Calculates the precision given the multivariate Effective Sample Size and the confidence region of the MCMC sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef estimate_multivariate_ess_sigma(samples, batch_size):\n    sample_means = np.mean(samples, axis=1, dtype=np.float64)\n    nmr_params, chain_length = samples.shape\n\n    nmr_batches = int(np.floor(chain_length / batch_size))\n    sigma = np.zeros((nmr_params, nmr_params))\n\n    nmr_offsets = chain_length - nmr_batches * batch_size + 1\n\n    for offset in range(nmr_offsets):\n        batches = np.reshape(samples[:, np.array(offset + np.arange(0, nmr_batches * batch_size), dtype=np.int)].T,\n                             [batch_size, nmr_batches, nmr_params], order='F')\n\n        batch_means = np.squeeze(np.mean(batches, axis=0, dtype=np.float64))\n\n        Z = batch_means - sample_means\n\n        for x, y in itertools.product(range(nmr_params), range(nmr_params)):\n            sigma[x, y] += np.sum(Z[:, x] * Z[:, y])\n\n    return sigma * batch_size / (nmr_batches - 1) / nmr_offsets", "response": "r Estimates the multivariate ESS sigma matrix for a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef monte_carlo_standard_error(chain, batch_size_generator=None, compute_method=None):\n    batch_size_generator = batch_size_generator or SquareRootSingleBatch()\n    compute_method = compute_method or BatchMeansMCSE()\n\n    batch_sizes = batch_size_generator.get_univariate_ess_batch_sizes(len(chain))\n\n    return np.min(list(compute_method.compute_standard_error(chain, b) for b in batch_sizes))", "response": "Compute Monte Carlo standard errors for the expectations\n ArcGIS Markov chain."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_gaussian(samples, ddof=0):\n    if len(samples.shape) == 1:\n        return np.mean(samples), np.std(samples, ddof=ddof)\n    return np.mean(samples, axis=1), np.std(samples, axis=1, ddof=ddof)", "response": "Calculates the mean and standard deviation of the given samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the circular mean for samples in a range of samples.", "response": "def fit_circular_gaussian(samples, high=np.pi, low=0):\n    \"\"\"Compute the circular mean for samples in a range\n\n    Args:\n        samples (ndarray): a one or two dimensional array. If one dimensional we calculate the fit using all\n            values. If two dimensional, we fit the Gaussian for every set of samples over the first dimension.\n        high (float): The maximum wrap point\n        low (float): The minimum wrap point\n    \"\"\"\n    cl_func = SimpleCLFunction.from_string('''\n        void compute(global mot_float_type* samples,\n                     global mot_float_type* means,\n                     global mot_float_type* stds,\n                     int nmr_samples,\n                     int low,\n                     int high){ \n        \n            double cos_mean = 0;\n            double sin_mean = 0;\n            double ang;\n\n            for(uint i = 0; i < nmr_samples; i++){\n                ang = (samples[i] - low)*2*M_PI / (high - low);\n\n                cos_mean += (cos(ang) - cos_mean) / (i + 1);\n                sin_mean += (sin(ang) - sin_mean) / (i + 1);\n            }\n\n            double R = hypot(cos_mean, sin_mean);\n            if(R > 1){\n                R = 1;\n            }\n\n            double stds = 1/2. * sqrt(-2 * log(R));\n\n            double res = atan2(sin_mean, cos_mean);\n            if(res < 0){\n                 res += 2 * M_PI;\n            }\n\n            *(means) = res*(high - low)/2.0/M_PI + low;\n            *(stds) = ((high - low)/2.0/M_PI) * sqrt(-2*log(R));\n        }\n    ''')\n\n    def run_cl(samples):\n        data = {'samples': Array(samples, 'mot_float_type'),\n                'means': Zeros(samples.shape[0], 'mot_float_type'),\n                'stds': Zeros(samples.shape[0], 'mot_float_type'),\n                'nmr_samples': Scalar(samples.shape[1]),\n                'low': Scalar(low),\n                'high': Scalar(high),\n                }\n\n        cl_func.evaluate(data, samples.shape[0])\n        return data['means'].get_data(), data['stds'].get_data()\n\n    if len(samples.shape) == 1:\n        mean, std = run_cl(samples[None, :])\n        return mean[0], std[0]\n    return run_cl(samples)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits a truncated Gaussian distribution on the given samples.", "response": "def fit_truncated_gaussian(samples, lower_bounds, upper_bounds):\n    \"\"\"Fits a truncated gaussian distribution on the given samples.\n\n    This will do a maximum likelihood estimation of a truncated Gaussian on the provided samples, with the\n    truncation points given by the lower and upper bounds.\n\n    Args:\n        samples (ndarray): a one or two dimensional array. If one dimensional we fit the truncated Gaussian on all\n            values. If two dimensional, we calculate the truncated Gaussian for every set of samples over the\n            first dimension.\n        lower_bounds (ndarray or float): the lower bound, either a scalar or a lower bound per problem (first index of\n            samples)\n        upper_bounds (ndarray or float): the upper bound, either a scalar or an upper bound per problem (first index of\n            samples)\n\n    Returns:\n        mean, std: the mean and std of the fitted truncated Gaussian\n    \"\"\"\n    if len(samples.shape) == 1:\n        return _TruncatedNormalFitter()((samples, lower_bounds, upper_bounds))\n\n    def item_generator():\n        for ind in range(samples.shape[0]):\n            if is_scalar(lower_bounds):\n                lower_bound = lower_bounds\n            else:\n                lower_bound = lower_bounds[ind]\n\n            if is_scalar(upper_bounds):\n                upper_bound = upper_bounds\n            else:\n                upper_bound = upper_bounds[ind]\n\n            yield (samples[ind], lower_bound, upper_bound)\n\n    results = np.array(multiprocess_mapping(_TruncatedNormalFitter(), item_generator()))\n    return results[:, 0], results[:, 1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the overlapping coefficient of two Gaussian continuous_distributions.", "response": "def gaussian_overlapping_coefficient(means_0, stds_0, means_1, stds_1, lower=None, upper=None):\n    \"\"\"Compute the overlapping coefficient of two Gaussian continuous_distributions.\n\n    This computes the :math:`\\int_{-\\infty}^{\\infty}{\\min(f(x), g(x))\\partial x}` where\n    :math:`f \\sim \\mathcal{N}(\\mu_0, \\sigma_0^{2})` and :math:`f \\sim \\mathcal{N}(\\mu_1, \\sigma_1^{2})` are normally\n    distributed variables.\n\n    This will compute the overlap for each element in the first dimension.\n\n    Args:\n        means_0 (ndarray): the set of means of the first distribution\n        stds_0 (ndarray): the set of stds of the fist distribution\n        means_1 (ndarray): the set of means of the second distribution\n        stds_1 (ndarray): the set of stds of the second distribution\n        lower (float): the lower limit of the integration. If not set we set it to -inf.\n        upper (float): the upper limit of the integration. If not set we set it to +inf.\n    \"\"\"\n    if lower is None:\n        lower = -np.inf\n    if upper is None:\n        upper = np.inf\n\n    def point_iterator():\n        for ind in range(means_0.shape[0]):\n            yield np.squeeze(means_0[ind]), np.squeeze(stds_0[ind]), np.squeeze(means_1[ind]), np.squeeze(stds_1[ind])\n\n    return np.array(list(multiprocess_mapping(_ComputeGaussianOverlap(lower, upper), point_iterator())))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef truncated_normal_log_likelihood(params, low, high, data):\n        mu = params[0]\n        sigma = params[1]\n        if sigma == 0:\n            return np.inf\n        ll = np.sum(norm.logpdf(data, mu, sigma))\n        ll -= len(data) * np.log((norm.cdf(high, mu, sigma) - norm.cdf(low, mu, sigma)))\n        return -ll", "response": "Calculate the log likelihood of the truncated normal distribution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef truncated_normal_ll_gradient(params, low, high, data):\n        if params[1] == 0:\n            return np.array([np.inf, np.inf])\n\n        return np.array([_TruncatedNormalFitter.partial_derivative_mu(params[0], params[1], low, high, data),\n                         _TruncatedNormalFitter.partial_derivative_sigma(params[0], params[1], low, high, data)])", "response": "Returns the gradient of the log likelihood of the truncated normal at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partial_derivative_mu(mu, sigma, low, high, data):\n        pd_mu = np.sum(data - mu) / sigma ** 2\n        pd_mu -= len(data) * ((norm.pdf(low, mu, sigma) - norm.pdf(high, mu, sigma))\n                              / (norm.cdf(high, mu, sigma) - norm.cdf(low, mu, sigma)))\n        return -pd_mu", "response": "The partial derivative of the log - likelihood at a given point."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_kernel_data(self):\n        return {\n            'nmsimplex_scratch': LocalMemory(\n                'mot_float_type', self._nmr_parameters * 2 + (self._nmr_parameters + 1) ** 2 + 1),\n            'initial_simplex_scale': LocalMemory('mot_float_type', self._nmr_parameters)\n        }", "response": "Get the kernel data needed for this optimization routine to work."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_kernel_data(self):\n        return {\n            'subplex_scratch_float': LocalMemory(\n                'mot_float_type', 4 + self._var_replace_dict['NMR_PARAMS'] * 2\n                                    + self._var_replace_dict['MAX_SUBSPACE_LENGTH'] * 2\n                                    + (self._var_replace_dict['MAX_SUBSPACE_LENGTH'] * 2\n                                       + self._var_replace_dict['MAX_SUBSPACE_LENGTH']+1)**2 + 1),\n            'subplex_scratch_int': LocalMemory(\n                'int', 2 + self._var_replace_dict['NMR_PARAMS']\n                         + (self._var_replace_dict['NMR_PARAMS'] // self._var_replace_dict['MIN_SUBSPACE_LENGTH'])),\n            'initial_simplex_scale': LocalMemory('mot_float_type', self._var_replace_dict['NMR_PARAMS'])\n        }", "response": "Get the kernel data needed for this optimization routine to work."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the kernel data needed for this optimization routine to work.", "response": "def get_kernel_data(self):\n        \"\"\"Get the kernel data needed for this optimization routine to work.\"\"\"\n        return {\n            'scratch_mot_float_type': LocalMemory(\n                'mot_float_type', 8 +\n                                  2 * self._var_replace_dict['NMR_OBSERVATIONS'] +\n                                  5 * self._var_replace_dict['NMR_PARAMS'] +\n                                  self._var_replace_dict['NMR_PARAMS'] * self._var_replace_dict['NMR_OBSERVATIONS']),\n            'scratch_int': LocalMemory('int', self._var_replace_dict['NMR_PARAMS'])\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bounds_to_array(bounds):\n    elements = []\n    for value in bounds:\n        if all_elements_equal(value):\n            elements.append(Scalar(get_single_value(value), ctype='mot_float_type'))\n        else:\n            elements.append(Array(value, ctype='mot_float_type', as_scalar=True))\n    return CompositeArray(elements, 'mot_float_type', address_space='local')", "response": "Create a CompositeArray to hold the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary with the default options for the given minimization method.", "response": "def get_minimizer_options(method):\n    \"\"\"Return a dictionary with the default options for the given minimization method.\n\n    Args:\n        method (str): the name of the method we want the options off\n\n    Returns:\n        dict: a dictionary with the default options\n    \"\"\"\n    if method == 'Powell':\n        return {'patience': 2,\n                'patience_line_search': None,\n                'reset_method': 'EXTRAPOLATED_POINT'}\n\n    elif method == 'Nelder-Mead':\n        return {'patience': 200,\n                'alpha': 1.0, 'beta': 0.5, 'gamma': 2.0, 'delta': 0.5, 'scale': 0.1,\n                'adaptive_scales': True}\n\n    elif method == 'Levenberg-Marquardt':\n        return {'patience': 250, 'step_bound': 100.0, 'scale_diag': 1, 'usertol_mult': 30}\n\n    elif method == 'Subplex':\n        return {'patience': 10,\n                'patience_nmsimplex': 100,\n                'alpha': 1.0, 'beta': 0.5, 'gamma': 2.0, 'delta': 0.5, 'scale': 1.0, 'psi': 0.0001, 'omega': 0.01,\n                'adaptive_scales': True,\n                'min_subspace_length': 'auto',\n                'max_subspace_length': 'auto'}\n\n    raise ValueError('Could not find the specified method \"{}\".'.format(method))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clean_options(method, provided_options):\n    provided_options = provided_options or {}\n    default_options = get_minimizer_options(method)\n\n    result = {}\n\n    for name, default in default_options.items():\n        if name in provided_options:\n            result[name] = provided_options[name]\n        else:\n            result[name] = default_options[name]\n    return result", "response": "Clean the given input options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nminimize the Powell algorithm.", "response": "def _minimize_powell(func, x0, cl_runtime_info, lower_bounds, upper_bounds,\n                     constraints_func=None, data=None, options=None):\n    \"\"\"\n    Options:\n        patience (int): Used to set the maximum number of iterations to patience*(number_of_parameters+1)\n        reset_method (str): one of 'EXTRAPOLATED_POINT' or 'RESET_TO_IDENTITY' lower case or upper case.\n        patience_line_search (int): the patience of the searching algorithm. Defaults to the\n            same patience as for the Powell algorithm itself.\n    \"\"\"\n    options = options or {}\n    nmr_problems = x0.shape[0]\n    nmr_parameters = x0.shape[1]\n\n    penalty_data, penalty_func = _get_penalty_function(nmr_parameters, constraints_func)\n\n    eval_func = SimpleCLFunction.from_string('''\n        double evaluate(local mot_float_type* x, void* data){\n            double penalty = _mle_penalty(\n                x, \n                ((_powell_eval_func_data*)data)->data,\n                ((_powell_eval_func_data*)data)->lower_bounds,\n                ((_powell_eval_func_data*)data)->upper_bounds,\n                ''' + str(options.get('penalty_weight', 1e30)) + ''',  \n                ((_powell_eval_func_data*)data)->penalty_data\n            );\n            \n            double func_val = ''' + func.get_cl_function_name() + '''(x, ((_powell_eval_func_data*)data)->data, 0);\n            \n            if(isnan(func_val)){\n                return INFINITY;\n            }\n            \n            return func_val + penalty;\n        }\n    ''', dependencies=[func, penalty_func])\n\n    optimizer_func = Powell(eval_func, nmr_parameters, **_clean_options('Powell', options))\n\n    kernel_data = {'model_parameters': Array(x0, ctype='mot_float_type', mode='rw'),\n                   'data': Struct({'data': data,\n                                   'lower_bounds': lower_bounds,\n                                   'upper_bounds': upper_bounds,\n                                   'penalty_data': penalty_data}, '_powell_eval_func_data')}\n    kernel_data.update(optimizer_func.get_kernel_data())\n\n    return_code = optimizer_func.evaluate(\n        kernel_data, nmr_problems,\n        use_local_reduction=all(env.is_gpu for env in cl_runtime_info.cl_environments),\n        cl_runtime_info=cl_runtime_info)\n\n    return OptimizeResults({'x': kernel_data['model_parameters'].get_data(),\n                            'status': return_code})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a numerical differentiated Jacobian function. This computes the Jacobian of the observations (function vector) with respect to the parameters. Args: eval_func (mot.lib.cl_function.CLFunction): the evaluation function nmr_params (int): the number of parameters nmr_observations (int): the number of observations (the length of the function vector). Returns: mot.lib.cl_function.CLFunction: CL function for numerically estimating the Jacobian.", "response": "def _lm_numdiff_jacobian(eval_func, nmr_params, nmr_observations):\n    \"\"\"Get a numerical differentiated Jacobian function.\n\n    This computes the Jacobian of the observations (function vector) with respect to the parameters.\n\n    Args:\n        eval_func (mot.lib.cl_function.CLFunction): the evaluation function\n        nmr_params (int): the number of parameters\n        nmr_observations (int): the number of observations (the length of the function vector).\n\n    Returns:\n        mot.lib.cl_function.CLFunction: CL function for numerically estimating the Jacobian.\n    \"\"\"\n    return SimpleCLFunction.from_string(r'''\n        /**\n         * Compute the Jacobian for use in the Levenberg-Marquardt optimization routine.\n         *\n         * This should place the output in the ``fjac`` matrix.\n         *\n         * Parameters:\n         *\n         *   model_parameters: (nmr_params,) the current point around which we want to know the Jacobian\n         *   data: the current modeling data, used by the objective function\n         *   fvec: (nmr_observations,), the function values corresponding to the current model parameters\n         *   fjac: (nmr_parameters, nmr_observations), the memory location for the Jacobian\n         */\n        void _lm_numdiff_jacobian(local mot_float_type* model_parameters,\n                                  void* data,\n                                  local mot_float_type* fvec,\n                                  local mot_float_type* const fjac){\n            \n            const uint nmr_params = ''' + str(nmr_params) + ''';\n            const uint nmr_observations = ''' + str(nmr_observations) + ''';\n            \n            local mot_float_type* lower_bounds = ((_lm_eval_func_data*)data)->lower_bounds;\n            local mot_float_type* upper_bounds = ((_lm_eval_func_data*)data)->upper_bounds;\n            local mot_float_type* jacobian_x_tmp = ((_lm_eval_func_data*)data)->jacobian_x_tmp;\n            \n            mot_float_type step_size = 30 * MOT_EPSILON;\n            \n            for (int i = 0; i < nmr_params; i++) {\n                if(model_parameters[i] + step_size > upper_bounds[i]){\n                    _lm_numdiff_jacobian_backwards(model_parameters, i, step_size, data, fvec, \n                                                   fjac + i*nmr_observations, jacobian_x_tmp);\n                }\n                else if(model_parameters[i] - step_size < lower_bounds[i]){\n                    _lm_numdiff_jacobian_forwards(model_parameters, i, step_size, data, fvec, \n                                                  fjac + i*nmr_observations, jacobian_x_tmp);\n                }\n                else{\n                    _lm_numdiff_jacobian_centered(model_parameters, i, step_size, data, fvec,  \n                                                  fjac + i*nmr_observations, jacobian_x_tmp);\n                }\n            }\n        }\n    ''', dependencies=[eval_func, SimpleCLFunction.from_string('''\n        void _lm_numdiff_jacobian_centered(\n                local mot_float_type* model_parameters,\n                uint px,\n                float step_size,\n                void* data,\n                local mot_float_type* fvec,\n                local mot_float_type* const fjac,\n                local mot_float_type* fjac_tmp){\n            \n            const uint nmr_observations = ''' + str(nmr_observations) + ''';\n            \n            uint observation_ind;\n            mot_float_type temp;\n            \n            uint local_id = get_local_id(0);\n            uint workgroup_size = get_local_size(0);\n            \n            // F(x + h)     \n            if(get_local_id(0) == 0){\n                temp = model_parameters[px];\n                model_parameters[px] = temp + step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac);\n            \n            \n            // F(x - h)\n            if(get_local_id(0) == 0){\n                model_parameters[px] = temp - step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac_tmp);\n            \n            \n            // combine\n            for(int i = 0; i < (nmr_observations + workgroup_size - 1) / workgroup_size; i++){\n                observation_ind = i * workgroup_size + local_id;\n                if(observation_ind < nmr_observations){\n                    fjac[observation_ind] = (fjac[observation_ind] - fjac_tmp[observation_ind]) / (2 * step_size);\n                }\n            }\n            \n            // restore parameter vector\n            if(get_local_id(0) == 0){\n                model_parameters[px] = temp;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n    '''), SimpleCLFunction.from_string('''\n        void _lm_numdiff_jacobian_backwards(\n               local mot_float_type* model_parameters,\n               uint px,\n               float step_size,\n               void* data,\n               local mot_float_type* fvec,\n               local mot_float_type* const fjac,\n               local mot_float_type* fjac_tmp){\n\n            const uint nmr_observations = ''' + str(nmr_observations) + ''';\n            \n            uint observation_ind;\n            mot_float_type temp;\n            \n            uint local_id = get_local_id(0);\n            uint workgroup_size = get_local_size(0);\n            \n            // F(x - h)     \n            if(get_local_id(0) == 0){\n               temp = model_parameters[px];\n               model_parameters[px] = temp - step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac);\n                        \n            \n            // F(x - 2*h)\n            if(get_local_id(0) == 0){\n               model_parameters[px] = temp - 2 * step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac_tmp);\n            \n            // combine\n            for(int i = 0; i < (nmr_observations + workgroup_size - 1) / workgroup_size; i++){\n               observation_ind = i * workgroup_size + local_id;\n               if(observation_ind < nmr_observations){\n                   fjac[observation_ind] = (  3 * fvec[observation_ind] \n                                            - 4 * fjac[observation_ind] \n                                            + fjac_tmp[observation_ind]) / (2 * step_size);\n               }\n            }\n            \n            // restore parameter vector\n            if(get_local_id(0) == 0){\n               model_parameters[px] = temp;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n    '''), SimpleCLFunction.from_string('''\n        void _lm_numdiff_jacobian_forwards(\n               local mot_float_type* model_parameters,\n               uint px,\n               float step_size,\n               void* data,\n               local mot_float_type* fvec,\n               local mot_float_type* const fjac,\n               local mot_float_type* fjac_tmp){\n\n            const uint nmr_observations = ''' + str(nmr_observations) + ''';\n            \n            uint observation_ind;\n            mot_float_type temp;\n            \n            uint local_id = get_local_id(0);\n            uint workgroup_size = get_local_size(0);\n            \n            // F(x + h)     \n            if(get_local_id(0) == 0){\n               temp = model_parameters[px];\n               model_parameters[px] = temp + step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac);\n            \n          \n            // F(x + 2*h)\n            if(get_local_id(0) == 0){\n               model_parameters[px] = temp + 2 * step_size;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            ''' + eval_func.get_cl_function_name() + '''(model_parameters, data, fjac);\n            \n            // combine\n            for(int i = 0; i < (nmr_observations + workgroup_size - 1) / workgroup_size; i++){\n               observation_ind = i * workgroup_size + local_id;\n               if(observation_ind < nmr_observations){\n                   fjac[observation_ind] = (- 3 * fvec[observation_ind] \n                                            + 4 * fjac[observation_ind] \n                                            - fjac_tmp[observation_ind]) / (2 * step_size);\n               }\n            }\n            \n            // restore parameter vector\n            if(get_local_id(0) == 0){\n               model_parameters[px] = temp;\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n    ''')])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_penalty_function(nmr_parameters, constraints_func=None):\n    dependencies = []\n    data_requirements = {'scratch': LocalMemory('double', 1)}\n    constraints_code = ''\n\n    if constraints_func and constraints_func.get_nmr_constraints() > 0:\n        nmr_constraints = constraints_func.get_nmr_constraints()\n        dependencies.append(constraints_func)\n        data_requirements['constraints'] = LocalMemory('mot_float_type', nmr_constraints)\n\n        constraints_code = '''\n            local mot_float_type* constraints = ((_mle_penalty_data*)scratch_data)->constraints;\n            \n            ''' + constraints_func.get_cl_function_name() + '''(x, data, constraints);\n            \n            for(int i = 0; i < ''' + str(nmr_constraints) + '''; i++){\n                *penalty_sum += pown(max((mot_float_type)0, constraints[i]), 2); \n            }\n        '''\n\n    data = Struct(data_requirements, '_mle_penalty_data')\n    func = SimpleCLFunction.from_string('''\n        double _mle_penalty(\n                local mot_float_type* x,\n                void* data, \n                local mot_float_type* lower_bounds, \n                local mot_float_type* upper_bounds,\n                float penalty_weight,\n                void* scratch_data){\n            \n            local double* penalty_sum = ((_mle_penalty_data*)scratch_data)->scratch;\n            \n            if(get_local_id(0) == 0){\n                *penalty_sum = 0;\n                \n                // boundary conditions\n                for(int i = 0; i < ''' + str(nmr_parameters) + '''; i++){\n                    if(isfinite(upper_bounds[i])){\n                        *penalty_sum += pown(max((mot_float_type)0, x[i] - upper_bounds[i]), 2);    \n                    }\n                    if(isfinite(lower_bounds[i])){\n                        *penalty_sum += pown(max((mot_float_type)0, lower_bounds[i] - x[i]), 2);\n                    }\n                }\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // constraints\n            ''' + constraints_code + '''\n            \n            return penalty_weight * *penalty_sum;\n        }\n    ''', dependencies=dependencies)\n    return data, func", "response": "Returns a function to compute the penalty term for the boundary conditions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_string(cls, cl_function, dependencies=(), nmr_constraints=None):\n        return_type, function_name, parameter_list, body = split_cl_function(cl_function)\n        return SimpleConstraintFunction(return_type, function_name, parameter_list, body, dependencies=dependencies,\n                                        nmr_constraints=nmr_constraints)", "response": "Parse the given string into a SimpleCLFunction object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the schema of a dict containing a top - level lane or a branch - level lane.", "response": "def validate_schema(yaml_def, branch=False):\n    \"\"\"Validates the schema of a dict\n\n    Parameters\n    ----------\n    yaml_def : dict\n        dict whose schema shall be validated\n    branch : bool\n        Indicates whether `yaml_def` is a dict of a top-level lane, or of a branch\n        inside a lane (needed for recursion)\n\n    Returns\n    -------\n    bool\n        True if validation was successful\n    \"\"\"\n    schema = Schema({\n        'lane' if not branch else 'branch': {\n            Optional('name'): str,\n            Optional('run_parallel'): bool,\n            'tasks': list\n        }\n    })\n\n    schema.validate(yaml_def)\n    from schema import And, Use\n    task_schema = Schema({\n        'class': str,\n        Optional('kwargs'): Or({str: object}),\n        Optional('args'): Or([object], And(Use(lambda a: isinstance(a, dict)), False))\n    })\n\n    def validate_tasks(tasks):  # pylint: disable=missing-docstring\n        for task in tasks:\n            try:\n                Schema({'branch': dict}).validate(task)\n                validate_schema(task, True)\n            except SchemaError:\n                task_schema.validate(task)\n\n        return True\n\n    return validate_tasks(yaml_def['lane']['tasks'] if not branch else yaml_def['branch']['tasks'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_params(cls, mtd_name, *args, **kwargs):\n    mtd = getattr(cls, mtd_name)\n\n    py3_mtd_condition = (not (inspect.isfunction(mtd) or inspect.ismethod(mtd))\n                         and hasattr(cls, mtd_name))\n    py2_mtd_condition = (not inspect.ismethod(mtd)\n                         and not isinstance(cls.__dict__[mtd_name], staticmethod))\n    if (PY3 and py3_mtd_condition) or (PY2 and py2_mtd_condition):\n        raise TypeError('Attribute `%s` of class `%s` must be a method. Got type `%s` instead.'\n                        % (mtd_name, cls.__name__, type(mtd)))\n\n    req_params, opt_params = arg_spec(cls, mtd_name)\n    n_params = len(req_params) + len(opt_params)\n    n_args_kwargs = len(args) + len(kwargs)\n\n    for k in kwargs:\n        if k not in req_params and k not in opt_params:\n            raise TaskInitializationError('kwarg `%s` is not a parameter of callable `%s`.'\n                                          % (k, mtd.__name__))\n\n    if n_args_kwargs < len(req_params):\n        raise TaskInitializationError('Not enough args/kwargs supplied for callable `%s`. '\n                                      'Required args: %s' % (mtd.__name__, str(req_params)))\n    if len(args) > n_params or n_args_kwargs > n_params or len(kwargs) > n_params:\n        raise TaskInitializationError('Too many args/kwargs supplied for callable `%s`. '\n                                      'Required args: %s' % (mtd.__name__, str(req_params)))\n\n    redundant_p = [p for p in kwargs if p not in req_params[len(args):] + opt_params]\n    if redundant_p:\n        raise TaskInitializationError('Supplied one or more kwargs that in the signature of '\n                                      'callable `%s`. Redundant kwargs: %s'\n                                      % (mtd.__name__, str(redundant_p)))\n\n    needed_kwargs = req_params[len(args):]\n    if not all([True if p in kwargs else False for p in needed_kwargs]):\n        raise TaskInitializationError('Not enough args/kwargs supplied for callable `%s`. '\n                                      'Required args: %s' % (mtd.__name__, str(req_params)))", "response": "Validates if the given args and kwargs match the method signature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncross - version argument signature inspection.", "response": "def arg_spec(cls, mtd_name):\n    \"\"\"Cross-version argument signature inspection\n\n    Parameters\n    ----------\n    cls : class\n    mtd_name : str\n        Name of the method to be inspected\n\n    Returns\n    -------\n    required_params : list of str\n        List of required, positional parameters\n    optional_params : list of str\n        List of optional parameters, i.e. parameters with a default value\n    \"\"\"\n    mtd = getattr(cls, mtd_name)\n\n    required_params = []\n    optional_params = []\n\n    if hasattr(inspect, 'signature'):  # Python 3\n        params = inspect.signature(mtd).parameters  # pylint: disable=no-member\n\n        for k in params.keys():\n            if params[k].default == inspect.Parameter.empty:  # pylint: disable=no-member\n                # Python 3 does not make a difference between unbound methods and functions, so the\n                # only way to distinguish if the first argument is of a regular method, or a class\n                # method, is to look for the conventional argument name. Yikes.\n                if not (params[k].name == 'self' or params[k].name == 'cls'):\n                    required_params.append(k)\n            else:\n                optional_params.append(k)\n    else:  # Python 2\n        params = inspect.getargspec(mtd)  # pylint: disable=deprecated-method\n        num = len(params[0]) if params[0] else 0\n        n_opt = len(params[3]) if params[3] else 0\n        n_req = (num - n_opt) if n_opt <= num else 0\n        for i in range(0, n_req):\n            required_params.append(params[0][i])\n        for i in range(n_req, num):\n            optional_params.append(params[0][i])\n\n        if inspect.isroutine(getattr(cls, mtd_name)):\n            bound_mtd = cls.__dict__[mtd_name]\n            if not isinstance(bound_mtd, staticmethod):\n                del required_params[0]\n\n    return required_params, optional_params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing random samples from the Uniform distribution.", "response": "def uniform(nmr_distributions, nmr_samples, low=0, high=1, ctype='float', seed=None):\n    \"\"\"Draw random samples from the Uniform distribution.\n\n    Args:\n        nmr_distributions (int): the number of unique continuous_distributions to create\n        nmr_samples (int): The number of samples to draw\n        low (double): The minimum value of the random numbers\n        high (double): The minimum value of the random numbers\n        ctype (str): the C type of the output samples\n        seed (float): the seed for the RNG\n\n    Returns:\n        ndarray: A two dimensional numpy array as (nmr_distributions, nmr_samples).\n    \"\"\"\n    if is_scalar(low):\n        low = np.ones((nmr_distributions, 1)) * low\n    if is_scalar(high):\n        high = np.ones((nmr_distributions, 1)) * high\n\n    kernel_data = {'low': Array(low, as_scalar=True),\n                   'high': Array(high, as_scalar=True)}\n\n    kernel = SimpleCLFunction.from_string('''\n        void compute(double low, double high, global uint* rng_state, global ''' + ctype + '''* samples){\n        \n            rand123_data rand123_rng_data = rand123_initialize_data((uint[]){\n                rng_state[0], rng_state[1], rng_state[2], rng_state[3], \n                rng_state[4], rng_state[5], 0});\n            void* rng_data = (void*)&rand123_rng_data;\n\n            for(uint i = 0; i < ''' + str(nmr_samples) + '''; i++){\n                double4 randomnr = rand4(rng_data);\n                samples[i] = (''' + ctype + ''')(low + randomnr.x * (high - low));\n            }\n        }\n    ''', dependencies=[Rand123()])\n\n    return _generate_samples(kernel, nmr_distributions, nmr_samples, ctype, kernel_data, seed=seed)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing random samples from the Gaussian distribution.", "response": "def normal(nmr_distributions, nmr_samples, mean=0, std=1, ctype='float', seed=None):\n    \"\"\"Draw random samples from the Gaussian distribution.\n\n    Args:\n        nmr_distributions (int): the number of unique continuous_distributions to create\n        nmr_samples (int): The number of samples to draw\n        mean (float or ndarray): The mean of the distribution\n        std (float or ndarray): The standard deviation or the distribution\n        ctype (str): the C type of the output samples\n        seed (float): the seed for the RNG\n\n    Returns:\n        ndarray: A two dimensional numpy array as (nmr_distributions, nmr_samples).\n    \"\"\"\n    if is_scalar(mean):\n        mean = np.ones((nmr_distributions, 1)) * mean\n    if is_scalar(std):\n        std = np.ones((nmr_distributions, 1)) * std\n\n    kernel_data = {'mean': Array(mean, as_scalar=True),\n                   'std': Array(std, as_scalar=True)}\n\n    kernel = SimpleCLFunction.from_string('''\n        void compute(double mean, double std, global uint* rng_state, global ''' + ctype + '''* samples){\n            rand123_data rand123_rng_data = rand123_initialize_data((uint[]){\n                rng_state[0], rng_state[1], rng_state[2], rng_state[3], \n                rng_state[4], rng_state[5], 0});\n            void* rng_data = (void*)&rand123_rng_data;\n\n            for(uint i = 0; i < ''' + str(nmr_samples) + '''; i++){\n                double4 randomnr = randn4(rng_data);\n                samples[i] = (''' + ctype + ''')(mean + randomnr.x * std);\n            }\n        }\n    ''', dependencies=[Rand123()])\n\n    return _generate_samples(kernel, nmr_distributions, nmr_samples, ctype, kernel_data, seed=seed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking additional samples from the given likelihood and prior using this sampler.", "response": "def sample(self, nmr_samples, burnin=0, thinning=1):\n        \"\"\"Take additional samples from the given likelihood and prior, using this sampler.\n\n        This method can be called multiple times in which the sample state is stored in between.\n\n        Args:\n            nmr_samples (int): the number of samples to return\n            burnin (int): the number of samples to discard before returning samples\n            thinning (int): how many sample we wait before storing a new one. This will draw extra samples such that\n                    the total number of samples generated is ``nmr_samples * (thinning)`` and the number of samples\n                    stored is ``nmr_samples``. If set to one or lower we store every sample after the burn in.\n\n        Returns:\n            SamplingOutput: the sample output object\n        \"\"\"\n        if not thinning or thinning < 1:\n            thinning = 1\n        if not burnin or burnin < 0:\n            burnin = 0\n\n        max_samples_per_batch = max(1000 // thinning, 100)\n\n        with self._logging(nmr_samples, burnin, thinning):\n            if burnin > 0:\n                for batch_start, batch_end in split_in_batches(burnin, max_samples_per_batch):\n                    self._sample(batch_end - batch_start, return_output=False)\n            if nmr_samples > 0:\n                outputs = []\n                for batch_start, batch_end in split_in_batches(nmr_samples, max_samples_per_batch):\n                    outputs.append(self._sample(batch_end - batch_start, thinning=thinning))\n                return SimpleSampleOutput(*[np.concatenate([o[ind] for o in outputs], axis=-1) for ind in range(3)])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sample(self, nmr_samples, thinning=1, return_output=True):\n        kernel_data = self._get_kernel_data(nmr_samples, thinning, return_output)\n        sample_func = self._get_compute_func(nmr_samples, thinning, return_output)\n        sample_func.evaluate(kernel_data, self._nmr_problems,\n                             use_local_reduction=all(env.is_gpu for env in self._cl_runtime_info.cl_environments),\n                             cl_runtime_info=self._cl_runtime_info)\n        self._sampling_index += nmr_samples * thinning\n        if return_output:\n            return (kernel_data['samples'].get_data(),\n                    kernel_data['log_likelihoods'].get_data(),\n                    kernel_data['log_priors'].get_data())", "response": "Sample the given number of samples with the given thinning."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the likelihood and prior using the given positions.", "response": "def _initialize_likelihood_prior(self, positions, log_likelihoods, log_priors):\n        \"\"\"Initialize the likelihood and the prior using the given positions.\n\n        This is a general method for computing the log likelihoods and log priors for given positions.\n\n        Subclasses can use this to instantiate secondary chains as well.\n        \"\"\"\n        func = SimpleCLFunction.from_string('''\n            void compute(global mot_float_type* chain_position,\n                         global mot_float_type* log_likelihood,\n                         global mot_float_type* log_prior,\n                         local mot_float_type* x_tmp,\n                         void* data){\n                \n                bool is_first_work_item = get_local_id(0) == 0;\n    \n                if(is_first_work_item){\n                    for(uint i = 0; i < ''' + str(self._nmr_params) + '''; i++){\n                        x_tmp[i] = chain_position[i];\n                    }\n                    *log_prior = _computeLogPrior(x_tmp, data);\n                }\n                barrier(CLK_LOCAL_MEM_FENCE);\n                    \n                *log_likelihood = _computeLogLikelihood(x_tmp, data);\n            }\n        ''', dependencies=[self._get_log_prior_cl_func(), self._get_log_likelihood_cl_func()])\n\n        kernel_data = {\n            'chain_position': Array(positions, 'mot_float_type', mode='rw', ensure_zero_copy=True),\n            'log_likelihood': Array(log_likelihoods, 'mot_float_type', mode='rw', ensure_zero_copy=True),\n            'log_prior': Array(log_priors, 'mot_float_type', mode='rw', ensure_zero_copy=True),\n            'x_tmp': LocalMemory('mot_float_type', self._nmr_params),\n            'data': self._data\n        }\n\n        func.evaluate(kernel_data, self._nmr_problems,\n                      use_local_reduction=all(env.is_gpu for env in self._cl_runtime_info.cl_environments),\n                      cl_runtime_info=self._cl_runtime_info)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_kernel_data(self, nmr_samples, thinning, return_output):\n        kernel_data = {\n            'data': self._data,\n            'method_data': self._get_mcmc_method_kernel_data(),\n            'nmr_iterations': Scalar(nmr_samples * thinning, ctype='ulong'),\n            'iteration_offset': Scalar(self._sampling_index, ctype='ulong'),\n            'rng_state': Array(self._rng_state, 'uint', mode='rw', ensure_zero_copy=True),\n            'current_chain_position': Array(self._current_chain_position, 'mot_float_type',\n                                            mode='rw', ensure_zero_copy=True),\n            'current_log_likelihood': Array(self._current_log_likelihood, 'mot_float_type',\n                                            mode='rw', ensure_zero_copy=True),\n            'current_log_prior': Array(self._current_log_prior, 'mot_float_type',\n                                       mode='rw', ensure_zero_copy=True),\n        }\n\n        if return_output:\n            kernel_data.update({\n                'samples': Zeros((self._nmr_problems, self._nmr_params, nmr_samples), ctype='mot_float_type'),\n                'log_likelihoods': Zeros((self._nmr_problems, nmr_samples), ctype='mot_float_type'),\n                'log_priors': Zeros((self._nmr_problems, nmr_samples), ctype='mot_float_type'),\n            })\n        return kernel_data", "response": "This method is used to get the kernel data for the current entry of the current entry in the MCMC sampler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the MCMC algorithm as a computable function.", "response": "def _get_compute_func(self, nmr_samples, thinning, return_output):\n        \"\"\"Get the MCMC algorithm as a computable function.\n\n        Args:\n            nmr_samples (int): the number of samples we will draw\n            thinning (int): the thinning factor we want to use\n            return_output (boolean): if the kernel should return output\n\n        Returns:\n            mot.lib.cl_function.CLFunction: the compute function\n        \"\"\"\n        cl_func = '''\n            void compute(global uint* rng_state, \n                         global mot_float_type* current_chain_position,\n                         global mot_float_type* current_log_likelihood,\n                         global mot_float_type* current_log_prior,\n                         ulong iteration_offset, \n                         ulong nmr_iterations, \n                         ''' + ('''global mot_float_type* samples, \n                                   global mot_float_type* log_likelihoods,\n                                   global mot_float_type* log_priors,''' if return_output else '') + '''\n                         void* method_data, \n                         void* data){\n                         \n                bool is_first_work_item = get_local_id(0) == 0;\n    \n                rand123_data rand123_rng_data = rand123_initialize_data((uint[]){\n                    rng_state[0], rng_state[1], rng_state[2], rng_state[3], \n                    rng_state[4], rng_state[5], 0, 0});\n                void* rng_data = (void*)&rand123_rng_data;\n        \n                for(ulong i = 0; i < nmr_iterations; i++){\n        '''\n        if return_output:\n            cl_func += '''\n                    if(is_first_work_item){\n                        if(i % ''' + str(thinning) + ''' == 0){\n                            log_likelihoods[i / ''' + str(thinning) + '''] = *current_log_likelihood;\n                            log_priors[i / ''' + str(thinning) + '''] = *current_log_prior;\n    \n                            for(uint j = 0; j < ''' + str(self._nmr_params) + '''; j++){\n                                samples[(ulong)(i / ''' + str(thinning) + ''') // remove the interval\n                                        + j * ''' + str(nmr_samples) + '''  // parameter index\n                                ] = current_chain_position[j];\n                            }\n                        }\n                    }\n        '''\n        cl_func += '''\n                    _advanceSampler(method_data, data, i + iteration_offset, rng_data, \n                                    current_chain_position, current_log_likelihood, current_log_prior);\n                }\n\n                if(is_first_work_item){\n                    uint state[8];\n                    rand123_data_to_array(rand123_rng_data, state);\n                    for(uint i = 0; i < 6; i++){\n                        rng_state[i] = state[i];\n                    }\n                }\n            }\n        '''\n        return SimpleCLFunction.from_string(\n            cl_func,\n            dependencies=[Rand123(), self._get_log_prior_cl_func(),\n                          self._get_log_likelihood_cl_func(),\n                          SimpleCLCodeObject(self._get_state_update_cl_func(nmr_samples, thinning, return_output))])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the CL log prior compute function.", "response": "def _get_log_prior_cl_func(self):\n        \"\"\"Get the CL log prior compute function.\n\n        Returns:\n            str: the compute function for computing the log prior.\n        \"\"\"\n        return SimpleCLFunction.from_string('''\n            mot_float_type _computeLogPrior(local const mot_float_type* x, void* data){\n                return ''' + self._log_prior_func.get_cl_function_name() + '''(x, data);\n            }\n        ''', dependencies=[self._log_prior_func])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the CL log likelihood compute function.", "response": "def _get_log_likelihood_cl_func(self):\n        \"\"\"Get the CL log likelihood compute function.\n\n        This uses local reduction to compute the log likelihood for every observation in CL local space.\n        The results are then summed in the first work item and returned using a pointer.\n\n        Returns:\n            str: the CL code for the log likelihood compute func.\n        \"\"\"\n        return SimpleCLFunction.from_string('''\n            double _computeLogLikelihood(local const mot_float_type* current_position, void* data){\n                return ''' + self._ll_func.get_cl_function_name() + '''(current_position, data);\n            }\n        ''', dependencies=[self._ll_func])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the mcmc method kernel data elements. Used by _get_mcmc_method_kernel_data.", "response": "def _get_mcmc_method_kernel_data_elements(self):\n        \"\"\"Get the mcmc method kernel data elements. Used by :meth:`_get_mcmc_method_kernel_data`.\"\"\"\n        return {'proposal_stds': Array(self._proposal_stds, 'mot_float_type', mode='rw', ensure_zero_copy=True),\n                'x_tmp': LocalMemory('mot_float_type', nmr_items=1 + self._nmr_params)}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate and return the log likelihood of the given model for the given parameters.", "response": "def compute_log_likelihood(ll_func, parameters, data=None, cl_runtime_info=None):\n    \"\"\"Calculate and return the log likelihood of the given model for the given parameters.\n\n    This calculates the log likelihoods for every problem in the model (typically after optimization),\n    or a log likelihood for every sample of every model (typically after sample). In the case of the first (after\n    optimization), the parameters must be an (d, p) array for d problems and p parameters. In the case of the\n    second (after sample), you must provide this function with a matrix of shape (d, p, n) with d problems,\n    p parameters and n samples.\n\n    Args:\n        ll_func (mot.lib.cl_function.CLFunction): The log-likelihood function. A CL function with the signature:\n\n                .. code-block:: c\n\n                        double <func_name>(local const mot_float_type* const x, void* data);\n\n        parameters (ndarray): The parameters to use in the evaluation of the model. This is either an (d, p) matrix\n            or (d, p, n) matrix with d problems, p parameters and n samples.\n        data (mot.lib.kernel_data.KernelData): the user provided data for the ``void* data`` pointer.\n        cl_runtime_info (mot.configuration.CLRuntimeInfo): the runtime information\n\n    Returns:\n        ndarray: per problem the log likelihood, or, per problem and per sample the log likelihood.\n    \"\"\"\n\n    def get_cl_function():\n        nmr_params = parameters.shape[1]\n\n        if len(parameters.shape) > 2:\n            return SimpleCLFunction.from_string('''\n                void compute(global mot_float_type* parameters, \n                             global mot_float_type* log_likelihoods,\n                             void* data){\n                             \n                    local mot_float_type x[''' + str(nmr_params) + '''];\n\n                    for(uint sample_ind = 0; sample_ind < ''' + str(parameters.shape[2]) + '''; sample_ind++){\n                        for(uint i = 0; i < ''' + str(nmr_params) + '''; i++){\n                            x[i] = parameters[i *''' + str(parameters.shape[2]) + ''' + sample_ind];\n                        }\n                        \n                        double ll = ''' + ll_func.get_cl_function_name() + '''(x, data);\n                        if(get_local_id(0) == 0){\n                            log_likelihoods[sample_ind] = ll;\n                        }\n                    }\n                }\n            ''', dependencies=[ll_func])\n\n        return SimpleCLFunction.from_string('''\n            void compute(local mot_float_type* parameters, \n                         global mot_float_type* log_likelihoods,\n                         void* data){\n                         \n                double ll = ''' + ll_func.get_cl_function_name() + '''(parameters, data);\n                if(get_local_id(0) == 0){\n                    *(log_likelihoods) = ll;\n                }\n            }\n        ''', dependencies=[ll_func])\n\n    kernel_data = {'data': data,\n                   'parameters': Array(parameters, 'mot_float_type', mode='r')}\n\n    shape = parameters.shape\n    if len(shape) > 2:\n        kernel_data.update({\n            'log_likelihoods': Zeros((shape[0], shape[2]), 'mot_float_type'),\n        })\n    else:\n        kernel_data.update({\n            'log_likelihoods': Zeros((shape[0],), 'mot_float_type'),\n        })\n\n    get_cl_function().evaluate(kernel_data, parameters.shape[0], use_local_reduction=True,\n                               cl_runtime_info=cl_runtime_info)\n\n    return kernel_data['log_likelihoods'].get_data()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_objective_value(objective_func, parameters, data=None, cl_runtime_info=None):\n    return objective_func.evaluate({'data': data, 'parameters': Array(parameters, 'mot_float_type', mode='r')},\n                                   parameters.shape[0], use_local_reduction=True, cl_runtime_info=cl_runtime_info)", "response": "Calculate and return the objective function value of the given model for the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_log(username):\n    redis = get_redis_client()\n    log_key = 'log:{}'.format(username)\n    raw_log = redis.lrange(log_key, 0, -1)\n    log = []\n    for raw_item in raw_log:\n        item = json.loads(raw_item.decode())\n        item['datetime'] = convert_timestamp(item.pop('time'))\n        log.append(item)\n    return log", "response": "Get a list of page views for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nobtaining an access token that can be passed to a websocket client.", "response": "def get_token(username, length=20, timeout=20):\n    \"\"\"\n    Obtain an access token that can be passed to a websocket client.\n    \"\"\"\n    redis = get_redis_client()\n    token = get_random_string(length)\n    token_key = 'token:{}'.format(token)\n    redis.set(token_key, username)\n    redis.expire(token_key, timeout)\n    return token"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_log(self, request, response):\n        return {\n            'method': request.method,\n            'path': request.get_full_path(),\n            'code': response.status_code,\n            'time': time.time(),\n        }", "response": "Return a dict of data to log for a given request and response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads a song from a Google Music library.", "response": "def download(self, song):\n\t\t\"\"\"Download a song from a Google Music library.\n\n\t\tParameters:\n\t\t\tsong (dict): A song dict.\n\n\t\tReturns:\n\t\t\ttuple: Song content as bytestring, suggested filename.\n\t\t\"\"\"\n\n\t\tsong_id = song['id']\n\n\t\tresponse = self._call(\n\t\t\tmm_calls.Export,\n\t\t\tself.uploader_id,\n\t\t\tsong_id)\n\t\taudio = response.body\n\t\tsuggested_filename = unquote(\n\t\t\tresponse.headers['Content-Disposition'].split(\"filename*=UTF-8''\")[-1]\n\t\t)\n\n\t\treturn (audio, suggested_filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the uploaded track count and allowance.", "response": "def quota(self):\n\t\t\"\"\"Get the uploaded track count and allowance.\n\n\t\tReturns:\n\t\t\ttuple: Number of uploaded tracks, number of tracks allowed.\n\t\t\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmm_calls.ClientState,\n\t\t\tself.uploader_id\n\t\t)\n\t\tclient_state = response.body.clientstate_response\n\n\t\treturn (client_state.total_track_count, client_state.locker_track_limit)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a listing of Music Library songs.", "response": "def songs(self, *, uploaded=True, purchased=True):\n\t\t\"\"\"Get a listing of Music Library songs.\n\n\t\tReturns:\n\t\t\tlist: Song dicts.\n\t\t\"\"\"\n\n\t\tif not uploaded and not purchased:\n\t\t\traise ValueError(\"'uploaded' and 'purchased' cannot both be False.\")\n\n\t\tif purchased and uploaded:\n\t\t\tsong_list = []\n\t\t\tfor chunk in self.songs_iter(export_type=1):\n\t\t\t\tsong_list.extend(chunk)\n\t\telif purchased:\n\t\t\tsong_list = []\n\t\t\tfor chunk in self.songs_iter(export_type=2):\n\t\t\t\tsong_list.extend(chunk)\n\t\telif uploaded:\n\t\t\tpurchased_songs = []\n\t\t\tfor chunk in self.songs_iter(export_type=2):\n\t\t\t\tpurchased_songs.extend(chunk)\n\n\t\t\tsong_list = [\n\t\t\t\tsong\n\t\t\t\tfor chunk in self.songs_iter(export_type=1)\n\t\t\t\tfor song in chunk\n\t\t\t\tif song not in purchased_songs\n\t\t\t]\n\n\t\treturn song_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a paged iterator of Music Library songs.", "response": "def songs_iter(self, *, continuation_token=None, export_type=1):\n\t\t\"\"\"Get a paged iterator of Music Library songs.\n\n\t\tParameters:\n\t\t\tcontinuation_token (str, Optional): The token of the page to return.\n\t\t\t\tDefault: Not sent to get first page.\n\t\t\texport_type (int, Optional): The type of tracks to return. 1 for all tracks, 2 for promotional and purchased.\n\t\t\t\tDefault: ``1``\n\n\t\tYields:\n\t\t\tlist: Song dicts.\n\t\t\"\"\"\n\n\t\tdef track_info_to_dict(track_info):\n\t\t\treturn dict(\n\t\t\t\t(field.name, value)\n\t\t\t\tfor field, value in track_info.ListFields()\n\t\t\t)\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmm_calls.ExportIDs,\n\t\t\t\tself.uploader_id,\n\t\t\t\tcontinuation_token=continuation_token,\n\t\t\t\texport_type=export_type\n\t\t\t)\n\n\t\t\titems = [\n\t\t\t\ttrack_info_to_dict(track_info)\n\t\t\t\tfor track_info in response.body.download_track_info\n\t\t\t]\n\n\t\t\tif items:\n\t\t\t\tyield items\n\n\t\t\tcontinuation_token = response.body.continuation_token\n\n\t\t\tif not continuation_token:\n\t\t\t\tbreak"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload(\n\t\tself,\n\t\tsong,\n\t\t*,\n\t\talbum_art_path=None,\n\t\tno_sample=False\n\t):\n\t\t\"\"\"Upload a song to a Google Music library.\n\n\t\tParameters:\n\t\t\tsong (os.PathLike or str or audio_metadata.Format):\n\t\t\t\tThe path to an audio file or an instance of :class:`audio_metadata.Format`.\n\t\t\talbum_art_path (os.PathLike or str, Optional):\n\t\t\t\tThe relative filename or absolute filepath to external album art.\n\t\t\tno_sample(bool, Optional):\n\t\t\t\tDon't generate an audio sample from song;\n\t\t\t\tsend empty audio sample.\n\t\t\t\tDefault: Create an audio sample using ffmpeg/avconv.\n\n\t\tReturns:\n\t\t\tdict: A result dict with keys: ``'filepath'``, ``'success'``, ``'reason'``, and ``'song_id'`` (if successful).\n\t\t\"\"\"\n\n\t\tif not isinstance(song, audio_metadata.Format):\n\t\t\ttry:\n\t\t\t\tsong = audio_metadata.load(song)\n\t\t\texcept audio_metadata.UnsupportedFormat:\n\t\t\t\traise ValueError(\"'song' must be FLAC, MP3, or WAV.\")\n\n\t\tif album_art_path:\n\t\t\talbum_art_path = Path(album_art_path).resolve()\n\n\t\t\tif album_art_path.is_file():\n\t\t\t\twith album_art_path.open('rb') as image_file:\n\t\t\t\t\texternal_art = image_file.read()\n\t\t\telse:\n\t\t\t\texternal_art = None\n\t\telse:\n\t\t\texternal_art = None\n\n\t\tresult = {'filepath': Path(song.filepath)}\n\n\t\ttrack_info = mm_calls.Metadata.get_track_info(song)\n\t\tresponse = self._call(\n\t\t\tmm_calls.Metadata,\n\t\t\tself.uploader_id,\n\t\t\t[track_info]\n\t\t)\n\n\t\tmetadata_response = response.body.metadata_response\n\n\t\tif metadata_response.signed_challenge_info:  # Sample requested.\n\t\t\tsample_request = metadata_response.signed_challenge_info[0]\n\n\t\t\ttry:\n\t\t\t\ttrack_sample = mm_calls.Sample.generate_sample(\n\t\t\t\t\tsong,\n\t\t\t\t\ttrack_info,\n\t\t\t\t\tsample_request,\n\t\t\t\t\texternal_art=external_art,\n\t\t\t\t\tno_sample=no_sample\n\t\t\t\t)\n\t\t\t\tresponse = self._call(\n\t\t\t\t\tmm_calls.Sample,\n\t\t\t\t\tself.uploader_id,\n\t\t\t\t\t[track_sample]\n\t\t\t\t)\n\t\t\t\ttrack_sample_response = response.body.sample_response.track_sample_response[0]\n\t\t\texcept (OSError, ValueError, subprocess.CalledProcessError):\n\t\t\t\traise  # TODO\n\t\telse:\n\t\t\ttrack_sample_response = metadata_response.track_sample_response[0]\n\n\t\tresponse_code = track_sample_response.response_code\n\n\t\tif response_code == upload_pb2.TrackSampleResponse.MATCHED:\n\t\t\tresult.update(\n\t\t\t\t{\n\t\t\t\t\t'success': True,\n\t\t\t\t\t'reason': 'Matched',\n\t\t\t\t\t'song_id': track_sample_response.server_track_id\n\t\t\t\t}\n\t\t\t)\n\t\telif response_code == upload_pb2.TrackSampleResponse.UPLOAD_REQUESTED:\n\t\t\tserver_track_id = track_sample_response.server_track_id\n\n\t\t\tself._call(\n\t\t\t\tmm_calls.UploadState,\n\t\t\t\tself.uploader_id,\n\t\t\t\t'START'\n\t\t\t)\n\n\t\t\tattempts = 0\n\t\t\tshould_retry = True\n\n\t\t\twhile should_retry and attempts <= 10:\n\t\t\t\t# Call with tenacity.retry_with to disable automatic retries.\n\t\t\t\tresponse = self._call.retry_with(stop=stop_after_attempt(1))(\n\t\t\t\t\tself,\n\t\t\t\t\tmm_calls.ScottyAgentPost,\n\t\t\t\t\tself.uploader_id,\n\t\t\t\t\tserver_track_id,\n\t\t\t\t\ttrack_info,\n\t\t\t\t\tsong,\n\t\t\t\t\texternal_art=external_art,\n\t\t\t\t\ttotal_song_count=1,\n\t\t\t\t\ttotal_uploaded_count=0\n\t\t\t\t)\n\n\t\t\t\tsession_response = response.body\n\n\t\t\t\tif 'sessionStatus' in session_response:\n\t\t\t\t\tbreak\n\n\t\t\t\ttry:\n\t\t\t\t\t# WHY, GOOGLE?! WHY???????????\n\t\t\t\t\tstatus_code = session_response['errorMessage']['additionalInfo'][\n\t\t\t\t\t\t'uploader_service.GoogleRupioAdditionalInfo'\n\t\t\t\t\t]['completionInfo']['customerSpecificInfo'][\n\t\t\t\t\t\t'ResponseCode'\n\t\t\t\t\t]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tstatus_code = None\n\n\t\t\t\tif status_code == 503:  # Upload server still syncing.\n\t\t\t\t\tshould_retry = True\n\t\t\t\t\treason = \"Server syncing\"\n\t\t\t\telif status_code == 200:  # Song is already uploaded.\n\t\t\t\t\tshould_retry = False\n\t\t\t\t\treason = \"Already uploaded\"\n\t\t\t\telif status_code == 404:  # Rejected.\n\t\t\t\t\tshould_retry = False\n\t\t\t\t\treason = \"Rejected\"\n\t\t\t\telse:\n\t\t\t\t\tshould_retry = True\n\t\t\t\t\treason = \"Unkown error\"\n\n\t\t\t\tattempts += 1\n\n\t\t\t\ttime.sleep(2)  # Give the server time to sync.\n\t\t\telse:\n\t\t\t\tresult.update(\n\t\t\t\t\t{\n\t\t\t\t\t\t'success': False,\n\t\t\t\t\t\t'reason': f'Could not get upload session: {reason}'\n\t\t\t\t\t}\n\t\t\t\t)\n\n\t\t\tif 'success' not in result:\n\t\t\t\ttransfer = session_response['sessionStatus']['externalFieldTransfers'][0]\n\n\t\t\t\tupload_url = transfer['putInfo']['url']\n\t\t\t\tcontent_type = transfer.get('content_type', 'audio/mpeg')\n\t\t\t\toriginal_content_type = track_info.original_content_type\n\n\t\t\t\ttranscode = (\n\t\t\t\t\tisinstance(song, audio_metadata.WAV)\n\t\t\t\t\tor original_content_type != locker_pb2.Track.MP3\n\t\t\t\t)\n\n\t\t\t\tif (\n\t\t\t\t\ttranscode\n\t\t\t\t\tor original_content_type == locker_pb2.Track.MP3\n\t\t\t\t):\n\t\t\t\t\tif transcode:\n\t\t\t\t\t\taudio_file = transcode_to_mp3(song, quality='320k')\n\t\t\t\t\telse:\n\t\t\t\t\t\twith open(song.filepath, 'rb') as f:\n\t\t\t\t\t\t\taudio_file = f.read()\n\n\t\t\t\t\t# Google Music allows a maximum file size of 300 MiB.\n\t\t\t\t\tif len(audio_file) >= 300 * 1024 * 1024:\n\t\t\t\t\t\tresult.update(\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t'success': False,\n\t\t\t\t\t\t\t\t'reason': 'Maximum allowed file size is 300 MiB.'\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tupload_response = self._call(\n\t\t\t\t\t\t\tmm_calls.ScottyAgentPut,\n\t\t\t\t\t\t\tupload_url,\n\t\t\t\t\t\t\taudio_file,\n\t\t\t\t\t\t\tcontent_type=content_type\n\t\t\t\t\t\t).body\n\n\t\t\t\t\t\tif upload_response.get('sessionStatus', {}).get('state'):\n\t\t\t\t\t\t\tresult.update(\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t'success': True,\n\t\t\t\t\t\t\t\t\t'reason': 'Uploaded',\n\t\t\t\t\t\t\t\t\t'song_id': track_sample_response.server_track_id\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tresult.update(\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t'success': False,\n\t\t\t\t\t\t\t\t\t'reason': upload_response  # TODO: Better error details.\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t# Do not upload files if transcode option set to False.\n\t\t\t\t\tresult.update(\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t'success': False,\n\t\t\t\t\t\t\t'reason': 'Transcoding disabled for file type.'\n\t\t\t\t\t\t}\n\t\t\t\t\t)\n\n\t\t\t\tself._call(mm_calls.UploadState, self.uploader_id, 'STOPPED')\n\t\telse:\n\t\t\tresponse_codes = upload_pb2._TRACKSAMPLERESPONSE.enum_types[0]\n\t\t\tresponse_type = response_codes.values_by_number[\n\t\t\t\ttrack_sample_response.response_code\n\t\t\t].name\n\n\t\t\treason = response_type\n\n\t\t\tresult.update(\n\t\t\t\t{\n\t\t\t\t\t'success': False,\n\t\t\t\t\t'reason': f'{reason}'\n\t\t\t\t}\n\t\t\t)\n\n\t\t\tif response_type == 'ALREADY_EXISTS':\n\t\t\t\tresult['song_id'] = track_sample_response.server_track_id\n\n\t\treturn result", "response": "Upload a song to a Google Music library."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog in to Google Music.", "response": "def login(self, username, *, token=None):\n\t\t\"\"\"Log in to Google Music.\n\n\t\tParameters:\n\t\t\tusername (str, Optional): Your Google Music username.\n\t\t\t\tUsed for keeping stored OAuth tokens for multiple accounts separate.\n\t\t\tdevice_id (str, Optional): A mobile device ID or music manager uploader ID.\n\t\t\t\tDefault: MAC address is used.\n\t\t\ttoken (dict, Optional): An OAuth token compatible with ``requests-oauthlib``.\n\n\t\tReturns:\n\t\t\tbool: ``True`` if successfully authenticated, ``False`` if not.\n\t\t\"\"\"\n\n\t\tself._username = username\n\t\tself._oauth(username, token=token)\n\n\t\treturn self.is_authenticated"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs in to Google Music with a different user.", "response": "def switch_user(self, username='', *, token=None):\n\t\t\"\"\"Log in to Google Music with a different user.\n\n\t\tParameters:\n\t\t\tusername (str, Optional): Your Google Music username.\n\t\t\t\tUsed for keeping stored OAuth tokens for multiple accounts separate.\n\t\t\ttoken (dict, Optional): An OAuth token compatible with ``requests-oauthlib``.\n\n\t\tReturns:\n\t\t\tbool: ``True`` if successfully authenticated, ``False`` if not.\n\t\t\"\"\"\n\n\t\tif self.logout():\n\t\t\treturn self.login(username, token=token)\n\n\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_tau(S, K, delta):\n    pivot = floor(K/S)\n    return [S/K * 1/d for d in range(1, pivot)] \\\n            + [S/K * log(S/delta)] \\\n            + [0 for d in range(pivot, K)]", "response": "The Robust part of the RSD"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_mu(K, delta, c):\n\n    S = c * log(K/delta) * sqrt(K) \n    tau = gen_tau(S, K, delta)\n    rho = gen_rho(K)\n    normalizer = sum(rho) + sum(tau)\n    return [(rho[d] + tau[d])/normalizer for d in range(K)]", "response": "The Robust Soliton Distribution on the degree of \n    transmitted blocks\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_rsd_cdf(K, delta, c):\n\n    mu = gen_mu(K, delta, c)\n    return [sum(mu[:d+1]) for d in range(K)]", "response": "The CDF of the RSD on block degree precomputed for\n    sampling speed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_next(self):\n\n        self.state = PRNG_A * self.state % PRNG_M\n        return self.state", "response": "Executes the next iteration of the PRNG\n        evolution process and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsample degree given the precomputed distributions above and the linear PRNG output .", "response": "def _sample_d(self):\n        \"\"\"Samples degree given the precomputed\n        distributions above and the linear PRNG output\n        \"\"\"\n\n        p = self._get_next() / PRNG_MAX_RAND\n        for ix, v in enumerate(self.cdf):\n            if v > p:\n                return ix + 1\n        return ix + 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(fn, blocksize, seed, c, delta):\n\n    with open(fn, 'rb') as f:\n        for block in encode.encoder(f, blocksize, seed, c, delta):\n            sys.stdout.buffer.write(block)", "response": "Run the encoder until the file is broken"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_subscribed(self):\n\n\t\tsubscribed = next(\n\t\t\t(\n\t\t\t\tconfig_item['value'] == 'true'\n\t\t\t\tfor config_item in self.config()\n\t\t\t\tif config_item['key'] == 'isNautilusUser'\n\t\t\t),\n\t\t\tNone\n\t\t)\n\n\t\tif subscribed:\n\t\t\tself.tier = 'aa'\n\t\telse:\n\t\t\tself.tier = 'fr'\n\n\t\treturn subscribed", "response": "Returns the subscription status of the account linked to the MobileClient instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef album(self, album_id, *, include_description=True, include_songs=True):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.FetchAlbum,\n\t\t\talbum_id,\n\t\t\tinclude_description=include_description,\n\t\t\tinclude_tracks=include_songs\n\t\t)\n\t\talbum_info = response.body\n\n\t\treturn album_info", "response": "Get information about an album."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget information about an artist.", "response": "def artist(\n\t\tself, artist_id, *, include_albums=True, num_related_artists=5, num_top_tracks=5\n\t):\n\t\t\"\"\"Get information about an artist.\n\n\t\tParameters:\n\t\t\tartist_id (str): An artist ID. Artist IDs start with an 'A'.\n\t\t\tinclude_albums (bool, Optional): Include albums by the artist in returned dict.\n\t\t\t\tDefault: ``True``.\n\t\t\tnum_related_artists (int, Optional): Include up to given number of related artists in returned dict.\n\t\t\t\tDefault: ``5``.\n\t\t\tnum_top_tracks (int, Optional): Include up to given number of top tracks in returned dict.\n\t\t\t\tDefault: ``5``.\n\n\t\tReturns:\n\t\t\tdict: Artist information.\n\t\t\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.FetchArtist,\n\t\t\tartist_id,\n\t\t\tinclude_albums=include_albums,\n\t\t\tnum_related_artists=num_related_artists,\n\t\t\tnum_top_tracks=num_top_tracks\n\t\t)\n\t\tartist_info = response.body\n\n\t\treturn artist_info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef browse_podcasts(self, podcast_genre_id='JZCpodcasttopchartall'):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.PodcastBrowse,\n\t\t\tpodcast_genre_id=podcast_genre_id\n\t\t)\n\t\tpodcast_series_list = response.body.get('series', [])\n\n\t\treturn podcast_series_list", "response": "Get the podcasts for a genre from the Podcasts browse tab."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef browse_podcasts_genres(self):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.PodcastBrowseHierarchy\n\t\t)\n\t\tgenres = response.body.get('groups', [])\n\n\t\treturn genres", "response": "Get the genres from the Podcasts browse tab dropdown."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the stations for a category from Browse Stations.", "response": "def browse_stations(self, station_category_id):\n\t\t\"\"\"Get the stations for a category from Browse Stations.\n\n\t\tParameters:\n\t\t\tstation_category_id (str): A station category ID as\n\t\t\t\tfound with :meth:`browse_stations_categories`.\n\n\t\tReturns:\n\t\t\tlist: Station dicts.\n\t\t\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.BrowseStations,\n\t\t\tstation_category_id\n\t\t)\n\t\tstations = response.body.get('stations', [])\n\n\t\treturn stations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the categories from Browse Stations.", "response": "def browse_stations_categories(self):\n\t\t\"\"\"Get the categories from Browse Stations.\n\n\t\tReturns:\n\t\t\tlist: Station categories that can contain subcategories.\n\t\t\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.BrowseStationCategories\n\t\t)\n\t\tstation_categories = response.body.get('root', {}).get('subcategories', [])\n\n\t\treturn station_categories"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef config(self):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.Config\n\t\t)\n\t\tconfig_list = response.body.get('data', {}).get('entries', [])\n\n\t\treturn config_list", "response": "Get a listing of mobile client configuration settings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef device_set(self, device):\n\n\t\tif device['id'].startswith('0x'):\n\t\t\tself.device_id = device['id'][2:]\n\t\telif device['id'].startswith('ios:'):\n\t\t\tself.device_id = device['id'].replace(':', '')\n\t\telse:\n\t\t\tself.device_id = device['id']", "response": "Set device used by MobileClient instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef devices(self):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.DeviceManagementInfo\n\t\t)\n\t\tregistered_devices = response.body.get('data', {}).get('items', [])\n\n\t\treturn registered_devices", "response": "Get a listing of devices registered to the Google Music account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a listing of song genres.", "response": "def explore_genres(self, parent_genre_id=None):\n\t\t\"\"\"Get a listing of song genres.\n\n\t\tParameters:\n\t\t\tparent_genre_id (str, Optional): A genre ID.\n\t\t\t\tIf given, a listing of this genre's sub-genres is returned.\n\n\t\tReturns:\n\t\t\tlist: Genre dicts.\n\t\t\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.ExploreGenres,\n\t\t\tparent_genre_id\n\t\t)\n\t\tgenre_list = response.body.get('genres', [])\n\n\t\treturn genre_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef explore_tabs(self, *, num_items=100, genre_id=None):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.ExploreTabs,\n\t\t\tnum_items=num_items,\n\t\t\tgenre_id=genre_id\n\t\t)\n\t\ttab_list = response.body.get('tabs', [])\n\n\t\texplore_tabs = {}\n\t\tfor tab in tab_list:\n\t\t\texplore_tabs[tab['tab_type'].lower()] = tab\n\n\t\treturn explore_tabs", "response": "Get a listing of explore tabs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a listing of items dismissed from Listen Now tab.", "response": "def listen_now_dismissed_items(self):\n\t\t\"\"\"Get a listing of items dismissed from Listen Now tab.\"\"\"\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.ListenNowGetDismissedItems\n\t\t)\n\t\tdismissed_items = response.body.get('items', [])\n\n\t\treturn dismissed_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef listen_now_items(self):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.ListenNowGetListenNowItems\n\t\t)\n\t\tlisten_now_item_list = response.body.get('listennow_items', [])\n\n\t\tlisten_now_items = defaultdict(list)\n\t\tfor item in listen_now_item_list:\n\t\t\ttype_ = f\"{ListenNowItemType(item['type']).name}s\"\n\t\t\tlisten_now_items[type_].append(item)\n\n\t\treturn dict(listen_now_items)", "response": "Get a listing of Listen Now items."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets information about a playlist song.", "response": "def playlist_song(self, playlist_song_id):\n\t\t\"\"\"Get information about a playlist song.\n\n\t\tNote:\n\t\t\tThis returns the playlist entry information only.\n\t\t\tFor full song metadata, use :meth:`song` with\n\t\t\tthe ``'trackId'`` field.\n\n\t\tParameters:\n\t\t\tplaylist_song_id (str): A playlist song ID.\n\n\t\tReturns:\n\t\t\tdict: Playlist song information.\n\t\t\"\"\"\n\n\t\tplaylist_song_info = next(\n\t\t\t(\n\t\t\t\tplaylist_song\n\t\t\t\tfor playlist in self.playlists(include_songs=True)\n\t\t\t\tfor playlist_song in playlist['tracks']\n\t\t\t\tif playlist_song['id'] == playlist_song_id\n\t\t\t),\n\t\t\tNone\n\t\t)\n\n\t\treturn playlist_song_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef playlist_song_add(\n\t\tself,\n\t\tsong,\n\t\tplaylist,\n\t\t*,\n\t\tafter=None,\n\t\tbefore=None,\n\t\tindex=None,\n\t\tposition=None\n\t):\n\t\t\"\"\"Add a song to a playlist.\n\n\t\tNote:\n\t\t\t* Provide no optional arguments to add to end.\n\t\t\t* Provide playlist song dicts for ``after`` and/or ``before``.\n\t\t\t* Provide a zero-based ``index``.\n\t\t\t* Provide a one-based ``position``.\n\n\t\t\tSongs are inserted *at* given index or position.\n\t\t\tIt's also possible to add to the end by using\n\t\t\t``len(songs)`` for index or ``len(songs) + 1`` for position.\n\n\t\tParameters:\n\t\t\tsong (dict): A song dict.\n\t\t\tplaylist (dict): A playlist dict.\n\t\t\tafter (dict, Optional): A playlist song dict ``songs`` will follow.\n\t\t\tbefore (dict, Optional): A playlist song dict ``songs`` will precede.\n\t\t\tindex (int, Optional): The zero-based index position to insert ``song``.\n\t\t\tposition (int, Optional): The one-based position to insert ``song``.\n\n\t\tReturns:\n\t\t\tdict: Playlist dict including songs.\n\t\t\"\"\"\n\n\t\tprev, next_ = get_ple_prev_next(\n\t\t\tself.playlist_songs(playlist),\n\t\t\tafter=after,\n\t\t\tbefore=before,\n\t\t\tindex=index,\n\t\t\tposition=position\n\t\t)\n\n\t\tif 'storeId' in song:\n\t\t\tsong_id = song['storeId']\n\t\telif 'trackId' in song:\n\t\t\tsong_id = song['trackId']\n\t\telse:\n\t\t\tsong_id = song['id']\n\n\t\tmutation = mc_calls.PlaylistEntriesBatch.create(\n\t\t\tsong_id, playlist['id'],\n\t\t\tpreceding_entry_id=prev.get('id'),\n\t\t\tfollowing_entry_id=next_.get('id')\n\t\t)\n\t\tself._call(mc_calls.PlaylistEntriesBatch, mutation)\n\n\t\treturn self.playlist(playlist['id'], include_songs=True)", "response": "Add a song to a playlist."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef playlist_songs_add(\n\t\tself,\n\t\tsongs,\n\t\tplaylist,\n\t\t*,\n\t\tafter=None,\n\t\tbefore=None,\n\t\tindex=None,\n\t\tposition=None\n\t):\n\t\t\"\"\"Add songs to a playlist.\n\n\t\tNote:\n\t\t\t* Provide no optional arguments to add to end.\n\t\t\t* Provide playlist song dicts for ``after`` and/or ``before``.\n\t\t\t* Provide a zero-based ``index``.\n\t\t\t* Provide a one-based ``position``.\n\n\t\t\tSongs are inserted *at* given index or position.\n\t\t\tIt's also possible to add to the end by using\n\t\t\t``len(songs)`` for index or ``len(songs) + 1`` for position.\n\n\t\tParameters:\n\t\t\tsongs (list): A list of song dicts.\n\t\t\tplaylist (dict): A playlist dict.\n\t\t\tafter (dict, Optional): A playlist song dict ``songs`` will follow.\n\t\t\tbefore (dict, Optional): A playlist song dict ``songs`` will precede.\n\t\t\tindex (int, Optional): The zero-based index position to insert ``songs``.\n\t\t\tposition (int, Optional): The one-based position to insert ``songs``.\n\n\t\tReturns:\n\t\t\tdict: Playlist dict including songs.\n\t\t\"\"\"\n\n\t\tplaylist_songs = self.playlist_songs(playlist)\n\n\t\tprev, next_ = get_ple_prev_next(\n\t\t\tplaylist_songs,\n\t\t\tafter=after,\n\t\t\tbefore=before,\n\t\t\tindex=index,\n\t\t\tposition=position\n\t\t)\n\n\t\tsongs_len = len(songs)\n\t\tfor i, song in enumerate(songs):\n\t\t\tif 'storeId' in song:\n\t\t\t\tsong_id = song['storeId']\n\t\t\telif 'trackId' in song:\n\t\t\t\tsong_id = song['trackId']\n\t\t\telse:\n\t\t\t\tsong_id = song['id']\n\n\t\t\tmutation = mc_calls.PlaylistEntriesBatch.create(\n\t\t\t\tsong_id, playlist['id'],\n\t\t\t\tpreceding_entry_id=prev.get('id'),\n\t\t\t\tfollowing_entry_id=next_.get('id')\n\t\t\t)\n\t\t\tresponse = self._call(mc_calls.PlaylistEntriesBatch, mutation)\n\t\t\tresult = response.body['mutate_response'][0]\n\n\t\t\t# TODO: Proper exception on failure.\n\t\t\tif result['response_code'] != 'OK':\n\t\t\t\tbreak\n\n\t\t\tif i < songs_len - 1:\n\t\t\t\twhile True:\n\t\t\t\t\tprev = self.playlist_song(result['id'])\n\t\t\t\t\tif prev:\n\t\t\t\t\t\tbreak\n\n\t\treturn self.playlist(playlist['id'], include_songs=True)", "response": "Add songs to a playlist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef playlist_song_delete(self, playlist_song):\n\n\t\tself.playlist_songs_delete([playlist_song])\n\n\t\treturn self.playlist(playlist_song['playlistId'], include_songs=True)", "response": "Delete a song from a playlist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef playlist_songs_delete(self, playlist_songs):\n\n\t\tif not more_itertools.all_equal(\n\t\t\tplaylist_song['playlistId']\n\t\t\tfor playlist_song in playlist_songs\n\t\t):\n\t\t\traise ValueError(\n\t\t\t\t\"All 'playlist_songs' must be from the same playlist.\"\n\t\t\t)\n\n\t\tmutations = [mc_calls.PlaylistEntriesBatch.delete(playlist_song['id']) for playlist_song in playlist_songs]\n\t\tself._call(mc_calls.PlaylistEntriesBatch, mutations)\n\n\t\treturn self.playlist(playlist_songs[0]['playlistId'], include_songs=True)", "response": "Delete songs from playlist."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove a song in a playlist.", "response": "def playlist_song_move(\n\t\tself,\n\t\tplaylist_song,\n\t\t*,\n\t\tafter=None,\n\t\tbefore=None,\n\t\tindex=None,\n\t\tposition=None\n\t):\n\t\t\"\"\"Move a song in a playlist.\n\n\t\tNote:\n\t\t\t* Provide no optional arguments to move to end.\n\t\t\t* Provide playlist song dicts for ``after`` and/or ``before``.\n\t\t\t* Provide a zero-based ``index``.\n\t\t\t* Provide a one-based ``position``.\n\n\t\t\tSongs are inserted *at* given index or position.\n\t\t\tIt's also possible to move to the end by using\n\t\t\t``len(songs)`` for index or ``len(songs) + 1`` for position.\n\n\t\tParameters:\n\t\t\tplaylist_song (dict): A playlist song dict.\n\t\t\tafter (dict, Optional): A playlist song dict ``songs`` will follow.\n\t\t\tbefore (dict, Optional): A playlist song dict ``songs`` will precede.\n\t\t\tindex (int, Optional): The zero-based index position to insert ``song``.\n\t\t\tposition (int, Optional): The one-based position to insert ``song``.\n\n\t\tReturns:\n\t\t\tdict: Playlist dict including songs.\n\t\t\"\"\"\n\n\t\tplaylist_songs = self.playlist(\n\t\t\tplaylist_song['playlistId'],\n\t\t\tinclude_songs=True\n\t\t)['tracks']\n\n\t\tprev, next_ = get_ple_prev_next(\n\t\t\tplaylist_songs,\n\t\t\tafter=after,\n\t\t\tbefore=before,\n\t\t\tindex=index,\n\t\t\tposition=position\n\t\t)\n\n\t\tmutation = mc_calls.PlaylistEntriesBatch.update(\n\t\t\tplaylist_song,\n\t\t\tpreceding_entry_id=prev.get('id'),\n\t\t\tfollowing_entry_id=next_.get('id')\n\t\t)\n\t\tself._call(mc_calls.PlaylistEntriesBatch, mutation)\n\n\t\treturn self.playlist(playlist_song['playlistId'], include_songs=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving songs in a playlist.", "response": "def playlist_songs_move(\n\t\tself,\n\t\tplaylist_songs,\n\t\t*,\n\t\tafter=None,\n\t\tbefore=None,\n\t\tindex=None,\n\t\tposition=None\n\t):\n\t\t\"\"\"Move songs in a playlist.\n\n\t\tNote:\n\t\t\t* Provide no optional arguments to move to end.\n\t\t\t* Provide playlist song dicts for ``after`` and/or ``before``.\n\t\t\t* Provide a zero-based ``index``.\n\t\t\t* Provide a one-based ``position``.\n\n\t\t\tSongs are inserted *at* given index or position.\n\t\t\tIt's also possible to move to the end by using\n\t\t\t``len(songs)`` for index or ``len(songs) + 1`` for position.\n\n\t\tParameters:\n\t\t\tplaylist_songs (list): A list of playlist song dicts.\n\t\t\tafter (dict, Optional): A playlist song dict ``songs`` will follow.\n\t\t\tbefore (dict, Optional): A playlist song dict ``songs`` will precede.\n\t\t\tindex (int, Optional): The zero-based index position to insert ``songs``.\n\t\t\tposition (int, Optional): The one-based position to insert ``songs``.\n\n\t\tReturns:\n\t\t\tdict: Playlist dict including songs.\n\t\t\"\"\"\n\n\t\tif not more_itertools.all_equal(\n\t\t\tplaylist_song['playlistId']\n\t\t\tfor playlist_song in playlist_songs\n\t\t):\n\t\t\traise ValueError(\n\t\t\t\t\"All 'playlist_songs' must be from the same playlist.\"\n\t\t\t)\n\n\t\tplaylist = self.playlist(\n\t\t\tplaylist_songs[0]['playlistId'],\n\t\t\tinclude_songs=True\n\t\t)\n\n\t\tprev, next_ = get_ple_prev_next(\n\t\t\tplaylist['tracks'],\n\t\t\tafter=after,\n\t\t\tbefore=before,\n\t\t\tindex=index,\n\t\t\tposition=position\n\t\t)\n\n\t\tplaylist_songs_len = len(playlist_songs)\n\t\tfor i, playlist_song in enumerate(playlist_songs):\n\t\t\tmutation = mc_calls.PlaylistEntriesBatch.update(\n\t\t\t\tplaylist_song,\n\t\t\t\tpreceding_entry_id=prev.get('id'),\n\t\t\t\tfollowing_entry_id=next_.get('id')\n\t\t\t)\n\t\t\tresponse = self._call(mc_calls.PlaylistEntriesBatch, mutation)\n\t\t\tresult = response.body['mutate_response'][0]\n\n\t\t\t# TODO: Proper exception on failure.\n\t\t\tif result['response_code'] != 'OK':\n\t\t\t\tbreak\n\n\t\t\tif i < playlist_songs_len - 1:\n\t\t\t\twhile True:\n\t\t\t\t\tprev = self.playlist_song(result['id'])\n\t\t\t\t\tif prev:\n\t\t\t\t\t\tbreak\n\n\t\treturn self.playlist(playlist_songs[0]['playlistId'], include_songs=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a listing of songs from a playlist.", "response": "def playlist_songs(self, playlist):\n\t\t\"\"\"Get a listing of songs from a playlist.\n\n\t\tParamters:\n\t\t\tplaylist (dict): A playlist dict.\n\n\t\tReturns:\n\t\t\tlist: Playlist song dicts.\n\t\t\"\"\"\n\n\t\tplaylist_type = playlist.get('type')\n\n\t\tplaylist_song_list = []\n\t\tif playlist_type in ('USER_GENERATED', None):\n\t\t\tstart_token = None\n\t\t\tplaylist_song_list = []\n\t\t\twhile True:\n\t\t\t\tresponse = self._call(\n\t\t\t\t\tmc_calls.PlaylistEntryFeed,\n\t\t\t\t\tmax_results=49995,\n\t\t\t\t\tstart_token=start_token\n\t\t\t\t)\n\t\t\t\titems = response.body.get('data', {}).get('items', [])\n\n\t\t\t\tif items:\n\t\t\t\t\tplaylist_song_list.extend(items)\n\n\t\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\t\tif start_token is None:\n\t\t\t\t\tbreak\n\t\telif playlist_type == 'SHARED':\n\t\t\tplaylist_share_token = playlist['shareToken']\n\n\t\t\tstart_token = None\n\t\t\tplaylist_song_list = []\n\t\t\twhile True:\n\t\t\t\tresponse = self._call(\n\t\t\t\t\tmc_calls.PlaylistEntriesShared,\n\t\t\t\t\tplaylist_share_token,\n\t\t\t\t\tmax_results=49995,\n\t\t\t\t\tstart_token=start_token\n\t\t\t\t)\n\t\t\t\tentry = response.body['entries'][0]\n\t\t\t\titems = entry.get('playlistEntry', [])\n\n\t\t\t\tif items:\n\t\t\t\t\tplaylist_song_list.extend(items)\n\n\t\t\t\tstart_token = entry.get('nextPageToken')\n\t\t\t\tif start_token is None:\n\t\t\t\t\tbreak\n\n\t\tplaylist_song_list.sort(key=itemgetter('absolutePosition'))\n\n\t\treturn playlist_song_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef playlist(self, playlist_id, *, include_songs=False):\n\n\t\tplaylist_info = next(\n\t\t\t(\n\t\t\t\tplaylist\n\t\t\t\tfor playlist in self.playlists(include_songs=include_songs)\n\t\t\t\tif playlist['id'] == playlist_id\n\t\t\t),\n\t\t\tNone\n\t\t)\n\n\t\treturn playlist_info", "response": "Get information about a playlist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a playlist. Parameters: name (str): Name to give the playlist. description (str): Description to give the playlist. make_public (bool, Optional): If ``True`` and account has a subscription, make playlist public. Default: ``False`` songs (list, Optional): A list of song dicts to add to the playlist. Returns: dict: Playlist information.", "response": "def playlist_create(\n\t\tself,\n\t\tname,\n\t\tdescription='',\n\t\t*,\n\t\tmake_public=False,\n\t\tsongs=None\n\t):\n\t\t\"\"\"Create a playlist.\n\n\t\tParameters:\n\t\t\tname (str): Name to give the playlist.\n\t\t\tdescription (str): Description to give the playlist.\n\t\t\tmake_public (bool, Optional): If ``True`` and account has a subscription,\n\t\t\t\tmake playlist public.\n\t\t\t\tDefault: ``False``\n\t\t\tsongs (list, Optional): A list of song dicts to add to the playlist.\n\n\t\tReturns:\n\t\t\tdict: Playlist information.\n\t\t\"\"\"\n\n\t\tshare_state = 'PUBLIC' if make_public else 'PRIVATE'\n\n\t\tplaylist = self._call(\n\t\t\tmc_calls.PlaylistsCreate,\n\t\t\tname,\n\t\t\tdescription,\n\t\t\tshare_state\n\t\t).body\n\n\t\tif songs:\n\t\t\tplaylist = self.playlist_songs_add(songs, playlist)\n\n\t\treturn playlist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef playlist_edit(self, playlist, *, name=None, description=None, public=None):\n\n\t\tif all(\n\t\t\tvalue is None\n\t\t\tfor value in (name, description, public)\n\t\t):\n\t\t\traise ValueError(\n\t\t\t\t'At least one of name, description, or public must be provided'\n\t\t\t)\n\n\t\tplaylist_id = playlist['id']\n\t\tplaylist = self.playlist(playlist_id)\n\n\t\tname = name if name is not None else playlist['name']\n\t\tdescription = (\n\t\t\tdescription if description is not None else playlist['description']\n\t\t)\n\t\tshare_state = 'PUBLIC' if public else playlist['accessControlled']\n\n\t\tplaylist = self._call(\n\t\t\tmc_calls.PlaylistsUpdate,\n\t\t\tplaylist_id,\n\t\t\tname,\n\t\t\tdescription,\n\t\t\tshare_state\n\t\t).body\n\n\t\treturn playlist", "response": "Edit playlist(s).\n\n\t\tParameters:\n\t\t\tplaylist (dict): A playlist dict.\n\t\t\tname (str): Name to give the playlist.\n\t\t\tdescription (str, Optional): Description to give the playlist.\n\t\t\tmake_public (bool, Optional): If ``True`` and account has a subscription,\n\t\t\t\tmake playlist public.\n\t\t\t\tDefault: ``False``\n\n\t\tReturns:\n\t\t\tdict: Playlist information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubscribe to a public playlist.", "response": "def playlist_subscribe(self, playlist):\n\t\t\"\"\"Subscribe to a public playlist.\n\n\t\tParameters:\n\t\t\tplaylist (dict): A public playlist dict.\n\n\t\tReturns:\n\t\t\tdict: Playlist information.\n\t\t\"\"\"\n\n\t\tmutation = mc_calls.PlaylistBatch.create(\n\t\t\tplaylist['name'],\n\t\t\tplaylist['description'],\n\t\t\t'SHARED',\n\t\t\towner_name=playlist.get('ownerName', ''),\n\t\t\tshare_token=playlist['shareToken']\n\t\t)\n\n\t\tresponse_body = self._call(\n\t\t\tmc_calls.PlaylistBatch,\n\t\t\tmutation\n\t\t).body\n\n\t\tplaylist_id = response_body['mutate_response'][0]['id']\n\n\t\treturn self.playlist(playlist_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a listing of library playlists.", "response": "def playlists(self, *, include_songs=False):\n\t\t\"\"\"Get a listing of library playlists.\n\n\t\tParameters:\n\t\t\tinclude_songs (bool, Optional): Include songs in the returned playlist dicts.\n\t\t\t\tDefault: ``False``.\n\n\t\tReturns:\n\t\t\tlist: A list of playlist dicts.\n\t\t\"\"\"\n\n\t\tplaylist_list = []\n\t\tfor chunk in self.playlists_iter(page_size=49995):\n\t\t\tfor playlist in chunk:\n\t\t\t\tif include_songs:\n\t\t\t\t\tplaylist['tracks'] = self.playlist_songs(playlist)\n\n\t\t\t\tplaylist_list.append(playlist)\n\n\t\treturn playlist_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a paged iterator of library playlists.", "response": "def playlists_iter(self, *, start_token=None, page_size=250):\n\t\t\"\"\"Get a paged iterator of library playlists.\n\n\t\tParameters:\n\t\t\tstart_token (str): The token of the page to return.\n\t\t\t\tDefault: Not sent to get first page.\n\t\t\tpage_size (int, Optional): The maximum number of results per returned page.\n\t\t\t\tMax allowed is ``49995``.\n\t\t\t\tDefault: ``250``\n\n\t\tYields:\n\t\t\tlist: Playlist dicts.\n\t\t\"\"\"\n\n\t\tstart_token = None\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.PlaylistFeed,\n\t\t\t\tmax_results=page_size,\n\t\t\t\tstart_token=start_token\n\t\t\t)\n\t\t\titems = response.body.get('data', {}).get('items', [])\n\n\t\t\tif items:\n\t\t\t\tyield items\n\n\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\tif start_token is None:\n\t\t\t\tbreak"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget information about a podcast series.", "response": "def podcast(self, podcast_series_id, *, max_episodes=50):\n\t\t\"\"\"Get information about a podcast series.\n\n\t\tParameters:\n\t\t\tpodcast_series_id (str): A podcast series ID.\n\t\t\tmax_episodes (int, Optional): Include up to given number of episodes in returned dict.\n\t\t\t\tDefault: ``50``\n\n\t\tReturns:\n\t\t\tdict: Podcast series information.\n\t\t\"\"\"\n\n\t\tpodcast_info = self._call(\n\t\t\tmc_calls.PodcastFetchSeries,\n\t\t\tpodcast_series_id,\n\t\t\tmax_episodes=max_episodes\n\t\t).body\n\n\t\treturn podcast_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef podcasts(self, *, device_id=None):\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tpodcast_list = []\n\t\tfor chunk in self.podcasts_iter(device_id=device_id, page_size=49995):\n\t\t\tpodcast_list.extend(chunk)\n\n\t\treturn podcast_list", "response": "Get a listing of subsribed podcast series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef podcasts_iter(self, *, device_id=None, page_size=250):\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tstart_token = None\n\t\tprev_items = None\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.PodcastSeries,\n\t\t\t\tdevice_id,\n\t\t\t\tmax_results=page_size,\n\t\t\t\tstart_token=start_token\n\t\t\t)\n\t\t\titems = response.body.get('data', {}).get('items', [])\n\n\t\t\t# Google does some weird shit.\n\t\t\tif items != prev_items:\n\t\t\t\tsubscribed_podcasts = [\n\t\t\t\t\titem\n\t\t\t\t\tfor item in items\n\t\t\t\t\tif item.get('userPreferences', {}).get('subscribed')\n\t\t\t\t]\n\n\t\t\t\tyield subscribed_podcasts\n\n\t\t\t\tprev_items = items\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\tif start_token is None:\n\t\t\t\tbreak", "response": "Get a paged iterator of subscribed podcast series."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef podcast_episode(self, podcast_episode_id):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.PodcastFetchEpisode,\n\t\t\tpodcast_episode_id\n\t\t)\n\t\tpodcast_episode_info = [\n\t\t\tpodcast_episode\n\t\t\tfor podcast_episode in response.body\n\t\t\tif not podcast_episode['deleted']\n\t\t]\n\n\t\treturn podcast_episode_info", "response": "Get information about a podcast episode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef podcast_episodes(self, *, device_id=None):\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tpodcast_episode_list = []\n\t\tfor chunk in self.podcast_episodes_iter(\n\t\t\tdevice_id=device_id,\n\t\t\tpage_size=49995\n\t\t):\n\t\t\tpodcast_episode_list.extend(chunk)\n\n\t\treturn podcast_episode_list", "response": "Get a listing of podcast episodes for all subscribed podcasts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef podcast_episodes_iter(self, *, device_id=None, page_size=250):\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tstart_token = None\n\t\tprev_items = None\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.PodcastEpisode,\n\t\t\t\tdevice_id,\n\t\t\t\tmax_results=page_size,\n\t\t\t\tstart_token=start_token\n\t\t\t)\n\t\t\titems = response.body.get('data', {}).get('items', [])\n\n\t\t\t# Google does some weird shit.\n\t\t\tif items != prev_items:\n\t\t\t\tyield items\n\n\t\t\t\tprev_items = items\n\t\t\telse:\n\t\t\t\tbreak\n\n\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\tif start_token is None:\n\t\t\t\tbreak", "response": "Get a paged iterator of podcast episode for all subscribed podcasts."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches Google Music and library for content.", "response": "def search(self, query, *, max_results=100, **kwargs):\n\t\t\"\"\"Search Google Music and library for content.\n\n\t\tParameters:\n\t\t\tquery (str): Search text.\n\t\t\tmax_results (int, Optional): Maximum number of results per type per\n\t\t\t\tlocation to retrieve. I.e up to 100 Google and 100 library\n\t\t\t\tfor a total of 200 for the default value.\n\t\t\t\tGoogle only accepts values up to 100.\n\t\t\t\tDefault: ``100``\n\t\t\tkwargs (bool, Optional): Any of ``albums``, ``artists``, ``genres``,\n\t\t\t\t``playlists``, ``podcasts``, ``situations``, ``songs``, ``stations``,\n\t\t\t\t``videos`` set to ``True`` will include that result type in the\n\t\t\t\treturned dict.\n\t\t\t\tSetting none of them will include all result types in the returned dict.\n\n\t\tReturns:\n\t\t\tdict: A dict of results separated into keys: ``'albums'``, ``'artists'``,\n\t\t\t\t``'genres'``, ``'playlists'``, ```'podcasts'``, ``'situations'``,\n\t\t\t\t``'songs'``, ``'stations'``, ``'videos'``.\n\n\t\tNote:\n\t\t\tFree account search is restricted so may not contain hits for all result types.\n\t\t\"\"\"\n\n\t\tresults = defaultdict(list)\n\n\t\tfor type_, results_ in self.search_library(\n\t\t\tquery,\n\t\t\tmax_results=max_results,\n\t\t\t**kwargs\n\t\t).items():\n\t\t\tresults[type_].extend(results_)\n\n\t\tfor type_, results_ in self.search_google(\n\t\t\tquery,\n\t\t\tmax_results=max_results,\n\t\t\t**kwargs\n\t\t).items():\n\t\t\tresults[type_].extend(results_)\n\n\t\treturn dict(results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_google(self, query, *, max_results=100, **kwargs):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.Query,\n\t\t\tquery,\n\t\t\tmax_results=max_results,\n\t\t\t**kwargs\n\t\t)\n\n\t\tclusters = response.body.get('clusterDetail', [])\n\t\tresults = defaultdict(list)\n\n\t\tfor cluster in clusters:\n\t\t\tresult_type = QueryResultType(cluster['cluster']['type']).name\n\n\t\t\tentries = cluster.get('entries', [])\n\t\t\tif len(entries) > 0:\n\t\t\t\tfor entry in entries:\n\t\t\t\t\titem_key = next(\n\t\t\t\t\t\tkey\n\t\t\t\t\t\tfor key in entry\n\t\t\t\t\t\tif key not in ['cluster', 'score', 'type']\n\t\t\t\t\t)\n\t\t\t\t\tresults[f\"{result_type}s\"].append(entry[item_key])\n\n\t\treturn dict(results)", "response": "Search Google Music for content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_library(self, query, *, max_results=100, **kwargs):\n\n\t\tdef match_fields(item, fields):\n\t\t\treturn any(\n\t\t\t\tquery.casefold() in item.get(field, '').casefold()\n\t\t\t\tfor field in fields\n\t\t\t)\n\n\t\ttypes = [\n\t\t\t(\n\t\t\t\t'playlists',\n\t\t\t\t['description', 'name'],\n\t\t\t\tself.playlists\n\t\t\t),\n\t\t\t(\n\t\t\t\t'podcasts',\n\t\t\t\t['author', 'description', 'title'],\n\t\t\t\tself.podcasts\n\t\t\t),\n\t\t\t(\n\t\t\t\t'songs',\n\t\t\t\t['album', 'albumArtist', 'artist', 'composer', 'genre', 'title'],\n\t\t\t\tself.songs\n\t\t\t),\n\t\t\t(\n\t\t\t\t'stations',\n\t\t\t\t['byline', 'description', 'name'],\n\t\t\t\tself.stations\n\t\t\t),\n\t\t]\n\n\t\tresults = {}\n\n\t\tfor type_, fields, func in types:\n\t\t\tif (not kwargs) or (type_ in kwargs):\n\t\t\t\tresults[type_] = [\n\t\t\t\t\titem\n\t\t\t\t\tfor item in func()\n\t\t\t\t\tif match_fields(item, fields)\n\t\t\t\t][:max_results]\n\n\t\treturn results", "response": "Search Google Music for content."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_suggestion(self, query):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.QuerySuggestion,\n\t\t\tquery\n\t\t)\n\t\tsuggested_queries = response.body.get('suggested_queries', [])\n\n\t\treturn [\n\t\t\tsuggested_query['suggestion_string']\n\t\t\tfor suggested_query in suggested_queries\n\t\t]", "response": "Get search query suggestions for query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a listing of album songs.", "response": "def shuffle_album(\n\t\tself, album, *, num_songs=100, only_library=False, recently_played=None\n\t):\n\t\t\"\"\"Get a listing of album shuffle/mix songs.\n\n\t\tParameters:\n\t\t\talbum (dict): An album dict.\n\t\t\tnum_songs (int, Optional): The maximum number of songs to return from the station.\n\t\t\t\tDefault: ``100``\n\t\t\tonly_library (bool, Optional): Only return content from library.\n\t\t\t\tDefault: False\n\t\t\trecently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'}\n\t\t\t\twhere ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song.\n\n\t\tReturns:\n\t\t\tlist: List of album shuffle/mix songs.\n\t\t\"\"\"\n\n\t\tstation_info = {\n\t\t\t'seed': {\n\t\t\t\t'albumId': album['albumId'],\n\t\t\t\t'seedType': StationSeedType.album.value\n\t\t\t},\n\t\t\t'num_entries': num_songs,\n\t\t\t'library_content_only': only_library,\n\t\t}\n\n\t\tif recently_played is not None:\n\t\t\tstation_info['recently_played'] = recently_played\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.RadioStationFeed,\n\t\t\tstation_infos=[station_info]\n\t\t)\n\t\tstation_feed = response.body.get('data', {}).get('stations', [])\n\n\t\ttry:\n\t\t\tstation = station_feed[0]\n\t\texcept IndexError:\n\t\t\tstation = {}\n\n\t\treturn station.get('tracks', [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a listing of artist shuffle/mix songs. Parameters: artist (dict): An artist dict. num_songs (int, Optional): The maximum number of songs to return from the station. Default: ``100`` only_library (bool, Optional): Only return content from library. Default: False recently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'} where ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song. only_artist (bool, Optional): If ``True``, only return songs from the artist, else return songs from artist and related artists. Default: ``False`` Returns: list: List of artist shuffle/mix songs.", "response": "def shuffle_artist(\n\t\tself,\n\t\tartist,\n\t\t*,\n\t\tnum_songs=100,\n\t\tonly_library=False,\n\t\trecently_played=None,\n\t\tonly_artist=False\n\t):\n\t\t\"\"\"Get a listing of artist shuffle/mix songs.\n\n\t\tParameters:\n\t\t\tartist (dict): An artist dict.\n\t\t\tnum_songs (int, Optional): The maximum number of songs to return from the station.\n\t\t\t\tDefault: ``100``\n\t\t\tonly_library (bool, Optional): Only return content from library.\n\t\t\t\tDefault: False\n\t\t\trecently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'}\n\t\t\t\twhere ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song.\n\t\t\tonly_artist (bool, Optional): If ``True``, only return songs from the artist,\n\t\t\t\t\telse return songs from artist and related artists.\n\t\t\t\t\tDefault: ``False``\n\n\t\tReturns:\n\t\t\tlist: List of artist shuffle/mix songs.\n\t\t\"\"\"\n\n\t\tstation_info = {\n\t\t\t'num_entries': num_songs,\n\t\t\t'library_content_only': only_library\n\t\t}\n\n\t\tif only_artist:\n\t\t\tstation_info['seed'] = {\n\t\t\t\t'artistId': artist['artistId'],\n\t\t\t\t'seedType': StationSeedType.artist_only.value\n\t\t\t}\n\t\telse:\n\t\t\tstation_info['seed'] = {\n\t\t\t\t'artistId': artist['artistId'],\n\t\t\t\t'seedType': StationSeedType.artist_related.value\n\t\t\t}\n\n\t\tif recently_played is not None:\n\t\t\tstation_info['recently_played'] = recently_played\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.RadioStationFeed,\n\t\t\tstation_infos=[station_info]\n\t\t)\n\t\tstation_feed = response.body.get('data', {}).get('stations', [])\n\n\t\ttry:\n\t\t\tstation = station_feed[0]\n\t\texcept IndexError:\n\t\t\tstation = {}\n\n\t\treturn station.get('tracks', [])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef shuffle_song(\n\t\tself, song, *, num_songs=100, only_library=False, recently_played=None\n\t):\n\t\t\"\"\"Get a listing of song shuffle/mix songs.\n\n\t\tParameters:\n\t\t\tsong (dict): A song dict.\n\t\t\tnum_songs (int, Optional): The maximum number of songs to return from the station.\n\t\t\t\tDefault: ``100``\n\t\t\tonly_library (bool, Optional): Only return content from library.\n\t\t\t\tDefault: False\n\t\t\trecently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'}\n\t\t\t\twhere ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song.\n\n\t\tReturns:\n\t\t\tlist: List of artist shuffle/mix songs.\n\t\t\"\"\"\n\n\t\tstation_info = {\n\t\t\t'num_entries': num_songs,\n\t\t\t'library_content_only': only_library\n\t\t}\n\n\t\tif 'storeId' in song:\n\t\t\tstation_info['seed'] = {\n\t\t\t\t'trackId': song['storeId'],\n\t\t\t\t'seedType': StationSeedType.store_track.value\n\t\t\t}\n\t\telse:\n\t\t\tstation_info['seed'] = {\n\t\t\t\t'trackLockerId': song['id'],\n\t\t\t\t'seedType': StationSeedType.library_track.value\n\t\t\t}\n\n\t\tif recently_played is not None:\n\t\t\tstation_info['recently_played'] = recently_played\n\n\t\tresponse = self._call(mc_calls.RadioStationFeed, station_infos=[station_info])\n\t\tstation_feed = response.body.get('data', {}).get('stations', [])\n\n\t\ttry:\n\t\t\tstation = station_feed[0]\n\t\texcept IndexError:\n\t\t\tstation = {}\n\n\t\treturn station.get('tracks', [])", "response": "Get a listing of a song shuffle or mix songs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef situations(self, *, tz_offset=None):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.ListenNowSituations,\n\t\t\ttz_offset\n\t\t)\n\t\tsituation_list = response.body.get('situations', [])\n\n\t\treturn situation_list", "response": "Get a listing of situations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about a song.", "response": "def song(self, song_id):\n\t\t\"\"\"Get information about a song.\n\n\t\tParameters:\n\t\t\tsong_id (str): A song ID.\n\n\t\tReturns:\n\t\t\tdict: Song information.\n\t\t\"\"\"\n\n\t\tif song_id.startswith('T'):\n\t\t\tsong_info = self._call(\n\t\t\t\tmc_calls.FetchTrack,\n\t\t\t\tsong_id\n\t\t\t).body\n\t\telse:\n\t\t\tsong_info = next(\n\t\t\t\t(\n\t\t\t\t\tsong\n\t\t\t\t\tfor song in self.songs()\n\t\t\t\t\tif song['id'] == song_id\n\t\t\t\t),\n\t\t\t\tNone\n\t\t\t)\n\n\t\treturn song_info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding store songs to your library.", "response": "def songs_add(self, songs):\n\t\t\"\"\"Add store songs to your library.\n\n\t\tParameters:\n\t\t\tsongs (list): A list of store song dicts.\n\n\t\tReturns:\n\t\t\tlist: Songs' library IDs.\n\t\t\"\"\"\n\n\t\tmutations = [mc_calls.TrackBatch.add(song) for song in songs]\n\t\tresponse = self._call(\n\t\t\tmc_calls.TrackBatch,\n\t\t\tmutations\n\t\t)\n\n\t\tsuccess_ids = [\n\t\t\tres['id']\n\t\t\tfor res in response.body['mutate_response']\n\t\t\tif res['response_code'] == 'OK'\n\t\t]\n\n\t\treturn success_ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef songs_delete(self, songs):\n\n\t\tmutations = [mc_calls.TrackBatch.delete(song['id']) for song in songs]\n\t\tresponse = self._call(\n\t\t\tmc_calls.TrackBatch,\n\t\t\tmutations\n\t\t)\n\n\t\tsuccess_ids = [\n\t\t\tres['id']\n\t\t\tfor res in response.body['mutate_response']\n\t\t\tif res['response_code'] == 'OK'\n\t\t]\n\n\t\t# TODO: Report failures.\n\t\t# failure_ids = [\n\t\t# \tres['id']\n\t\t# \tfor res in response.body['mutate_response']\n\t\t# \tif res['response_code'] != 'OK'\n\t\t# ]\n\n\t\treturn success_ids", "response": "Delete songs from library."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds play to song play count.", "response": "def song_play(self, song):\n\t\t\"\"\"Add play to song play count.\n\n\t\tParameters:\n\t\t\tsong (dict): A song dict.\n\n\t\tReturns:\n\t\t\tbool: ``True`` if successful, ``False`` if not.\n\t\t\"\"\"\n\n\t\tif 'storeId' in song:\n\t\t\tsong_id = song['storeId']\n\t\telif 'trackId' in song:\n\t\t\tsong_id = song['trackId']\n\t\telse:\n\t\t\tsong_id = song['id']\n\n\t\tsong_duration = song['durationMillis']\n\n\t\tevent = mc_calls.ActivityRecordRealtime.play(song_id, song_duration)\n\t\tresponse = self._call(\n\t\t\tmc_calls.ActivityRecordRealtime,\n\t\t\tevent\n\t\t)\n\n\t\treturn True if response.body['eventResults'][0]['code'] == 'OK' else False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrates song. Parameters: song (dict): A song dict. rating (int): 0 (not rated), 1 (thumbs down), or 5 (thumbs up). Returns: bool: ``True`` if successful, ``False`` if not.", "response": "def song_rate(self, song, rating):\n\t\t\"\"\"Rate song.\n\n\t\tParameters:\n\t\t\tsong (dict): A song dict.\n\t\t\trating (int): 0 (not rated), 1 (thumbs down), or 5 (thumbs up).\n\n\t\tReturns:\n\t\t\tbool: ``True`` if successful, ``False`` if not.\n\t\t\"\"\"\n\n\t\tif 'storeId' in song:\n\t\t\tsong_id = song['storeId']\n\t\telif 'trackId' in song:\n\t\t\tsong_id = song['trackId']\n\t\telse:\n\t\t\tsong_id = song['id']\n\n\t\tevent = mc_calls.ActivityRecordRealtime.rate(song_id, rating)\n\t\tresponse = self._call(\n\t\t\tmc_calls.ActivityRecordRealtime,\n\t\t\tevent\n\t\t)\n\n\t\treturn True if response.body['eventResults'][0]['code'] == 'OK' else False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef songs(self):\n\n\t\tsong_list = []\n\t\tfor chunk in self.songs_iter(page_size=49995):\n\t\t\tsong_list.extend(chunk)\n\n\t\treturn song_list", "response": "Get a listing of library songs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef songs_iter(self, *, page_size=250):\n\n\t\tstart_token = None\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.TrackFeed,\n\t\t\t\tmax_results=page_size,\n\t\t\t\tstart_token=start_token\n\t\t\t)\n\t\t\titems = response.body.get('data', {}).get('items', [])\n\n\t\t\tif items:\n\t\t\t\tyield items\n\n\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\tif start_token is None:\n\t\t\t\tbreak", "response": "Get a paged iterator of library songs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about a station.", "response": "def station(self, station_id, *, num_songs=25, recently_played=None):\n\t\t\"\"\"Get information about a station.\n\n\t\tParameters:\n\t\t\tstation_id (str): A station ID. Use 'IFL' for I'm Feeling Lucky.\n\t\t\tnum_songs (int, Optional): The maximum number of songs to return from the station.\n\t\t\t\tDefault: ``25``\n\t\t\trecently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'}\n\t\t\t\twhere ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song.\n\n\t\tReturns:\n\t\t\tdict: Station information.\n\t\t\"\"\"\n\n\t\tstation_info = {\n\t\t\t'station_id': station_id,\n\t\t\t'num_entries': num_songs,\n\t\t\t'library_content_only': False\n\t\t}\n\n\t\tif recently_played is not None:\n\t\t\tstation_info['recently_played'] = recently_played\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.RadioStationFeed,\n\t\t\tstation_infos=[station_info]\n\t\t)\n\t\tstation_feed = response.body.get('data', {}).get('stations', [])\n\n\t\ttry:\n\t\t\tstation = station_feed[0]\n\t\texcept IndexError:\n\t\t\tstation = {}\n\n\t\treturn station"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef station_feed(self, *, num_songs=25, num_stations=4):\n\n\t\tresponse = self._call(\n\t\t\tmc_calls.RadioStationFeed,\n\t\t\tnum_entries=num_songs,\n\t\t\tnum_stations=num_stations\n\t\t)\n\t\tstation_feed = response.body.get('data', {}).get('stations', [])\n\n\t\treturn station_feed", "response": "Generate stations.\n\n\t\tNote:\n\t\t\tA Google Music subscription is required.\n\n\t\tParameters:\n\t\t\tnum_songs (int, Optional): The total number of songs to return. Default: ``25``\n\t\t\tnum_stations (int, Optional): The number of stations to return when no station_infos is provided.\n\t\t\t\tDefault: ``5``\n\n\t\tReturns:\n\t\t\tlist: Station information dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a listing of songs from a station.", "response": "def station_songs(self, station, *, num_songs=25, recently_played=None):\n\t\t\"\"\"Get a listing of songs from a station.\n\n\t\tParameters:\n\t\t\tstation (str): A station dict.\n\t\t\tnum_songs (int, Optional): The maximum number of songs to return from the station. Default: ``25``\n\t\t\trecently_played (list, Optional): A list of dicts in the form of {'id': '', 'type'}\n\t\t\t\twhere ``id`` is a song ID and ``type`` is 0 for a library song and 1 for a store song.\n\n\t\tReturns:\n\t\t\tlist: Station song dicts.\n\t\t\"\"\"\n\n\t\tstation_id = station['id']\n\n\t\tstation = self.station(\n\t\t\tstation_id,\n\t\t\tnum_songs=num_songs,\n\t\t\trecently_played=recently_played\n\t\t)\n\n\t\treturn station.get('tracks', [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stations(self, *, generated=True, library=True):\n\n\t\tstation_list = []\n\t\tfor chunk in self.stations_iter(page_size=49995):\n\t\t\tfor station in chunk:\n\t\t\t\tif (\n\t\t\t\t\t(generated and not station.get('inLibrary'))\n\t\t\t\t\tor (library and station.get('inLibrary'))\n\t\t\t\t):\n\t\t\t\t\tstation_list.append(station)\n\n\t\treturn station_list", "response": "Get a listing of library stations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stations_iter(self, *, page_size=250):\n\n\t\tstart_token = None\n\n\t\twhile True:\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.RadioStation,\n\t\t\t\tmax_results=page_size,\n\t\t\t\tstart_token=start_token\n\t\t\t)\n\t\t\tyield response.body.get('data', {}).get('items', [])\n\n\t\t\tstart_token = response.body.get('nextPageToken')\n\t\t\tif start_token is None:\n\t\t\t\tbreak", "response": "Get a paged iterator of library stations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget MP3 stream of a podcast episode library song station_song or store song.", "response": "def stream(self, item, *, device_id=None, quality='hi', session_token=None):\n\t\t\"\"\"Get MP3 stream of a podcast episode, library song, station_song, or store song.\n\n\t\tNote:\n\t\t\tStreaming requires a ``device_id`` from a valid, linked mobile device.\n\n\t\tParameters:\n\t\t\titem (str): A podcast episode, library song, station_song, or store song.\n\t\t\t\tA Google Music subscription is required to stream store songs.\n\t\t\tdevice_id (str, Optional): A mobile device ID.\n\t\t\t\tDefault: Use ``device_id`` of the :class:`MobileClient` instance.\n\t\t\tquality (str, Optional): Stream quality is one of ``'hi'`` (320Kbps), ``'med'`` (160Kbps), or ``'low'`` (128Kbps).\n\t\t\t\tDefault: ``'hi'``.\n\t\t\tsession_token (str): Session token from a station dict required for unsubscribed users to stream a station song.\n\t\t\t\tstation['sessionToken'] as returend by :meth:`station` only exists for free accounts.\n\n\t\tReturns:\n\t\t\tbytes: An MP3 file.\n\t\t\"\"\"\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tstream_url = self.stream_url(\n\t\t\titem,\n\t\t\tdevice_id=device_id,\n\t\t\tquality=quality,\n\t\t\tsession_token=session_token\n\t\t)\n\t\tresponse = self.session.get(stream_url)\n\t\taudio = response.content\n\n\t\treturn audio"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a URL to stream a podcast episode library song station_song or store song.", "response": "def stream_url(self, item, *, device_id=None, quality='hi', session_token=None):\n\t\t\"\"\"Get a URL to stream a podcast episode, library song, station_song, or store song.\n\n\t\tNote:\n\t\t\tStreaming requires a ``device_id`` from a valid, linked mobile device.\n\n\t\tParameters:\n\t\t\titem (str): A podcast episode, library song, station_song, or store song.\n\t\t\t\tA Google Music subscription is required to stream store songs.\n\t\t\tdevice_id (str, Optional): A mobile device ID.\n\t\t\t\tDefault: Use ``device_id`` of the :class:`MobileClient` instance.\n\t\t\tquality (str, Optional): Stream quality is one of ``'hi'`` (320Kbps), ``'med'`` (160Kbps), or ``'low'`` (128Kbps).\n\t\t\t\tDefault: ``'hi'``.\n\t\t\tsession_token (str): Session token from a station dict required for unsubscribed users to stream a station song.\n\t\t\t\tstation['sessionToken'] as returend by :meth:`station` only exists for free accounts.\n\n\t\tReturns:\n\t\t\tstr: A URL to an MP3 file.\n\t\t\"\"\"\n\n\t\tif device_id is None:\n\t\t\tdevice_id = self.device_id\n\n\t\tif 'episodeId' in item:  # Podcast episode.\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.PodcastEpisodeStreamURL,\n\t\t\t\titem['episodeId'],\n\t\t\t\tquality=quality,\n\t\t\t\tdevice_id=device_id\n\t\t\t)\n\t\telif 'wentryid' in item:  # Free account station song.\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.RadioStationTrackStreamURL,\n\t\t\t\titem['storeId'],\n\t\t\t\titem['wentryid'],\n\t\t\t\tsession_token,\n\t\t\t\tquality=quality,\n\t\t\t\tdevice_id=device_id\n\t\t\t)\n\t\telif 'trackId' in item:  # Playlist song.\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.TrackStreamURL,\n\t\t\t\titem['trackId'],\n\t\t\t\tquality=quality,\n\t\t\t\tdevice_id=device_id\n\t\t\t)\n\t\telif 'storeId' in item and self.is_subscribed:  # Store song.\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.TrackStreamURL,\n\t\t\t\titem['storeId'],\n\t\t\t\tquality=quality,\n\t\t\t\tdevice_id=device_id\n\t\t\t)\n\t\telif 'id' in item:  # Library song.\n\t\t\tresponse = self._call(\n\t\t\t\tmc_calls.TrackStreamURL,\n\t\t\t\titem['id'],\n\t\t\t\tquality=quality,\n\t\t\t\tdevice_id=device_id\n\t\t\t)\n\t\telse:\n\t\t\t# TODO: Create an exception for not being subscribed or use a better builtin exception for this case.\n\t\t\tif 'storeId' in item and not self.is_subscribed:\n\t\t\t\tmsg = \"Can't stream a store song without a subscription.\"\n\t\t\telse:\n\t\t\t\tmsg = \"Item does not contain an ID field.\"\n\n\t\t\traise ValueError(msg)\n\n\t\ttry:\n\t\t\tstream_url = response.headers['Location']\n\t\texcept KeyError:\n\t\t\tstream_url = response.body['url']\n\n\t\treturn stream_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a listing of Thumbs Up songs.", "response": "def thumbs_up_songs(self, *, library=True, store=True):\n\t\t\"\"\"Get a listing of 'Thumbs Up' store songs.\n\n\t\tParameters:\n\t\t\tlibrary (bool, Optional): Include 'Thumbs Up' songs from library.\n\t\t\t\tDefault: True\n\t\t\tgenerated (bool, Optional): Include 'Thumbs Up' songs from store.\n\t\t\t\tDefault: True\n\n\t\tReturns:\n\t\t\tlist: Dicts of 'Thumbs Up' songs.\n\t\t\"\"\"\n\n\t\tthumbs_up_songs = []\n\n\t\tif library is True:\n\t\t\tthumbs_up_songs.extend(\n\t\t\t\tsong\n\t\t\t\tfor song in self.songs()\n\t\t\t\tif song.get('rating', '0') == '5'\n\t\t\t)\n\n\t\tif store is True:\n\t\t\tresponse = self._call(mc_calls.EphemeralTop)\n\t\t\tthumbs_up_songs.extend(response.body.get('data', {}).get('items', []))\n\n\t\treturn thumbs_up_songs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef top_charts(self):\n\n\t\tresponse = self._call(mc_calls.BrowseTopChart)\n\t\ttop_charts = response.body\n\n\t\treturn top_charts", "response": "Get a listing of the default top charts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef top_charts_for_genre(self, genre_id):\n\n\t\tresponse = self._call(mc_calls.BrowseTopChartForGenre, genre_id)\n\t\ttop_chart_for_genre = response.body\n\n\t\treturn top_chart_for_genre", "response": "Get a listing of top charts for a top chart genre."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef top_charts_genres(self):\n\n\t\tresponse = self._call(mc_calls.BrowseTopChartGenres)\n\t\ttop_chart_genres = response.body.get('genres', [])\n\n\t\treturn top_chart_genres", "response": "Get a listing of genres from the browse top charts tab."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread from stream applying the LT decoding algorithm to incoming encoded blocks until sufficiently many blocks have been received.", "response": "def run(stream=sys.stdin.buffer):\n    \"\"\"Reads from stream, applying the LT decoding algorithm\n    to incoming encoded blocks until sufficiently many blocks\n    have been received to reconstruct the entire file.\n    \"\"\"\n    payload = decode.decode(stream)\n    sys.stdout.write(payload.decode('utf8'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _split_file(f, blocksize):\n\n    f_bytes = f.read()\n    blocks = [int.from_bytes(f_bytes[i:i+blocksize].ljust(blocksize, b'0'), sys.byteorder) \n            for i in range(0, len(f_bytes), blocksize)]\n    return len(f_bytes), blocks", "response": "Split file into blocksize chunks"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an infinite sequence of blocks to transmit to the receiver", "response": "def encoder(f, blocksize, seed=None, c=sampler.DEFAULT_C, delta=sampler.DEFAULT_DELTA):\n    \"\"\"Generates an infinite sequence of blocks to transmit\n    to the receiver\n    \"\"\"\n\n    # Generate seed if not provided\n    if seed is None:\n        seed = randint(0, 1 << 31 - 1)\n\n    # get file blocks\n    filesize, blocks = _split_file(f, blocksize)\n\n    # init stream vars\n    K = len(blocks)\n    prng = sampler.PRNG(params=(K, delta, c))\n    prng.set_seed(seed)\n\n    # block generation loop\n    while True:\n        blockseed, d, ix_samples = prng.get_src_blocks()\n        block_data = 0\n        for ix in ix_samples:\n            block_data ^= blocks[ix]\n\n        # Generate blocks of XORed data in network byte order\n        block = (filesize, blocksize, blockseed, int.to_bytes(block_data, blocksize, sys.byteorder))\n        yield pack('!III%ss'%blocksize, *block)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading block data from network into integer type", "response": "def _read_block(blocksize, stream):\n    \"\"\"Read block data from network into integer type\n    \"\"\"\n    blockdata = stream.read(blocksize)\n    return int.from_bytes(blockdata, 'big')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_blocks(stream):\n    while True:\n        header = _read_header(stream)\n        block  = _read_block(header[1], stream)\n        yield (header, block)", "response": "Generate parsed blocks from input stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new check node and edges between that node and all the source nodes it connects resolving all messages passes as a result.", "response": "def add_block(self, nodes, data):\n        \"\"\"Adds a new check node and edges between that node and all\n        source nodes it connects, resolving all message passes that\n        become possible as a result.\n        \"\"\"\n\n        # We can eliminate this source node\n        if len(nodes) == 1:\n            to_eliminate = list(self.eliminate(next(iter(nodes)), data))\n\n            # Recursively eliminate all nodes that can now be resolved\n            while len(to_eliminate):\n                other, check = to_eliminate.pop()\n                to_eliminate.extend(self.eliminate(other, check))\n        else:\n\n            # Pass messages from already-resolved source nodes\n            for node in list(nodes):\n                if node in self.eliminated:\n                    nodes.remove(node)\n                    data ^= self.eliminated[node]\n\n            # Resolve if we are left with a single non-resolved source node\n            if len(nodes) == 1:\n                return self.add_block(nodes, data)\n            else:\n\n                # Add edges for all remaining nodes to this check\n                check = CheckNode(nodes, data)\n                for node in nodes:\n                    self.checks[node].append(check)\n\n        # Are we done yet?\n        return len(self.eliminated) >= self.num_blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eliminate(self, node, data):\n\n        # Cache resolved value\n        self.eliminated[node] = data\n        others = self.checks[node]\n        del self.checks[node]\n\n        # Pass messages to all associated checks\n        for check in others:\n            check.check ^= data\n            check.src_nodes.remove(node)\n\n            # Yield all nodes that can now be resolved\n            if len(check.src_nodes) == 1:\n                yield (next(iter(check.src_nodes)), check.check)", "response": "Resolves a source node passing the message to all associated checks\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and authenticate a Google Music mobile client.", "response": "def mobileclient(username=None, device_id=None, *, token=None, locale='en_US'):\n\t\"\"\"Create and authenticate a Google Music mobile client.\n\n\t>>> import google_music\n\t>>> mc = google_music.mobileclient('username')\n\n\tParameters:\n\t\tusername (str, Optional): Your Google Music username.\n\t\t\tThis is used to store OAuth credentials for different accounts separately.\n\t\tdevice_id (str, Optional): A mobile device ID. Default: MAC address is used.\n\t\ttoken (dict, Optional): An OAuth token compatible with ``requests-oauthlib``.\n\t\tlocale (str, Optional): `ICU <http://www.localeplanet.com/icu/>`__ locale used to localize some\n\t\t\tresponses. This must be a locale supported by Android. Default: `'en_US'``.\n\n\tReturns:\n\t\tMobileClient: An authenticated :class:`~google_music.MobileClient` instance.\n\t\"\"\"\n\n\treturn MobileClient(\n\t\tusername,\n\t\tdevice_id,\n\t\ttoken=token,\n\t\tlocale=locale\n\t)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dendropy_to_dataframe(\n    tree,\n    add_node_labels=True,\n    use_uids=True):\n    \"\"\"Convert Dendropy tree to Pandas dataframe.\"\"\"\n    # Maximum distance from root.\n    tree.max_distance_from_root()\n\n    # Initialize the data object.\n    idx = []\n    data = {\n        'type': [],\n        'id': [],\n        'parent': [],\n        'length': [],\n        'label': [],\n        'distance': []}\n\n    if use_uids:\n        data['uid'] = []\n\n    # Add labels to internal nodes if set to true.\n    if add_node_labels:\n        for i, node in enumerate(tree.internal_nodes()):\n            node.label = str(i)\n\n    for node in tree.nodes():\n        # Get node type\n        if node.is_leaf():\n            type_ = 'leaf'\n            label = str(node.taxon.label).replace(' ', '_')\n        elif node.is_internal():\n            type_ = 'node'\n            label = str(node.label)\n\n        # Set node label and parent.\n        id_ = label\n        parent_node = node.parent_node\n        length = node.edge_length\n        distance = node.distance_from_root()\n\n        # Is this node a root?\n        if parent_node is None and length is None:\n            parent_label = None\n            parent_node = None\n            length = 0\n            distance = 0\n            type_ = 'root'\n\n        # Set parent node label\n        elif parent_node.is_internal():\n            parent_label = str(parent_node.label)\n\n        else:\n            raise Exception(\"Subtree is not attached to tree?\")\n\n        # Add this node to the data.\n        data['type'].append(type_)\n        data['id'].append(id_)\n        data['parent'].append(parent_label)\n        data['length'].append(length)\n        data['label'].append(label)\n        data['distance'].append(distance)\n\n        if use_uids:\n            data['uid'].append(get_random_id(10))\n\n    # Construct dataframe.\n    df = pandas.DataFrame(data)\n    return df", "response": "Convert Dendropy tree to Pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(\n    filename=None,\n    data=None,\n    schema=None,\n    add_node_labels=True,\n    use_uids=True\n    ):\n    \"\"\"Read a phylogenetic tree into a phylopandas.DataFrame.\n\n    The resulting DataFrame has the following columns:\n        - name: label for each taxa or node.\n        - id: unique id (created by phylopandas) given to each node.\n        - type: type of node (leaf, internal, or root).\n        - parent: parent id. necessary for constructing trees.\n        - length: length of branch from parent to node.\n        - distance: distance from root.\n\n    Parameters\n    ----------\n    filename: str (default is None)\n        newick file to read into DataFrame.\n\n    data: str (default is None)\n        newick string to parse and read into DataFrame.\n\n    add_node_labels: bool\n        If true, labels the internal nodes with numbers.\n\n    Returns\n    -------\n    df: phylopandas.DataFrame.\n    \"\"\"\n    if filename is not None:\n        # Use Dendropy to parse tree.\n        tree = dendropy.Tree.get(\n            path=filename,\n            schema=schema,\n            preserve_underscores=True)\n    elif data is not None:\n        tree = dendropy.Tree.get(\n            data=data,\n            schema=schema,\n            preserve_underscores=True)\n    else:\n        raise Exception('No tree given?')\n\n    df = _dendropy_to_dataframe(\n        tree, \n        add_node_labels=add_node_labels,\n        use_uids=use_uids\n    )\n    return df", "response": "Read a phylogenetic tree into a phylopandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_function(schema):\n    def func(\n        filename=None,\n        data=None,\n        add_node_labels=True,\n        use_uids=True,\n        **kwargs):\n        # Use generic write class to write data.\n        return _read(\n            filename=filename,\n            data=data,\n            schema=schema,\n            add_node_labels=add_node_labels,\n            use_uids=use_uids,\n            **kwargs\n        )\n    # Update docs\n    func.__doc__ = _read_doc_template(schema)\n    return func", "response": "Add a read method for named schema to a class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pandas_df_to_biopython_seqrecord(\n    df,\n    id_col='uid',\n    sequence_col='sequence',\n    extra_data=None,\n    alphabet=None,\n    ):\n    \"\"\"Convert pandas dataframe to biopython seqrecord for easy writing.\n\n    Parameters\n    ----------\n    df : Dataframe\n        Pandas dataframe to convert\n\n    id_col : str\n        column in dataframe to use as sequence label\n\n    sequence_col str:\n        column in dataframe to use as sequence data\n\n    extra_data : list\n        extra columns to use in sequence description line\n\n    alphabet :\n        biopython Alphabet object\n\n    Returns\n    -------\n    seq_records :\n        List of biopython seqrecords.\n    \"\"\"\n    seq_records = []\n\n    for i, row in df.iterrows():\n        # Tries getting sequence data. If a TypeError at the seqrecord\n        # creation is thrown, it is assumed that this row does not contain\n        # sequence data and therefore the row is ignored.\n        try:\n            # Get sequence\n            seq = Seq(row[sequence_col], alphabet=alphabet)\n\n            # Get id\n            id = row[id_col]\n\n            # Build a description\n            description = \"\"\n            if extra_data is not None:\n                description = \" \".join([row[key] for key in extra_data])\n\n            # Build a record\n            record = SeqRecord(\n                seq=seq,\n                id=id,\n                description=description,\n            )\n            seq_records.append(record)\n        except TypeError:\n            pass\n\n    return seq_records", "response": "Convert pandas dataframe to biopython seqrecord for easy writing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pandas_series_to_biopython_seqrecord(\n    series,\n    id_col='uid',\n    sequence_col='sequence',\n    extra_data=None,\n    alphabet=None\n    ):\n    \"\"\"Convert pandas series to biopython seqrecord for easy writing.\n\n    Parameters\n    ----------\n    series : Series\n        Pandas series to convert\n\n    id_col : str\n        column in dataframe to use as sequence label\n\n    sequence_col : str\n        column in dataframe to use as sequence data\n\n    extra_data : list\n        extra columns to use in sequence description line\n\n    Returns\n    -------\n    seq_records :\n        List of biopython seqrecords.\n    \"\"\"\n    # Get sequence\n    seq = Seq(series[sequence_col], alphabet=alphabet)\n\n    # Get id\n    id = series[id_col]\n\n    # Build a description\n    description = \"\"\n    if extra_data is not None:\n        description = \" \".join([series[key] for key in extra_data])\n\n    # Build a record\n    record = SeqRecord(\n        seq=seq,\n        id=id,\n        description=description,\n    )\n\n    seq_records = [record]\n    return seq_records", "response": "Convert pandas series to biopython seqrecord for easy writing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_method(schema):\n    def method(\n        self,\n        filename=None,\n        schema=schema,\n        id_col='uid',\n        sequence_col='sequence',\n        extra_data=None,\n        alphabet=None,\n        **kwargs):\n        # Use generic write class to write data.\n        return _write(\n            self._data,\n            filename=filename,\n            schema=schema,\n            id_col=id_col,\n            sequence_col=sequence_col,\n            extra_data=extra_data,\n            alphabet=alphabet,\n            **kwargs\n        )\n    # Update docs\n    method.__doc__ = _write_doc_template(schema)\n    return method", "response": "Add a write method to a class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a write method to a class.", "response": "def _write_function(schema):\n    \"\"\"Add a write method for named schema to a class.\n    \"\"\"\n    def func(\n        data,\n        filename=None,\n        schema=schema,\n        id_col='uid',\n        sequence_col='sequence',\n        extra_data=None,\n        alphabet=None,\n        **kwargs):\n        # Use generic write class to write data.\n        return _write(\n            data,\n            filename=filename,\n            schema=schema,\n            id_col=id_col,\n            sequence_col=sequence_col,\n            extra_data=extra_data,\n            alphabet=alphabet,\n            **kwargs\n        )\n    # Update docs\n    func.__doc__ = _write_doc_template(schema)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read(\n    filename,\n    schema,\n    seq_label='sequence',\n    alphabet=None,\n    use_uids=True,\n    **kwargs):\n    \"\"\"Use BioPython's sequence parsing module to convert any file format to\n    a Pandas DataFrame.\n\n    The resulting DataFrame has the following columns:\n        - name\n        - id\n        - description\n        - sequence\n    \"\"\"\n    # Check Alphabet if given\n    if alphabet is None:\n        alphabet = Bio.Alphabet.Alphabet()\n\n    elif alphabet in ['dna', 'rna', 'protein', 'nucleotide']:\n        alphabet = getattr(Bio.Alphabet, 'generic_{}'.format(alphabet))\n\n    else:\n        raise Exception(\n            \"The alphabet is not recognized. Must be 'dna', 'rna', \"\n            \"'nucleotide', or 'protein'.\")\n\n    kwargs.update(alphabet=alphabet)\n\n    # Prepare DataFrame fields.\n    data = {\n        'id': [],\n        seq_label: [],\n        'description': [],\n        'label': []\n    }\n    if use_uids:\n        data['uid'] = []\n\n    # Parse Fasta file.\n    for i, s in enumerate(SeqIO.parse(filename, format=schema, **kwargs)):\n        data['id'].append(s.id)\n        data[seq_label].append(str(s.seq))\n        data['description'].append(s.description)\n        data['label'].append(s.name)\n\n        if use_uids:\n            data['uid'].append(get_random_id(10))\n\n    # Port to DataFrame.\n    return pd.DataFrame(data)", "response": "Use BioPython s sequence parsing module to convert any file format to a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a read method to a class.", "response": "def _read_method(schema):\n    \"\"\"Add a write method for named schema to a class.\n    \"\"\"\n    def func(\n        self,\n        filename,\n        seq_label='sequence',\n        alphabet=None,\n        combine_on='uid',\n        use_uids=True,\n        **kwargs):\n        # Use generic write class to write data.\n        df0 = self._data\n        df1 = _read(\n            filename=filename,\n            schema=schema,\n            seq_label=seq_label,\n            alphabet=alphabet,\n            use_uids=use_uids,\n            **kwargs\n        )\n        return df0.phylo.combine(df1, on=combine_on)\n\n    # Update docs\n    func.__doc__ = _read_doc_template(schema)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a read method for named schema to a class.", "response": "def _read_function(schema):\n    \"\"\"Add a write method for named schema to a class.\n    \"\"\"\n    def func(\n        filename,\n        seq_label='sequence',\n        alphabet=None,\n        use_uids=True,\n        **kwargs):\n        # Use generic write class to write data.\n        return _read(\n            filename=filename,\n            schema=schema,\n            seq_label=seq_label,\n            alphabet=alphabet,\n            use_uids=use_uids,\n            **kwargs\n        )\n    # Update docs\n    func.__doc__ = _read_doc_template(schema)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread BLAST XML format.", "response": "def read_blast_xml(filename, **kwargs):\n    \"\"\"Read BLAST XML format.\"\"\"\n    # Read file.\n    with open(filename, 'r') as f:\n        blast_record = NCBIXML.read(f)\n\n    # Prepare DataFrame fields.\n    data = {'accession': [],\n            'hit_def': [],\n            'hit_id': [],\n            'title': [],\n            'length': [],\n            'e_value': [],\n            'sequence': []}\n\n    # Get alignments from blast result.\n    for i, s in enumerate(blast_record.alignments):\n        data['accession'] = s.accession\n        data['hit_def'] = s.hit_def\n        data['hit_id'] = s.hit_id\n        data['title'] = s.title\n        data['length'] = s.length\n        data['e_value'] = s.hsps[0].expect\n        data['sequence'] = s.hsps[0].sbjct\n\n    # Port to DataFrame.\n    return pd.DataFrame(data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pandas_df_to_dendropy_tree(\n    df,\n    taxon_col='uid',\n    taxon_annotations=[],\n    node_col='uid',\n    node_annotations=[],\n    branch_lengths=True,\n    ):\n    \"\"\"Turn a phylopandas dataframe into a dendropy tree.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tree data.\n\n    taxon_col : str (optional)\n        Column in dataframe to label the taxon. If None, the index will be used.\n\n    taxon_annotations : str\n        List of columns to annotation in the tree taxon.\n\n    node_col : str (optional)\n        Column in dataframe to label the nodes. If None, the index will be used.\n\n    node_annotations : str\n        List of columns to annotation in the node taxon.\n\n    branch_lengths : bool\n        If True, inclues branch lengths.\n    \"\"\"\n    if isinstance(taxon_col, str) is False:\n        raise Exception(\"taxon_col must be a string.\")\n\n    if isinstance(node_col, str) is False:\n        raise Exception(\"taxon_col must be a string.\")\n\n    # Construct a list of nodes from dataframe.\n    taxon_namespace = dendropy.TaxonNamespace()\n    nodes = {}\n    for idx in df.index:\n        # Get node data.\n        data = df.loc[idx]\n\n        # Get taxon for node (if leaf node).\n        taxon = None\n        if data['type'] == 'leaf':\n            taxon = dendropy.Taxon(label=data[taxon_col])\n            # Add annotations data.\n            for ann in taxon_annotations:\n                taxon.annotations.add_new(ann, data[ann])\n            taxon_namespace.add_taxon(taxon)\n\n        # Get label for node.\n        label = data[node_col]\n\n        # Get edge length.\n        edge_length = None\n        if branch_lengths is True:\n            edge_length = data['length']\n\n        # Build a node\n        n = dendropy.Node(\n            taxon=taxon,\n            label=label,\n            edge_length=edge_length\n        )\n        \n        # Add node annotations\n        for ann in node_annotations:\n            n.annotations.add_new(ann, data[ann])\n\n        nodes[idx] = n\n\n    # Build branching pattern for nodes.\n    root = None\n    for idx, node in nodes.items():\n        # Get node data.\n        data = df.loc[idx]\n\n        # Get children nodes\n        children_idx = df[df['parent'] == data['id']].index\n        children_nodes = [nodes[i] for i in children_idx]\n\n        # Set child nodes\n        nodes[idx].set_child_nodes(children_nodes)\n\n        # Check if this is root.\n        if data['parent'] is None:\n            root = nodes[idx]\n\n    # Build tree.\n    tree = dendropy.Tree(\n        seed_node=root,\n        taxon_namespace=taxon_namespace\n    )\n    return tree", "response": "Turn a phylopandas dataframe into a dendropy tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a phylopandas tree dataframe to various formats.", "response": "def _write(\n    df,\n    filename=None,\n    schema='newick',\n    taxon_col='uid',\n    taxon_annotations=[],\n    node_col='uid',\n    node_annotations=[],\n    branch_lengths=True,\n    **kwargs\n    ):\n    \"\"\"Write a phylopandas tree DataFrame to various formats.\n\n    Parameters\n    ----------\n    df : DataFrame\n        DataFrame containing tree data.\n\n    filename : str\n        filepath to write out tree. If None, will return string.\n\n    schema : str\n        tree format to write out.\n\n    taxon_col : str (optional)\n        Column in dataframe to label the taxon. If None, the index will be used.\n\n    taxon_annotations : str\n        List of columns to annotation in the tree taxon.\n\n    node_col : str (optional)\n        Column in dataframe to label the nodes. If None, the index will be used.\n\n    node_annotations : str\n        List of columns to annotation in the node taxon.\n\n    branch_lengths : bool\n        If True, inclues branch lengths.\n    \"\"\"\n    tree = _pandas_df_to_dendropy_tree(\n        df,\n        taxon_col=taxon_col,\n        taxon_annotations=taxon_annotations,\n        node_col=node_col,\n        node_annotations=node_annotations,\n        branch_lengths=branch_lengths,\n    )\n\n    # Write out format\n    print(schema)\n    if filename is not None:\n        tree.write(path=filename, schema=schema, suppress_annotations=False, **kwargs)\n    else:\n        return tree.as_string(schema=schema)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_method(schema):\n    def method(\n        self,\n        filename=None,\n        schema=schema,\n        taxon_col='uid',\n        taxon_annotations=[],\n        node_col='uid',\n        node_annotations=[],\n        branch_lengths=True,\n        **kwargs):\n        # Use generic write class to write data.\n        return _write(\n            self._data,\n            filename=filename,\n            schema=schema,\n            taxon_col=taxon_col,\n            taxon_annotations=taxon_annotations,\n            node_col=node_col,\n            node_annotations=node_annotations,\n            branch_lengths=branch_lengths,\n            **kwargs\n        )\n    # Update docs\n    method.__doc__ = _write_doc_template(schema)\n    return method", "response": "Add a write method to a class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a write method to a class.", "response": "def _write_function(schema):\n    \"\"\"Add a write method for named schema to a class.\n    \"\"\"\n    def func(\n        data,\n        filename=None,\n        schema=schema,\n        taxon_col='uid',\n        taxon_annotations=[],\n        node_col='uid',\n        node_annotations=[],\n        branch_lengths=True,\n        **kwargs):\n        # Use generic write class to write data.\n        return _write(\n            data,\n            filename=filename,\n            schema=schema,\n            taxon_col=taxon_col,\n            taxon_annotations=taxon_annotations,\n            node_col=node_col,\n            node_annotations=node_annotations,\n            branch_lengths=branch_lengths,\n            **kwargs\n        )\n    # Update docs\n    func.__doc__ = _write_doc_template(schema)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a random alpha - numerical id.", "response": "def get_random_id(length):\n    \"\"\"Generate a random, alpha-numerical id.\"\"\"\n    alphabet = string.ascii_uppercase + string.ascii_lowercase + string.digits\n    return ''.join(random.choice(alphabet) for _ in range(length))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_folder(self, folder):\n        self.folder = folder\n        self.templates.directories[0] = folder\n        self.app.root_path = folder", "response": "Sets the folder where the files to serve are located."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlaunch a development web server.", "response": "def run(self, host='0.0.0.0', port=8080):\n        \"\"\"\n        Launch a development web server.\n        \"\"\"\n        waitress.serve(self.app, host=host, port=port)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the content of a file in the specified folder", "response": "def get(self, path):\n        \"\"\"\n        Get the content of a file, indentified by its path relative to the folder configured\n        in PyGreen. If the file extension is one of the extensions that should be processed\n        through Mako, it will be processed.\n        \"\"\"\n        data = self.app.test_client().get(\"/%s\" % path).data\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a complete static version of the web site.", "response": "def gen_static(self, output_folder):\n        \"\"\"\n        Generates a complete static version of the web site. It will stored in\n        output_folder.\n        \"\"\"\n        files = []\n        for l in self.file_listers:\n            files += l()\n        for f in files:\n            _logger.info(\"generating %s\" % f)\n            content = self.get(f)\n            loc = os.path.join(output_folder, f)\n            d = os.path.dirname(loc)\n            if not os.path.exists(d):\n                os.makedirs(d)\n            with open(loc, \"wb\") as file_:\n                file_.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli(self, cmd_args=None):\n        logging.basicConfig(level=logging.INFO, format='%(message)s')\n\n        parser = argparse.ArgumentParser(description='PyGreen, micro web framework/static web site generator')\n        subparsers = parser.add_subparsers(dest='action')\n\n        parser_serve = subparsers.add_parser('serve', help='serve the web site')\n        parser_serve.add_argument('-p', '--port', type=int, default=8080, help='port to serve on')\n        parser_serve.add_argument('-f', '--folder', default=\".\", help='folder containg files to serve')\n        parser_serve.add_argument('-d', '--disable-templates', action='store_true', default=False, help='just serve static files, do not use Mako')\n        def serve():\n            if args.disable_templates:\n                self.template_exts = set([])\n            self.run(port=args.port)\n        parser_serve.set_defaults(func=serve)\n\n        parser_gen = subparsers.add_parser('gen', help='generate a static version of the site')\n        parser_gen.add_argument('output', help='folder to store the files')\n        parser_gen.add_argument('-f', '--folder', default=\".\", help='folder containing files to generate')\n        def gen():\n            self.gen_static(args.output)\n        parser_gen.set_defaults(func=gen)\n\n        args = parser.parse_args(cmd_args)\n        self.set_folder(args.folder)\n        print(parser.description)\n        print(\"\")\n        args.func()", "response": "Command line interface of PyGreen."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init(self, switch_configuration, terminal_controller, logger, piping_processor, *args):\n\n        self.switch_configuration = switch_configuration\n        self.terminal_controller = terminal_controller\n        self.logger = logger\n        self.piping_processor = piping_processor\n        self.sub_processor = None\n        self.continuing_to = None\n        self.is_done = False\n        self.replace_input = False\n        self.awaiting_keystroke = False", "response": "Initializes the object with the given switch configuration."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gccalc(lon, lat, azimuth, maxdist=None):\n    glat1 = lat * np.pi / 180.\n    glon1 = lon * np.pi / 180.\n    s = maxdist / 1.852243\n    faz = azimuth * np.pi / 180.\n \n    EPS = 0.00000000005\n    if ((np.abs(np.cos(glat1)) < EPS) and not (np.abs(np.sin(faz)) < EPS)):\n        raise CourseException(\"Only North-South courses are meaningful\")\n\n    a = 6378.137 / 1.852243\n    f = 1 / 298.257223563\n    r = 1 - f\n    tu = r * np.tan(glat1)\n    sf = np.sin(faz)\n    cf = np.cos(faz)\n    if (cf == 0):\n        b = 0.\n    else:\n        b = 2. * np.arctan2 (tu, cf)\n\n    cu = 1. / np.sqrt(1 + tu * tu)\n    su = tu * cu\n    sa = cu * sf\n    c2a = 1 - sa * sa\n    x = 1. + np.sqrt(1. + c2a * (1. / (r * r) - 1.))\n    x = (x - 2.) / x\n    c = 1. - x\n    c = (x * x / 4. + 1.) / c\n    d = (0.375 * x * x - 1.) * x\n    tu = s / (r * a * c)\n    y = tu\n    c = y + 1\n    while (np.abs (y - c) > EPS):\n        sy = np.sin(y)\n        cy = np.cos(y)\n        cz = np.cos(b + y)\n        e = 2. * cz * cz - 1.\n        c = y\n        x = e * cy\n        y = e + e - 1.\n        y = (((sy * sy * 4. - 3.) * y * cz * d / 6. + x) *\n            d / 4. - cz) * sy * d + tu\n\n    b = cu * cy * cf - su * sy\n    c = r * np.sqrt(sa * sa + b * b)\n    d = su * cy + cu * sy * cf\n    glat2 = (np.arctan2(d, c) + np.pi) % (2*np.pi) - np.pi\n    c = cu * cy - su * sy * cf\n    x = np.arctan2(sy * sf, c)\n    c = ((-3. * c2a + 4.) * f + 4.) * c2a * f / 16.\n    d = ((e * cy * c + cz) * sy * c + y) * sa\n    glon2 = ((glon1 + x - (1. - c) * d * f + np.pi) % (2*np.pi)) - np.pi\n\n    baz = (np.arctan2(sa, b) + np.pi) % (2 * np.pi)\n\n    glon2 *= 180./np.pi\n    glat2 *= 180./np.pi\n    baz *= 180./np.pi\n    return (glon2, glat2, baz)", "response": "This function calculates the base of the GC algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns lon lat tuples of a circle which matches the chosen Basemap projection", "response": "def circle(m, centerlon, centerlat, radius, *args, **kwargs):\n    \"\"\"\n    Return lon, lat tuples of a \"circle\" which matches the chosen Basemap projection\n    Takes the following arguments:\n    m = basemap instance\n    centerlon = originating lon\n    centrelat = originating lat\n    radius = radius\n\n    \"\"\"\n    glon1 = centerlon\n    glat1 = centerlat\n    X = []\n    Y = []\n    for azimuth in range(0, 360):\n        glon2, glat2, baz = _gccalc(glon1, glat1, azimuth, radius)\n        X.append(glon2)\n        Y.append(glat2)\n    X.append(X[0])\n    Y.append(Y[0])\n\n    proj_x, proj_y = m(X,Y)\n    return zip(proj_x, proj_y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a new mime type.", "response": "def register_mime(shortname, mime_types):\n    \"\"\"\n    Register a new mime type.\n    Usage example:\n        mimerender.register_mime('svg', ('application/x-svg', 'application/svg+xml',))\n    After this you can do:\n        @mimerender.mimerender(svg=render_svg)\n        def GET(...\n            ...\n    \"\"\"\n    if shortname in _MIME_TYPES:\n        raise MimeRenderException('\"%s\" has already been registered'%shortname)\n    _MIME_TYPES[shortname] = mime_types"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wsgi_wrap(app):\n    '''\n    Wraps a standard wsgi application e.g.:\n        def app(environ, start_response)\n    It intercepts the start_response callback and grabs the results from it\n    so it can return the status, headers, and body as a tuple\n    '''\n    @wraps(app)\n    def wrapped(environ, start_response):\n        status_headers = [None, None]\n        def _start_response(status, headers):\n            status_headers[:] = [status, headers]\n        body = app(environ, _start_response)\n        ret = body, status_headers[0], status_headers[1]\n        return ret\n    return wrapped", "response": "A decorator that wraps a wsgi application e. g. returns a status headers and body as a tuple\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map_exceptions(self, mapping, *args, **kwargs):\n        @self.__call__(*args, **kwargs)\n        def helper(e, status):\n            return dict(exception=e), status\n\n        def wrap(target):\n            @wraps(target)\n            def wrapper(*args, **kwargs):\n                try:\n                    return target(*args, **kwargs)\n                except BaseException as e:\n                    for klass, status in mapping:\n                        if isinstance(e, klass):\n                            return helper(e, status)\n                    raise\n            return wrapper\n        return wrap", "response": "A decorator which maps exceptions to the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relative(self):\n\t\t\n\t\tscheme = self.scheme\n\t\t\n\t\tif not scheme:\n\t\t\treturn True\n\t\t\n\t\treturn scheme.is_relative(self)", "response": "Identify if this URI is relative to some current context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to resolve a new URI given an updated URI partial or complete.", "response": "def resolve(self, uri=None, **parts):\n\t\t\"\"\"Attempt to resolve a new URI given an updated URI, partial or complete.\"\"\"\n\t\t\n\t\tif uri:\n\t\t\tresult = self.__class__(urljoin(str(self), str(uri)))\n\t\telse:\n\t\t\tresult = self.__class__(self)\n\t\t\n\t\tfor part, value in parts.items():\n\t\t\tif part not in self.__all_parts__:\n\t\t\t\traise TypeError(\"Unknown URI component: \" + part)\n\t\t\t\n\t\t\tsetattr(result, part, value)\n\t\t\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace(self, year=None, month=None, day=None):\n\n        return JalaliDate(\n            year if year else self.year,\n            month if month else self.month,\n            day if day else self.day\n        )", "response": "Replaces the given arguments on this instance and returns a new instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the corresponding date in the gregorian calendar.", "response": "def todate(self):\n        \"\"\"\n        Calculates the corresponding day in the gregorian calendar. this is the main use case of this library.\n\n        :return: Corresponding date in gregorian calendar.\n        :rtype: :py:class:`datetime.date`\n        \"\"\"\n        arr = get_gregorian_date_from_julian_day(self.tojulianday())\n        return datetime.date(int(arr[0]), int(arr[1]), int(arr[2]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timetuple(self):\n        return time.struct_time((\n            self.year,\n            self.month,\n            self.day,\n            0,\n            0,\n            0,\n            self.weekday(),\n            self.dayofyear(),\n            -1\n        ))", "response": "Returns a time. struct_time object for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef weekofyear(self, first_day_of_week=SATURDAY):\n        first_day_of_year = self.firstdayofyear()\n        days = (self - first_day_of_year).days\n        offset = first_day_of_week - first_day_of_year.weekday()\n        if offset < 0:\n            offset += 7\n\n        if days < offset:\n            return 0\n\n        return int((days - offset) / 7 + 1)", "response": "returns the week number of the year."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new instance of the class cls from the given timestamp.", "response": "def fromtimestamp(cls, timestamp, tz=None):\n        \"\"\"\n        Creates a new :py:class:`khayyam.JalaliDatetime` instance from the given posix timestamp.\n\n        If optional argument tz is :py:obj:`None` or not specified, the timestamp is converted to\n        the platform's local date and time, and the returned datetime object is naive.\n\n        Else tz must be an instance of a class :py:class:`datetime.tzinfo` subclass,\n        and the timestamp is converted to tz's time zone. In this case the result is\n        equivalent to `tz.fromutc(JalaliDatetime.utcfromtimestamp(timestamp).replace(tzinfo=tz))`.\n\n        This method may raise `ValueError`, if the timestamp is out of the range of values\n        supported by the platform C localtime() or gmtime() functions.\n        It's common for this to be restricted to years in 1970 through 2038.\n\n        Note that on non-POSIX systems that include leap seconds in their\n        notion of a timestamp, leap seconds are ignored by fromtimestamp(), and then\n        it's possible to have two timestamps differing by a second that yield\n        identical datetime objects. See also :py:class:`khayyam.JalaliDatetime.utcfromtimestamp`.\n\n        .. testsetup:: api-datetime-fromtimestamp\n\n            import khayyam\n            from khayyam import JalaliDatetime\n\n        .. doctest:: api-datetime-fromtimestamp\n\n            >>> JalaliDatetime.fromtimestamp(1313132131.21232)\n            khayyam.JalaliDatetime(1390, 5, 21, 11, 25, 31, 212320, Jomeh)\n\n        :param timestamp: float the posix timestamp, i.e 1014324234.23423423.\n        :param tz: :py:class:`datetime.tzinfo` The optional timezone to get local date & time from the given timestamp.\n        :return: The local date and time corresponding to the POSIX timestamp, such as is returned by :py:func:`time.time()`.\n        :rtype: :py:class:`khayyam.JalaliDatetime`\n        \"\"\"\n        return cls(datetime.fromtimestamp(timestamp, tz=tz))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new jalali datetime object whose date members are equal to the given date object s and whose _time members are equal to the given time object s.", "response": "def combine(cls, date, _time):\n        \"\"\"\n        Return a new jalali datetime object whose date members are equal to the given date object's, and whose _time\n        and tzinfo members are equal to the given _time object's.\n        For any datetime object d, d == datetime.combine(d.date(), d.timetz()). If date is a datetime object, its _time\n        and tzinfo members are ignored.\n\n        :param date: :py:class:`khayyam.JalaliDate` the date object to combine.\n        :param _time: :py:class:`datetime.time` the time object to combine.\n        :return: the combined jalali date & time object.\n        :rtype: :py:class:`khayyam.JalaliDatetime`\n        \"\"\"\n        if isinstance(date, (JalaliDatetime, khayyam.JalaliDate)):\n            date = date.todate()\n        return cls(datetime.combine(date, _time))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef todatetime(self):\n        arr = get_gregorian_date_from_julian_day(self.tojulianday())\n        return datetime(int(arr[0]), int(arr[1]), int(arr[2]), self.hour, self.minute, self.second, self.microsecond,\n                        self.tzinfo)", "response": "Converts the current instance to the python builtins datetime instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef date(self):\n        return khayyam.JalaliDate(self.year, self.month, self.day)", "response": "Return a new object with same year month and day."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=None):\n        \"\"\"\n        Return a :py:class:`khayyam.JalaliDatetime` instance with the same attributes, except for those attributes\n        given new values by whichever keyword arguments are specified. Note that tzinfo=None can be specified to create\n        a naive datetime from an aware datetime with no conversion of date and time data, without adjusting the date\n        the and time based tzinfo.\n\n        :param year: int\n        :param month: int\n        :param day: int\n        :param hour: int\n        :param minute: int\n        :param second: int\n        :param microsecond: int\n        :param tzinfo: :py:class:`datetime.tzinfo`\n        :rtype: :py:class:`khayyam.JalaliDatetime`\n        \"\"\"\n        year, month, day = self._validate(\n            year if year else self.year,\n            month if month else self.month,\n            day if day else self.day\n        )\n\n        result = JalaliDatetime(\n            year,\n            month,\n            day,\n            self.hour if hour is None else hour,\n            self.minute if minute is None else minute,\n            self.second if second is None else second,\n            self.microsecond if microsecond is None else microsecond,\n            tzinfo if tzinfo != self.tzinfo else self.tzinfo\n        )\n        return result", "response": "Returns a new instance of the same attributes except for those attributes\n        given new values by whichever keyword arguments are specified."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new object with new time zone information set to tz.", "response": "def astimezone(self, tz):\n        \"\"\"\n        Return a :py:class:`khayyam.JalaliDatetime` object with new :py:meth:`khayyam.JalaliDatetime.tzinfo` attribute\n        tz, adjusting the date and time data so the result is the same UTC time as self, but in *tz*\u2018s local time.\n\n        *tz* must be an instance of a :py:class:`datetime.tzinfo` subclass, and\n        its :py:meth:`datetime.tzinfo.utcoffset()` and :py:meth:`datetime.tzinfo.dst()` methods must not\n        return :py:obj:`None`. *self* must be aware (`self.tzinfo` must not be `None`, and `self.utcoffset()` must\n        not return `None`).\n\n        If `self.tzinfo` is `tz`, `self.astimezone(tz)` is equal to `self`: no adjustment of date or time data is\n        performed. Else the result is local time in time zone `tz`, representing the same UTC time as `self`:\n        after `astz = dt.astimezone(tz), astz - astz.utcoffset()` will usually have the same date and time data as\n        `dt - dt.utcoffset()`. The discussion of class :py:class:`datetime.tzinfo` explains the cases at Daylight\n        Saving Time transition boundaries where this cannot be achieved (an issue only if `tz` models both\n        standard and daylight time).\n\n        If you merely want to attach a time zone object `tz` to a datetime dt without adjustment of date and time data,\n        use `dt.replace(tzinfo=tz)`. If you merely want to remove the time zone object from an aware datetime dt\n        without conversion of date and time data, use `dt.replace(tzinfo=None)`.\n\n        Note that the default :py:meth:`datetime.tzinfo.fromutc()` method can be overridden in a\n        :py:class:`datetime.tzinfo` subclass to affect the result returned\n        by :py:meth:`khayyam.JalaliDatetime.astimezone()`. Ignoring error\n        cases, :py:meth:`khayyam.JalaliDatetime.astimezone()` acts like:\n\n        .. code-block:: python\n           :emphasize-lines: 3,5\n\n           def astimezone(self, tz):  # doctest: +SKIP\n\n               if self.tzinfo is tz:\n                   return self\n               if self.tzinfo:\n                   utc = self - self.utcoffset()\n               else:\n                   utc = self\n               return tz.fromutc(utc.replace(tzinfo=tz))\n\n\n        :param tz: :py:class:`datetime.tzinfo`\n        :rtype: :py:class:`khayyam.JalaliDatetime`\n        \"\"\"\n        if self.tzinfo is tz:\n            return self\n        if self.tzinfo:\n            utc = self - self.utcoffset()\n        else:\n            utc = self\n        return tz.fromutc(utc.replace(tzinfo=tz))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hour12(self):\n        res = self.hour\n        if res > 12:\n            res -= 12\n        elif res == 0:\n            res = 12\n        return res", "response": "Return The hour value between 1 - 12. use khayyam. JalaliDatetime. ampmascii to determineante meridiem and post meridiem. use khayyam. JalaliDatetime. ampmascii to determine post meridiem. use khayyam. jcjalaliDatetime. ampmascii to determineante meridiem and post meridiem."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the Linear Prediction Coefficients for the input signal and the given order.", "response": "def lpc_ref(signal, order):\n    \"\"\"Compute the Linear Prediction Coefficients.\n\n    Return the order + 1 LPC coefficients for the signal. c = lpc(x, k) will\n    find the k+1 coefficients of a k order linear filter:\n\n      xp[n] = -c[1] * x[n-2] - ... - c[k-1] * x[n-k-1]\n\n    Such as the sum of the squared-error e[i] = xp[i] - x[i] is minimized.\n\n    Parameters\n    ----------\n    signal: array_like\n        input signal\n    order : int\n        LPC order (the output will have order + 1 items)\n\n    Notes\n    ----\n    This is just for reference, as it is using the direct inversion of the\n    toeplitz matrix, which is really slow\"\"\"\n    if signal.ndim > 1:\n        raise ValueError(\"Array of rank > 1 not supported yet\")\n    if order > signal.size:\n        raise ValueError(\"Input signal must have a lenght >= lpc order\")\n\n    if order > 0:\n        p = order + 1\n        r = np.zeros(p, 'float32')\n        # Number of non zero values in autocorrelation one needs for p LPC\n        # coefficients\n        nx = np.min([p, signal.size])\n        x = np.correlate(signal, signal, 'full')\n        r[:nx] = x[signal.size - 1:signal.size + order]\n        phi = np.dot(sp.linalg.inv(sp.linalg.toeplitz(r[:-1])), -r[1:])\n        return np.concatenate(([1.], phi))\n    else:\n        return np.ones(1, dtype='float32')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the biased autocorrelation of x along the given axis.", "response": "def acorr_lpc(x, axis=-1):\n    \"\"\"Compute autocorrelation of x along the given axis.\n\n    This compute the biased autocorrelation estimator (divided by the size of\n    input signal)\n\n    Notes\n    -----\n        The reason why we do not use acorr directly is for speed issue.\"\"\"\n    if not np.isrealobj(x):\n        raise ValueError(\"Complex input not supported yet\")\n\n    maxlag = x.shape[axis]\n    nfft = int(2 ** nextpow2(2 * maxlag - 1))\n\n    if axis != -1:\n        x = np.swapaxes(x, -1, axis)\n    a = _acorr_last_axis(x, nfft, maxlag)\n    if axis != -1:\n        a = np.swapaxes(a, -1, axis)\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the Linear Prediction Coefficients.", "response": "def lpc(signal, order, axis=-1):\n    \"\"\"Compute the Linear Prediction Coefficients.\n\n    Return the order + 1 LPC coefficients for the signal. c = lpc(x, k) will\n    find the k+1 coefficients of a k order linear filter:\n\n      xp[n] = -c[1] * x[n-2] - ... - c[k-1] * x[n-k-1]\n\n    Such as the sum of the squared-error e[i] = xp[i] - x[i] is minimized.\n\n    Parameters\n    ----------\n    signal: array_like\n        input signal\n    order : int\n        LPC order (the output will have order + 1 items)\n\n    Returns\n    -------\n    a : array-like\n        the solution of the inversion.\n    e : array-like\n        the prediction error.\n    k : array-like\n        reflection coefficients.\n\n    Notes\n    -----\n    This uses Levinson-Durbin recursion for the autocorrelation matrix\n    inversion, and fft for the autocorrelation computation.\n\n    For small order, particularly if order << signal size, direct computation\n    of the autocorrelation is faster: use levinson and correlate in this case.\"\"\"\n    n = signal.shape[axis]\n    if order > n:\n        raise ValueError(\"Input signal must have length >= order\")\n\n    r = acorr_lpc(signal, axis)\n    return levinson_1d(r, order)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake in an explicit mapping of full paths to. wav files to have acoustic similarity computed.", "response": "def acoustic_similarity_mapping(path_mapping, analysis_function, distance_function, stop_check=None, call_back=None, multiprocessing=True):\n    \"\"\"Takes in an explicit mapping of full paths to .wav files to have\n    acoustic similarity computed.\n\n    Parameters\n    ----------\n    path_mapping : iterable of iterables\n        Explicit mapping of full paths of .wav files, in the form of a\n        list of tuples to be compared.\n\n\n    Returns\n    -------\n    dict\n        Returns a list of tuples corresponding to the `path_mapping` input,\n        with a new final element in the tuple being the similarity/distance\n        score for that mapping.\n\n    \"\"\"\n\n    num_cores = int((3 * cpu_count()) / 4)\n    segments = set()\n    for x in path_mapping:\n        segments.update(x)\n    if multiprocessing:\n        cache = generate_cache_mp(segments, analysis_function, num_cores, call_back, stop_check)\n        asim = calculate_distances_mp(path_mapping, cache, distance_function, num_cores, call_back, stop_check)\n    else:\n        cache = generate_cache_th(segments, analysis_function, num_cores, call_back, stop_check)\n        asim = calculate_distances_th(path_mapping, cache, distance_function, num_cores, call_back, stop_check)\n    return asim"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking in an explicit mapping of full paths to. wav files to have acoustic similarity computed.", "response": "def axb_mapping(path_mapping, analysis_function, distance_function, stop_check=None, call_back=None, multiprocessing=True):\n    \"\"\"Takes in an explicit mapping of full paths to .wav files to have\n    acoustic similarity computed.\n\n    Parameters\n    ----------\n    path_mapping : iterable of iterables\n        Explicit mapping of full paths of .wav files, in the form of a\n        list of tuples to be compared.\n\n\n    Returns\n    -------\n    dict\n        Returns a list of tuples corresponding to the `path_mapping` input,\n        with a new final element in the tuple being the similarity/distance\n        score for that mapping.\n\n    \"\"\"\n\n    num_cores = int((3 * cpu_count()) / 4)\n    segments = set()\n    for x in path_mapping:\n        segments.update(x)\n    if multiprocessing:\n        cache = generate_cache_mp(segments, analysis_function, num_cores, call_back, stop_check)\n        asim = calculate_axb_ratio_mp(path_mapping, cache, distance_function, num_cores, call_back, stop_check)\n    else:\n        cache = generate_cache_th(segments, analysis_function, num_cores, call_back, stop_check)\n        asim = calculate_axb_ratio_th(path_mapping, cache, distance_function, num_cores, call_back, stop_check)\n    return asim"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef acoustic_similarity_directories(directories, analysis_function, distance_function, stop_check=None, call_back=None, multiprocessing=True):\n\n    files = []\n\n    if call_back is not None:\n        call_back('Mapping directories...')\n        call_back(0, len(directories))\n        cur = 0\n    for d in directories:\n        if not os.path.isdir(d):\n            continue\n        if stop_check is not None and stop_check():\n            return\n        if call_back is not None:\n            cur += 1\n            if cur % 3 == 0:\n                call_back(cur)\n\n        files += [os.path.join(d, x) for x in os.listdir(d) if x.lower().endswith('.wav')]\n\n    if len(files) == 0:\n        raise (ConchError(\"The directories specified do not contain any wav files\"))\n\n    if call_back is not None:\n        call_back('Mapping directories...')\n        call_back(0, len(files) * len(files))\n        cur = 0\n    path_mapping = list()\n    for x in files:\n        for y in files:\n            if stop_check is not None and stop_check():\n                return\n            if call_back is not None:\n                cur += 1\n                if cur % 20 == 0:\n                    call_back(cur)\n            if not x.lower().endswith('.wav'):\n                continue\n            if not y.lower().endswith('.wav'):\n                continue\n            if x == y:\n                continue\n            path_mapping.append((x, y))\n\n    result = acoustic_similarity_mapping(path_mapping, analysis_function, distance_function, stop_check, call_back, multiprocessing)\n    return result", "response": "Analyze many directories and return a list of acoustic similarity files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the distance between two representations using Dynamic Time Warping.", "response": "def dtw_distance(rep_one, rep_two, norm=True):\n    \"\"\"Computes the distance between two representations with the same\n    number of filters using Dynamic Time Warping.\n\n    Parameters\n    ----------\n    rep_one : 2D array\n        First representation to compare. First dimension is time in frames\n        or samples and second dimension is the features.\n    rep_two : 2D array\n        Second representation to compare. First dimension is time in frames\n        or samples and second dimension is the features.\n\n    Returns\n    -------\n    float\n        Distance of dynamically time warping `rep_one` to `rep_two`.\n\n    \"\"\"\n    if not isinstance(rep_one, np.ndarray):\n        rep_one = rep_one.to_array()\n    if not isinstance(rep_two, np.ndarray):\n        rep_two = rep_two.to_array()\n    assert (rep_one.shape[1] == rep_two.shape[1])\n    distMat = generate_distance_matrix(rep_one, rep_two)\n    return regularDTW(distMat, norm=norm)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_distance_matrix(source, target, weights=None):\n    if weights is None:\n        weights = ones((source.shape[1], 1))\n    sLen = source.shape[0]\n    tLen = target.shape[0]\n    distMat = zeros((sLen, tLen))\n    for i in range(sLen):\n        for j in range(tLen):\n            distMat[i, j] = euclidean(source[i, :], target[j, :])\n    return distMat", "response": "Generates a local distance matrix for use in dynamic time warping."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing a local distance matrix to perform dynamic time warping.", "response": "def regularDTW(distMat, norm=True):\n    \"\"\"Use a local distance matrix to perform dynamic time warping.\n\n    Parameters\n    ----------\n    distMat : 2D array\n        Local distance matrix.\n\n    Returns\n    -------\n    float\n        Total unweighted distance of the optimal path through the\n        local distance matrix.\n\n    \"\"\"\n    sLen, tLen = distMat.shape\n    totalDistance = zeros((sLen, tLen))\n    totalDistance[0:sLen, 0:tLen] = distMat\n\n    minDirection = zeros((sLen, tLen))\n\n    for i in range(1, sLen):\n        totalDistance[i, 0] = totalDistance[i, 0] + totalDistance[i - 1, 0]\n\n    for j in range(1, tLen):\n        totalDistance[0, j] = totalDistance[0, j] + totalDistance[0, j - 1]\n\n    for i in range(1, sLen):\n        for j in range(1, tLen):\n            # direction,minPrevDistance = min(enumerate([totalDistance[i,j],totalDistance[i,j+1],totalDistance[i+1,j]]), key=operator.itemgetter(1))\n            # totalDistance[i+1,j+1] = totalDistance[i+1,j+1] + minPrevDistance\n            # minDirection[i,j] = direction\n            minDirection[i, j], totalDistance[i, j] = min(\n                enumerate([totalDistance[i - 1, j - 1] + 2 * totalDistance[i, j],\n                           totalDistance[i - 1, j] + totalDistance[i, j],\n                           totalDistance[i, j - 1] + totalDistance[i, j]]), key=operator.itemgetter(1))\n    if norm:\n        return totalDistance[sLen - 1, tLen - 1] / (sLen + tLen)\n    return totalDistance[sLen - 1, tLen - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter the signal x with the FIR filter described by the filter described by the overlap - add method.", "response": "def fftfilt(b, x, *n):\n    \"\"\"Filter the signal x with the FIR filter described by the\n    coefficients in b using the overlap-add method. If the FFT\n    length n is not specified, it and the overlap-add block length\n    are selected so as to minimize the computational cost of\n    the filtering operation.\"\"\"\n\n    N_x = len(x)\n    N_b = len(b)\n\n    # Determine the FFT length to use:\n    if len(n):\n\n        # Use the specified FFT length (rounded up to the nearest\n        # power of 2), provided that it is no less than the filter\n        # length:\n        n = n[0]\n        if n != int(n) or n <= 0:\n            raise ValueError('n must be a nonnegative integer')\n        if n < N_b:\n            n = N_b\n        N_fft = 2 ** nextpow2(n)\n    else:\n\n        if N_x > N_b:\n\n            # When the filter length is smaller than the signal,\n            # choose the FFT length and block size that minimize the\n            # FLOPS cost. Since the cost for a length-N FFT is\n            # (N/2)*log2(N) and the filtering operation of each block\n            # involves 2 FFT operations and N multiplications, the\n            # cost of the overlap-add method for 1 length-N block is\n            # N*(1+log2(N)). For the sake of efficiency, only FFT\n            # lengths that are powers of 2 are considered:\n            N = 2 ** np.arange(np.ceil(np.log2(N_b)), np.floor(np.log2(N_x)))\n            cost = np.ceil(N_x / (N - N_b + 1)) * N * (np.log2(N) + 1)\n            if len(cost) > 0:\n                N_fft = N[np.argmin(cost)]\n            else:\n                N_fft = 2 ** nextpow2(N_b + N_x - 1)\n\n        else:\n\n            # When the filter length is at least as long as the signal,\n            # filter the signal using a single block:\n            N_fft = 2 ** nextpow2(N_b + N_x - 1)\n\n    N_fft = int(N_fft)\n\n    # Compute the block length:\n    L = int(N_fft - N_b + 1)\n\n    # Compute the transform of the filter:\n    H = fft(b, N_fft)\n\n    y = np.zeros(N_x, np.float32)\n    i = 0\n    while i <= N_x:\n        il = np.min([i + L, N_x])\n        k = np.min([i + N_fft, N_x])\n        yt = ifft(fft(x[i:il], N_fft) * H, N_fft)  # Overlap..\n        y[i:k] = y[i:k] + yt[:k - i]  # and add\n        i += L\n    return y"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a spectrum into a cepstrum via type - III DCT.", "response": "def dct_spectrum(spec):\n    \"\"\"Convert a spectrum into a cepstrum via type-III DCT (following HTK).\n\n    Parameters\n    ----------\n    spec : array\n        Spectrum to perform a DCT on.\n\n    Returns\n    -------\n    array\n        Cepstrum of the input spectrum.\n\n    \"\"\"\n    ncep = spec.shape[0]\n    dctm = np.zeros((ncep, ncep))\n    for i in range(ncep):\n        dctm[i, :] = np.cos(i * np.arange(1, 2 * ncep, 2) / (2 * ncep) * np.pi) * np.sqrt(2 / ncep)\n    dctm *= 0.230258509299405\n    cep = np.dot(dctm, (10 * np.log10(spec + np.spacing(1))))\n    return cep"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a filterbank for a set of mel - frequency filters.", "response": "def construct_filterbank(num_filters, nfft, sr, min_freq, max_freq):\n    \"\"\"Constructs a mel-frequency filter bank.\n\n            Parameters\n            ----------\n            nfft : int\n                Number of points in the FFT.\n\n            Returns\n            -------\n            array\n                Filter bank to multiply an FFT spectrum to create a mel-frequency\n                spectrum.\n\n            \"\"\"\n\n    min_mel = freq_to_mel(min_freq)\n    max_mel = freq_to_mel(max_freq)\n    mel_points = np.linspace(min_mel, max_mel, num_filters + 2)\n    bin_freqs = mel_to_freq(mel_points)\n    # bins = round((nfft - 1) * bin_freqs / sr)\n\n    fftfreqs = np.arange(int(nfft / 2 + 1)) / nfft * sr\n\n    fbank = np.zeros((num_filters, int(nfft / 2 + 1)))\n    for i in range(num_filters):\n        fs = bin_freqs[i + np.arange(3)]\n        fs = fs[1] + (fs - fs[1])\n        loslope = (fftfreqs - fs[0]) / (fs[1] - fs[0])\n        highslope = (fs[2] - fftfreqs) / (fs[2] - fs[1])\n        fbank[i, :] = np.maximum(np.zeros(loslope.shape), np.minimum(loslope, highslope))\n    return fbank.transpose()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef windowed_iterable(self):\n        # Seek to offset\n        effective_offset = max(0,self.item_view.iterable_index)\n        for i,item in enumerate(self.iterable):\n            if i<effective_offset:\n                continue\n            elif i>=(effective_offset+self.item_view.iterable_fetch_size):\n                return\n            yield item", "response": "Yields the items in the iterable that are in the window."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes the items of the pattern. This method destroys the old items and creates and initializes the new items.", "response": "def refresh_items(self):\n        \"\"\" Refresh the items of the pattern.\n\n        This method destroys the old items and creates and initializes\n        the new items.\n\n        \"\"\"\n        old_items = self.items[:]# if self._dirty else []\n        old_iter_data = self._iter_data# if self._dirty else {}\n        iterable = self.windowed_iterable\n        pattern_nodes = self.pattern_nodes\n        new_iter_data = sortedmap()\n        new_items = []\n\n        if iterable is not None and len(pattern_nodes) > 0:\n            for loop_index, loop_item in enumerate(iterable):\n                iteration = old_iter_data.get(loop_item)\n                if iteration is not None:\n                    new_iter_data[loop_item] = iteration\n                    new_items.append(iteration)\n                    old_items.remove(iteration)\n                    continue\n                iteration = []\n                new_iter_data[loop_item] = iteration\n                new_items.append(iteration)\n                for nodes, key, f_locals in pattern_nodes:\n                    with new_scope(key, f_locals) as f_locals:\n                        f_locals['loop_index'] = loop_index\n                        f_locals['loop_item'] = loop_item\n                        for node in nodes:\n                            child = node(None)\n                            if isinstance(child, list):\n                                iteration.extend(child)\n                            else:\n                                iteration.append(child)\n        \n        # Add to old items list\n        #self.old_items.extend(old_items)\n        \n        #if self._dirty:\n        for iteration in old_items:\n            for old in iteration:\n                if not old.is_destroyed:\n                    old.destroy()\n\n        if len(new_items) > 0:\n            expanded = []\n            recursive_expand(sum(new_items, []), expanded)\n            self.parent.insert_children(self, expanded)\n\n        self.items = new_items# if self._dirty else new_items+old_items\n        self._iter_data = new_iter_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_widget(self):\n        widget = QDoubleSpinBox(self.parent_widget())\n        widget.setKeyboardTracking(False)\n        self.widget = widget", "response": "Create the underlying QDoubleSpinBox widget."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize(self):\n        super(Block, self).initialize()\n        if self.block:\n            self.block.parent.insert_children(self.block, self.children)", "response": "A reimplemented initializer.\n\n        This method will add the include objects to the parent of the\n        include and ensure that they are initialized."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the index of the resource table.", "response": "def _update_index(self):\n        \"\"\" Update the reference to the index within the table \"\"\" \n        d = self.declaration\n        self.index = self.view.model.index(d.row, d.column)\n        if self.delegate:\n            self._refresh_count += 1\n            timed_call(self._loading_interval, self._update_delegate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the delegate cell widget.", "response": "def _update_delegate(self):\n        \"\"\" Update the delegate cell widget. This is deferred so it\n        does not get called until the user is done scrolling. \n        \"\"\"\n        self._refresh_count -= 1\n        if self._refresh_count != 0:\n            return\n        try:\n            delegate = self.delegate\n            if not self.is_visible():\n                return\n            # The table destroys when it goes out of view\n            # so we always have to make a new one\n            delegate.create_widget()\n            delegate.init_widget()\n            \n            #  Set the index widget\n            self.view.widget.setIndexWidget(self.index, delegate.widget)\n        except RuntimeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_changed(self, change):\n        index = self.index\n        if index:\n            self.view.model.dataChanged.emit(index, index)", "response": "Notify the model that the data has changed in this cell!"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetHandle(self):\r\n        ''' returns an the identifier of the GUI widget.\r\n        It must be an integer\r\n        '''\r\n        win_id = self.winId()  # this returns either an int or voitptr\r\n\r\n        if \"%s\"%type(win_id) == \"<type 'PyCObject'>\":  # PySide\r\n            ### with PySide, self.winId() does not return an integer\r\n            if sys.platform == \"win32\":\r\n                ## Be careful, this hack is py27 specific\r\n                ## does not work with python31 or higher\r\n                ## since the PyCObject api was changed\r\n                import ctypes\r\n                ctypes.pythonapi.PyCObject_AsVoidPtr.restype = ctypes.c_void_p\r\n                ctypes.pythonapi.PyCObject_AsVoidPtr.argtypes = [\r\n                    ctypes.py_object]\r\n                win_id = ctypes.pythonapi.PyCObject_AsVoidPtr(win_id)\r\n        elif type(win_id) is not int:  #PyQt4 or 5\r\n            ## below integer cast may be required because self.winId() can\r\n            ## returns a sip.voitptr according to the PyQt version used\r\n            ## as well as the python version\r\n            win_id = int(win_id)\r\n        return win_id", "response": "Returns the handle of the most recent available entry in the GUI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a generator that yields the set of entities that are in the same topology.", "response": "def _map_shapes_and_ancestors(self, topoTypeA, topoTypeB, topologicalEntity):\r\n        '''\r\n        using the same method\r\n        @param topoTypeA:\r\n        @param topoTypeB:\r\n        @param topologicalEntity:\r\n        '''\r\n        topo_set = set()\r\n        _map = TopTools_IndexedDataMapOfShapeListOfShape()\r\n        topexp_MapShapesAndAncestors(self.myShape, topoTypeA, topoTypeB, _map)\r\n        results = _map.FindFromKey(topologicalEntity)\r\n        if results.IsEmpty():\r\n            yield None\r\n\r\n        topology_iterator = TopTools_ListIteratorOfListOfShape(results)\r\n        while topology_iterator.More():\r\n\r\n            topo_entity = self.topoFactory[topoTypeB](topology_iterator.Value())\r\n\r\n            # return the entity if not in set\r\n            # to assure we're not returning entities several times\r\n            if not topo_entity in topo_set:\r\n                if self.ignore_orientation:\r\n                    unique = True\r\n                    for i in topo_set:\r\n                        if i.IsSame(topo_entity):\r\n                            unique = False\r\n                            break\r\n                    if unique:\r\n                        yield topo_entity\r\n                else:\r\n                    yield topo_entity\r\n\r\n            topo_set.add(topo_entity)\r\n            topology_iterator.Next()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _number_shapes_ancestors(self, topoTypeA, topoTypeB, topologicalEntity):\r\n        '''returns the number of shape ancestors\r\n        If you want to know how many edges a faces has:\r\n        _number_shapes_ancestors(self, TopAbs_EDGE, TopAbs_FACE, edg)\r\n        will return the number of edges a faces has\r\n        @param topoTypeA:\r\n        @param topoTypeB:\r\n        @param topologicalEntity:\r\n        '''\r\n        topo_set = set()\r\n        _map = TopTools_IndexedDataMapOfShapeListOfShape()\r\n        topexp_MapShapesAndAncestors(self.myShape, topoTypeA, topoTypeB, _map)\r\n        results = _map.FindFromKey(topologicalEntity)\r\n        if results.IsEmpty():\r\n            return None\r\n        topology_iterator = TopTools_ListIteratorOfListOfShape(results)\r\n        while topology_iterator.More():\r\n            topo_set.add(topology_iterator.Value())\r\n            topology_iterator.Next()\r\n        return len(topo_set)", "response": "returns the number of shape ancestors"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_layout(self):\r\n        for child in self.children():\r\n            self.child_added(child)\r\n        self.update_shape({})", "response": "Initialize the layout of the toolkit shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_widget(self):\n        super(QtKeyEvent, self).init_widget()\n        d = self.declaration\n        widget = self.widget\n        self._keyPressEvent = widget.keyPressEvent\n        self._keyReleaseEvent = widget.keyReleaseEvent\n        self.set_enabled(d.enabled)\n        self.set_keys(d.keys)", "response": "The KeyEvent uses the parent_widget as it s widget"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_keys(self, keys):\n        codes = {}\n        for key in keys:\n            parts = [k.strip().lower() for k in key.split(\"+\")]\n            code = KEYS.get(parts[-1])\n            modifier = Qt.KeyboardModifiers()\n            if code is None:\n                raise KeyError(\"Invalid key code '{}'\".format(key))\n            if len(parts) > 1:\n                for mod in parts[:-1]:\n                    mod_code = MODIFIERS.get(mod)\n                    if mod_code is None:\n                        raise KeyError(\"Invalid key modifier '{}'\"\n                                       .format(mod_code))\n                    modifier |= mod_code\n            if code not in codes:\n                codes[code] = []\n            codes[code].append(modifier)\n        self.codes = codes", "response": "Parse all the key codes and save them in the codes attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_proxy(self, change):\n        if change['name'] == 'items':\n            self._update_visible_area()\n        \n        super(TableView, self)._update_proxy(change)", "response": "An observer which sends state change to the proxy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _teardown_features(self):\n        features = self._features\n        if not features:\n            return\n        if features & Feature.FocusTraversal:\n            self.unhook_focus_traversal()\n        if features & Feature.FocusEvents:\n            self.unhook_focus_events()\n        if features & Feature.DragEnabled:\n            self.unhook_drag()\n        if features & Feature.DropEnabled:\n            self.unhook_drop()\n\n        features = self._extra_features\n        if features & GraphicFeature.WheelEvent:\n            self.unhook_wheel()\n        if features & GraphicFeature.DrawEvent:\n            self.unhook_draw()", "response": "Teardowns the advanced widget feature handlers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles a custom tab focus request. This method is called when focus is being set on the proxy as a result of a user-implemented focus traversal handler. This can be reimplemented by subclasses as needed. Parameters ---------- reason : Qt.FocusReason The reason value for the focus request. Returns ------- result : bool True if focus was set, False otherwise.", "response": "def tab_focus_request(self, reason):\n        \"\"\" Handle a custom tab focus request.\n\n        This method is called when focus is being set on the proxy\n        as a result of a user-implemented focus traversal handler.\n        This can be reimplemented by subclasses as needed.\n\n        Parameters\n        ----------\n        reason : Qt.FocusReason\n            The reason value for the focus request.\n\n        Returns\n        -------\n        result : bool\n            True if focus was set, False otherwise.\n\n        \"\"\"\n        widget = self.focus_target()\n        if not widget.focusPolicy & Qt.TabFocus:\n            return False\n        if not widget.isEnabled():\n            return False\n        if not widget.isVisibleTo(widget.window()):\n            return False\n        widget.setFocus(reason)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall the hooks for focus events.", "response": "def hook_focus_events(self):\n        \"\"\" Install the hooks for focus events.\n\n        This method may be overridden by subclasses as needed.\n\n        \"\"\"\n        widget = self.widget\n        widget.focusInEvent = self.focusInEvent\n        widget.focusOutEvent = self.focusOutEvent"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninstall the hooks for drag operations.", "response": "def hook_drag(self):\n        \"\"\" Install the hooks for drag operations.\n\n        \"\"\"\n        widget = self.widget\n        widget.mousePressEvent = self.mousePressEvent\n        widget.mouseMoveEvent = self.mouseMoveEvent\n        widget.mouseReleaseEvent = self.mouseReleaseEvent"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unhook_drag(self):\n        widget = self.widget\n        del widget.mousePressEvent\n        del widget.mouseMoveEvent\n        del widget.mouseReleaseEvent", "response": "Remove the hooks for drag operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mousePressEvent(self, event):\n        if event.button() == Qt.LeftButton:\n            self._drag_origin = event.pos()\n        widget = self.widget\n        type(widget).mousePressEvent(widget, event)", "response": "Handle the mouse press event for a drag operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the mouse move event for a drag operation.", "response": "def mouseMoveEvent(self, event):\n        \"\"\" Handle the mouse move event for a drag operation.\n\n        \"\"\"\n        #if event.buttons() & Qt.LeftButton and self._drag_origin is not None:\n            #dist = (event.pos() - self._drag_origin).manhattanLength()\n            #if dist >= QApplication.startDragDistance():\n                #self.do_drag(event.widget())\n                #self._drag_origin = None\n                #return # Don't returns\n        widget = self.widget\n        type(widget).mouseMoveEvent(widget, event)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the mouse release event for the drag operation.", "response": "def mouseReleaseEvent(self, event):\n        \"\"\" Handle the mouse release event for the drag operation.\n\n        \"\"\"\n        if event.button() == Qt.LeftButton:\n            self._drag_origin = None\n        widget = self.widget\n        type(widget).mouseReleaseEvent(widget, event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall hooks for drop operations.", "response": "def hook_drop(self):\n        \"\"\" Install hooks for drop operations.\n\n        \"\"\"\n        widget = self.widget\n        widget.setAcceptDrops(True)\n        widget.dragEnterEvent = self.dragEnterEvent\n        widget.dragMoveEvent = self.dragMoveEvent\n        widget.dragLeaveEvent = self.dragLeaveEvent\n        widget.dropEvent = self.dropEvent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unhook_drop(self):\n        widget = self.widget\n        widget.setAcceptDrops(False)\n        del widget.dragEnterEvent\n        del widget.dragMoveEvent\n        del widget.dragLeaveEvent\n        del widget.dropEvent", "response": "Remove hooks for drop operations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming the drag operation for the current window.", "response": "def do_drag(self, widget):\n        \"\"\" Perform the drag operation for the widget.\n        \n        Parameters\n        ----------\n        widget: QWidget\n            A reference to the viewport widget.\n\n        \"\"\"\n        drag_data = self.declaration.drag_start()\n        if drag_data is None:\n            return\n        #widget = self.widget\n        qdrag = QDrag(widget)\n        qdrag.setMimeData(drag_data.mime_data.q_data())\n        if drag_data.image is not None:\n            qimg = get_cached_qimage(drag_data.image)\n            qdrag.setPixmap(QPixmap.fromImage(qimg))\n        #else:\n            #if __version_info__ < (5, ):\n                #qdrag.setPixmap(QPixmap.grabWidget(self.widget))\n            #else:\n                #qdrag.setPixmap(widget.grab())\n        if drag_data.hotspot:\n            qdrag.setHotSpot(QPoint(*drag_data.hotspot))\n        else:\n            cursor_position = widget.mapFromGlobal(QCursor.pos())\n            qdrag.setHotSpot(cursor_position)\n        default = Qt.DropAction(drag_data.default_drop_action)\n        supported = Qt.DropActions(drag_data.supported_actions)\n        qresult = qdrag.exec_(supported, default)\n        self.declaration.drag_end(drag_data, DropAction(int(qresult)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw(self, painter, options, widget):\n        self.declaration.draw(painter, options, widget)", "response": "Handle the draw event for the\n        widget."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the shared widget action for this widget.", "response": "def get_action(self, create=False):\n        \"\"\" Get the shared widget action for this widget.\n\n        This API is used to support widgets in tool bars and menus.\n\n        Parameters\n        ----------\n        create : bool, optional\n            Whether to create the action if it doesn't already exist.\n            The default is False.\n\n        Returns\n        -------\n        result : QWidgetAction or None\n            The cached widget action or None, depending on arguments.\n\n        \"\"\"\n        action = self._widget_action\n        if action is None and create:\n            action = self._widget_action = QWidgetAction(None)\n            action.setDefaultWidget(self.widget)\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mousePressEvent(self, event):\n        self.declaration.mouse_press_event(event)\n        super(QtGraphicsView, self).mousePressEvent(event)", "response": "Handle mouse press events for a drag operation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mouseMoveEvent(self, event):\n        self.declaration.mouse_move_event(event)\n        super(QtGraphicsView, self).mouseMoveEvent(event)", "response": "Handle the mouse move event for a drag operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mouseReleaseEvent(self, event):\n        self.declaration.mouse_release_event(event)\n        super(QtGraphicsView, self).mouseReleaseEvent(event)", "response": "Handle the mouse release event for the drag operation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the viewport widget.", "response": "def set_renderer(self, renderer):\n        \"\"\" Set the viewport widget. \n        \n        \"\"\"\n        viewport = None\n        if renderer == 'opengl':\n            from enaml.qt.QtWidgets import QOpenGLWidget\n            viewport = QOpenGLWidget()\n        elif renderer == 'default':\n            try:\n                from enaml.qt.QtWidgets import QOpenGLWidget\n                viewport = QOpenGLWidget()\n            except ImportError as e:\n                warnings.warn(\n                    \"QOpenGLWidget could not be imported: {}\".format(e))\n        self.widget.setViewport(viewport)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_selection_changed(self):\n        d = self.declaration\n        selection = self.scene.selectedItems()\n        self._guards |= 0x01\n        try:\n            d.selected_items = [item.ref().declaration for item in selection\n                                if item.ref()]\n        finally:\n            self._guards &= ~0x01", "response": "Callback invoked one the selection has changed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_background(self, background):\n        scene = self.scene\n        scene.setBackgroundBrush(QColor.fromRgba(background.argb))", "response": "Set the background color of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscales the zoom but keep in the min and max zoom bounds.", "response": "def scale_view(self, x, y):\n        \"\"\" Scale the zoom but keep in in the min and max zoom bounds.\n\n        \"\"\"\n        d = self.declaration\n        factor = self.widget.transform().scale(x, y).mapRect(\n            QRectF(0, 0, 1, 1)).width()\n        if (d.min_zoom > factor > d.max_zoom):\n            return\n        self.widget.scale(x, y)\n        return factor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef destroy(self):\n        self._teardown_features()\n        focus_registry.unregister(self.widget)\n        widget = self.widget\n        if widget is not None:\n            del self.widget\n        super(QtGraphicsItem, self).destroy()\n        # If a QWidgetAction was created for this widget, then it has\n        # taken ownership of the widget and the widget will be deleted\n        # when the QWidgetAction is garbage collected. This means the\n        # superclass destroy() method must run before the reference to\n        # the QWidgetAction is dropped.\n        del self._widget_action", "response": "Destroys the underlying QWidget object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parent_widget(self):\n        parent = self.parent()\n        if parent is not None and isinstance(parent, QtGraphicsItem):\n            return parent.widget", "response": "Reimplemented to only return GraphicsItems"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_layout(self):\n        self.widget = QGraphicsProxyWidget(self.parent_widget())\n        widget = self.widget\n        for item in self.child_widgets():\n            widget.setWidget(item)\n            break\n        super(QtGraphicsWidget, self).init_widget()\n        super(QtGraphicsWidget, self).init_layout()", "response": "Create the widget in the layout pass after the child widget has been created and intialized."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dequeue_update(self,change):\r\n        self._update_count -=1\r\n        if self._update_count !=0:\r\n            return\r\n        self.update_shape(change)", "response": "Dequeue an update for the specified duplicate."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the toolkit shape for the object.", "response": "def create_shape(self):\r\n        \"\"\" Create the toolkit shape for the proxy object.\r\n\r\n        This method is called during the top-down pass, just before the\r\n        'init_shape()' method is called. This method should create the\r\n        toolkit widget and assign it to the 'widget' attribute.\r\n\r\n        \"\"\"\r\n        d = self.declaration\r\n        if d.shape1 and d.shape2:\r\n            self.shape = self._do_operation(d.shape1, d.shape2)\r\n        else:\r\n            self.shape = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures the widget is shown.", "response": "def show(self):\n        \"\"\" Ensure the widget is shown.\n        Calling this method will also set the widget visibility to True.\n        \"\"\"\n        self.visible = True\n        if self.proxy_is_active:\n            self.proxy.ensure_visible()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hide(self):\n        self.visible = False\n        if self.proxy_is_active:\n            self.proxy.ensure_hidden()", "response": "Ensure the widget is hidden."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the items at the given position", "response": "def get_item_at(self, *args, **kwargs):\n        \"\"\" Return the items at the given position \"\"\"\n        return self.proxy.get_item_at(coerce_point(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncenters on the given item or point.", "response": "def center_on(self, item):\n        \"\"\" Center on the given item or point. \"\"\"\n        if not isinstance(item, GraphicsItem):\n            item = coerce_point(item)\n        self.proxy.center_on(item)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _custom_token_stream(self):\n    token_stream = default_make_token_stream(self)\n    for processor in _token_stream_processors:\n        token_stream = processor(token_stream)\n    return token_stream", "response": "A wrapper for the BaseEnamlLexer s make_token_stream which allows the token stream to be customized by adding the token_stream_processors list to the token_stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_enamldef_def_to_func(token_stream):\n    in_enamldef = False\n    depth = 0\n    for tok in token_stream:\n        if tok.type == 'ENAMLDEF':\n            in_enamldef = True\n        elif tok.type == 'INDENT':\n            depth += 1\n        elif in_enamldef and tok.type == 'DEF':\n            # Since functions are not allowed on the RHS we can\n            # transform the token type to a NAME so it's picked up by the\n            # parser as a decl_funcdef instead of funcdef\n            tok.type = 'NAME'\n            tok.value = 'func'\n        elif tok.type == 'DEDENT':\n            depth -= 1\n            if depth == 0:\n                in_enamldef = False\n        yield tok", "response": "A token stream processor which processes all enaml declarative functions \n    to allow using def instead of func."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef child_added(self, child):\n        super(AbstractItemView, self).child_added(child)\n        self.get_member('_items').reset(self)", "response": "Reset the item cache when a child is added"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the item cache when a child is removed", "response": "def child_removed(self, child):\n        \"\"\" Reset the item cache when a child is removed \"\"\"\n        super(AbstractItemView, self).child_removed(child)\n        self.get_member('_items').reset(self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef child_added(self, child):\n        super(AbstractWidgetItemGroup, self).child_added(child)\n        self.get_member('_items').reset(self)", "response": "Reset the item cache when a child is added"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef child_removed(self, child):\n        super(AbstractWidgetItemGroup, self).child_removed(child)\n        self.get_member('_items').reset(self)", "response": "Reset the item cache when a child is removed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_proxy(self, change):\n        if change['name'] in ['row', 'column']:\n            super(AbstractWidgetItem, self)._update_proxy(change)\n        else:\n            self.proxy.data_changed(change)", "response": "An observer which sends state change to the proxy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_signal(self):\r\n        signal.signal(signal.SIGINT, lambda sig, frame: self.exit(-2))\r\n        # need a timer, so that QApplication doesn't block until a real\r\n        # Qt event fires (can require mouse movement)\r\n        # timer trick from http://stackoverflow.com/q/4938723/938949\r\n        timer = QtCore.QTimer()\r\n        # Let the interpreter run each 200 ms:\r\n        timer.timeout.connect(lambda: None)\r\n        timer.start(200)\r\n        # hold onto ref, so the timer doesn't get cleaned up\r\n        self._sigint_timer = timer", "response": "allow clean shutdown on sigint"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_position(self, change):\r\n        if change['type']!='update':\r\n            return\r\n        pt = gp_Pnt(self.x,self.y,self.z)\r\n        if not pt.IsEqual(self.position,self.tolerance):\r\n            self.position = pt", "response": "Update the position of the object based on the current values of the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_xyz(self, change):\r\n        self.x,self.y,self.z = self.position.X(),self.position.Y(),self.position.Z()", "response": "Update x y z values based on the current position"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_state(self, change):\r\n        self._block_updates = True\r\n        try:\r\n            self.position = self.axis.Location()\r\n            self.direction = self.axis.Direction()\r\n        finally:\r\n            self._block_updates = False", "response": "Update position and direction of the current object with the new values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrefreshes the multi - axis of the plot.", "response": "def _refresh_multi_axis(self):\n        \"\"\" If linked axis' are used, setup and link them \"\"\"\n        d = self.declaration\n        \n        #: Create a separate viewbox\n        self.viewbox = pg.ViewBox()\n        \n        #: If this is the first nested plot, use the parent right axis\n        _plots = [c for c in self.parent().children() if isinstance(c,AbstractQtPlotItem)]\n        i = _plots.index(self)\n        if i==0:\n            self.axis = self.widget.getAxis('right') \n            self.widget.showAxis('right')\n        else:\n            self.axis = pg.AxisItem('right')\n            self.axis.setZValue(-10000)\n            \n            #: Add new axis to scene\n            self.widget.layout.addItem(self.axis,2,i+2)\n        \n        #: Link x axis to the parent axis\n        self.viewbox.setXLink(self.widget.vb)\n        \n        #: Link y axis to the view\n        self.axis.linkToView(self.viewbox)\n        \n        #: Set axis label\n        self.axis.setLabel(d.label_right)\n        \n        #: Add Viewbox to parent scene\n        self.parent().parent_widget().scene().addItem(self.viewbox)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_range_x(self,val):\n        d = self.declaration\n        if d.auto_range[0]:\n            return\n        self.widget.setXRange(*val,padding=0)", "response": "Set visible range of x data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets visible range of y data.", "response": "def set_range_y(self,val):\n        \"\"\" Set visible range of y data. \n        \n        Note: Padding must be 0 or it will create an infinite loop\n        \n        \"\"\"\n        d = self.declaration\n        if d.auto_range[1]:\n            return\n        self.widget.setYRange(*val,padding=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of child TreeViewColumns including this item as the first column", "response": "def _get_columns(self):\n        \"\"\" List of child TreeViewColumns including \n            this item as the first column\n        \"\"\"\n        return [self] + [c for c in self.children\n                         if isinstance(c, TreeViewColumn)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the row and column numbers of child items.", "response": "def _update_rows(self):\n        \"\"\" Update the row and column numbers of child items. \"\"\"\n        for row, item in enumerate(self._items):\n            item.row = row  # Row is the Parent item\n            item.column = 0\n        \n        for column, item in enumerate(self._columns):\n            item.row = self.row  # Row is the Parent item\n            item.column = column"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setDeclaration(self, declaration):\n        assert isinstance(declaration.proxy, ProxyAbstractItemView), \\\n            \"The model declaration must be a QtAbstractItemView subclass. \" \\\n            \"Got {]\".format(declaration)\n\n        self.declaration = declaration", "response": "Set the declaration this model will use for rendering the headers. \n "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the data for the item at the given index", "response": "def data(self, index, role):\n        \"\"\" Retrieve the data for the item at the given index\n        \"\"\"\n        item = self.itemAt(index)\n        if not item:\n            return None\n        d = item.declaration\n        if role == Qt.DisplayRole:\n            return d.text\n        elif role == Qt.ToolTipRole:\n            return d.tool_tip\n        elif role == Qt.CheckStateRole and d.checkable:\n            return d.checked and Qt.Checked or Qt.Unchecked\n        elif role == Qt.DecorationRole and d.icon:\n            return get_cached_qicon(d.icon)\n        elif role == Qt.EditRole and d.editable:\n            return d.text\n        elif role == Qt.StatusTipRole:\n            return d.status_tip\n        elif role == Qt.TextAlignmentRole:\n            h, v = d.text_alignment\n            return TEXT_H_ALIGNMENTS[h] | TEXT_V_ALIGNMENTS[v]\n        elif role == Qt.ForegroundRole and d.foreground:\n            return get_cached_qcolor(d.foreground)\n        elif role == Qt.BackgroundRole and d.background:\n            return get_cached_qcolor(d.background)\n        #elif role == Qt.SizeHintRole and d.minimum_size:\n        #    return d.minimum_size\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setData(self, index, value, role=Qt.EditRole):\n        item = self.itemAt(index)\n        if not item:\n            return False\n        d = item.declaration\n        if role == Qt.CheckStateRole:\n            checked = value == Qt.Checked\n            if checked != d.checked:\n                d.checked = checked\n                d.toggled(checked)\n            return True\n        elif role == Qt.EditRole:\n            if value != d.text:\n                d.text = value\n            return True\n        return super(QAbstractAtomItemModel, self).setData(index, value, role)", "response": "Set the data for the item at the given index to the given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data for the item with the given orientation and role.", "response": "def headerData(self, index, orientation, role):\n        \"\"\" QHeaderView respects the following item data roles: \n                TextAlignmentRole, \n                DisplayRole, \n                FontRole, \n                DecorationRole, \n                ForegroundRole, \n                BackgroundRole.\n        \"\"\"\n        d = self.declaration\n        if orientation == Qt.Horizontal and role == Qt.DisplayRole:\n            try:\n                return d.horizontal_headers[index] \\\n                    if d.horizontal_headers else index\n            except IndexError:\n                return index\n        elif orientation == Qt.Vertical and role == Qt.DisplayRole:\n            try:\n                return d.vertical_headers[index] \\\n                    if d.vertical_headers else index\n            except IndexError:\n                return index\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the signals for the current locale.", "response": "def init_signals(self):\n        \"\"\" Connect signals \"\"\"\n        self.widget.activated.connect(self.on_item_activated)\n        self.widget.clicked.connect(self.on_item_clicked)\n        self.widget.doubleClicked.connect(self.on_item_double_clicked)\n        self.widget.entered.connect(self.on_item_entered)\n        self.widget.pressed.connect(self.on_item_pressed)\n        self.widget.customContextMenuRequested.connect(\n            self.on_custom_context_menu_requested)\n        self.selection_model = self.widget.selectionModel()\n        self.selection_model.selectionChanged.connect(\n            self.on_selection_changed)\n        self.widget.horizontalScrollBar().valueChanged.connect(\n            self.on_horizontal_scrollbar_moved)\n        self.widget.verticalScrollBar().valueChanged.connect(\n            self.on_vertical_scrollbar_moved)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_items(self, items):\n        self._pending_view_refreshes +=1\n        timed_call(self._pending_timeout, self._refresh_layout)", "response": "Set the items of the view."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _refresh_layout(self):\n        self._pending_view_refreshes -= 1\n        if self._pending_view_refreshes == 0:\n            try:\n                self.model.layoutChanged.emit()\n                self.on_layout_refreshed()\n            except RuntimeError:\n                # View can be destroyed before we get here\n                return\n            self._refresh_sizes()", "response": "Refreshes the layout of the items in the items table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index(self, row, column, parent):\n        item = parent.internalPointer()\n        #: If the parent is None\n        d = self.declaration if item is None else item.declaration\n        if row < len(d._items):\n            proxy = d._items[row].proxy\n            assert isinstance(proxy, QtTreeViewItem), \\\n                \"Invalid item {}\".format(proxy)\n        else:\n            proxy = d.proxy\n        return self.createIndex(row, column, proxy)", "response": "Creates an index for the given row column and parent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _default_view(self):\n        parent = self.parent()\n        if isinstance(parent, QtTreeView):\n            return parent\n        return parent.view", "response": "Return the default view of the item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(cls, five9, filters):\n        return cls._name_search(five9.configuration.getWebConnectors, filters)", "response": "Search for a record on the remote and return the results."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(cls, five9, external_id):\n        results = cls.search(five9, {cls.__uid_field__: external_id})\n        if not results:\n            return None\n        return results[0]", "response": "Return a record singleton for the ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the current memory record with the given data dictionary.", "response": "def update(self, data):\n        \"\"\"Update the current memory record with the given data dict.\n\n        Args:\n            data (dict): Data dictionary to update the record attributes with.\n        \"\"\"\n        for key, value in data.items():\n            setattr(self, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls the remote method with data and optionally refresh the record.", "response": "def _call_and_serialize(cls, method, data, refresh=False):\n        \"\"\"Call the remote method with data, and optionally refresh.\n\n        Args:\n            method (callable): The method on the Authenticated Five9 object\n                that should be called.\n            data (dict): A data dictionary that will be passed as the first\n                and only position argument to ``method``.\n            refresh (bool, optional): Set to ``True`` to get the record data\n                from Five9 before returning the record.\n\n        Returns:\n            BaseModel: The newly created record. If ``refresh`` is ``True``,\n                this will be fetched from Five9. Otherwise, it's the data\n                record that was sent to the server.\n        \"\"\"\n        method(data)\n        if refresh:\n            return cls.read(method.__self__, data[cls.__uid_field__])\n        else:\n            return cls.deserialize(cls._get_non_empty_dict(data))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_name_filters(cls, filters):\n        filters = filters.get(cls.__uid_field__)\n        if not filters:\n            filters = '.*'\n        elif not isinstance(filters, string_types):\n            filters = r'(%s)' % ('|'.join(filters))\n        return filters", "response": "Return a regex filter for the UID column only."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_non_empty_dict(cls, mapping):\n        res = {}\n        for key, value in mapping.items():\n            if hasattr(value, 'items'):\n                value = cls._get_non_empty_dict(value)\n            elif isinstance(value, list):\n                value = cls._get_non_empty_list(value)\n            if value not in [[], {}, None]:\n                res[key] = value\n        return res", "response": "Return the mapping without any None values ( recursive )."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of the input excluding all None values.", "response": "def _get_non_empty_list(cls, iter):\n        \"\"\"Return a list of the input, excluding all ``None`` values.\"\"\"\n        res = []\n        for value in iter:\n            if hasattr(value, 'items'):\n                value = cls._get_non_empty_dict(value) or None\n            if value is not None:\n                res.append(value)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _name_search(cls, method, filters):\n        filters = cls._get_name_filters(filters)\n        return [\n            cls.deserialize(cls._zeep_to_dict(row)) for row in method(filters)\n        ]", "response": "Helper for search methods that use name filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a zeep object to a dictionary.", "response": "def _zeep_to_dict(cls, obj):\n        \"\"\"Convert a zeep object to a dictionary.\"\"\"\n        res = serialize_object(obj)\n        res = cls._get_non_empty_dict(res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __check_field(self, key):\n        if not self._props.get(key):\n            raise KeyError(\n                'The field \"%s\" does not exist on \"%s\"' % (\n                    key, self.__class__.__name__,\n                ),\n            )", "response": "Raises a KeyError if the field doesn t exist on the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an authenticated connection for use open new if required.", "response": "def supervisor(self):\n        \"\"\"Return an authenticated connection for use, open new if required.\n\n        Returns:\n            SupervisorWebService: New or existing session with the Five9\n            Statistics API.\n        \"\"\"\n        supervisor = self._cached_client('supervisor')\n        if not self._api_supervisor_session:\n            self._api_supervisor_session = self.__create_supervisor_session(\n                supervisor,\n            )\n        return supervisor"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_mapping(record, keys):\n\n        ordered = OrderedDict()\n        field_mappings = []\n\n        for key, value in record.items():\n            ordered[key] = value\n            field_mappings.append({\n                'columnNumber': len(ordered),  # Five9 is not zero indexed.\n                'fieldName': key,\n                'key': key in keys,\n            })\n\n        return {\n            'field_mappings': field_mappings,\n            'data': ordered,\n            'fields': list(ordered.values()),\n        }", "response": "Create a field mapping for use in API updates and creates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_response(fields, records):\n        data = [i['values']['data'] for i in records]\n        return [\n            {fields[idx]: row for idx, row in enumerate(d)}\n            for d in data\n        ]", "response": "Parse an API response into usable objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_criteria(cls, query):\n        criteria = []\n        for name, value in query.items():\n            if isinstance(value, list):\n                for inner_value in value:\n                    criteria += cls.create_criteria({name: inner_value})\n            else:\n                criteria.append({\n                    'criteria': {\n                        'field': name,\n                        'value': value,\n                    },\n                })\n        return criteria or None", "response": "Create a criteria from a dictionary containing a query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an authenticated SOAP client.", "response": "def _get_authenticated_client(self, wsdl):\n        \"\"\"Return an authenticated SOAP client.\n\n        Returns:\n            zeep.Client: Authenticated API client.\n        \"\"\"\n        return zeep.Client(\n            wsdl % quote(self.username),\n            transport=zeep.Transport(\n                session=self._get_authenticated_session(),\n            ),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an authenticated requests session.", "response": "def _get_authenticated_session(self):\n        \"\"\"Return an authenticated requests session.\n\n        Returns:\n            requests.Session: Authenticated session for use.\n        \"\"\"\n        session = requests.Session()\n        session.auth = self.auth\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __create_supervisor_session(self, supervisor):\n        session_params = {\n            'forceLogoutSession': self.force_logout_session,\n            'rollingPeriod': self.rolling_period,\n            'statisticsRange': self.statistics_range,\n            'shiftStart': self.__to_milliseconds(\n                self.shift_start_hour,\n            ),\n            'timeZone': self.__to_milliseconds(\n                self.time_zone_offset,\n            ),\n        }\n        supervisor.setSessionParameters(session_params)\n        return session_params", "response": "Create a new supervisor session on the supervisor service."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse this to decorate methods that expect a model.", "response": "def model(method):\n        \"\"\"Use this to decorate methods that expect a model.\"\"\"\n        def wrapper(self, *args, **kwargs):\n            if self.__model__ is None:\n                raise ValidationError(\n                    'You cannot perform CRUD operations without selecting a '\n                    'model first.',\n                )\n            return method(self, *args, **kwargs)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse this to decorate methods that expect a record set.", "response": "def recordset(method):\n        \"\"\"Use this to decorate methods that expect a record set.\"\"\"\n        def wrapper(self, *args, **kwargs):\n            if self.__records__ is None:\n                raise ValidationError(\n                    'There are no records in the set.',\n                )\n            return method(self, *args, **kwargs)\n        return Api.model(wrapper)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the data on the remote optionally refreshing.", "response": "def create(self, data, refresh=False):\n        \"\"\"Create the data on the remote, optionally refreshing.\"\"\"\n        self.__model__.create(self.__five9__, data)\n        if refresh:\n            return self.read(data[self.__model__.__name__])\n        else:\n            return self.new(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new(self, data):\n        data = self.__model__._get_non_empty_dict(data)\n        return self.__class__(\n            self.__five9__,\n            self.__model__,\n            records=[self.__model__.deserialize(data)],\n        )", "response": "Create a new memory record but do not create on the remote."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, filters):\n        records = self.__model__.search(self.__five9__, filters)\n        return self.__class__(\n            self.__five9__, self.__model__, records,\n        )", "response": "Search Five9 given a filter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(cls, five9, data, refresh=False):\n        return cls._call_and_serialize(\n            five9.configuration.createDisposition, data, refresh,\n        )", "response": "Create a record on Five9."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for a record on the remote and return the results.", "response": "def search(cls, five9, filters):\n        \"\"\"Search for a record on the remote and return the results.\n\n        Args:\n            five9 (five9.Five9): The authenticated Five9 remote.\n            filters (dict): A dictionary of search parameters, keyed by the\n                name of the field to search. This should conform to the\n                schema defined in :func:`five9.Five9.create_criteria`.\n\n        Returns:\n            list[BaseModel]: A list of records representing the result.\n        \"\"\"\n        return cls._name_search(five9.configuration.getDispositions, filters)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authentication_required(meth):\n\n    def check(cls, *args, **kwargs):\n        if cls.authenticated:\n            return meth(cls, *args, **kwargs)\n        raise Error(\"Authentication required\")\n\n    return check", "response": "Simple class method decorator that checks if the client is currently connected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a block of size bytes from the server.", "response": "def __read_block(self, size):\n        \"\"\"Read a block of 'size' bytes from the server.\n\n        An internal buffer is used to read data from the server. If\n        enough data is available from it, we return that data.\n\n        Eventually, we try to grab the missing part from the server\n        for Client.read_timeout seconds. If no data can be\n        retrieved, it is considered as a fatal error and an 'Error'\n        exception is raised.\n\n        :param size: number of bytes to read\n        :rtype: string\n        :returns: the read block (can be empty)\n        \"\"\"\n        buf = b\"\"\n        if len(self.__read_buffer):\n            limit = (\n                size if size <= len(self.__read_buffer) else\n                len(self.__read_buffer)\n            )\n            buf = self.__read_buffer[:limit]\n            self.__read_buffer = self.__read_buffer[limit:]\n            size -= limit\n        if not size:\n            return buf\n        try:\n            buf += self.sock.recv(size)\n        except (socket.timeout, ssl.SSLError):\n            raise Error(\"Failed to read %d bytes from the server\" % size)\n        self.__dprint(buf)\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __read_line(self):\n        ret = b\"\"\n        while True:\n            try:\n                pos = self.__read_buffer.index(CRLF)\n                ret = self.__read_buffer[:pos]\n                self.__read_buffer = self.__read_buffer[pos + len(CRLF):]\n                break\n            except ValueError:\n                pass\n            try:\n                nval = self.sock.recv(self.read_size)\n                self.__dprint(nval)\n                if not len(nval):\n                    break\n                self.__read_buffer += nval\n            except (socket.timeout, ssl.SSLError):\n                raise Error(\"Failed to read data from the server\")\n\n        if len(ret):\n            m = self.__size_expr.match(ret)\n            if m:\n                raise Literal(int(m.group(1)))\n\n            m = self.__respcode_expr.match(ret)\n            if m:\n                if m.group(1) == b\"BYE\":\n                    raise Error(\"Connection closed by server\")\n                if m.group(1) == b\"NO\":\n                    self.__parse_error(m.group(2))\n                raise Response(m.group(1), m.group(2))\n        return ret", "response": "Read one line from the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __read_response(self, nblines=-1):\n        resp, code, data = (b\"\", None, None)\n        cpt = 0\n        while True:\n            try:\n                line = self.__read_line()\n            except Response as inst:\n                code = inst.code\n                data = inst.data\n                break\n            except Literal as inst:\n                resp += self.__read_block(inst.value)\n                if not resp.endswith(CRLF):\n                    resp += self.__read_line() + CRLF\n                continue\n            if not len(line):\n                continue\n            resp += line + CRLF\n            cpt += 1\n            if nblines != -1 and cpt == nblines:\n                break\n\n        return (code, data, resp)", "response": "Read a response from the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __prepare_args(self, args):\n        ret = []\n        for a in args:\n            if isinstance(a, six.binary_type):\n                if self.__size_expr.match(a):\n                    ret += [a]\n                else:\n                    ret += [b'\"' + a + b'\"']\n                continue\n            ret += [bytes(str(a).encode(\"utf-8\"))]\n        return ret", "response": "Format command arguments before sending them."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a command to the server.", "response": "def __send_command(\n            self, name, args=None, withcontent=False, extralines=None,\n            nblines=-1):\n        \"\"\"Send a command to the server.\n\n        If args is not empty, we concatenate the given command with\n        the content of this list. If extralines is not empty, they are\n        sent one by one to the server. (CLRF are automatically\n        appended to them)\n\n        We wait for a response just after the command has been sent.\n\n        :param name: the command to sent\n        :param args: a list of arguments for this command\n        :param withcontent: tells the function to return the server's response\n                            or not\n        :param extralines: a list of extra lines to sent after the command\n        :param nblines: the number of response lines to read (all by default)\n\n        :returns: a tuple of the form (code, data[, response])\n\n        \"\"\"\n        tosend = name.encode(\"utf-8\")\n        if args:\n            tosend += b\" \" + b\" \".join(self.__prepare_args(args))\n        self.__dprint(b\"Command: \" + tosend)\n        self.sock.sendall(tosend + CRLF)\n        if extralines:\n            for l in extralines:\n                self.sock.sendall(l + CRLF)\n        code, data, content = self.__read_response(nblines)\n\n        if isinstance(code, six.binary_type):\n            code = code.decode(\"utf-8\")\n        if isinstance(data, six.binary_type):\n            data = data.decode(\"utf-8\")\n\n        if withcontent:\n            return (code, data, content)\n        return (code, data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an error received from the server.", "response": "def __parse_error(self, text):\n        \"\"\"Parse an error received from the server.\n\n        if text corresponds to a size indication, we grab the\n        remaining content from the server.\n\n        Otherwise, we try to match an error of the form \\(\\w+\\)?\\s*\".+\"\n\n        On succes, the two public members errcode and errmsg are\n        filled with the parsing results.\n\n        :param text: the response to parse\n        \"\"\"\n        m = self.__size_expr.match(text)\n        if m is not None:\n            self.errcode = b\"\"\n            self.errmsg = self.__read_block(int(m.group(1)) + 2)\n            return\n\n        m = self.__error_expr.match(text)\n        if m is None:\n            raise Error(\"Bad error message\")\n        if m.group(1) is not None:\n            self.errcode = m.group(1).strip(b\"()\")\n        else:\n            self.errcode = b\"\"\n        self.errmsg = m.group(2).strip(b'\"')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _digest_md5_authentication(self, login, password, authz_id=\"\"):\n        code, data, challenge = \\\n            self.__send_command(\"AUTHENTICATE\", [b\"DIGEST-MD5\"],\n                                withcontent=True, nblines=1)\n        dmd5 = DigestMD5(challenge, \"sieve/%s\" % self.srvaddr)\n\n        code, data, challenge = self.__send_command(\n            '\"%s\"' % dmd5.response(login, password, authz_id),\n            withcontent=True, nblines=1\n        )\n        if not challenge:\n            return False\n        if not dmd5.check_last_challenge(login, password, challenge):\n            self.errmsg = \"Bad challenge received from server\"\n            return False\n        code, data = self.__send_command('\"\"')\n        if code == \"OK\":\n            return True\n        return False", "response": "SASL DIGEST - MD5 authentication"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate the user with the specified authentication mechanism.", "response": "def __authenticate(self, login, password, authz_id=b\"\", authmech=None):\n        \"\"\"AUTHENTICATE command\n\n        Actually, it is just a wrapper to the real commands (one by\n        mechanism). We try all supported mechanisms (from the\n        strongest to the weakest) until we find one supported by the\n        server.\n\n        Then we try to authenticate (only once).\n\n        :param login: username\n        :param password: clear password\n        :param authz_id: authorization ID\n        :param authmech: prefered authentication mechanism\n        :return: True on success, False otherwise\n        \"\"\"\n        if \"SASL\" not in self.__capabilities:\n            raise Error(\"SASL not supported by the server\")\n        srv_mechanisms = self.get_sasl_mechanisms()\n\n        if authmech is None or authmech not in SUPPORTED_AUTH_MECHS:\n            mech_list = SUPPORTED_AUTH_MECHS\n        else:\n            mech_list = [authmech]\n\n        for mech in mech_list:\n            if mech not in srv_mechanisms:\n                continue\n            mech = mech.lower().replace(\"-\", \"_\")\n            auth_method = getattr(self, \"_%s_authentication\" % mech)\n            if auth_method(login, password, authz_id):\n                self.authenticated = True\n                return True\n            return False\n\n        self.errmsg = b\"No suitable mechanism found\"\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __starttls(self, keyfile=None, certfile=None):\n        if not self.has_tls_support():\n            raise Error(\"STARTTLS not supported by the server\")\n        code, data = self.__send_command(\"STARTTLS\")\n        if code != \"OK\":\n            return False\n        try:\n            nsock = ssl.wrap_socket(self.sock, keyfile, certfile)\n        except ssl.SSLError as e:\n            raise Error(\"SSL error: %s\" % str(e))\n        self.sock = nsock\n        self.__capabilities = {}\n        self.__get_capabilities()\n        return True", "response": "This method is used to start TLS on the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sieve_capabilities(self):\n        if isinstance(self.__capabilities[\"SIEVE\"], six.string_types):\n            self.__capabilities[\"SIEVE\"] = self.__capabilities[\"SIEVE\"].split()\n        return self.__capabilities[\"SIEVE\"]", "response": "Returns the SIEVE extensions supported by the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(\n            self, login, password, authz_id=b\"\", starttls=False,\n            authmech=None):\n        \"\"\"Establish a connection with the server.\n\n        This function must be used. It read the server capabilities\n        and wraps calls to STARTTLS and AUTHENTICATE commands.\n\n        :param login: username\n        :param password: clear password\n        :param starttls: use a TLS connection or not\n        :param authmech: prefered authenticate mechanism\n        :rtype: boolean\n        \"\"\"\n        try:\n            self.sock = socket.create_connection((self.srvaddr, self.srvport))\n            self.sock.settimeout(Client.read_timeout)\n        except socket.error as msg:\n            raise Error(\"Connection to server failed: %s\" % str(msg))\n\n        if not self.__get_capabilities():\n            raise Error(\"Failed to read capabilities from server\")\n        if starttls and not self.__starttls():\n            return False\n        if self.__authenticate(login, password, authz_id, authmech):\n            return True\n        return False", "response": "Establish a connection with the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef capability(self):\n        code, data, capabilities = (\n            self.__send_command(\"CAPABILITY\", withcontent=True))\n        if code == \"OK\":\n            return capabilities\n        return None", "response": "Ask server capabilities.\n\n        See MANAGESIEVE specifications, section 2.4 This command does\n        not affect capabilities recorded by this client.\n\n        :rtype: string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasking for available space.", "response": "def havespace(self, scriptname, scriptsize):\n        \"\"\"Ask for available space.\n\n        See MANAGESIEVE specifications, section 2.5\n\n        :param scriptname: script's name\n        :param scriptsize: script's size\n        :rtype: boolean\n        \"\"\"\n        code, data = self.__send_command(\n            \"HAVESPACE\", [scriptname.encode(\"utf-8\"), scriptsize])\n        if code == \"OK\":\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist available scripts. See MANAGESIEVE specifications, section 2.7 :returns: a 2-uple (active script, [script1, ...])", "response": "def listscripts(self):\n        \"\"\"List available scripts.\n\n        See MANAGESIEVE specifications, section 2.7\n\n        :returns: a 2-uple (active script, [script1, ...])\n        \"\"\"\n        code, data, listing = self.__send_command(\n            \"LISTSCRIPTS\", withcontent=True)\n        if code == \"NO\":\n            return None\n        ret = []\n        active_script = None\n        for l in listing.splitlines():\n            if self.__size_expr.match(l):\n                continue\n            m = re.match(br'\"([^\"]+)\"\\s*(.+)', l)\n            if m is None:\n                ret += [l.strip(b'\"').decode(\"utf-8\")]\n                continue\n            script = m.group(1).decode(\"utf-8\")\n            if self.__active_expr.match(m.group(2)):\n                active_script = script\n                continue\n            ret += [script]\n        self.__dprint(ret)\n        return (active_script, ret)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getscript(self, name):\n        code, data, content = self.__send_command(\n            \"GETSCRIPT\", [name.encode(\"utf-8\")], withcontent=True)\n        if code == \"OK\":\n            lines = content.splitlines()\n            if self.__size_expr.match(lines[0]) is not None:\n                lines = lines[1:]\n            return u\"\\n\".join([line.decode(\"utf-8\") for line in lines])\n        return None", "response": "Download a script from the server"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload a script to the server See MANAGESIEVE specifications, section 2.6 :param name: script's name :param content: script's content :rtype: boolean", "response": "def putscript(self, name, content):\n        \"\"\"Upload a script to the server\n\n        See MANAGESIEVE specifications, section 2.6\n\n        :param name: script's name\n        :param content: script's content\n        :rtype: boolean\n        \"\"\"\n        content = tools.to_bytes(content)\n        content = tools.to_bytes(\"{%d+}\" % len(content)) + CRLF + content\n        code, data = (\n            self.__send_command(\"PUTSCRIPT\", [name.encode(\"utf-8\"), content]))\n        if code == \"OK\":\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deletescript(self, name):\n        code, data = self.__send_command(\n            \"DELETESCRIPT\", [name.encode(\"utf-8\")])\n        if code == \"OK\":\n            return True\n        return False", "response": "Delete a script from the server"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef renamescript(self, oldname, newname):\n        if \"VERSION\" in self.__capabilities:\n            code, data = self.__send_command(\n                \"RENAMESCRIPT\",\n                [oldname.encode(\"utf-8\"), newname.encode(\"utf-8\")])\n            if code == \"OK\":\n                return True\n            return False\n\n        (active_script, scripts) = self.listscripts()\n        condition = (\n            oldname != active_script and\n            (scripts is None or oldname not in scripts)\n        )\n        if condition:\n            self.errmsg = b\"Old script does not exist\"\n            return False\n        if newname in scripts:\n            self.errmsg = b\"New script already exists\"\n            return False\n        oldscript = self.getscript(oldname)\n        if oldscript is None:\n            return False\n        if not self.putscript(newname, oldscript):\n            return False\n        if active_script == oldname:\n            if not self.setactive(newname):\n                return False\n        if not self.deletescript(oldname):\n            return False\n        return True", "response": "Rename a script on the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setactive(self, scriptname):\n        code, data = self.__send_command(\n            \"SETACTIVE\", [scriptname.encode(\"utf-8\")])\n        if code == \"OK\":\n            return True\n        return False", "response": "Define the active script in the current active script."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef checkscript(self, content):\n        if \"VERSION\" not in self.__capabilities:\n            raise NotImplementedError(\n                \"server does not support CHECKSCRIPT command\")\n        content = tools.to_bytes(content)\n        content = tools.to_bytes(\"{%d+}\" % len(content)) + CRLF + content\n        code, data = self.__send_command(\"CHECKSCRIPT\", [content])\n        if code == \"OK\":\n            return True\n        return False", "response": "Check whether a script is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nanalyses some data in text and yield a tuple of the last two - tuple containing its name and parsed value.", "response": "def scan(self, text):\n        \"\"\"Analyse some data\n\n        Analyse the passed content. Each time a token is recognized, a\n        2-uple containing its name and parsed value is raised (via\n        yield).\n\n        On error, a ParseError exception is raised.\n\n        :param text: a binary string containing the data to parse\n        \"\"\"\n        self.pos = 0\n        self.text = text\n        while self.pos < len(text):\n            m = self.wsregexp.match(text, self.pos)\n            if m is not None:\n                self.pos = m.end()\n                continue\n\n            m = self.regexp.match(text, self.pos)\n            if m is None:\n                raise ParseError(\"unknown token %s\" % text[self.pos:])\n\n            self.pos = m.end()\n            yield (m.lastgroup, m.group(m.lastgroup))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the parser s internal variables to an initial state. Useful when creating a new parser.", "response": "def __reset_parser(self):\n        \"\"\"Reset parser's internal variables\n\n        Restore the parser to an initial state. Useful when creating a\n        new parser or reusing an existing one.\n        \"\"\"\n        self.result = []\n        self.hash_comments = []\n\n        self.__cstate = None\n        self.__curcommand = None\n        self.__curstringlist = None\n        self.__expected = None\n        self.__opened_blocks = 0\n        RequireCommand.loaded_extensions = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __up(self, onlyrecord=False):\n        if self.__curcommand.must_follow is not None:\n            if not self.__curcommand.parent:\n                prevcmd = self.result[-1] if len(self.result) else None\n            else:\n                prevcmd = self.__curcommand.parent.children[-2] \\\n                    if len(self.__curcommand.parent.children) >= 2 else None\n            if prevcmd is None or prevcmd.name not in self.__curcommand.must_follow:\n                raise ParseError(\"the %s command must follow an %s command\" %\n                                 (self.__curcommand.name,\n                                  \" or \".join(self.__curcommand.must_follow)))\n\n        if not self.__curcommand.parent:\n            # collect current amount of hash comments for later\n            # parsing into names and desciptions\n            self.__curcommand.hash_comments = self.hash_comments\n            self.hash_comments = []\n            self.result += [self.__curcommand]\n\n        if not onlyrecord:\n            while self.__curcommand:\n                self.__curcommand = self.__curcommand.parent\n                # Make sure to detect all done tests (including 'not' ones).\n                condition = (\n                    self.__curcommand and\n                    self.__curcommand.get_type() == \"test\" and\n                    self.__curcommand.iscomplete()\n                )\n                if condition:\n                    continue\n                break", "response": "Return to the current command s parent and if onlyrecord is True then only record the new command into its parent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __check_command_completion(self, testsemicolon=True):\n        if not self.__curcommand.iscomplete():\n            return True\n\n        ctype = self.__curcommand.get_type()\n        if ctype == \"action\" or \\\n                (ctype == \"control\" and\n                 not self.__curcommand.accept_children):\n            if testsemicolon:\n                self.__set_expected(\"semicolon\")\n            return True\n\n        while self.__curcommand.parent:\n            cmd = self.__curcommand\n            self.__curcommand = self.__curcommand.parent\n            if self.__curcommand.get_type() in [\"control\", \"test\"]:\n                if self.__curcommand.iscomplete():\n                    if self.__curcommand.get_type() == \"control\":\n                        break\n                    continue\n                if not self.__curcommand.check_next_arg(\"test\", cmd, add=False):\n                    return False\n                if not self.__curcommand.iscomplete():\n                    if self.__curcommand.variable_args_nb:\n                        self.__set_expected(\"comma\", \"right_parenthesis\")\n                    break\n\n        return True", "response": "Checks if the command is complete and if it is not then returns False."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __command(self, ttype, tvalue):\n        if self.__cstate is None:\n            if ttype == \"right_cbracket\":\n                self.__up()\n                self.__opened_blocks -= 1\n                self.__cstate = None\n                return True\n\n            if ttype != \"identifier\":\n                return False\n            command = get_command_instance(\n                tvalue.decode(\"ascii\"), self.__curcommand)\n            if command.get_type() == \"test\":\n                raise ParseError(\n                    \"%s may not appear as a first command\" % command.name)\n            if command.get_type() == \"control\" and command.accept_children \\\n               and command.has_arguments():\n                self.__set_expected(\"identifier\")\n            if self.__curcommand is not None:\n                if not self.__curcommand.addchild(command):\n                    raise ParseError(\"%s unexpected after a %s\" %\n                                     (tvalue, self.__curcommand.name))\n            self.__curcommand = command\n            self.__cstate = self.__arguments\n\n            return True\n\n        if self.__cstate(ttype, tvalue):\n            return True\n\n        if ttype == \"left_cbracket\":\n            self.__opened_blocks += 1\n            self.__cstate = None\n            return True\n\n        if ttype == \"semicolon\":\n            self.__cstate = None\n            if not self.__check_command_completion(testsemicolon=False):\n                return False\n            self.__curcommand.complete_cb()\n            self.__up()\n            return True\n        return False", "response": "This method is called by the parser when a command is detected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(self, text):\n        if isinstance(text, text_type):\n            text = text.encode(\"utf-8\")\n\n        self.__reset_parser()\n        try:\n            for ttype, tvalue in self.lexer.scan(text):\n                if ttype == \"hash_comment\":\n                    self.hash_comments += [tvalue.strip()]\n                    continue\n                if ttype == \"bracket_comment\":\n                    continue\n                if self.__expected is not None:\n                    if ttype not in self.__expected:\n                        if self.lexer.pos < len(text):\n                            msg = (\n                                \"%s found while %s expected near '%s'\"\n                                % (ttype, \"|\".join(self.__expected),\n                                   text[self.lexer.pos])\n                            )\n                        else:\n                            msg = (\n                                \"%s found while %s expected at end of file\"\n                                  % (ttype, \"|\".join(self.__expected))\n                            )\n                        raise ParseError(msg)\n                    self.__expected = None\n\n                if not self.__command(ttype, tvalue):\n                    msg = (\n                        \"unexpected token '%s' found near '%s'\"\n                        % (tvalue.decode(), text.decode()[self.lexer.pos])\n                    )\n                    raise ParseError(msg)\n            if self.__opened_blocks:\n                self.__set_expected(\"right_cbracket\")\n            if self.__expected is not None:\n                raise ParseError(\"end of script reached while %s expected\" %\n                                 \"|\".join(self.__expected))\n\n        except (ParseError, CommandError) as e:\n            self.error = \"line %d: %s\" % (self.lexer.curlineno(), str(e))\n            return False\n        return True", "response": "The entry point of the parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_file(self, name):\n        with open(name, \"rb\") as fp:\n            return self.parse(fp.read())", "response": "Parse the content of a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(self, target=sys.stdout):\n        for r in self.result:\n            r.dump(target=target)", "response": "Dump the parsing tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_commands(cmds):\n    if not isinstance(cmds, Iterable):\n        cmds = [cmds]\n\n    for command in cmds:\n        if command.__name__.endswith(\"Command\"):\n            globals()[command.__name__] = command", "response": "Adds one or more commands to the module namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to guess and create the appropriate command instance based on the command name.", "response": "def get_command_instance(name, parent=None, checkexists=True):\n    \"\"\"Try to guess and create the appropriate command instance\n\n    Given a command name (encountered by the parser), construct the\n    associated class name and, if known, return a new instance.\n\n    If the command is not known or has not been loaded using require,\n    an UnknownCommand exception is raised.\n\n    :param name: the command's name\n    :param parent: the eventual parent command\n    :return: a new class instance\n    \"\"\"\n    cname = \"%sCommand\" % name.lower().capitalize()\n    gl = globals()\n    condition = (\n        cname not in gl or\n        (checkexists and gl[cname].extension and\n         gl[cname].extension not in RequireCommand.loaded_extensions)\n    )\n    if condition:\n        raise UnknownCommand(name)\n    return gl[cname](parent)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tosieve(self, indentlevel=0, target=sys.stdout):\n        self.__print(self.name, indentlevel, nocr=True, target=target)\n        if self.has_arguments():\n            for arg in self.args_definition:\n                if not arg[\"name\"] in self.arguments:\n                    continue\n                target.write(\" \")\n                value = self.arguments[arg[\"name\"]]\n                atype = arg[\"type\"]\n                if \"tag\" in atype:\n                    target.write(value)\n                    if arg[\"name\"] in self.extra_arguments:\n                        value = self.extra_arguments[arg[\"name\"]]\n                        atype = arg[\"extra_arg\"][\"type\"]\n                        target.write(\" \")\n                    else:\n                        continue\n\n                if type(value) == list:\n                    if self.__get_arg_type(arg[\"name\"]) == [\"testlist\"]:\n                        target.write(\"(\")\n                        for t in value:\n                            t.tosieve(target=target)\n                            if value.index(t) != len(value) - 1:\n                                target.write(\", \")\n                        target.write(\")\")\n                    else:\n                        target.write(\n                            \"[{}]\".format(\", \".join(\n                                ['\"%s\"' % v.strip('\"') for v in value])\n                            )\n                        )\n                    continue\n                if isinstance(value, Command):\n                    value.tosieve(indentlevel, target=target)\n                    continue\n\n                if \"string\" in atype:\n                    target.write(value)\n                    if not value.startswith('\"') and not value.startswith(\"[\"):\n                        target.write(\"\\n\")\n                else:\n                    target.write(value)\n\n        if not self.accept_children:\n            if self.get_type() != \"test\":\n                target.write(\";\\n\")\n            return\n        if self.get_type() != \"control\":\n            return\n        target.write(\" {\\n\")\n        for ch in self.children:\n            ch.tosieve(indentlevel + 4, target=target)\n        self.__print(\"}\", indentlevel, target=target)", "response": "Generate the sieve syntax corresponding to this command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay the command ongoing and pending commands.", "response": "def dump(self, indentlevel=0, target=sys.stdout):\n        \"\"\"Display the command\n\n        Pretty printing of this command and its eventual arguments and\n        children. (recursively)\n\n        :param indentlevel: integer that indicates indentation level to apply\n        \"\"\"\n        self.__print(self, indentlevel, target=target)\n        indentlevel += 4\n        if self.has_arguments():\n            for arg in self.args_definition:\n                if not arg[\"name\"] in self.arguments:\n                    continue\n                value = self.arguments[arg[\"name\"]]\n                atype = arg[\"type\"]\n                if \"tag\" in atype:\n                    self.__print(str(value), indentlevel, target=target)\n                    if arg[\"name\"] in self.extra_arguments:\n                        value = self.extra_arguments[arg[\"name\"]]\n                        atype = arg[\"extra_arg\"][\"type\"]\n                    else:\n                        continue\n                if type(value) == list:\n                    if self.__get_arg_type(arg[\"name\"]) == [\"testlist\"]:\n                        for t in value:\n                            t.dump(indentlevel, target)\n                    else:\n                        self.__print(\"[\" + (\",\".join(value)) + \"]\",\n                                     indentlevel, target=target)\n                    continue\n                if isinstance(value, Command):\n                    value.dump(indentlevel, target)\n                    continue\n                self.__print(str(value), indentlevel, target=target)\n        for ch in self.children:\n            ch.dump(indentlevel, target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addchild(self, child):\n        if not self.accept_children:\n            return False\n        self.children += [child]\n        return True", "response": "Add a new child to the command sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iscomplete(self, atype=None, avalue=None):\n        if self.variable_args_nb:\n            return False\n        if self.required_args == -1:\n            self.required_args = 0\n            for arg in self.args_definition:\n                if arg.get(\"required\", False):\n                    self.required_args += 1\n        return (\n            (not self.curarg or\n             \"extra_arg\" not in self.curarg or\n             (\"valid_for\" in self.curarg[\"extra_arg\"] and\n              atype and atype in self.curarg[\"extra_arg\"][\"type\"] and\n              avalue not in self.curarg[\"extra_arg\"][\"valid_for\"])) and\n            (self.rargs_cnt == self.required_args)\n        )", "response": "Check if the command is complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if value is allowed for arg COOKIE", "response": "def __is_valid_value_for_arg(self, arg, value, check_extension=True):\n        \"\"\"Check if value is allowed for arg\n\n        Some commands only allow a limited set of values. The method\n        always returns True for methods that do not provide such a\n        set.\n\n        :param arg: the argument's name\n        :param value: the value to check\n        :param check_extension: check if value requires an extension\n        :return: True on succes, False otherwise\n        \"\"\"\n        if \"values\" not in arg and \"extension_values\" not in arg:\n            return True\n        if \"values\" in arg and value.lower() in arg[\"values\"]:\n            return True\n        if \"extension_values\" in arg:\n            extension = arg[\"extension_values\"].get(value.lower())\n            if extension:\n                condition = (\n                    check_extension and\n                    extension not in RequireCommand.loaded_extensions\n                )\n                if condition:\n                    raise ExtensionNotLoaded(extension)\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if type is valid based on input type list string is special because it can be used for stringlist", "response": "def __is_valid_type(self, typ, typlist):\n        \"\"\" Check if type is valid based on input type list\n            \"string\" is special because it can be used for stringlist\n\n        :param typ: the type to check\n        :param typlist: the list of type to check\n        :return: True on success, False otherwise\n        \"\"\"\n        typ_is_str = typ == \"string\"\n        str_list_in_typlist = \"stringlist\" in typlist\n\n        return typ in typlist or (typ_is_str and str_list_in_typlist)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef args_as_tuple(self):\n        value = self.arguments[\"header-names\"]\n        if isinstance(value, list):\n            value = \"[{}]\".format(\n                \",\".join('\"{}\"'.format(item) for item in value))\n        if not value.startswith(\"[\"):\n            return ('exists', value.strip('\"'))\n        return (\"exists\", ) + tuple(tools.to_list(value))", "response": "Return the arguments as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn arguments as a list.", "response": "def args_as_tuple(self):\n        \"\"\"Return arguments as a list.\"\"\"\n        if \",\" in self.arguments[\"header-names\"]:\n            result = tuple(tools.to_list(self.arguments[\"header-names\"]))\n        else:\n            result = (self.arguments[\"header-names\"].strip('\"'),)\n        result = result + (self.arguments[\"match-type\"],)\n        if \",\" in self.arguments[\"key-list\"]:\n            result = result + tuple(\n                tools.to_list(self.arguments[\"key-list\"], unquote=False))\n        else:\n            result = result + (self.arguments[\"key-list\"].strip('\"'),)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef args_as_tuple(self):\n        result = (\"body\", )\n        result = result + (\n            self.arguments[\"body-transform\"], self.arguments[\"match-type\"])\n        if self.arguments[\"key-list\"].startswith(\"[\"):\n            result = result + tuple(\n                tools.to_list(self.arguments[\"key-list\"]))\n        else:\n            result = result + (self.arguments[\"key-list\"].strip('\"'),)\n        return result", "response": "Return arguments as a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndeal with optional stringlist before a required one.", "response": "def reassign_arguments(self):\n        \"\"\"Deal with optional stringlist before a required one.\"\"\"\n        condition = (\n            \"variable-list\" in self.arguments and\n            \"list-of-flags\" not in self.arguments\n        )\n        if condition:\n            self.arguments[\"list-of-flags\"] = (\n                self.arguments.pop(\"variable-list\"))\n            self.rargs_cnt = 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn arguments as a list.", "response": "def args_as_tuple(self):\n        \"\"\"Return arguments as a list.\"\"\"\n        result = (\"currentdate\", )\n        result += (\n            \":zone\",\n            self.extra_arguments[\"zone\"].strip('\"'),\n            self.arguments[\"match-type\"],\n        )\n        if self.arguments[\"match-type\"] in [\":count\", \":value\"]:\n            result += (self.extra_arguments[\"match-type\"].strip('\"'), )\n        result += (self.arguments[\"date-part\"].strip('\"'), )\n        value = self.arguments[\"key-list\"]\n        if isinstance(value, list):\n            # FIXME\n            value = \"[{}]\".format(\n                \",\".join('\"{}\"'.format(item) for item in value))\n        if value.startswith(\"[\"):\n            result = result + tuple(tools.to_list(value))\n        else:\n            result = result + (value.strip('\"'),)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_bytes(s, encoding=\"utf-8\"):\n    if isinstance(s, six.binary_type):\n        return s\n    if six.PY3:\n        return bytes(s, encoding)\n    return s.encode(encoding)", "response": "Convert a string to bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string representing a list to real list.", "response": "def to_list(stringlist, unquote=True):\n    \"\"\"Convert a string representing a list to real list.\"\"\"\n    stringlist = stringlist[1:-1]\n    return [\n        string.strip('\"') if unquote else string\n        for string in stringlist.split(\",\")\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef monkeypatch_i18n():\n    import jinja2.ext\n    from puente.ext import PuenteI18nExtension\n\n    jinja2.ext.InternationalizationExtension = PuenteI18nExtension\n    jinja2.ext.i18n = PuenteI18nExtension", "response": "Monkeypatch the i18n translation for the trans blocks in the Jinja2 template."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_keywords(additional_keywords=None):\n    # Shallow copy\n    keywords = dict(BABEL_KEYWORDS)\n\n    keywords.update({\n        '_lazy': None,\n        'gettext_lazy': None,\n        'ugettext_lazy': None,\n        'gettext_noop': None,\n        'ugettext_noop': None,\n\n        'ngettext_lazy': (1, 2),\n        'ungettext_lazy': (1, 2),\n\n        'npgettext': ((1, 'c'), 2, 3),\n        'pgettext_lazy': ((1, 'c'), 2),\n        'npgettext_lazy': ((1, 'c'), 2, 3),\n    })\n\n    # Add specified keywords\n    if additional_keywords:\n        for key, val in additional_keywords.items():\n            keywords[key] = val\n    return keywords", "response": "Generates gettext keywords list for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collapse_whitespace(message):\n    return u' '.join(map(lambda s: s.strip(),\n                         filter(None, message.strip().splitlines())))", "response": "Collapses consecutive whitespace into a single space"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_options_map():\n    try:\n        return settings.PUENTE['JINJA2_CONFIG']\n    except KeyError:\n        pass\n\n    # If using Django 1.8+, we can skim the TEMPLATES for a backend that we\n    # know about and extract the settings from that.\n    for tmpl_config in getattr(settings, 'TEMPLATES', []):\n        try:\n            backend = tmpl_config['BACKEND']\n        except KeyError:\n            continue\n\n        if backend == 'django_jinja.backend.Jinja2':\n            extensions = tmpl_config.get('OPTIONS', {}).get('extensions', [])\n            return {\n                '**.*': {\n                    'extensions': ','.join(extensions),\n                    'silent': 'False',\n                }\n            }\n\n    # If this is Django 1.7 and Jingo, try to grab extensions from\n    # JINJA_CONFIG.\n    if getattr(settings, 'JINJA_CONFIG'):\n        jinja_config = settings.JINJA_CONFIG\n        if callable(jinja_config):\n            jinja_config = jinja_config()\n        return {\n            '**.*': {\n                'extensions': ','.join(jinja_config['extensions']),\n                'silent': 'False',\n            }\n        }\n\n    raise CommandError(\n        'No valid jinja2 config found in settings. See configuration '\n        'documentation.'\n    )", "response": "Generate an options_map that can be used to extract the contents of a Jinja2 environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_command(outputdir, domain_methods, text_domain, keywords,\n                    comment_tags, base_dir, project, version,\n                    msgid_bugs_address):\n    \"\"\"Extracts strings into .pot files\n\n    :arg domain: domains to generate strings for or 'all' for all domains\n    :arg outputdir: output dir for .pot files; usually\n        locale/templates/LC_MESSAGES/\n    :arg domain_methods: DOMAIN_METHODS setting\n    :arg text_domain: TEXT_DOMAIN settings\n    :arg keywords: KEYWORDS setting\n    :arg comment_tags: COMMENT_TAGS setting\n    :arg base_dir: BASE_DIR setting\n    :arg project: PROJECT setting\n    :arg version: VERSION setting\n    :arg msgid_bugs_address: MSGID_BUGS_ADDRESS setting\n\n    \"\"\"\n    # Must monkeypatch first to fix i18n extensions stomping issues!\n    monkeypatch_i18n()\n\n    # Create the outputdir if it doesn't exist\n    outputdir = os.path.abspath(outputdir)\n    if not os.path.isdir(outputdir):\n        print('Creating output dir %s ...' % outputdir)\n        os.makedirs(outputdir)\n\n    domains = domain_methods.keys()\n\n    def callback(filename, method, options):\n        if method != 'ignore':\n            print('  %s' % filename)\n\n    # Extract string for each domain\n    for domain in domains:\n        print('Extracting all strings in domain %s...' % domain)\n\n        methods = domain_methods[domain]\n\n        catalog = Catalog(\n            header_comment='',\n            project=project,\n            version=version,\n            msgid_bugs_address=msgid_bugs_address,\n            charset='utf-8',\n        )\n        extracted = extract_from_dir(\n            base_dir,\n            method_map=methods,\n            options_map=generate_options_map(),\n            keywords=keywords,\n            comment_tags=comment_tags,\n            callback=callback,\n        )\n\n        for filename, lineno, msg, cmts, ctxt in extracted:\n            catalog.add(msg, None, [(filename, lineno)], auto_comments=cmts,\n                        context=ctxt)\n\n        with open(os.path.join(outputdir, '%s.pot' % domain), 'wb') as fp:\n            write_po(fp, catalog, width=80)\n\n    print('Done')", "response": "Extracts strings into. pot files for a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_command(create, backup, base_dir, domain_methods, languages):\n    locale_dir = os.path.join(base_dir, 'locale')\n\n    # Verify existence of msginit and msgmerge\n    if not call(['which', 'msginit'], stdout=PIPE) == 0:\n        raise CommandError('You do not have gettext installed.')\n\n    if not call(['which', 'msgmerge'], stdout=PIPE) == 0:\n        raise CommandError('You do not have gettext installed.')\n\n    if languages and isinstance(languages[0], (tuple, list)):\n        # Django's LANGUAGES setting takes a value like:\n        #\n        # LANGUAGES = (\n        #    ('de', _('German')),\n        #    ('en', _('English')),\n        # )\n        #\n        # but we only want the language codes, so we pull the first\n        # part from all the tuples.\n        languages = [lang[0] for lang in languages]\n\n    if create:\n        for lang in languages:\n            d = os.path.join(locale_dir, lang.replace('-', '_'),\n                             'LC_MESSAGES')\n            if not os.path.exists(d):\n                os.makedirs(d)\n\n    domains = domain_methods.keys()\n    for domain in domains:\n        print('Merging %s strings to each locale...' % domain)\n        domain_pot = os.path.join(locale_dir, 'templates', 'LC_MESSAGES',\n                                  '%s.pot' % domain)\n        if not os.path.isfile(domain_pot):\n            raise CommandError('Can not find %s.pot' % domain)\n\n        for locale in os.listdir(locale_dir):\n            if ((not os.path.isdir(os.path.join(locale_dir, locale)) or\n                 locale.startswith('.') or\n                 locale == 'templates')):\n                continue\n\n            domain_po = os.path.join(locale_dir, locale, 'LC_MESSAGES',\n                                     '%s.po' % domain)\n\n            if not os.path.isfile(domain_po):\n                print(' Can not find (%s).  Creating...' % domain_po)\n                p1 = Popen([\n                    'msginit',\n                    '--no-translator',\n                    '--locale=%s' % locale,\n                    '--input=%s' % domain_pot,\n                    '--output-file=%s' % domain_po,\n                    '--width=200'\n                ])\n                p1.communicate()\n\n            print('Merging %s.po for %s' % (domain, locale))\n            with open(domain_pot) as domain_pot_file:\n                if locale == 'en_US':\n                    # Create an English translation catalog, then merge\n                    with TemporaryFile('w+t') as enmerged:\n                        p2 = Popen(['msgen', '-'], stdin=domain_pot_file,\n                                   stdout=enmerged)\n                        p2.communicate()\n                        _msgmerge(domain_po, enmerged, backup)\n                else:\n                    _msgmerge(domain_po, domain_pot_file, backup)\n\n        print('Domain %s finished' % domain)\n\n    print('All finished')", "response": "This function will merge the gettext files into one or more languages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _msgmerge(po_path, pot_file, backup):\n    pot_file.seek(0)\n    command = [\n        'msgmerge',\n        '--update',\n        '--width=200',\n        '--backup=%s' % ('simple' if backup else 'off'),\n        po_path,\n        '-'\n    ]\n    p3 = Popen(command, stdin=pot_file)\n    p3.communicate()", "response": "Merge an existing. po file with new translations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self): # No \"_\" in the name, but nevertheless, running in the backed\n        self.preRun_()\n        self.running=True\n        \n        while(self.running):\n            self.cycle_()\n            self.handleSignal_()\n        \n        self.postRun_()", "response": "This function is called by the daemon to start the process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the shared memory client immediately after fork", "response": "def preRun_(self):\n        \"\"\"Create the shared memory client immediately after fork\n        \"\"\"\n        self.report(\"preRun_\")\n        super().preRun_()\n        self.client = ShmemRGBClient(\n            name=self.shmem_name,\n            n_ringbuffer=self.n_buffer,   # size of ring buffer\n            width=self.image_dimensions[0],\n            height=self.image_dimensions[1],\n            # client timeouts if nothing has been received in 1000 milliseconds\n            mstimeout=1000,\n            verbose=False\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nactivate the shared mem info.", "response": "def activate_(self, n_buffer, image_dimensions, shmem_name):\n        \"\"\"Shared mem info is given.  Now we can create the shmem client\n        \"\"\"\n        self.active = True\n        self.image_dimensions = image_dimensions\n        self.client = ShmemRGBClient(\n            name            =shmem_name,\n            n_ringbuffer    =n_buffer,   # size of ring buffer\n            width           =image_dimensions[0],\n            height          =image_dimensions[1],\n            # client timeouts if nothing has been received in 1000 milliseconds\n            mstimeout   =1000,\n            verbose     =False\n        )\n        self.postActivate_()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deactivate_(self):\n        self.preDeactivate_()\n        self.active = False\n        self.image_dimensions = None\n        self.client = None", "response": "Deactivate the current thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntell the external analyzer to reset itself", "response": "def reset(self):\n        \"\"\"Tell the external analyzer to reset itself\n        \"\"\"\n        self.report(\"sending reset\")\n        try:\n            self.p.stdin.write(bytes(\"T\\n\",\"utf-8\"))\n            self.p.stdin.flush()\n        except IOError:\n            self.report(\"could not send reset command\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntelling the process to exit the current process.", "response": "def close(self):\n        \"\"\"Tell the process to exit\n        \"\"\"\n        try:\n            self.p.stdin.write(bytes(\"X\\n\",\"utf-8\"))\n            self.p.stdin.flush()\n        except IOError:\n            self.report(\"could not send exit command\")\n        self.p.wait() # wait until the process is closed\n        try:\n            os.remove(self.tmpfile) # clean up the temporary file\n        except FileNotFoundError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readStdout(self):\n        btt = bytes()\n        while True:\n            bt = self.p.stdout.read(1)\n            if bt:\n                btt += bt\n            else:\n                # print(\"!\")\n                break\n            \"\"\"\n            if (bt == \"\\n\"):\n                break\n            \"\"\"\n        return btt[0:-1].decode(\"utf-8\")", "response": "Reads the stdout of the current process and returns the bytes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating temporary file for image dumps and the analyzer itself", "response": "def postActivate_(self):\n        \"\"\"Create temporary file for image dumps and the analyzer itself\n        \"\"\"\n        self.tmpfile = os.path.join(constant.tmpdir,\"valkka-\"+str(os.getpid())) # e.g. \"/tmp/valkka-10968\" \n        self.analyzer = ExternalDetector(\n            executable = self.executable,\n            image_dimensions = self.image_dimensions,\n            tmpfile = self.tmpfile\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating custom pages of file storage type.", "response": "def generate_pages_by_file():\n    \"\"\"Generates custom pages of 'file' storage type.\"\"\"\n    from veripress import app\n    from veripress.model import storage\n    from veripress.model.parsers import get_standard_format_name\n    from veripress.helpers import traverse_directory\n\n    deploy_dir = get_deploy_dir()\n\n    def copy_file(src, dst):\n        makedirs(os.path.dirname(dst), mode=0o755, exist_ok=True)\n        shutil.copyfile(src, dst)\n\n    with app.app_context(), app.test_client() as client:\n        root_path = os.path.join(app.instance_path, 'pages')\n        for path in traverse_directory(root_path):\n            # e.g. 'a/b/c/index.md'\n            rel_path = os.path.relpath(path, root_path)\n            # e.g. ('a/b/c/index', '.md')\n            filename, ext = os.path.splitext(rel_path)\n            if get_standard_format_name(ext[1:]) is not None:\n                # is source of custom page\n                rel_url = filename.replace(os.path.sep, '/') + '.html'\n                page = storage.get_page(rel_url, include_draft=False)\n                if page is not None:\n                    # it's not a draft, so generate the html page\n                    makedirs(os.path.join(deploy_dir,\n                                          os.path.dirname(rel_path)),\n                             mode=0o755, exist_ok=True)\n                    with open(os.path.join(deploy_dir, filename + '.html'),\n                              'wb') as f:\n                        f.write(client.get('/' + rel_url).data)\n                if app.config['PAGE_SOURCE_ACCESSIBLE']:\n                    copy_file(path, os.path.join(deploy_dir, rel_path))\n            else:\n                # is other direct files\n                copy_file(path, os.path.join(deploy_dir, rel_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefine column patterns and collections", "response": "def define(self):\n        \"\"\"Define column patterns and collections\n        \"\"\"\n        self.collections = []\n\n        self.camera_collection = \\\n            SimpleCollection(filename=os.path.join(self.directory, \"devices.dat\"),\n                             row_classes=[\n                DataModel.EmptyRow,\n                DataModel.RTSPCameraRow,\n                DataModel.USBCameraRow\n            ]\n            )\n        self.collections.append(self.camera_collection)\n\n        self.config_collection = \\\n            SimpleCollection(filename=os.path.join(self.directory, \"config.dat\"),\n                             row_classes=[  # we could dump here all kinds of info related to different kind of configuration forms\n                DataModel.MemoryConfigRow\n            ]\n            )\n        self.collections.append(self.config_collection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of RTSPCameraDevices by ID.", "response": "def getDevicesById(self): # , query):\n        \"\"\"\n        rows = self.camera_collection.get(query)\n        devices_by_id = {}\n        for row in rows:\n            row.pop(\"classname\")\n            device = DataModel.RTSPCameraDevice(**row)\n            devices_by_id[device._id] = device\n        return devices_by_id\n        \"\"\"\n        rows = self.camera_collection.get()\n        devices_by_id = {}\n        for row in rows:\n            classname=row.pop(\"classname\")\n            if (classname == \"RTSPCameraRow\"):\n                device = DataModel.RTSPCameraDevice(**row)\n            elif (classname == \"USBCameraRow\"):\n                device = DataModel.USBCameraDevice(**row)\n            else:\n                device = None\n            if (device):\n                devices_by_id[device._id] = device\n        return devices_by_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the LicensePlateDetector object.", "response": "def init(self):\n        \"\"\"Init alpr\n        \n        The LicensePlateDetector object gets instantiated in the multiprocess, so the library is imported in the multiprocess (i.e. \"other side of the fork\") as well\n        \"\"\"\n        # some modules might need to be imported \"on the other side of the fork\"\n        # .. but the, when importing this module, the import is not tested\n        #\n        # \n        # from openalpr import Alpr\n        from valkka.mvision.alpr.openalpr_fix import Alpr\n        self.movement = MovementDetector()\n        self.alpr = Alpr(self.country, self.conf_file, self.runtime_data)\n        if not self.alpr.is_loaded():\n            self.alpr = None\n            return\n        self.alpr.set_top_n(self.top_n)\n        self.reset()\n        \n        \"\"\"\n        # test in ipython:\n        from valkka.mvision.alpr.openalpr_fix import Alpr\n        country=\"eu\"\n        conf_file=\"/usr/share/openalpr/config/openalpr.defaults.conf\"\n        runtime_data=\"/usr/share/openalpr/runtime_data\"\n        a = Alpr(country, conf_file, runtime_data)\n        a.is_loaded()        \n        \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef postActivate_(self):\n        self.analyzer = LicensePlateDetector(**self.analyzer_pars) # this is called after the fork (i.e. after the multiprocess has been spawned)\n        self.report(\"analyzer object=\", self.analyzer)", "response": "This is called after the fork has been called."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inject(self):\n        if self.is_injected():\n            return False\n\n        with open(self, mode=\"a+\") as fp:\n            fp.seek(0)\n            fp.write(\"\\n\".join([\n                \"\",\n                \"try:\",\n                \"    import pout\",\n                \"except ImportError:\",\n                \"    pass\",\n                \"else:\",\n                \"    pout.inject()\",\n                \"\",\n            ]))\n\n        return True", "response": "inject code into sitecustomize. py that will inject pout into the builtins\n        so it will be available globally"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fix_relative_url(self, publish_type, rel_url):\n        if publish_type == 'post':\n            return self.fix_post_relative_url(rel_url), False\n        elif publish_type == 'page':\n            return self.fix_page_relative_url(rel_url)\n        else:\n            raise ValueError(\n                'Publish type \"{}\" is not supported'.format(publish_type))", "response": "Fix post or page relative url to a standard uniform format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfix post relative url to a standard uniform format.", "response": "def fix_post_relative_url(rel_url):\n        \"\"\"\n        Fix post relative url to a standard, uniform format.\n\n        Possible input:\n        - 2016/7/8/my-post\n        - 2016/07/08/my-post.html\n        - 2016/8/09/my-post/\n        - 2016/8/09/my-post/index\n        - 2016/8/09/my-post/index.htm\n        - 2016/8/09/my-post/index.html\n\n        :param rel_url: relative url to fix\n        :return: fixed relative url, or None if cannot recognize\n        \"\"\"\n        m = re.match(\n            r'^(?P<year>\\d{4})/(?P<month>\\d{1,2})/(?P<day>\\d{1,2})/'\n            r'(?P<post_name>[^/]+?)'\n            r'(?:(?:\\.html)|(?:/(?P<index>index(?:\\.html?)?)?))?$',\n            rel_url\n        )\n        if not m:\n            return None\n\n        year, month, day, post_name = m.groups()[:4]\n        try:\n            d = date(year=int(year), month=int(month), day=int(day))\n            return '/'.join((d.strftime('%Y/%m/%d'), post_name,\n                             'index.html' if m.group('index') else ''))\n        except (TypeError, ValueError):\n            # the date is invalid\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter result with given filter functions.", "response": "def _filter_result(result, filter_functions=None):\n        \"\"\"\n        Filter result with given filter functions.\n\n        :param result: an iterable object\n        :param filter_functions: some filter functions\n        :return: a filter object (filtered result)\n        \"\"\"\n        if filter_functions is not None:\n            for filter_func in filter_functions:\n                result = filter(filter_func, result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_posts_with_limits(self, include_draft=False, **limits):\n        filter_funcs = []\n\n        for attr in ('title', 'layout', 'author',\n                     'email', 'tags', 'categories'):\n            if limits.get(attr):\n                filter_set = set(to_list(limits.get(attr)))\n\n                def get_filter_func(filter_set_, attr_):\n                    return lambda p: filter_set_.intersection(\n                        to_list(getattr(p, attr_)))\n\n                filter_funcs.append(get_filter_func(filter_set, attr))\n\n        for attr in ('created', 'updated'):\n            interval = limits.get(attr)\n            if isinstance(interval, (list, tuple)) and len(interval) == 2 \\\n                    and isinstance(interval[0], date) and isinstance(\n                interval[1], date):\n                # [start date(time), end date(time)]\n                start, end = interval\n                start = to_datetime(start)\n                if not isinstance(end, datetime):\n                    # 'end' is a date,\n                    # we should convert it to 00:00:00 of the next day,\n                    # so that posts of that day will be included\n                    end = datetime.strptime(\n                        '%04d-%02d-%02d' % (end.year, end.month, end.day),\n                        '%Y-%m-%d')\n                    end += timedelta(days=1)\n\n                def get_filter_func(attr_, start_dt, end_dt):\n                    return lambda p: start_dt <= getattr(p, attr_) < end_dt\n\n                filter_funcs.append(get_filter_func(attr, start, end))\n\n        return self.get_posts(include_draft=include_draft,\n                              filter_functions=filter_funcs)", "response": "Get all posts and filter them as needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for a query text.", "response": "def search_for(self, query, include_draft=False):\n        \"\"\"\n        Search for a query text.\n\n        :param query: keyword to query\n        :param include_draft: return draft posts/pages or not\n        :return: an iterable object of posts and pages (if allowed).\n        \"\"\"\n        query = query.lower()\n        if not query:\n            return []\n\n        def contains_query_keyword(post_or_page):\n            contains = query in post_or_page.title.lower() \\\n                       or query in Markup(\n                get_parser(post_or_page.format).parse_whole(\n                    post_or_page.raw_content)\n            ).striptags().lower()\n            return contains\n\n        return filter(contains_query_keyword,\n                      chain(self.get_posts(include_draft=include_draft),\n                            self.get_pages(include_draft=include_draft)\n                            if current_app.config[\n                                'ALLOW_SEARCH_PAGES'] else []))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_page_relative_url(rel_url):\n        rel_url = rel_url.lstrip('/')  # trim all heading '/'\n        endswith_slash = rel_url.endswith('/')\n        rel_url = rel_url.rstrip('/') + (\n            '/' if endswith_slash else '')  # preserve only one trailing '/'\n        if not rel_url or rel_url == '/':\n            return None, False\n\n        file_path = os.path.join(current_app.instance_path, 'pages',\n                                 rel_url.replace('/', os.path.sep))\n        if rel_url.endswith('/'):\n            index_html_file_path = os.path.join(file_path, 'index.html')\n            if os.path.isfile(index_html_file_path):\n                # index.html exists\n                return index_html_file_path, True\n            return rel_url, False\n        elif os.path.isfile(file_path):\n            ext = os.path.splitext(file_path)[1][1:]\n            if get_standard_format_name(ext) is not None:\n                # is source of custom page\n                if current_app.config['PAGE_SOURCE_ACCESSIBLE']:\n                    return file_path, True\n            else:\n                # is other direct files\n                return file_path, True\n        elif os.path.isdir(file_path):\n            return rel_url + '/', False\n\n        sp = rel_url.rsplit('/', 1)\n        m = re.match(r'(.+)\\.html?', sp[-1])\n        if m:\n            sp[-1] = m.group(1) + '.html'\n        else:\n            sp[-1] += '.html'\n        return '/'.join(sp), False", "response": "Fix page relative url to a standard uniform format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_file(search_root, search_filename,\n                    instance_relative_root=False):\n        \"\"\"\n        Search for a filename in a specific search root dir.\n\n        :param search_root: root dir to search\n        :param search_filename: filename to search (no extension)\n        :param instance_relative_root: search root is relative to instance path\n        :return: tuple(full_file_path, extension without heading dot)\n        \"\"\"\n        if instance_relative_root:\n            search_root = os.path.join(current_app.instance_path, search_root)\n        file_path = None\n        file_ext = None\n        for file in os.listdir(search_root):\n            filename, ext = os.path.splitext(file)\n            if filename == search_filename and ext and ext != '.':\n                file_path = os.path.join(search_root, filename + ext)\n                file_ext = ext[1:]  # remove heading '.' (dot)\n                break\n        return file_path, file_ext", "response": "Search for a filename in a specific search root dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread yaml head and raw content from a file.", "response": "def read_file(file_path):\n        \"\"\"\n        Read yaml head and raw body content from a file.\n\n        :param file_path: file path\n        :return: tuple(meta, raw_content)\n        \"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            whole = f.read().strip()\n\n        if whole.startswith('---'):\n            # may has yaml meta info, so we try to split it out\n            sp = re.split(r'-{3,}', whole.lstrip('-'), maxsplit=1)\n            if len(sp) == 2:\n                # do have yaml meta info, so we read it\n                return yaml.load(sp[0]), sp[1].lstrip()\n        return {}, whole"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all posts from the filesystem.", "response": "def get_posts(self, include_draft=False, filter_functions=None):\n        \"\"\"\n        Get all posts from filesystem.\n\n        :param include_draft: return draft posts or not\n        :param filter_functions: filter to apply BEFORE result being sorted\n        :return: an iterable of Post objects (the first is the latest post)\n        \"\"\"\n\n        def posts_generator(path):\n            \"\"\"Loads valid posts one by one in the given path.\"\"\"\n            if os.path.isdir(path):\n                for file in os.listdir(path):\n                    filename, ext = os.path.splitext(file)\n                    format_name = get_standard_format_name(ext[1:])\n                    if format_name is not None and re.match(\n                            r'\\d{4}-\\d{2}-\\d{2}-.+', filename):\n                        # the format is supported and the filename is valid,\n                        # so load this post\n                        post = Post()\n                        post.format = format_name\n                        post.meta, post.raw_content = FileStorage.read_file(\n                            os.path.join(path, file))\n                        post.rel_url = filename.replace('-', '/', 3) + '/'\n                        post.unique_key = '/post/' + post.rel_url\n                        yield post\n\n        posts_path = os.path.join(current_app.instance_path, 'posts')\n        result = filter(lambda p: include_draft or not p.is_draft,\n                        posts_generator(posts_path))\n        result = self._filter_result(result, filter_functions)\n\n        return sorted(result, key=lambda p: p.created, reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a post object for given relative url from filesystem.", "response": "def get_post(self, rel_url, include_draft=False):\n        \"\"\"\n        Get post for given relative url from filesystem.\n\n        Possible input:\n        - 2017/01/01/my-post/\n        - 2017/01/01/my-post/index.html\n\n        :param rel_url: relative url\n        :param include_draft: return draft post or not\n        :return: a Post object\n        \"\"\"\n        raw_rel_url = str(rel_url)\n        if rel_url.endswith('/index.html'):\n            rel_url = rel_url.rsplit('/', 1)[\n                          0] + '/'  # remove the trailing 'index.html'\n        post_filename = rel_url[:-1].replace('/', '-')\n\n        post_file_path, post_file_ext = FileStorage.search_instance_file(\n            'posts', post_filename)\n        if post_file_path is None or post_file_ext is None or \\\n                get_standard_format_name(post_file_ext) is None:\n            # no such post\n            return None\n\n        # construct the post object\n        post = Post()\n        post.rel_url = raw_rel_url\n        # 'rel_url' contains no trailing 'index.html'\n        post.unique_key = '/post/' + rel_url\n        post.format = get_standard_format_name(post_file_ext)\n        post.meta, post.raw_content = FileStorage.read_file(post_file_path)\n        return post if include_draft or not post.is_draft else None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tags(self):\n        posts = self.get_posts(include_draft=True)\n        result = {}\n        for post in posts:\n            for tag_name in set(post.tags):\n                result[tag_name] = result.setdefault(\n                    tag_name, Pair(0, 0)) + Pair(1, 0 if post.is_draft else 1)\n        return list(result.items())", "response": "Get all tags and post count of each tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all categories and post count of each category.", "response": "def get_categories(self):\n        \"\"\"\n        Get all categories and post count of each category.\n\n        :return dict_item(category_name, Pair(count_all, count_published))\n        \"\"\"\n        posts = self.get_posts(include_draft=True)\n        result = {}\n        for post in posts:\n            for category_name in set(post.categories):\n                result[category_name] = result.setdefault(\n                    category_name,\n                    Pair(0, 0)) + Pair(1, 0 if post.is_draft else 1)\n        return list(result.items())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pages(self, include_draft=False):\n\n        def pages_generator(pages_root_path):\n            for file_path in traverse_directory(pages_root_path,\n                                                yield_dir=False):\n                rel_path = os.path.relpath(file_path, pages_root_path)\n                rel_path, ext = os.path.splitext(rel_path)\n                if not ext or ext == '.' or get_standard_format_name(\n                        ext[1:]) is None:\n                    continue  # pragma: no cover\n\n                if rel_path.endswith(os.path.sep + 'index'):\n                    rel_path = rel_path[:-len('index')]\n                else:\n                    rel_path += '.html'\n                page = self.get_page(rel_path.replace(os.path.sep, '/'),\n                                     include_draft=include_draft)\n                if page is not None:\n                    yield page\n\n        pages_path = os.path.join(current_app.instance_path, 'pages')\n        return list(pages_generator(pages_path))", "response": "Get all custom pages and return a list of Page objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_page(self, rel_url, include_draft=False):\n        page_dir = os.path.dirname(rel_url.replace('/', os.path.sep))\n        page_path = os.path.join(current_app.instance_path, 'pages', page_dir)\n        if not os.path.isdir(page_path):\n            # no such directory\n            return None\n\n        page_filename = rel_url[len(page_dir):].lstrip('/')\n        if not page_filename:\n            page_filename = 'index'\n        else:\n            page_filename = os.path.splitext(page_filename)[0]\n\n        page_file_path, page_file_ext = FileStorage.search_file(page_path,\n                                                                page_filename)\n        if page_file_path is None or page_file_ext is None or \\\n                get_standard_format_name(page_file_ext) is None:\n            # no such page\n            return None\n\n        page = Page()\n        page.rel_url = rel_url\n        page.unique_key = '/' + (\n            rel_url.rsplit('/', 1)[0] + '/' if rel_url.endswith(\n                '/index.html') else rel_url)\n        page.format = get_standard_format_name(page_file_ext)\n        page.meta, page.raw_content = FileStorage.read_file(page_file_path)\n        return page if include_draft or not page.is_draft else None", "response": "Get custom page for given relative url from filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_widgets(self, position=None, include_draft=False):\n\n        def widgets_generator(path):\n            \"\"\"Loads valid widgets one by one in the given path.\"\"\"\n            if os.path.isdir(path):\n                for file in os.listdir(path):\n                    _, ext = os.path.splitext(file)\n                    format_name = get_standard_format_name(ext[1:])\n                    if format_name is not None:\n                        # the format is supported, so load it\n                        widget = Widget()\n                        widget.format = format_name\n                        widget.meta, widget.raw_content = \\\n                            FileStorage.read_file(os.path.join(path, file))\n                        yield widget\n\n        widgets_path = os.path.join(current_app.instance_path, 'widgets')\n        positions = to_list(position) if position is not None else position\n        result = filter(\n            lambda w: (w.position in positions\n                       if positions is not None else True) and\n                      (include_draft or not w.is_draft),\n            widgets_generator(widgets_path))\n        return sorted(result, key=lambda w: (w.position, w.order))", "response": "Get widgets for given position from filesystem."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef custom_render_template(template_name_or_list, **context):\n    response_str = render_template(\n        functools.reduce(lambda x, y: x + [os.path.join('custom', y), y],\n                         to_list(template_name_or_list), []),\n        **context\n    )\n    if hasattr(g, 'status_code'):\n        status_code = g.status_code\n    else:\n        status_code = 200\n    return response_str, status_code", "response": "Render a custom template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef templated(template=None, *templates):\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            template_ = template\n            if template_ is None:\n                template_ = request.endpoint.split('.', 1)[1].replace(\n                    '.', '/') + '.html'\n            context = func(*args, **kwargs)\n            if context is None:\n                context = {}\n            elif not isinstance(context, dict):\n                return context\n            return custom_render_template(\n                list(chain(to_list(template_), templates)), **context)\n\n        return wrapper\n\n    return decorator", "response": "Decorator for views that returns a function with one or more default template name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy all content in src directory to dst directory.", "response": "def copy_folder_content(src, dst):\n    \"\"\"\n    Copy all content in src directory to dst directory.\n    The src and dst must exist.\n    \"\"\"\n    for file in os.listdir(src):\n        file_path = os.path.join(src, file)\n        dst_file_path = os.path.join(dst, file)\n        if os.path.isdir(file_path):\n            shutil.copytree(file_path, dst_file_path)\n        else:\n            shutil.copyfile(file_path, dst_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves all content in the given folder.", "response": "def remove_folder_content(path, ignore_hidden_file=False):\n    \"\"\"\n    Remove all content in the given folder.\n    \"\"\"\n    for file in os.listdir(path):\n        if ignore_hidden_file and file.startswith('.'):\n            continue\n\n        file_path = os.path.join(path, file)\n        if os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n        else:\n            os.remove(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makedirs(path, mode=0o777, exist_ok=False):\n    os.makedirs(path, mode, exist_ok)", "response": "A wrapper of os. makedirs that creates the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unload(self):\n\n        if self.loaded:\n            self.loaded = False\n            self._openalprpy_lib.dispose(self.alpr_pointer)", "response": "Unloads OpenALPR from memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recognize_array(self, byte_array):\n        if type(byte_array) != bytes:\n            raise TypeError(\"Expected a byte array (string in Python 2, bytes in Python 3)\")\n        pb = ctypes.cast(byte_array, ctypes.POINTER(ctypes.c_ubyte))\n        ptr = self._recognize_array_func(self.alpr_pointer, pb, len(byte_array))\n        json_data = ctypes.cast(ptr, ctypes.c_char_p).value\n        json_data = _convert_from_charp(json_data)\n        response_obj = json.loads(json_data)\n        self._free_json_mem_func(ctypes.c_void_p(ptr))\n        return response_obj", "response": "This causes OpenALPR to attempt to recognize an image passed in as a byte array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recognize_ndarray(self, ndarray):\n        if self._recognize_raw_image_func is None:\n            raise RuntimeError('NumPy missing')\n        height, width = ndarray.shape[:2]\n        bpp = ndarray.shape[2] if len(ndarray.shape) > 2 else 1\n        ptr = self._recognize_raw_image_func(self.alpr_pointer, ndarray.flatten(), bpp, width, height)\n        json_data = ctypes.cast(ptr, ctypes.c_char_p).value\n        json_data = _convert_from_charp(json_data)\n        \n        # there is a bug in the openalpr python bindings\n        # sometimes there are real numbers with \",\" as the decimal point..!\n        # print(\"openalpr_lib : recognize_ndarray : json_data =\", json_data)\n        p = re.compile('\\d(\\,)\\d')\n        json_data = p.subn(\".\", json_data)[0]\n        \n        response_obj = json.loads(json_data)\n        self._free_json_mem_func(ctypes.c_void_p(ptr))\n        return response_obj", "response": "This causes OpenALPR to attempt to recognize an image passed in as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the country for detecting license plates.", "response": "def set_country(self, country):\n        \"\"\"\n        This sets the country for detecting license plates. For example,\n        setting country to \"us\" for United States or \"eu\" for Europe.\n\n        :param country: A unicode/ascii string (Python 2/3) or bytes array (Python 3)\n        :return: None\n        \"\"\"\n        country = _convert_to_charp(country)\n        self._set_country_func(self.alpr_pointer, country)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the prewarp configuration used to skew images in OpenALPR before processing.", "response": "def set_prewarp(self, prewarp):\n        \"\"\"\n        Updates the prewarp configuration used to skew images in OpenALPR before\n        processing.\n\n        :param prewarp: A unicode/ascii string (Python 2/3) or bytes array (Python 3)\n        :return: None\n        \"\"\"\n        prewarp = _convert_to_charp(prewarp)\n        self._set_prewarp_func(self.alpr_pointer, prewarp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if this call string is complete meaning it has a function name and balanced parens", "response": "def is_complete(self):\n        \"\"\"Return True if this call string is complete, meaning it has a function\n        name and balanced parens\"\"\"\n        try:\n            [t for t in self.tokens]\n            ret = True\n            logger.debug('CallString [{}] is complete'.format(self.strip()))\n\n        except tokenize.TokenError:\n            logger.debug('CallString [{}] is NOT complete'.format(self.strip()))\n            ret = False\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_calls(self, ast_tree, called_module, called_func):\n        '''\n        scan the abstract source tree looking for possible ways to call the called_module\n        and called_func\n\n        since -- 7-2-12 -- Jay\n\n        example -- \n            # import the module a couple ways:\n            import pout\n            from pout import v\n            from pout import v as voom\n            import pout as poom\n\n            # this function would return: ['pout.v', 'v', 'voom', 'poom.v']\n\n        module finder might be useful someday\n        link -- http://docs.python.org/library/modulefinder.html\n        link -- http://stackoverflow.com/questions/2572582/return-a-list-of-imported-python-modules-used-in-a-script\n\n        ast_tree -- _ast.* instance -- the internal ast object that is being checked, returned from compile()\n            with ast.PyCF_ONLY_AST flag\n        called_module -- string -- we are checking the ast for imports of this module\n        called_func -- string -- we are checking the ast for aliases of this function\n\n        return -- set -- the list of possible calls the ast_tree could make to call the called_func\n        ''' \n        s = set()\n\n        # always add the default call, the set will make sure there are no dupes...\n        s.add(\"{}.{}\".format(called_module, called_func))\n\n        if hasattr(ast_tree, 'name'):\n            if ast_tree.name == called_func:\n                # the function is defined in this module\n                s.add(called_func)\n\n        if hasattr(ast_tree, 'body'):\n            # further down the rabbit hole we go\n            if isinstance(ast_tree.body, Iterable):\n                for ast_body in ast_tree.body:\n                    s.update(self._find_calls(ast_body, called_module, called_func))\n\n        elif hasattr(ast_tree, 'names'):\n            # base case\n            if hasattr(ast_tree, 'module'):\n                # we are in a from ... import ... statement\n                if ast_tree.module == called_module:\n                    for ast_name in ast_tree.names:\n                        if ast_name.name == called_func:\n                            s.add(unicode(ast_name.asname if ast_name.asname is not None else ast_name.name))\n\n            else:\n                # we are in a import ... statement\n                for ast_name in ast_tree.names:\n                    if hasattr(ast_name, 'name') and (ast_name.name == called_module):\n                        call = \"{}.{}\".format(\n                            ast_name.asname if ast_name.asname is not None else ast_name.name,\n                            called_func\n                        )\n                        s.add(call)\n\n        return s", "response": "finds possible calls for a given module and function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_arg_info(self):\n        '''\n        get all the info of a method call\n\n        this will find what arg names you passed into the method and tie them to their passed in values,\n        it will also find file and line number\n\n        return -- dict -- a bunch of info on the call\n        '''\n        ret_dict = {\n            'args': [],\n            #'frame': None,\n            'line': 'Unknown',\n            'file': 'Unknown',\n            'arg_names': []\n        }\n        arg_vals = self.arg_vals\n        #modname = self.modname\n\n        c = self.call\n        ret_dict.update(c.info)\n\n        if len(arg_vals) > 0:\n            args = []\n\n            if len(ret_dict['arg_names']) > 0:\n                # match the found arg names to their respective values\n                for i, arg_name in enumerate(ret_dict['arg_names']):\n                    args.append({'name': arg_name, 'val': arg_vals[i]})\n\n            else:\n                # we can't autodiscover the names, in an interactive shell session?\n                for i, arg_val in enumerate(arg_vals):\n                    args.append({'name': 'Unknown {}'.format(i), 'val': arg_val})\n\n            ret_dict['args'] = args\n\n        return ret_dict", "response": "get all the info of a method call in a bunch of info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_entry_call(self, frames):\n\n        back_i = 0\n        pout_path = self._get_src_file(self.modname)\n        for frame_i, frame in enumerate(frames):\n            if frame[1] == pout_path:\n                back_i = frame_i\n\n        return Call(frames[back_i])", "response": "find the correct entry in the frames list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef url_rule(blueprint_or_app, rules,\n             endpoint=None, view_func=None, **options):\n    \"\"\"\n    Add one or more url rules to the given Flask blueprint or app.\n\n    :param blueprint_or_app: Flask blueprint or app\n    :param rules: a single rule string or a list of rules\n    :param endpoint: endpoint\n    :param view_func: view function\n    :param options: other options\n    \"\"\"\n    for rule in to_list(rules):\n        blueprint_or_app.add_url_rule(rule,\n                                      endpoint=endpoint,\n                                      view_func=view_func,\n                                      **options)", "response": "Adds one or more url rules to the given Flask blueprint or app."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_list(item_or_list):\n    if isinstance(item_or_list, list):\n        return item_or_list\n    elif isinstance(item_or_list, (str, bytes)):\n        return [item_or_list]\n    elif isinstance(item_or_list, Iterable):\n        return list(item_or_list)\n    else:\n        return [item_or_list]", "response": "Convert a single item a tuple a generator or anything else to a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a date object to a datetime object.", "response": "def to_datetime(date_or_datetime):\n    \"\"\"\n    Convert a date object to a datetime object,\n    or return as it is if it's not a date object.\n\n    :param date_or_datetime: date or datetime object\n    :return: a datetime object\n    \"\"\"\n    if isinstance(date_or_datetime, date) and \\\n            not isinstance(date_or_datetime, datetime):\n        d = date_or_datetime\n        return datetime.strptime(\n            '%04d-%02d-%02d' % (d.year, d.month, d.day), '%Y-%m-%d')\n    return date_or_datetime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a timezone string to a timezone object.", "response": "def timezone_from_str(tz_str):\n    \"\"\"\n    Convert a timezone string to a timezone object.\n\n    :param tz_str: string with format 'Asia/Shanghai' or 'UTC\u00b1[hh]:[mm]'\n    :return: a timezone object (tzinfo)\n    \"\"\"\n    m = re.match(r'UTC([+|-]\\d{1,2}):(\\d{2})', tz_str)\n    if m:\n        # in format 'UTC\u00b1[hh]:[mm]'\n        delta_h = int(m.group(1))\n        delta_m = int(m.group(2)) if delta_h >= 0 else -int(m.group(2))\n        return timezone(timedelta(hours=delta_h, minutes=delta_m))\n\n    # in format 'Asia/Shanghai'\n    try:\n        return pytz.timezone(tz_str)\n    except pytz.exceptions.UnknownTimeZoneError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntraversing through a directory recursively.", "response": "def traverse_directory(dir_path, yield_dir=False):\n    \"\"\"\n    Traverse through a directory recursively.\n\n    :param dir_path: directory path\n    :param yield_dir: yield subdirectory or not\n    :return: a generator\n    \"\"\"\n    if not os.path.isdir(dir_path):\n        return\n\n    for item in os.listdir(dir_path):\n        new_path = os.path.join(dir_path, item)\n        if os.path.isdir(new_path):\n            if yield_dir:\n                yield new_path + os.path.sep\n            yield from traverse_directory(new_path, yield_dir)\n        else:\n            yield new_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse TOC of HTML content.", "response": "def parse_toc(html_content):\n    \"\"\"\n    Parse TOC of HTML content if the SHOW_TOC config is true.\n\n    :param html_content: raw HTML content\n    :return: tuple(processed HTML, toc list, toc HTML unordered list)\n    \"\"\"\n    from flask import current_app\n    from veripress.model.toc import HtmlTocParser\n\n    if current_app.config['SHOW_TOC']:\n        toc_parser = HtmlTocParser()\n        toc_parser.feed(html_content)\n        toc_html = toc_parser.toc_html(\n            depth=current_app.config['TOC_DEPTH'],\n            lowest_level=current_app.config['TOC_LOWEST_LEVEL'])\n        toc = toc_parser.toc(\n            depth=current_app.config['TOC_DEPTH'],\n            lowest_level=current_app.config['TOC_LOWEST_LEVEL'])\n        return toc_parser.html, toc, toc_html\n    else:\n        return html_content, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new string with indent_count spaces added to the beginning of each line of self.", "response": "def indent(self, indent_count):\n        '''\n        add whitespace to the beginning of each line of val\n\n        link -- http://code.activestate.com/recipes/66055-changing-the-indentation-of-a-multi-line-string/\n\n        val -- string\n        indent -- integer -- how much whitespace we want in front of each line of val\n\n        return -- string -- val with more whitespace\n        '''\n        if indent_count < 1: return self\n\n        s = [(\"\\t\" * indent_count) + line for line in self.splitlines(False)]\n        s = \"\\n\".join(s)\n        return type(self)(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating H264 stream Input image output video file time in seconds", "response": "def genH264(infile, outfile, T):\n    \"\"\"Generate H264 stream\n    \n    Input image, output video file, time in seconds\n    \n    Example:\n    \n    ::\n    \n        genH264(\"/home/sampsa/python3/tests/lprtest/RealImages/IMG_20170308_093511.jpg\",\"testi.mkv\", 10)\n    \n    \"\"\"\n    x264opts=\"-x264opts keyint=10:min-keyint=10:bframes=0\"\n    h264_dummystream_flavor=\"-pix_fmt yuv420p -vprofile main\" \n\n    opts=\"-vcodec h264 \"+h264_dummystream_flavor+\" \"+x264opts+\" -fflags +genpts -r 25 -t \"+str(T)\n    com=\"ffmpeg -y -loop 1 -fflags +genpts -r 25 -i \"+infile+\" \"+opts+\" \"+outfile\n    print(com)\n    os.system(com)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index(self, row, column, parent):\n\n        if not self.hasIndex(row, column, parent):\n            return QtCore.QModelIndex()\n\n        if not parent.isValid():\n            parentItem = self.root\n        else:\n            # So, here we go from QModelIndex to the actual object .. ?\n            parentItem = parent.internalPointer()\n\n        # the only place where a child item is queried\n        childItem = parentItem.getChild(row)\n        if childItem:\n            # return self.createIndex(row, column)\n            return self.createIndex(row, column, childItem)\n            \"\"\"\n            # .. that one does not work for PySide 5.12+\n            TypeError: 'PySide2.QtCore.QAbstractItemModel.createIndex' called with wrong argument types:\n            PySide2.QtCore.QAbstractItemModel.createIndex(int, int, ServerListItem)\n            Supported signatures:\n            PySide2.QtCore.QAbstractItemModel.createIndex(int, int, quintptr = 0)\n            PySide2.QtCore.QAbstractItemModel.createIndex(int, int, void = nullptr)\n            \"\"\"\n        else:\n            return QtCore.QModelIndex()", "response": "Returns the index of the item in the model specified by the given row column and parent index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parent(self, index):\n        if not index.isValid():\n            return QtCore.QModelIndex()\n\n        childItem = index.internalPointer()\n        # the only place where the parent item is queried\n        parentItem = childItem.getParent()\n\n        if parentItem == self.root:\n            return QtCore.QModelIndex()\n\n        return self.createIndex(parentItem.row(), 0, parentItem)", "response": "Returns the parent of the model item with the given index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef columnCount(self, parent):\n        # print(\"columnCount:\",self)\n        if parent.isValid():\n            return parent.internalPointer().columnCount()\n        else:\n            return self.root.columnCount()", "response": "Returns the number of columns for the given parent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the number of rows under the given parent.", "response": "def rowCount(self, parent):\n        \"\"\" Returns the number of rows under the given parent. When the parent is valid it means that rowCount is returning the number of children of parent.\n        \"\"\"\n\n        if parent.column() > 0:\n            return 0\n\n        if not parent.isValid():\n            parentItem = self.root\n        else:\n            parentItem = parent.internalPointer()\n\n        return parentItem.childCount()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flags(self, index):\n\n        if not index.isValid():\n            return QtCore.Qt.NoItemFlags\n\n        item = index.internalPointer()\n\n        # return QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable |\n        # QtCore.Qt.ItemIsDragEnabled\n        return item.getFlags()", "response": "Returns the item flags for the given index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the full value with the path also", "response": "def full_value(self):\n        \"\"\"Returns the full value with the path also (ie, name = value (path))\n\n        :returns: String\n        \"\"\"\n        s = self.name_value()\n        s += self.path_value()\n        s += \"\\n\\n\"\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _printstr(self, args, call_info=None):\n        # unicode sandwich, everything printed should be a byte string\n        s = \"\\n\"\n\n        for arg in args:\n            #s += arg.encode('utf-8', 'pout.replace')\n            s += arg\n\n        if call_info:\n            s += \"({}:{})\\n\\n\".format(self._get_path(call_info['file']), call_info['line'])\n\n        return s", "response": "this gets all the args ready to be printed see self. _print"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _str(self, name, val):\n        '''\n        return a string version of name = val that can be printed\n\n        example -- \n            _str('foo', 'bar') # foo = bar\n\n        name -- string -- the variable name that was passed into one of the public methods\n        val -- mixed -- the variable at name's value\n\n        return -- string\n        '''\n        s = ''\n        v = Value(val)\n\n        if name:\n            logger.debug(\"{} is type {}\".format(name, v.typename))\n            try:\n                count = len(val)\n                s = \"{} ({}) = {}\".format(name, count, v.string_value())\n\n            except (TypeError, KeyError, AttributeError) as e:\n                logger.info(e, exc_info=True)\n                s = \"{} = {}\".format(name, v.string_value())\n\n        else:\n            s = v.string_value()\n\n        return s", "response": "return a string version of name = val that can be printed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the loggers that should be activated", "response": "def loggers(self):\n        \"\"\"Return all the loggers that should be activated\"\"\"\n        ret = []\n        if self.logger_name:\n            if isinstance(self.logger_name, logging.Logger):\n                ret.append((self.logger_name.name, self.logger_name))\n            else:\n                ret.append((self.logger_name, logging.getLogger(self.logger_name)))\n\n        else:\n            ret = list(logging.Logger.manager.loggerDict.items())\n            ret.append((\"root\", logging.getLogger()))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a nicely formatted backtrace", "response": "def _get_backtrace(self, frames, inspect_packages=False, depth=0):\n        '''\n        get a nicely formatted backtrace\n\n        since -- 7-6-12\n\n        frames -- list -- the frame_tuple frames to format\n        inpsect_packages -- boolean -- by default, this only prints code of packages that are not \n            in the pythonN directories, that cuts out a lot of the noise, set this to True if you\n            want a full stacktrace\n        depth -- integer -- how deep you want the stack trace to print (ie, if you only care about\n            the last three calls, pass in depth=3 so you only get the last 3 rows of the stack)\n\n        return -- list -- each line will be a nicely formatted entry of the backtrace\n        '''\n        calls = []\n        #count = 1\n\n        #for count, f in enumerate(frames[1:], 1):\n        for count, f in enumerate(frames, 1):\n            #prev_f = frames[i]\n            #called_module = inspect.getmodule(prev_f[0]).__name__\n            #called_func = prev_f[3]\n\n            call = self.call_class(f)\n            s = self._get_call_summary(call, inspect_packages=inspect_packages, index=count)\n            calls.append(s)\n            #count += 1\n\n            if depth and (count > depth):\n                break\n\n        # reverse the order on return so most recent is on the bottom\n        return calls[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a nicely formatted string synopsis of the call", "response": "def _get_call_summary(self, call, index=0, inspect_packages=True):\n        '''\n        get a call summary\n\n        a call summary is a nicely formatted string synopsis of the call\n\n        handy for backtraces\n\n        since -- 7-6-12\n\n        call_info -- dict -- the dict returned from _get_call_info()\n        index -- integer -- set to something above 0 if you would like the summary to be numbered\n        inspect_packages -- boolean -- set to True to get the full format even for system frames\n\n        return -- string\n        '''\n        call_info = call.info\n        inspect_regex = re.compile(r'[\\\\\\\\/]python\\d(?:\\.\\d+)?', re.I)\n\n        # truncate the filepath if it is super long\n        f = call_info['file']\n        if len(f) > 75:\n            f = \"{}...{}\".format(f[0:30], f[-45:])\n\n        if inspect_packages or not inspect_regex.search(call_info['file']): \n\n            s = \"{}:{}\\n\\n{}\\n\\n\".format(\n                f,\n                call_info['line'],\n                String(call_info['call']).indent(1)\n            )\n\n        else:\n\n            s = \"{}:{}\\n\".format(\n                f,\n                call_info['line']\n            )\n\n        if index > 0:\n            s = \"{:02d} - {}\".format(index, s)\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the video stream to the given device.", "response": "def setDevice(self, device): \n        \"\"\"Sets the video stream\n        \n        :param device:      A rather generic device class.  In this case DataModel.RTSPCameraDevice.\n        \"\"\"\n        self.report(\"setDevice :\", device)\n        \n        if (self.mvision_process == None):\n            return\n        \n        if (not device and not self.device): # None can be passed as an argument when the device has not been set yet\n            return\n            \n        if (self.device):\n            if self.device == device:\n                self.report(\"setDevice : same device\")\n                return\n            \n        if self.filterchain: # there's video already\n            self.clearDevice()\n        \n        self.device = device\n        self.video.setDevice(self.device) # inform the video widget so it can start drags\n        \n        # ManagedFilterChain.addViewPort accepts ViewPort instance\n        self.filterchain = self.filterchain_group.get(_id = self.device._id)\n        \n        if self.filterchain:\n            self.viewport.setXScreenNum(self.n_xscreen)\n            self.viewport.setWindowId  (int(self.video.winId()))\n            self.filterchain.addViewPort(self.viewport)\n            \n            # now the shared mem / semaphore part :\n            self.shmem_name = self.filterchain.getShmem()\n            print(self.pre, \"setDevice : got shmem name\", self.shmem_name)\n            \n            self.mvision_widget = self.mvision_process.getWidget()\n            self.mvision_widget.setParent(self.main_widget)\n            self.main_layout.addWidget(self.mvision_widget)\n            \n            self.mvision_process.activate(\n                n_buffer         = constant.shmem_n_buffer,\n                image_dimensions = constant.shmem_image_dimensions,\n                shmem_name       = self.shmem_name\n                )\n                \n            self.thread.addProcess(self.mvision_process)\n            \n            # is there a signal giving the bounding boxes..?  let's connect it\n            if hasattr(self.mvision_process.signals,\"bboxes\"):\n                print(self.pre, \"setDevice : connecting bboxes signal\")\n                self.mvision_process.signals.bboxes.connect(self.set_bounding_boxes_slot)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the current stream and remove the current filterchain", "response": "def clearDevice(self):\n        \"\"\"Remove the current stream\n        \"\"\"\n        print(self.pre, \"clearDevice: \")\n        \n        self.report(\"clearDevice\")\n        if not self.device:\n            return\n        if (self.mvision_process==None):\n            return\n        \n        self.filterchain.delViewPort(self.viewport)\n        self.filterchain.releaseShmem(self.shmem_name)\n\n        self.mvision_process.deactivate() # put process back to sleep ..\n        \n        self.main_layout.removeWidget(self.mvision_widget)\n        \n        self.filterchain = None\n        self.device = None\n        \n        self.video.update()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parser(format_name, ext_names=None):\n\n    def decorator(cls):\n        format_name_lower = format_name.lower()\n        if ext_names is None:\n            _ext_format_mapping[format_name_lower] = format_name_lower\n        else:\n            for ext in to_list(ext_names):\n                _ext_format_mapping[ext.lower()] = format_name_lower\n        _format_parser_mapping[format_name_lower] = cls()\n        return cls\n\n    return decorator", "response": "Decorator to register a parser class for a given format name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_preview(self, raw_content):\n        if self._read_more_exp is None:\n            return self.parse_whole(raw_content), False\n\n        sp = self._read_more_exp.split(raw_content, maxsplit=1)\n        if len(sp) == 2 and sp[0]:\n            has_more_content = True\n            result = sp[0].rstrip()\n        else:\n            has_more_content = False\n            result = raw_content\n        # since the preview part contains no read_more_sep,\n        # we can safely use the parse_whole method\n        return self.parse_whole(result), has_more_content", "response": "Parse the preview part of the content and return the parsed string and whether there is more content or not."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the first read_more_sep that occurs in raw_content.", "response": "def remove_read_more_sep(self, raw_content):\n        \"\"\"\n        Removes the first read_more_sep that occurs in raw_content.\n        Subclasses should call this method to preprocess raw_content.\n        \"\"\"\n        if self._read_more_exp is None:\n            return raw_content\n\n        sp = self._read_more_exp.split(raw_content, maxsplit=1)\n        if len(sp) == 2 and sp[0]:\n            result = '\\n\\n'.join((sp[0].rstrip(), sp[1].lstrip()))\n        else:\n            result = raw_content\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef v(*args, **kwargs):\n    '''\n    print the name = values of any passed in variables\n\n    this prints out the passed in name, the value, and the file:line where the v()\n    method was called so you can easily find it and remove it later\n\n    example -- \n        foo = 1\n        bar = [1, 2, 3]\n        out.v(foo, bar)\n        \"\"\" prints out:\n        foo = 1\n\n        bar = \n        [\n            0: 1,\n            1: 2,\n            2: 3\n        ]\n\n        (/file:line)\n        \"\"\"\n\n    *args -- list -- the variables you want to see pretty printed for humans\n    '''\n    if not args:\n        raise ValueError(\"you didn't pass any arguments to print out\")\n\n    with Reflect.context(args, **kwargs) as r:\n        instance = V_CLASS(r, stream, **kwargs)\n        instance()", "response": "prints out the name value and file line where the v method was called"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vs(*args, **kwargs):\n    if not args:\n        raise ValueError(\"you didn't pass any arguments to print out\")\n\n    with Reflect.context(args, **kwargs) as r:\n        instance = V_CLASS(r, stream, **kwargs)\n        instance.writeline(instance.value())", "response": "prints out the sequence of variable names or file positions of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef s(*args, **kwargs):\n    if not args:\n        raise ValueError(\"you didn't pass any arguments to print out\")\n\n    with Reflect.context(args, **kwargs) as r:\n        instance = V_CLASS(r, stream, **kwargs)\n        return instance.full_value().strip()", "response": "exactly like v but returns the string instead of printing it out\n    since 10 - 15 - 2015\n    return -- str\n    since 10 - 15 - 2015\n    return -- str\n    since 10 - 15 - 2015\n    return -- str\n    since 10 - 15 - 2015\n    return -- str\n    since 10 - 15 - 2015\n    return -- str\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef r(*args, **kwargs):\n\n    if len(args) <= 0:\n        raise ValueError(\"you didn't pass any arguments to print out\")\n\n    with Reflect.context(args, **kwargs) as r:\n        instance = R_CLASS(r, stream, **kwargs)\n        instance()", "response": "Similar to pout. v but gets rid of name and file information so it can be used\n    in loops and stuff"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexits the sequence of objects", "response": "def x(*args, **kwargs):\n    '''\n    same as sys.exit(1) but prints out where it was called from before exiting\n\n    I just find this really handy for debugging sometimes\n\n    since -- 2013-5-9\n\n    exit_code -- int -- if you want it something other than 1\n    '''\n    with Reflect.context(args, **kwargs) as r:\n        instance = V_CLASS(r, stream, **kwargs)\n        if args:\n            instance()\n\n        else:\n            instance.writelines([\n                'exit at line {}\\n'.format(instance.reflect.info[\"line\"]),\n                instance.path_value()\n            ])\n\n    exit_code = 1\n    sys.exit(exit_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting the count of items in the", "response": "def h(count=0, **kwargs):\n    '''\n    prints \"here count\"\n\n    example -- \n        h(1) # here 1 (/file:line)\n        h() # here line (/file:line)\n\n    count -- integer -- the number you want to put after \"here\"\n    '''\n    with Reflect.context(**kwargs) as r:\n        kwargs[\"count\"] = count\n        instance = H_CLASS(r, stream, **kwargs)\n        instance()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a big text break", "response": "def b(*args, **kwargs):\n    '''\n    create a big text break, you just kind of have to run it and see\n\n    since -- 2013-5-9\n\n    *args -- 1 arg = title if string, rows if int\n        2 args = title, int\n        3 args = title, int, sep\n    '''\n    with Reflect.context(**kwargs) as r:\n        kwargs[\"args\"] = args\n        instance = B_CLASS(r, stream, **kwargs)\n        instance()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the alphabetical tree of characters", "response": "def c(*args, **kwargs):\n    '''\n    kind of like od -c on the command line, basically it dumps each character and info\n    about that char\n\n    since -- 2013-5-9\n\n    *args -- tuple -- one or more strings to dump\n    '''\n    with Reflect.context(**kwargs) as r:\n        kwargs[\"args\"] = args\n        instance = C_CLASS(r, stream, **kwargs)\n        instance()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef j(*args, **kwargs):\n    if not args:\n        raise ValueError(\"you didn't pass any arguments to print out\")\n\n    with Reflect.context(args, **kwargs) as r:\n        instance = J_CLASS(r, stream, **kwargs)\n        instance()", "response": "dump json\n    since 2013 - 9 - 10"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting out memory usage at this point in time http://docs. python. org / 2. 7. 0", "response": "def m(name='', **kwargs):\n    \"\"\"\n    Print out memory usage at this point in time\n\n    http://docs.python.org/2/library/resource.html\n    http://stackoverflow.com/a/15448600/5006\n    http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended\n    \"\"\"\n    with Reflect.context(**kwargs) as r:\n        kwargs[\"name\"] = name\n        instance = M_CLASS(r, stream, **kwargs)\n        instance()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsleeps for a given number of seconds", "response": "def sleep(seconds, **kwargs):\n    '''\n    same as time.sleep(seconds) but prints out where it was called before sleeping\n    and then again after finishing sleeping\n\n    I just find this really handy for debugging sometimes\n\n    since -- 2017-4-27\n\n    :param seconds: float|int, how many seconds to sleep\n    '''\n    if seconds <= 0.0:\n        raise ValueError(\"Invalid seconds {}\".format(seconds))\n\n    with Reflect.context(**kwargs) as r:\n        instance = V_CLASS(r, stream, **kwargs)\n        instance.writeline(\"Sleeping {} second{} at {}\".format(\n            seconds,\n            \"s\" if seconds != 1.0 else \"\",\n            instance.path_value()\n        ))\n\n        time.sleep(seconds)\n        instance.writelines([\"...Done Sleeping\\n\", instance.path_value()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a backtrace of the current stack trace of the current object", "response": "def t(inspect_packages=False, depth=0, **kwargs):\n    '''\n    print a backtrace\n\n    since -- 7-6-12\n\n    inpsect_packages -- boolean -- by default, this only prints code of packages that are not \n        in the pythonN directories, that cuts out a lot of the noise, set this to True if you\n        want a full stacktrace\n    depth -- integer -- how deep you want the stack trace to print (ie, if you only care about\n        the last three calls, pass in depth=3 so you only get the last 3 rows of the stack)\n    '''\n\n\n    #frame = inspect.currentframe()\n\n    try:\n        frames = inspect.stack()\n        kwargs[\"frames\"] = frames\n        kwargs[\"inspect_packages\"] = inspect_packages\n        kwargs[\"depth\"] = depth\n\n        with Reflect.context(**kwargs) as r:\n            instance = T_CLASS(r, stream, **kwargs)\n            instance()\n\n    finally:\n        del frames"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninject pout into the builtins module so it can be called from anywhere without having to be explicitely imported", "response": "def inject():\n    \"\"\"Injects pout into the builtins module so it can be called from anywhere without\n    having to be explicitely imported, this is really just for convenience when\n    debugging\n\n    https://stackoverflow.com/questions/142545/python-how-to-make-a-cross-module-variable\n    \"\"\"\n    try:\n        from .compat import builtins\n\n        module = sys.modules[__name__]\n        setattr(builtins, __name__, module)\n        #builtins.pout = pout\n\n    except ImportError:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getH264V4l2(verbose=False):\n    import glob\n    from subprocess import Popen, PIPE\n    \n    cams=[]\n\n    for device in glob.glob(\"/sys/class/video4linux/*\"):\n        devname=device.split(\"/\")[-1]\n        devfile=os.path.join(\"/dev\",devname)\n        \n        lis=(\"v4l2-ctl --list-formats -d \"+devfile).split()\n\n        p = Popen(lis, stdout=PIPE, stderr=PIPE)\n        # p.communicate()\n        # print(dir(p))\n        # print(p.returncode)\n        # print(p.stderr.read().decode(\"utf-8\"))\n        st = p.stdout.read().decode(\"utf-8\")\n        # print(st)\n        \n        if (st.lower().find(\"h264\")>-1):\n            namefile=os.path.join(device, \"name\")\n            # print(namefile)\n            f=open(namefile, \"r\"); name=f.read(); f.close()\n            fullname = name.strip() + \" (\"+devname+\")\"\n            cams.append((devfile, fullname))\n\n    if (verbose):\n        for cam in cams:\n            print(cam)\n        \n    return cams", "response": "Find all V4l2 cameras with H264 encoding and return a list of tuples with.."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_static_file(self, filename):\n        if self.config['MODE'] == 'api-only':\n            # if 'api-only' mode is set, we should not send static files\n            abort(404)\n\n        theme_static_folder = getattr(self, 'theme_static_folder', None)\n        if theme_static_folder:\n            try:\n                return send_from_directory(theme_static_folder, filename)\n            except NotFound:\n                pass\n        return super(CustomFlask, self).send_static_file(filename)", "response": "Send static files from the static folder in the current selected theme prior to the global static folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setDevice(self, device): \n        print(self.pre, \"setDevice :\", device)\n        \n        if (not device and not self.device): # None can be passed as an argument when the device has not been set yet\n            return\n            \n        if (self.device):\n            if self.device == device:\n                print(self.pre, \"setDevice : same device\")\n                return\n            \n        if self.filterchain: # there's video already\n            self.clearDevice()\n        \n        self.device = device\n        self.video.setDevice(self.device) # inform the video widget so it can start drags\n        \n        # ManagedFilterChain.addViewPort accepts ViewPort instance\n        self.filterchain = self.filterchain_group.get(_id = self.device._id)\n        \n        if self.filterchain:\n            self.viewport.setXScreenNum(self.n_xscreen)\n            self.viewport.setWindowId  (int(self.video.winId()))\n            self.filterchain.addViewPort(self.viewport)", "response": "Sets the video stream to the given device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clearDevice(self):\n        print(self.pre, \"clearDevice\")\n        if not self.device:\n            return\n        \n        self.filterchain.delViewPort(self.viewport)\n        \n        self.filterchain = None\n        self.device = None\n        \n        self.video.update()", "response": "Remove the current stream and clear the filterchain"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_left_double_click(self, info):\n        if (self.double_click_focus == False):  # turn focus on\n            print(self.pre, \"handle_left_double_click: focus on\")\n            self.cb_focus()\n        else:  # turn focus off\n            print(self.pre, \"handle_left_double_click: focus off\")\n            self.cb_unfocus()\n        self.double_click_focus = not(\n            self.double_click_focus)", "response": "This function is called when the left button is double - clicked with the left button."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget storage object of current app context.", "response": "def get_storage():\n    \"\"\"\n    Get storage object of current app context,\n    will create a new one if not exists.\n\n    :return: a storage object\n    :raise: ConfigurationError: storage type in config is not supported\n    \"\"\"\n    storage_ = getattr(g, '_storage', None)\n    if storage_ is None:\n        storage_type = current_app.config['STORAGE_TYPE']\n        if storage_type == 'file':\n            storage_ = g._storage = storages.FileStorage()\n        else:\n            raise ConfigurationError(\n                'Storage type \"{}\" is not supported.'.format(storage_type))\n    return storage_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall this slot chooses the form to be shown :param element: an object that has *_id* and *classname* attributes :param element_old: an object that has *_id* and *classname* attributes This slot is typically connected to List classes, widget attribute's, currentItemChanged method (List.widget is QListWidget that has currentItemChanged slot), so the element and element_old parameters are QListWidgetItem instances with extra attributes \"_id\" and \"_classname\" attached. Queries the database for element._id", "response": "def chooseForm_slot(self, element, element_old):\n        \"\"\"Calling this slot chooses the form to be shown\n\n        :param element:      an object that has *_id* and *classname* attributes\n        :param element_old:  an object that has *_id* and *classname* attributes\n\n        This slot is typically connected to List classes, widget attribute's, currentItemChanged method (List.widget is QListWidget that has currentItemChanged slot), so the element and element_old parameters are QListWidgetItem instances with extra attributes \"_id\" and \"_classname\" attached.\n\n        Queries the database for element._id\n        \"\"\"\n\n        self.current_slot = None\n\n        if (verbose):\n            # enable this if you're unsure what's coming here..\n            print(self.pre, \"chooseForm_slot :\", element)\n        if (isinstance(element, type(None))):\n            self.current_row = None\n            self.element = None\n        else:\n            # print(self.pre,\"chooseForm_slot :\",element)\n            assert(hasattr(element, \"_id\"))\n            assert(hasattr(element, \"classname\"))\n            try:\n                self.current_row = self.row_instance_by_name[element.classname]\n            except KeyError:\n                print(\n                    self.pre,\n                    \"chooseForm_slot : no such classname for this FormSet : \",\n                    element.classname)\n                self.current_row = None\n                self.element = None\n            else:\n                self.resetForm()\n                self.current_row.get(self.collection, element._id)\n                self.element = element\n                self.current_slot = self.current_row.get_column_value(\"slot\")\n\n        self.showCurrent()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nkeeping updating the dropdown list.", "response": "def update_dropdown_list_slot(self):\n        \"\"\"Keep updating the dropdown list.  Say, don't let the user choose USB devices if none is available\n        \"\"\"\n        self.dropdown_widget.clear() # this will trigger dropdown_changed_slot\n        self.row_instance_by_index = []\n        for i, key in enumerate(self.row_instance_by_name.keys()):\n            row_instance = self.row_instance_by_name[key]\n            if (row_instance.isActive()):\n                self.row_instance_by_index.append(row_instance)\n                display_name = row_instance.getName()\n                self.dropdown_widget.insertItem(i, display_name)\n            row_instance.updateWidget()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self):\n        self.reset()\n        for dic in self.datamodel.camera_collection.get(): # TODO: search directly for RTSPCameraRow\n            if (self.verbose): print(self.pre, \"read : dic\", dic)\n            \n            if (dic[\"classname\"] == DataModel.RTSPCameraRow.__name__):\n                affinity = -1\n                if self.cpu_scheme:\n                    affinity = self.cpu_scheme.getAV()\n                \n                dic.pop(\"classname\")\n                device = DataModel.RTSPCameraDevice(**dic) # a neat object with useful methods\n                \n                print(\"FilterChainGroup : read : slot    =\", device.getLiveMainSlot())\n                print(\"FilterChainGroup : read : address =\", device.getMainAddress())\n                print(\"FilterChainGroup : read : _id     =\", device._id)\n                \n                # chain = ManagedFilterchain( # decoding and branching the stream happens here\n                # chain = ManagedFilterchain2( # decoding and branching the stream happens here\n                chain = LiveManagedFilterchain( # decoding and branching the stream happens here\n                    livethread  = self.livethread,\n                    openglthreads\n                                = self.gpu_handler.openglthreads,\n                    address     = device.getMainAddress(),\n                    slot        = device.getLiveMainSlot(),\n                    _id         = device._id,\n                    affinity    = affinity,\n                    msreconnect = 10000,\n                    # verbose     = True,\n                    verbose     =False,\n                    \n                    shmem_image_dimensions = constant.shmem_image_dimensions,\n                    shmem_n_buffer = constant.shmem_n_buffer,\n                    shmem_image_interval = constant.shmem_image_interval\n                )\n                self.chains.append(chain) # important .. otherwise chain will go out of context and get garbage collected\n                \n            elif (dic[\"classname\"] == DataModel.USBCameraRow.__name__):\n                affinity = -1\n                if self.cpu_scheme:\n                    affinity = self.cpu_scheme.getAV()\n                \n                dic.pop(\"classname\")\n                device = DataModel.USBCameraDevice(**dic) # a neat object with useful methods\n                \n                print(\"FilterChainGroup : read : slot    =\", device.getLiveMainSlot())\n                print(\"FilterChainGroup : read : address =\", device.getMainAddress())\n                print(\"FilterChainGroup : read : _id     =\", device._id)\n                \n                chain = USBManagedFilterchain( # decoding and branching the stream happens here\n                    usbthread   = self.usbthread,\n                    openglthreads\n                                = self.gpu_handler.openglthreads,\n                    address     = device.getMainAddress(),\n                    slot        = device.getLiveMainSlot(),\n                    _id         = device._id,\n                    affinity    = affinity,\n                    msreconnect = 10000,\n                    # verbose     = True,\n                    verbose     =False,\n                    \n                    shmem_image_dimensions = constant.shmem_image_dimensions,\n                    shmem_n_buffer = constant.shmem_n_buffer,\n                    shmem_image_interval = constant.shmem_image_interval\n                )\n                self.chains.append(chain)", "response": "Reads all devices from the database and creates a list of filterchains that are created for each."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n        raise(AssertionError(\"out of date\"))\n        \n        new_ids = []\n        old_ids = []\n        \n        # collect old ip addresses\n        for chain in self.chains:\n            if (self.verbose): print(self.pre, \"old :\", chain, chain.get__id(), chain.get_address(), chain._id)\n            old_ids.append(chain.get__id())\n            \n        # collect devices\n        devices = []\n        for dic in self.datamodel.camera_collection.get():\n            if (self.verbose): print(self.pre, \"update : dic\", dic)\n            if (dic[\"classname\"] == DataModel.RTSPCameraRow.__name__):\n                devices.append(dic)\n            \n        devices_by_id={}\n        for dic in devices: # DataModel.RTSPCameraRow instances\n            _id = dic[\"_id\"]\n            new_ids.append(_id)\n            devices_by_id[_id] = dic\n        \n        if (self.verbose):\n            print(self.pre, \"update : new_ids =\", new_ids)\n            print(self.pre, \"update : old_ids =\", old_ids)\n        \n        add_list = list(set(new_ids).difference(set(old_ids))) # cams to be added\n        rem_list = list(set(old_ids).difference(set(new_ids))) # cams to be removed\n        \n        if (self.verbose):\n            print(self.pre, \"update : add_list =\", add_list)\n            print(self.pre, \"update : rem_list =\", rem_list)\n        \n        # purge removed chains\n        for i, chain in enumerate(self.chains):\n            if (chain.get__id() in rem_list):\n                chain_ = self.chains.pop(i)\n                if (self.verbose): print(self.pre, \"closing chain\", chain_)\n                chain_.close()\n        \n        # add new chains\n        for new_address in add_list:\n            dic = devices_by_id[new_address]\n            chain = ManagedFilterchain( # decoding and branching the stream happens here\n                livethread  = self.livethread,\n                openglthreads\n                            = self.gpu_handler.openglthreads,\n                address     = DataModel.RTSPCameraRow.getMainAddressFromDict(dic),\n                slot        = dic[\"slot\"],\n                _id         = dic[\"_id\"],\n                # affinity    = a,\n                msreconnect = 10000,\n                verbose = True\n            )\n            if (self.verbose): print(self.pre, \"adding chain\", chain)\n            self.chains.append(chain)", "response": "Reads all devices from the database and updates the filterchains with new ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, **kwargs):\n        for chain in self.chains:\n            for key in kwargs:\n                getter_name = \"get_\"+key\n                # scan all possible getters\n                if (hasattr(chain, getter_name)):\n                    getter = getattr(chain, getter_name) # e.g. \"get_address\"\n                    if (getter() == kwargs[key]):\n                        return chain\n        return None", "response": "Find correct filterchain based on generic variables\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getDevice(self, **kwargs): \n        filterchain = self.get(**kwargs)\n        \n        if not filterchain:\n            return None\n        \n        # get filterchain init parameters that are compatible with RTSPCameraDevice input parameters\n        pars = filterchain.getParDic(DataModel.RTSPCameraDevice.parameter_defs) \n        # .. and instantiate an RTSPCameraDevice with those parameters\n        device = DataModel.RTSPCameraDevice(**pars)\n        \n        print(self.pre, \"getDevice :\", pars, device)\n        \n        return device", "response": "Like get but returns a Device instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dict(self):\n        return {k: getattr(self, k) for k in filter(\n            lambda k: not k.startswith('_') and k != 'to_dict', dir(self))}", "response": "Convert attributes and properties to a dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting this object to a dict object for serialization.", "response": "def to_dict(self):\n        \"\"\"Convert self to a dict object for serialization.\"\"\"\n        return {\n            'level': self.level,\n            'id': self.id,\n            'text': self.text,\n            'inner_html': self.inner_html,\n            'children': [child.to_dict() for child in self.children]\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the table of currently fed HTML string.", "response": "def toc(self, depth=6, lowest_level=6):\n        \"\"\"\n        Get table of content of currently fed HTML string.\n\n        :param depth: the depth of TOC\n        :param lowest_level: the allowed lowest level of header tag\n        :return: a list representing the TOC\n        \"\"\"\n        depth = min(max(depth, 0), 6)\n        depth = 6 if depth == 0 else depth\n        lowest_level = min(max(lowest_level, 1), 6)\n        toc = self._root.to_dict()['children']\n\n        def traverse(curr_toc, dep, lowest_lvl, curr_depth=1):\n            if curr_depth > dep:\n                # clear all items of this depth and exit the recursion\n                curr_toc.clear()\n                return\n\n            items_to_remove = []\n            for item in curr_toc:\n                if item['level'] > lowest_lvl:\n                    # record item with low header level, for removing it later\n                    items_to_remove.append(item)\n                else:\n                    traverse(item['children'], dep, lowest_lvl, curr_depth + 1)\n            [curr_toc.remove(item) for item in items_to_remove]\n\n        traverse(toc, depth, lowest_level)\n        return toc"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toc_html(self, depth=6, lowest_level=6):\n        toc = self.toc(depth=depth, lowest_level=lowest_level)\n        if not toc:\n            return ''\n\n        def map_toc_list(toc_list):\n            result = ''\n            if toc_list:\n                result += '<ul>\\n'\n                result += ''.join(\n                    map(lambda x: '<li>'\n                                  '<a href=\"#{}\">{}</a>{}'\n                                  '</li>\\n'.format(\n                        x['id'], x['inner_html'],\n                        map_toc_list(x['children'])),\n                        toc_list)\n                )\n                result += '</ul>'\n            return result\n\n        return map_toc_list(toc)", "response": "Return the TOC of currently fed HTML string in form of HTML string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the level of the header tag.", "response": "def _get_level(tag):\n        \"\"\"\n        Match the header level in the given tag name,\n        or None if it's not a header tag.\n        \"\"\"\n        m = re.match(r'^h([123456])$', tag, flags=re.IGNORECASE)\n        if not m:\n            return None\n        return int(m.group(1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef requiredGPU_MB(self, n):\n        from darknet.core import darknet_with_cuda\n        if (darknet_with_cuda()): # its using cuda\n            free = getFreeGPU_MB()\n            print(\"Yolo: requiredGPU_MB: required, free\", n, free)\n            if (free == -1): # could not detect ..\n                return True\n            return (free>=n)\n        else:\n            return True", "response": "Check if the required GPU memory in MBytes is available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef postActivate_(self):\n        if (self.requiredGPU_MB(self.required_mb)):\n            self.analyzer = YoloV3Analyzer(verbose = self.verbose)\n        else:\n            self.warning_message = \"WARNING: not enough GPU memory!\"\n            self.analyzer = None", "response": "Whatever you need to do after creating the shmem client\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the widget that will be used to display the textual information for the current locale.", "response": "def getWidget(self):\n        \"\"\"Some ideas for your widget:\n        - Textual information (alert, license place number)\n        - Check boxes : if checked, send e-mail to your mom when the analyzer spots something\n        - .. or send an sms to yourself\n        - You can include the cv2.imshow window to the widget to see how the analyzer proceeds\n        \"\"\"\n        self.widget = QtWidgets.QTextEdit()\n        self.widget.setStyleSheet(style.detector_test)\n        self.widget.setReadOnly(True)\n        self.signals.objects.connect(self.objects_slot)\n        return self.widget"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the type of the object in the current module", "response": "def typename(self):\n        '''\n        get the type of val\n\n        there are multiple places where we want to know if val is an object, or a string, or whatever,\n        this method allows us to find out that information\n\n        since -- 7-10-12\n\n        val -- mixed -- the value to check\n\n        return -- string -- the type\n        '''\n        t = 'DEFAULT'\n        # http://docs.python.org/2/library/types.html\n#         func_types = (\n#             types.FunctionType,\n#             types.BuiltinFunctionType,\n#             types.MethodType,\n#             types.UnboundMethodType,\n#             types.BuiltinFunctionType,\n#             types.BuiltinMethodType,\n#             classmethod\n#         )\n\n        if self.is_primitive():\n            t = 'DEFAULT'\n\n        elif self.is_dict():\n            t = 'DICT'\n\n        elif self.is_list():\n            t = 'LIST'\n\n        elif self.is_array():\n            t = 'ARRAY'\n\n        elif self.is_tuple():\n            t = 'TUPLE'\n\n        elif self.is_type():\n            t = 'TYPE'\n\n        elif self.is_binary():\n            t = 'BINARY'\n\n        elif self.is_str():\n            t = 'STRING'\n\n        elif self.is_exception():\n            t = 'EXCEPTION'\n\n        elif self.is_module():\n            # this has to go before the object check since a module will pass the object tests\n            t = 'MODULE'\n\n        elif self.is_callable():\n            t = 'FUNCTION'\n\n            # not doing this one since it can cause the class instance to do unexpected\n            # things just to print it out\n            #elif isinstance(val, property):\n            # uses the @property decorator and the like\n            #t = 'PROPERTY'\n\n        elif self.is_dict_proxy():\n            # maybe we have a dict proxy?\n            t = 'DICT_PROXY'\n\n        elif self.is_generator():\n            t = 'GENERATOR'\n\n        elif self.is_set():\n            t = 'SET'\n\n        elif self.is_object():\n            t = 'OBJECT'\n\n#         elif isinstance(val, func_types) and hasattr(val, '__call__'):\n#             # this has to go after object because lots of times objects can be classified as functions\n#             # http://stackoverflow.com/questions/624926/\n#             t = 'FUNCTION'\n\n        elif self.is_regex():\n            t = 'REGEX'\n\n        else:\n            t = 'DEFAULT'\n\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbeing the value a primitive type?", "response": "def is_primitive(self):\n        \"\"\"is the value a built-in type?\"\"\"\n        if is_py2:\n            return isinstance(\n                self.val, \n                (\n                    types.NoneType,\n                    types.BooleanType,\n                    types.IntType,\n                    types.LongType,\n                    types.FloatType\n                )\n            )\n\n        else:\n            return isinstance(\n                self.val,\n                (\n                    type(None),\n                    bool,\n                    int,\n                    float\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nturning an iteratable value into a string representation", "response": "def _str_iterator(self, iterator, name_callback=None, prefix=\"\\n\", left_paren='[', right_paren=']', depth=0):\n        '''\n        turn an iteratable value into a string representation\n\n        iterator -- iterator -- the value to be iterated through\n        name_callback -- callback -- if not None, a function that will take the key of each iteration\n        prefix -- string -- what will be prepended to the generated value\n        left_paren -- string -- what will open the generated value\n        right_paren -- string -- what will close the generated value\n        depth -- integer -- how deep into recursion we are\n\n        return -- string\n        '''\n        indent = 1 if depth > 0 else 0\n\n        s = []\n        s.append('{}{}'.format(prefix, self._add_indent(left_paren, indent)))\n\n        s_body = []\n\n        for k, v in iterator:\n            k = k if name_callback is None else name_callback(k)\n            v = Value(v, depth+1)\n            try:\n                # TODO -- right here we should check some flag or something to\n                # see if lists should render objects\n                if k is None:\n                    s_body.append(\"{}\".format(v))\n                else:\n                    s_body.append(\"{}: {}\".format(k, v))\n\n            except RuntimeError as e:\n                # I've never gotten this to work\n                s_body.append(\"{}: ... Recursion error ...\".format(k))\n\n            except UnicodeError as e:\n                print(v.val)\n                print(type(v.val))\n\n        s_body = \",\\n\".join(s_body)\n        s_body = self._add_indent(s_body, indent + 1)\n\n        s.append(s_body)\n        s.append(\"{}\".format(self._add_indent(right_paren, indent)))\n\n        return \"\\n\".join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding whitespace to the beginning of each line of val with more whitespace", "response": "def _add_indent(self, val, indent_count):\n        '''\n        add whitespace to the beginning of each line of val\n\n        link -- http://code.activestate.com/recipes/66055-changing-the-indentation-of-a-multi-line-string/\n\n        val -- string\n        indent -- integer -- how much whitespace we want in front of each line of val\n\n        return -- string -- val with more whitespace\n        '''\n        if isinstance(val, Value):\n            val = val.string_value()\n\n        return String(val).indent(indent_count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps around getattr that suppresses any exception raised", "response": "def _getattr(self, val, key, default_val):\n        \"\"\"wrapper around global getattr(...) method that suppresses any exception raised\"\"\"\n        try:\n            ret = getattr(val, key, default_val)\n\n        except Exception as e:\n            logger.exception(e)\n            ret = default_val\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the full name of the object", "response": "def _get_name(self, val, src_file, default='Unknown'):\n        '''\n        get the full namespaced (module + class) name of the val object\n\n        since -- 6-28-12\n\n        val -- mixed -- the value (everything is an object) object\n        default -- string -- the default name if a decent name can't be found programmatically\n\n        return -- string -- the full.module.Name\n        '''\n        module_name = ''\n        if src_file:\n            module_name = '{}.'.format(self._getattr(val, '__module__', default)).lstrip('.')\n\n        class_name = self._getattr(val, '__name__', None)\n        if not class_name:\n            class_name = default\n            cls = self._getattr(val, '__class__', None)\n            if cls:\n                class_name = self._getattr(cls, '__name__', default)\n\n        full_name = \"{}{}\".format(module_name, class_name)\n\n        return full_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the source file path since 7 - 19 - 12 val return the string", "response": "def _get_src_file(self, val, default='Unknown'):\n        '''\n        return the source file path\n\n        since -- 7-19-12\n\n        val -- mixed -- the value whose path you want\n\n        return -- string -- the path, or something like 'Unknown' if you can't find the path\n        '''\n        path = default\n\n        try:\n            # http://stackoverflow.com/questions/6761337/inspect-getfile-vs-inspect-getsourcefile\n            # first try and get the actual source file\n            source_file = inspect.getsourcefile(val)\n            if not source_file:\n                # get the raw file since val doesn't have a source file (could be a .pyc or .so file)\n                source_file = inspect.getfile(val)\n\n            if source_file:\n                path = os.path.realpath(source_file)\n\n        except TypeError as e:\n            path = default\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makeWidget(self, qscreen: QtGui.QScreen):\n        # (re)create the widget, do the same for children\n        # how children are placed on the parent widget, depends on the subclass\n        self.window = self.ContainerWindow(\n            self.signals, self.title, self.parent)\n\n        # send to correct x-screen\n        self.window.show()\n        self.window.windowHandle().setScreen(qscreen)\n        self.n_xscreen = self.gpu_handler.getXScreenNum(qscreen) # the correct x-screen number must be passed upstream, to the VideoContainer\n\n        # continue window / widget construction in the correct x screen\n        self.main_widget = self.ContainerWidget(self.window)\n        self.main_layout = QtWidgets.QVBoxLayout(self.main_widget)\n        self.window.setCentralWidget(self.main_widget)\n\n        # add here any extra turf to the widget you want in addition to the\n        # grid\n\n        # create the grid\n        self.grid_widget = self.GridWidget(self.main_widget)\n        self.main_layout.addWidget(self.grid_widget)\n\n        self.grid_layout = QtWidgets.QGridLayout(self.grid_widget)\n        self.grid_layout.setHorizontalSpacing(2)\n        self.grid_layout.setVerticalSpacing(2)\n        # ( int left, int top, int right, int bottom )\n        self.grid_layout.setContentsMargins(0, 0, 0, 0)\n\n        class ScreenMenu(QuickMenu):\n            title = \"Change Screen\"\n            elements = [\n                QuickMenuElement(title=\"Screen 1\"),\n                QuickMenuElement(title=\"Screen 2\")\n            ]\n            \n        \"\"\" TODO: activate after gpu-hopping has been debugged\n        self.screenmenu = ScreenMenu(self.window)\n        self.screenmenu.screen_1.triggered.connect(self.test_slot)\n        self.screenmenu.screen_2.triggered.connect(self.test_slot)\n        \"\"\"\n\n        if (len(self.gpu_handler.true_screens) > 1):\n            # so, there's more than a single x screen: create a button for\n            # changing x-screens\n            self.button = QtWidgets.QPushButton(\n                \"Change Screen\", self.main_widget)\n            self.main_layout.addWidget(self.button)\n            self.button.setSizePolicy(\n                QtWidgets.QSizePolicy.Minimum,\n                QtWidgets.QSizePolicy.Minimum)\n            self.button.clicked.connect(self.change_xscreen_slot)\n\n        self.placeChildren()", "response": "create the widget and add it to the main_widget and grid_widget"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        if (self.closed):\n            return\n        print(self.pre, \"close\")\n        for child in self.children:\n            child.close()\n        self.openglthread = None\n        self.gpu_handler = None\n        self.closed = True\n        self.window.unSetPropagate() # we don't want the window to send the close signal .. which would call this *again* (through close_slot)\n        self.window.close()", "response": "Called by the main gui to close the containers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize information about the widget", "response": "def serialize(self):\n        \"\"\"Serialize information about the widget: coordinates, size, which cameras are selected.\n        \"\"\"\n        ids = []\n        for child in self.children:\n            device = child.getDevice()\n            if device:\n                ids.append(device._id) # e.g. DataModel.RTSPCameraDevice._id\n            else:\n                ids.append(None)\n\n        # gather all information to re-construct this RootVideoContainer\n        dic = {  # these are used when re-instantiating the view\n            \"classname\": self.__class__.__name__,\n            \"kwargs\": { # parameters that we're used to instantiate this class\n                \"title\"                 : self.title,\n                \"n_xscreen\"             : self.n_xscreen,\n                \"child_class\"           : self.child_class,\n                \"child_class_pars\"      : self.get_child_class_pars() # serialize only relevant child class pars\n                },  \n            # these parameters are used by deserialize\n            \"x\": self.window.x(),\n            \"y\": self.window.y(),\n            \"width\": self.window.width(),\n            \"height\": self.window.height(),\n            \"ids\": ids\n        }\n\n        return dic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the analyzer state", "response": "def reset(self):\n        \"\"\"Reset analyzer state\n        \"\"\"\n        self.prevframe = None\n        self.wasmoving = False\n        self.t0 = 0\n        self.ismoving = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the widget that will be used to display some information about the current locale.", "response": "def getWidget(self):\n        \"\"\"Some ideas for your widget:\n        - Textual information (alert, license place number)\n        - Check boxes : if checked, send e-mail to your mom when the analyzer spots something\n        - .. or send an sms to yourself\n        - You can include the cv2.imshow window to the widget to see how the analyzer proceeds\n        \"\"\"\n        widget = QtWidgets.QLabel(\"NO MOVEMENT YET\")\n        widget.setStyleSheet(style.detector_test)\n        self.signals.start_move.connect(lambda : widget.setText(\"MOVEMENT START\"))\n        self.signals.stop_move. connect(lambda : widget.setText(\"MOVEMENT STOP\"))\n        return widget"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rule(rules, strict_slashes=False, api_func=None, *args, **kwargs):\n    return url_rule(api_blueprint, rules, strict_slashes=strict_slashes,\n                    view_func=json_api(api_func) if api_func else None,\n                    *args, **kwargs)", "response": "Add a API route to the blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind out which screens are virtual and adds them to self. true_screens", "response": "def findXScreens(self):\n        qapp = QtCore.QCoreApplication.instance()\n        if not qapp: # QApplication has not been started\n            return\n        \n        screens = qapp.screens()\n        \"\"\"\n        let's find out which screens are virtual\n\n        screen, siblings:\n\n        One big virtual desktop:\n\n        A [A, B, C]\n        B [A, B, C]\n        C [A, B, C]\n\n        A & B in one xscreen, C in another:\n\n        A [A, B]\n        B [A, B]\n        C [C]\n\n        \"\"\"\n        virtual_screens = set()\n        for screen in screens:\n            # if screen has been deemed as \"virtual\", don't check its siblings\n            if (screen not in virtual_screens):\n                siblings = screen.virtualSiblings()\n                # remove the current screen under scrutiny from the siblings\n                # list\n                virtual_screens.update(set(siblings).difference(set([screen])))\n                # .. the ones left over are virtual\n\n        # print(\"GPUHandler: findXScreens: virtual screens\",virtual_screens)\n        true_screens = list(set(screens) - virtual_screens)\n\n        # sort'em\n        for screen in true_screens:\n            self.true_screens.insert(screens.index(screen), screen)\n\n        print(\"GPUHandler: findXScreens: true screens:\", self.true_screens)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readDB(self):\n        self.dm = DataModel(directory = tools.getConfigDir())\n        if (self.first_start):\n            print(pre, \"readDB : first start\")\n            self.dm.clearAll()\n            self.dm.saveAll()\n\n        # If camera collection is corrupt\n        if not self.dm.checkCameraCollection():\n            self.dm.clearCameraCollection()", "response": "Reads the data model and saves it to disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating some member functions that are needed for the class.", "response": "def generateMethods(self):\n        \"\"\"Generate some member functions\n        \"\"\"\n        for i in range(1, 5):\n            # adds member function grid_ixi_slot(self)\n            self.make_grid_slot(i, i)\n\n        for cl in self.mvision_classes:\n            self.make_mvision_slot(cl)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef QCapsulate(self, widget, name, blocking = False, nude = False):\n\n        class QuickWindow(QtWidgets.QMainWindow):\n\n            class Signals(QtCore.QObject):\n                close = QtCore.Signal()\n                show  = QtCore.Signal()\n\n            def __init__(self, blocking = False, parent = None, nude = False):\n                super().__init__(parent)\n                self.propagate = True # send signals or not\n                self.setStyleSheet(style.main_gui)\n                if (blocking):\n                    self.setWindowModality(QtCore.Qt.ApplicationModal)\n                if (nude):\n                    # http://doc.qt.io/qt-5/qt.html#WindowType-enum\n                    # TODO: create a widget for a proper splashscreen (omitting X11 and centering manually)\n                    # self.setWindowFlags(QtCore.Qt.Popup) # Qt 5.9+ : setFlags()\n                    # self.setWindowFlags(QtCore.Qt.SplashScreen | QtCore.Qt.WindowStaysOnTopHint)\n                    self.setWindowFlags(QtCore.Qt.Dialog)\n                self.signals = self.Signals()\n                \n\n            def closeEvent(self, e):\n                if (self.propagate):\n                    self.signals.close.emit()\n                e.accept()\n                \n            def showEvent(self, e):\n                if (self.propagate):\n                    self.signals.show.emit()\n                e.accept()\n                \n            def setPropagate(self):\n                self.propagate = True\n                \n            def unSetPropagate(self):\n                self.propagate = False\n                \n\n        win = QuickWindow(blocking = blocking, nude = nude)\n        win.setCentralWidget(widget)\n        win.setLayout(QtWidgets.QHBoxLayout())\n        win.setWindowTitle(name)\n        return win", "response": "Returns a QMainWindow that encapsulates the given QWidget and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef updateCameraTree(self):\n        self.treelist.reset_()\n\n        self.server = ServerListItem(\n            name = \"Localhost\", ip = \"127.0.0.1\", parent = self.root)\n        \"\"\"\n        self.server1 = ServerListItem(\n            name=\"First Server\", ip=\"192.168.1.20\", parent=self.root)\n        \"\"\"\n        \"\"\"\n        self.camera1 = RTSPCameraListItem(camera=RTSPCameraDevice(\n            ip=\"192.168.1.4\", username=\"admin\", password=\"1234\"), parent=self.server1)\n        self.camera2 = RTSPCameraListItem(camera=RTSPCameraDevice(\n            ip=\"192.168.1.4\", username=\"admin\", password=\"1234\"), parent=self.server1)\n        \"\"\"\n        devices = []\n\n        for row in self.dm.camera_collection.get():\n            # print(pre, \"makeCameraTree : row\", row)\n            if (row[\"classname\"] == DataModel.RTSPCameraRow.__name__):\n                row.pop(\"classname\")\n                devices.append(\n                    RTSPCameraListItem(\n                        camera = DataModel.RTSPCameraDevice(**row),\n                        parent = self.server\n                    )\n                )\n            elif (row[\"classname\"] == DataModel.USBCameraRow.__name__):\n                row.pop(\"classname\")\n                devices.append(\n                    USBCameraListItem(\n                        camera = DataModel.USBCameraDevice(**row),\n                        parent = self.server\n                    )\n                )\n                \n        self.treelist.update()\n        self.treelist.expandAll()", "response": "Update the internal list of all the camera entries in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects the events to the menu bar and the menu bar.", "response": "def makeLogic(self):\n        # *** When camera list has been closed, re-create the cameralist tree and update filterchains ***\n        # self.manage_cameras_win.signals.close.connect(self.updateCameraTree) # now put into save_camera_config_slot\n        \n        # self.manage_cameras_win.signals.close.connect(self.filterchain_group.update) # TODO: use this once fixed\n        # self.manage_cameras_win.signals.close.connect(self.filterchain_group.read) # TODO: eh.. lets be sure of this .. (are we releasing slots in the LiveThread etc.)\n        \n        # self.manage_cameras_win.signals.close.connect(self.save_camera_config_slot)\n        # self.manage_memory_container.signals.save.connect(self.save_memory_conf_slot)\n\n        # *** Menu bar connections ***\n        # the self.filemenu.exit attribute was autogenerated\n        self.filemenu.exit.               triggered.connect(self.exit_slot)\n        self.filemenu.save_window_layout. triggered.connect(\n            self.save_window_layout_slot)\n        self.filemenu.load_window_layout. triggered.connect(\n            self.load_window_layout_slot)\n\n        \"\"\"\n        self.configmenu.manage_cameras.   triggered.connect(\n            self.manage_cameras_slot)\n        self.configmenu.memory_usage.     triggered.connect(\n            self.memory_usage_slot)\n        \"\"\"\n        \n        self.configmenu.configuration_dialog.triggered.connect(self.config_dialog_slot)\n        \n        self.viewmenu.camera_list.        triggered.connect(self.camera_list_slot)\n        self.aboutmenu.about_valkka_live. triggered.connect(self.about_slot)\n\n        # *** Connect autogenerated menu calls into autogenerated slot functions ***\n        for i in range(1, 5):\n            # gets member function grid_ixi_slot\n            slot_func = getattr(self, \"grid_%ix%i_slot\" % (i, i))\n            # gets member function grid_ixi from self.viewmenu.video_grid\n            menu_func = getattr(self.viewmenu.video_grid,\n                                \"grid_%ix%i\" % (i, i))\n            menu_func.triggered.connect(slot_func)\n            # i.e., like this : self.viewmenu.video_grid.grid_1x1.triggered.connect(slot_func)\n\n\n        # *** autogenerated machine vision menu and slots ***\n        for cl in self.mvision_classes:\n            getattr(self.mvisionmenu,cl.name).triggered.connect(getattr(self,cl.name+\"_slot\"))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serializeContainers(self):\n\n        \"\"\" each serialized container looks like this:\n        dic={# these are used when re-instantiating the view\n            \"classname\"  : self.__class__.__name__,\n            \"kwargs\"     : {}, # parameters that we're used to instantiate this class\n            # these parameters are used by deserialize\n            \"x\"          : self.window.x(),\n            \"y\"          : self.window.y(),\n            \"width\"      : self.window.width(),\n            \"height\"     : self.window.height(),\n            \"streams\"    : streams\n            }\n        \"\"\"\n        container_list = []\n        mvision_container_list = []\n        \n        for container in self.containers:\n            print(\"gui: serialize containers : container=\", container)\n            container_list.append(container.serialize())\n        \n        for container in self.mvision_containers:\n            mvision_container_list.append(container.serialize())\n            \n        return {\"container_list\" : container_list, \"mvision_container_list\" : mvision_container_list}", "response": "Serializes the current view of open video grids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef startProcesses(self):\n        self.process_map = {} # each key is a list of started multiprocesses\n        # self.process_avail = {} # count instances\n        \n        for mvision_class in self.mvision_classes:\n            name = mvision_class.name\n            tag  = mvision_class.tag\n            num  = mvision_class.max_instances        \n            if (tag not in self.process_map):\n                self.process_map[tag] = []\n                # self.process_avail[tag] = num\n                for n in range(0, num):\n                    p = mvision_class()\n                    p.start()\n                    self.process_map[tag].append(p)", "response": "Create and start python multiprocesses\nAttributeNames\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_grid_slot(self, n, m):\n        def slot_func():\n            cont = container.VideoContainerNxM(gpu_handler=self.gpu_handler, \n                                                    filterchain_group=self.filterchain_group, \n                                                    n_dim=n, \n                                                    m_dim=m)\n            cont.signals.closing.connect(self.rem_container_slot)\n            self.containers.append(cont)\n        setattr(self, \"grid_%ix%i_slot\" % (n, m), slot_func)", "response": "Create a n x m video grid and add it to the list of video containers that are used by the grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmapping to pout.inject on the command line, makes it easy to make pout global without having to actually import it in your python environment .. since:: 2018-08-13 :param args: Namespace, the parsed CLI arguments passed into the application :returns: int, the return code of the CLI", "response": "def main_inject(args):\n    \"\"\"\n    mapped to pout.inject on the command line, makes it easy to make pout global\n    without having to actually import it in your python environment\n\n    .. since:: 2018-08-13\n\n    :param args: Namespace, the parsed CLI arguments passed into the application\n    :returns: int, the return code of the CLI\n    \"\"\"\n    ret = 0\n\n    try:\n        filepath = SiteCustomizeFile()\n        if filepath.is_injected():\n            logger.info(\"Pout has already been injected into {}\".format(filepath))\n\n        else:\n            if filepath.inject():\n                logger.info(\"Injected pout into {}\".format(filepath))\n            else:\n                logger.info(\"Failed to inject pout into {}\".format(filepath))\n\n    except IOError as e:\n        ret = 1\n        logger.info(str(e))\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main_info(args):\n    if args.site_packages:\n        logger.info(SitePackagesDir())\n\n    else:\n        logger.info(\"Python executable: {}\".format(sys.executable))\n        logger.info(\"Python version: {}\".format(platform.python_version()))\n        logger.info(\"Python site-packages: {}\".format(SitePackagesDir()))\n        logger.info(\"Python sitecustomize: {}\".format(SiteCustomizeFile()))\n        # https://stackoverflow.com/questions/4152963/get-the-name-of-current-script-with-python\n        #logger.info(\"Pout executable: {}\".format(subprocess.check_output([\"which\", \"pout\"])))\n        logger.info(\"Pout executable: {}\".format(os.path.abspath(os.path.expanduser(str(sys.argv[0])))))\n        logger.info(\"Pout version: {}\".format(pout.__version__))\n\n        filepath = SiteCustomizeFile()\n        logger.info(\"Pout injected: {}\".format(filepath.is_injected()))", "response": "Prints out info about the pout installation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling the supplied function with the supplied arguments catching and returning any exception that it throws.", "response": "def catch(func, *args, **kwargs):\n    \"\"\"\n    Call the supplied function with the supplied arguments,\n    catching and returning any exception that it throws.\n\n    Arguments:\n        func: the function to run.\n        *args: positional arguments to pass into the function.\n        **kwargs: keyword arguments to pass into the function.\n    Returns:\n        If the function throws an exception, return the exception.\n        If the function does not throw an exception, return None.\n    \"\"\"\n    try:\n        func(*args, **kwargs)\n    except Exception as e:\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef time(func, *args, **kwargs):\n    start_time = time_module.time()\n    func(*args, **kwargs)\n    end_time = time_module.time()\n    return end_time - start_time", "response": "This function is used to get the execution time of a function in the order of execution time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a Pdb instance at the calling frame with stdout routed to sys. stdout__.", "response": "def set_trace():\n    \"\"\"Start a Pdb instance at the calling frame, with stdout routed to sys.__stdout__.\"\"\"\n    # https://github.com/nose-devs/nose/blob/master/nose/tools/nontrivial.py\n    pdb.Pdb(stdout=sys.__stdout__).set_trace(sys._getframe().f_back)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lmom_fit(self, data=[], lmom_ratios=[]):\n        n_min = self.numargs + 2\n        if len(data) > 0:\n            if len(data) <= n_min:\n                raise ValueError(\"At least {} data points must be provided.\".format(n_min))\n            lmom_ratios = lm.lmom_ratios(data, nmom=n_min)\n        elif not lmom_ratios:\n            raise Exception(\"Either `data` or `lmom_ratios` must be provided.\")\n        elif len(lmom_ratios) < n_min:\n            raise ValueError(\"At least {} number of L-moments must be provided.\".format(n_min))\n\n        return self._lmom_fit(lmom_ratios)", "response": "Fit the distribution function to the given data or given L - moments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the distribution s L - moments e. g. l1 l2 l3 l4 l5...", "response": "def lmom(self, *args, nmom=5, **kwds):\n        \"\"\"\n        Compute the distribution's L-moments, e.g. l1, l2, l3, l4, ..\n\n        :param args: Distribution parameters in order of shape(s), loc, scale\n        :type args: float\n        :param nmom: Number of moments to calculate\n        :type nmom: int\n        :param kwds: Distribution parameters as named arguments. See :attr:`rv_continous.shapes` for names of shape\n                     parameters\n        :type kwds: float\n        :returns: List of L-moments\n        :rtype: list\n        \"\"\"\n        ratios = self.lmom_ratios(*args, nmom=nmom, **kwds)\n        moments = ratios[0:2]\n        moments += [ratio * moments[1] for ratio in ratios[2:]]\n        return moments"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lmom_ratios(self, *args, nmom=5, **kwds):\n        if nmom > 20:\n            return ValueError(\"Parameter nmom too large. Max of 20.\")\n\n        shapes, loc, scale = self._parse_args(*args, **kwds)\n\n        if scale <= 0:\n            return ValueError(\"Invalid scale parameter.\")\n\n        return self._lmom_ratios(*shapes, loc=loc, scale=scale, nmom=nmom)", "response": "Compute the distribution s L - moment ratios e. g. l1 l2 t3 t4..."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run():\n    plugin_list = load_plugins()\n\n    module = sys.modules['__main__']\n    plugin_list.insert(0, ObjectSupplier(module))\n\n    return run_with_plugins(plugin_list)", "response": "Run all the test classes in the main module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncarrying out a test run with the supplied list of plugin instances.", "response": "def run_with_plugins(plugin_list):\n    \"\"\"\n    Carry out a test run with the supplied list of plugin instances.\n    The plugins are expected to identify the object to run.\n\n    Parameters:\n        plugin_list: a list of plugin instances (objects which implement some subset of PluginInterface)\n    Returns: exit code as an integer.\n        The default behaviour (which may be overridden by plugins) is to return a 0\n        exit code if the test run succeeded, and 1 if it failed.\n    \"\"\"\n    composite = core.PluginComposite(plugin_list)\n\n    to_run = composite.get_object_to_run()\n\n    test_run = core.TestRun(to_run, composite)\n    test_run.run()\n\n    return composite.get_exit_code()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialise(self, args=None, env=None, file=None, cwd=None):\n        self.spec_file = args and args.specs or None\n        self.cwd = cwd or os.getcwd()\n        self.file = file\n        return self.spec_file is not None", "response": "Initializes the object with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates nmom number of L - moments from a sample data.", "response": "def lmom_ratios(data, nmom=5):\n    \"\"\"\n    Estimate `nmom` number of L-moments from a sample `data`.\n\n    :param data: Sequence of (sample) data\n    :type data: list or array-like sequence\n    :param nmom: number of L-moments to estimate\n    :type nmom: int\n    :return: L-moment ratios like this: l1, l2, t3, t4, t5, .. . As in: items 3 and higher are L-moment ratios.\n    :rtype: list\n    \"\"\"\n\n    if nmom <= 5:\n        return _samlmusmall(data, nmom)\n    else:\n        return _samlmularge(data, nmom)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a request to the API and return the response.", "response": "def _run(self, url_path, headers=None, **kwargs):\n        \"\"\"\n        Requests API\n        \"\"\"\n        url = self._construct_url(url_path)\n\n        payload = kwargs\n        payload.update({'api_token': self.api_token})\n\n        return self._make_request(url, payload, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkeeps for backwards compatibility of this client See \"self.clear_reference_language\"", "response": "def _apiv1_run(self, action, headers=None, **kwargs):\n        \"\"\"\n        Kept for backwards compatibility of this client\n        See \"self.clear_reference_language\"\n        \"\"\"\n        warnings.warn(\n            \"POEditor API v1 is deprecated. Use POEditorAPI._run method to call API v2\",\n            DeprecationWarning, stacklevel=2\n        )\n\n        url = \"https://poeditor.com/api/\"\n        payload = kwargs\n        payload.update({'action': action, 'api_token': self.api_token})\n\n        return self._make_request(url, payload, headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the data returned by the detail view", "response": "def _project_formatter(self, data):\n        \"\"\"\n        Project object\n        \"\"\"\n        open_ = False if not data['open'] or data['open'] == '0' else True\n        public = False if not data['public'] or data['public'] == '0' else True\n        output = {\n            'created': parse_datetime(data['created']),\n            'id': int(data['id']),\n            'name': data['name'],\n            'open': open_,\n            'public': public,\n        }\n\n        # the detail view returns more info than the list view\n        # see https://poeditor.com/docs/api#projects_view\n        for key in ['description', 'reference_language', 'terms']:\n            if key in data:\n                output[key] = data[key]\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of projects owned by user.", "response": "def list_projects(self):\n        \"\"\"\n        Returns the list of projects owned by user.\n        \"\"\"\n        data = self._run(\n            url_path=\"projects/list\"\n        )\n        projects = data['result'].get('projects', [])\n        return [self._project_formatter(item) for item in projects]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new project. Returns the id of the project.", "response": "def create_project(self, name, description=None):\n        \"\"\"\n        creates a new project. Returns the id of the project (if successful)\n        \"\"\"\n        description = description or ''\n        data = self._run(\n            url_path=\"projects/add\",\n            name=name,\n            description=description\n        )\n        return data['result']['project']['id']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the project settings.", "response": "def update_project(self, project_id, name=None, description=None,\n                       reference_language=None):\n        \"\"\"\n        Updates project settings (name, description, reference language)\n        If optional parameters are not sent, their respective fields are not updated.\n        \"\"\"\n        kwargs = {}\n        if name is not None:\n            kwargs['name'] = name\n        if description is not None:\n            kwargs['description'] = description\n        if reference_language is not None:\n            kwargs['reference_language'] = reference_language\n\n        data = self._run(\n            url_path=\"projects/update\",\n            id=project_id,\n            **kwargs\n        )\n        return data['result']['project']['id']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef view_project_details(self, project_id):\n        data = self._run(\n            url_path=\"projects/view\",\n            id=project_id\n        )\n        return self._project_formatter(data['result']['project'])", "response": "Returns project s details."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn project languages percentage of translation done for each and the datetime when the last change was made.", "response": "def list_project_languages(self, project_id):\n        \"\"\"\n        Returns project languages, percentage of translation done for each and the\n        datetime (UTC - ISO 8601) when the last change was made.\n        \"\"\"\n        data = self._run(\n            url_path=\"languages/list\",\n            id=project_id\n        )\n        return data['result'].get('languages', [])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new language to a project", "response": "def add_language_to_project(self, project_id, language_code):\n        \"\"\"\n        Adds a new language to project\n        \"\"\"\n        self._run(\n            url_path=\"languages/add\",\n            id=project_id,\n            language=language_code\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the project terms. Lets you change the text context reference plural and tags. Lets you change the text context reference plural and tags. Lets you change the text context reference plural and tags. Lets you change the text context reference plural and tags.", "response": "def update_terms(self, project_id, data, fuzzy_trigger=None):\n        \"\"\"\n        Updates project terms. Lets you change the text, context, reference, plural and tags.\n\n        >>> data = [\n                {\n                    \"term\": \"Add new list\",\n                    \"context\": \"\",\n                    \"new_term\": \"Save list\",\n                    \"new_context\": \"\",\n                    \"reference\": \"\\/projects\",\n                    \"plural\": \"\",\n                    \"comment\": \"\",\n                    \"tags\": [\n                        \"first_tag\",\n                        \"second_tag\"\n                    ]\n                },\n                {\n                    \"term\": \"Display list\",\n                    \"context\": \"\",\n                    \"new_term\": \"Show list\",\n                    \"new_context\": \"\"\n                }\n            ]\n        \"\"\"\n        kwargs = {}\n        if fuzzy_trigger is not None:\n            kwargs['fuzzy_trigger'] = fuzzy_trigger\n\n        data = self._run(\n            url_path=\"terms/update\",\n            id=project_id,\n            data=json.dumps(data),\n            **kwargs\n        )\n        return data['result']['terms']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd comments to existing terms.", "response": "def add_comment(self, project_id, data):\n        \"\"\"\n        Adds comments to existing terms.\n        >>> data = [\n                {\n                    \"term\": \"Add new list\",\n                    \"context\": \"\",\n                    \"comment\": \"This is a button\"\n                },\n                {\n                    \"term\": \"one project found\",\n                    \"context\": \"\",\n                    \"comment\": \"Make sure you translate the plural forms\"\n                },\n                {\n                    \"term\": \"Show all projects\",\n                    \"context\": \"\",\n                    \"comment\": \"This is a button\"\n                }\n            ]\n        \"\"\"\n        data = self._run(\n            url_path=\"terms/add_comment\",\n            id=project_id,\n            data=json.dumps(data)\n        )\n        return data['result']['terms']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the language of a project.", "response": "def update_project_language(self, project_id, language_code, data, fuzzy_trigger=None):\n        \"\"\"\n        Inserts / overwrites translations.\n        >>> data = [\n            {\n                \"term\": \"Projects\",\n                \"context\": \"project list\",\n                \"translation\": {\n                    \"content\": \"Des projets\",\n                    \"fuzzy\": 0\n                }\n            }\n        ]\n        \"\"\"\n        kwargs = {}\n        if fuzzy_trigger is not None:\n            kwargs['fuzzy_trigger'] = fuzzy_trigger\n\n        data = self._run(\n            url_path=\"languages/update\",\n            id=project_id,\n            language=language_code,\n            data=json.dumps(data),\n            **kwargs\n        )\n        return data['result']['translations']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, project_id, language_code, file_type='po', filters=None,\n               tags=None, local_file=None):\n        \"\"\"\n        Return terms / translations\n\n        filters - filter by self._filter_by\n        tags - filter results by tags;\n        local_file - save content into it. If None, save content into\n            random temp file.\n\n        >>> tags = 'name-of-tag'\n        >>> tags = [\"name-of-tag\"]\n        >>> tags = [\"name-of-tag\", \"name-of-another-tag\"]\n\n        >>> filters = 'translated'\n        >>> filters = [\"translated\"]\n        >>> filters = [\"translated\", \"not_fuzzy\"]\n        \"\"\"\n        if file_type not in self.FILE_TYPES:\n            raise POEditorArgsException(\n                'content_type: file format {}'.format(self.FILE_TYPES))\n\n        if filters and isinstance(filters, str) and filters not in self.FILTER_BY:\n            raise POEditorArgsException(\n                \"filters - filter results by {}\".format(self.FILTER_BY))\n        elif filters and set(filters).difference(set(self.FILTER_BY)):\n            raise POEditorArgsException(\n                \"filters - filter results by {}\".format(self.FILTER_BY))\n\n        data = self._run(\n            url_path=\"projects/export\",\n            id=project_id,\n            language=language_code,\n            type=file_type,\n            filters=filters,\n            tags=tags\n        )\n        # The link of the file (expires after 10 minutes).\n        file_url = data['result']['url']\n\n        # Download file content:\n        res = requests.get(file_url, stream=True)\n        if not local_file:\n            tmp_file = tempfile.NamedTemporaryFile(\n                delete=False, suffix='.{}'.format(file_type))\n            tmp_file.close()\n            local_file = tmp_file.name\n\n        with open(local_file, 'w+b') as po_file:\n            for data in res.iter_content(chunk_size=1024):\n                po_file.write(data)\n        return file_url, local_file", "response": "Export the terms and translations of a single language code into a single file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _upload(self, project_id, updating, file_path, language_code=None,\n                overwrite=False, sync_terms=False, tags=None, fuzzy_trigger=None):\n        \"\"\"\n        Internal: updates terms / translations\n\n        File uploads are limited to one every 30 seconds\n        \"\"\"\n        options = [\n            self.UPDATING_TERMS,\n            self.UPDATING_TERMS_TRANSLATIONS,\n            self.UPDATING_TRANSLATIONS\n        ]\n        if updating not in options:\n            raise POEditorArgsException(\n                'Updating arg must be in {}'.format(options)\n            )\n\n        options = [\n            self.UPDATING_TERMS_TRANSLATIONS,\n            self.UPDATING_TRANSLATIONS\n        ]\n        if language_code is None and updating in options:\n            raise POEditorArgsException(\n                'Language code is required only if updating is '\n                'terms_translations or translations)'\n            )\n\n        if updating == self.UPDATING_TRANSLATIONS:\n            tags = None\n            sync_terms = None\n\n        # Special content type:\n        tags = tags or ''\n        language_code = language_code or ''\n        sync_terms = '1' if sync_terms else '0'\n        overwrite = '1' if overwrite else '0'\n        fuzzy_trigger = '1' if fuzzy_trigger else '0'\n        project_id = str(project_id)\n\n        with open(file_path, 'r+b') as local_file:\n            data = self._run(\n                url_path=\"projects/upload\",\n                id=project_id,\n                language=language_code,\n                file=local_file,\n                updating=updating,\n                tags=tags,\n                sync_terms=sync_terms,\n                overwrite=overwrite,\n                fuzzy_trigger=fuzzy_trigger\n            )\n        return data['result']", "response": "Upload a file to the ACS Academy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_terms(self, project_id, file_path=None, language_code=None,\n                     overwrite=False, sync_terms=False, tags=None, fuzzy_trigger=None):\n        \"\"\"\n        Updates terms\n\n        overwrite: set it to True if you want to overwrite translations\n        sync_terms: set it to True if you want to sync your terms (terms that\n            are not found in the uploaded file will be deleted from project\n            and the new ones added). Ignored if updating = translations\n        tags: Add tags to the project terms; available when updating terms or terms_translations;\n              you can use the following keys: \"all\" - for the all the imported terms, \"new\" - for\n              the terms which aren't already in the project, \"obsolete\" - for the terms which are\n              in the project but not in the imported file and \"overwritten_translations\" - for the\n              terms for which translations change\n        fuzzy_trigger: set it to True to mark corresponding translations from the\n            other languages as fuzzy for the updated values\n        \"\"\"\n        return self._upload(\n            project_id=project_id,\n            updating=self.UPDATING_TERMS,\n            file_path=file_path,\n            language_code=language_code,\n            overwrite=overwrite,\n            sync_terms=sync_terms,\n            tags=tags,\n            fuzzy_trigger=fuzzy_trigger\n        )", "response": "Update the set of terms that are not already in the project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_terms_translations(self, project_id, file_path=None,\n                                 language_code=None, overwrite=False,\n                                 sync_terms=False, tags=None, fuzzy_trigger=None):\n        \"\"\"\n        Updates terms translations\n\n        overwrite: set it to True if you want to overwrite translations\n        sync_terms: set it to True if you want to sync your terms (terms that\n            are not found in the uploaded file will be deleted from project\n            and the new ones added). Ignored if updating = translations\n        tags: Add tags to the project terms; available when updating terms or terms_translations;\n              you can use the following keys: \"all\" - for the all the imported terms, \"new\" - for\n              the terms which aren't already in the project, \"obsolete\" - for the terms which are\n              in the project but not in the imported file and \"overwritten_translations\" - for the\n              terms for which translations change\n        fuzzy_trigger: set it to True to mark corresponding translations from the\n            other languages as fuzzy for the updated values\n        \"\"\"\n        return self._upload(\n            project_id=project_id,\n            updating=self.UPDATING_TERMS_TRANSLATIONS,\n            file_path=file_path,\n            language_code=language_code,\n            overwrite=overwrite,\n            sync_terms=sync_terms,\n            tags=tags,\n            fuzzy_trigger=fuzzy_trigger\n        )", "response": "Updates the terms translations for the specified project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the translations of the current language for the current language.", "response": "def update_translations(self, project_id, file_path=None,\n                            language_code=None, overwrite=False, fuzzy_trigger=None):\n        \"\"\"\n        Updates translations\n\n        overwrite: set it to True if you want to overwrite definitions\n        fuzzy_trigger: set it to True to mark corresponding translations from the\n            other languages as fuzzy for the updated values\n        \"\"\"\n        return self._upload(\n            project_id=project_id,\n            updating=self.UPDATING_TRANSLATIONS,\n            file_path=file_path,\n            language_code=language_code,\n            overwrite=overwrite,\n            fuzzy_trigger=fuzzy_trigger\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of contributors in the specified project.", "response": "def list_contributors(self, project_id=None, language_code=None):\n        \"\"\"\n        Returns the list of contributors\n        \"\"\"\n        data = self._run(\n            url_path=\"contributors/list\",\n            id=project_id,\n            language=language_code\n        )\n        return data['result'].get('contributors', [])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a contributor to a project language_code", "response": "def add_contributor(self, project_id, name, email, language_code):\n        \"\"\"\n        Adds a contributor to a project language\n        \"\"\"\n        self._run(\n            url_path=\"contributors/add\",\n            id=project_id,\n            name=name,\n            email=email,\n            language=language_code\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a contributor to a project language", "response": "def add_administrator(self, project_id, name, email):\n        \"\"\"\n        Adds a contributor to a project language\n        \"\"\"\n        self._run(\n            url_path=\"contributors/add\",\n            id=project_id,\n            name=name,\n            email=email,\n            admin=True\n        )\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_contributor(self, project_id, email, language):\n        self._run(\n            url_path=\"contributors/remove\",\n            id=project_id,\n            email=email,\n            language=language\n        )\n        return True", "response": "Removes a contributor from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_stations(html):\n    html = html.replace('SLs.sls=', '').replace(';SLs.showSuggestion();', '')\n    html = json.loads(html)\n    return html['suggestions']", "response": "Parses the stations from the given HTML."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the delay from the details view and save the parsed data", "response": "def parse_delay(data):\n    \"\"\"\n        Prase the delay\n    \"\"\"\n    # parse data from the details view\n    rsp = requests.get(data['details'])\n    soup = BeautifulSoup(rsp.text, \"html.parser\")\n\n    # get departure delay\n    delay_departure_raw = soup.find('div', class_=\"routeStart\").find('span', class_=[\"delay\", \"delayOnTime\"])\n    if delay_departure_raw:\n        delay_departure = calculate_delay(data['departure'],\n                                          delay_departure_raw.text)\n    else:\n        delay_departure = 0\n\n    # get arrival delay\n    delay_arrival_raw = soup.find('div', class_=[\"routeEnd\",\"routeEndAdditional\"]).find('span', class_=[\"delay\", \"delayOnTime\"])\n    if delay_arrival_raw:\n        delay_arrival = calculate_delay(data['arrival'],\n                                        delay_arrival_raw.text)\n    else:\n        delay_arrival = 0\n\n    # save the parsed data\n    if delay_departure + delay_arrival == 0:\n        data['ontime'] = True\n    else:\n        data['ontime'] = False\n    data['delay'] = {\n        'delay_departure': int(delay_departure),\n        'delay_arrival': int(delay_arrival)\n    }\n\n    # TODO: this should not be hardcoded!\n    data['canceled'] = False\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the delay between original and delayed", "response": "def calculate_delay(original, delay):\n    \"\"\"\n        Calculate the delay\n    \"\"\"\n    original = datetime.strptime(original, '%H:%M')\n    delayed = datetime.strptime(delay, '%H:%M')\n    diff = delayed - original\n    return diff.total_seconds() // 60"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stations(self, station, limit=10):\n        query = {\n            'start': 1,\n            'S': station + '?',\n            'REQ0JourneyStopsB': limit\n        }\n        rsp = requests.get('http://reiseauskunft.bahn.de/bin/ajax-getstop.exe/dn', params=query)\n        return parse_stations(rsp.text)", "response": "Returns a list of stations for a given station"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all connections between two stations", "response": "def connections(self, origin, destination, dt=datetime.now(), only_direct=False):\n        \"\"\"\n        Find connections between two stations\n\n        Args:\n            origin (str): origin station\n            destination (str): destination station\n            dt (datetime): date and time for query\n            only_direct (bool): only direct connections\n        \"\"\"\n        query = {\n            'S': origin,\n            'Z': destination,\n            'date': dt.strftime(\"%d.%m.%y\"),\n            'time': dt.strftime(\"%H:%M\"),\n            'start': 1,\n            'REQ0JourneyProduct_opt0': 1 if only_direct else 0\n        }\n        rsp = requests.get('http://mobile.bahn.de/bin/mobil/query.exe/dox?', params=query)\n        return parse_connections(rsp.text)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscattering elements of sequence into n blocks.", "response": "def _scatter(sequence, n):\n  \"\"\"Scatters elements of ``sequence`` into ``n`` blocks.\"\"\"\n  chunklen = int(math.ceil(float(len(sequence)) / float(n)))\n\n  return [\n    sequence[ i*chunklen : (i+1)*chunklen ] for i in range(n)\n  ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lease(self, seconds=600, num_tasks=1, tag=None):\n    tag = tag if tag else None\n    tasks = self._api.lease(\n      numTasks=num_tasks, \n      seconds=seconds,\n      groupByTag=(tag is not None),\n      tag=tag,\n    )\n\n    if not len(tasks):\n      raise QueueEmpty\n\n    task = tasks[0]\n    return totask(task)", "response": "Acquires a lease on the topmost N unowned tasks in the specified queue."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef purge(self):\n    try:\n      return self._api.purge()\n    except AttributeError:\n      while True:\n        lst = self.list()\n        if len(lst) == 0:\n          break\n\n        for task in lst:\n          self.delete(task)\n        self.wait()\n      return self", "response": "Deletes all tasks in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef poll(\n    self, lease_seconds=LEASE_SECONDS, tag=None, \n    verbose=False, execute_args=[], execute_kwargs={}, \n    stop_fn=None, backoff_exceptions=[], min_backoff_window=30, \n    max_backoff_window=120, log_fn=None\n  ):\n    \"\"\"\n    Poll a queue until a stop condition is reached (default forever). Note\n    that this function is not thread safe as it requires a global variable\n    to intercept SIGINT.\n    lease_seconds: each task should be leased for this many seconds\n    tag: if specified, query for only tasks that match this tag\n    execute_args / execute_kwargs: pass these arguments to task execution\n    backoff_exceptions: A list of exceptions that instead of causing a crash,\n      instead cause the polling to back off for an increasing exponential \n      random window.\n    min_backoff_window: The minimum sized window (in seconds) to select a \n      random backoff time. \n    max_backoff_window: The window doubles each retry. This is the maximum value\n      in seconds.\n    stop_fn: A boolean returning function that accepts no parameters. When\n      it returns True, the task execution loop will terminate. It is evaluated\n      once after every task.\n    log_fn: Feed error messages to this function, default print (when verbose is enabled).\n    verbose: print out the status of each step\n    Return: number of tasks executed\n    \"\"\"\n    global LOOP\n\n    if not callable(stop_fn) and stop_fn is not None:\n      raise ValueError(\"stop_fn must be a callable. \" + str(stop_fn))\n    elif not callable(stop_fn):\n      stop_fn = lambda: False\n\n    def random_exponential_window_backoff(n):\n      n = min(n, min_backoff_window)\n      # 120 sec max b/c on avg a request every ~250msec if 500 containers \n      # in contention which seems like a quite reasonable volume of traffic \n      # to handle\n      high = min(2 ** n, max_backoff_window) \n      return random.uniform(0, high)\n\n    def printv(*args, **kwargs):\n      if verbose:\n        print(*args, **kwargs)\n\n    LOOP = True\n\n    def sigint_handler(signum, frame):\n      global LOOP\n      printv(\"Interrupted. Exiting after this task completes...\")\n      LOOP = False\n    \n    prev_sigint_handler = signal.getsignal(signal.SIGINT)\n    signal.signal(signal.SIGINT, sigint_handler)\n\n    if log_fn is None:\n      log_fn = printv\n\n    tries = 0\n    executed = 0\n\n    backoff = False\n    backoff_exceptions = tuple(list(backoff_exceptions) + [ QueueEmpty ])\n\n    while LOOP:\n      task = 'unknown' # for error message prior to leasing\n      try:\n        task = self.lease(seconds=int(lease_seconds))\n        tries += 1\n        printv(task)\n        task.execute(*execute_args, **execute_kwargs)\n        executed += 1\n        printv(\"Delete enqueued task...\")\n        self.delete(task)\n        log_fn('INFO', task , \"succesfully executed\")\n        tries = 0\n      except backoff_exceptions:\n        backoff = True\n      except Exception as e:\n        printv('ERROR', task, \"raised {}\\n {}\".format(e , traceback.format_exc()))\n        raise #this will restart the container in kubernetes\n        \n      if stop_fn():\n        break\n\n      if backoff:\n        time.sleep(random_exponential_window_backoff(tries))\n\n      backoff = False\n\n    printv(\"Task execution loop exited.\")\n    signal.signal(signal.SIGINT, prev_sigint_handler)\n\n    return executed", "response": "Poll a queue until a stop condition is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(self, task, args=[], kwargs={}, delay_seconds=0):\n    body = {\n      \"payload\": task.payload(),\n      \"queueName\": self._queue_name,\n      \"groupByTag\": True,\n      \"tag\": task.__class__.__name__\n    }\n\n    def cloud_insertion(api):\n      api.insert(body, delay_seconds)\n\n    if len(self._threads):\n      self.put(cloud_insertion)\n    else:\n      cloud_insertion(self._api)\n\n    return self", "response": "Insert a task into an existing queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a task from a TaskQueue.", "response": "def delete(self, task_id):\n    \"\"\"Deletes a task from a TaskQueue.\"\"\"\n    if isinstance(task_id, RegisteredTask):\n      task_id = task_id.id\n\n    def cloud_delete(api):\n      api.delete(task_id)\n\n    if len(self._threads):\n      self.put(cloud_delete)\n    else:\n      cloud_delete(self._api)\n\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert(self, task, args=[], kwargs={}, delay_seconds=0):\n    body = {\n      \"payload\": task.payload(),\n      \"queueName\": self._queue_name,\n      \"groupByTag\": True,\n      \"tag\": task.__class__.__name__\n    }\n\n    def cloud_insertion():\n      self._api.insert(body, delay_seconds)\n\n    self._pool.spawn(cloud_insertion)\n\n    return self", "response": "Insert a task into the queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if this is a user upload from the user", "response": "def is_upload(action):\n    \"\"\"Checks if this should be a user upload\n\n    :param action:\n    :return: True if this is a file we intend to upload from the user\n    \"\"\"\n    return 'r' in action.type._mode and (action.default is None or\n                                         getattr(action.default, 'name') not in (sys.stderr.name, sys.stdout.name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_django(self):\n        exclude = {'name', 'model'}\n        field_module = 'models'\n        django_kwargs = {}\n        if self.node_attrs['model'] == 'CharField':\n            django_kwargs['max_length'] = 255\n        django_kwargs['blank'] = not self.node_attrs['required']\n        try:\n            django_kwargs['default'] = self.node_attrs['value']\n        except KeyError:\n            pass\n        return u'{0} = {1}.{2}({3})'.format(self.node_attrs['name'], field_module, self.node_attrs['model'],\n                                           ', '.join(['{0}={1}'.format(i,v) for i,v in six.iteritems(django_kwargs)]),)", "response": "Convert the node attributes to a django representation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a modified dict where all the keys that are anything but str get converted to str.", "response": "def str_dict_keys(a_dict):\n    \"\"\"return a modified dict where all the keys that are anything but str get\n    converted to str.\n    E.g.\n\n      >>> result = str_dict_keys({u'name': u'Peter', u'age': 99, 1: 2})\n      >>> # can't compare whole dicts in doctests\n      >>> result['name']\n      u'Peter'\n      >>> result['age']\n      99\n      >>> result[1]\n      2\n\n    The reason for this is that in Python <= 2.6.4 doing\n    ``MyClass(**{u'name': u'Peter'})`` would raise a TypeError\n\n    Note that only unicode types are converted to str types.\n    The reason for that is you might have a class that looks like this::\n\n        class Option(object):\n            def __init__(self, foo=None, bar=None, **kwargs):\n                ...\n\n    And it's being used like this::\n\n        Option(**{u'foo':1, u'bar':2, 3:4})\n\n    Then you don't want to change that {3:4} part which becomes part of\n    `**kwargs` inside the __init__ method.\n    Using integers as parameter keys is a silly example but the point is that\n    due to the python 2.6.4 bug only unicode keys are converted to str.\n    \"\"\"\n    new_dict = {}\n    for key in a_dict:\n        if six.PY2 and isinstance(key, six.text_type):\n            new_dict[str(key)] = a_dict[key]\n        else:\n            new_dict[key] = a_dict[key]\n    return new_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str_to_boolean(input_str):\n    if not isinstance(input_str, six.string_types):\n        raise ValueError(input_str)\n    input_str = str_quote_stripper(input_str)\n    return input_str.lower() in (\"true\", \"t\", \"1\", \"y\", \"yes\")", "response": "a conversion function for boolean\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str_to_python_object(input_str):\n    if not input_str:\n        return None\n    if six.PY3 and isinstance(input_str, six.binary_type):\n        input_str = to_str(input_str)\n    if not isinstance(input_str, six.string_types):\n        # gosh, we didn't get a string, we can't convert anything but strings\n        # we're going to assume that what we got is actually what was wanted\n        # as the output\n        return input_str\n    input_str = str_quote_stripper(input_str)\n    if '.' not in input_str and input_str in known_mapping_str_to_type:\n        return known_mapping_str_to_type[input_str]\n    parts = [x.strip() for x in input_str.split('.') if x.strip()]\n    try:\n        try:\n            # first try as a complete module\n            package = __import__(input_str)\n        except ImportError:\n            # it must be a class from a module\n            if len(parts) == 1:\n                # since it has only one part, it must be a class from __main__\n                parts = ('__main__', input_str)\n            package = __import__('.'.join(parts[:-1]), globals(), locals(), [])\n        obj = package\n        for name in parts[1:]:\n            obj = getattr(obj, name)\n        return obj\n    except AttributeError as x:\n        raise CannotConvertError(\"%s cannot be found\" % input_str)\n    except ImportError as x:\n        raise CannotConvertError(str(x))", "response": "a conversion that will import a module and class name\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a comma delimited list of class names into actual classes within a numbered namespace", "response": "def str_to_classes_in_namespaces(\n    template_for_namespace=\"cls%d\",\n    name_of_class_option='cls',\n    instantiate_classes=False\n):\n    \"\"\"take a comma delimited  list of class names, convert each class name\n    into an actual class as an option within a numbered namespace.  This\n    function creates a closure over a new function.  That new function,\n    in turn creates a class derived from RequiredConfig.  The inner function,\n    'class_list_converter', populates the InnerClassList with a Namespace for\n    each of the classes in the class list.  In addition, it puts the each class\n    itself into the subordinate Namespace.  The requirement discovery mechanism\n    of configman then reads the InnerClassList's requried config, pulling in\n    the namespaces and associated classes within.\n\n    For example, if we have a class list like this: \"Alpha, Beta\", then this\n    converter will add the following Namespaces and options to the\n    configuration:\n\n        \"cls0\" - the subordinate Namespace for Alpha\n        \"cls0.cls\" - the option containing the class Alpha itself\n        \"cls1\" - the subordinate Namespace for Beta\n        \"cls1.cls\" - the option containing the class Beta itself\n\n    Optionally, the 'class_list_converter' inner function can embue the\n    InnerClassList's subordinate namespaces with aggregates that will\n    instantiate classes from the class list.  This is a convenience to the\n    programmer who would otherwise have to know ahead of time what the\n    namespace names were so that the classes could be instantiated within the\n    context of the correct namespace.  Remember the user could completely\n    change the list of classes at run time, so prediction could be difficult.\n\n        \"cls0\" - the subordinate Namespace for Alpha\n        \"cls0.cls\" - the option containing the class Alpha itself\n        \"cls0.cls_instance\" - an instance of the class Alpha\n        \"cls1\" - the subordinate Namespace for Beta\n        \"cls1.cls\" - the option containing the class Beta itself\n        \"cls1.cls_instance\" - an instance of the class Beta\n\n    parameters:\n        template_for_namespace - a template for the names of the namespaces\n                                 that will contain the classes and their\n                                 associated required config options.  The\n                                 namespaces will be numbered sequentially.  By\n                                 default, they will be \"cls1\", \"cls2\", etc.\n        class_option_name - the name to be used for the class option within\n                            the nested namespace.  By default, it will choose:\n                            \"cls1.cls\", \"cls2.cls\", etc.\n        instantiate_classes - a boolean to determine if there should be an\n                              aggregator added to each namespace that\n                              instantiates each class.  If True, then each\n                              Namespace will contain elements for the class, as\n                              well as an aggregator that will instantiate the\n                              class.\n                              \"\"\"\n\n    # these are only used within this method.  No need to pollute the module\n    # scope with them and avoid potential circular imports\n    from configman.namespace import Namespace\n    from configman.required_config import RequiredConfig\n\n    #--------------------------------------------------------------------------\n    def class_list_converter(class_list_str):\n        \"\"\"This function becomes the actual converter used by configman to\n        take a string and convert it into the nested sequence of Namespaces,\n        one for each class in the list.  It does this by creating a proxy\n        class stuffed with its own 'required_config' that's dynamically\n        generated.\"\"\"\n        if isinstance(class_list_str, six.string_types):\n            class_list = [x.strip() for x in class_list_str.split(',')]\n            if class_list == ['']:\n                class_list = []\n        else:\n            raise TypeError('must be derivative of %s' % six.string_types)\n\n        #======================================================================\n        class InnerClassList(RequiredConfig):\n            \"\"\"This nested class is a proxy list for the classes.  It collects\n            all the config requirements for the listed classes and places them\n            each into their own Namespace.\n            \"\"\"\n            # we're dynamically creating a class here.  The following block of\n            # code is actually adding class level attributes to this new class\n            required_config = Namespace()  # 1st requirement for configman\n            subordinate_namespace_names = []  # to help the programmer know\n                                              # what Namespaces we added\n            namespace_template = template_for_namespace  # save the template\n                                                         # for future reference\n            class_option_name = name_of_class_option  # save the class's option\n                                                      # name for the future\n            # for each class in the class list\n            for namespace_index, a_class in enumerate(class_list):\n                # figure out the Namespace name\n                namespace_name = template_for_namespace % namespace_index\n                subordinate_namespace_names.append(namespace_name)\n                # create the new Namespace\n                required_config[namespace_name] = Namespace()\n                # add the option for the class itself\n                required_config[namespace_name].add_option(\n                    name_of_class_option,\n                    #doc=a_class.__doc__  # not helpful if too verbose\n                    default=a_class,\n                    from_string_converter=class_converter\n                )\n                if instantiate_classes:\n                    # add an aggregator to instantiate the class\n                    required_config[namespace_name].add_aggregation(\n                        \"%s_instance\" % name_of_class_option,\n                        lambda c, lc, a: lc[name_of_class_option](lc)\n                    )\n\n            @classmethod\n            def to_str(cls):\n                \"\"\"this method takes this inner class object and turns it back\n                into the original string of classnames.  This is used\n                primarily as for the output of the 'help' option\"\"\"\n                return ', '.join(\n                    py_obj_to_str(v[name_of_class_option].value)\n                    for v in cls.get_required_config().values()\n                    if isinstance(v, Namespace)\n                )\n\n        return InnerClassList  # result of class_list_converter\n    return class_list_converter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_to_list(\n    input_str,\n    item_converter=lambda x: x,\n    item_separator=',',\n    list_to_collection_converter=None,\n):\n    \"\"\" a conversion function for list\n    \"\"\"\n    if not isinstance(input_str, six.string_types):\n        raise ValueError(input_str)\n    input_str = str_quote_stripper(input_str)\n    result = [\n        item_converter(x.strip())\n        for x in input_str.split(item_separator) if x.strip()\n    ]\n    if list_to_collection_converter is not None:\n        return list_to_collection_converter(result)\n    return result", "response": "a conversion function for list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arbitrary_object_to_string(a_thing):\n    # is it None?\n    if a_thing is None:\n        return ''\n    # is it already a string?\n    if isinstance(a_thing, six.string_types):\n        return a_thing\n    if six.PY3 and isinstance(a_thing, six.binary_type):\n        try:\n            return a_thing.decode('utf-8')\n        except UnicodeDecodeError:\n            pass\n    # does it have a to_str function?\n    try:\n        return a_thing.to_str()\n    except (AttributeError, KeyError, TypeError):\n        # AttributeError - no to_str function?\n        # KeyError - DotDict has no to_str?\n        # TypeError - problem converting\n        # nope, no to_str function\n        pass\n    # is this a type proxy?\n    try:\n        return arbitrary_object_to_string(a_thing.a_type)\n    except (AttributeError, KeyError, TypeError):\n        #\n        # nope, no a_type property\n        pass\n    # is it a built in?\n    try:\n        return known_mapping_type_to_str[a_thing]\n    except (KeyError, TypeError):\n        # nope, not a builtin\n        pass\n    # is it something from a loaded module?\n    try:\n        if a_thing.__module__ not in ('__builtin__', 'builtins', 'exceptions'):\n            if a_thing.__module__ == \"__main__\":\n                module_name = (\n                    sys.modules['__main__']\n                    .__file__[:-3]\n                    .replace('/', '.')\n                    .strip('.')\n                )\n            else:\n                module_name = a_thing.__module__\n            return \"%s.%s\" % (module_name, a_thing.__name__)\n    except AttributeError:\n        # nope, not one of these\n        pass\n    # maybe it has a __name__ attribute?\n    try:\n        return a_thing.__name__\n    except AttributeError:\n        # nope, not one of these\n        pass\n    # punt and see what happens if we just cast it to string\n    return str(a_thing)", "response": "take a python object of some sort and convert it into a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_source(node, indent_with=' ' * 4, add_line_information=False):\n    generator = SourceGenerator(indent_with, add_line_information)\n    generator.visit(node)\n    return ''.join(str(s) for s in generator.result)", "response": "This function can convert a node tree into a python sourcecode string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connection(self, name=None):\n        if not name:\n            name = threading.currentThread().getName()\n        if name in self.pool:\n            return self.pool[name]\n        self.pool[name] = FakeDatabaseConnection(self.dsn)\n        return self.pool[name]", "response": "return a named connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close_connection(self, connection, force=False):\n        if force:\n            print('PostgresPooled - delegating connection closure')\n            try:\n                super(PostgresPooled, self).close_connection(connection,\n                                                                  force)\n            except self.operational_exceptions:\n                print('PostgresPooled - failed closing')\n            for name, conn in self.pool.iteritems():\n                if conn is connection:\n                    break\n            del self.pool[name]\n        else:\n            print('PostgresPooled - refusing to close connection')", "response": "overriding the baseclass function this routine will decline to\n        close a connection at the end of a transaction context. This routine will close the connection at the end of a transaction context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a function within the context of a transaction", "response": "def do_transaction(self, function, *args, **kwargs):\n        \"\"\"execute a function within the context of a transaction\"\"\"\n        with self.config.db_transaction() as trans:\n            function(trans, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsleeping for the specified number of seconds logging every wait_log_interval seconds with progress info.", "response": "def responsive_sleep(self, seconds, wait_reason=''):\n        \"\"\"Sleep for the specified number of seconds, logging every\n        'wait_log_interval' seconds with progress info.\"\"\"\n        for x in range(int(seconds)):\n            if (self.config.wait_log_interval and\n                not x % self.config.wait_log_interval):\n                print('%s: %dsec of %dsec' % (wait_reason,\n                                              x,\n                                              seconds))\n            time.sleep(1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_transaction(self, function, *args, **kwargs):\n        for wait_in_seconds in self.backoff_generator():\n            try:\n                with self.config.db_transaction() as trans:\n                    function(trans, *args, **kwargs)\n                    trans.commit()\n                    break\n            except self.config.db_transaction.operational_exceptions:\n                pass\n            print(('failure in transaction - retry in %s seconds' %\n                   wait_in_seconds))\n            self.responsive_sleep(wait_in_seconds,\n                                  \"waiting for retry after failure in \"\n                                  \"transaction\")", "response": "execute a function within the context of a transaction"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _expand_files(self, file_name, original_path, indent=\"\"):\n        expanded_file_contents = []\n        with open(file_name) as f:\n            for a_line in f:\n                match = ConfigObjWithIncludes._include_re.match(a_line)\n                if match:\n                    include_file = match.group(2)\n                    include_file = os.path.join(\n                        original_path,\n                        include_file\n                    )\n                    new_lines = self._expand_files(\n                        include_file,\n                        os.path.dirname(include_file),\n                        indent + match.group(1)\n                    )\n                    expanded_file_contents.extend(new_lines)\n                else:\n                    expanded_file_contents.append(indent + a_line.rstrip())\n        return expanded_file_contents", "response": "This function recursively finds all the contents of a file and returns a list of the contents of the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_values(self, config_manager, ignore_mismatches, obj_hook=DotDict):\n        if self.delayed_parser_instantiation:\n            try:\n                app = config_manager._get_option('admin.application')\n                source = \"%s%s\" % (app.value.app_name, file_name_extension)\n                self.config_obj = configobj.ConfigObj(source)\n                self.delayed_parser_instantiation = False\n            except AttributeError:\n                # we don't have enough information to get the ini file\n                # yet.  we'll ignore the error for now\n                return obj_hook()  # return empty dict of the obj_hook type\n        if isinstance(self.config_obj, obj_hook):\n            return self.config_obj\n        return obj_hook(initializer=self.config_obj)", "response": "Return a nested dictionary representing the values in the ini file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef error(self, message):\n        if (\n            \"not allowed\" in message\n            or \"ignored\" in message\n            or \"expected\" in message\n            or \"invalid\" in message\n            or self.add_help\n        ):\n            # when we have \"help\" then we must also have proper error\n            # processing.  Without \"help\", we suppress the errors by\n            # doing nothing here\n            super(IntermediateConfigmanParser, self).error(message)", "response": "we can t suppress errors by the expansion process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configman_keys(a_mapping):\n    configmanized_keys_dict = DotDict()\n    for k, v in iteritems_breadth_first(a_mapping):\n        if '__' in k and k != k.upper():\n            k = k.replace('__', '.')\n        configmanized_keys_dict[k] = v\n    return configmanized_keys_dict", "response": "return a DotDict that is a copy of the provided mapping with keys\n     transformed into a configman compatible form"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_key_translating_dot_dict(\n    new_class_name,\n    translation_tuples,\n    base_class=DotDict\n):\n    \"\"\"this function will generate a DotDict derivative class that has key\n    translation built in.  If the key is not found, translations (as specified\n    by the translation_tuples) are performed on the key and the lookup is\n    tried again.  Only on failure of this second lookup will the KeyError\n    exception be raised.\n\n    parameters:\n        new_class_name - the name of the returned class\n        translation_tuples - a sequence of 2-tuples of the form:\n                             (original_substring, substitution_string)\n        base_class - the baseclass on which this new class is to be based\n    \"\"\"\n    #==========================================================================\n    class DotDictWithKeyTranslations(base_class):\n\n        def __init__(self, *args, **kwargs):\n            self.__dict__['_translation_tuples'] = translation_tuples\n            super(DotDictWithKeyTranslations, self).__init__(*args, **kwargs)\n\n        #----------------------------------------------------------------------\n        @memoize()\n        def _translate_key(self, key):\n            for original, replacement in self._translation_tuples:\n                key = key.replace(original, replacement)\n            return key\n\n        #----------------------------------------------------------------------\n        def assign(self, key, value):\n            super(DotDictWithKeyTranslations, self).assign(\n                self._translate_key(key),\n                value\n            )\n\n        #----------------------------------------------------------------------\n        def __setattr__(self, key, value):\n            super(DotDictWithKeyTranslations, self).__setattr__(\n                self._translate_key(key),\n                value\n            )\n\n        #----------------------------------------------------------------------\n        def __getattr__(self, key):\n            alt_key = self._translate_key(key)\n            if alt_key == key:\n                return super(DotDictWithKeyTranslations, self).__getattr__(key)\n            try:\n                return getattr(self, alt_key)\n            except KeyError:\n                raise KeyError(key)\n\n        #----------------------------------------------------------------------\n        def __delattr__(self, key):\n            super(DotDictWithKeyTranslations, self).__delattr__(\n                self._translate_key(key)\n            )\n\n    if six.PY2:\n        new_class_name = six.binary_type(new_class_name)\n    DotDictWithKeyTranslations.__name__ = new_class_name\n    return DotDictWithKeyTranslations", "response": "This function creates a new class that has key translation built in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef memoize(max_cache_size=1000):\n    def wrapper(f):\n        @wraps(f)\n        def fn(*args, **kwargs):\n            if kwargs:\n                key = (args, tuple(kwargs.items()))\n            else:\n                key = args\n            try:\n                return fn.cache[key]\n            except KeyError:\n                if fn.count >= max_cache_size:\n                    fn.cache = {}\n                    fn.count = 0\n                result = f(*args, **kwargs)\n                fn.cache[key] = result\n                fn.count += 1\n                return result\n            except TypeError:\n                return f(*args, **kwargs)\n        fn.cache = {}\n        fn.count = 0\n        return fn\n    return wrapper", "response": "A function decorator that creates a cache that can be used to store the result of a function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self):\n        o = Option(\n            name=self.name,\n            default=self.default,\n            doc=self.doc,\n            from_string_converter=self.from_string_converter,\n            to_string_converter=self.to_string_converter,\n            value=self.value,\n            short_form=self.short_form,\n            exclude_from_print_conf=self.exclude_from_print_conf,\n            exclude_from_dump_conf=self.exclude_from_dump_conf,\n            is_argument=self.is_argument,\n            likely_to_be_changed=self.likely_to_be_changed,\n            not_for_definition=self.not_for_definition,\n            reference_value_from=self.reference_value_from,\n            secret=self.secret,\n            has_changed=self.has_changed,\n            foreign_data=self.foreign_data,\n        )\n        return o", "response": "returns a copy of the option"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a config as a context that calls close on every item when it goes out of scope", "response": "def context(self, mapping_class=DotDictWithAcquisition):\n        \"\"\"return a config as a context that calls close on every item when\n        it goes out of scope\"\"\"\n        config = None\n        try:\n            config = self.get_config(mapping_class=mapping_class)\n            yield config\n        finally:\n            if config:\n                self._walk_and_close(config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a summary of the current command line options.", "response": "def output_summary(self, output_stream=sys.stdout):\n        \"\"\"outputs a usage tip and the list of acceptable commands.\n        This is useful as the output of the 'help' option.\n\n        parameters:\n            output_stream - an open file-like object suitable for use as the\n                            target of a print function\n        \"\"\"\n        if self.app_name or self.app_description:\n            print('Application: ', end='', file=output_stream)\n        if self.app_name:\n            print(self.app_name, self.app_version, file=output_stream)\n        if self.app_description:\n            print(self.app_description, file=output_stream)\n        if self.app_name or self.app_description:\n            print('', file=output_stream)\n\n        names_list = self.get_option_names()\n        print(\n            \"usage:\\n%s [OPTIONS]... \" % self.app_invocation_name,\n            end='', file=output_stream\n        )\n        bracket_count = 0\n        # this section prints the non-switch command line arguments\n        for key in names_list:\n            an_option = self.option_definitions[key]\n            if an_option.is_argument:\n                if an_option.default is None:\n                    # there's no option, assume the user must set this\n                    print(an_option.name, end='', file=output_stream)\n                elif (\n                    inspect.isclass(an_option.value)\n                    or inspect.ismodule(an_option.value)\n                ):\n                    # this is already set and it could have expanded, most\n                    # likely this is a case where a sub-command has been\n                    # loaded and we're looking to show the help for it.\n                    # display show it as a constant already provided rather\n                    # than as an option the user must provide\n                    print(an_option.default, end='', file=output_stream)\n                else:\n                    # this is an argument that the user may alternatively\n                    # provide\n                    print(\"[ %s\" % an_option.name, end='', file=output_stream)\n                    bracket_count += 1\n        print(']' * bracket_count, '\\n', file=output_stream)\n\n        names_list.sort()\n        if names_list:\n            print('OPTIONS:', file=output_stream)\n\n        pad = ' ' * 4\n\n        for name in names_list:\n            if name in self.options_banned_from_help:\n                continue\n            option = self._get_option(name)\n\n            line = ' ' * 2  # always start with 2 spaces\n            if option.short_form:\n                line += '-%s, ' % option.short_form\n            line += '--%s' % name\n            line += '\\n'\n\n            doc = option.doc if option.doc is not None else ''\n            if doc:\n                line += '%s%s\\n' % (pad, doc)\n            try:\n                value = option.value\n                type_of_value = type(value)\n                converter_function = to_string_converters[type_of_value]\n                default = converter_function(value)\n            except KeyError:\n                default = option.value\n            if default is not None:\n                if (\n                    (option.secret or 'password' in name.lower()) and\n                    not self.option_definitions.admin.expose_secrets.default\n                ):\n                    default = '*********'\n                if name not in ('help',):\n                    # don't bother with certain dead obvious ones\n                    line += '%s(default: %s)\\n' % (pad, default)\n\n            print(line, file=output_stream)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_conf(self):\n\n        config_file_type = self._get_option('admin.print_conf').value\n\n        @contextlib.contextmanager\n        def stdout_opener():\n            yield sys.stdout\n\n        skip_keys = [\n            k for (k, v)\n            in six.iteritems(self.option_definitions)\n            if isinstance(v, Option) and v.exclude_from_print_conf\n        ]\n        self.write_conf(config_file_type, stdout_opener, skip_keys=skip_keys)", "response": "write a config file to the pathname specified in the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a config file to the pathname specified in the parameter.", "response": "def dump_conf(self, config_pathname=None):\n        \"\"\"write a config file to the pathname specified in the parameter.  The\n        file extention determines the type of file written and must match a\n        registered type.\n\n        parameters:\n            config_pathname - the full path and filename of the target config\n                               file.\"\"\"\n\n        if not config_pathname:\n            config_pathname = self._get_option('admin.dump_conf').value\n\n        opener = functools.partial(open, config_pathname, 'w')\n        config_file_type = os.path.splitext(config_pathname)[1][1:]\n\n        skip_keys = [\n            k for (k, v)\n            in six.iteritems(self.option_definitions)\n            if isinstance(v, Option) and v.exclude_from_dump_conf\n        ]\n\n        self.write_conf(config_file_type, opener, skip_keys=skip_keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a configuration file to a file - like object.", "response": "def write_conf(self, config_file_type, opener, skip_keys=None):\n        \"\"\"write a configuration file to a file-like object.\n\n        parameters:\n            config_file_type - a string containing a registered file type OR\n                               a for_XXX module from the value_source\n                               package.  Passing in an string that is\n                               unregistered will result in a KeyError\n            opener - a callable object or function that returns a file like\n                     object that works as a context in a with statement.\"\"\"\n\n        blocked_keys = self.keys_blocked_from_output\n        if skip_keys:\n            blocked_keys.extend(skip_keys)\n\n        if blocked_keys:\n            option_defs = self.option_definitions.safe_copy()\n            for a_blocked_key in blocked_keys:\n                try:\n                    del option_defs[a_blocked_key]\n                except (AttributeError, KeyError):\n                    # okay that key isn't here\n                    pass\n            # remove empty namespaces\n            all_keys = [k for k in\n                        option_defs.keys_breadth_first(include_dicts=True)]\n            for key in all_keys:\n                candidate = option_defs[key]\n                if (isinstance(candidate, Namespace) and not len(candidate)):\n                    del option_defs[key]\n        else:\n            option_defs = self.option_definitions\n\n        # find all of the secret options and overwrite their values with\n        # '*' * 16\n        if not self.option_definitions.admin.expose_secrets.default:\n            for a_key in option_defs.keys_breadth_first():\n                an_option = option_defs[a_key]\n                if (\n                    (not a_key.startswith('admin'))\n                    and isinstance(an_option, Option)\n                    and an_option.secret\n                ):\n                    # force the option to be a string of *\n                    option_defs[a_key].value = '*' * 16\n                    option_defs[a_key].from_string_converter = str\n\n        dispatch_request_to_write(config_file_type, option_defs, opener)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_config(self, logger):\n\n        logger.info(\"app_name: %s\", self.app_name)\n        logger.info(\"app_version: %s\", self.app_version)\n        logger.info(\"current configuration:\")\n        config = [(key, self.option_definitions[key].value)\n                  for key in self.option_definitions.keys_breadth_first()\n                  if key not in self.keys_blocked_from_output]\n        config.sort()\n        for key, val in config:\n            if (\n                self.option_definitions[key].secret\n                or 'password' in key.lower()\n            ):\n                logger.info('%s: *********', key)\n            else:\n                try:\n                    logger.info('%s: %s', key,\n                                to_string_converters[type(key)](val))\n                except KeyError:\n                    logger.info('%s: %s', key, val)", "response": "write out the current configuration to a log - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_option_names(self):\n        return [x for x in self.option_definitions.keys_breadth_first()\n                if isinstance(self.option_definitions[x], Option)]", "response": "returns a list of fully qualified option names."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_reference_value_options(self, keys, finished_keys):\n        # a set of known reference_value_from_links\n        set_of_reference_value_option_names = set()\n        for key in keys:\n            if key in finished_keys:\n                continue\n            an_option = self.option_definitions[key]\n            if an_option.reference_value_from:\n\n                fully_qualified_reference_name = '.'.join((\n                    an_option.reference_value_from,\n                    an_option.name\n                ))\n                if fully_qualified_reference_name in keys:\n                    continue  # this referenced value has already been defined\n                              # no need to repeat it - skip on to the next key\n                reference_option = an_option.copy()\n                reference_option.reference_value_from = None\n                reference_option.name = fully_qualified_reference_name\n                # wait, aren't we setting a fully qualified dotted name into\n                # the name field?  Yes, 'add_option' below sees that\n                # full pathname and does the right thing with it to ensure\n                # that the reference_option is created within the\n                # correct namespace\n                set_of_reference_value_option_names.add(\n                    fully_qualified_reference_name\n                )\n                self.option_definitions.add_option(reference_option)\n\n        for a_reference_value_option_name in set_of_reference_value_option_names:\n            for x in range(a_reference_value_option_name.count('.')):\n                namespace_path = \\\n                    a_reference_value_option_name.rsplit('.', x + 1)[0]\n                self.option_definitions[namespace_path].ref_value_namespace()\n\n        return set_of_reference_value_option_names", "response": "This method creates the reference_value_from links and name fields for the option definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for mismatches from value sources", "response": "def _check_for_mismatches(self, known_keys):\n        \"\"\"check for bad options from value sources\"\"\"\n        for a_value_source in self.values_source_list:\n            try:\n                if a_value_source.always_ignore_mismatches:\n                    continue\n            except AttributeError:\n                # ok, this values source doesn't have the concept\n                # always igoring mismatches, we won't tolerate mismatches\n                pass\n            # we want to fetch the keys from the value sources so that we can\n            # check for mismatches.  Commandline value sources, are different,\n            # we never want to allow unmatched keys from the command line.\n            # By detecting if this value source is a command line source, we\n            # can employ the command line's own mismatch detection.  The\n            # boolean 'allow_mismatches' controls application of the tollerance\n            # for mismatches.\n            if hasattr(a_value_source, 'command_line_value_source'):\n                allow_mismatches = False\n            else:\n                allow_mismatches = True\n            # make a set of all the keys from a value source in the form\n            # of strings like this: 'x.y.z'\n            value_source_mapping = a_value_source.get_values(\n                self,\n                allow_mismatches,\n                self.value_source_object_hook\n            )\n            value_source_keys_set = set([\n                k for k in\n                DotDict(value_source_mapping).keys_breadth_first()\n            ])\n            # make a set of the keys that didn't match any of the known\n            # keys in the requirements\n            unmatched_keys = value_source_keys_set.difference(known_keys)\n            # some of the unmatched keys may actually be ok because the were\n            # used during acquisition.\n            # remove keys of the form 'y.z' if they match a known key of the\n            # form 'x.y.z'\n            for key in unmatched_keys.copy():\n                key_is_okay = six.moves.reduce(\n                    lambda x, y: x or y,\n                    (known_key.endswith(key) for known_key in known_keys)\n                )\n                if key_is_okay:\n                    unmatched_keys.remove(key)\n            # anything left in the unmatched_key set is a badly formed key.\n            # issue a warning\n            if unmatched_keys:\n                if self.option_definitions.admin.strict.default:\n                    # raise hell...\n                    if len(unmatched_keys) > 1:\n                        raise NotAnOptionError(\n                            \"%s are not valid Options\" % unmatched_keys\n                        )\n                    elif len(unmatched_keys) == 1:\n                        raise NotAnOptionError(\n                            \"%s is not a valid Option\" % unmatched_keys.pop()\n                        )\n                else:\n                    warnings.warn(\n                        'Invalid options: %s' % ', '.join(sorted(unmatched_keys))\n                    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_config(self, mapping_class):\n        config = mapping_class()\n        self._walk_config_copy_values(\n            self.option_definitions,\n            config,\n            mapping_class\n        )\n        return config", "response": "This routine generates a copy of the DotDict based config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        print(\"PGPooledTransaction - shutting down connection pool\")\n        for name, conn in self.pool.iteritems():\n            conn.close()\n            print(\"PGPooledTransaction - connection %s closed\" % name)", "response": "close all pooled connections"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_action_name_by_value(registry, target_action_instance):\n    target_type = type(target_action_instance)\n    for key, value in six.iteritems(registry['action']):\n        if value is target_type:\n            if key is None:\n                return 'store'\n            return key\n    return None", "response": "This routine finds the name of an action class with a human readable\n    string is exposed externally only at the time of argument definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_args_and_values(parser, an_action):\n    args = inspect.getargspec(an_action.__class__.__init__).args\n    kwargs = dict(\n        (an_attr, getattr(an_action, an_attr))\n        for an_attr in args\n        if (\n            an_attr not in ('self', 'required')\n            and getattr(an_action, an_attr) is not None\n        )\n    )\n    action_name = find_action_name_by_value(\n        parser._optionals._registries,\n        an_action\n    )\n    if 'required' in kwargs:\n        del kwargs['required']\n    kwargs['action'] = action_name\n    if 'option_strings' in kwargs:\n        args = tuple(kwargs['option_strings'])\n        del kwargs['option_strings']\n    else:\n        args = ()\n    return args, kwargs", "response": "this rountine attempts to reconstruct the kwargs that were used in the action object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_namespace(self, name, a_namespace):\n        # save a local copy of the namespace\n        self.namespaces[name] = a_namespace\n        # iterate through the namespace branding each of the options with the\n        # name of the subparser to which they belong\n        for k in a_namespace.keys_breadth_first():\n            an_option = a_namespace[k]\n            if not an_option.foreign_data:\n                an_option.foreign_data = DotDict()\n            an_option.foreign_data['argparse.owning_subparser_name'] = name", "response": "add a namespace to the argparse object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_required_config(self):\n        required_config = Namespace()\n        # add current options to a copy of required config\n        for k, v in iteritems_breadth_first(self.required_config):\n            required_config[k] = v\n        # get any option found in any subparsers\n        try:\n            subparser_namespaces = (\n                self.configman_subparsers_option.foreign_data\n                .argparse.subprocessor_from_string_converter\n            )\n            subparsers = (\n                self._argparse_subparsers._configman_option.foreign_data\n                .argparse.subparsers\n            )\n            # each subparser needs to have its configman options set up\n            # in the subparser's configman option.  This routine copies\n            # the required_config of each subparser into the\n            # SubparserFromStringConverter defined above.\n            for subparser_name, subparser_data in six.iteritems(subparsers):\n                subparser_namespaces.add_namespace(\n                    subparser_name,\n                    subparser_data.subparser.get_required_config()\n                )\n        except AttributeError:\n            # there is no subparser\n            pass\n        return required_config", "response": "This method returns the required_config dictionary that is a copy of the namespace rather than the actual embedded\n        namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_argument(self, *args, **kwargs):\n        # pull out each of the argument definition components from the args\n        # so that we can deal with them one at a time in a well labeled manner\n        # In this section, variables beginning with the prefix \"argparse\" are\n        # values that define Action object.  Variables that begin with\n        # \"configman\" are the arguments to create configman Options.\n        argparse_action_name = kwargs.get('action', None)\n        argparse_dest = kwargs.get('dest', None)\n        argparse_const = kwargs.get('const', None)\n        argparse_default = kwargs.get('default', None)\n        if argparse_default is argparse.SUPPRESS:\n            # we'll be forcing all options to have the attribute of\n            # argparse.SUPPRESS later.  It's our way of making sure that\n            # argparse returns only values that the user explicitly added to\n            # the command line.\n            argparse_default = None\n        argparse_nargs = kwargs.get('nargs', None)\n        argparse_type = kwargs.get('type', None)\n        argparse_suppress_help = kwargs.pop('suppress_help', False)\n        if argparse_suppress_help:\n            configman_doc = kwargs.get('help', '')\n            kwargs['help'] = argparse.SUPPRESS\n        else:\n            argparse_help = kwargs.get('help', '')\n            if argparse_help == argparse.SUPPRESS:\n                configman_doc = ''\n            else:\n                configman_doc = argparse_help\n\n        # we need to make sure that all arguments that the user has not\n        # explicily set on the command line have this attribute.  This means\n        # that when the argparse parser returns the command line values, it\n        # will not return values that the user did not mention on the command\n        # line.  The defaults that otherwise would have been returned will be\n        # handled by configman.\n        kwargs['default'] = argparse.SUPPRESS\n        # forward all parameters to the underlying base class to create a\n        # normal argparse action object.\n        an_action = super(ArgumentParser, self).add_argument(\n            *args,\n            **kwargs\n        )\n        argparse_option_strings = an_action.option_strings\n\n        # get a human readable string that identifies the type of the argparse\n        # action class that was created\n        if argparse_action_name is None:\n            argparse_action_name = find_action_name_by_value(\n                self._optionals._registries,\n                an_action\n            )\n\n        configman_is_argument = False\n\n        # each of argparse's Action types must be handled separately.\n        #--------------------------------------------------------------------\n        # STORE\n        if argparse_action_name == 'store':\n            if argparse_dest is None:\n                configman_name, configman_is_argument = self._get_option_name(\n                    args\n                )\n                if not configman_name:\n                    configman_name = args[0]\n            else:\n                configman_name = argparse_dest\n                configman_is_argument = not argparse_option_strings\n            configman_default = argparse_default\n            if argparse_nargs and argparse_nargs in \"1?\":\n                if argparse_type:\n                    configman_from_string = argparse_type\n                elif argparse_default:\n                    configman_from_string = (\n                        str_to_instance_of_type_converters.get(\n                            type(argparse_default),\n                            str\n                        )\n                    )\n                else:\n                    configman_from_string = str\n            elif argparse_nargs and argparse_type:\n                configman_from_string = partial(\n                    str_to_list,\n                    item_converter=argparse_type,\n                    item_separator=' ',\n                )\n            elif argparse_nargs and argparse_default:\n                configman_from_string = partial(\n                    str_to_list,\n                    item_converter=str_to_instance_of_type_converters.get(\n                        type(argparse_default),\n                        str\n                    ),\n                    item_separator=' ',\n                )\n            elif argparse_nargs:\n                configman_from_string = partial(\n                    str_to_list,\n                    item_converter=str,\n                    item_separator=' ',\n                )\n            elif argparse_type:\n                configman_from_string = argparse_type\n            elif argparse_default:\n                configman_from_string = str_to_instance_of_type_converters.get(\n                    type(argparse_default),\n                    str\n                )\n            else:\n                configman_from_string = str\n            configman_to_string = to_str\n\n        #--------------------------------------------------------------------\n        # STORE_CONST\n        elif (\n            argparse_action_name == 'store_const'\n            or argparse_action_name == 'count'\n        ):\n            if argparse_dest is None:\n                configman_name, configman_is_argument = self._get_option_name(\n                    args\n                )\n                if not configman_name:\n                    configman_name = args[0]\n            else:\n                configman_name = argparse_dest\n            configman_default = argparse_default\n            if argparse_type:\n                configman_from_string = argparse_type\n            else:\n                configman_from_string = str_to_instance_of_type_converters.get(\n                    type(argparse_const),\n                    str\n                )\n            configman_to_string = to_str\n\n        #--------------------------------------------------------------------\n        # STORE_TRUE /  STORE_FALSE\n        elif (\n            argparse_action_name == 'store_true'\n            or argparse_action_name == 'store_false'\n        ):\n            if argparse_dest is None:\n                configman_name, configman_is_argument = self._get_option_name(\n                    args\n                )\n                if not configman_name:\n                    configman_name = args[0]\n            else:\n                configman_name = argparse_dest\n            configman_default = argparse_default\n            configman_from_string = boolean_converter\n            configman_to_string = to_str\n\n        #--------------------------------------------------------------------\n        # APPEND\n        elif argparse_action_name == 'append':\n            if argparse_dest is None:\n                configman_name, configman_is_argument = self._get_option_name(\n                    args\n                )\n                if not configman_name:\n                    configman_name = args[0]\n            else:\n                configman_name = argparse_dest\n            configman_default = argparse_default\n            if argparse_type:\n                configman_from_string = argparse_type\n            else:\n                configman_from_string = str\n            configman_to_string = to_str\n\n        #--------------------------------------------------------------------\n        # APPEND_CONST\n        elif argparse_action_name == 'append_const':\n            if argparse_dest is None:\n                configman_name, configman_is_argument = self._get_option_name(\n                    args\n                )\n                if not configman_name:\n                    configman_name = args[0]\n            else:\n                configman_name = argparse_dest\n            configman_default = argparse_default\n            if argparse_type:\n                configman_from_string = argparse_type\n            else:\n                configman_from_string = str_to_instance_of_type_converters.get(\n                    type(argparse_const),\n                    str\n                )\n            configman_to_string = to_str\n\n        #--------------------------------------------------------------------\n        # VERSION\n        elif argparse_action_name == 'version':\n            return an_action\n\n        #--------------------------------------------------------------------\n        # OTHER\n        else:\n            configman_name = argparse_dest\n\n        # configman uses the switch name as the name of the key inwhich to\n        # store values.  argparse is able to use different names for each.\n        # this means that configman may encounter repeated targets.  Rather\n        # than overwriting Options with new ones with the same name, configman\n        # renames them by appending the '$' character.\n        while configman_name in self.required_config:\n            configman_name = \"%s$\" % configman_name\n        configman_not_for_definition = configman_name.endswith('$')\n\n        # it's finally time to create the configman Option object and add it\n        # to the required_config.\n        self.required_config.add_option(\n            name=configman_name,\n            default=configman_default,\n            doc=configman_doc,\n            from_string_converter=configman_from_string,\n            to_string_converter=configman_to_string,\n            #short_form=configman_short_form,\n            is_argument=configman_is_argument,\n            not_for_definition=configman_not_for_definition,\n            # we're going to save the args & kwargs that created the\n            # argparse Action.  This enables us to perfectly reproduce the\n            # the original Action object later during the configman overlay\n            # process.\n            foreign_data=DotDict({\n                'argparse.flags.subcommand': False,\n                'argparse.args': args,\n                'argparse.kwargs': kwargs,\n                'argparse.owning_subparser_name': self.subparser_name,\n            })\n        )\n        return an_action", "response": "This method adds an argument to the required_config\n            section of the configman object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_subparsers(self, *args, **kwargs):\n\n        kwargs['parser_class'] = self.__class__\n        kwargs['action'] = ConfigmanSubParsersAction\n\n        subparser_action = super(ArgumentParser, self).add_subparsers(\n            *args,\n            **kwargs\n        )\n        self._argparse_subparsers = subparser_action\n\n        if \"dest\" not in kwargs or kwargs['dest'] is None:\n            kwargs['dest'] = 'subcommand'\n        configman_name = kwargs['dest']\n        configman_default = None\n        configman_doc = kwargs.get('help', '')\n        subprocessor_from_string_converter = SubparserFromStringConverter()\n        configman_to_string = str\n        configman_is_argument = True\n        configman_not_for_definition = True\n\n        # it's finally time to create the configman Option object and add it\n        # to the required_config.\n        self.required_config.add_option(\n            name=configman_name,\n            default=configman_default,\n            doc=configman_doc,\n            from_string_converter=subprocessor_from_string_converter,\n            to_string_converter=configman_to_string,\n            is_argument=configman_is_argument,\n            not_for_definition=configman_not_for_definition,\n            # we're going to save the input parameters that created the\n            # argparse Action.  This enables us to perfectly reproduce the\n            # the original Action object later during the configman overlay\n            # process.\n            foreign_data=DotDict({\n                'argparse.flags.subcommand': subparser_action,\n                'argparse.args': args,\n                'argparse.kwargs': kwargs,\n                'argparse.subparsers': DotDict(),\n                'argparse.subprocessor_from_string_converter':\n                subprocessor_from_string_converter\n            })\n        )\n\n        self.configman_subparsers_option = self.required_config[configman_name]\n        subparser_action.add_configman_option(self.configman_subparsers_option)\n\n        return subparser_action", "response": "When adding a subparser we need to create the configman Option object for the subparser and pack it s foreign data section with the original args & kwargs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_known_args(self, args=None, namespace=None):\n        # load the config_manager within the scope of the method that uses it\n        # so that we avoid circular references in the outer scope\n        from configman.config_manager import ConfigurationManager\n        configuration_manager = ConfigurationManager(\n            definition_source=[self.get_required_config()],\n            values_source_list=self.value_source_list,\n            argv_source=args,\n            app_name=self.prog,\n            app_version=self.version,\n            app_description=self.description,\n            use_auto_help=False,\n        )\n        conf = configuration_manager.get_config(\n            mapping_class=create_key_translating_dot_dict(\n                \"HyphenUnderscoreDict\",\n                (('-', '_'),)\n            )\n        )\n        return conf", "response": "this method hijacks the normal argparse Namespace generation and shimming configman into the process. The return value will be a\n        configman DotDict rather than an argparse Namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sequence_to_string(\n    a_list,\n    open_bracket_char='[',\n    close_bracket_char=']',\n    delimiter=\", \"\n):\n    \"\"\"a dedicated function that turns a list into a comma delimited string\n    of items converted.  This method will flatten nested lists.\"\"\"\n    return \"%s%s%s\" % (\n        open_bracket_char,\n        delimiter.join(\n            local_to_str(x)\n            for x in a_list\n        ),\n        close_bracket_char\n    )", "response": "a dedicated function that turns a list into a comma delimited string\n    of items converted."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a type return a tuple of the module - path type_name", "response": "def get_import_for_type(t):\n    \"\"\"given a type, return a tuple of the (module-path, type_name)\n    or (None, None) if it is a built in.\"\"\"\n    t_as_string = to_str(t)\n    if not is_identifier(t_as_string):\n        # this class expanded into something other than a single identifier\n        # we can ignore it.  This is the case when we encounter something\n        # like the configman.converter.str_to_classes_in_namespaces\n        # InnerClassList.  We can safely ignore these things here.\n        return (None, None)\n    if '.' in t_as_string:\n        parts = t_as_string.split('.')\n        return ('.'.join(parts[:-1]), parts[-1])\n    else:\n        if t_as_string in known_mapping_str_to_type:\n            return (None, None)\n        return (None, t_as_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(source_mapping, output_stream=sys.stdout):\n        # a set of classes, modules and/or functions that are values in\n        # configman options.  These values will have to be imported in the\n        # module that this method is writing.\n        set_of_classes_needing_imports = set()\n        # once symbols are imported, they are in the namespace of the module,\n        # but that's not where we want them.  We only want them to be values\n        # in configman Options.  This set will be used to make a list of\n        # these symbols, to forewarn a future configman that reads this\n        # module, that it can ignore these symbols. This will prevent that\n        # future configman from issuing a \"mismatced symbols\" warinng.\n        symbols_to_ignore = set()\n\n        # look ahead to see what sort of imports we're going to have to do\n        for key in source_mapping.keys_breadth_first():\n            value = source_mapping[key]\n\n            if isinstance(value, Aggregation):\n                # Aggregations don't get included, skip on\n                continue\n\n            if '.' in key:\n                # this indicates that there are things in nested namespaces,\n                # we will use the DotDict class to represent namespaces\n                set_of_classes_needing_imports.add(DotDict)\n\n            option = None\n            if isinstance(value, Option):\n                # it's the value inside the option, not the option itself\n                # that is of interest to us\n                option = value\n                value = option.value\n\n            if value is None:\n                # we don't need in import anything having to do with None\n                continue\n\n            if isclass(value) or ismodule(value) or isfunction(value):\n                # we know we need to import any of these types\n                set_of_classes_needing_imports.add(value)\n            else:\n                try:\n                    # perhaps the value is an instance of a class?  If so,\n                    # we'll likely need to import that class, but only if\n                    # we don't have a way to convert a string to that class\n                    set_of_classes_needing_imports.add(value.__class__)\n                except AttributeError:\n                    # it's not a class instance, we can skip on\n                    pass\n\n        # for everyone of the imports that we're going to have to create\n        # we need to know the dotted module pathname and the name of the\n        # of the class/module/function.  This routine make a list of 3-tuples\n        # class, dotted_module_path, class_name\n        class_and_module_path_and_class_name = []\n        for a_class in set_of_classes_needing_imports:\n            module_path, class_name = get_import_for_type(a_class)\n            if (not module_path) and (not class_name):\n                continue\n            class_and_module_path_and_class_name.append(\n                (a_class, module_path, class_name)\n            )\n\n        # using the collection of 3-tuples, create a lookup mapping where a\n        # class is the key to a 2-tuple of the dotted_module_path & class_name.\n        # This is also the appropriate time to detect any name collisions\n        # and create a mapping of aliases, so we can resolve name collisions.\n        class_name_by_module_path_list = defaultdict(list)\n        alias_by_class = {}\n        previously_used_names = set()\n        for (\n            a_class,\n            a_module_path,\n            class_name\n        ) in class_and_module_path_and_class_name:\n            if class_name:\n                if class_name in previously_used_names:\n                    new_class_name_alias = \"%s_%s\" % (\n                        a_module_path.replace('.', '_'),\n                        class_name\n                    )\n                    alias_by_class[a_class] = new_class_name_alias\n                    previously_used_names.add(new_class_name_alias)\n                else:\n                    previously_used_names.add(class_name)\n                class_name_by_module_path_list[a_module_path].append(\n                    (a_class, class_name)\n                )\n\n        # start writing the output module\n        print(\"# generated Python configman file\\n\", file=output_stream)\n\n        # the first section that we're going to write is imports of the form:\n        #     from X import Y\n        # and\n        #     from X import (\n        #         A,\n        #         B,\n        #     )\n        sorted_list = [x.value for x in sorted([OrderableObj(x) for x in\n                       class_name_by_module_path_list.keys()])]\n        for a_module_path in sorted_list:\n            print(a_module_path)\n            # if there is no module path, then it is something that we don't\n            # need to import.  If the module path begins with underscore then\n            # it is private and we ought not step into that mire.  If that\n            # causes the output module to fail, it is up to the implementer\n            # of the configman option to have created an approprate\n            # \"from_string\" & \"to_string\" configman Option function references.\n            if a_module_path is None or a_module_path.startswith('_'):\n                continue\n            list_of_class_names = \\\n                class_name_by_module_path_list[a_module_path]\n            if len(list_of_class_names) > 1:\n                output_line = \"from %s import (\\n\" % a_module_path\n                sorted_list = [x.value for x in sorted([OrderableTuple(x)\n                               for x in list_of_class_names])]\n                for a_class, a_class_name in sorted_list:\n                    if a_class in alias_by_class:\n                        output_line = \"%s\\n    %s as %s,\" % (\n                            output_line,\n                            a_class_name,\n                            alias_by_class[a_class]\n                        )\n                        symbols_to_ignore.add(alias_by_class[a_class])\n                    else:\n                        output_line = \"%s    %s,\\n\" % (\n                            output_line,\n                            a_class_name\n                        )\n                        symbols_to_ignore.add(a_class_name)\n\n                output_line = output_line + ')'\n                print(output_line.strip(), file=output_stream)\n            else:\n                a_class, a_class_name = list_of_class_names[0]\n                output_line = \"from %s import %s\" % (\n                    a_module_path,\n                    a_class_name\n                )\n                if a_class in alias_by_class:\n                    output_line = \"%s as %s\" % (\n                        output_line,\n                        alias_by_class[a_class]\n                    )\n                    symbols_to_ignore.add(alias_by_class[a_class])\n                else:\n                    symbols_to_ignore.add(a_class_name)\n                print(output_line.strip(), file=output_stream)\n        print('', file=output_stream)\n\n        # The next section to write will be the imports of the form:\n        #     import X\n        sorted_list = [x.value for x in sorted([OrderableObj(x) for x in\n                       class_name_by_module_path_list.keys()])]\n        for a_module_path in sorted_list:\n            list_of_class_names = \\\n                class_name_by_module_path_list[a_module_path]\n            a_class, a_class_name = list_of_class_names[0]\n            if a_module_path:\n                continue\n            import_str = (\"import %s\" % a_class_name).strip()\n            symbols_to_ignore.add(a_class_name)\n            print(import_str, file=output_stream)\n\n        # See the explanation of 'symbols_to_ignore' above\n        if symbols_to_ignore:\n            print(\n                \"\\n\" \\\n                \"# the following symbols will be ignored by configman when\\n\" \\\n                \"# this module is used as a value source.  This will\\n\" \\\n                \"# suppress the mismatch warning since these symbols are\\n\" \\\n                \"# values for options, not option names themselves.\",\n                file=output_stream\n            )\n            print(\"ignore_symbol_list = [\", file=output_stream)\n            for a_symbol in sorted(symbols_to_ignore):\n                print('    \"%s\",' % a_symbol, file=output_stream)\n            print(']\\n', file=output_stream)\n\n        # finally, as the last step, we need to write out the keys and values\n        # will be used by a future configman as Options and values.\n        sorted_keys = sorted(\n            source_mapping.keys_breadth_first(include_dicts=True)\n        )\n        for key in sorted_keys:\n            value = source_mapping[key]\n            if isinstance(value, Namespace):\n                ValueSource.write_namespace(key, value, output_stream)\n            elif isinstance(value, Option):\n                ValueSource.write_option(\n                    key,\n                    value,\n                    alias_by_class,\n                    output_stream\n                )\n            elif isinstance(value, Aggregation):\n                # skip Aggregations\n                continue\n            else:\n                ValueSource.write_bare_value(key, value, output_stream)", "response": "This method writes a Python module respresenting all the keys\n        and values known to configman."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datetime_from_ISO_string(s):\n    try:\n        return datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%S')\n    except ValueError:\n        try:\n            return datetime.datetime.strptime(s, '%Y-%m-%d')\n        except ValueError:\n            return datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f')", "response": "Take an ISO date string of the form YYYY - MM - DDTHH - MM - SS. S\n    and convert it into an instance of datetime. datetime\nIVA."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef str_to_timedelta(input_str):\n    try:\n        input_str = input_str.replace(' ', ':')\n    except (TypeError, AttributeError):\n        from configman.converters import to_str\n        raise TypeError('%s should have been a string' % to_str(input_str))\n    days, hours, minutes, seconds = 0, 0, 0, 0\n    details = input_str.split(':')\n    if len(details) >= 4:\n        days = int(details[-4])\n    if len(details) >= 3:\n        hours = int(details[-3])\n    if len(details) >= 2:\n        minutes = int(details[-2])\n    if len(details) >= 1:\n        seconds = int(details[-1])\n    return datetime.timedelta(days=days,\n                              hours=hours,\n                              minutes=minutes,\n                              seconds=seconds)", "response": "a string conversion function for timedelta for strings in the format\n    DD HHMMSS or D HHMMSS or D HHMMSS or D HHMMSS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timedelta_to_str(aTimedelta):\n    days = aTimedelta.days\n    temp_seconds = aTimedelta.seconds\n    hours = int(temp_seconds / 3600)\n    minutes = int((temp_seconds - hours * 3600) / 60)\n    seconds = temp_seconds - hours * 3600 - minutes * 60\n    return '%d %02d:%02d:%02d' % (days, hours, minutes, seconds)", "response": "a conversion function for time deltas to string in the form DDHHMMSS"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_source_file(file_name):\n    with open(file_name, 'r') as f:\n        s = f.read()\n\n    nodes = ast.parse(s)\n\n    module_imports = get_nodes_by_instance_type(nodes, _ast.Import)\n    specific_imports = get_nodes_by_instance_type(nodes, _ast.ImportFrom)\n\n    assignment_objs = get_nodes_by_instance_type(nodes, _ast.Assign)\n    call_objects = get_nodes_by_instance_type(nodes, _ast.Call)\n\n    argparse_assignments = get_nodes_by_containing_attr(assignment_objs, 'ArgumentParser')\n    group_arg_assignments = get_nodes_by_containing_attr(assignment_objs, 'add_argument_group')\n    add_arg_assignments = get_nodes_by_containing_attr(call_objects, 'add_argument')\n    parse_args_assignment = get_nodes_by_containing_attr(call_objects, 'parse_args')\n    # there are cases where we have custom argparsers, such as subclassing ArgumentParser. The above\n    # will fail on this. However, we can use the methods known to ArgumentParser to do a duck-type like\n    # approach to finding what is the arg parser\n    if not argparse_assignments:\n        aa_references = set([i.func.value.id for i in chain(add_arg_assignments, parse_args_assignment)])\n        argparse_like_objects = [getattr(i.value.func, 'id', None) for p_ref in aa_references for i in get_nodes_by_containing_attr(assignment_objs, p_ref)]\n        argparse_like_objects = filter(None, argparse_like_objects)\n        argparse_assignments = [get_nodes_by_containing_attr(assignment_objs, i) for i in argparse_like_objects]\n        # for now, we just choose one\n        try:\n            argparse_assignments = argparse_assignments[0]\n        except IndexError:\n            pass\n\n\n    # get things that are assigned inside ArgumentParser or its methods\n    argparse_assigned_variables = get_node_args_and_keywords(assignment_objs, argparse_assignments, 'ArgumentParser')\n    add_arg_assigned_variables = get_node_args_and_keywords(assignment_objs, add_arg_assignments, 'add_argument')\n    parse_args_assigned_variables = get_node_args_and_keywords(assignment_objs, parse_args_assignment, 'parse_args')\n\n    ast_argparse_source = chain(\n        module_imports,\n        specific_imports,\n        argparse_assigned_variables,\n        add_arg_assigned_variables,\n        parse_args_assigned_variables,\n        argparse_assignments,\n        group_arg_assignments,\n        add_arg_assignments,\n    )\n    return ast_argparse_source", "response": "Parses the source file and returns the grammar tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an option to the namespace. This can take two forms of name args and kwargs are the parameters of the option and the name of the object.", "response": "def add_option(self, name, *args, **kwargs):\n        \"\"\"add an option to the namespace.   This can take two forms:\n              'name' is a string representing the name of an option and the\n              kwargs are its parameters, or 'name' is an instance of an Option\n              object\n        \"\"\"\n        if isinstance(name, Option):\n            an_option = name\n            name = an_option.name\n        else:\n            an_option = Option(name, *args, **kwargs)\n\n        current_namespace = self\n        name_parts = name.split('.')\n        for a_path_component in name_parts[:-1]:\n            if a_path_component not in current_namespace:\n                current_namespace[a_path_component] = Namespace()\n            current_namespace = current_namespace[a_path_component]\n        an_option.name = name_parts[-1]\n\n        setattr(current_namespace, an_option.name, an_option)\n        return an_option"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmeasuring the cumulative Regression sum of Squares for each individual component and the sum of Squares for each individual component.", "response": "def _cummulativefit(self, x, y):\n        \"\"\"\n        Measure the cumulative Regression sum of Squares for each individual component.\n\n        :param x: Data matrix to fit the PLS model.\n        :type x: numpy.ndarray, shape [n_samples, n_features]\n        :param y: Data matrix to fit the PLS model.\n        :type y: numpy.ndarray, shape [n_samples, n_features]\n        :return: dictionary object containing the total Regression Sum of Squares and the Sum of Squares\n        per components, for both the X and Y data blocks.\n        :rtype: dict\n        \"\"\"\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n        if x.ndim == 1:\n            x = x.reshape(-1, 1)\n\n        xscaled = self.x_scaler.fit_transform(x)\n        yscaled = self.y_scaler.fit_transform(y)\n\n        ssx_comp = list()\n        ssy_comp = list()\n\n        # Obtain residual sum of squares for whole data set and per component\n        SSX = np.sum(xscaled ** 2)\n        SSY = np.sum(yscaled ** 2)\n        ssx_comp = list()\n        ssy_comp = list()\n\n        for curr_comp in range(1, self.ncomps + 1):\n            model = self._reduce_ncomps(curr_comp)\n\n            ypred = self.y_scaler.transform(model.predict(x, y=None))\n            xpred = self.x_scaler.transform(model.predict(x=None, y=y))\n            rssy = np.sum((yscaled - ypred) ** 2)\n            rssx = np.sum((xscaled - xpred) ** 2)\n            ssx_comp.append(rssx)\n            ssy_comp.append(rssy)\n\n        cumulative_fit = {'SSX': SSX, 'SSY': SSY, 'SSXcomp': np.array(ssx_comp), 'SSYcomp': np.array(ssy_comp)}\n\n        return cumulative_fit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the leverages of the current sample index", "response": "def plot_leverages(self):\n        \"\"\"\n        Leverage (h) per observation, with a red line plotted at y = 1/Number of samples (expected\n        :return: Plot with observation leverages (h)\n        \"\"\"\n        plt.figure()\n        lev = self.leverages()\n        plt.xlabel('Sample Index')\n        plt.ylabel('Leverage')\n        plt.bar(left=range(lev.size), height=lev)\n        plt.hlines(y=1/lev.size, xmin=0, xmax=lev.size, colors='r', linestyles='--')\n        plt.show()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_zeros_in_scale(scale, copy=True):\n\n    # if we are fitting on 1D arrays, scale might be a scalar\n    if numpy.isscalar(scale):\n        if scale == .0:\n            scale = 1.\n        return scale\n    elif isinstance(scale, numpy.ndarray):\n        if copy:\n            # New array to avoid side-effects\n            scale = scale.copy()\n        scale[scale == 0.0] = 1.0\n        return scale", "response": "Handles zero - based scalers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset(self):\n\n        # Checking one attribute is enough, because they are all set together\n        # in partial_fit\n        if hasattr(self, 'scale_'):\n            del self.scale_\n            del self.n_samples_seen_\n            del self.mean_\n            del self.var_", "response": "Reset internal data - dependent state of the scaler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the mean and standard deviation from a dataset to use in future scaling operations.", "response": "def fit(self, X, y=None):\n        \"\"\"\n        Compute the mean and standard deviation from a dataset to use in future scaling operations.\n\n        :param X: Data matrix to scale.\n        :type X: numpy.ndarray, shape [n_samples, n_features]\n        :param y: Passthrough for Scikit-learn ``Pipeline`` compatibility.\n        :type y: None\n        :return: Fitted object.\n        :rtype: pyChemometrics.ChemometricsScaler\n        \"\"\"\n\n        # Reset internal state before fitting\n        self._reset()\n        return self.partial_fit(X, y)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef partial_fit(self, X, y=None):\n\n        X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\n        # Even in the case of `with_mean=False`, we update the mean anyway\n        # This is needed for the incremental computation of the var\n        # See incr_mean_variance_axis and _incremental_mean_variance_axis\n\n        if sparse.issparse(X):\n            if self.with_mean:\n                raise ValueError(\n                    \"Cannot center sparse matrices: pass `with_mean=False` \"\n                    \"instead. See docstring for motivation and alternatives.\")\n            if self.with_std:\n                # First pass\n                if not hasattr(self, 'n_samples_seen_'):\n                    self.mean_, self.var_ = mean_variance_axis(X, axis=0)\n                    self.n_samples_seen_ = X.shape[0]\n                # Next passes\n                else:\n                    self.mean_, self.var_, self.n_samples_seen_ = \\\n                        incr_mean_variance_axis(X, axis=0,\n                                                last_mean=self.mean_,\n                                                last_var=self.var_,\n                                                last_n=self.n_samples_seen_)\n            else:\n                self.mean_ = None\n                self.var_ = None\n        else:\n            # First pass\n            if not hasattr(self, 'n_samples_seen_'):\n                self.mean_ = .0\n                self.n_samples_seen_ = 0\n                if self.with_std:\n                    self.var_ = .0\n                else:\n                    self.var_ = None\n\n            self.mean_, self.var_, self.n_samples_seen_ = \\\n                _incremental_mean_and_var(X, self.mean_, self.var_,\n                                          self.n_samples_seen_)\n\n        if self.with_std:\n            self.scale_ = _handle_zeros_in_scale(numpy.sqrt(self.var_)) ** self.scale_power\n        else:\n            self.scale_ = None\n\n        return self", "response": "Performs online computation of mean and standard deviation on X for later scaling."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform(self, X, y=None, copy=None):\n        check_is_fitted(self, 'scale_')\n\n        copy = copy if copy is not None else self.copy\n\n        X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n                        estimator=self, dtype=FLOAT_DTYPES)\n\n        if sparse.issparse(X):\n            if self.with_mean:\n                raise ValueError(\n                    \"Cannot center sparse matrices: pass `with_mean=False` \"\n                    \"instead. See docstring for motivation and alternatives.\")\n            if self.scale_ is not None:\n                inplace_column_scale(X, 1 / self.scale_)\n        else:\n            if self.with_mean:\n                X -= self.mean_\n            if self.with_std:\n                X /= self.scale_\n        return X", "response": "Perform standardization by centering and scaling using the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inverse_transform(self, X, copy=None):\n        check_is_fitted(self, 'scale_')\n\n        copy = copy if copy is not None else self.copy\n        if sparse.issparse(X):\n            if self.with_mean:\n                raise ValueError(\n                    \"Cannot uncenter sparse matrices: pass `with_mean=False` \"\n                    \"instead See docstring for motivation and alternatives.\")\n            if not sparse.isspmatrix_csr(X):\n                X = X.tocsr()\n                copy = False\n            if copy:\n                X = X.copy()\n            if self.scale_ is not None:\n                inplace_column_scale(X, self.scale_)\n        else:\n            X = numpy.asarray(X)\n            if copy:\n                X = X.copy()\n            if self.with_std:\n                X *= self.scale_\n            if self.with_mean:\n                X += self.mean_\n\n        return X", "response": "Inverse transform the data matrix X to the original representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the leverages for each observation in the hierarchy.", "response": "def leverages(self, block='X'):\n        \"\"\"\n        Calculate the leverages for each observation\n        :return:\n        :rtype:\n        \"\"\"\n        # TODO check with matlab and simca\n        try:\n            if block == 'X':\n                return np.dot(self.scores_t, np.dot(np.linalg.inv(np.dot(self.scores_t.T, self.scores_t), self.scores_t.T)))\n            elif block == 'Y':\n                return np.dot(self.scores_u, np.dot(np.linalg.inv(np.dot(self.scores_u.T, self.scores_u), self.scores_u.T)))\n            else:\n                raise ValueError\n        except ValueError as verr:\n            raise ValueError('block option must be either X or Y')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _recurse_replace(obj, key, new_key, sub, remove):\n    if isinstance(obj, list):\n        return [_recurse_replace(x, key, new_key, sub, remove) for x in obj]\n    if isinstance(obj, dict):\n        for k, v in list(obj.items()):\n            if k == key and v in sub:\n                obj[new_key] = sub[v]\n                if remove:\n                    del obj[key]\n            else:\n                obj[k] = _recurse_replace(v, key, new_key, sub, remove)\n    return obj", "response": "Recursive helper for replace_by_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces values that match a key and return a new key.", "response": "def replace_by_key(pif, key, subs, new_key=None, remove=False):\n    \"\"\"Replace values that match a key\n\n    Deeply traverses the pif object, looking for `key` and\n    replacing values in accordance with `subs`.  If `new_key`\n    is set, the replaced values are assigned to that key.  If\n    `remove` is `True`, the old `key` pairs are removed.\n    \"\"\"\n    if not new_key:\n        new_key = key\n        remove = False\n    orig = pif.as_dictionary()\n    new  = _recurse_replace(orig, to_camel_case(key), to_camel_case(new_key), subs, remove)\n    return pypif.pif.loads(json.dumps(new))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a property by name", "response": "def get_propety_by_name(pif, name):\n    \"\"\"Get a property by name\"\"\"\n    warn(\"This method has been deprecated in favor of get_property_by_name\")\n    return next((x for x in pif.properties if x.name == name), None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_property_by_name(pif, name):\n    return next((x for x in pif.properties if x.name == name), None)", "response": "Get a property by name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_keypair(key, value, ambig, unambig):\n    if key in ambig:\n        return\n\n    if key in unambig and value != unambig[key]:\n            ambig.add(key)\n            del unambig[key]\n            return\n\n    unambig[key] = value\n    return", "response": "Check new keypair against existing unambiguous dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds information about decodings of a child object.", "response": "def add_child_ambig(child_ambig, child_unambig, ambig, unambig):\n    \"\"\"\n    Add information about decodings of a child object\n\n    :param child_ambig: ambiguous set from child\n    :param child_unambig: unambiguous set from child\n    :param ambig: set of keys storing ambig decodings\n    :param unambig: dictionary storing unambiguous decodings\n    :return:\n    \"\"\"\n    for k in child_ambig:\n        ambig.add(k)\n        if k in unambig:\n            del unambig[k]\n\n    for k, v in child_unambig.items():\n        new_keypair(k, v, ambig, unambig)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_client(site=None):\n    if 'CITRINATION_API_KEY' not in environ:\n        raise ValueError(\"'CITRINATION_API_KEY' is not set as an environment variable\")\n    if not site:\n        site = environ.get(\"CITRINATION_SITE\", \"https://citrination.com\")\n    return CitrinationClient(environ['CITRINATION_API_KEY'], site)", "response": "Get a citrination client"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_uids(pifs, uids=None):\n    if not uids:\n        uids = [str(hash(dumps(x))) for x in pifs]\n    for pif, uid in zip(pifs, uids):\n        pif.uid = uid\n    return pifs", "response": "Set the UIDs in a PIF explicitly if the list of UIDs is passed in\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs the URL for a PIF on a site", "response": "def get_url(pif, dataset, version=1, site=\"https://citrination.com\"):\n    \"\"\"\n    Construct the URL of a PIF on a site\n    :param pif: to construct URL for\n    :param dataset: the pif will belong to\n    :param version: of the PIF (default: 1)\n    :param site: for the dataset (default: https://citrination.com)\n    :return: the URL as a string\n    \"\"\"\n    return \"{site}/datasets/{dataset}/version/{version}/pif/{uid}\".format(\n        uid=pif.uid, version=version, dataset=dataset, site=site\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_ideal_atomic_percent(pif):\n    if not isinstance(pif, ChemicalSystem):\n        return pif\n    if not pif.chemical_formula:\n        return pif\n    else:\n        expanded_formula_no_special_char = _expand_formula_(\n            pif.chemical_formula)\n        element_array = _create_emprical_compositional_array_(\n            expanded_formula_no_special_char)\n        appended_e_array = _add_atomic_percents_(element_array)\n        for e in appended_e_array:\n            # Checks if a Composition element decribing that element already\n            # exists.\n            if _get_element_in_pif_composition_(pif, e[\"symbol\"]):\n                # If it exists, it removes the old Composition object, and\n                # inserts a new one with ideal atomic percent added.\n                in_pif = _get_element_in_pif_composition_(pif, e[\"symbol\"])\n                comp = in_pif[0]\n                pif.composition.pop(in_pif[1])\n                comp.idealAtomicPercent = e[\"atomic_percent\"]\n                pif.composition.append(comp)\n            else:\n                # If not, it creates a new Composition object with the element\n                # and ideal atomic percent.\n                comp = Composition()\n                comp.element = e[\"symbol\"]\n                comp.idealAtomicPercent = e[\"atomic_percent\"]\n                pif.composition.append(comp)\n        return pif", "response": "Calculates ideal atomic percents from a chemical formula string from a pif. Returns an appended pif with composition elements modified or added."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate ideal atomic weight percents from a chemical formula string from a pif. Returns an appended pif with composition elements modified or added.", "response": "def calculate_ideal_weight_percent(pif):\n    \"\"\"\n    Calculates ideal atomic weight percents from a chemical formula string from a pif. Returns an appended pif with composition elements modified or added.\n\n    :param pif: a ChemicalSystem pif\n    :return: modified pif object\n    \"\"\"\n    if not isinstance(pif, ChemicalSystem):\n        return pif\n    if not pif.chemical_formula:\n        return pif\n    else:\n        expanded_formula_no_special_char = _expand_formula_(\n            pif.chemical_formula)\n        element_array = _create_emprical_compositional_array_(\n            expanded_formula_no_special_char)\n        appended_e_array = _add_ideal_atomic_weights_(element_array)\n        a_array_with_pcts = _add_ideal_weight_percent_(appended_e_array)\n        for e in a_array_with_pcts:\n            # Checks if a Composition element decribing that element already\n            # exists.\n            if _get_element_in_pif_composition_(pif, e[\"symbol\"]):\n                # If it exists, it removes the old Composition object, and\n                # inserts a new one with ideal atomic weight percent added\n                in_pif = _get_element_in_pif_composition_(pif, e[\"symbol\"])\n                comp = in_pif[0]\n                pif.composition.pop(in_pif[1])\n                comp.idealWeightPercent = e[\"weight_percent\"]\n                pif.composition.append(comp)\n            else:\n                # If not, it creates a new Composition object with the element\n                # and ideal atomic weight percent.\n                comp = Composition()\n                comp.element = e[\"symbol\"]\n                comp.idealWeightPercent = e[\"weight_percent\"]\n                pif.composition.append(comp)\n        return pif"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexpand a chemical formula string into a non - emperical but expanded form.", "response": "def _expand_formula_(formula_string):\n    \"\"\"\n    Accounts for the many ways a user may write a formula string, and returns an expanded chemical formula string.\n    Assumptions:\n    -The Chemical Formula string it is supplied is well-written, and has no hanging parethneses\n    -The number of repeats occurs after the elemental symbol or ) ] character EXCEPT in the case of a hydrate where it is assumed to be in front of the first element\n    -All hydrates explicitly use the \u00b7 symbol\n    -Only (, (,[, ], ., \u00b7 are \"important\" symbols to intrepreting the string.\n    -IONS ARE NOT HANDLED\n\n    :param formula_string: a messy chemical formula string\n    :return: a non-emperical but expanded formula string\n    \"\"\"\n    formula_string = re.sub(r'[^A-Za-z0-9\\(\\)\\[\\]\\\u00b7\\.]+', '', formula_string)\n    hydrate_pos = formula_string.find('\u00b7')\n    if hydrate_pos >= 0:\n        formula_string = _expand_hydrate_(hydrate_pos, formula_string)\n    search_result = re.search(\n        r'(?:[\\(\\[]([A-Za-z0-9]+)[\\)\\]](\\d*))',\n        formula_string)\n    if search_result is None:\n        return formula_string\n    this_start = search_result.start()\n    this_end = search_result.end()\n    this_string = search_result.group()\n    this_expansion_array = re.findall(\n        r'(?:[\\(\\[]([A-Za-z0-9]+)[\\)\\]](\\d*))', this_string)\n    for a in this_expansion_array:\n        if a[1] == \"\":\n            a = (a[0], 1)\n        parenth_expanded = \"\"\n        multiplier = float(a[1])\n        element_array = re.findall('[A-Z][^A-Z]*', a[0])\n        for e in element_array:\n            occurance_array = re.findall('[0-9][^0-9]*', e)\n            if len(occurance_array) == 0:\n                occurance_array.append(1)\n            for o in occurance_array:\n                symbol = re.findall('[A-Z][a-z]*', e)\n                total_num = float(o) * multiplier\n                if total_num.is_integer():\n                    total_num = int(total_num)\n                total_str = str(total_num)\n                if total_str == \"1\":\n                    total_str = \"\"\n                new_string = symbol[0] + total_str\n                parenth_expanded += new_string\n        formula_string = formula_string[0:this_start] + \\\n            parenth_expanded + formula_string[this_end:]\n        return _expand_formula_(formula_string)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand_hydrate_(hydrate_pos, formula_string):\n    hydrate = formula_string[hydrate_pos + 1:]\n    hydrate_string = \"\"\n    multiplier = float(re.search(r'^[\\d\\.]+', hydrate).group())\n    element_array = re.findall('[A-Z][^A-Z]*', hydrate)\n    for e in element_array:\n        occurance_array = re.findall('[0-9][^0-9]*', e)\n        if len(occurance_array) == 0:\n            occurance_array.append(1)\n        for o in occurance_array:\n            symbol = re.findall('[A-Z][a-z]*', e)\n            total_num = float(o) * multiplier\n            if total_num.is_integer():\n                total_num = int(total_num)\n            total_str = str(total_num)\n            if total_str == \"1\":\n                total_str = \"\"\n            new_string = symbol[0] + total_str\n            hydrate_string += new_string\n    return formula_string[:hydrate_pos] + hydrate_string", "response": "Expands the hydrate portion of a chemical formula string to all elements of the chemical formula."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_compositional_array_(expanded_chemical_formaula_string):\n    element_array = re.findall(\n        '[A-Z][^A-Z]*',\n        expanded_chemical_formaula_string)\n    split_element_array = []\n    for s in element_array:\n        m = re.match(r\"([a-zA-Z]+)([0-9\\.]*)\", s, re.I)\n        if m:\n            items = m.groups()\n            if items[1] == \"\":\n                items = (items[0], 1)\n        this_e = {\"symbol\": items[0], \"occurances\": float(items[1])}\n\n        split_element_array.append(this_e)\n    return split_element_array", "response": "Splits an expanded chemical formula string into an array of dictionaries containing information about each element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconsolidate an elemental array of element dictionaries with no repeating elements.", "response": "def _consolidate_elemental_array_(elemental_array):\n    \"\"\"\n    Accounts for non-empirical chemical formulas by taking in the compositional array generated by _create_compositional_array_() and returning a consolidated array of dictionaries with no repeating elements\n\n    :param elemental_array: an elemental array generated from _create_compositional_array_()\n    :return: an array of element dictionaries\n    \"\"\"\n    condensed_array = []\n    for e in elemental_array:\n        exists = False\n        for k in condensed_array:\n            if k[\"symbol\"] == e[\"symbol\"]:\n                exists = True\n                k[\"occurances\"] += e[\"occurances\"]\n                break\n        if not exists:\n            condensed_array.append(e)\n    return condensed_array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_ideal_atomic_weights_(elemental_array):\n    for a in elemental_array:\n        this_atomic_weight = elements_data[a[\"symbol\"]][\"atomic_weight\"]\n        a[\"weight\"] = a[\"occurances\"] * this_atomic_weight\n    return elemental_array", "response": "Adds the atomic weight property to each of the dictionaries in elemental_array"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the weight_percent property to each of the dictionaries in elemental_array", "response": "def _add_ideal_weight_percent_(elemental_array):\n    \"\"\"\n    Adds the \"weight_percent\" property to each of the dictionaries in elemental_array\n\n    :param elemental_array: an array of dictionaries containing information about the elements in the system\n    :return: the appended elemental_array\n    \"\"\"\n    t_mass = _calculate_total_mass_(elemental_array)\n    for a in elemental_array:\n        a[\"weight_percent\"] = a[\"weight\"] / t_mass * 100\n    return elemental_array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_atomic_percents_(elemental_array):\n    n_atoms = _calculate_n_atoms_(elemental_array)\n    for e in elemental_array:\n        e[\"atomic_percent\"] = e[\"occurances\"] / n_atoms * 100\n    return elemental_array", "response": "Adds ideal atomic percents to the emperical compositional element array generated using _create_emprical_compositional_array_"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the Composition object and the position in the composition array if the element in question if in the composition array is equal to the atomic symbol of the element in question if in the composition array otherwise returns False", "response": "def _get_element_in_pif_composition_(pif, elemental_symbol):\n    \"\"\"\n    If the element in question if in the composition array in the pif, it returns that Composition object and the position in the composition array otherwise it returns False\n\n    :param pif: ChemicalSystem Pif in question\n    :param elemental_symbol: string of the atomic symbol of the element in question\n    :return: either False if not found in the composition or the Compositional object along with its index in the composition array in the pif\n    \"\"\"\n    if pif.composition is None:\n        pif.composition = []\n    for i, c in enumerate(pif.composition):\n        if c.element == elemental_symbol or c.element.lower(\n        ) == elements_data[elemental_symbol][\"name\"].lower():\n            return [c, i]\n        i += 1\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_name_string(full_name):\n    name = Name()\n    if \",\" in full_name:\n        toks = full_name.split(\",\")\n        name.family = toks[0]\n        name.given = \",\".join(toks[1:]).strip()\n    else:\n        toks = full_name.split()\n        name.given = toks[0]\n        name.family = \" \".join(toks[1:]).strip()\n    return name", "response": "Parse a full name into a Name object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef creator_to_person(creator):\n    name = Name()\n    if \"creatorName\" in creator:\n        name = parse_name_string(creator[\"creatorName\"])\n    if \"familyName\" in creator:\n        name.family = creator[\"familyName\"]\n    if \"givenName\" in creator:\n        name.given = creator[\"givenName\"]\n\n    person = Person(name=name, tags=creator.get(\"affiliations\"))\n    return person", "response": "Parse the creator block in datacite into a Person object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a top - level datacite dictionary into a Reference object", "response": "def datacite_to_pif_reference(dc):\n    \"\"\"\n    Parse a top-level datacite dictionary into a Reference\n    :param dc: dictionary containing datacite metadata\n    :return: Reference corresponding to that datacite entry\n    \"\"\"\n    ref = Reference()\n    if dc.get('identifier', {}).get('identifierType') == \"DOI\":\n        ref.doi = dc.get('identifier', {}).get('identifier')\n    ref.title = dc.get('title')\n    ref.publisher = dc.get('publisher')\n    ref.year = dc.get('publicationYear')\n\n    ref.authors = [creator_to_person(x).name for x in dc.get('creators', [])] or None\n\n    return ref"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate a query and return a list of MDF records", "response": "def query_to_mdf_records(query=None, dataset_id=None, mdf_acl=None):\n    \"\"\"Evaluate a query and return a list of MDF records\n\n    If a datasetID is specified by there is no query, a simple\n    whole dataset query is formed for the user\n    \"\"\"\n    if not query and not dataset_id:\n        raise ValueError(\"Either query or dataset_id must be specified\")\n    if query and dataset_id:\n        raise ValueError(\"Both query and dataset_id were specified; pick one or the other.\")\n    if not query:\n        query = PifSystemReturningQuery(\n            query=DataQuery(\n                dataset=DatasetQuery(\n                    id=Filter(equal=dataset_id)\n                )\n            ),\n            size = 10000 # Don't pull down all the results by default\n        )\n\n    client = get_client()\n\n    if not mdf_acl:\n        raise ValueError('Access controls (mdf_acl) must be specified.  Use [\"public\"] for public access')\n\n    pif_result = client.pif_search(query)\n    if len(pif_result.hits) == 0:\n        return []\n\n    example_uid = pif_result.hits[0].system.uid\n    dataset_query = DatasetReturningQuery(\n        query=DataQuery(\n            system=PifSystemQuery(\n                uid=Filter(equal=example_uid)\n            )\n        ),\n        size = 1 # we only expect one dataset to hit\n    )\n\n    dataset_result = client.dataset_search(dataset_query)\n\n    records = []\n    for hit in pif_result.hits:\n        records.append(pif_to_mdf_record(hit.system, dataset_result.hits[0], mdf_acl))\n\n    return records"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pif_to_mdf_record(pif_obj, dataset_hit, mdf_acl):\n    res = {}\n    res[\"mdf\"] = _to_meta_data(pif_obj, dataset_hit, mdf_acl)\n    res[res[\"mdf\"][\"source_name\"]] = _to_user_defined(pif_obj)\n    return dumps(res)", "response": "Convert a PIF into a MDF record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_meta_data(pif_obj, dataset_hit, mdf_acl):\n    pif = pif_obj.as_dictionary()\n    dataset = dataset_hit.as_dictionary()\n    mdf = {}\n    try:\n        if pif.get(\"names\"):\n            mdf[\"title\"] = pif[\"names\"][0]\n        else:\n            mdf[\"title\"] = \"Citrine PIF \" + str(pif[\"uid\"])\n\n        if pif.get(\"chemicalFormula\"):\n            mdf[\"composition\"] = pif[\"chemicalFormula\"]\n        elif pif.get(\"composition\"):\n            mdf[\"composition\"] = ''.join([comp[\"element\"] for comp in pif[\"composition\"] if comp[\"element\"]])\n        if not mdf[\"composition\"]:\n            mdf.pop(\"composition\")\n\n        mdf[\"acl\"] = mdf_acl\n        mdf[\"source_name\"] = _construct_new_key(dataset[\"name\"])\n\n        if pif.get(\"contacts\"):\n            mdf[\"data_contact\"] = []\n            for contact in pif[\"contacts\"]:\n                data_c = {\n                    \"given_name\": contact[\"name\"][\"given\"],  #REQ\n                    \"family_name\": contact[\"name\"][\"family\"]  #REQ\n                    }\n                if contact.get(\"email\"):\n                    data_c[\"email\"] = contact.get(\"email\", \"\")\n                if contact.get(\"orcid\"):\n                    data_c[\"orcid\"] = contact.get(\"orcid\", \"\")\n                mdf[\"data_contact\"].append(data_c)\n            if not mdf[\"data_contact\"]:\n                mdf.pop(\"data_contact\")\n        \n        mdf[\"data_contributor\"] = [{}] \n        if \"owner\" in dataset:\n            name = dataset[\"owner\"].split()\n            contributor = {\n                \"given_name\": name[0],\n                \"family_name\": name[1],\n                \"email\": dataset[\"email\"]\n            }\n            mdf[\"data_contributor\"] = [contributor]\n\n        mdf[\"links\"] = {\n            \"landing_page\": \"https://citrination.com/datasets/{}\".format(dataset[\"id\"]),\n            \"publication\": []\n            }\n        if pif.get(\"references\"):\n            mdf[\"author\"] = []\n            mdf[\"citation\"] = []\n            for ref in pif[\"references\"]:\n                if ref.get(\"doi\"):\n                    mdf[\"citation\"].append(ref[\"doi\"]) #TODO: Make actual citation\n                    mdf[\"links\"][\"publication\"].append(ref[\"doi\"])\n                if ref.get(\"authors\"):\n                    for author in ref[\"authors\"]:\n                        if author.get(\"given\") and author.get(\"family\"):\n                            mdf[\"author\"].append({\n                                \"given_name\": author[\"given\"],\n                                \"family_name\": author[\"family\"]\n                                })\n            # Remove fields if blank\n            if not mdf[\"author\"]:\n                mdf.pop(\"author\")\n            if not mdf[\"citation\"]:\n                mdf.pop(\"citation\")\n        if not mdf[\"links\"][\"publication\"]:\n            mdf[\"links\"].pop(\"publication\")\n\n        if pif.get(\"licenses\", [{}])[0].get(\"url\"):\n            mdf[\"license\"] = pif[\"licenses\"][0][\"url\"]\n        if pif.get(\"tags\"):\n            mdf[\"tags\"] = pif[\"tags\"]\n\n    # If required MDF metadata is missing from PIF, abort\n    except KeyError as e:\n        print(\"Error: Required MDF metadata\", str(e), \"not found in PIF\", pif[\"uid\"])\n        return None\n\n    return mdf", "response": "Convert the meta - data from the PIF into the MDF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the system in the PIF to populate the user - defined portion of the system.", "response": "def _to_user_defined(pif_obj):\n    \"\"\"Read the systems in the PIF to populate the user-defined portion\"\"\"\n    res = {}\n\n    # make a read view to flatten the hierarchy\n    rv = ReadView(pif_obj)\n\n    # Iterate over the keys in the read view\n    for k in rv.keys():\n        name, value = _extract_key_value(rv[k].raw)\n        # add any objects that can be extracted\n        if name and value is not None:\n            res[name] = value\n\n    # Grab interesting values not in the ReadView\n    pif = pif_obj.as_dictionary()\n\n    elements = {}\n    if pif.get(\"composition\"):\n        for comp in pif[\"composition\"]:\n            if comp.get(\"actualAtomicPercent\"):\n                elements[comp[\"element\"]] = float(comp[\"actualAtomicPercent\"][\"value\"])\n            elif comp.get(\"actualWeightPercent\"):\n                elements[comp[\"element\"]] = float(comp[\"actualWeightPercent\"][\"value\"])\n        if elements:\n            res[\"elemental_percent\"] = elements\n    elif pif.get(\"chemicalFormula\"):\n        symbol = \"\"\n        num = \"\"\n        # Chemical formulae are comprised of letters, numbers, and potentially characters we don't care about\n        for char in pif[\"chemicalFormula\"]:\n            # Uppercase char indicates beginning of new symbol\n            if char.isupper():\n                # If there is already a symbol in holding, process it\n                if symbol:\n                    try:\n                        elements[symbol] = int(num)\n                    # If num is a float, raises ValueError\n                    except ValueError:\n                        elements[symbol] = float(num) if num else 1\n                    symbol = \"\"\n                    num = \"\"\n                symbol += char\n            # Lowercase chars or digits are continuations of a symbol\n            elif char.islower():\n                symbol += char\n            elif char.isdigit():\n                num += char\n            elif char == \".\":\n                num += char\n            # All other chars are not useful\n        if elements:\n            res[\"elemental_proportion\"] = elements\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a MDF safe key from the name and units", "response": "def _construct_new_key(name, units=None):\n    \"\"\"Construct an MDF safe key from the name and units\"\"\"\n    to_replace = [\"/\", \"\\\\\", \"*\", \"^\", \"#\", \" \", \"\\n\", \"\\t\", \",\", \".\", \")\", \"(\", \"'\", \"`\", \"-\"]\n    to_remove = [\"$\", \"{\", \"}\"]\n\n    cat = name\n    if units:\n        cat = \"_\".join([name, units])\n    for c in to_replace:\n       cat = cat.replace(c, \"_\")\n    for c in to_remove:\n       cat = cat.replace(c, \"\")\n\n    cat = re.sub('_+','_', cat)\n\n    return cat"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_key_value(obj):\n    key = None; value = None\n\n    # Parse a Value object, which includes Properties\n    if isinstance(obj, Value):\n        key = _construct_new_key(obj.name, obj.units)\n        value = []\n        if obj.scalars:\n            value = [(val.value if isinstance(val, Scalar) else val)\n                     for val in obj.scalars]\n        elif obj.vectors and len(obj.vectors) == 1:\n            value = [(val.value if isinstance(val, Scalar) else val)\n                     for val in obj.vectors[0]]\n        if len(value) == 1:\n            value = value[0]\n        elif len(value) == 0:\n            value = None\n\n    # If there is a process step, pul out its name as the value\n    # TODO: resolve duplicates\n    if isinstance(obj, ProcessStep):\n        key = \"Processing\"\n        value = obj.name\n        \n    return key, value", "response": "Extract the value from the object and make a descriptive key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_input_buffer():\n    from polysh.control_commands_helpers import handle_control_command\n    data = the_stdin_thread.input_buffer.get()\n    remote_dispatcher.log(b'> ' + data)\n\n    if data.startswith(b':'):\n        try:\n            handle_control_command(data[1:-1].decode())\n        except UnicodeDecodeError as e:\n            console_output(b'Could not decode command.')\n        return\n\n    if data.startswith(b'!'):\n        try:\n            retcode = subprocess.call(data[1:], shell=True)\n        except OSError as e:\n            if e.errno == errno.EINTR:\n                console_output(b'Child was interrupted\\n')\n                retcode = 0\n            else:\n                raise\n        if retcode > 128 and retcode <= 192:\n            retcode = 128 - retcode\n        if retcode > 0:\n            console_output('Child returned {:d}\\n'.format(retcode).encode())\n        elif retcode < 0:\n            console_output('Child was terminated by signal {:d}\\n'.format(\n                -retcode).encode())\n        return\n\n    for r in dispatchers.all_instances():\n        try:\n            r.dispatch_command(data)\n        except asyncore.ExitNow as e:\n            raise e\n        except Exception as msg:\n            raise msg\n            console_output('{} for {}, disconnecting\\n'.format(\n                str(msg), r.display_name).encode())\n            r.disconnect()\n        else:\n            if r.enabled and r.state is remote_dispatcher.STATE_IDLE:\n                r.change_state(remote_dispatcher.STATE_RUNNING)", "response": "Send the content of the input buffer to all remote processes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_main_socket(c):\n    the_stdin_thread.socket_write.send(c)\n    while True:\n        try:\n            the_stdin_thread.socket_write.recv(1)\n        except socket.error as e:\n            if e.errno != errno.EINTR:\n                raise\n        else:\n            break", "response": "Synchronous write to the main socket wait for ACK"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to get the PID of the stdin thread otherwise get the whole process ID", "response": "def get_stdin_pid(cached_result=None):\n    \"\"\"Try to get the PID of the stdin thread, otherwise get the whole process\n    ID\"\"\"\n    if cached_result is None:\n        try:\n            tasks = os.listdir('/proc/self/task')\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise\n            cached_result = os.getpid()\n        else:\n            tasks.remove(str(os.getpid()))\n            assert len(tasks) == 1\n            cached_result = int(tasks[0])\n    return cached_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interrupt_stdin_thread():\n    dupped_stdin = os.dup(0)  # Backup the stdin fd\n    assert not the_stdin_thread.interrupt_asked  # Sanity check\n    the_stdin_thread.interrupt_asked = True  # Not user triggered\n    os.lseek(tempfile_fd, 0, 0)  # Rewind in the temp file\n    os.dup2(tempfile_fd, 0)  # This will make raw_input() return\n    pid = get_stdin_pid()\n    os.kill(pid, signal.SIGWINCH)  # Try harder to wake up raw_input()\n    the_stdin_thread.out_of_raw_input.wait()  # Wait for this return\n    the_stdin_thread.interrupt_asked = False  # Restore sanity\n    os.dup2(dupped_stdin, 0)  # Restore stdin\n    os.close(dupped_stdin)", "response": "Interrupt the stdin thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, data):\n        assert isinstance(data, bytes)\n        with self.lock:\n            self.buf += data", "response": "Add data to the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self):\n        data = b''\n        with self.lock:\n            data, self.buf = self.buf, b''\n\n        return data", "response": "Get the content of the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle all the available character commands in the socket.", "response": "def handle_read(self):\n        \"\"\"Handle all the available character commands in the socket\"\"\"\n        while True:\n            try:\n                c = self.recv(1)\n            except socket.error as e:\n                if e.errno == errno.EWOULDBLOCK:\n                    return\n                else:\n                    raise\n            else:\n                self._do(c)\n                self.socket.setblocking(True)\n                self.send(b'A')\n                self.socket.setblocking(False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decker_sum(a, b):\n    x = a + b\n    y = b - (x - a) if abs(a) > abs(b) else a - (x - b)\n    return x, y", "response": "Compute the knuth sum of two numbers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ksum(p, K=2):\n    # Don't override the input data.\n    q = p.copy()\n    distill(q, K - 1)\n    return numpy.sum(q[:-1], axis=0) + q[-1]", "response": "Sum the data in a sequence of K elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables or disable the specified shells.", "response": "def toggle_shells(command, enable):\n    \"\"\"Enable or disable the specified shells. If the command would have\n    no effect, it changes all other shells to the inverse enable value.\"\"\"\n    selection = list(selected_shells(command))\n    if command and command != '*' and selection:\n        for i in selection:\n            if i.state != remote_dispatcher.STATE_DEAD and i.enabled != enable:\n                break\n        else:\n            toggle_shells('*', not enable)\n\n    for i in selection:\n        if i.state != remote_dispatcher.STATE_DEAD:\n            i.set_enabled(enable)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef selected_shells(command):\n    if not command or command == '*':\n        for i in dispatchers.all_instances():\n            yield i\n        return\n    selected = set()\n    instance_found = False\n    for pattern in command.split():\n        found = False\n        for expanded_pattern in expand_syntax(pattern):\n            for i in dispatchers.all_instances():\n                instance_found = True\n                if fnmatch(i.display_name, expanded_pattern):\n                    found = True\n                    if i not in selected:\n                        selected.add(i)\n                        yield i\n        if instance_found and not found:\n            console_output('{} not found\\n'.format(pattern).encode())", "response": "Iterator over the shells with names matching the patterns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complete_shells(line, text, predicate=lambda i: True):\n    res = [i.display_name + ' ' for i in dispatchers.all_instances() if\n           i.display_name.startswith(text) and\n           predicate(i) and\n           ' ' + i.display_name + ' ' not in line]\n    return res", "response": "Return the shell names to include in the completion"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kdot(x, y, K=2):\n    xx = x.reshape(-1, x.shape[-1])\n    yy = y.reshape(y.shape[0], -1)\n\n    xx = numpy.ascontiguousarray(xx)\n    yy = numpy.ascontiguousarray(yy)\n\n    r = _accupy.kdot_helper(xx, yy).reshape((-1,) + x.shape[:-1] + y.shape[1:])\n    return ksum(r, K - 1)", "response": "Algorithm 5. 10. Dot product algorithm in K - fold working precision"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kill_all():\n    for i in dispatchers.all_instances():\n        try:\n            os.kill(-i.pid, signal.SIGKILL)\n        except OSError:\n            # The process was already dead, no problem\n            pass", "response": "Kill all the remote shells"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n\n    sentry_dsn = os.environ.get('POLYSH_SENTRY_DSN')\n\n    if sentry_dsn:\n        from raven import Client\n        client = Client(\n            dsn=sentry_dsn,\n            release='.'.join(map(str, VERSION)),\n            ignore_exceptions=[\n                KeyboardInterrupt\n            ]\n        )\n\n        try:\n            run()\n        except Exception:\n            client.captureException()\n\n    else:\n        run()", "response": "Wrapper around run to setup sentry"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a read chunk of data from the socket.", "response": "def _handle_read_chunk(self):\n        \"\"\"Some data can be read\"\"\"\n        new_data = b''\n        buffer_length = len(self.read_buffer)\n        try:\n            while buffer_length < self.MAX_BUFFER_SIZE:\n                try:\n                    piece = self.recv(4096)\n                except OSError as e:\n                    if e.errno == errno.EAGAIN:\n                        # End of the available data\n                        break\n                    elif e.errno == errno.EIO and new_data:\n                        # Hopefully we could read an error message before the\n                        # actual termination\n                        break\n                    else:\n                        raise\n\n                if not piece:\n                    # A closed connection is indicated by signaling a read\n                    # condition, and having recv() return 0.\n                    break\n\n                new_data += piece\n                buffer_length += len(piece)\n\n        finally:\n            new_data = new_data.replace(b'\\r', b'\\n')\n            self.read_buffer += new_data\n        return new_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naugmenting the buffer with stuff to write when possible", "response": "def dispatch_write(self, buf):\n        \"\"\"Augment the buffer with stuff to write when possible\"\"\"\n        self.write_buffer += buf\n        if len(self.write_buffer) > self.MAX_BUFFER_SIZE:\n            console_output('Buffer too big ({:d}) for {}\\n'.format(\n                len(self.write_buffer), str(self)).encode())\n            raise asyncore.ExitNow(1)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_write(buf):\n    assert isinstance(buf, bytes)\n    while True:\n        try:\n            os.write(1, buf)\n            break\n        except IOError as e:\n            if e.errno != errno.EINTR:\n                raise", "response": "Write a buffer to the internal memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse instead of print to clear the status information before printing", "response": "def console_output(msg, logging_msg=None):\n    \"\"\"Use instead of print, to clear the status information before printing\"\"\"\n    assert isinstance(msg, bytes)\n    assert isinstance(logging_msg, bytes) or logging_msg is None\n\n    from polysh import remote_dispatcher\n\n    remote_dispatcher.log(logging_msg or msg)\n    if remote_dispatcher.options.interactive:\n        from polysh.stdin import the_stdin_thread\n        the_stdin_thread.no_raw_input()\n        global last_status_length\n        if last_status_length:\n            safe_write('\\r{}\\r'.format(\n                last_status_length * ' ').encode())\n            last_status_length = 0\n    safe_write(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tuple with the number of awaited processes and the total number of all active resources.", "response": "def count_awaited_processes():\n    \"\"\"Return a tuple with the number of awaited processes and the total\n    number\"\"\"\n    awaited = 0\n    total = 0\n    for i in all_instances():\n        if i.enabled:\n            total += 1\n            if i.state is not remote_dispatcher.STATE_IDLE:\n                awaited += 1\n    return awaited, total"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_terminal_size():\n    w, h = terminal_size()\n    w = max(w - display_names.max_display_name_length - 2, min(w, 10))\n    # python bug http://python.org/sf/1112949 on amd64\n    # from ajaxterm.py\n    bug = struct.unpack('i', struct.pack('I', termios.TIOCSWINSZ))[0]\n    packed_size = struct.pack('HHHH', h, w, 0, 0)\n    term_size = w, h\n    for i in all_instances():\n        if i.enabled and i.term_size != term_size:\n            i.term_size = term_size\n            fcntl.ioctl(i.fd, bug, packed_size)", "response": "Propagate the terminal size to the remote shells accounting for the longest name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a 2 - dimension list of bytes into a 1 - dimension list of bytes with correct spacing", "response": "def format_info(info_list):\n    \"\"\"Turn a 2-dimension list of bytes into a 1-dimension list of bytes with\n    correct spacing\"\"\"\n\n    max_lengths = []\n    if info_list:\n        nr_columns = len(info_list[0])\n    else:\n        nr_columns = 0\n    for i in range(nr_columns):\n        max_lengths.append(max([len(info[i]) for info in info_list]))\n\n    flattened_info_list = []\n    for info_id in range(len(info_list)):\n        info = info_list[info_id]\n        for str_id in range(len(info) - 1):\n            # Don't justify the last column (i.e. the last printed line)\n            # as it can get much longer in some shells than in others\n            orig_str = info[str_id]\n            indent = max_lengths[str_id] - len(orig_str)\n            info[str_id] = orig_str + indent * b' '\n        flattened_info_list.append(b' '.join(info) + b'\\n')\n\n    return flattened_info_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_ill_conditioned_dot_product(n, c, dps=100):\n    # Algorithm 6.1 from\n    #\n    # ACCURATE SUM AND DOT PRODUCT,\n    # TAKESHI OGITA, SIEGFRIED M. RUMP, AND SHIN'ICHI OISHI.\n    assert n >= 6\n    n2 = round(n / 2)\n    x = numpy.zeros(n)\n    y = numpy.zeros(n)\n\n    b = math.log2(c)\n    # vector of exponents between 0 and b/2:\n    e = numpy.rint(numpy.random.rand(n2) * b / 2).astype(int)\n    # make sure exponents b/2 and 0 actually occur in e\n    # vectors x,y\n    e[0] = round(b / 2) + 1\n    e[-1] = 0\n\n    # generate first half of vectors x, y\n    rx, ry = numpy.random.rand(2, n2)\n    x[:n2] = (2 * rx - 1) * 2 ** e\n    y[:n2] = (2 * ry - 1) * 2 ** e\n\n    def dot_exact(x, y):\n        mp.dps = dps\n        # convert to list first, see\n        # <https://github.com/fredrik-johansson/mpmath/pull/385>\n        return mp.fdot(x.tolist(), y.tolist())\n\n    # for i=n2+1:n and v=1:i,\n    #     generate x_i, y_i such that (*) x(v)\u2019*y(v) ~ 2^e(i-n2)\n    # generate exponents for second half\n    e = numpy.rint(numpy.linspace(b / 2, 0, n - n2)).astype(int)\n    rx, ry = numpy.random.rand(2, n2)\n    for i in range(n2, n):\n        # x_i random with generated exponent\n        x[i] = (2 * rx[i - n2] - 1) * 2 ** e[i - n2]\n        # y_i according to (*)\n        y[i] = (\n            (2 * ry[i - n2] - 1) * 2 ** e[i - n2] - dot_exact(x[: i + 1], y[: i + 1])\n        ) / x[i]\n\n    x, y = numpy.random.permutation((x, y))\n    # the true dot product rounded to nearest floating point\n    d = dot_exact(x, y)\n    # the actual condition number\n    C = 2 * dot_exact(abs(x), abs(y)) / abs(d)\n\n    return x, y, d, C", "response": "Generate a random illumination conditioned DOT product."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of RemoteDispatcher.handle_read() calls made by this iteration", "response": "def main_loop_iteration(timeout=None):\n    \"\"\"Return the number of RemoteDispatcher.handle_read() calls made by this\n    iteration\"\"\"\n    prev_nr_read = nr_handle_read\n    asyncore.loop(count=1, timeout=timeout, use_poll=True)\n    return nr_handle_read - prev_nr_read"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlaunch the ssh command in the child process", "response": "def launch_ssh(self, name, port):\n        \"\"\"Launch the ssh command in the child process\"\"\"\n        if options.user:\n            name = '%s@%s' % (options.user, name)\n        evaluated = options.ssh % {'host': name, 'port': port}\n        if evaluated == options.ssh:\n            evaluated = '%s %s' % (evaluated, name)\n        os.execlp('/bin/sh', 'sh', '-c', evaluated)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change_state(self, state):\n        if state is not self.state:\n            if self.debug:\n                self.print_debug(b'state => ' + STATE_NAMES[state].encode())\n            if self.state is STATE_NOT_STARTED:\n                self.read_in_state_not_started = b''\n            self.state = state", "response": "Change the state of the remote process logging the change"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect(self):\n        try:\n            os.kill(-self.pid, signal.SIGKILL)\n        except OSError:\n            # The process was already dead, no problem\n            pass\n        self.read_buffer = b''\n        self.write_buffer = b''\n        self.set_enabled(False)\n        if self.read_in_state_not_started:\n            self.print_lines(self.read_in_state_not_started)\n            self.read_in_state_not_started = b''\n        if options.abort_error and self.state is STATE_NOT_STARTED:\n            raise asyncore.ExitNow(1)\n        self.change_state(STATE_DEAD)", "response": "Disconnects from the remote process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure the terminal to use the termios module.", "response": "def configure_tty(self):\n        \"\"\"We don't want \\n to be replaced with \\r\\n, and we disable the echo\"\"\"\n        attr = termios.tcgetattr(self.fd)\n        attr[1] &= ~termios.ONLCR  # oflag\n        attr[3] &= ~termios.ECHO  # lflag\n        termios.tcsetattr(self.fd, termios.TCSANOW, attr)\n        # unsetopt zle prevents Zsh from resetting the tty\n        return b'unsetopt zle 2> /dev/null;stty -echo -onlcr -ctlecho;'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_prompt(self):\n        # No right prompt\n        command_line = b'PS2=;RPS1=;RPROMPT=;'\n        command_line += b'PROMPT_COMMAND=;'\n        command_line += b'TERM=ansi;'\n        command_line += b'unset HISTFILE;'\n        prompt1, prompt2 = callbacks.add(b'prompt', self.seen_prompt_cb, True)\n        command_line += b'PS1=\"' + prompt1 + b'\"\"' + prompt2 + b'\\n\"\\n'\n        return command_line", "response": "Set the prompt for the process holding the lock."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the fast case of reading the entries in the log file.", "response": "def handle_read_fast_case(self, data):\n        \"\"\"If we are in a fast case we'll avoid the long processing of each\n        line\"\"\"\n        if self.state is not STATE_RUNNING or callbacks.any_in(data):\n            # Slow case :-(\n            return False\n\n        last_nl = data.rfind(b'\\n')\n        if last_nl == -1:\n            # No '\\n' in data => slow case\n            return False\n        self.read_buffer = data[last_nl + 1:]\n        self.print_lines(data[:last_nl])\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle a single line of the read buffer.", "response": "def handle_read(self):\n        \"\"\"We got some output from a remote shell, this is one of the state\n        machine\"\"\"\n        if self.state == STATE_DEAD:\n            return\n        global nr_handle_read\n        nr_handle_read += 1\n        new_data = self._handle_read_chunk()\n        if self.debug:\n            self.print_debug(b'==> ' + new_data)\n        if self.handle_read_fast_case(self.read_buffer):\n            return\n        lf_pos = new_data.find(b'\\n')\n        if lf_pos >= 0:\n            # Optimization: we knew there were no '\\n' in the previous read\n            # buffer, so we searched only in the new_data and we offset the\n            # found index by the length of the previous buffer\n            lf_pos += len(self.read_buffer) - len(new_data)\n        elif self.state is STATE_NOT_STARTED and \\\n                options.password is not None and \\\n                b'password:' in self.read_buffer.lower():\n            self.dispatch_write('{}\\n'.format(options.password).encode())\n            self.read_buffer = b''\n            return\n        while lf_pos >= 0:\n            # For each line in the buffer\n            line = self.read_buffer[:lf_pos + 1]\n            if callbacks.process(line):\n                pass\n            elif self.state in (STATE_IDLE, STATE_RUNNING):\n                self.print_lines(line)\n            elif self.state is STATE_NOT_STARTED:\n                self.read_in_state_not_started += line\n                if b'The authenticity of host' in line:\n                    msg = line.strip(b'\\n') + b' Closing connection.'\n                    self.disconnect()\n                elif b'REMOTE HOST IDENTIFICATION HAS CHANGED' in line:\n                    msg = b'Remote host identification has changed.'\n                else:\n                    msg = None\n\n                if msg:\n                    self.print_lines(msg + b' Consider manually connecting or '\n                                     b'using ssh-keyscan.')\n\n            # Go to the next line in the buffer\n            self.read_buffer = self.read_buffer[lf_pos + 1:]\n            if self.handle_read_fast_case(self.read_buffer):\n                return\n            lf_pos = self.read_buffer.find(b'\\n')\n        if self.state is STATE_NOT_STARTED and not self.init_string_sent:\n            self.dispatch_write(self.init_string)\n            self.init_string_sent = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_unfinished_line(self):\n        if self.state is STATE_RUNNING:\n            if not callbacks.process(self.read_buffer):\n                self.print_lines(self.read_buffer)\n            self.read_buffer = b''", "response": "Print the unfinished line of the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_write(self):\n        num_sent = self.send(self.write_buffer)\n        if self.debug:\n            if self.state is not STATE_NOT_STARTED or options.password is None:\n                self.print_debug(b'<== ' + self.write_buffer[:num_sent])\n        self.write_buffer = self.write_buffer[num_sent:]", "response": "Let s write as much as we can."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs some debugging information to the console", "response": "def print_debug(self, msg):\n        \"\"\"Log some debugging information to the console\"\"\"\n        assert isinstance(msg, bytes)\n        state = STATE_NAMES[self.state].encode()\n        console_output(b'[dbg] ' + self.display_name.encode() + b'[' + state +\n                       b']: ' + msg + b'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_info(self):\n        return [self.display_name.encode(),\n                self.enabled and b'enabled' or b'disabled',\n                STATE_NAMES[self.state].encode() + b':',\n                self.last_printed_line.strip()]", "response": "Return a list with all information available about this process"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndispatches a buffer to the appropriate handler.", "response": "def dispatch_write(self, buf):\n        \"\"\"There is new stuff to write when possible\"\"\"\n        if self.state != STATE_DEAD and self.enabled:\n            super().dispatch_write(buf)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the name of the shell possibly updating the maximum name length", "response": "def change_name(self, new_name):\n        \"\"\"Change the name of the shell, possibly updating the maximum name\n        length\"\"\"\n        if not new_name:\n            name = self.hostname\n        else:\n            name = new_name.decode()\n        self.display_name = display_names.change(\n            self.display_name, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending to the remote shell its new name to be shell expanded", "response": "def rename(self, name):\n        \"\"\"Send to the remote shell, its new name to be shell expanded\"\"\"\n        if name:\n            # defug callback add?\n            rename1, rename2 = callbacks.add(\n                b'rename', self.change_name, False)\n            self.dispatch_command(b'/bin/echo \"' + rename1 + b'\"\"' + rename2 +\n                                  b'\"' + name + b'\\n')\n        else:\n            self.change_name(self.hostname.encode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef terminal_size():  # decide on *some* terminal size\n    cr = _ioctl_GWINSZ(0) or _ioctl_GWINSZ(\n        1) or _ioctl_GWINSZ(2)  # try open fds\n    if not cr:                                                  # ...then ctty\n        try:\n            fd = os.open(os.ctermid(), os.O_RDONLY)\n            cr = _ioctl_GWINSZ(fd)\n            os.close(fd)\n        except BaseException:\n            pass\n        if not cr:                            # env vars or finally defaults\n            try:\n                cr = os.environ['LINES'], os.environ['COLUMNS']\n            except BaseException:\n                cr = 25, 80\n    return int(cr[1]), int(cr[0])", "response": "Return the number of lines and columns of a terminal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef complete(text, state):\n    global completion_results\n    if state == 0:\n        line = readline.get_line_buffer()\n        if line.startswith(':'):\n            # Control command completion\n            completion_results = complete_control_command(line, text)\n        else:\n            if line.startswith('!') and text and line.startswith(text):\n                dropped_exclam = True\n                text = text[1:]\n            else:\n                dropped_exclam = False\n            completion_results = []\n            # Complete local paths\n            completion_results += complete_local_path(text)\n            # Complete from history\n            l = len(text)\n            completion_results += [w + ' ' for w in history_words if\n                                   len(w) > l and w.startswith(text)]\n            if readline.get_begidx() == 0:\n                # Completing first word from $PATH\n                completion_results += [w + ' ' for w in user_commands_in_path\n                                           if len(w) > l and w.startswith(text)]\n            completion_results = remove_dupes(completion_results)\n            if dropped_exclam:\n                completion_results = ['!' + r for r in completion_results]\n\n    if state < len(completion_results):\n        return completion_results[state]\n    completion_results = None\n    return None", "response": "Completes the given text with the next possible completion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Popen(*args, **kwargs):\n\n    read_line = None\n    if 'read_line' in kwargs:\n        read_line = kwargs['read_line']\n        del kwargs['read_line']\n\n    p = subprocess.Popen(*args, **kwargs)\n    wait_stdout = None\n    wait_stderr = None\n\n    if p.stdout:\n        wait_stdout = sys.stdout.attach(p.stdout, read_line=read_line)\n    if p.stderr:\n        wait_stderr = sys.stderr.attach(p.stderr)\n\n    original_wait = p.wait\n    def wait():\n        original_wait()\n\n        if wait_stdout:\n            wait_stdout()\n        if wait_stderr:\n            wait_stderr()\n\n    p.wait = wait\n\n    return p", "response": "Executes a command using subprocess. Popen and redirects output to AETROS and stdout API calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef context():\n    job = JobBackend()\n\n    offline = False\n    if '1' == os.getenv('AETROS_OFFLINE', ''):\n        offline = True\n\n    if os.getenv('AETROS_JOB_ID'):\n        job.load(os.getenv('AETROS_JOB_ID'))\n        if not offline:\n            job.connect()\n    else:\n        job.create()\n        if not offline:\n            job.connect()\n\n    job.start(offline=offline)\n\n    return job", "response": "Returns a JobBackend instance which connects to AETROS Trainer\n    based on model name in aetros. yml or environment variable AETROS_MODEL_NAME environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall by the process when SIGINT is received.", "response": "def on_sigint(self, sig, frame):\n        \"\"\"\n        We got SIGINT signal.\n        \"\"\"\n\n        if self.stop_requested or self.stop_requested_force:\n            # signal has already been sent or we force a shutdown.\n            # handles the keystroke 2x CTRL+C to force an exit.\n            self.stop_requested_force = True\n            self.logger.warning('Force stopped: ' + str(sig))\n\n            # just kill the process, we don't care about the results\n            self.on_force_exit()\n            os._exit(1)\n            # with force_exit we really close the process, killing it in unknown state\n            # self.fail('Force stopped', force_exit=True)\n            # return\n\n        if self.is_master_process():\n            self.logger.warning('Received signal '+str(sig)+'. Send again to force stop. Stopping ...')\n        else:\n            self.logger.debug(\"Got child signal \" + str(sig))\n\n        self.stop_requested = True\n\n        # the default SIGINT handle in python is not always installed, so we can't rely on the\n        # KeyboardInterrupt exception to be thrown.\n        # thread.interrupt_main would call sigint again.\n        # the shutdown listener will do the rest like committing rest memory files into Git and closing connections.\n        sys.exit(0 if self.in_early_stop else 1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef external_aborted(self, params):\n        self.ended = True\n        self.running = False\n\n        # When the server sends an abort signal, we really have to close immediately,\n        # since for example the job has been already deleted.\n        # without touching the git and client any further\n        os._exit(1)", "response": "This is called when the server sends an external abort signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef external_stop(self, force):\n\n        # only the master processes handles the regular stop signal from the server, sending a SIGINT to\n        # all its child (means to us, non-master process)\n        if not self.is_master_process():\n            if force:\n                # make sure even the subprocess dies really on force\n                os._exit(1)\n\n            return\n\n        self.logger.warning(\"Received stop signal by server.\")\n        if not self.stop_requested_force:\n            self.stop_requested_force = force\n\n        raise_sigint()", "response": "Stop signal by server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef step(self, step, total, label='STEP', speed_label='STEPS/S', size=1):\n\n        self.lock.acquire()\n        try:\n            time_diff = time.time() - self.last_step_time\n\n            if self.last_step > step:\n                # it restarted\n                self.last_step = 0\n\n            made_steps_since_last_call = step - self.last_step\n            self.last_step = step\n\n            self.made_steps_since_last_sync += made_steps_since_last_call\n            self.made_steps_size_since_last_sync += made_steps_since_last_call * size\n\n            if time_diff >= 1 or step == total:  # only each second or last batch\n                self.set_system_info('step', step, True)\n                self.set_system_info('steps', total, True)\n\n                steps_per_second = self.made_steps_since_last_sync / time_diff\n                samples_per_second = self.made_steps_size_since_last_sync / time_diff\n                self.last_step_time = time.time()\n\n                if size:\n                    self.report_speed(samples_per_second)\n\n                epochs_per_second = steps_per_second / total  # all batches\n                self.set_system_info('epochsPerSecond', epochs_per_second, True)\n\n                current_epochs = self.current_epoch if self.current_epoch else 1\n                total_epochs = self.total_epochs if self.total_epochs else 1\n\n                self.made_steps_since_last_sync = 0\n                self.made_steps_size_since_last_sync = 0\n\n                eta = 0\n                if step < total:\n                    # time to end this epoch\n                    if steps_per_second != 0:\n                        eta = (total - step) / steps_per_second\n\n                # time until all epochs are done\n                if total_epochs - current_epochs > 0:\n                    if epochs_per_second != 0:\n                        eta += (total_epochs - (current_epochs)) / epochs_per_second\n\n                self.git.store_file('aetros/job/times/eta.json', simplejson.dumps(eta))\n\n            if label and self.step_label != label:\n                self.set_system_info('stepLabel', label, True)\n                self.step_label = label\n\n            if speed_label and self.step_speed_label != speed_label:\n                self.set_system_info('stepSpeedLabel', speed_label, True)\n                self.step_speed_label = speed_label\n        finally:\n            self.lock.release()", "response": "Increase the step indicator."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new loss channel.", "response": "def create_loss_channel(self, name='loss', xaxis=None, yaxis=None, layout=None):\n        \"\"\"\n        :param name: string\n        :return: JobLossGraph\n        \"\"\"\n\n        return JobLossChannel(self, name, xaxis, yaxis, layout)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_channel(self, name, traces=None,\n                       main=False, kpi=False, kpiTrace=0, max_optimization=True,\n                       type=JobChannel.NUMBER,\n                       xaxis=None, yaxis=None, layout=None):\n        \"\"\"\n        :param name: str\n        :param traces: None|list : per default create a trace based on \"name\".\n        :param main: bool : whether this channel is visible in the job list as column for better comparison.\n\n        :param kpi: bool : whether this channel is the KPI (key performance indicator).\n                           Used for hyperparameter optimization. Only one channel can be a kpi. Only first trace used.\n        :param kpiTrace: bool : if you have multiple traces, define which is the KPI. 0 based index.\n\n        :param max_optimization: bool : whether the optimization maximizes or minmizes the kpi. Use max_optimization=False to\n                                        tell the optimization algorithm that his channel minimizes a kpi, for instance the loss of a model.\n\n        :param type: str : One of JobChannel.NUMBER, JobChannel.TEXT\n        :param xaxis: dict\n        :param yaxis: dict\n        :param layout: dict\n        \"\"\"\n        return JobChannel(self, name, traces, main, kpi, kpiTrace, max_optimization, type, xaxis, yaxis, layout)", "response": "Create a new channel based on the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by the daemon to handle shutdown.", "response": "def on_shutdown(self):\n        \"\"\"\n        Shutdown routine. Sets the last progress (done, aborted, failed) and tries to send last logs and git commits.\n        Also makes sure the ssh connection is closed (thus, the job marked as offline).\n\n        Is triggered by atexit.register().\n        \"\"\"\n\n        self.in_shutdown = True\n\n        self.logger.debug('on_shutdown, stopped=%s, ended=%s, early_stop=%s, stop_requested=%s'\n                          % (str(self.stopped), str(self.ended), str(self.in_early_stop), str(self.stop_requested)))\n        if self.stopped or self.ended:\n            # make really sure, ssh connection closed\n            self.client.close()\n            return\n\n        if self.in_early_stop:\n            self.done()\n            return\n\n        if self.stop_requested:\n            # when SIGINT has been triggered\n            if self.stop_requested_force:\n                if not self.is_master_process():\n                    # if not master process, we just stop everything. status/progress is set by master\n                    self.stop(force_exit=True)\n                else:\n                    # master process\n                    self.fail('Force stopped.', force_exit=True)\n            else:\n                if not self.is_master_process():\n                    # if not master process, we just stop everything. status/progress is set by master\n                    self.stop()\n                else:\n                    # master process\n                    self.abort()\n\n            return\n\n        if hasattr(sys, 'last_value'):\n            # sys.last_value contains a exception, when there was an uncaught one\n            if isinstance(sys.last_value, KeyboardInterrupt):\n                # can only happen when KeyboardInterrupt has been raised manually\n                # since the one from the default sigint handler will never reach here\n                # since we catch the sigint signal and sys.exit() before the default sigint handler\n                # is able to raise KeyboardInterrupt\n                self.abort()\n            else:\n                self.fail(type(sys.last_value).__name__ + ': ' + str(sys.last_value))\n\n        elif self.running:\n            self.done()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fail(self, message=None, force_exit=False):\n        global last_exit_code\n\n        if not last_exit_code:\n            last_exit_code = 1\n\n        with self.git.batch_commit('FAILED'):\n            self.set_status('FAILED', add_section=False)\n            self.git.commit_json_file('FAIL_MESSAGE', 'aetros/job/crash/error', str(message) if message else '')\n            if isinstance(sys.stderr, GeneralLogger):\n                self.git.commit_json_file('FAIL_MESSAGE_LAST_LOG', 'aetros/job/crash/last_message', sys.stderr.last_messages)\n\n        self.logger.debug('Crash report stored in commit ' + self.git.get_head_commit())\n        self.stop(JOB_STATUS.PROGRESS_STATUS_FAILED, force_exit=force_exit)", "response": "Mark the job as failed saves the given error message and force exists the process when force_exit = True."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_log(self, message):\n        if self.stream_log and not self.ended:\n            # points to the Git stream write\n            self.stream_log.write(message)\n            return True", "response": "Write a message to the Git log."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets an arbitrary status visible in the big wheel of the job view.", "response": "def set_status(self, status, add_section=True):\n        \"\"\"\n        Set an arbitrary status, visible in the big wheel of the job view.\n        \"\"\"\n        status = str(status)\n\n        if add_section:\n            self.section(status)\n\n        self.job_add_status('status', status)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, create_info=None, hyperparameter=None, server='local', insights=False):\n        if not create_info:\n            create_info = {\n                'server': server,\n                'config': {\n                    'insights': insights,\n                    'command': ' '.join(sys.argv)\n                }\n            }\n            config = find_config(self.config_path, logger=self.logger)\n\n            if not config['model']:\n                raise Exception('AETROS config file (aetros.yml) not found.')\n\n            # first transform simple format in the full definition with parameter types\n            # (string, number, group, choice_group, etc)\n            full_hyperparameters = lose_parameters_to_full(config['parameters'])\n\n            # now extract hyperparameters from full definition, and overwrite stuff using\n            # incoming_hyperparameter if available\n            hyperparameter = extract_parameters(full_hyperparameters, hyperparameter)\n            create_info['config']['parameters'] = hyperparameter\n\n        self.job = create_info\n\n        if 'server' not in self.job and server:\n            # setting this disables server assignment\n            self.job['server'] = server\n\n        self.job['optimization'] = None\n        self.job['type'] = 'custom'\n\n        if 'parameters' not in self.job['config']:\n            self.job['config']['parameters'] = {}\n\n        if 'insights' not in self.job['config']:\n            self.job['config']['insights'] = insights\n\n        self.job['created'] = time.time()\n        self.git.create_job_id(self.job)\n\n        self.logger.debug(\"Job created with Git ref \" + self.git.ref_head)\n\n        return self.job_id", "response": "Creates a new job in git and pushes it to the local store."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads hyperparameter from job configuration.", "response": "def get_parameter(self, path, default=None, return_group=False):\n        \"\"\"\n        Reads hyperparameter from job configuration. If nothing found use given default.\n\n        :param path: str \n        :param default: *\n        :param return_group: If true and path is a choice_group, we return the dict instead of the group name.\n        :return: *\n        \"\"\"\n        value = read_parameter_by_path(self.job['config']['parameters'], path, return_group)\n\n        if value is None:\n            return default\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, job_id):\n        self.git.read_job(job_id, checkout=self.is_master_process())\n        self.load_job_from_ref()", "response": "Load a job into index and work - tree restarts its ref and sets as current."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_job_from_ref(self):\n        if not self.job_id:\n            raise Exception('Job not loaded yet. Use load(id) first.')\n\n        if not os.path.exists(self.git.work_tree + '/aetros/job.json'):\n            raise Exception('Could not load aetros/job.json from git repository. Make sure you have created the job correctly.')\n\n        with open(self.git.work_tree + '/aetros/job.json') as f:\n            self.job = simplejson.loads(f.read(), object_pairs_hook=collections.OrderedDict)\n\n        if not self.job:\n            raise Exception('Could not parse aetros/job.json from git repository. Make sure you have created the job correctly.')\n\n        self.logger.debug('job: ' + str(self.job))", "response": "Loads the job. json into self. job"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a JobModel instance with current loaded job data attached.", "response": "def get_job_model(self):\n        \"\"\"\n        Returns a new JobModel instance with current loaded job data attached.\n        :return: JobModel\n        \"\"\"\n        if not self.job:\n            raise Exception('Job not loaded yet. Use load(id) first.')\n\n        return JobModel(self.job_id, self.job, self.home_config['storage_dir'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all files in the working directory.", "response": "def file_list(self):\n        \"\"\"\n        Lists all files in the working directory.\n        \"\"\"\n        blacklist = ['.git', 'aetros']\n        working_tree = self.git.work_tree\n\n        def recursive(path='.'):\n            if os.path.basename(path) in blacklist:\n                return 0, 0\n\n            if os.path.isdir(path):\n                files = []\n                for file in os.listdir(path):\n                    if path and path != '.':\n                        file = path + '/' + file\n\n                    added_files = recursive(file)\n                    files += added_files\n\n                return files\n            else:\n                if path.endswith('.pyc'):\n                    return []\n\n                if is_ignored(path, self.config['ignore']):\n                    return []\n\n                return [os.path.relpath(path, working_tree)]\n\n        return recursive(working_tree)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds all files in a working_tree to the list of files in the working_tree.", "response": "def add_files(self, working_tree, report=False):\n        \"\"\"\n        Commits all files from limited in aetros.yml. `files` is a whitelist, `exclude_files` is a blacklist.\n        If both are empty, we commit all files smaller than 10MB.\n        :return:\n        \"\"\"\n        blacklist = ['.git']\n\n        def add_resursiv(path = '.', report=report):\n            if os.path.basename(path) in blacklist:\n                return 0, 0\n\n            if working_tree + '/aetros' == path:\n                # ignore in work_tree the folder ./aetros/, as it could be\n                # that we checked out a job and start it again.\n                return 0, 0\n\n            if os.path.isdir(path):\n                files = 0\n                size = 0\n                for file in os.listdir(path):\n                    if path and path != '.':\n                        file = path + '/' + file\n\n                    added_files, added_size = add_resursiv(file)\n                    files += added_files\n                    size += added_size\n\n                return files, size\n            else:\n                if path.endswith('.pyc'):\n                    return 0, 0\n\n                relative_path = os.path.relpath(path, working_tree)\n\n                if is_ignored(relative_path, self.config['ignore']):\n                    return 0, 0\n\n                self.logger.debug(\"added file to job \" + relative_path)\n                if report:\n                    print(\"Added job file: \" + relative_path)\n\n                self.git.add_file_path_in_work_tree(path, working_tree, verbose=False)\n\n                return 1, os.path.getsize(path)\n\n        return add_resursiv(working_tree, report=report)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_embedding_word2vec(self, x, path, dimensions=None, header_with_dimensions=True):\n        if path.endswith('.txt'):\n            if not os.path.exists(path):\n                raise Exception(\"Given word2vec file does not exist: \" + path)\n\n            f = open(path, 'r')\n\n            if not header_with_dimensions and not dimensions:\n                raise Exception('Either the word2vec file should contain the dimensions as header or it needs to be'\n                                'specified manually using dimensions=[x,y] argument.')\n\n            if header_with_dimensions:\n                line = f.readline()\n                if ' ' not in line:\n                    raise Exception('Given word2vec file should have in first line the dimensions, e.g.: 1000 200')\n                dimensions = np.fromstring(line, dtype=np.uint, sep=' ').tolist()\n\n            labels = ''\n            vectors = ''\n            line_pos = 1 if header_with_dimensions else 0\n\n            if len(dimensions) != 2:\n                raise Exception('dimensions invalid shape. e.g. [200, 32] => 200 rows, 32 cols.')\n\n            for line in iter(f.readline, ''):\n                line_pos += 1\n                space_pos = line.find(' ')\n                if -1 == space_pos:\n                    message = 'Given word2vec does not have correct format in line ' + str(line_pos)\n                    message += '\\nGot: ' + str(line)\n                    raise Exception(message)\n\n                labels += line[:space_pos] + '\\n'\n                vectors += line[space_pos+1:] + ' '\n\n            vectors = np.fromstring(vectors, dtype=np.float32, sep=' ').tobytes()\n        else:\n            raise Exception(\"Given word2vec is not a .txt file. Other file formats are not supported.\")\n\n        info = {\n            'dimensions': dimensions\n        }\n\n        name = os.path.basename(path)\n        self._ensure_insight(x)\n        remote_path = 'aetros/job/insight/'+str(x)+'/embedding/'\n        with self.git.batch_commit('INSIGHT_EMBEDDING ' + str(x)):\n            self.git.commit_file('WORD2VEC', remote_path + name + '/tensor.bytes', vectors)\n            self.git.commit_file('WORD2VEC', remote_path + name + '/metadata.tsv', labels)\n            self.git.commit_file('WORD2VEC', remote_path + name + '/info.json', simplejson.dumps(info))", "response": "Add an embedding word2vec file to the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new embedding to the list of images and vectors.", "response": "def add_embedding_path(self, x, dimensions, vectors_path, metadata=None, image_shape=None, image=None):\n        \"\"\"\n        Adds a new embedding with optional metadata.\n        \n        Example how to generate vectors based on 2D numpy array:\n\n        # 4 vectors, each size of 3\n        vectors = [\n            [2.3, 4.0, 33],\n            [2.4, 4.2, 44],\n            [2.5, 3.9, 34],\n            [5.5, 200.2, 66]\n        ]\n\n        metadata = [\n            # header, only necessary when more then on column\n            # can be anything.\n            ['label', 'count'],\n\n            # for each vector from above an entry.\n            ['red', '123'],\n            ['white', '143'],\n            ['yellow', '344'],\n            ['house', '24'],\n        ]\n\n        numpy.array(vectors, dtype=numpy.float32).tofile('vectors.bytes')\n        numpy.savetxt('metadata.tsv', numpy.array(metadata), delimiter='\\t', fmt='%s')\n\n        job.add_embedding_path([4, 3], 'vectors.bytes', 'metadata.tsv')\n\n        Metadata format example:\n\n        Label\\tCount\\n\n        red\\t4\\n\n        yellow\\t6\\n\n\n        :param x: The x axis of the insights. \n        :param dimensions: 2D List of dimension, e.g [200, 20], means 200 vectors and each vector has size of 20.\n        \n        :param vectors_path: A path to a floats64 bytes file, no separators, sum(dimensions)*floats64 long.\n                       Example: If dimensions [200, 20] then the tensor file has 200*20 float32 bytes in it\n                        \n        :param metadata: A TSV file. If only one column long (=no tab separator per line), then there's no need for a header.\n                         If you have more than one column, use the first line as header.\n                       \n        :param image_shape: Size of the image of each vector.\n        :param image: Path to an image sprite.\n        :return: \n        \"\"\"\n        if not os.path.exists(vectors_path):\n            raise Exception(\"Given embedding vectors file does not exist: \" + vectors_path)\n\n        if metadata and not os.path.exists(metadata):\n            raise Exception(\"Given embedding metadata file does not exist: \" + metadata)\n\n        name = os.path.basename(vectors_path)\n        self._ensure_insight(x)\n        remote_path = 'aetros/job/insight/'+str(x)+'/embedding/'\n\n        info = {\n            'dimensions': dimensions,\n            'image_shape': image_shape,\n            'image': os.path.basename(image) if image else None,\n        }\n\n        with self.git.lock_write():\n            self.git.add_file_path(remote_path + name + '/tensor.bytes', vectors_path)\n            self.git.add_file_path(remote_path + name + '/metadata.tsv', metadata)\n            self.git.add_file(remote_path + name + '/info.json', simplejson.dumps(info))\n\n            if image:\n                self.git.add_file(remote_path + name + '/' + os.path.basename(image), image)\n\n            self.git.commit_index('INSIGHT_EMBEDDING ' + str(x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse sane directory output usually ls - l", "response": "def split_file_info(fileinfo):\n    \"\"\" Parse sane directory output usually ls -l\n        Adapted from https://gist.github.com/tobiasoberrauch/2942716\n    \"\"\"\n    current_year = datetime.datetime.now().strftime('%Y')\n    files = []\n    for line in fileinfo:\n        parts = re.split(\n            r'^([\\-dbclps])' +                  # Directory flag [1]\n            r'([\\-rwxs]{9})\\s+' +               # Permissions [2]\n            r'(\\d+)\\s+' +                       # Number of items [3]\n            r'([a-zA-Z0-9_-]+)\\s+' +            # File owner [4]\n            r'([a-zA-Z0-9_-]+)\\s+' +            # File group [5]\n            r'(\\d+)\\s+' +                       # File size in bytes [6]\n            r'(\\w{3}\\s+\\d{1,2})\\s+' +           # 3-char month and 1/2-char day of the month [7]\n            r'(\\d{1,2}:\\d{1,2}|\\d{4})\\s+' +     # Time or year (need to check conditions) [+= 7]\n            r'(.+)$',                           # File/directory name [8]\n            line\n        )\n\n        date = parts[7]\n        time = parts[8] if ':' in parts[8] else '00:00'\n        year = parts[8] if ':' not in parts[8] else current_year\n        dt_obj = parser.parse(\"%s %s %s\" % (date, year, time))\n\n        files.append({\n            'directory': parts[1],\n            'perms': parts[2],\n            'items': parts[3],\n            'owner': parts[4],\n            'group': parts[5],\n            'size': int(parts[6]),\n            'date': date,\n            'time': time,\n            'year': year,\n            'name': parts[9],\n            'datetime': dt_obj\n        })\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, remote, local=None):\n        if isinstance(local, file_type):  # open file, leave open\n            local_file = local\n        elif local is None:  # return string\n            local_file = buffer_type()\n        else:  # path to file, open, write/close return None\n            local_file = open(local, 'wb')\n\n        self.conn.retrbinary(\"RETR %s\" % remote, local_file.write)\n\n        if isinstance(local, file_type):\n            pass\n        elif local is None:\n            contents = local_file.getvalue()\n            local_file.close()\n            return contents\n        else:\n            local_file.close()\n\n        return None", "response": "Gets the file from FTP server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nput a local file on the FTP server and return the size of the file.", "response": "def put(self, local, remote, contents=None, quiet=False):\n        \"\"\" Puts a local file (or contents) on to the FTP server\n\n            local can be:\n                a string: path to inpit file\n                a file: opened for reading\n                None: contents are pushed\n        \"\"\"\n        remote_dir = os.path.dirname(remote)\n        remote_file = os.path.basename(local)\\\n            if remote.endswith('/') else os.path.basename(remote)\n\n        if contents:\n            # local is ignored if contents is set\n            local_file = buffer_type(contents)\n        elif isinstance(local, file_type):\n            local_file = local\n        else:\n            local_file = open(local, 'rb')\n        current = self.conn.pwd()\n        self.descend(remote_dir, force=True)\n\n        size = 0\n        try:\n            self.conn.storbinary('STOR %s' % remote_file, local_file)\n            size = self.conn.size(remote_file)\n        except:\n            if not quiet:\n                raise\n        finally:\n            local_file.close()\n            self.conn.cwd(current)\n        return size"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, remote='.', extra=False, remove_relative_paths=False):\n        if extra:\n            self.tmp_output = []\n            self.conn.dir(remote, self._collector)\n            directory_list = split_file_info(self.tmp_output)\n        else:\n            directory_list = self.conn.nlst(remote)\n\n        if remove_relative_paths:\n            return list(filter(self.is_not_relative_path, directory_list))\n\n        return directory_list", "response": "Return a list of all the files in the local directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndescend the current directory with the remote directory.", "response": "def descend(self, remote, force=False):\n        \"\"\" Descend, possibly creating directories as needed \"\"\"\n        remote_dirs = remote.split('/')\n        for directory in remote_dirs:\n            try:\n                self.conn.cwd(directory)\n            except Exception:\n                if force:\n                    self.conn.mkd(directory)\n                    self.conn.cwd(directory)\n        return self.conn.pwd()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, remote):\n        try:\n            self.conn.delete(remote)\n        except Exception:\n            return False\n        else:\n            return True", "response": "Delete a file from server"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges working directory on server", "response": "def cd(self, remote):\n        \"\"\" Change working directory on server \"\"\"\n        try:\n            self.conn.cwd(remote)\n        except Exception:\n            return False\n        else:\n            return self.pwd()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart a job with all logging of a job_id", "response": "def start(logger, full_id, fetch=True, env=None, volumes=None, cpus=None, memory=None, gpu_devices=None, offline=False):\n    \"\"\"\n    Starts the job with all logging of a job_id\n    \"\"\"\n\n    owner, name, id = unpack_full_job_id(full_id)\n\n    if isinstance(sys.stdout, GeneralLogger):\n        # we don't want to have stuff written to stdout before in job's log\n        sys.stdout.clear_buffer()\n\n    job_backend = JobBackend(model_name=owner + '/' + name)\n\n    if fetch:\n        job_backend.fetch(id)\n\n    job_backend.restart(id)\n    job_backend.start(collect_system=False, offline=offline)\n    job_backend.set_status('PREPARE', add_section=False)\n\n    job = job_backend.get_job_model()\n\n    if not cpus:\n        cpus = job.get_cpu()\n\n    if not memory:\n        memory = job.get_memory()\n\n    if not gpu_devices and job.get_gpu():\n        # if requested 2 GPUs and we have 3 GPUs with id [0,1,2], gpus should be [0,1]\n        gpu_devices = []\n        for i in range(0, job.get_gpu()):\n            gpu_devices.append(i)\n\n    start_command(logger, job_backend, env, volumes, cpus=cpus, memory=memory, gpu_devices=gpu_devices, offline=offline)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromimage(im, flatten=False, mode=None):\n    if not Image.isImageType(im):\n        raise TypeError(\"Input is not a PIL image.\")\n\n    if mode is not None:\n        if mode != im.mode:\n            im = im.convert(mode)\n    elif im.mode == 'P':\n        # Mode 'P' means there is an indexed \"palette\".  If we leave the mode\n        # as 'P', then when we do `a = array(im)` below, `a` will be a 2-D\n        # containing the indices into the palette, and not a 3-D array\n        # containing the RGB or RGBA values.\n        if 'transparency' in im.info:\n            im = im.convert('RGBA')\n        else:\n            im = im.convert('RGB')\n\n    if flatten:\n        im = im.convert('F')\n    elif im.mode == '1':\n        # Workaround for crash in PIL. When im is 1-bit, the call array(im)\n        # can cause a seg. fault, or generate garbage. See\n        # https://github.com/scipy/scipy/issues/2138 and\n        # https://github.com/python-pillow/Pillow/issues/350.\n        #\n        # This converts im from a 1-bit image to an 8-bit image.\n        im = im.convert('L')\n\n    a = array(im)\n    return a", "response": "Returns a copy of a PIL image as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n            mode=None, channel_axis=None):\n    \"\"\"Takes a numpy array and returns a PIL image.\n\n    The mode of the PIL image depends on the array shape and the `pal` and\n    `mode` keywords.\n\n    For 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n    (from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\n    is given as 'F' or 'I' in which case a float and/or integer array is made.\n\n    Notes\n    -----\n    For 3-D arrays, the `channel_axis` argument tells which dimension of the\n    array holds the channel data.\n\n    For 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\n    by default or 'YCbCr' if selected.\n\n    The numpy array must be either 2 dimensional or 3 dimensional.\n\n    \"\"\"\n    data = asarray(arr)\n    if iscomplexobj(data):\n        raise ValueError(\"Cannot convert a complex-valued array.\")\n    shape = list(data.shape)\n    valid = len(shape) == 2 or ((len(shape) == 3) and\n                                ((3 in shape) or (4 in shape)))\n    if not valid:\n        raise ValueError(\"'arr' does not have a suitable array shape for \"\n                         \"any mode.\")\n    if len(shape) == 2:\n        shape = (shape[1], shape[0])  # columns show up first\n        if mode == 'F':\n            data32 = data.astype(numpy.float32)\n            image = Image.frombytes(mode, shape, data32.tostring())\n            return image\n        if mode in [None, 'L', 'P']:\n            bytedata = bytescale(data, high=high, low=low,\n                                 cmin=cmin, cmax=cmax)\n            image = Image.frombytes('L', shape, bytedata.tostring())\n            if pal is not None:\n                image.putpalette(asarray(pal, dtype=uint8).tostring())\n                # Becomes a mode='P' automagically.\n            elif mode == 'P':  # default gray-scale\n                pal = (arange(0, 256, 1, dtype=uint8)[:, newaxis] *\n                       ones((3,), dtype=uint8)[newaxis, :])\n                image.putpalette(asarray(pal, dtype=uint8).tostring())\n            return image\n        if mode == '1':  # high input gives threshold for 1\n            bytedata = (data > high)\n            image = Image.frombytes('1', shape, bytedata.tostring())\n            return image\n        if cmin is None:\n            cmin = amin(ravel(data))\n        if cmax is None:\n            cmax = amax(ravel(data))\n        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n        if mode == 'I':\n            data32 = data.astype(numpy.uint32)\n            image = Image.frombytes(mode, shape, data32.tostring())\n        else:\n            raise ValueError(_errstr)\n        return image\n\n    # if here then 3-d array with a 3 or a 4 in the shape length.\n    # Check for 3 in datacube shape --- 'RGB' or 'YCbCr'\n    if channel_axis is None:\n        if (3 in shape):\n            ca = numpy.flatnonzero(asarray(shape) == 3)[0]\n        else:\n            ca = numpy.flatnonzero(asarray(shape) == 4)\n            if len(ca):\n                ca = ca[0]\n            else:\n                raise ValueError(\"Could not find channel dimension.\")\n    else:\n        ca = channel_axis\n\n    numch = shape[ca]\n    if numch not in [3, 4]:\n        raise ValueError(\"Channel axis dimension is not valid.\")\n\n    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n    if ca == 2:\n        strdata = bytedata.tostring()\n        shape = (shape[1], shape[0])\n    elif ca == 1:\n        strdata = transpose(bytedata, (0, 2, 1)).tostring()\n        shape = (shape[2], shape[0])\n    elif ca == 0:\n        strdata = transpose(bytedata, (1, 2, 0)).tostring()\n        shape = (shape[2], shape[1])\n    if mode is None:\n        if numch == 3:\n            mode = 'RGB'\n        else:\n            mode = 'RGBA'\n\n    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n        raise ValueError(_errstr)\n\n    if mode in ['RGB', 'YCbCr']:\n        if numch != 3:\n            raise ValueError(\"Invalid array shape for mode.\")\n    if mode in ['RGBA', 'CMYK']:\n        if numch != 4:\n            raise ValueError(\"Invalid array shape for mode.\")\n\n    # Here we know data and mode is correct\n    image = Image.frombytes(mode, shape, strdata)\n    return image", "response": "Takes a numpy array and returns a PIL image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef imresize(arr, size, interp='bilinear', mode=None):\n    im = toimage(arr, mode=mode)\n    ts = type(size)\n    if issubdtype(ts, int):\n        percent = size / 100.0\n        size = tuple((array(im.size)*percent).astype(int))\n    elif issubdtype(type(size), float):\n        size = tuple((array(im.size)*size).astype(int))\n    else:\n        size = (size[1], size[0])\n    func = {'nearest': 0, 'lanczos': 1, 'bilinear': 2, 'bicubic': 3, 'cubic': 3}\n    imnew = im.resize(size, resample=func[interp])\n    return fromimage(imnew)", "response": "Resize an image.\n\n    Parameters\n    ----------\n    arr : ndarray\n        The array of image to be resized.\n\n    size : int, float or tuple\n        * int   - Percentage of current size.\n        * float - Fraction of current size.\n        * tuple - Size of the output image.\n\n    interp : str, optional\n        Interpolation to use for re-sizing ('nearest', 'lanczos', 'bilinear', 'bicubic'\n        or 'cubic').\n\n    mode : str, optional\n        The PIL image mode ('P', 'L', etc.) to convert `arr` before resizing.\n\n    Returns\n    -------\n    imresize : ndarray\n        The resized array of image.\n\n    See Also\n    --------\n    toimage : Implicitly used to convert `arr` according to `mode`.\n    scipy.ndimage.zoom : More generic implementation that does not use PIL."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to a specific channel.", "response": "def connect(self, channel):\n        \"\"\"\n        In the write-thread we detect that no connection is living anymore and try always again.\n        Up to the 3 connection try, we report to user. We keep trying but in silence.\n        Also, when more than 10 connection tries are detected, we delay extra 15 seconds.\n        \"\"\"\n        if self.connection_tries > 10:\n            time.sleep(10)\n\n        if self.in_connecting[channel]:\n            return False\n\n        self.in_connecting[channel] = True\n\n        self.logger.debug('[%s] Wanna connect ...' % (channel, ))\n\n        try:\n            if self.is_connected(channel) or self.online is False:\n                if self.is_connected(channel):\n                    self.logger.debug('[%s] Already connected' % (channel, ))\n                if self.online is False:\n                    self.logger.debug('[%s] self.online=False' % (channel, ))\n\n                return True\n\n            self.channel_lock[channel].acquire()\n\n            self.connected[channel] = None\n            self.registered[channel] = None\n            self.ssh_stream[channel] = False\n            self.ssh_channel[channel] = False\n            messages = None\n            stderrdata = ''\n\n            try:\n                if not self.ssh_stream[channel]:\n                    self.logger.debug('[%s] Open ssh connection' % (channel, ))\n                    self.ssh_stream[channel] = create_ssh_stream(self.config, exit_on_failure=False)\n\n                self.logger.debug('[%s] open channel' % (channel, ))\n\n                self.ssh_channel[channel] = self.ssh_stream[channel].get_transport().open_session()\n                self.ssh_channel[channel].exec_command('stream')\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except Exception as e:\n                self.connected[channel] = False\n                self.registered[channel] = False\n                self.logger.debug('[%s] connection failed: %s'  % (channel, str(e)))\n                return False\n            finally:\n                self.channel_lock[channel].release()\n\n            if self.ssh_channel[channel]:\n                messages = self.wait_for_at_least_one_message(channel)\n\n            if not messages:\n                stderrdata = self.ssh_channel[channel].recv_stderr().decode(\"utf-8\").strip()\n                self.connected[channel] = False\n                self.registered[channel] = False\n            else:\n                self.logger.debug('[%s] opened and received %d messages' % (channel, len(messages)))\n                self.connected[channel] = True\n                self.registered[channel] = self.on_connect(self.was_connected_once[channel], channel)\n                self.connected_since[channel] = time.time()\n\n                if channel == '' and self.registered[channel] and self.was_connected_once[channel]:\n                    self.logger.info(\"Successfully reconnected.\")\n\n            if not self.registered[channel]:\n                # make sure to close channel and connection first\n                try:\n                    self.ssh_channel[channel] and self.ssh_channel[channel].close()\n                except: pass\n\n                try:\n                    self.ssh_stream[channel] and self.ssh_stream[channel].close()\n                except: pass\n\n                self.logger.debug(\"[%s] Client: registration failed. stderrdata: %s\" % (channel, stderrdata))\n                self.connected[channel] = False\n\n                try:\n                    self.logger.debug('[%s] Client: ssh_tream close due to registration failure' % (channel, ))\n                    self.ssh_stream[channel].close()\n                except (KeyboardInterrupt, SystemExit):\n                    raise\n\n                self.connection_tries += 1\n                if not self.was_connected_once[channel] and self.go_offline_on_first_failed_attempt:\n                    # initial try needs to be online, otherwise we go offline\n                    self.go_offline()\n\n                if stderrdata:\n                    if 'Connection refused' not in stderrdata and 'Permission denied' not in stderrdata:\n                        self.logger.error(stderrdata)\n\n                if 'Permission denied' in stderrdata:\n                    if self.connection_tries < 3:\n                        self.logger.warning(\"Access denied. Did you setup your SSH public key correctly \"\n                                            \"and saved it in your AETROS Trainer user account?\")\n\n                    self.close()\n                    sys.exit(1)\n\n                self.connection_error(channel, \"Connection error during connecting to %s: %s\" % (self.host, str(stderrdata)))\n            else:\n                self.was_connected_once[channel] = True\n\n        except Exception as error:\n            self.connection_error(channel, error)\n        finally:\n            self.in_connecting[channel] = False\n\n        return self.is_connected(channel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _end_channel(self, channel):\n        self.stop_on_empty_queue[channel] = True\n\n        # by joining the we wait until its loop finishes.\n        # it won't loop forever since we've set self.stop_on_empty_queue=True\n        write_thread = self.thread_write_instances[channel]\n\n        thread_join_non_blocking(write_thread)", "response": "End the channel and wait for the write thread to finish."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_sending_last_messages(self):\n        if self.active and self.online is not False:\n            self.logger.debug(\"client sends last %s messages ...\"\n                              % ([str(i) + ':' + str(len(x)) for i, x in six.iteritems(self.queues)],))\n\n            for channel, messages in six.iteritems(self.queues):\n                for idx, message in enumerate(messages):\n                    self.logger.debug(\"[%s] %d: %s\" % (channel, idx, str(message)[0:120]))\n\n            # send all missing messages\n\n            # by joining we wait until its loop finish.\n            # it won't loop forever since we've set self.stop_on_empty_queue=True\n            for channel in six.iterkeys(self.ssh_channel):\n                if channel != '':\n                    self._end_channel(channel)\n\n            # last is control channel\n            self._end_channel('')", "response": "Send all messages to the last available channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting until all queues of channels are empty.", "response": "def wait_until_queue_empty(self, channels, report=True, clear_end=True):\n        \"\"\"\n        Waits until all queues of channels are empty.\n        \"\"\"\n        state = {'message': ''}\n\n        self.logger.debug(\"wait_until_queue_empty: report=%s %s\"\n                          % (str(report), str([channel+':'+str(len(self.queues[channel])) for channel in channels]), ))\n        queues = []\n        for channel in channels:\n            queues += self.queues[channel][:]\n\n        def print_progress():\n            if report:\n                self.logger.debug(\"all_empty=%s\" % (str(all_empty),))\n\n                sys.__stderr__.write('\\b' * len(state['message']))\n                sys.__stderr__.write(\"\\033[K\")\n\n                state['message'] = \"%.2f kB/s // %.2fkB of %.2fkB // %.2f%%\" \\\n                          % (self.bytes_speed / 1024, self.bytes_sent / 1024, self.bytes_total / 1024,\n                            (self.bytes_sent / self.bytes_total * 100) if self.bytes_total else 0)\n\n                sys.__stderr__.write(state['message'])\n                sys.__stderr__.flush()\n\n        while True:\n            all_empty = all(m['_sent'] for m in queues)\n\n            print_progress()\n\n            if all_empty:\n                break\n\n            time.sleep(0.2)\n\n        print_progress()\n\n        if report and clear_end:\n            sys.__stderr__.write('\\b' * len(state['message']))\n            sys.__stderr__.write(\"\\033[K\")\n            sys.__stderr__.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread until we receive at least one message. Return all found messages.", "response": "def wait_for_at_least_one_message(self, channel):\n        \"\"\"\n        Reads until we receive at least one message we can unpack. Return all found messages.\n        \"\"\"\n\n        unpacker = msgpack.Unpacker(encoding='utf-8')\n\n        while True:\n            try:\n                start = time.time()\n                chunk = self.ssh_channel[channel].recv(1024)\n                end = time.time()\n                self.read_speeds.append( len(chunk) / (end-start) )\n                if len(self.read_speeds) > 20:\n                    self.read_speeds = self.read_speeds[10:]\n\n                if chunk == b'':\n                    # happens only when connection broke. If nothing is to be received, it hangs instead.\n                    self.connection_error(channel, 'Connection broken w')\n                    return False\n            except Exception as error:\n                self.connection_error(channel, error)\n                raise\n\n            unpacker.feed(chunk)\n\n            messages = [m for m in unpacker]\n            if messages:\n                return messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, channel):\n\n        if not self.ssh_channel[channel].recv_ready():\n            return\n\n        try:\n            start = time.time()\n            chunk = self.ssh_channel[channel].recv(1024)\n            end = time.time()\n\n            self.read_speeds.append(len(chunk) / (end-start))\n            if len(self.read_speeds) > 20:\n                self.read_speeds = self.read_speeds[10:]\n\n        except Exception as error:\n            self.connection_error(channel, error)\n            raise\n\n        if chunk == b'':\n            # socket connection broken\n            self.connection_error(channel, 'Connection broken')\n            return None\n\n        # self.read_buffer.seek(0, 2) #make sure we write at the end\n        self.read_unpacker.feed(chunk)\n\n        # self.read_buffer.seek(0)\n        messages = [m for m in self.read_unpacker]\n\n        return messages if messages else None", "response": "Reads from the socket and returns the message."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises the SIGINT signal in the current process and all sub - processes.", "response": "def raise_sigint():\n    \"\"\"\n    Raising the SIGINT signal in the current process and all sub-processes.\n\n    os.kill() only issues a signal in the current process (without subprocesses).\n    CTRL+C on the console sends the signal to the process group (which we need).\n    \"\"\"\n    if hasattr(signal, 'CTRL_C_EVENT'):\n        # windows. Need CTRL_C_EVENT to raise the signal in the whole process group\n        os.kill(os.getpid(), signal.CTRL_C_EVENT)\n    else:\n        # unix.\n        pgid = os.getpgid(os.getpid())\n        if pgid == 1:\n            os.kill(os.getpid(), signal.SIGINT)\n        else:\n            os.killpg(os.getpgid(os.getpid()), signal.SIGINT)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef human_size(size_bytes, precision=0):\n    if size_bytes == 1:\n        # because I really hate unnecessary plurals\n        return \"1 byte\"\n\n    suffixes_table = [('bytes',0),('KB',0),('MB',1),('GB',2),('TB',2), ('PB',2)]\n\n    num = float(size_bytes)\n    for suffix, precision in suffixes_table:\n        if num < 1024.0:\n            break\n        num /= 1024.0\n\n    if precision == 0:\n        formatted_size = \"%d\" % num\n    else:\n        formatted_size = str(round(num, ndigits=precision))\n\n    return \"%s %s\" % (formatted_size, suffix)", "response": "Format a size in bytes into a human readable file size e. g. 1 byte KB GB TB PB"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an array to an image.", "response": "def array_to_img(x, scale=True):\n    \"\"\"\n    x should be shape (channels, width, height)\n    \"\"\"\n    from PIL import Image\n    if x.ndim != 3:\n        raise Exception('Unsupported shape : ', str(x.shape), '. Need (channels, width, height)')\n    if scale:\n        x += max(-np.min(x), 0)\n        x /= np.max(x)\n        x *= 255\n    if x.shape[0] == 3:\n        # RGB\n        if x.dtype != 'uint8':\n            x = x.astype('uint8')\n        return Image.fromarray(x.astype('uint8'), 'RGB')\n    elif x.shape[0] == 1:\n        # grayscale\n        if x.dtype != 'uint8':\n            x = x.astype('uint8')\n        return Image.fromarray(x.reshape(x.shape[1], x.shape[2]), 'L')\n    else:\n        raise Exception('Unsupported channel number: ', x.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_generator_validation_nb(self, number):\n\n        self.nb_val_samples = number\n        diff_to_batch = number % self.get_batch_size()\n        if diff_to_batch > 0:\n            self.nb_val_samples += self.get_batch_size() - diff_to_batch\n\n        import keras\n        if '1' != keras.__version__[0]:\n            self.nb_val_samples = self.nb_val_samples // self.get_batch_size()", "response": "Sets self. nb_val_samples which is used in model. fit if input is a generator\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets self. samples_per_epoch which is used in model. fit if input is a generator ArcGIS object.", "response": "def set_generator_training_nb(self, number):\n        \"\"\"\n        sets self.samples_per_epoch which is used in model.fit if input is a generator\n        :param number:\n        :return:\n        \"\"\"\n\n        self.samples_per_epoch = number\n        diff_to_batch = number % self.get_batch_size()\n        if diff_to_batch > 0:\n            self.samples_per_epoch += self.get_batch_size() - diff_to_batch"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attach(self, buffer, read_line=None):\n\n        bid = id(buffer)\n        self.attach_last_messages[bid] = b''\n\n        def reader():\n            current_line = b''\n\n            def handle_line(buf):\n                if chunk == b'':\n                    return\n\n                if read_line and callable(read_line):\n                    res = read_line(buf)\n                    if res is False:\n                        return False\n\n                    elif res is not None:\n                        buf = res\n                        if hasattr(buf, 'encode'):\n                            buf = buf.encode('utf-8')\n\n                self.attach_last_messages[bid] += buf\n\n                if len(self.attach_last_messages[bid]) > 21 * 1024:\n                    self.attach_last_messages[bid] = self.attach_last_messages[bid][-20 * 1024:]\n\n                self.write(buf)\n\n            flush_char = b'\\n'\n\n            while True:\n                try:\n                    # needs to be 1 so we fetch data in near real-time\n                    chunk = buffer.read(1)\n                    if chunk == b'':\n                        if current_line:\n                            handle_line(current_line)\n                        return\n\n                    current_line += chunk\n\n                    while flush_char in current_line:\n                        pos = current_line.find(flush_char)\n                        line = current_line[:pos+1]\n                        current_line = current_line[pos+1:]\n                        handle_line(line)\n\n                    # todo, periodically flush by '\\r' only (progress bars for example)\n                    # and make sure only necessary data is sent (by applying \\r and \\b control characters)\n\n                except (KeyboardInterrupt, SystemExit):\n                    raise\n                except Exception:\n                    # we need to make sure, we continue to read otherwise the process of this buffer\n                    # will block and we have a stuck process.\n                    sys.__stderr__.write(traceback.format_exc() + '\\n')\n                    sys.__stderr__.flush()\n\n        thread = Thread(target=reader)\n        thread.daemon = True\n        thread.start()\n\n        def wait():\n            thread_join_non_blocking(thread)\n            self.send_buffer()\n\n        return wait", "response": "Reads a buffer and sends it to the logger and self. job_backend."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an upscaled image array for the given ratio.", "response": "def upscale(image, ratio):\n    \"\"\"\n    return upscaled image array\n    Arguments:\n    image -- a (H,W,C) numpy.ndarray\n    ratio -- scaling factor (>1)\n    \"\"\"\n    if not isinstance(image, np.ndarray):\n        raise ValueError('Expected ndarray')\n    if ratio < 1:\n        raise ValueError('Ratio must be greater than 1 (ratio=%f)' % ratio)\n    width = int(math.floor(image.shape[1] * ratio))\n    height = int(math.floor(image.shape[0] * ratio))\n    channels = image.shape[2]\n    out = np.ndarray((height, width, channels), dtype=np.uint8)\n    for x, y in np.ndindex((width, height)):\n        out[y, x] = image[int(math.floor(y / ratio)), int(math.floor(x / ratio))]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef embed_image_html(image):\n    if image is None:\n        return None\n    elif isinstance(image, PIL.Image.Image):\n        pass\n    elif isinstance(image, np.ndarray):\n        image = PIL.Image.fromarray(image)\n    else:\n        raise ValueError('image must be a PIL.Image or a np.ndarray')\n\n    # Read format from the image\n    fmt = image.format\n    if not fmt:\n        # default to JPEG\n        fmt = 'jpeg'\n    else:\n        fmt = fmt.lower()\n\n    string_buf = StringIO()\n    image.save(string_buf, format=fmt)\n    data = string_buf.getvalue().encode('base64').replace('\\n', '')\n    return 'data:image/%s;base64,%s' % (fmt, data)", "response": "Returns an image embedded in HTML base64 format"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd bounding boxes to an image", "response": "def add_bboxes_to_image(image, bboxes, color='red', width=1):\n    \"\"\"\n    Draw rectangles on the image for the bounding boxes\n    Returns a PIL.Image\n    Arguments:\n    image -- input image\n    bboxes -- bounding boxes in the [((l, t), (r, b)), ...] format\n    Keyword arguments:\n    color -- color to draw the rectangles\n    width -- line width of the rectangles\n    Example:\n    image = Image.open(filename)\n    add_bboxes_to_image(image, bboxes[filename], width=2, color='#FF7700')\n    image.show()\n    \"\"\"\n    def expanded_bbox(bbox, n):\n        \"\"\"\n        Grow the bounding box by n pixels\n        \"\"\"\n        l = min(bbox[0][0], bbox[1][0])\n        r = max(bbox[0][0], bbox[1][0])\n        t = min(bbox[0][1], bbox[1][1])\n        b = max(bbox[0][1], bbox[1][1])\n        return ((l - n, t - n), (r + n, b + n))\n\n    from PIL import Image, ImageDraw\n    draw = ImageDraw.Draw(image)\n    for bbox in bboxes:\n        for n in range(width):\n            draw.rectangle(expanded_bbox(bbox, n), outline=color)\n\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a vis_square for the given layer data", "response": "def get_layer_vis_square(data,\n                         allow_heatmap=True,\n                         normalize=True,\n                         min_img_dim=100,\n                         max_width=1200,\n                         channel_order='RGB',\n                         colormap='jet',\n                         ):\n    \"\"\"\n    Returns a vis_square for the given layer data\n    Arguments:\n    data -- a np.ndarray\n    Keyword arguments:\n    allow_heatmap -- if True, convert single channel images to heatmaps\n    normalize -- whether to normalize the data when visualizing\n    max_width -- maximum width for the vis_square\n    \"\"\"\n    if channel_order not in ['RGB', 'BGR']:\n        raise ValueError('Unsupported channel_order %s' % channel_order)\n    if data.ndim == 1:\n        # interpret as 1x1 grayscale images\n        # (N, 1, 1)\n        data = data[:, np.newaxis, np.newaxis]\n    elif data.ndim == 2:\n        # interpret as 1x1 grayscale images\n        # (N, 1, 1)\n        data = data.reshape((data.shape[0] * data.shape[1], 1, 1))\n    elif data.ndim == 3:\n        if data.shape[0] == 3:\n            # interpret as a color image\n            # (1, H, W,3)\n            if channel_order == 'BGR':\n                data = data[[2, 1, 0], ...]  # BGR to RGB (see issue #59)\n            data = data.transpose(1, 2, 0)\n            data = data[np.newaxis, ...]\n        else:\n            # interpret as grayscale images\n            # (N, H, W)\n            pass\n    elif data.ndim == 4:\n        if data.shape[0] == 3:\n            # interpret as HxW color images\n            # (N, H, W, 3)\n            data = data.transpose(1, 2, 3, 0)\n            if channel_order == 'BGR':\n                data = data[:, :, :, [2, 1, 0]]  # BGR to RGB (see issue #59)\n        elif data.shape[1] == 3:\n            # interpret as HxW color images\n            # (N, H, W, 3)\n            data = data.transpose(0, 2, 3, 1)\n            if channel_order == 'BGR':\n                data = data[:, :, :, [2, 1, 0]]  # BGR to RGB (see issue #59)\n        else:\n            # interpret as HxW grayscale images\n            # (N, H, W)\n            data = data.reshape((data.shape[0] * data.shape[1], data.shape[2], data.shape[3]))\n    else:\n        raise RuntimeError('unrecognized data shape: %s' % (data.shape,))\n\n    return get_layer_vis_square_raw(data,\n                         allow_heatmap,\n                         normalize,\n                         min_img_dim,\n                         max_width,\n                         colormap,\n                         )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_color_map(name):\n    redmap = [0]\n    greenmap = [0]\n    bluemap = [0]\n    if name == 'white':\n        # essentially a noop\n        redmap = [0, 1]\n        greenmap = [0, 1]\n        bluemap = [0, 1]\n    elif name == 'simple':\n        redmap = [0, 1, 1, 1]\n        greenmap = [0, 0, 1, 1]\n        bluemap = [0, 0, 0, 1]\n    elif name == 'hot':\n        redmap = [0, 0.03968253968253968, 0.07936507936507936, 0.119047619047619, 0.1587301587301587, 0.1984126984126984, 0.2380952380952381, 0.2777777777777778, 0.3174603174603174, 0.3571428571428571, 0.3968253968253968, 0.4365079365079365, 0.4761904761904762, 0.5158730158730158, 0.5555555555555556, 0.5952380952380952,\n                  0.6349206349206349, 0.6746031746031745, 0.7142857142857142, 0.753968253968254, 0.7936507936507936, 0.8333333333333333, 0.873015873015873, 0.9126984126984127, 0.9523809523809523, 0.992063492063492, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        greenmap = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.03174603174603163, 0.0714285714285714, 0.1111111111111112, 0.1507936507936507, 0.1904761904761905, 0.23015873015873, 0.2698412698412698, 0.3095238095238093, 0.3492063492063491, 0.3888888888888888, 0.4285714285714284,\n                    0.4682539682539679, 0.5079365079365079, 0.5476190476190477, 0.5873015873015872, 0.6269841269841268, 0.6666666666666665, 0.7063492063492065, 0.746031746031746, 0.7857142857142856, 0.8253968253968254, 0.8650793650793651, 0.9047619047619047, 0.9444444444444442, 0.984126984126984, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        bluemap = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04761904761904745, 0.1269841269841265,\n                   0.2063492063492056, 0.2857142857142856, 0.3650793650793656, 0.4444444444444446, 0.5238095238095237, 0.6031746031746028, 0.6825396825396828, 0.7619047619047619, 0.8412698412698409, 0.92063492063492, 1]\n    elif name == 'rainbow':\n        redmap = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.9365079365079367, 0.8571428571428572, 0.7777777777777777, 0.6984126984126986, 0.6190476190476191, 0.53968253968254, 0.4603174603174605, 0.3809523809523814, 0.3015873015873018, 0.2222222222222223, 0.1428571428571432,\n                  0.06349206349206415, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.03174603174603208, 0.08465608465608465, 0.1375661375661377, 0.1904761904761907, 0.2433862433862437, 0.2962962962962963, 0.3492063492063493, 0.4021164021164023, 0.4550264550264553, 0.5079365079365079, 0.5608465608465609, 0.6137566137566139, 0.666666666666667]\n        greenmap = [0, 0.03968253968253968, 0.07936507936507936, 0.119047619047619, 0.1587301587301587, 0.1984126984126984, 0.2380952380952381, 0.2777777777777778, 0.3174603174603174, 0.3571428571428571, 0.3968253968253968, 0.4365079365079365, 0.4761904761904762, 0.5158730158730158, 0.5555555555555556, 0.5952380952380952, 0.6349206349206349, 0.6746031746031745, 0.7142857142857142, 0.753968253968254, 0.7936507936507936,\n                    0.8333333333333333, 0.873015873015873, 0.9126984126984127, 0.9523809523809523, 0.992063492063492, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.9841269841269842, 0.9047619047619047, 0.8253968253968256, 0.7460317460317465, 0.666666666666667, 0.587301587301587, 0.5079365079365079, 0.4285714285714288, 0.3492063492063493, 0.2698412698412698, 0.1904761904761907, 0.1111111111111116, 0.03174603174603208, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        bluemap = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01587301587301582, 0.09523809523809534, 0.1746031746031744, 0.2539682539682535,\n                   0.333333333333333, 0.412698412698413, 0.4920634920634921, 0.5714285714285712, 0.6507936507936507, 0.7301587301587302, 0.8095238095238093, 0.8888888888888884, 0.9682539682539679, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n    elif name == 'winter':\n        greenmap = [0, 1]\n        bluemap = [1, 0.5]\n    else:\n        if name != 'jet':\n            print('Warning: colormap \"%s\" not supported. Using jet instead.' % name)\n        redmap = [0, 0, 0, 0, 0.5, 1, 1, 1, 0.5]\n        greenmap = [0, 0, 0.5, 1, 1, 1, 0.5, 0, 0]\n        bluemap = [0.5, 1, 1, 1, 0.5, 0, 0, 0, 0]\n    return 255.0 * np.array(redmap), 255.0 * np.array(greenmap), 255.0 * np.array(bluemap)", "response": "Returns a colormap as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_index_file(self):\n        if os.getenv('AETROS_GIT_INDEX_FILE'):\n            self.index_path = os.getenv('AETROS_GIT_INDEX_FILE')\n            return\n\n        import tempfile\n        h, path = tempfile.mkstemp('aetros-git', '', self.temp_path)\n\n        self.index_path = path\n\n        # we give git a unique file path for that index. However, git expect it to be non-existent for empty indexes.\n        # empty file would lead to \"fatal: index file smaller than expected\"\n        os.close(h)\n        os.unlink(self.index_path)\n\n        self.logger.debug('GIT_INDEX_FILE created at ' + self.index_path)", "response": "Prepare the index file for the current job."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_job(self, job_id, checkout=False):\n        self.job_id = job_id\n\n        self.logger.debug(\"Git fetch job reference %s\" % (self.ref_head, ))\n        out, code, err = self.command_exec(['ls-remote', 'origin', self.ref_head])\n\n        if code:\n            self.logger.error('Could not find the job ' + job_id + ' on the server. Are you online and does the job exist?')\n            sys.exit(1)\n\n        try:\n            self.command_exec(['fetch', '-f', '-n', 'origin', self.ref_head+':'+self.ref_head])\n        except Exception:\n            self.logger.error(\"Could not load job information for \" + job_id + '. You need to be online to start pre-configured jobs.')\n            raise\n\n        self.read_job(job_id, checkout)", "response": "Fetch the current job from origin and read its tree into working director."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the head and reads the tree into index and checkout the work - tree when checkout = True.", "response": "def read_job(self, job_id, checkout=False):\n        \"\"\"\n        Reads head and reads the tree into index,\n        and checkout the work-tree when checkout=True.\n\n        This does not fetch the job from the actual server. It needs to be in the local git already.\n        \"\"\"\n        self.job_id = job_id\n\n        commit = self.get_head_commit()\n        self.logger.debug('Job ref points to ' + commit)\n        self.command_exec(['read-tree', self.ref_head])\n\n        if checkout:\n            self.logger.debug('Working directory in ' + self.work_tree)\n\n            # make sure we have checked out all files we have added until now. Important for simple models,\n            # so we have the actual model.py and dataset scripts.\n            if os.path.exists(self.work_tree):\n                shutil.rmtree(self.work_tree)\n\n            os.makedirs(self.work_tree)\n\n            # make the working tree reflect exactly the tree of ref_head.\n            # since we removed the dir before, we have exactly the tree of the reference\n            # '--', '.' is important to not update HEAD\n            self.command_exec(['--work-tree', self.work_tree, 'checkout', self.ref_head, '--', '.'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_job_id(self, data):\n        self.add_file('aetros/job.json', simplejson.dumps(data, indent=4))\n        tree_id = self.write_tree()\n\n        self.job_id = self.command_exec(['commit-tree', '-m', \"JOB_CREATED\", tree_id])[0].decode('utf-8').strip()\n\n        out, code, err = self.command_exec(['show-ref', self.ref_head], allowed_to_fail=True)\n        if not code:\n            self.logger.warning(\"Generated job id already exists, because exact same experiment values given. Ref \" + self.ref_head)\n\n        self.command_exec(['update-ref', self.ref_head, self.job_id])\n\n        # make sure we have checkedout all files we have added until now. Important for simple models, so we have the\n        # actual model.py and dataset scripts.\n        if not os.path.exists(self.work_tree):\n            os.makedirs(self.work_tree)\n\n        # updates index and working tree\n        # '--', '.' is important to not update HEAD\n        self.command_exec(['--work-tree', self.work_tree, 'checkout', self.ref_head, '--', '.'])\n\n        # every caller needs to make sure to call git.push\n        return self.job_id", "response": "Create a new job id and a reference."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_push_sync(self):\n        self.active_thread = True\n        self.active_push = True\n\n        self.thread_push_instance = Thread(target=self.thread_push)\n        self.thread_push_instance.daemon = True\n        self.thread_push_instance.start()", "response": "Starts the detection of unsynced Git data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop(self):\n        self.active_thread = False\n\n        if self.thread_push_instance and self.thread_push_instance.isAlive():\n            self.thread_push_instance.join()\n\n        with self.batch_commit('STREAM_END'):\n            for path, handle in six.iteritems(self.streamed_files.copy()):\n                # open again and read full content\n                full_path = os.path.normpath(self.temp_path + '/stream-blob/' + self.job_id + '/' + path)\n                self.logger.debug('Git stream end for file: ' + full_path)\n\n                del self.streamed_files[path]\n\n                # make sure its written to the disk\n                try:\n                    self.stream_files_lock.acquire()\n                    if not handle.closed:\n                        handle.flush()\n                        handle.close()\n                finally:\n                    self.stream_files_lock.release()\n\n                with open(full_path, 'r') as f:\n                    self.commit_file(path, path, f.read())\n\n                if not self.keep_stream_files:\n                    os.unlink(full_path)\n\n        with self.batch_commit('STORE_END'):\n            for path, bar in six.iteritems(self.store_files.copy()):\n                full_path = os.path.normpath(self.temp_path + '/store-blob/' + self.job_id + '/' + path)\n                self.logger.debug('Git store end for file: ' + full_path)\n\n                del self.store_files[path]\n\n                try:\n                    self.stream_files_lock.acquire()\n                    self.commit_file(path, path, open(full_path, 'r').read())\n                finally:\n                    self.stream_files_lock.release()\n\n                if not self.keep_stream_files:\n                    os.unlink(full_path)", "response": "Stops the git push thread and commits all streamed files followed by a final git push."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_file(self, path, data, fast_lane=True):\n\n        self.stream_files_lock.acquire()\n        try:\n\n            full_path = os.path.normpath(self.temp_path + '/store-blob/' + self.job_id + '/' + path)\n            if not os.path.exists(os.path.dirname(full_path)):\n                os.makedirs(os.path.dirname(full_path))\n\n            if hasattr(data, 'encode'):\n                data = data.encode(\"utf-8\", 'replace')\n\n            already_set = path in self.store_files and self.store_files[path] == data\n\n            if is_debug3():\n                sys.__stderr__.write('git:store_file(%s, %s, %s), already_set=%s\\n'\n                                     % (str(path), str(data)[0:180], str(fast_lane), str(already_set)))\n\n            if already_set:\n                return\n\n            open(full_path, 'wb').write(data)\n            self.store_files[path] = data\n\n            if self.client.online is not False:\n                self.client.send({'type': 'store-blob', 'path': path, 'data': data}, channel='' if fast_lane else 'files')\n        finally:\n            self.stream_files_lock.release()", "response": "Store the file at the given path and data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstream a file to the server.", "response": "def stream_file(self, path, fast_lane=True):\n        \"\"\"\n        Create a temp file, stream it to the server if online and append its content using the write() method. \n        This makes sure that we have all newest data of this file on the server directly.\n        \n        At the end of the job, the content the server received is stored as git blob on the server. It is then committed \n        locally and pushed. Git detects that the server already has the version (through the continuous streaming)\n        and won't push it again. Very handy for rather large files that will append over time (like channel data, logs)\n        \n        Example:\n        \n        self.log_stream = git.stream_file('log.txt')\n        \n        self.log_stream.write(\"new line\\n\");\n        self.log_stream.write(\"another line\\n\");\n        \"\"\"\n\n        # create temp file\n        # open temp file\n\n        # register stream file and write locally\n        # on end() git_commit that file locally\n\n        # create socket connection to server\n        # stream file to server\n        # on end() send server end signal, so he can store its content in git as blob as well.\n        # A git push would detect that both sides have the same content already,\n        # except when server connection broke between start() and end().\n        # Result -> already transmitted logs/channel data (probably many MBs) won't transfered twice\n        # when doing a git-push.\n\n        # return handler to write to this file\n\n        full_path = os.path.normpath(self.temp_path + '/stream-blob/' + self.job_id + '/' + path)\n        if not os.path.exists(os.path.dirname(full_path)):\n            os.makedirs(os.path.dirname(full_path))\n\n        handle = open(full_path, 'wb')\n        self.streamed_files[path] = handle\n\n        class Stream():\n            def __init__(self, git):\n                self.git = git\n\n            def write(self, data):\n                if path not in self.git.streamed_files:\n                    # already committed to server\n                    return\n\n                if hasattr(data, 'encode'):\n                    data = data.encode(\"utf-8\", 'replace')\n\n                try:\n                    self.git.stream_files_lock.acquire()\n                    if not handle.closed:\n                        handle.write(data)\n                        handle.flush()\n                except IOError as e:\n                    handle.close()\n\n                    if 'No space left' in e.__str__():\n                        sys.stderr.write(traceback.format_exc() + '\\n')\n                        self.git.logger.error(e.__str__())\n                finally:\n                    self.git.stream_files_lock.release()\n\n                if self.git.client.online is not False:\n                    self.git.client.send({'type': 'stream-blob', 'path': path, 'data': data}, channel='' if fast_lane else 'files')\n\n        return Stream(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd new entry to the current index.", "response": "def add_index(self, mode, blob_id, path):\n        \"\"\"\n        Add new entry to the current index\n        :param tree: \n        :return: \n        \"\"\"\n        self.command_exec(['update-index', '--add', '--cacheinfo', mode, blob_id, path])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new file to the storage and add its tree entry into the index.", "response": "def add_file(self, git_path, content):\n        \"\"\"\n        Add a new file as blob in the storage and add its tree entry into the index.\n        \n        :param git_path: str\n        :param content: str\n        \"\"\"\n        blob_id = self.write_blob(content)\n        self.add_index('100644', blob_id, git_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new file to the storage and add its tree entry into the index.", "response": "def add_file_path_in_work_tree(self, path, work_tree, verbose=True):\n        \"\"\"\n        Add a new file as blob in the storage and add its tree entry into the index.\n        \"\"\"\n        args = ['--work-tree', work_tree, 'add', '-f']\n        if verbose:\n            args.append('--verbose')\n        args.append(path)\n        self.command_exec(args, show_output=verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new file as blob in the storage and commit the index.", "response": "def commit_file(self, message, path, content):\n        \"\"\"\n        Add a new file as blob in the storage, add its tree entry into the index and commit the index.\n\n        :param message: str\n        :param path: str\n        :param content: str\n        :return: \n        \"\"\"\n        if self.git_batch_commit:\n            self.add_file(path, content)\n            self.git_batch_commit_messages.append(message)\n        else:\n            with self.lock_write():\n                if self.job_id:\n                    self.read_tree(self.ref_head)\n\n                self.add_file(path, content)\n\n                return self.commit_index(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diff_objects(self, latest_commit_sha):\n        base = ['git', '--bare', '--git-dir', self.git_path]\n\n        object_shas = []\n        summary = {'commits': [], 'trees': [], 'files': []}\n\n        def read_parents_and_tree_from(commit):\n            if commit in self.synced_object_shas or commit in object_shas:\n                # this commit has already been synced or read\n                return None, None\n\n            self.synced_object_shas[commit] = True\n            summary['commits'].append(commit)\n            object_shas.append(commit)\n\n            object_content = subprocess.check_output(base + ['cat-file', '-p', commit]).decode('utf-8').strip()\n            parents = []\n            tree = ''\n            for line in object_content.splitlines():\n                if line.startswith('tree '):\n                    tree = line[len('tree '):]\n                if line.startswith('parent '):\n                    parents.append(line[len('parent '):])\n\n            return parents, tree\n\n        def collect_files_from_tree(tree):\n            if tree in self.synced_object_shas or tree in object_shas:\n                # we have exactly this tree already synced or read, meaning all its objects as well\n                return\n\n            self.synced_object_shas[tree] = True\n            summary['trees'].append(tree)\n            object_shas.append(tree)\n\n            object_content = subprocess.check_output(base + ['ls-tree', '-r', '-t', tree]).decode('utf-8').strip()\n\n            for line in object_content.splitlines():\n                exploded = line.split(' ')\n\n                if len(exploded) < 3:\n                    sys.stderr.write(\"Error: Wrong line format of ls-tree for %s: %s\\n\" % (tree, line,))\n                    sys.exit(1)\n\n                object_to_add = str(exploded[2][:40])\n                path = str(exploded[2][41:])\n\n                if object_to_add in self.synced_object_shas or object_to_add in object_shas:\n                    # have it already in the list or already synced\n                    continue\n\n                object_shas.append(object_to_add)\n                self.synced_object_shas[object_to_add] = True\n                summary['files'].append([object_to_add, path])\n\n        commits_to_check = [latest_commit_sha]\n\n        while len(commits_to_check):\n            sha = commits_to_check.pop(0)\n            parents, tree = read_parents_and_tree_from(sha)\n\n            if parents:\n                for parent in parents:\n                    if parent not in commits_to_check:\n                        commits_to_check.append(parent)\n\n            if tree:\n                collect_files_from_tree(tree)\n\n        is_debug2() and self.logger.debug(\"shas_to_check %d: %s \" % (len(object_shas), str(object_shas),))\n\n        if not object_shas:\n            return [], summary\n\n        try:\n            is_debug2() and self.logger.debug(\"Do git-cat-file-check.sh\")\n\n            ssh_stream = create_ssh_stream(read_home_config(), exit_on_failure=False)\n            channel = ssh_stream.get_transport().open_session()\n            channel.exec_command('git-cat-file-check.sh \"%s\"' % (self.model_name + '.git',))\n            channel.sendall('\\n'.join(object_shas))\n            channel.shutdown_write()\n\n            def readall(c):\n                content = b''\n                while True:\n                    try:\n                        chunk = c.recv(1024)\n                        if chunk == b'':\n                            break\n                        content += chunk\n                    except (KeyboardInterrupt, SystemExit):\n                        return\n\n                return content\n\n            missing_objects = readall(channel).decode('utf-8').splitlines()\n            channel.close()\n            ssh_stream.close()\n\n            # make sure we have in summary only SHAs we actually will sync\n            for stype in six.iterkeys(summary):\n                ids = summary[stype][:]\n                for sha in ids:\n                    if stype == 'files':\n                        if sha[0] not in missing_objects:\n                            summary[stype].remove(sha)\n                    else:\n                        if sha not in missing_objects:\n                            summary[stype].remove(sha)\n\n            return missing_objects, summary\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except Exception as e:\n            self.logger.error(\"Failed to generate diff_objects: %s\" % (str(e),))\n            for sha in object_shas:\n                if sha in self.synced_object_shas:\n                    del self.synced_object_shas[sha]\n            return None, None", "response": "Push all changes to origin based on objects on commits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commit_index(self, message):\n        tree_id = self.write_tree()\n\n        args = ['commit-tree', tree_id, '-p', self.ref_head]\n\n        # todo, this can end in a race-condition with other processes adding commits\n        commit = self.command_exec(args, message)[0].decode('utf-8').strip()\n        self.command_exec(['update-ref', self.ref_head, commit])\n\n        return commit", "response": "Commit the current index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the given path of current ref_head and returns its content as utf - 8", "response": "def contents(self, path):\n        \"\"\"\n        Reads the given path of current ref_head and returns its content as utf-8\n        \"\"\"\n        try:\n            out, code, err = self.command_exec(['cat-file', '-p', self.ref_head+':'+path])\n            if not code:\n                return out.decode('utf-8')\n        except Exception:\n            pass\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting training of a job.", "response": "def job_start(job_backend, trainer, keras_callback):\n    \"\"\"\n    Starts the training of a job. Needs job_prepare() first.\n\n    :type job_backend: JobBackend\n    :type trainer: Trainer\n    :return:\n    \"\"\"\n\n    job_backend.set_status('STARTING')\n    job_model = job_backend.get_job_model()\n\n    model_provider = job_model.get_model_provider()\n\n    job_backend.set_status('LOAD DATA')\n    datasets = job_model.get_datasets(trainer)\n\n    print('trainer.input_shape = %s\\n' % (simplejson.dumps(trainer.input_shape, default=invalid_json_values),))\n    print('trainer.classes = %s\\n' % (simplejson.dumps(trainer.classes, default=invalid_json_values),))\n\n    multiple_inputs = len(datasets) > 1\n    insights_x = [] if multiple_inputs else None\n\n    for dataset_name in job_model.get_input_dataset_names():\n        dataset = datasets[dataset_name]\n\n        if is_generator(dataset['X_train']):\n            batch_x, batch_y = dataset['X_train'].next()\n\n            if multiple_inputs:\n                insights_x.append(batch_x[0])\n            else:\n                insights_x = batch_x[0]\n        else:\n            if multiple_inputs:\n                insights_x.append(dataset['X_train'][0])\n            else:\n                insights_x = dataset['X_train'][0]\n\n    keras_callback.insights_x = insights_x\n    print('Insights sample shape', keras_callback.insights_x.shape)\n    keras_callback.write(\"Possible data keys '%s'\\n\" % \"','\".join(list(datasets.keys())))\n\n    data_train = model_provider.get_training_data(trainer, datasets)\n    data_validation = model_provider.get_validation_data(trainer, datasets)\n\n    keras_callback.set_validation_data(data_validation, trainer.nb_val_samples)\n\n    trainer.set_status('CONSTRUCT')\n    model = model_provider.get_model(trainer)\n    trainer.set_model(model)\n\n    trainer.set_status('COMPILING')\n    loss = model_provider.get_loss(trainer)\n    optimizer = model_provider.get_optimizer(trainer)\n    model_provider.compile(trainer, model, loss, optimizer)\n    model.summary()\n\n    trainer.callbacks.append(keras_callback)\n    model_provider.train(trainer, model, data_train, data_validation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_images_in_memory(job_model, dataset, node, trainer):\n    concurrent = psutil.cpu_count()\n\n    dataset_config = dataset['config']\n    controller = {'running': True}\n    q = Queue(concurrent)\n\n    result = {\n        'X_train': [],\n        'Y_train': [],\n        'X_test': [],\n        'Y_test': []\n    }\n\n    images = []\n    max = 0\n\n    path = job_model.get_dataset_downloads_dir(dataset)\n    if 'path' in dataset['config']:\n        path = dataset['config']['path']\n\n    classes_count = 0\n    category_map = {}\n    classes = []\n\n    trainer.set_status('LOAD IMAGES INTO MEMORY')\n\n    try:\n        for i in range(concurrent):\n            t = ImageReadWorker(q, job_model, node, path, images, controller)\n            t.daemon = True\n            t.start()\n\n        for validation_or_training in ['validation', 'training']:\n            if os.path.isdir(os.path.normpath(path + '/' + validation_or_training)):\n                for category_name in os.listdir(os.path.normpath(path + '/' + validation_or_training)):\n                    if os.path.isdir(os.path.normpath(path + '/' + validation_or_training + '/' + category_name)):\n\n                        if category_name not in category_map:\n                            category_map[category_name] = classes_count\n                            if 'classes' in dataset_config and 'category_' in category_name:\n                                category_idx = int(category_name.replace('category_', ''))\n                                category_map[category_name] = category_idx\n                                target_category = dataset_config['classes'][category_idx]\n                                classes.append(target_category['title'] or 'Class %s' % (category_idx, ))\n                            else:\n                                classes.append(category_name)\n\n                            classes_count += 1\n\n                        for id in os.listdir(os.path.normpath(path + '/' + validation_or_training + '/' + category_name)):\n                            file_path = os.path.join(path, validation_or_training, category_name, id)\n                            q.put([file_path, validation_or_training == 'validation', category_name])\n                            max += 1\n\n        q.join()\n        controller['running'] = False\n\n        train_images = []\n        test_images = []\n\n        for v in images:\n            image, validation, category_dir = v\n            if validation is True:\n                test_images.append([image, category_map[category_dir]])\n            else:\n                train_images.append([image, category_map[category_dir]])\n\n        train_datagen = None\n        augmentation = bool(get_option(dataset_config, 'augmentation', False))\n        if augmentation:\n            train_datagen = get_image_data_augmentor_from_dataset(dataset)\n\n        train = InMemoryDataGenerator(train_datagen, train_images, classes_count, job_model.job['config']['batchSize'])\n\n        test = InMemoryDataGenerator(None, test_images, classes_count, job_model.job['config']['batchSize'])\n\n        nb_sample = len(train_images)\n        trainer.set_info('Dataset size', {'training': nb_sample, 'validation': len(test_images)})\n        trainer.set_generator_training_nb(nb_sample)\n        trainer.set_generator_validation_nb(len(test_images))\n\n        trainer.logger.info((\"Found %d classes, %d images (%d in training [%saugmented], %d in validation). Read all images into memory from %s\" %\n               (classes_count, max, len(train_images), 'not ' if augmentation is False else '', len(test_images), path)))\n\n        if classes_count == 0:\n            trainer.logger.warning(\"Could not find any classes. Does the directory contains images?\")\n            sys.exit(1)\n\n        trainer.output_size = classes_count\n        trainer.set_info('classes', classes)\n        trainer.classes = classes\n\n        result['X_train'] = train\n        result['Y_train'] = train\n        result['X_test'] = test\n        result['Y_test'] = test\n\n        return result\n\n    except KeyboardInterrupt:\n        controller['running'] = False\n        sys.exit(1)", "response": "Reads all images into memory and applies augmentation if enabled"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the list of devices that are available on the device system.", "response": "def get_ordered_devices():\n    \"\"\"\n    Default CUDA_DEVICE_ORDER is not compatible with nvidia-docker.\n    Nvidia-Docker is using CUDA_DEVICE_ORDER=PCI_BUS_ID.\n\n    https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker#gpu-isolation\n    \"\"\"\n\n    libcudart = get_libcudart()\n\n    devices = {}\n    for i in range(0, get_installed_devices()):\n        gpu = get_device_properties(i)\n\n        pciBusId = ctypes.create_string_buffer(64)\n        libcudart.cudaDeviceGetPCIBusId(ctypes.byref(pciBusId), 64, i)\n        full_id = pciBusId.value.decode('utf-8')\n        gpu['fullId'] = full_id\n        devices[full_id] = gpu\n\n    ordered = []\n\n    i = 0\n    for key in sorted(devices):\n        devices[key]['id'] = i\n        ordered.append(devices[key])\n        i += 1\n\n    del libcudart\n\n    return ordered"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sorted_nicely(l):\n    convert = lambda text: int(text) if text.isdigit() else text\n    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n    return sorted(l, key = alphanum_key)", "response": "Sort the given iterable in natural order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the part number equivalent to the aracore. n.", "response": "def to_pn(self, sub_letter=None):\n        \"\"\"\n        Returns the part number equivalent.  For instance,\n        a '1k' would still be '1k', but a\n        '1.2k' would, instead, be a '1k2'\n        :return:\n        \"\"\"\n        string = str(self)\n        if '.' not in string:\n            return string\n\n        # take care of the case of when there is no scaling unit\n        if not string[-1].isalpha():\n            if sub_letter is not None:\n                return string.replace('.', sub_letter)\n\n            return string\n\n        letter = string[-1]\n        return string.replace('.', letter)[:-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns two lists - the last N items of li whose total size is less than MAX_SIZE - the rest of the original list li", "response": "def reduce_list_size(li):\n    \"\"\"Return two lists\n        - the last N items of li whose total size is less than MAX_SIZE\n        - the rest of the original list li\n    \"\"\"\n    # sys.getsizeof is nearly useless. All our data is stringable so rather\n    # use that as a measure of size.\n    size = len(repr(li))\n    keep = li\n    toss = []\n    n = len(li)\n    decrement_by = max(n / 10, 10)\n    while (size >= MAX_SIZE) and (n > 0):\n        n -= decrement_by\n        toss = li[:-n]\n        keep = li[-n:]\n        size = len(repr(keep))\n    return keep, toss"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninspect request for objects in _ultracache and set appropriate entries in Django s cache.", "response": "def cache_meta(request, cache_key, start_index=0):\n    \"\"\"Inspect request for objects in _ultracache and set appropriate entries\n    in Django's cache.\"\"\"\n\n    path = request.get_full_path()\n    # todo: cache headers on the request since they never change during the\n    # request.\n\n    # Reduce headers to the subset as defined by the settings\n    headers = OrderedDict()\n    for k, v in sorted(request.META.items()):\n        if (k == \"HTTP_COOKIE\") and CONSIDER_COOKIES:\n            cookie = SimpleCookie()\n            cookie.load(v)\n            headers[\"cookie\"] = \"; \".join([\n                \"%s=%s\" % (k, morsel.value) for k, morsel \\\n                    in sorted(cookie.items()) if k in CONSIDER_COOKIES\n            ])\n        elif k.startswith(\"HTTP_\"):\n            k = k[5:].replace(\"_\", \"-\").lower()\n            if k in CONSIDER_HEADERS:\n                headers[k] = v\n\n    # Lists needed for cache.get_many\n    to_set_get_keys = []\n    to_set_paths_get_keys = []\n    to_set_content_types_get_keys = []\n    to_set_content_types_paths_get_keys = []\n\n    # Dictionaries needed for cache.set_many\n    to_set = {}\n    to_set_paths = {}\n    to_set_content_types = {}\n    to_set_content_types_paths = {}\n\n    to_delete = []\n    to_set_objects = []\n\n    for ctid, obj_pk in request._ultracache[start_index:]:\n        # The object appears in these cache entries. If the object is modified\n        # then these cache entries are deleted.\n        key = \"ucache-%s-%s\" % (ctid, obj_pk)\n        if key not in to_set_get_keys:\n            to_set_get_keys.append(key)\n\n        # The object appears in these paths. If the object is modified then any\n        # caches that are read from when browsing to this path are cleared.\n        key = \"ucache-pth-%s-%s\" % (ctid, obj_pk)\n        if key not in to_set_paths_get_keys:\n            to_set_paths_get_keys.append(key)\n\n        # The content type appears in these cache entries. If an object of this\n        # content type is created then these cache entries are cleared.\n        key = \"ucache-ct-%s\" % ctid\n        if key not in to_set_content_types_get_keys:\n            to_set_content_types_get_keys.append(key)\n\n        # The content type appears in these paths. If an object of this content\n        # type is created then any caches that are read from when browsing to\n        # this path are cleared.\n        key = \"ucache-ct-pth-%s\" % ctid\n        if key not in to_set_content_types_paths_get_keys:\n            to_set_content_types_paths_get_keys.append(key)\n\n        # A list of objects that contribute to a cache entry\n        tu = (ctid, obj_pk)\n        if tu not in to_set_objects:\n            to_set_objects.append(tu)\n\n    # todo: rewrite to handle absence of get_many\n    di = cache.get_many(to_set_get_keys)\n    for key in to_set_get_keys:\n        v = di.get(key, None)\n        keep = []\n        if v is not None:\n            keep, toss = reduce_list_size(v)\n            if toss:\n                to_set[key] = keep\n                to_delete.extend(toss)\n        if cache_key not in keep:\n            if key not in to_set:\n                to_set[key] = keep\n            to_set[key] = to_set[key] + [cache_key]\n    if to_set == di:\n        to_set = {}\n\n    di = cache.get_many(to_set_paths_get_keys)\n    for key in to_set_paths_get_keys:\n        v = di.get(key, None)\n        keep = []\n        if v is not None:\n            keep, toss = reduce_list_size(v)\n            if toss:\n                to_set_paths[key] = keep\n        if [path, headers] not in keep:\n            if key not in to_set_paths:\n                to_set_paths[key] = keep\n            to_set_paths[key] = to_set_paths[key] + [[path, headers]]\n    if to_set_paths == di:\n        to_set_paths = {}\n\n    di = cache.get_many(to_set_content_types_get_keys)\n    for key in to_set_content_types_get_keys:\n        v = di.get(key, None)\n        keep = []\n        if v is not None:\n            keep, toss = reduce_list_size(v)\n            if toss:\n                to_set_content_types[key] = keep\n                to_delete.extend(toss)\n        if cache_key not in keep:\n            if key not in to_set_content_types:\n                to_set_content_types[key] = keep\n            to_set_content_types[key] = to_set_content_types[key] + [cache_key]\n    if to_set_content_types == di:\n        to_set_content_types = {}\n\n    di = cache.get_many(to_set_content_types_paths_get_keys)\n    for key in to_set_content_types_paths_get_keys:\n        v = di.get(key, None)\n        keep = []\n        if v is not None:\n            keep, toss = reduce_list_size(v)\n            if toss:\n                to_set_content_types_paths[key] = keep\n        if [path, headers] not in keep:\n            if key not in to_set_content_types_paths:\n                to_set_content_types_paths[key] = keep\n            to_set_content_types_paths[key] = to_set_content_types_paths[key] \\\n                + [[path, headers]]\n    if to_set_content_types_paths == di:\n        to_set_content_types_paths = {}\n\n    # Deletion must happen first because set may set some of these keys\n    if to_delete:\n        try:\n            cache.delete_many(to_delete)\n        except NotImplementedError:\n            for k in to_delete:\n                cache.delete(k)\n\n    # Do one set_many\n    di = {}\n    di.update(to_set)\n    del to_set\n    di.update(to_set_paths)\n    del to_set_paths\n    di.update(to_set_content_types)\n    del to_set_content_types\n    di.update(to_set_content_types_paths)\n    del to_set_content_types_paths\n\n    if to_set_objects:\n        di[cache_key + \"-objs\"] = to_set_objects\n\n    if di:\n        try:\n            cache.set_many(di, 86400)\n        except NotImplementedError:\n            for k, v in di.items():\n                cache.set(k, v, 86400)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the TRUSTEE indy - sdk role for an anchor acting in an AnchorSmith capacity.", "response": "def least_role() -> Role:\n        \"\"\"\n        Return the TRUSTEE indy-sdk role for an anchor acting in an AnchorSmith capacity.\n\n        :return: TRUSTEE role\n        \"\"\"\n\n        LOGGER.debug('AnchorSmith.least_role >>>')\n\n        rv = Role.TRUSTEE.token()\n\n        LOGGER.debug('AnchorSmith.least_role <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def send_nym(self, did: str, verkey: str = None, alias: str = None, role: Role = None) -> None:\n\n        LOGGER.debug(\n            'AnchorSmith.send_nym >>> did: %s, verkey: %s, alias: %s, role: %s', did, verkey, alias, role)\n\n        if not ok_did(did):\n            LOGGER.debug('AnchorSmith.send_nym <!< Bad DID %s', did)\n            raise BadIdentifier('Bad DID {}'.format(did))\n\n        req_json = await ledger.build_nym_request(self.did, did, verkey, alias, (role or Role.USER).token())\n        await self._sign_submit(req_json)\n\n        LOGGER.debug('AnchorSmith.send_nym <<<')", "response": "Send input anchor s cryptonym to the distributed ledger."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset wallets as configured for setnym operation.", "response": "async def _set_wallets(an_data: dict) -> dict:\n    \"\"\"\n    Set wallets as configured for setnym operation.\n\n    :param an_data: dict mapping profiles to anchor data\n    :return: dict mapping anchor names to wallet objects\n    \"\"\"\n\n    w_mgr = WalletManager()\n    rv = {}\n    for profile in an_data:\n        w_cfg = {'id': an_data[profile].name}\n        if an_data[profile].wallet_type:\n            w_cfg['storage_type'] = an_data[profile].wallet_type\n        if an_data[profile].seed:\n            w_cfg['seed'] = an_data[profile].seed\n        if an_data[profile].did:\n            w_cfg['did'] = an_data[profile].did\n        if an_data[profile].wallet_create:\n            try:\n                await w_mgr.create(w_cfg, access=an_data[profile].wallet_access)\n            except ExtantWallet:\n                pass\n        rv[profile] = w_mgr.get(w_cfg, access=an_data[profile].wallet_access)\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset nym for the current node in the ledger.", "response": "async def setnym(ini_path: str) -> int:\n    \"\"\"\n    Set configuration. Open pool, trustee anchor, and wallet of anchor whose nym to send.\n    Register exit hooks to close pool and trustee anchor.\n\n    Engage trustee anchor to send nym for VON anchor, if it differs on the ledger from configuration.\n\n    :param ini_path: path to configuration file\n    :return: 0 for OK, 1 for failure\n    \"\"\"\n\n    config = inis2dict(ini_path)\n    if config['Trustee Anchor']['name'] == config['VON Anchor']['name']:\n        raise ExtantWallet('Wallet names must differ between VON Anchor and Trustee Anchor')\n\n    cfg_van_role = config['VON Anchor'].get('role', None) or None  # nudge empty value from '' to None\n    if not ok_role(cfg_van_role):\n        raise BadRole('Configured role {} is not valid'.format(cfg_van_role))\n\n    pool_data = NodePoolData(\n        config['Node Pool']['name'],\n        config['Node Pool'].get('genesis.txn.path', None) or None)\n\n    an_data = {\n        'tan': AnchorData(\n            Role.TRUSTEE,\n            config['Trustee Anchor']['name'],\n            config['Trustee Anchor'].get('seed', None) or None,\n            config['Trustee Anchor'].get('did', None) or None,\n            config['Trustee Anchor'].get('wallet.create', '0').lower() in ['1', 'true', 'yes'],\n            config['Trustee Anchor'].get('wallet.type', None) or None,\n            config['Trustee Anchor'].get('wallet.access', None) or None),\n        'van': AnchorData(\n            Role.get(cfg_van_role),\n            config['VON Anchor']['name'],\n            config['VON Anchor'].get('seed', None) or None,\n            config['VON Anchor'].get('did', None) or None,\n            config['VON Anchor'].get('wallet.create', '0').lower() in ['1', 'true', 'yes'],\n            config['VON Anchor'].get('wallet.type', None) or None,\n            config['VON Anchor'].get('wallet.access', None) or None)\n    }\n\n    an_wallet = await _set_wallets(an_data)\n\n    p_mgr = NodePoolManager()\n    if pool_data.name not in await p_mgr.list():\n        if pool_data.genesis_txn_path:\n            await p_mgr.add_config(pool_data.name, pool_data.genesis_txn_path)\n        else:\n            raise AbsentPool('Node pool {} has no ledger configuration, but {} specifies no genesis txn path'.format(\n                pool_data.name,\n                ini_path))\n\n    async with an_wallet['tan'] as w_tan, (\n            an_wallet['van']) as w_van, (\n            p_mgr.get(pool_data.name)) as pool, (\n            TrusteeAnchor(w_tan, pool)) as tan, (\n            NominalAnchor(w_van, pool)) as van:\n\n        send_verkey = van.verkey\n        try:\n            nym_role = await tan.get_nym_role(van.did)\n            if an_data['van'].role == nym_role:\n                return 0  # ledger is as per configuration\n            send_verkey = None  # only owner can touch verkey\n            if nym_role != Role.USER:  # only remove role when it is not already None on the ledger\n                await tan.send_nym(van.did, send_verkey, van.wallet.name, Role.ROLE_REMOVE)\n        except AbsentNym:\n            pass  # cryptonym not there yet, fall through\n\n        await tan.send_nym(van.did, send_verkey, van.wallet.name, an_data['van'].role)\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(args: Sequence[str] = None) -> int:\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)-15s | %(levelname)-8s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S')\n    logging.getLogger('von_anchor').setLevel(logging.WARNING)\n    logging.getLogger('indy').setLevel(logging.ERROR)\n\n    if args is None:\n        args = sys.argv[1:]\n\n    if len(sys.argv) == 2:\n        try:\n            return do_wait(setnym(sys.argv[1]))\n        except VonAnchorError as vax:\n            print(str(vax))\n            return 1\n    else:\n        usage()\n        return 1", "response": "Main line for script."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns schema identifier for input origin DID schema name and schema version.", "response": "def schema_id(origin_did: str, name: str, version: str) -> str:\n    \"\"\"\n    Return schema identifier for input origin DID, schema name, and schema version.\n\n    :param origin_did: DID of schema originator\n    :param name: schema name\n    :param version: schema version\n    :return: schema identifier\n    \"\"\"\n\n    return '{}:2:{}:{}'.format(origin_did, name, version)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ok_did(token: str) -> bool:\n\n    return bool(re.match('[{}]{{21,22}}$'.format(B58), token or ''))", "response": "Checks if input token looks like a valid distributed identifier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning schema key for schema identifier components.", "response": "def schema_key(s_id: str) -> SchemaKey:\n    \"\"\"\n    Return schema key (namedtuple) convenience for schema identifier components.\n\n    :param s_id: schema identifier\n    :return: schema key (namedtuple) object\n    \"\"\"\n\n    s_key = s_id.split(':')\n    s_key.pop(1)  # take out indy-sdk schema marker: 2 marks indy-sdk schema id\n\n    return SchemaKey(*s_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cred_def_id(issuer_did: str, schema_seq_no: int, protocol: Protocol = None) -> str:\n\n    return (protocol or Protocol.DEFAULT).cred_def_id(issuer_did, schema_seq_no)", "response": "Returns the credential definition identifier for input issuer DID and schema sequence number."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether input token looks like a credential definition identifier from input issuer DID.", "response": "def ok_cred_def_id(token: str, issuer_did: str = None) -> bool:\n    \"\"\"\n    Whether input token looks like a valid credential definition identifier from input issuer DID (default any); i.e.,\n    <issuer-did>:3:CL:<schema-seq-no>:<cred-def-id-tag> for protocol >= 1.4, or\n    <issuer-did>:3:CL:<schema-seq-no> for protocol == 1.3.\n\n    :param token: candidate string\n    :param issuer_did: issuer DID to match, if specified\n    :return: whether input token looks like a valid credential definition identifier\n    \"\"\"\n\n    cd_id_m = re.match('([{}]{{21,22}}):3:CL:[1-9][0-9]*(:.+)?$'.format(B58), token or '')\n    return bool(cd_id_m) and ((not issuer_did) or cd_id_m.group(1) == issuer_did)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cred_def_id2seq_no(cd_id: str) -> int:\n\n    if ok_cred_def_id(cd_id):\n        return int(cd_id.split(':')[3])  # sequence number is token at 0-based position 3\n    raise BadIdentifier('Bad credential definition identifier {}'.format(cd_id))", "response": "Given a credential definition identifier return its schema sequence number. Raise BadIdentifier on input that is not a credential definition identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a credential definition identifier and a tag return the corresponding revocation registry identifier.", "response": "def rev_reg_id(cd_id: str, tag: Union[str, int]) -> str:\n    \"\"\"\n    Given a credential definition identifier and a tag, return the corresponding\n    revocation registry identifier, repeating the issuer DID from the\n    input identifier.\n\n    :param cd_id: credential definition identifier\n    :param tag: tag to use\n    :return: revocation registry identifier\n    \"\"\"\n\n    return '{}:4:{}:CL_ACCUM:{}'.format(cd_id.split(\":\", 1)[0], cd_id, tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ok_rev_reg_id(token: str, issuer_did: str = None) -> bool:\n\n    rr_id_m = re.match(\n        '([{0}]{{21,22}}):4:([{0}]{{21,22}}):3:CL:[1-9][0-9]*(:.+)?:CL_ACCUM:.+$'.format(B58),\n        token or '')\n    return bool(rr_id_m) and ((not issuer_did) or (rr_id_m.group(1) == issuer_did and rr_id_m.group(2) == issuer_did))", "response": "Checks if input token looks like a revocation registry identifier from input issuer DID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a revocation registry identifier return its corresponding credential definition identifier. Raise BadIdentifier if input is not a revocation registry identifier.", "response": "def rev_reg_id2cred_def_id(rr_id: str) -> str:\n    \"\"\"\n    Given a revocation registry identifier, return its corresponding credential definition identifier.\n    Raise BadIdentifier if input is not a revocation registry identifier.\n\n    :param rr_id: revocation registry identifier\n    :return: credential definition identifier\n    \"\"\"\n\n    if ok_rev_reg_id(rr_id):\n        return ':'.join(rr_id.split(':')[2:-2])  # rev reg id comprises (prefixes):<cred_def_id>:(suffixes)\n    raise BadIdentifier('Bad revocation registry identifier {}'.format(rr_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a revocation registry identifier return its corresponding credential definition identifier and tag. Raise BadIdentifier if input is not a revocation registry identifier.", "response": "def rev_reg_id2cred_def_id_tag(rr_id: str) -> (str, str):\n    \"\"\"\n    Given a revocation registry identifier, return its corresponding credential definition identifier and\n    (stringified int) tag. Raise BadIdentifier if input is not a revocation registry identifier.\n\n    :param rr_id: revocation registry identifier\n    :return: credential definition identifier and tag\n    \"\"\"\n\n    if ok_rev_reg_id(rr_id):\n        return (\n            ':'.join(rr_id.split(':')[2:-2]),  # rev reg id comprises (prefixes):<cred_def_id>:(suffixes)\n            str(rr_id.split(':')[-1])  # tag is last token\n        )\n    raise BadIdentifier('Bad revocation registry identifier {}'.format(rr_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_briefs(briefs: Union[dict, Sequence[dict]]) -> tuple:\n\n    if isinstance(briefs, dict):\n        if all(ok_wallet_reft(k) for k in briefs):\n            return tuple(briefs.values())\n        return (briefs,)\n    return tuple(briefs)", "response": "Given a cred - briefs dict or sequence of cred - briefs return tuple with all contained cred - briefs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef box_ids(briefs: Union[dict, Sequence[dict]], cred_ids: Union[Sequence[str], str] = None) -> dict:\n\n    rv = {}\n    for brief in iter_briefs(briefs):\n        cred_info = brief.get('cred_info', {}) or brief  # briefs could be cred-infos or cred-briefs\n        cred_id = cred_info['referent']\n        if ((cred_id not in rv) and (not cred_ids or cred_id in [cred_ids, [cred_ids]][isinstance(cred_ids, str)])):\n            rv[cred_id] = {\n                'schema_id': cred_info['schema_id'],\n                'cred_def_id': cred_info['cred_def_id'],\n                'rev_reg_id': cred_info['rev_reg_id']\n            }\n\n    return rv", "response": "Given one or more cred - briefs and cred - ids and optional sequence of credential identifiers return a dict mapping each credential identifier to its corresponding box ids structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all creds out of the input json structure that do not match any of the input credential identifiers.", "response": "def prune_creds_json(creds: dict, cred_ids: set) -> str:\n    \"\"\"\n    Strip all creds out of the input json structure that do not match any of the input credential identifiers.\n\n    :param creds: indy-sdk creds structure\n    :param cred_ids: the set of credential identifiers of interest\n    :return: the reduced creds json\n    \"\"\"\n\n    rv = deepcopy(creds)\n    for key in ('attrs', 'predicates'):\n        for attr_uuid, creds_by_uuid in rv[key].items():\n            rv[key][attr_uuid] = [cred for cred in creds_by_uuid if cred['cred_info']['referent'] in cred_ids]\n\n        empties = [attr_uuid for attr_uuid in rv[key] if not rv[key][attr_uuid]]\n        for attr_uuid in empties:\n            del rv[key][attr_uuid]\n\n    return json.dumps(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proof_req_infos2briefs(proof_req: dict, infos: Union[dict, Sequence[dict]]) -> list:\n\n    rv = []\n    refts = proof_req_attr_referents(proof_req)\n    for info in iter_briefs(infos):\n        if info['cred_def_id'] not in refts:\n            continue\n        brief = {\n            'cred_info': info,\n            'interval': {}\n        }\n        fro = None\n        to = None\n        for uuid in refts[info['cred_def_id']].values():\n            interval = proof_req['requested_attributes'][uuid].get('non_revoked', {})\n            if 'from' in interval:\n                fro = min(fro or interval['from'], interval['from'])\n            if 'to' in interval:\n                to = max(to or interval['to'], interval['to'])\n\n        if to:\n            brief['interval']['to'] = to\n        if fro:\n            brief['interval']['from'] = fro\n        if not brief['interval']:\n            brief['interval'] = None\n\n        rv.append(brief)\n\n    return rv", "response": "Given a proof request and corresponding cred - info s return a list of cred - briefs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef proof_req_briefs2req_creds(proof_req: dict, briefs: Union[dict, Sequence[dict]]) -> dict:\n\n    rv = {\n        'self_attested_attributes': {},\n        'requested_attributes': {},\n        'requested_predicates': {}\n    }\n\n    attr_refts = proof_req_attr_referents(proof_req)\n    pred_refts = proof_req_pred_referents(proof_req)\n    for brief in iter_briefs(briefs):\n        cred_info = brief['cred_info']\n        timestamp = (brief['interval'] or {}).get('to', None)\n        for attr in cred_info['attrs']:\n            if attr in attr_refts.get(cred_info['cred_def_id'], {}):\n                req_attr = {\n                    'cred_id': cred_info['referent'],\n                    'revealed': attr not in pred_refts.get(cred_info['cred_def_id'], {}),\n                    'timestamp': timestamp\n                }\n                if not timestamp:\n                    req_attr.pop('timestamp')\n                rv['requested_attributes'][attr_refts[cred_info['cred_def_id']][attr]] = req_attr\n            if attr in pred_refts.get(cred_info['cred_def_id'], {}):\n                for uuid in pred_refts[cred_info['cred_def_id']][attr]:\n                    req_pred = {\n                        'cred_id': cred_info['referent'],\n                        'timestamp': timestamp\n                    }\n                    if not timestamp:\n                        req_pred.pop('timestamp')\n                    rv['requested_predicates'][uuid] = req_pred\n    return rv", "response": "Given a proof request and cred - brief dict return a requested - creds structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef creds_display(creds: Union[dict, Sequence[dict]], filt: dict = None, filt_dflt_incl: bool = False) -> dict:\n\n    def _add(briefs):\n        nonlocal rv, filt\n        for brief in briefs:\n            cred_info = brief.get('cred_info', {}) or brief  # briefs could be cred-infos or cred-briefs\n            if cred_info['referent'] in rv:\n                continue\n            cred_cd_id = cred_info['cred_def_id']\n            if (not filt) or (filt_dflt_incl and cred_cd_id not in filt):\n                rv[cred_info['referent']] = cred_info\n                continue\n            if filt and cred_cd_id in filt:\n                if ({k: str(filt[cred_cd_id][k]) for k in filt[cred_cd_id]}.items() <= cred_info['attrs'].items()):\n                    rv[cred_info['referent']] = cred_info\n\n    rv = {}\n    if filt is None:\n        filt = {}\n    if isinstance(creds, dict):\n        if all(ok_wallet_reft(k) for k in creds):\n            _add(creds.values())\n        else:\n            for uuid2briefs in (creds.get('attrs', {}), creds.get('predicates', {})):\n                for briefs in uuid2briefs.values():\n                    _add(briefs)\n    else:\n        _add(creds)\n    return rv", "response": "Return a human - legible summary of the given creds."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef proof_req2wql_all(proof_req: dict, x_cd_ids: Union[str, Sequence[str]] = None) -> dict:\n\n    rv = {}\n    attr_refts = proof_req_attr_referents(proof_req)\n    for cd_id in [k for k in attr_refts if k not in ([x_cd_ids] if isinstance(x_cd_ids, str) else x_cd_ids or [])]:\n        rv[set(attr_refts[cd_id].values()).pop()] = {\"cred_def_id\": cd_id}\n\n    return rv", "response": "Given a proof request and a list of cred def ids to omit return an extra WQL query dict that will find all corresponding credentials in search."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a nested dict mapping cred def id to name and attribute referents by cred def id and attribute.", "response": "def proof_req_attr_referents(proof_req: dict) -> dict:\n    \"\"\"\n    Given a proof request with all requested attributes having cred def id restrictions,\n    return its attribute referents by cred def id and attribute.\n\n    The returned structure can be useful in populating the extra WQL query parameter\n    in the credential search API.\n\n    :param proof_req: proof request with all requested attribute specifications having cred def id restriction; e.g.,\n\n    ::\n\n        {\n            'name\": 'proof_req',\n            'version': '0.0',\n            'requested_attributes': {\n                '18_greenLevel_uuid': {\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:18:tag'\n                        }\n                    ],\n                    'name': 'greenLevel',\n                    'non_revoked': {\n                        'to': 1532367957,\n                        'from': 1532367957\n                    }\n                },\n                '18_legalName_uuid': {\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:18:tag'\n                        }\n                    ],\n                    'name': 'legalName',\n                    'non_revoked': {\n                        'to': 1532367957,\n                        'from': 1532367957\n                    }\n                },\n                '15_id_uuid': {  # this specification will not show up in response: no cred def id restriction :-(\n                    'name': 'id',\n                    'non_revoked': {\n                        'to': 1532367957,\n                        'from': 1532367957\n                    }\n                }\n            }\n            'requested_predicates': {\n            }\n        }\n\n    :return: nested dict mapping cred def id to name to proof request referent; e.g.,\n\n    ::\n\n        {\n            'WgWxqztrNooG92RXvxSTWv:3:CL:18:tag': {\n                'legalName': '18_legalName_uuid'\n                'greenLevel': '18_greenLevel_uuid'\n            }\n        }\n\n    \"\"\"\n\n    rv = {}\n    for uuid, spec in proof_req['requested_attributes'].items():\n        cd_id = None\n        for restriction in spec.get('restrictions', []):\n            cd_id = restriction.get('cred_def_id', None)\n            if cd_id:\n                break\n        if not cd_id:\n            continue\n        if cd_id not in rv:  # cd_id of None is not OK\n            rv[cd_id] = {}\n        rv[cd_id][spec['name']] = uuid\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a proof request return its predicate referents by cred def id and attribute mapping a predicate and a limit.", "response": "def proof_req_pred_referents(proof_req: dict) -> dict:\n    \"\"\"\n    Given a proof request with all requested predicates having cred def id restrictions,\n    return its predicate referents by cred def id and attribute, mapping a predicate and a limit.\n\n    The returned structure can be useful in downstream processing to filter cred-infos for predicates.\n\n    :param proof_req: proof request with all requested predicate specifications having cred def id restriction; e.g.,\n\n    ::\n\n        {\n            'name': 'proof_req',\n            'version': '0.0',\n            'requested_attributes': {\n                ...\n            }\n            'requested_predicates': {\n                '194_highscore_GE_uuid': {\n                    'name': 'highscore',\n                    'p_type': '>=',\n                    'p_value': '100000',\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:194:tag'\n                        }\n                    ],\n                    'non_revoked': {\n                        ...\n                    }\n                },\n                '194_level_GE_uuid': {\n                    'name': 'level',\n                    'p_type': '>=',\n                    'p_value': '10',\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:194:tag'\n                        }\n                    ],\n                    'non_revoked': {\n                        ...\n                    }\n                },\n                '194_attempts_LE_uuid': {\n                    'name': 'attempts',\n                    'p_type': '<=',\n                    'p_value': '3',\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:194:tag'\n                        }\n                    ],\n                    'non_revoked': {\n                        ...\n                    }\n                },\n                '198_employees_LT_uuid': {\n                    'name': 'employees',\n                    'p_type': '<',\n                    'p_value': '100',\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:198:tag'\n                        }\n                    ],\n                    'non_revoked': {\n                        ...\n                    }\n                },\n                '198_employees_GE_uuid': {\n                    'name': 'employees',\n                    'p_type': '>=',\n                    'p_value': '50',\n                    'restrictions': [\n                        {\n                            'cred_def_id': 'WgWxqztrNooG92RXvxSTWv:3:CL:198:tag'\n                        }\n                    ],\n                    'non_revoked': {\n                        ...\n                    }\n                },\n            }\n        }\n\n    :return: nested dict mapping cred def id to name to proof request referent to predicate and limit; e.g.,\n\n    ::\n\n        {\n            'WgWxqztrNooG92RXvxSTWv:3:CL:194:tag': {\n                'highscore': {\n                    '194_level_GE_uuid': ['>=', 100000]\n                },\n                'level': {\n                    '194_level_GE_uuid': ['>=', 10]\n                },\n                'attempts': {\n                    '194_attempts_LE_uuid': ['<=', 3]\n                }\n            },\n            'WgWxqztrNooG92RXvxSTWv:3:CL:198:tag': {\n                'employees': {  # may have many preds per attr, but always 1 uuid and 1 relation per pred\n                    '198_LT_employees_uuid': ['<=', 100]\n                    '198_GE_employees_uuid': ['>=', 50]\n                }\n            }\n        }\n\n    \"\"\"\n\n    rv = {}\n    for uuid, spec in proof_req['requested_predicates'].items():\n        cd_id = None\n        for restriction in spec.get('restrictions', []):\n            cd_id = restriction.get('cred_def_id', None)\n            if cd_id:\n                break\n        if not cd_id:\n            continue\n        if cd_id not in rv:  # cd_id of None is not OK\n            rv[cd_id] = {}\n        if spec['name'] not in rv[cd_id]:\n            rv[cd_id][spec['name']] = {}\n        rv[cd_id][spec['name']][uuid] = [spec['p_type'], Predicate.to_int(spec['p_value'])]\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef revoc_info(briefs: Union[dict, Sequence[dict]], filt: dict = None) -> dict:\n\n    rv = {}\n    for brief in iter_briefs(briefs):\n        cred_info = brief.get('cred_info', {}) or brief  # briefs could be cred-infos or cred-briefs\n        (rr_id, cr_id) = (cred_info['rev_reg_id'], cred_info['cred_rev_id'])\n        if (rr_id, cr_id) in rv or rr_id is None or cr_id is None:\n            continue\n        if not filt:\n            rv[(rr_id, cr_id)] = cred_info['attrs']\n            continue\n        if ({attr: str(filt[attr]) for attr in filt}.items() <= cred_info['attrs'].items()):\n            rv[(rr_id, cr_id)] = cred_info['attrs']\n\n    return rv", "response": "Given a cred - brief cred - info or sequence of either return a dict mapping keys to values of interest."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches revealed attributes from input proof and return dict mapping cred - ids to dicts each mapping attribute names to raw values for processing in further creds downstream.", "response": "def revealed_attrs(proof: dict) -> dict:\n    \"\"\"\n    Fetch revealed attributes from input proof and return dict mapping credential definition identifiers\n    to dicts, each dict mapping attribute names to (raw) values, for processing in further creds downstream.\n\n    :param proof: indy-sdk proof as dict\n    :return: dict mapping cred-ids to dicts, each mapping revealed attribute names to (raw) values\n    \"\"\"\n\n    rv = {}\n    for sub_index in range(len(proof['identifiers'])):\n        cd_id = proof['identifiers'][sub_index]['cred_def_id']\n        rv[cd_id] = ({  # uses von_anchor convention for uuid (referent) construction: will break on foreign anchor's\n            '_'.join(uuid.split('_')[1:-1]): proof['requested_proof']['revealed_attrs'][uuid]['raw']\n            for uuid in proof['requested_proof']['revealed_attrs']\n            if proof['requested_proof']['revealed_attrs'][uuid]['sub_proof_index'] == sub_index})\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef config(self, value: dict) -> None:\n\n        self._config = value or {}\n        validate_config('verifier', self._config)", "response": "Set the configuration dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _build_rr_state_json(self, rr_id: str, timestamp: int) -> (str, int):\n\n        LOGGER.debug('_Verifier._build_rr_state_json >>> rr_id: %s, timestamp: %s', rr_id, timestamp)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Verifier._build_rr_state_json <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        rr_json = None\n        ledger_timestamp = None\n\n        get_rr_req_json = await ledger.build_get_revoc_reg_request(self.did, rr_id, timestamp)\n        resp_json = await self._submit(get_rr_req_json)\n        resp = json.loads(resp_json)\n        if resp.get('result', {}).get('data', None) and resp['result']['data'].get('value', None):\n            # timestamp at or beyond rev reg creation, carry on\n            try:\n                (_, rr_json, ledger_timestamp) = await ledger.parse_get_revoc_reg_response(resp_json)\n            except IndyError:  # ledger replied, but there is no such rev reg available\n                LOGGER.debug('Verifier._build_rr_state_json <!< no rev reg exists on %s', rr_id)\n                raise AbsentRevReg('No rev reg exists on {}'.format(rr_id))\n        else:\n            LOGGER.debug(\n                '_Verifier._build_rr_state_json <!< Rev reg %s created after asked-for time %s',\n                rr_id,\n                timestamp)\n            raise BadRevStateTime('Rev reg {} created after asked-for time {}'.format(rr_id, timestamp))\n\n        rv = (rr_json, ledger_timestamp)\n        LOGGER.debug('_Verifier._build_rr_state_json <<< %s', rv)\n        return rv", "response": "Builds rev reg state json at a given requested timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds and returns indy - sdk proof request for input attributes and non - revocation intervals by cred def id.", "response": "async def build_proof_req_json(self, cd_id2spec: dict) -> str:\n        \"\"\"\n        Build and return indy-sdk proof request for input attributes and non-revocation intervals by cred def id.\n\n        :param cd_id2spec: dict mapping cred def ids to:\n\n            - (optionally) 'attrs': lists of names of attributes of interest (omit for all, empty list or None for none)\n            - (optionally) '>=': (pred) inclusive int lower-bounds of interest (omit, empty list, or None for none)\n            - (optionally) '>': (pred) exclusive int lower-bounds of interest (omit, empty list, or None for none)\n            - (optionally) '<=': (pred) inclusive int upper-bounds of interest (omit, empty list, or None for none)\n            - (optionally) '<': (pred) exclusive int upper-bounds of interest (omit, empty list, or None for none)\n            - (optionally), 'interval': either\n                - (2-tuple) pair of epoch second counts marking 'from' and 'to' timestamps, or\n                - | single epoch second count to set 'from' and 'to' the same; default\n                  | (now, now) for cred defs supporting revocation or None otherwise; e.g.,\n\n        ::\n\n            {\n                'Vx4E82R17q...:3:CL:16:tag': {\n                    'attrs': [  # request attrs 'name' and 'favouriteDrink' from this cred def's schema\n                        'name',\n                        'favouriteDrink'\n                    ],\n                    '>=': {  # request predicate score>=80 from this cred def\n                        'score': 80\n                    }\n                    '<=': {  # request ranking <=10 from this cred def\n                        'ranking': 10\n                    }\n                    'interval': 1528116008  # same instant for all attrs and preds of corresponding schema\n                },\n                'R17v42T4pk...:3:CL:19:tag': None,  # request all attrs, no preds, default intervals on all attrs\n                'e3vc5K168n...:3:CL:23:tag': {},  # request all attrs, no preds, default intervals on all attrs\n                'Z9ccax812j...:3:CL:27:tag': {  # request all attrs, no preds, this interval on all attrs\n                    'interval': (1528112408, 1528116008)\n                },\n                '9cHbp54C8n...:3:CL:37:tag': {  # request no attrs and some predicates; specify interval\n                    'attrs': [],  # or equivalently, 'attrs': None\n                    '>=': {\n                        'employees': '50'  # nicety: implementation converts to int for caller\n                    },\n                    '>=': {\n                        'revenue': '10000000'  # nicety: implementation converts to int for caller\n                        'ebidta': 0\n                    }\n                    'interval': (1528029608, 1528116008)\n                },\n                '6caBcmLi33...:3:CL:41:tag': {  # all attrs, one pred, default intervals to now on attrs & pred\n                    '>': {\n                        'regEpoch': 1514782800\n                    }\n                },\n                ...\n            }\n\n        :return: indy-sdk proof request json\n        \"\"\"\n\n        LOGGER.debug('Verifier.build_proof_req_json >>> cd_id2spec: %s', cd_id2spec)\n\n        cd_id2schema = {}\n        now = int(time())\n        rv = {\n            'nonce': str(int(time())),\n            'name': 'proof_req',\n            'version': '0.0',\n            'requested_attributes': {},\n            'requested_predicates': {}\n        }\n\n        for cd_id in cd_id2spec:\n            if not ok_cred_def_id(cd_id):\n                LOGGER.debug('Verifier.build_proof_req_json <!< Bad cred def id %s', cd_id)\n                raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n            interval = None\n            cred_def = json.loads(await self.get_cred_def(cd_id))\n            seq_no = cred_def_id2seq_no(cd_id)\n            cd_id2schema[cd_id] = json.loads(await self.get_schema(seq_no))\n\n            if 'revocation' in cred_def['value']:\n                fro_to = cd_id2spec[cd_id].get('interval', (now, now)) if cd_id2spec[cd_id] else (now, now)\n                interval = {\n                    'from': fro_to if isinstance(fro_to, int) else min(fro_to),\n                    'to': fro_to if isinstance(fro_to, int) else max(fro_to)\n                }\n\n            for attr in (cd_id2spec[cd_id].get('attrs', cd_id2schema[cd_id]['attrNames']) or []\n                    if cd_id2spec[cd_id] else cd_id2schema[cd_id]['attrNames']):\n                attr_uuid = '{}_{}_uuid'.format(seq_no, canon(attr))\n                rv['requested_attributes'][attr_uuid] = {\n                    'name': attr,\n                    'restrictions': [{\n                        'cred_def_id': cd_id\n                    }]\n                }\n                if interval:\n                    rv['requested_attributes'][attr_uuid]['non_revoked'] = interval\n\n            for pred in Predicate:\n                for attr in (cd_id2spec[cd_id].get(pred.value.math, {}) or {} if cd_id2spec[cd_id] else {}):\n                    pred_uuid = '{}_{}_{}_uuid'.format(seq_no, canon(attr), pred.value.fortran)\n                    try:\n                        rv['requested_predicates'][pred_uuid] = {\n                            'name': attr,\n                            'p_type': pred.value.math,\n                            'p_value': Predicate.to_int(cd_id2spec[cd_id][pred.value.math][attr]),\n                            'restrictions': [{\n                                'cred_def_id': cd_id\n                            }]\n                        }\n                    except ValueError:\n                        LOGGER.info(\n                            'cannot build %s predicate on non-int bound %s for %s',\n                            pred.value.fortran,\n                            cd_id2spec[cd_id][pred.value.math][attr],\n                            attr)\n                        continue  # int conversion failed - reject candidate\n                    if interval:\n                        rv['requested_predicates'][pred_uuid]['non_revoked'] = interval\n\n        LOGGER.debug('Verifier.build_proof_req_json <<< %s', json.dumps(rv))\n        return json.dumps(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def load_cache_for_verification(self, archive: bool = False) -> int:\n\n        LOGGER.debug('Verifier.load_cache_for_verification >>> archive: %s', archive)\n\n        rv = int(time())\n        for s_id in self.config.get('archive-verifier-caches-on-close', {}).get('schema_id', {}):\n            if ok_schema_id(s_id):\n                with SCHEMA_CACHE.lock:\n                    await self.get_schema(s_id)\n            else:\n                LOGGER.info('Not archiving schema for specified bad id %s', s_id)\n        for cd_id in self.config.get('archive-verifier-caches-on-close', {}).get('cred_def_id', {}):\n            if ok_cred_def_id(cd_id):\n                with CRED_DEF_CACHE.lock:\n                    await self.get_cred_def(cd_id)\n            else:\n                LOGGER.info('Not archiving cred def for specified bad id %s', cd_id)\n        for rr_id in self.config.get('archive-verifier-caches-on-close', {}).get('rev_reg_id', {}):\n            if ok_rev_reg_id(rr_id):\n                await self.get_rev_reg_def(rr_id)\n                with REVO_CACHE.lock:\n                    revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                    if revo_cache_entry:\n                        try:\n                            await revo_cache_entry.get_state_json(self._build_rr_state_json, rv, rv)\n                        except ClosedPool:\n                            LOGGER.warning(\n                                'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s',\n                                self.name,\n                                self.pool.name,\n                                rr_id,\n                                rv)\n                        except AbsentPool:\n                            LOGGER.warning(\n                                'Verifier %s has no pool, cannot update revo cache reg state for %s to %s',\n                                self.name,\n                                rr_id,\n                                rv)\n            else:\n                LOGGER.info('Not archiving rev reg for specified bad id %s', rr_id)\n\n        if archive:\n            ArchivableCaches.archive(self.dir_cache)\n        LOGGER.debug('Verifier.load_cache_for_verification <<< %s', rv)\n        return rv", "response": "Load schema cred def revocation cache and revocation registry for verification."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening current object and parse cache from archive if parse - caches - on - open is set to True.", "response": "async def open(self) -> 'Verifier':\n        \"\"\"\n        Explicit entry. Perform ancestor opening operations,\n        then parse cache from archive if so configured, and\n        synchronize revocation registry to tails tree content.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Verifier.open >>>')\n\n        await super().open()\n        if self.config.get('parse-caches-on-open', False):\n            ArchivableCaches.parse(self.dir_cache)\n\n        LOGGER.debug('Verifier.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the current object.", "response": "async def close(self) -> None:\n        \"\"\"\n        Explicit exit. If so configured, populate cache to prove for any creds on schemata,\n        cred defs, and rev regs marked of interest in configuration at initialization,\n        archive cache, and purge prior cache archives.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Verifier.close >>>')\n\n        if self.config.get('archive-verifier-caches-on-close', {}):\n            await self.load_cache_for_verification(True)\n            ArchivableCaches.purge_archives(self.dir_cache, True)\n\n        await BaseAnchor.close(self)\n\n        LOGGER.debug('Verifier.close <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_encoding(proof_req: dict, proof: dict) -> bool:\n\n        LOGGER.debug('Verifier.check_encoding <<< proof_req: %s, proof: %s', proof_req, proof)\n\n        cd_id2proof_id = {}  # invert proof['identifiers'] per cd_id\n        p_preds = {}  # cd_id and attr to bound\n        for idx in range(len(proof['identifiers'])):\n            cd_id = proof['identifiers'][idx]['cred_def_id']\n            cd_id2proof_id[cd_id] = idx  # since at most 1 cred per cred def\n            p_preds[cd_id] = {\n                ge_proof['predicate']['attr_name']: ge_proof['predicate']['value']\n                for ge_proof in proof['proof']['proofs'][idx]['primary_proof']['ge_proofs']\n            }\n\n        for (uuid, req_attr) in proof_req['requested_attributes'].items():  # proof req xref proof per revealed attr\n            canon_attr = canon(req_attr['name'])\n            proof_ident_idx = cd_id2proof_id[req_attr['restrictions'][0]['cred_def_id']]\n            enco = proof['proof']['proofs'][proof_ident_idx]['primary_proof']['eq_proof']['revealed_attrs'].get(\n                canon_attr)\n            if not enco:\n                continue  # requested but declined from revelation in proof: must appear in a predicate\n            if enco != proof['requested_proof']['revealed_attrs'][uuid]['encoded']:\n                LOGGER.debug('Verifier.check_proof_encoding <<< False')\n                return False\n            if enco != encode(proof['requested_proof']['revealed_attrs'][uuid]['raw']):\n                LOGGER.debug('Verifier.check_proof_encoding <<< False')\n                return False\n\n        for (uuid, req_pred) in proof_req['requested_predicates'].items():  # proof req xref proof per pred\n            canon_attr = canon(req_pred['name'])\n            if p_preds[req_pred['restrictions'][0]['cred_def_id']].get(canon_attr) != req_pred['p_value']:\n                LOGGER.debug('Verifier.check_proof_encoding <<< False')\n                return False\n\n        LOGGER.debug('Verifier.check_proof_encoding <<< True')\n        return True", "response": "Check if the raw values of the encodings of the creddef entries in the proof are cross - referenced against their encodings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(value: str) -> 'Protocol':\n\n        for pktype in PublicKeyType:\n            if value in (pktype.ver_type, pktype.authn_type):\n                return pktype\n        return None", "response": "Returns the protocol corresponding to input version value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn dict representation of public key.", "response": "def to_dict(self):\n        \"\"\"\n        Return dict representation of public key to embed in DID document.\n        \"\"\"\n\n        return {\n            'id': self.id,\n            'type': str(self.type.ver_type),\n            'controller': canon_ref(self.did, self.controller),\n            **self.type.specification(self.value)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def main(wallet_name: str) -> None:\n\n    logging.basicConfig(level=logging.WARN, format='%(levelname)-8s | %(name)-12s | %(message)s')\n    logging.getLogger('indy').setLevel(logging.ERROR)\n\n    path_start = join(RevRegBuilder.dir_tails_sentinel(wallet_name), '.start')\n\n    with open(path_start, 'r') as fh_start:\n        start_data = json.loads(fh_start.read())\n    remove(path_start)\n\n    logging.getLogger(__name__).setLevel(start_data['logging']['level'])\n    for path_log in start_data['logging']['paths']:\n        logging.getLogger(__name__).addHandler(logging.FileHandler(path_log))\n\n    wallet = WalletManager().get(\n        {\n            'id': wallet_name,\n            'storage_type': start_data['wallet']['storage_type'],\n            **start_data['wallet']['config'],\n        },\n        access=start_data['wallet']['access_creds'].get('key', None))\n    async with wallet, RevRegBuilder(wallet, rrbx=True) as rrban:\n        await rrban.serve()", "response": "Main function for revocation registry builder operating in external process on behalf of issuer agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _start_data_json(self) -> str:\n\n        rv = {\n            'logging': {\n                'paths': []\n            },\n            'wallet': {\n            }\n        }\n\n        logger = LOGGER\n        while not logger.level:\n            logger = logger.parent\n            if logger is None:\n                break\n        rv['logging']['level'] = logger.level\n\n        logger = LOGGER\n        log_paths = [realpath(h.baseFilename) for h in logger.handlers if hasattr(h, 'baseFilename')]\n        while not log_paths:\n            logger = logger.parent\n            if logger is None:\n                break\n            log_paths = [realpath(h.baseFilename) for h in logger.handlers if hasattr(h, 'baseFilename')]\n        for log_path in log_paths:\n            rv['logging']['paths'].append(log_path)\n\n        rv['wallet']['storage_type'] = self.wallet.storage_type\n        rv['wallet']['config'] = self.wallet.config\n        rv['wallet']['access_creds'] = self.wallet.access_creds\n\n        return json.dumps(rv)", "response": "Output json with start data to write for external revocation registry builder process pickup."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_state(wallet_name: str) -> _STATE:\n\n        dir_sentinel = RevRegBuilder.dir_tails_sentinel(wallet_name)\n        file_pid = join(dir_sentinel, '.pid')\n        file_start = join(dir_sentinel, '.start')\n        file_stop = join(dir_sentinel, '.stop')\n\n        if isfile(file_stop):\n            return _STATE.STOPPING\n\n        if isfile(file_start) or isfile(file_pid):\n            return _STATE.RUNNING\n\n        return _STATE.ABSENT", "response": "Return current state of revocation registry builder process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning top of tails tree for input revocation registry identifier.", "response": "def dir_tails_top(self, rr_id) -> str:\n        \"\"\"\n        Return top of tails tree for input rev reg id.\n\n        :param rr_id: revocation registry identifier\n        :return: top of tails tree\n        \"\"\"\n\n        return join(self.dir_tails_hopper, rr_id) if self.external else self.dir_tails"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dir_tails_target(self, rr_id) -> str:\n\n        return join(self.dir_tails_top(rr_id), rev_reg_id2cred_def_id(rr_id))", "response": "Return target directory for revocation registry and tails file production."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mark_in_progress(self, rr_id: str, rr_size: int) -> None:\n        try:\n            makedirs(join(self._dir_tails_sentinel, rr_id), exist_ok=False)\n        except FileExistsError:\n            LOGGER.warning('Rev reg %s construction already in progress', rr_id)\n        else:\n            open(join(self._dir_tails_sentinel, rr_id, '.{}'.format(rr_size)), 'w').close()", "response": "Mark the revocation registry as in progress."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserving the tails file.", "response": "async def serve(self) -> None:\n        \"\"\"\n        Write pidfile to sentinel directory if need be, and wait for sentinels\n        to shut down or build revocation registry and tails file.\n        \"\"\"\n\n        LOGGER.debug('RevRegBuilder.serve >>>')\n\n        assert self.external\n\n        file_pid = join(self._dir_tails_sentinel, '.pid')\n        if isfile(file_pid):\n            with open(file_pid, 'r') as fh_pid:\n                pid = int(fh_pid.read())\n            try:\n                kill(pid, 0)\n            except ProcessLookupError:\n                remove(file_pid)\n                LOGGER.info('RevRegBuilder removed derelict .pid file')\n            except PermissionError:\n                LOGGER.info('RevRegBuilder process already running with pid %s: exiting', pid)\n                LOGGER.debug('RevRegBuilder.serve <<<')\n                return\n            else:\n                LOGGER.info('RevRegBuilder process already running with pid %s: exiting', pid)\n                LOGGER.debug('RevRegBuilder.serve <<<')\n                return\n\n        pid = getpid()\n        with open(file_pid, 'w') as pid_fh:\n            print(str(pid), file=pid_fh)\n\n        file_stop = join(self._dir_tails_sentinel, '.stop')\n\n        while True:\n            if isfile(file_stop):  # stop now, pick up any pending tasks next invocation\n                remove(file_stop)\n                remove(file_pid)\n                break\n\n            p_pending = [join(self._dir_tails_sentinel, d) for d in listdir(self._dir_tails_sentinel)\n                if isdir(join(self._dir_tails_sentinel, d))]\n            p_pending = [p for p in p_pending if [s for s in listdir(p) if s.startswith('.')]]  # size marker\n            if p_pending:\n                pdir = basename(p_pending[0])\n                rr_id = pdir\n                rr_size = int([s for s in listdir(p_pending[0]) if s.startswith('.')][0][1:])\n                open(join(p_pending[0], '.in-progress'), 'w').close()\n                await self.create_rev_reg(rr_id, rr_size or None)\n                rmtree(p_pending[0])\n            await asyncio.sleep(1)\n\n        LOGGER.debug('RevRegBuilder.serve <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstop an external revocation registry builder.", "response": "async def stop(wallet_name: str) -> None:\n        \"\"\"\n        Gracefully stop an external revocation registry builder, waiting for its current.\n\n        The indy-sdk toolkit uses a temporary directory for tails file mustration,\n        and shutting down the toolkit removes the directory, crashing the external\n        tails file write. This method allows a graceful stop to wait for completion\n        of such tasks already in progress.\n\n        :wallet_name: name external revocation registry builder to check\n        :return: whether a task is pending.\n        \"\"\"\n\n        LOGGER.debug('RevRegBuilder.stop >>>')\n\n        dir_sentinel = join(RevRegBuilder.dir_tails_sentinel(wallet_name))\n\n        if isdir(dir_sentinel):\n            open(join(dir_sentinel, '.stop'), 'w').close()  # touch\n\n            while any(isfile(join(dir_sentinel, d, '.in-progress')) for d in listdir(dir_sentinel)):\n                await asyncio.sleep(1)\n\n        LOGGER.debug('RevRegBuilder.stop <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates revocation registry artifacts and new tails file.", "response": "async def create_rev_reg(self, rr_id: str, rr_size: int = None) -> None:\n        \"\"\"\n        Create revocation registry artifacts and new tails file (with association to\n        corresponding revocation registry identifier via symbolic link name)\n        for input revocation registry identifier. Symbolic link presence signals completion.\n        If revocation registry builder operates in a process external to its Issuer's,\n        target directory is hopper directory.\n\n        Raise WalletState for closed wallet.\n\n        :param rr_id: revocation registry identifier\n        :param rr_size: revocation registry size (defaults to 64)\n        \"\"\"\n\n        LOGGER.debug('RevRegBuilder.create_rev_reg >>> rr_id: %s, rr_size: %s', rr_id, rr_size)\n\n        if not self.wallet.handle:\n            LOGGER.debug('RevRegBuilder.create_rev_reg <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('RevRegBuilder.create_rev_reg <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        rr_size = rr_size or 64\n\n        (cd_id, tag) = rev_reg_id2cred_def_id_tag(rr_id)\n\n        dir_tails = self.dir_tails_top(rr_id)\n        dir_target = self.dir_tails_target(rr_id)\n        if self.external:\n            try:\n                makedirs(dir_target, exist_ok=False)\n            except FileExistsError:\n                LOGGER.warning(\n                    'RevRegBuilder.create_rev_reg found dir %s, but task not in progress: rebuilding rev reg %s',\n                    dir_target,\n                    rr_id)\n                rmtree(dir_target)\n                makedirs(dir_target, exist_ok=False)\n\n        LOGGER.info('Creating revocation registry (capacity %s) for rev reg id %s', rr_size, rr_id)\n        tails_writer_handle = await blob_storage.open_writer(\n            'default',\n            json.dumps({\n                'base_dir': dir_target,\n                'uri_pattern': ''\n            }))\n\n        (created_rr_id, rr_def_json, rr_ent_json) = await anoncreds.issuer_create_and_store_revoc_reg(\n            self.wallet.handle,\n            self.did,\n            'CL_ACCUM',\n            tag,\n            cd_id,\n            json.dumps({\n                'max_cred_num': rr_size,\n                'issuance_type': 'ISSUANCE_BY_DEFAULT'\n            }),\n            tails_writer_handle)\n\n        tails_hash = basename(Tails.unlinked(dir_target).pop())\n        with open(join(dir_target, 'rr_def.json'), 'w') as rr_def_fh:\n            print(rr_def_json, file=rr_def_fh)\n        with open(join(dir_target, 'rr_ent.json'), 'w') as rr_ent_fh:\n            print(rr_ent_json, file=rr_ent_fh)\n        Tails.associate(dir_tails, created_rr_id, tails_hash)  # associate last: symlink signals completion\n\n        LOGGER.debug('RevRegBuilder.create_rev_reg <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbases on Django s default cache template tag", "response": "def do_ultracache(parser, token):\n    \"\"\"Based on Django's default cache template tag\"\"\"\n    nodelist = parser.parse((\"endultracache\",))\n    parser.delete_first_token()\n    tokens = token.split_contents()\n    if len(tokens) < 3:\n        raise TemplateSyntaxError(\"\"%r\" tag requires at least 2 arguments.\" % tokens[0])\n    return UltraCacheNode(nodelist,\n        parser.compile_filter(tokens[1]),\n        tokens[2], # fragment_name can\"t be a variable.\n        [parser.compile_filter(token) for token in tokens[3:]])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ppjson(dumpit: Any, elide_to: int = None) -> str:\n\n    if elide_to is not None:\n        elide_to = max(elide_to, 3) # make room for ellipses '...'\n    try:\n        rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else dumpit, indent=4)\n    except TypeError:\n        rv = '{}'.format(pformat(dumpit, indent=4, width=120))\n    return rv if elide_to is None or len(rv) <= elide_to else '{}...'.format(rv[0 : elide_to - 3])", "response": "Pretty - print a node s hierarchy in json format."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a synchronous operation ; await then return the result.", "response": "def do_wait(coro: Callable) -> Any:\n    \"\"\"\n    Perform aynchronous operation; await then return the result.\n\n    :param coro: coroutine to await\n    :return: coroutine result\n    \"\"\"\n\n    event_loop = None\n    try:\n        event_loop = asyncio.get_event_loop()\n    except RuntimeError:\n        event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(event_loop)\n    return event_loop.run_until_complete(coro)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake one or more ini files and return a dict with configuration from all. ini files.", "response": "def inis2dict(ini_paths: Union[str, Sequence[str]]) -> dict:\n    \"\"\"\n    Take one or more ini files and return a dict with configuration from all,\n    interpolating bash-style variables ${VAR} or ${VAR:-DEFAULT}.\n\n    :param ini_paths: path or paths to .ini files\n    \"\"\"\n\n    var_dflt = r'\\${(.*?):-(.*?)}'\n    def _interpolate(content):\n        rv = expandvars(content)\n        while True:\n            match = re.search(var_dflt, rv)\n            if match is None:\n                break\n            bash_var = '${{{}}}'.format(match.group(1))\n            value = expandvars(bash_var)\n            rv = re.sub(var_dflt, match.group(2) if value == bash_var else value, rv, count=1)\n\n        return rv\n\n    parser = ConfigParser()\n\n    for ini in [ini_paths] if isinstance(ini_paths, str) else ini_paths:\n        if not isfile(ini):\n            raise FileNotFoundError('No such file: {}'.format(ini))\n        with open(ini, 'r') as ini_fh:\n            ini_text = _interpolate(ini_fh.read())\n            parser.read_string(ini_text)\n\n    return {s: dict(parser[s].items()) for s in parser.sections()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mark(self, digits: int = None) -> float:\n\n        self._mark[:] = [self._mark[1], time()]\n        rv = self._mark[1] - self._mark[0]\n\n        if digits is not None and digits > 0:\n            rv = round(rv, digits)\n        elif digits == 0 or self._digits == 0:\n            rv = int(rv)\n        elif self._digits is not None and self._digits > 0:\n            rv = round(rv, self._digits)\n\n        return rv", "response": "Return time in seconds since last mark reset or construction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbegin the search operation.", "response": "async def open(self) -> None:\n        \"\"\"\n        Begin the search operation.\n        \"\"\"\n\n        LOGGER.debug('StorageRecordSearch.open >>>')\n\n        if self.opened:\n            LOGGER.debug('StorageRecordSearch.open <!< Search is already opened')\n            raise BadSearch('Search is already opened')\n\n        if not self._wallet.opened:\n            LOGGER.debug('StorageRecordSearch.open <!< Wallet %s is closed', self._wallet.name)\n            raise WalletState('Wallet {} is closed'.format(self._wallet.name))\n\n        self._handle = await non_secrets.open_wallet_search(\n            self._wallet.handle,\n            self._type,\n            self._query_json,\n            StorageRecordSearch.OPTIONS_JSON)\n\n        LOGGER.debug('StorageRecordSearch.open <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning archivable cache identifier for this pool.", "response": "def cache_id(self) -> str:\n        \"\"\"\n        Return identifier for archivable caches, computing it first and retaining it if need be.\n        Raise AbsentPool if ledger configuration is not yet available.\n\n        :param name: pool name\n        :return: archivable cache identifier\n        \"\"\"\n\n        if self._cache_id:\n            return self._cache_id\n\n        with open(join(expanduser('~'), '.indy_client', 'pool', self.name, '{}.txn'.format(self.name))) as fh_genesis:\n            genesis = [json.loads(line) for line in fh_genesis.readlines() if line]\n\n        hps = []\n        for gen_txn in genesis:\n            hps.append(self.protocol.genesis_host_port(gen_txn))\n        hps.sort()  # canonicalize to make order irrelevant\n        self._cache_id = ':'.join('{}:{}'.format(hp[0], hp[1]) for hp in hps)\n\n        return self._cache_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen pool with given configuration.", "response": "async def open(self) -> 'NodePool':\n        \"\"\"\n        Explicit entry. Opens pool as configured, for later closure via close().\n        Creates pool if it does not yet exist, using configured genesis transaction file.\n        For use when keeping pool open across multiple calls.\n\n        Raise any AbsentPool if node pool ledger configuration is not available.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('NodePool.open >>>')\n\n        await pool.set_protocol_version(self.protocol.indy())\n        LOGGER.info('Pool ledger %s set protocol %s', self.name, self.protocol)\n\n        try:\n            self._handle = await pool.open_pool_ledger(self.name, json.dumps(self.config))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.PoolLedgerNotCreatedError:\n                LOGGER.debug('NodePool.open <!< Absent node pool %s ledger configuration', self.name)\n                raise AbsentPool('Absent node pool {} ledger configuration'.format(self.name))\n            LOGGER.debug(\n                'NodePool.open <!< cannot open node pool %s: indy error code %s',\n                self.name,\n                x_indy.error_code)\n            raise\n\n        LOGGER.debug('NodePool.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def close(self) -> None:\n\n        LOGGER.debug('NodePool.close >>>')\n\n        if not self.handle:\n            LOGGER.warning('Abstaining from closing pool %s: already closed', self.name)\n        else:\n            await pool.close_pool_ledger(self.handle)\n        self._handle = None\n\n        LOGGER.debug('NodePool.close <<<')", "response": "Explicit exit. Closes pool. For use when keeping pool open across multiple calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes local copy of pool ledger and update node pool connections.", "response": "async def refresh(self) -> None:\n        \"\"\"\n        Refresh local copy of pool ledger and update node pool connections.\n        \"\"\"\n\n        LOGGER.debug('NodePool.refresh >>>')\n\n        await pool.refresh_pool_ledger(self.handle)\n\n        LOGGER.debug('NodePool.refresh <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def reseed(self, seed: str = None) -> None:\n\n        LOGGER.debug('BaseAnchor.reseed >>> seed: [SEED]')\n\n        verkey = await self.wallet.reseed_init(seed)\n        req_json = await ledger.build_nym_request(\n            self.did,\n            self.did,\n            verkey,\n            self.name,\n            (await self.get_nym_role()).token())\n        await self._sign_submit(req_json)\n        await self.wallet.reseed_apply()\n\n        LOGGER.debug('BaseAnchor.reseed <<<')", "response": "Reseed the current key pair."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget json cryptonym for input ( anchor ) DID.", "response": "async def get_nym(self, target_did: str = None) -> str:\n        \"\"\"\n        Get json cryptonym (including current verification key) for input (anchor) DID from ledger.\n        Return empty production {} if the ledger has no such cryptonym.\n\n        Raise BadLedgerTxn on failure. Raise WalletState if target DID is default (own DID) value but\n        wallet does not have it (neither created nor opened since initialization).\n\n        :param target_did: DID of cryptonym to fetch (default own DID)\n        :return: cryptonym json\n        \"\"\"\n\n        LOGGER.debug('BaseAnchor.get_nym >>> target_did: %s', target_did)\n\n        if target_did and not ok_did(target_did):\n            LOGGER.debug('BaseAnchor.get_nym <!< Bad DID %s', target_did)\n            raise BadIdentifier('Bad DID {}'.format(target_did))\n\n        if not (target_did or self.did):\n            LOGGER.debug('BaseAnchor.get_nym <!< Bad wallet state: DID for %s unavailable', self.name)\n            raise WalletState('Bad wallet state: DID for {} unavailable'.format(self.name))\n\n        rv = json.dumps({})\n        get_nym_req = await ledger.build_get_nym_request(self.did, target_did or self.did)\n        resp_json = await self._submit(get_nym_req)\n\n        data_json = (json.loads(resp_json))['result']['data']  # it's double-encoded on the ledger\n        if data_json:\n            rv = data_json\n\n        LOGGER.debug('BaseAnchor.get_nym <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_nym_role(self, target_did: str = None) -> Role:\n\n        LOGGER.debug('BaseAnchor.get_nym_role >>> target_did: %s', target_did)\n\n        nym = json.loads(await self.get_nym(target_did))\n        if not nym:\n            LOGGER.debug('BaseAnchor.get_nym_role <!< Ledger has no cryptonym for anchor %s', self.name)\n            raise AbsentNym('Ledger has no cryptonym for anchor {}'.format(self.name))\n\n        rv = Role.get(nym['role'])\n\n        LOGGER.debug('BaseAnchor.get_nym_role <<< %s', rv)\n        return rv", "response": "Get the cryptonym role for the input did."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef least_role() -> Role:\n\n        LOGGER.debug('BaseAnchor.least_role >>>')\n\n        rv = Role.TRUST_ANCHOR\n\n        LOGGER.debug('BaseAnchor.least_role <<< %s', rv)\n        return rv", "response": "Return the indy - sdk role of the least privilege for an anchor class in building\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def set_did_endpoint(self, remote_did: str, did_endpoint: str) ->  EndpointInfo:\n\n        LOGGER.debug('BaseAnchor.set_did_endpoint >>> remote_did: %s, did_endpoint: %s', remote_did, did_endpoint)\n\n        if not ok_did(remote_did):\n            LOGGER.debug('BaseAnchor.set_did_endpoint <!< Bad DID %s', remote_did)\n            raise BadIdentifier('Bad DID {}'.format(remote_did))\n\n        pairwise_info = (await self.wallet.get_pairwise(remote_did)).get(remote_did, None)\n        if not pairwise_info:\n            LOGGER.debug(\n                'BaseAnchor.set_did_endpoint <!< Anchor %s has no pairwise relation for remote DID %s',\n                self.name,\n                remote_did)\n            raise AbsentRecord('Anchor {} has no pairwise relation for remote DID {}'.format(\n                self.name,\n                remote_did))\n\n        await self.wallet.write_pairwise(\n            pairwise_info.their_did,\n            pairwise_info.their_verkey,\n            pairwise_info.my_did,\n            {'did_endpoint': did_endpoint})\n\n        rv = EndpointInfo(did_endpoint, pairwise_info.their_verkey)\n\n        LOGGER.debug('BaseAnchor.set_did_endpoint <<< %s', rv)\n        return rv", "response": "Set endpoint for remote DID in wallet and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def send_endpoint(self, endpoint: str) -> None:\n\n        LOGGER.debug('BaseAnchor.send_endpoint >>> endpoint: %s', endpoint)\n\n        ledger_endpoint = await self.get_endpoint()\n        if ledger_endpoint == endpoint:\n            LOGGER.info('%s endpoint already set as %s', self.name, endpoint)\n            LOGGER.debug('BaseAnchor.send_endpoint <<< (%s already set for %s )')\n            return\n\n        attr_json = json.dumps({\n            'endpoint': {\n                'endpoint': endpoint  # indy-sdk likes 'ha' here but won't map 'ha' to a URL, only ip:port\n            }\n        })\n        req_json = await ledger.build_attrib_request(self.did, self.did, None, attr_json, None)\n        await self._sign_submit(req_json)\n\n        for _ in range(16):  # reasonable timeout\n            if await self.get_endpoint(None, False) == endpoint:\n                break\n            await asyncio.sleep(1)\n            LOGGER.info('Sent endpoint %s to ledger, waiting 1s for its confirmation', endpoint)\n        else:\n            LOGGER.debug('BaseAnchor.send_endpoint <!< timed out waiting on send endpoint %s', endpoint)\n            raise BadLedgerTxn('Timed out waiting on sent endpoint {}'.format(endpoint))\n\n        LOGGER.debug('BaseAnchor.send_endpoint <<<')", "response": "Send endpoint to ledger and return local cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def get_endpoint(self, target_did: str = None, from_cache: bool = True) -> str:\n\n        LOGGER.debug('BaseAnchor.get_endpoint >>> target_did: %s, from_cache: %s', target_did, from_cache)\n\n        rv = None\n\n        if not (target_did or self.did):\n            LOGGER.debug('BaseAnchor.get_endpoint <!< Bad wallet state: DID for %s unavailable', self.name)\n            raise WalletState('Bad wallet state: DID for {} unavailable'.format(self.name))\n\n        target_did = target_did or self.did\n        if not ok_did(target_did):\n            LOGGER.debug('BaseAnchor.get_endpoint <!< Bad DID %s', target_did)\n            raise BadIdentifier('Bad DID {}'.format(target_did))\n\n        if from_cache:\n            with ENDPOINT_CACHE.lock:\n                if target_did in ENDPOINT_CACHE:\n                    LOGGER.info('BaseAnchor.get_endpoint: got endpoint for %s from cache', target_did)\n                    rv = ENDPOINT_CACHE[target_did]\n                    LOGGER.debug('BaseAnchor.get_endpoint <<< %s', rv)\n                    return rv\n\n        req_json = await ledger.build_get_attrib_request(\n            self.did,\n            target_did,\n            'endpoint',\n            None,\n            None)\n        resp_json = await self._submit(req_json)\n\n        data_json = (json.loads(resp_json))['result']['data']  # it's double-encoded on the ledger\n        if data_json:\n            rv = json.loads(data_json)['endpoint'].get('endpoint', None)\n        else:\n            LOGGER.info('_AgentCore.get_endpoint: ledger query returned response with no data')\n\n        with ENDPOINT_CACHE.lock:\n            if rv:\n                ENDPOINT_CACHE[target_did] = rv\n            else:\n                ENDPOINT_CACHE.pop(target_did, None)\n                assert target_did not in ENDPOINT_CACHE\n\n        LOGGER.debug('BaseAnchor.get_endpoint <<< %s', rv)\n        return rv", "response": "Get endpoint attribute for target DID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a verification key for a target DID.", "response": "async def _verkey_for(self, target: str) -> str:\n        \"\"\"\n        Given a DID, retrieve its verification key, looking in wallet, then pool.\n        Given a verification key or None, return input.\n\n        Raise WalletState if the wallet is closed. Given a recipient DID not in the wallet,\n        raise AbsentPool if the instance has no pool or ClosedPool if its pool is closed.\n        If no such verification key is on the ledger, raise AbsentNym.\n\n        :param target: verification key, or DID to resolve to such\n        :return: verification key\n        \"\"\"\n\n        LOGGER.debug('BaseAnchor._verkey_for >>> target: %s', target)\n\n        rv = target\n        if rv is None or not ok_did(rv):  # it's None or already a verification key\n            LOGGER.debug('BaseAnchor._verkey_for <<< %s', rv)\n            return rv\n\n        if self.wallet.handle:\n            try:\n                rv = await did.key_for_local_did(self.wallet.handle, target)\n                LOGGER.info('Anchor %s got verkey for DID %s from wallet', self.name, target)\n                LOGGER.debug('BaseAnchor._verkey_for <<< %s', rv)\n                return rv\n            except IndyError as x_indy:\n                if x_indy.error_code != ErrorCode.WalletItemNotFound:  # on not found, try the pool\n                    LOGGER.debug(\n                        'BaseAnchor._verkey_for <!< key lookup for local DID %s raised indy error code %s',\n                        target,\n                        x_indy.error_code)\n                    raise\n\n        nym = json.loads(await self.get_nym(target))\n        if not nym:\n            LOGGER.debug(\n                'BaseAnchor._verkey_for <!< Wallet %s closed and ledger has no cryptonym for DID %s',\n                self.name,\n                target)\n            raise AbsentNym('Wallet {} closed, and ledger has no cryptonym for DID {}'.format(self.name, target))\n\n        rv = json.loads(await self.get_nym(target))['verkey']\n        LOGGER.info('Anchor %s got verkey for DID %s from pool %s', self.name, target, self.pool.name)\n\n        LOGGER.debug('BaseAnchor._verkey_for <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencrypt plaintext for owner of DID or verification key.", "response": "async def encrypt(self, message: bytes, authn: bool = False, recip: str = None) -> bytes:\n        \"\"\"\n        Encrypt plaintext for owner of DID or verification key, anonymously or via\n        authenticated encryption scheme. If given DID, first check wallet and then pool\n        for corresponding verification key.\n\n        Raise WalletState if the wallet is closed. Given a recipient DID not in the wallet,\n        raise AbsentPool if the instance has no pool or ClosedPool if its pool is closed.\n\n        :param message: plaintext, as bytes\n        :param authn: whether to use authenticated encryption scheme\n        :param recip: DID or verification key of recipient, None for anchor's own\n        :return: ciphertext, as bytes\n        \"\"\"\n\n        LOGGER.debug('BaseAnchor.encrypt >>> message: %s, authn: %s, recip: %s', message, authn, recip)\n\n        if not self.wallet.handle:\n            LOGGER.debug('BaseAnchor.encrypt <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rv = await self.wallet.encrypt(message, authn, await self._verkey_for(recip))\n\n        LOGGER.debug('BaseAnchor.auth_encrypt <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def verify(self, message: bytes, signature: bytes, signer: str = None) -> bool:\n\n        LOGGER.debug('BaseAnchor.verify >>> signer: %s, message: %s, signature: %s', signer, message, signature)\n\n        if not self.wallet.handle:\n            LOGGER.debug('BaseAnchor.verify <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        verkey = None\n        if signer:\n            verkey = await self._verkey_for(signer)\n        rv = await self.wallet.verify(message, signature, verkey)\n\n        LOGGER.debug('BaseAnchor.verify <<< %s', rv)\n        return rv", "response": "Verify signature with input signer verification key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a transaction on the distributed ledger by its sequence number.", "response": "async def get_txn(self, seq_no: int) -> str:\n        \"\"\"\n        Find a transaction on the distributed ledger by its sequence number.\n\n        :param seq_no: transaction number\n        :return: json sequence number of transaction, null for no match\n        \"\"\"\n\n        LOGGER.debug('BaseAnchor.get_txn >>> seq_no: %s', seq_no)\n\n        rv_json = json.dumps({})\n        req_json = await ledger.build_get_txn_request(self.did, None, seq_no)\n        resp = json.loads(await self._submit(req_json))\n\n        rv_json = self.pool.protocol.txn2data(resp)\n\n        LOGGER.debug('BaseAnchor.get_txn <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n\n        rv = {\n            'id': self.id,\n            'type': self.type,\n            'priority': self.priority\n        }\n        if self.recip_keys:\n            rv['routingKeys'] = [canon_ref(k.did, k.id, '#') for k in  self.recip_keys]\n        if self.routing_keys:\n            rv['routingKeys'] = [canon_ref(k.did, k.id, '#') for k in self.routing_keys]\n        rv['serviceEndpoint'] = self.endpoint\n\n        return rv", "response": "Return dict representation of a resource in DID document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsynchronize revocation registry identifier with tails file.", "response": "async def _sync_revoc_for_proof(self, rr_id: str) -> None:\n        \"\"\"\n        Pick up tails file reader handle for input revocation registry identifier.  If no symbolic\n        link is present, get the revocation registry definition to retrieve its tails file hash,\n        then find the tails file and link it.\n\n        Raise AbsentTails for missing corresponding tails file.\n\n        :param rr_id: revocation registry identifier\n        \"\"\"\n\n        LOGGER.debug('HolderProver._sync_revoc_for_proof >>> rr_id: %s', rr_id)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('HolderProver._sync_revoc_for_proof <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        (cd_id, tag) = rev_reg_id2cred_def_id_tag(rr_id)\n\n        try:\n            json.loads(await self.get_cred_def(cd_id))\n        except AbsentCredDef:\n            LOGGER.debug(\n                'HolderProver._sync_revoc_for_proof <!< corrupt tails tree %s may be for another ledger',\n                self._dir_tails)\n            raise AbsentCredDef('Corrupt tails tree {} may be for another ledger'.format(self._dir_tails))\n        except ClosedPool:\n            pass  # carry on, may be OK from cache only\n\n        with REVO_CACHE.lock:\n            revo_cache_entry = REVO_CACHE.get(rr_id, None)\n            tails = revo_cache_entry.tails if revo_cache_entry else None\n            if tails is None:  #  it's not yet set in cache\n                try:\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()\n                except AbsentTails:  # get hash from ledger and check for tails file\n                    rr_def = json.loads(await self.get_rev_reg_def(rr_id))\n                    tails_hash = rr_def['value']['tailsHash']\n                    path_tails = join(Tails.dir(self._dir_tails, rr_id), tails_hash)\n                    if not isfile(path_tails):\n                        LOGGER.debug('HolderProver._sync_revoc_for_proof <!< No tails file present at %s', path_tails)\n                        raise AbsentTails('No tails file present at {}'.format(path_tails))\n                    Tails.associate(self._dir_tails, rr_id, tails_hash)\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()  # OK now since tails file present\n\n                if revo_cache_entry is None:\n                    REVO_CACHE[rr_id] = RevoCacheEntry(None, tails)\n                else:\n                    REVO_CACHE[rr_id].tails = tails\n\n        LOGGER.debug('HolderProver._sync_revoc_for_proof <<<')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dir_tails(self, rr_id: str) -> str:\n\n        LOGGER.debug('HolderProver.dir_tails >>>')\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('HolderProver.dir_tails <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        rv = Tails.dir(self._dir_tails, rr_id)\n        LOGGER.debug('HolderProver.dir_tails <<< %s', rv)\n        return rv", "response": "Return path to the correct directory for the tails file on input revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def open(self) -> 'HolderProver':\n\n        LOGGER.debug('HolderProver.open >>>')\n\n        await super().open()\n        if self.config.get('parse-caches-on-open', False):\n            ArchivableCaches.parse(self.dir_cache)\n\n        for path_rr_id in Tails.links(self._dir_tails):\n            await self._sync_revoc_for_proof(basename(path_rr_id))\n\n        LOGGER.debug('HolderProver.open <<<')\n        return self", "response": "Explicit entry. Perform ancestor opening operations,\n        then parse cache from archive if so configured, and\n        synchronize revocation registry to tails tree content.\n\n        :return: current object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def close(self) -> None:\n\n        LOGGER.debug('HolderProver.close >>>')\n\n        if self.config.get('archive-holder-prover-caches-on-close', False):\n            await self.load_cache_for_proof(True)\n            ArchivableCaches.purge_archives(self.dir_cache, True)\n\n        await BaseAnchor.close(self)\n        for path_rr_id in Tails.links(self._dir_tails):\n            rr_id = basename(path_rr_id)\n            try:\n                await self._sync_revoc_for_proof(rr_id)\n            except ClosedPool:\n                LOGGER.warning('HolderProver sync-revoc on close required ledger for %s but pool was closed', rr_id)\n\n        LOGGER.debug('HolderProver.close <<<')", "response": "Close the HolderProver instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def rev_regs(self) -> list:\n\n        LOGGER.debug('HolderProver.rev_regs >>>')\n\n        for path_rr_id in Tails.links(self._dir_tails):\n            await self._sync_revoc_for_proof(basename(path_rr_id))\n\n        rv = [basename(f) for f in Tails.links(self._dir_tails)]\n        LOGGER.debug('HolderProver.rev_regs <<< %s', rv)\n        return rv", "response": "Return list of revocation registry identifiers for which HolderProver has associated tails files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def offline_intervals(self, cd_ids: Union[str, Sequence[str]]) -> dict:\n\n        LOGGER.debug('HolderProver.offline_intervals >>> cd_ids: %s', cd_ids)\n\n        rv = {}\n        for cd_id in [cd_ids] if isinstance(cd_ids, str) else cd_ids:\n            if not ok_cred_def_id(cd_id):\n                LOGGER.debug('HolderProver.offline_intervals <!< Bad cred def id %s', cd_id)\n                raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n            try:\n                cred_def = json.loads(await self.get_cred_def(cd_id))\n            except ClosedPool:\n                LOGGER.debug('HolderProver.offline_intervals <!< no such cred def %s in cred def cache', cd_id)\n                raise CacheIndex('No cached delta for non-revoc interval on {}'.format(cd_id))\n\n            rv[cd_id] = {}\n            if 'revocation' in cred_def['value']:\n                with REVO_CACHE.lock:\n                    (fro, to) = REVO_CACHE.dflt_interval(cd_id)\n                    if not (fro and to):\n                        LOGGER.debug(\n                            'HolderProver.offline_intervals <!< no cached delta for non-revoc interval on %s',\n                            cd_id)\n                        raise CacheIndex('No cached delta for non-revoc interval on {}'.format(cd_id))\n\n                    rv[cd_id]['interval'] = to if fro == to else (fro, to)\n\n        LOGGER.debug('HolderProver.offline_intervals <<< %s', rv)\n        return rv", "response": "Return default non - revocation intervals for input cred def ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating link secret in wallet.", "response": "async def create_link_secret(self, label: str) -> None:\n        \"\"\"\n        Create link secret (a.k.a. master secret) used in proofs by HolderProver, if the\n        current link secret does not already correspond to the input link secret label.\n\n        Raise WalletState if wallet is closed, or any other IndyError causing failure\n        to set link secret in wallet.\n\n        :param label: label for link secret; indy-sdk uses label to generate link secret\n        \"\"\"\n\n        LOGGER.debug('HolderProver.create_link_secret >>> label: %s', label)\n\n        await self.wallet.create_link_secret(label)\n\n        LOGGER.debug('HolderProver.create_link_secret <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore credential in wallet as HolderProver.", "response": "async def store_cred(self, cred_json: str, cred_req_metadata_json: str) -> str:\n        \"\"\"\n        Store cred in wallet as HolderProver, return its credential identifier as created in wallet.\n\n        Raise AbsentTails if tails file not available for revocation registry for input credential.\n        Raise WalletState if wallet is closed.\n\n        :param cred_json: credential json as HolderProver created\n        :param cred_req_metadata_json: credential request metadata json as HolderProver created via create_cred_req()\n        :return: credential identifier within wallet\n        \"\"\"\n\n        LOGGER.debug(\n            'HolderProver.store_cred >>> cred_json: %s, cred_req_metadata_json: %s',\n            cred_json,\n            cred_req_metadata_json)\n\n        if not self.wallet.handle:\n            LOGGER.debug('HolderProver.store_cred <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        cred = json.loads(cred_json)\n        cred_def_json = await self.get_cred_def(cred['cred_def_id'])\n        rr_id = cred['rev_reg_id']\n        rr_def_json = None\n        if rr_id:\n            await self._sync_revoc_for_proof(rr_id)\n            rr_def_json = await self.get_rev_reg_def(rr_id)\n\n        rv = await anoncreds.prover_store_credential(\n            self.wallet.handle,\n            None,  # cred_id, let indy-sdk generate random uuid\n            cred_req_metadata_json,\n            cred_json,\n            cred_def_json,\n            rr_def_json)\n\n        LOGGER.debug('HolderProver.store_cred <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def load_cache_for_proof(self, archive: bool = False) -> int:\n\n        LOGGER.debug('HolderProver.load_cache_for_proof >>> archive: %s', archive)\n\n        rv = int(time())\n        box_ids = json.loads(await self.get_box_ids_held())\n        for s_id in box_ids['schema_id']:\n            with SCHEMA_CACHE.lock:\n                await self.get_schema(s_id)\n        for cd_id in box_ids['cred_def_id']:\n            with CRED_DEF_CACHE.lock:\n                await self.get_cred_def(cd_id)\n        for rr_id in box_ids['rev_reg_id']:\n            await self.get_rev_reg_def(rr_id)\n            with REVO_CACHE.lock:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                if revo_cache_entry:\n                    try:\n                        await revo_cache_entry.get_delta_json(self._build_rr_delta_json, rv, rv)\n                    except ClosedPool:\n                        LOGGER.warning(\n                            'HolderProver %s is offline from pool %s, cannot update revo cache reg delta for %s to %s',\n                            self.name,\n                            self.pool.name,\n                            rr_id,\n                            rv)\n                    except AbsentPool:\n                        LOGGER.warning(\n                            'HolderProver %s has no pool, cannot update revo cache reg delta for %s to %s',\n                            self.name,\n                            rr_id,\n                            rv)\n\n        if archive:\n            ArchivableCaches.archive(self.dir_cache)\n        LOGGER.debug('HolderProver.load_cache_for_proof <<< %s', rv)\n        return rv", "response": "Load schema cred def revocation cache and revocation cache delta for proof on all credentials in wallet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_box_ids_held(self) -> str:\n\n        LOGGER.debug('HolderProver.get_box_ids_held >>>')\n\n        if not self.wallet.handle:\n            LOGGER.debug('HolderProver.get_box_ids_held <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rr_ids = {basename(link) for link in Tails.links(self._dir_tails)}\n\n        un_rr_ids = set()\n        for rr_id in rr_ids:\n            if not json.loads(await self.get_cred_infos_by_q(json.dumps({'rev_reg_id': rr_id}), 1)):\n                un_rr_ids.add(rr_id)\n        rr_ids -= un_rr_ids\n\n        cd_ids = {cd_id for cd_id in listdir(self._dir_tails)\n            if isdir(join(self._dir_tails, cd_id)) and ok_cred_def_id(cd_id)}\n        s_ids = set()\n        for cd_id in cd_ids:\n            s_ids.add(json.loads(await self.get_schema(cred_def_id2seq_no(cd_id)))['id'])\n\n        un_cd_ids = set()\n        for cd_id in cd_ids:\n            if not json.loads(await self.get_cred_infos_by_q(json.dumps({'cred_def_id': cd_id}), 1)):\n                un_cd_ids.add(cd_id)\n        cd_ids -= un_cd_ids\n\n        un_s_ids = set()\n        for s_id in s_ids:\n            if not json.loads(await self.get_cred_infos_by_q(json.dumps({'schema_id': s_id}), 1)):\n                un_s_ids.add(s_id)\n        s_ids -= un_s_ids\n\n        rv = json.dumps({\n            'schema_id': list(s_ids),\n            'cred_def_id': list(cd_ids),\n            'rev_reg_id': list(rr_ids)\n        })\n        LOGGER.debug('HolderProver.get_box_ids_held <<< %s', rv)\n        return rv", "response": "Get all unique box identifiers for credentials in wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_cred_infos_by_q(self, query_json: str, limit: int = None) -> str:\n\n        LOGGER.debug('HolderProver.get_cred_infos_by_q >>> query_json: %s, limit: %s', query_json, limit)\n\n        if not self.wallet.handle:\n            LOGGER.debug('HolderProver.get_cred_infos_by_q <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        infos = []\n        if limit and limit < 0:\n            limit = None\n\n        (handle, cardinality) = await anoncreds.prover_search_credentials(\n            self.wallet.handle,\n            json.dumps(canon_cred_wql(json.loads(query_json))))  # indy-sdk requires attr name canonicalization\n        chunk = min(cardinality, limit or cardinality, Wallet.DEFAULT_CHUNK)\n        if limit:\n            cardinality = min(limit, cardinality)\n        try:\n            while len(infos) != cardinality:\n                batch = json.loads(await anoncreds.prover_fetch_credentials(handle, chunk))\n                infos.extend(batch)\n                if len(batch) < chunk:\n                    break\n            if len(infos) != cardinality:\n                LOGGER.warning('Credential search/limit indicated %s results but fetched %s', cardinality, len(infos))\n        finally:\n            await anoncreds.prover_close_credentials_search(handle)\n\n        rv_json = json.dumps(infos)\n        LOGGER.debug('HolderProver.get_cred_infos_by_q <<< %s', rv_json)\n        return rv_json", "response": "Get cred - infos from wallet by WQL query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_cred_infos_by_filter(self, filt: dict = None) -> str:\n\n        LOGGER.debug('HolderProver.get_cred_infos_by_filter >>> filt: %s', filt)\n\n        if not self.wallet.handle:\n            LOGGER.debug('HolderProver.get_cred_infos_by_filter <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rv_json = await anoncreds.prover_get_credentials(self.wallet.handle, json.dumps(filt or {}))\n        LOGGER.debug('HolderProver.get_cred_infos_by_filter <<< %s', rv_json)\n        return rv_json", "response": "Get credential info from wallet by input filter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create_proof(self, proof_req: dict, briefs: Union[dict, Sequence[dict]], requested_creds: dict) -> str:\n\n        LOGGER.debug(\n            'HolderProver.create_proof >>> proof_req: %s, briefs: %s, requested_creds: %s',\n            proof_req,\n            briefs,\n            requested_creds)\n\n        if not self.wallet.handle:\n            LOGGER.debug('HolderProver.create_proof <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        label = await self._assert_link_secret('create_proof')\n\n        cd_ids = set()\n        x_cd_ids = set()\n        for brief in iter_briefs(briefs):\n            cd_id = brief['cred_info']['cred_def_id']\n            if cd_id in cd_ids and cd_id not in x_cd_ids:\n                x_cd_ids.add(cd_id)\n            cd_ids.add(cd_id)\n            if x_cd_ids:\n                LOGGER.debug('HolderProver.create_proof <!< briefs specification out of focus (non-uniqueness)')\n                raise CredentialFocus('Briefs list repeats cred defs: {}'.format(x_cd_ids))\n\n        s_id2schema = {}  # schema identifier to schema\n        cd_id2cred_def = {}  # credential definition identifier to credential definition\n        rr_id2timestamp = {}  # revocation registry of interest to timestamp of interest (or None)\n        rr_id2cr_id = {}  # revocation registry of interest to credential revocation identifier\n        for brief in iter_briefs(briefs):\n            interval = brief.get('interval', None)\n            cred_info = brief['cred_info']\n            s_id = cred_info['schema_id']\n            if not ok_schema_id(s_id):\n                LOGGER.debug('HolderProver.create_proof <!< Bad schema id %s', s_id)\n                raise BadIdentifier('Bad schema id {}'.format(s_id))\n\n            if s_id not in s_id2schema:\n                schema = json.loads(await self.get_schema(s_id))  # add to cache en passant\n                if not schema:\n                    LOGGER.debug(\n                        'HolderProver.create_proof <!< absent schema %s, proof req may be for another ledger',\n                        s_id)\n                    raise AbsentSchema('Absent schema {}, proof req may be for another ledger'.format(s_id))\n                s_id2schema[s_id] = schema\n\n            cd_id = cred_info['cred_def_id']\n            if not ok_cred_def_id(cd_id):\n                LOGGER.debug('HolderProver.create_proof <!< Bad cred def id %s', cd_id)\n                raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n            if cd_id not in cd_id2cred_def:\n                cred_def = json.loads(await self.get_cred_def(cd_id))  # add to cache en passant\n                cd_id2cred_def[cd_id] = cred_def\n\n            rr_id = cred_info['rev_reg_id']\n            if rr_id:\n                if not ok_rev_reg_id(rr_id):\n                    LOGGER.debug('HolderProver.create_proof <!< Bad rev reg id %s', rr_id)\n                    raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n                await self._sync_revoc_for_proof(rr_id)  # link tails file to its rr_id if it's new\n                if interval:\n                    if rr_id not in rr_id2timestamp:\n                        if interval['to'] > int(time()):\n                            LOGGER.debug(\n                                'HolderProver.create_proof <!< interval to %s for rev reg %s is in the future',\n                                interval['to'],\n                                rr_id)\n                            raise BadRevStateTime(\n                                'Revocation registry {} timestamp {} is in the future'.format(rr_id, interval['to']))\n                        rr_id2timestamp[rr_id] = interval['to']\n                elif 'revocation' in cd_id2cred_def[cd_id]['value']:\n                    LOGGER.debug(\n                        'HolderProver.create_proof <!< brief on cred def id %s missing non-revocation interval',\n                        cd_id)\n                    raise AbsentInterval('Brief on cred def id {} missing non-revocation interval'.format(cd_id))\n                if rr_id in rr_id2cr_id:\n                    continue\n                rr_id2cr_id[rr_id] = cred_info['cred_rev_id']\n\n        rr_id2rev_state = {}  # revocation registry identifier to its state\n        with REVO_CACHE.lock:\n            for rr_id in rr_id2timestamp:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                tails = revo_cache_entry.tails if revo_cache_entry else None\n                if tails is None:  # missing tails file\n                    LOGGER.debug('HolderProver.create_proof <!< missing tails file for rev reg id %s', rr_id)\n                    raise AbsentTails('Missing tails file for rev reg id {}'.format(rr_id))\n                rr_def_json = await self.get_rev_reg_def(rr_id)\n                (rr_delta_json, ledger_timestamp) = await revo_cache_entry.get_delta_json(\n                    self._build_rr_delta_json,\n                    rr_id2timestamp[rr_id],\n                    rr_id2timestamp[rr_id])\n                rr_state_json = await anoncreds.create_revocation_state(\n                    tails.reader_handle,\n                    rr_def_json,\n                    rr_delta_json,\n                    ledger_timestamp,\n                    rr_id2cr_id[rr_id])\n                rr_id2rev_state[rr_id] = {\n                    rr_id2timestamp[rr_id]: json.loads(rr_state_json)\n                }\n\n        rv = await anoncreds.prover_create_proof(\n            self.wallet.handle,\n            json.dumps(proof_req),\n            json.dumps(requested_creds),\n            label,\n            json.dumps(s_id2schema),\n            json.dumps(cd_id2cred_def),\n            json.dumps(rr_id2rev_state))\n        LOGGER.debug('HolderProver.create_proof <<< %s', rv)\n        return rv", "response": "Create a HolderProver proof for the given cred - id and briefs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def reset_wallet(self, seed: str = None) -> Wallet:\n\n        LOGGER.debug('HolderProver.reset_wallet >>>')\n\n        self.wallet = await WalletManager().reset(self.wallet, seed)\n\n        rv = self.wallet\n        LOGGER.debug('HolderProver.reset_wallet <<< %s', rv)\n        return rv", "response": "Close and delete HolderProver wallet, then create and open a replacement on prior link secret.\n        Note that this operation effectively destroys private keys for credential definitions. Its\n        intended use is primarily for testing and demonstration.\n\n        Raise AbsentLinkSecret if link secret not set. Raise WalletState if the wallet is closed.\n\n        :param seed: seed to use for new wallet (default random)\n        :return: replacement wallet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef my_resolve_lookup(self, context):\n        current = context\n        try:  # catch-all for silent variable failures\n            for bit in self.lookups:\n                try:  # dictionary lookup\n                    current = current[bit]\n                    # ValueError/IndexError are for numpy.array lookup on\n                    # numpy < 1.9 and 1.9+ respectively\n                except (TypeError, AttributeError, KeyError, ValueError, IndexError):\n                    try:  # attribute lookup\n                        # Don\"t return class attributes if the class is the context:\n                        if isinstance(current, BaseContext) and getattr(type(current), bit):\n                            raise AttributeError\n                        current = getattr(current, bit)\n                    except (TypeError, AttributeError) as e:\n                        # Reraise an AttributeError raised by a @property\n                        if (isinstance(e, AttributeError) and\n                                not isinstance(current, BaseContext) and bit in dir(current)):\n                            raise\n                        try:  # list-index lookup\n                            current = current[int(bit)]\n                        except (IndexError,  # list index out of range\n                                ValueError,  # invalid literal for int()\n                                KeyError,    # current is a dict without `int(bit)` key\n                                TypeError):  # unsubscriptable object\n                            raise VariableDoesNotExist(\"Failed lookup for key \"\n                                                       \"[%s] in %r\",\n                                                       (bit, current))  # missing attribute\n                if callable(current):\n                    if getattr(current, \"do_not_call_in_templates\", False):\n                        pass\n                    elif getattr(current, \"alters_data\", False):\n                        try:\n                            current = context.template.engine.string_if_invalid\n                        except AttributeError:\n                            current = settings.TEMPLATE_STRING_IF_INVALID\n                    else:\n                        try:  # method call (assuming no args required)\n                            current = current()\n                        except TypeError:\n                            try:\n                                inspect.getcallargs(current)\n                            except TypeError:  # arguments *were* required\n                                current = context.template.engine.string_if_invalid  # invalid method call\n                            else:\n                                raise\n                elif isinstance(current, Model):\n                    if (\"request\" in context) and hasattr(context[\"request\"], \"_ultracache\"):\n                        # get_for_model itself is cached\n                        ct = ContentType.objects.get_for_model(current.__class__)\n                        context[\"request\"]._ultracache.append((ct.id, current.pk))\n\n        except Exception as e:\n            template_name = getattr(context, \"template_name\", None) or \"unknown\"\n            if logger is not None:\n                logger.debug(\n                    \"Exception while resolving variable \\\"%s\\\" in template \\\"%s\\\".\",\n                    bit,\n                    template_name,\n                    exc_info=True,\n                )\n\n            if getattr(e, \"silent_variable_failure\", False):\n                current = context.template.engine.string_if_invalid\n            else:\n                raise\n\n        return current", "response": "This method is called by the lookup methods to resolve a real variable against the given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index(self) -> dict:\n\n        LOGGER.debug('SchemaCache.index >>>')\n\n        rv = self._seq_no2schema_key\n        LOGGER.debug('SchemaCache.index <<< %s', rv)\n        return rv", "response": "Return a dictionary mapping content sequence numbers to schema keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schema_key_for(self, seq_no: int) -> SchemaKey:\n\n        LOGGER.debug('SchemaCache.schema_key_for >>> seq_no: %s', seq_no)\n\n        rv = self._seq_no2schema_key.get(seq_no, None)\n\n        LOGGER.debug('SchemaCache.schema_key_for <<< %s', rv)\n        return rv", "response": "Return schema key for given sequence number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef schemata(self) -> list:\n\n        LOGGER.debug('SchemaCache.schemata >>>')\n\n        LOGGER.debug('SchemaCache.schemata <<<')\n        return [self._schema_key2schema[seq_no] for seq_no in self._schema_key2schema]", "response": "Return list with schemata in cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _get_update(self, rr_builder: Callable, fro: int, to: int, delta: bool) -> (str, int):\n\n        LOGGER.debug(\n            'RevoCacheEntry.get_update >>> rr_builder: %s, fro: %s, to: %s, delta: %s',\n            rr_builder.__name__,\n            fro,\n            to,\n            delta)\n\n        if fro > to:\n            (fro, to) = (to, fro)\n\n        now = int(time())\n        if to > now:\n            LOGGER.debug(\n                'RevoCacheEntry._get_update <!< Cannot query a rev reg %s in the future (%s > %s)',\n                'delta' if delta else 'state',\n                to,\n                now)\n            raise BadRevStateTime('Cannot query a rev reg {} in the future ({} > {})'.format(\n                'delta' if delta else 'state',\n                to,\n                now))\n\n        cache_frame = None\n        rr_update_json = None\n        rr_frames = self.rr_delta_frames if delta else self.rr_state_frames\n\n        frames = [frame for frame in rr_frames if frame.timestamp <= to <= frame.to]\n        if frames:\n            cache_frame = max(frames, key=lambda f: f.timestamp)  # should be unique in any case\n            # do not update frame.to, it's already past asked-for 'to'\n        else:\n            frames = [frame for frame in rr_frames if (fro <= frame.to and frame.timestamp <= to)]\n            if frames:\n                cache_frame = max(frames, key=lambda f: f.timestamp)\n                # do not update frame.to - another update might occur, but we don't care; fro < frame.to, good enough\n        if not frames:\n            frames = [frame for frame in rr_frames if frame.timestamp < to]  # frame.to < to since not frames coming in\n            if frames:\n                latest_cached = max(frames, key=lambda frame: frame.timestamp)\n                if delta:\n                    (rr_update_json, timestamp) = await rr_builder(\n                        self.rev_reg_def['id'],\n                        to=to,\n                        fro=latest_cached.timestamp,\n                        fro_delta=latest_cached.rr_update)\n                else:\n                    (rr_update_json, timestamp) = await rr_builder(self.rev_reg_def['id'], to)\n                if timestamp == latest_cached.timestamp:\n                    latest_cached.to = to  # this timestamp now known good through more recent 'to'\n                    cache_frame = latest_cached\n            else:\n                (rr_update_json, timestamp) = await rr_builder(self.rev_reg_def['id'], to)\n\n        if cache_frame is None:\n            cache_frame = RevRegUpdateFrame(to, timestamp, json.loads(rr_update_json))  # sets qtime to now\n            rr_frames.append(cache_frame)\n            self.cull(delta)\n        else:\n            cache_frame.qtime = int(time())\n\n        rv = (json.dumps(cache_frame.rr_update), cache_frame.timestamp)\n        LOGGER.debug('RevoCacheEntry._get_update <<< %s', rv)\n        return rv", "response": "Get the update json and timestamp for a given delta and state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_delta_json(\n            self,\n            rr_delta_builder: Callable[['HolderProver', str, int, int, dict], Awaitable[Tuple[str, int]]],\n            fro: int,\n            to: int) -> (str, int):\n        \"\"\"\n        Get rev reg delta json, and its timestamp on the distributed ledger,\n        from cached rev reg delta frames list or distributed ledger,\n        updating cache as necessary.\n\n        Raise BadRevStateTime if caller asks for a delta to the future.\n\n        On return of any previously existing rev reg delta frame, always update its query time beforehand.\n\n        :param rr_delta_builder: callback to build rev reg delta if need be (specify anchor instance's\n            _build_rr_delta())\n        :param fro: least time (epoch seconds) of interest; lower-bounds 'to' on frame housing return data\n        :param to: greatest time (epoch seconds) of interest; upper-bounds returned revocation delta timestamp\n        :return: rev reg delta json and ledger timestamp (epoch seconds)\n        \"\"\"\n\n        LOGGER.debug(\n            'RevoCacheEntry.get_delta_json >>> rr_delta_builder: %s, fro: %s, to: %s',\n            rr_delta_builder.__name__,\n            fro,\n            to)\n\n        rv = await self._get_update(rr_delta_builder, fro, to, True)\n        LOGGER.debug('RevoCacheEntry.get_delta_json <<< %s', rv)\n        return rv", "response": "Get revocation delta json and its timestamp on the distributed ledger and return it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get_state_json(\n            self,\n            rr_state_builder: Callable[['Verifier', str, int], Awaitable[Tuple[str, int]]],\n            fro: int,\n            to: int) -> (str, int):\n        \"\"\"\n        Get rev reg state json, and its timestamp on the distributed ledger,\n        from cached rev reg state frames list or distributed ledger,\n        updating cache as necessary.\n\n        Raise BadRevStateTime if caller asks for a state in the future.\n\n        On return of any previously existing rev reg state frame, always update its query time beforehand.\n\n        :param rr_state_builder: callback to build rev reg state if need be (specify anchor instance's\n            _build_rr_state())\n        :param fro: least time (epoch seconds) of interest; lower-bounds 'to' on frame housing return data\n        :param to: greatest time (epoch seconds) of interest; upper-bounds returned revocation state timestamp\n        :return: rev reg state json and ledger timestamp (epoch seconds)\n        \"\"\"\n\n        LOGGER.debug(\n            'RevoCacheEntry.get_state_json >>> rr_state_builder: %s, fro: %s, to: %s',\n            rr_state_builder.__name__,\n            fro,\n            to)\n\n        rv = await self._get_update(rr_state_builder, fro, to, False)\n        LOGGER.debug('RevoCacheEntry.get_state_json <<< %s', rv)\n        return rv", "response": "Get revocation state json and its timestamp on the distributed ledger."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear() -> None:\n\n        LOGGER.debug('clear >>>')\n\n        with SCHEMA_CACHE.lock:\n            SCHEMA_CACHE.clear()\n        with CRED_DEF_CACHE.lock:\n            CRED_DEF_CACHE.clear()\n        with REVO_CACHE.lock:\n            REVO_CACHE.clear()\n\n        LOGGER.debug('clear <<<')", "response": "Clear all archivable caches in memory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef archive(base_dir: str) -> int:\n\n        LOGGER.debug('archive >>> base_dir: %s', base_dir)\n\n        rv = int(time())\n        timestamp_dir = join(base_dir, str(rv))\n        makedirs(timestamp_dir, exist_ok=True)\n\n        with SCHEMA_CACHE.lock:\n            with open(join(timestamp_dir, 'schema'), 'w') as archive:\n                print(json.dumps(SCHEMA_CACHE.schemata()), file=archive)\n\n        with CRED_DEF_CACHE.lock:\n            with open(join(timestamp_dir, 'cred_def'), 'w') as archive:\n                print(json.dumps(CRED_DEF_CACHE), file=archive)\n\n        with REVO_CACHE.lock:\n            with open(join(timestamp_dir, 'revocation'), 'w') as archive:\n                revo_cache_dict = {}\n                for rr_id in REVO_CACHE:\n                    revo_cache_dict[rr_id] = {\n                        'rev_reg_def': REVO_CACHE[rr_id].rev_reg_def,\n                        'rr_delta_frames': [vars(f) for f in REVO_CACHE[rr_id].rr_delta_frames],\n                        'rr_state_frames': [vars(f) for f in REVO_CACHE[rr_id].rr_state_frames]\n                    }\n                print(json.dumps(revo_cache_dict), file=archive)\n\n        LOGGER.debug('archive <<< %s', rv)\n        return rv", "response": "Archive schema cred def revocation caches to disk as json."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npurges all cache archives.", "response": "def purge_archives(base_dir: str, retain_latest: bool = False) -> None:\n        \"\"\"\n        Erase all (or nearly all) cache archives.\n\n        :param base_dir: archive base directory\n        :param retain_latest: retain latest archive if present, purge all others\n        \"\"\"\n\n        LOGGER.debug('purge_archives >>> base_dir: %s, retain_latest: %s', base_dir, retain_latest)\n\n        if isdir(base_dir):\n            timestamps = sorted([int(t) for t in listdir(base_dir) if t.isdigit()])\n            if retain_latest and timestamps:\n                timestamps.pop()\n            for timestamp in timestamps:\n                timestamp_dir = join(base_dir, str(timestamp))\n                rmtree(timestamp_dir)\n                LOGGER.info('Purged archive cache directory %s', timestamp_dir)\n\n        LOGGER.debug('purge_archives <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pairwise_info2tags(pairwise: PairwiseInfo) -> dict:\n\n    rv = {\n        canon_pairwise_tag(tag): raw(pairwise.metadata[tag]) for tag in pairwise.metadata or {}\n    }\n    rv['~their_did'] = pairwise.their_did\n    rv['~their_verkey'] = pairwise.their_verkey\n    rv['~my_did'] = pairwise.my_did\n    rv['~my_verkey'] = pairwise.my_verkey\n\n    if not StorageRecord.ok_tags(rv):\n        raise BadRecord('Pairwise metadata {} must map strings to strings'.format(rv))\n\n    return rv", "response": "Given a pairwise info with metadata mapping tags to values return corresponding\n    indy - sdk non_secrets record tags dict to store same in wallet"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive indy - sdk non - secrets implementation of pairwise storage record dict return corresponding PairwiseInfo.", "response": "def storage_record2pairwise_info(storec: StorageRecord) -> PairwiseInfo:\n    \"\"\"\n    Given indy-sdk non_secrets implementation of pairwise storage record dict, return corresponding PairwiseInfo.\n\n    :param storec: (non-secret) storage record to convert to PairwiseInfo\n    :return: PairwiseInfo on record DIDs, verkeys, metadata\n    \"\"\"\n\n    return PairwiseInfo(\n        storec.id,  # = their did\n        storec.value,  # = their verkey\n        storec.tags['~my_did'],\n        storec.tags['~my_verkey'],\n        {\n            tag[tag.startswith('~'):]: storec.tags[tag] for tag in (storec.tags or {})  # strip any leading '~'\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a configuration dict with indy and possibly more configuration values return the corresponding indy wallet configuration dict from current default and input values.", "response": "def _config2indy(self, config: dict) -> dict:\n        \"\"\"\n        Given a configuration dict with indy and possibly more configuration values, return the\n        corresponding indy wallet configuration dict from current default and input values.\n\n        :param config: input configuration\n        :return: configuration dict for indy wallet\n        \"\"\"\n\n        assert {'name', 'id'} & {k for k in config}\n        return {\n            'id': config.get('name', config.get('id')),\n            'storage_type': config.get('storage_type', self.default_storage_type),\n            'freshness_time': config.get('freshness_time', self.default_freshness_time)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _config2von(self, config: dict, access: str = None) -> dict:\n\n        rv = {k: config.get(k, self._defaults[k]) for k in ('auto_create', 'auto_remove')}\n        rv['access'] = access or self.default_access\n        for key in ('seed', 'did', 'link_secret_label'):\n            if key in config:\n                rv[key] = config[key]\n        return rv", "response": "Given a configuration dict with indy and possibly more configuration values return the corresponding VON wallet configuration dict from current default and input values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def create(self, config: dict = None, access: str = None, replace: bool = False) -> Wallet:\n\n        LOGGER.debug('WalletManager.create >>> config %s, access %s, replace %s', config, access, replace)\n\n        assert {'name', 'id'} & {k for k in config}\n        wallet_name = config.get('name', config.get('id'))\n        if replace:\n            von_wallet = self.get(config, access)\n            if not await von_wallet.remove():\n                LOGGER.debug('WalletManager.create <!< Failed to remove wallet %s for replacement', wallet_name)\n                raise ExtantWallet('Failed to remove wallet {} for replacement'.format(wallet_name))\n\n        indy_config = self._config2indy(config)\n        von_config = self._config2von(config, access)\n        rv = Wallet(indy_config, von_config)\n        await rv.create()\n        LOGGER.debug('WalletManager.create <<< %s', rv)\n        return rv", "response": "Create new wallet on input name with given configuration and access credential value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, config: dict, access: str = None) -> Wallet:\n\n        LOGGER.debug('WalletManager.get >>> config %s, access %s', config, access)\n\n        rv = Wallet(\n            self._config2indy(config),\n            self._config2von(config, access))\n\n        LOGGER.debug('WalletManager.get <<< %s', rv)\n        return rv", "response": "Instantiate and return VON anchor wallet object on given configuration data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def reseed_local(self, local_wallet: Wallet, next_seed: str = None) -> DIDInfo:\n\n        LOGGER.debug('WalletManager.reseed_local >>> local_wallet %s', local_wallet)\n\n        await local_wallet.reseed_init(next_seed)\n\n        rv = await local_wallet.reseed_apply()\n        LOGGER.debug('WalletManager.reseed_local <<< %s', rv)\n        return rv", "response": "Generate and apply new verification key for local DID based on input seed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def export_wallet(self, von_wallet: Wallet, path: str) -> None:\n\n        LOGGER.debug('WalletManager.export_wallet >>> von_wallet %s, path %s', von_wallet, path)\n\n        if not von_wallet.handle:\n            LOGGER.debug('WalletManager.export_wallet <!< Wallet %s is closed', von_wallet.name)\n            raise WalletState('Wallet {} is closed'.format(von_wallet.name))\n\n        await wallet.export_wallet(\n            von_wallet.handle,\n            json.dumps({\n                'path': path,\n                **von_wallet.access_creds\n            }))\n\n        LOGGER.debug('WalletManager.export_wallet <<<')", "response": "Export an existing VON anchor wallet. Raise WalletState if wallet is closed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def import_wallet(self, indy_config: dict, path: str, access: str = None) -> None:\n\n        LOGGER.debug('WalletManager.import_wallet >>> indy_config %s, path: %s', indy_config, path)\n\n        try:\n            await wallet.import_wallet(\n                json.dumps(indy_config),\n                json.dumps({'key': access or self.default_access}),\n                json.dumps({'path': path, 'key': access or self.default_access}))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.CommonInvalidStructure:  # indy-sdk raises on bad access\n                LOGGER.debug(\n                    'WalletManager.import_wallet <!< bad access credential value for wallet %s',\n                    indy_config.get('id', '(no id)'))\n                raise BadAccess('Bad access credential value for wallet {}'.format(indy_config.get('id', '(no id)')))\n            LOGGER.debug(\n                'WalletManager.import_wallet <!< indy error code %s on wallet %s import',\n                x_indy.error_code,\n                indy_config.get('id', '(no id)'))\n            raise\n\n        LOGGER.debug('WalletManager.import_wallet <<<')", "response": "Import a VON anchor wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets wallet and return new wallet with new link secret.", "response": "async def reset(self, von_wallet: Wallet, seed: str = None) -> Wallet:\n        \"\"\"\n        Close and delete (open) VON anchor wallet and then create, open, and return\n        replacement on current link secret.\n\n        Note that this operation effectively destroys private keys for keyed data\n        structures such as credential offers or credential definitions.\n\n        Raise WalletState if the wallet is closed.\n\n        :param von_wallet: open wallet\n        :param seed: seed to use for new wallet (default random)\n        :return: replacement wallet\n        \"\"\"\n\n        LOGGER.debug('WalletManager.reset >>> von_wallet %s', von_wallet)\n\n        if not von_wallet.handle:\n            LOGGER.debug('WalletManager.reset <!< Wallet %s is closed', von_wallet.name)\n            raise WalletState('Wallet {} is closed'.format(von_wallet.name))\n\n        w_config = von_wallet.config  # wallet under reset, no need to make copy\n        w_config['did'] = von_wallet.did\n        w_config['seed'] = seed\n        w_config['auto_create'] = von_wallet.auto_create  # in case both auto_remove+auto_create set (create every open)\n        w_config['auto_remove'] = von_wallet.auto_remove\n\n        label = await von_wallet.get_link_secret_label()\n        if label:\n            w_config['link_secret_label'] = label\n\n        await von_wallet.close()\n        if not von_wallet.auto_remove:\n            await self.remove(von_wallet)\n\n        rv = await self.create(w_config, von_wallet.access)\n        await rv.open()\n\n        LOGGER.debug('WalletManager.reset <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def register_storage_library(storage_type: str, c_library: str, entry_point: str) -> None:\n\n        LOGGER.debug(\n            'WalletManager.register_storage_library >>> storage_type %s, c_library %s, entry_point %s',\n            storage_type,\n            c_library,\n            entry_point)\n\n        try:\n            stg_lib = CDLL(c_library)\n            result = stg_lib[entry_point]()\n            if result:\n                LOGGER.debug(\n                    'WalletManager.register_storage_library <!< indy error code %s on storage library entry at %s',\n                    result,\n                    entry_point)\n                raise IndyError(result)\n            LOGGER.info('Loaded storage library type %s (%s)', storage_type, c_library)\n        except IndyError as x_indy:\n            LOGGER.debug(\n                'WalletManager.register_storage_library <!< indy error code %s on load of storage library %s %s',\n                x_indy.error_code,\n                storage_type,\n                c_library)\n            raise\n\n        LOGGER.debug('WalletManager.register_storage_library <<<')", "response": "Register a wallet storage library entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpire ultracache cache keys affected by this object", "response": "def on_post_save(sender, **kwargs):\n    \"\"\"Expire ultracache cache keys affected by this object\n    \"\"\"\n    if not invalidate:\n        return\n    if kwargs.get(\"raw\", False):\n        return\n    if sender is MigrationRecorder.Migration:\n        return\n    if issubclass(sender, Model):\n        obj = kwargs[\"instance\"]\n        if isinstance(obj, Model):\n            # get_for_model itself is cached\n            try:\n                ct = ContentType.objects.get_for_model(sender)\n            except RuntimeError:\n                # This happens when ultracache is being used by another product\n                # during a test run.\n                return\n\n            if kwargs.get(\"created\", False):\n                # Expire cache keys that contain objects of this content type\n                key = \"ucache-ct-%s\" % ct.id\n                to_delete = cache.get(key, [])\n                if to_delete:\n                    try:\n                        cache.delete_many(to_delete)\n                    except NotImplementedError:\n                        for k in to_delete:\n                            cache.delete(k)\n                cache.delete(key)\n\n                # Purge paths in reverse caching proxy that contain objects of\n                # this content type.\n                key = \"ucache-ct-pth-%s\" % ct.id\n                if purger is not None:\n                    for li in cache.get(key, []):\n                        purger(li[0], li[1])\n                cache.delete(key)\n\n            else:\n                # Expire cache keys\n                key = \"ucache-%s-%s\" % (ct.id, obj.pk)\n                to_delete = cache.get(key, [])\n                if to_delete:\n                    try:\n                        cache.delete_many(to_delete)\n                    except NotImplementedError:\n                        for k in to_delete:\n                            cache.delete(k)\n                cache.delete(key)\n\n                # Purge paths in reverse caching proxy\n                key = \"ucache-pth-%s-%s\" % (ct.id, obj.pk)\n                if purger is not None:\n                    for li in cache.get(key, []):\n                        purger(li[0], li[1])\n                cache.delete(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_post_delete(sender, **kwargs):\n    if not invalidate:\n        return\n    if kwargs.get(\"raw\", False):\n        return\n    if sender is MigrationRecorder.Migration:\n        return\n    if issubclass(sender, Model):\n        obj = kwargs[\"instance\"]\n        if isinstance(obj, Model):\n            # get_for_model itself is cached\n            try:\n                ct = ContentType.objects.get_for_model(sender)\n            except RuntimeError:\n                # This happens when ultracache is being used by another product\n                # during a test run.\n                return\n\n            # Expire cache keys\n            key = \"ucache-%s-%s\" % (ct.id, obj.pk)\n            to_delete = cache.get(key, [])\n            if to_delete:\n                try:\n                    cache.delete_many(to_delete)\n                except NotImplementedError:\n                    for k in to_delete:\n                        cache.delete(k)\n            cache.delete(key)\n\n            # Invalidate paths in reverse caching proxy\n            key = \"ucache-pth-%s-%s\" % (ct.id, obj.pk)\n            if purger is not None:\n                for li in cache.get(key, []):\n                    purger(li[0], li[1])\n            cache.delete(key)", "response": "Expire ultracache cache keys affected by this object\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_signing_key(self, seed: str = None, metadata: dict = None) -> KeyInfo:\n\n        LOGGER.debug('Wallet.create_signing_key >>> seed: [SEED], metadata: %s', metadata)\n\n        if not self.handle:\n            LOGGER.debug('Wallet.create_signing_key <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        try:\n            verkey = await crypto.create_key(self.handle, json.dumps({'seed': seed} if seed else {}))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletItemAlreadyExists:\n                LOGGER.debug('Wallet.create_signing_key <!< Verification key already present in wallet %s', self.name)\n                raise ExtantRecord('Verification key already present in wallet {}'.format(self.name))\n            LOGGER.debug('Wallet.create_signing_key <!< indy-sdk raised error %s', x_indy.error_code)\n            raise\n\n        await crypto.set_key_metadata(self.handle, verkey, json.dumps(metadata or {}))  # coerce None to empty\n\n        rv = KeyInfo(verkey, metadata or {})\n        LOGGER.debug('Wallet.create_signing_key <<< %s', rv)\n        return rv", "response": "Create a new signing key pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates and store a new local DID for use in pairwise DID relations.", "response": "async def create_local_did(self, seed: str = None, loc_did: str = None, metadata: dict = None) -> DIDInfo:\n        \"\"\"\n        Create and store a new local DID for use in pairwise DID relations.\n\n        :param seed: seed from which to create (default random)\n        :param loc_did: local DID value (default None to let indy-sdk generate)\n        :param metadata: metadata to associate with the local DID\n            (operation always sets 'since', 'modified' epoch timestamps)\n        :return: DIDInfo for new local DID\n        \"\"\"\n\n        LOGGER.debug('Wallet.create_local_did >>> seed: [SEED] loc_did: %s metadata: %s', loc_did, metadata)\n\n        cfg = {}\n        if seed:\n            cfg['seed'] = seed\n        if loc_did:\n            cfg['did'] = loc_did\n\n        if not self.handle:\n            LOGGER.debug('Wallet.create_local_did <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        try:\n            (created_did, verkey) = await did.create_and_store_my_did(self.handle, json.dumps(cfg))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.DidAlreadyExistsError:\n                LOGGER.debug('Wallet.create_local_did <!< DID %s already present in wallet %s', loc_did, self.name)\n                raise ExtantRecord('Local DID {} already present in wallet {}'.format(loc_did, self.name))\n            LOGGER.debug('Wallet.create_local_did <!< indy-sdk raised error %s', x_indy.error_code)\n            raise\n\n        now = int(time())\n        loc_did_metadata = {**(metadata or {}), 'since': now, 'modified': now}\n        await did.set_did_metadata(self.handle, created_did, json.dumps(loc_did_metadata))\n\n        rv = DIDInfo(created_did, verkey, loc_did_metadata)\n\n        LOGGER.debug('Wallet.create_local_did <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace the metadata associated with a local DID with new metadata.", "response": "async def replace_local_did_metadata(self, loc_did: str, metadata: dict) -> DIDInfo:\n        \"\"\"\n        Replace the metadata associated with a local DID.\n\n        Raise WalletState if wallet is closed, AbsentRecord for no such local DID.\n\n        :param loc_did: local DID of interest\n        :param metadata: new metadata to store\n        :return: DIDInfo for local DID after write\n        \"\"\"\n\n        LOGGER.debug('Wallet.replace_local_did_metadata >>> loc_did: %s, metadata: %s', loc_did, metadata)\n\n        old = await self.get_local_did(loc_did)  # raises exceptions if applicable\n        now = int(time())\n        loc_did_metadata = {**(metadata or {}), 'since': (old.metadata or {}).get('since', now), 'modified': now}\n        try:\n            await did.set_did_metadata(self.handle, loc_did, json.dumps(loc_did_metadata))\n        except IndyError as x_indy:\n            LOGGER.debug('Wallet.replace_local_did_metadata <!< indy-sdk raised error %s', x_indy.error_code)\n            raise\n\n        rv = await self.get_local_did(loc_did)\n        LOGGER.debug('Wallet.replace_local_did_metadata <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_local_dids(self) -> Sequence[DIDInfo]:\n\n        LOGGER.debug('Wallet.get_local_dids >>>')\n\n        dids_with_meta = json.loads(did.list_my_dids_with_meta(self.handle))  # list\n\n        rv = []\n        for did_with_meta in dids_with_meta:\n            meta = json.loads(did_with_meta['metadata']) if did_with_meta['metadata'] else {}\n            if meta.get('anchor', False):\n                continue  # exclude anchor DIDs past and present\n            rv.append(DIDInfo(did_with_meta['did'], did_with_meta['verkey'], meta))\n\n        LOGGER.debug('Wallet.get_local_dids <<< %s', rv)\n        return rv", "response": "Get list of local DIDInfos for local DIDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting current anchor DID by metadata.", "response": "async def get_anchor_did(self) -> str:\n        \"\"\"\n        Get current anchor DID by metadata, None for not yet set.\n\n        :return: DID\n        \"\"\"\n\n        LOGGER.debug('Wallet.get_anchor_did >>>')\n\n        if not self.handle:\n            LOGGER.debug('Wallet.get_anchor_did <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rv = None\n        dids_with_meta = json.loads(await did.list_my_dids_with_meta(self.handle))  # list\n\n        latest = 0\n        for did_with_meta in dids_with_meta:\n            try:\n                meta = json.loads(did_with_meta['metadata']) if did_with_meta['metadata'] else {}\n                if not meta.get('anchor', False):\n                    continue\n                if isinstance(meta, dict) and meta.get('since', -1) > latest:\n                    rv = did_with_meta.get('did')\n            except json.decoder.JSONDecodeError:\n                continue  # it's not an anchor DID, carry on\n\n        LOGGER.debug('Wallet.get_anchor_did <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def create_link_secret(self, label: str) -> None:\n\n        LOGGER.debug('Wallet.create_link_secret >>> label: %s', label)\n\n        if not self.handle:\n            LOGGER.debug('Wallet.create_link_secret <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        try:\n            await anoncreds.prover_create_master_secret(self.handle, label)\n            await self._write_link_secret_label(label)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.AnoncredsMasterSecretDuplicateNameError:\n                LOGGER.warning(\n                    'Wallet %s link secret already current: abstaining from updating label record', self.name)\n                await self._write_link_secret_label(label)\n            else:\n                LOGGER.debug(\n                    'Wallet.create_link_secret <!< cannot create link secret for wallet %s, indy error code %s',\n                    self.name,\n                    x_indy.error_code)\n                raise\n\n        LOGGER.debug('Wallet.create_link_secret <<<')", "response": "Create link secret in wallet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate non - secret storage record with link secret label.", "response": "async def _write_link_secret_label(self, label) -> None:\n        \"\"\"\n        Update non-secret storage record with link secret label.\n\n        :param label: link secret label\n        \"\"\"\n\n        LOGGER.debug('Wallet._write_link_secret_label <<< %s', label)\n\n        if await self.get_link_secret_label() == label:\n            LOGGER.info('Wallet._write_link_secret_label abstaining - already current')\n        else:\n            await self.write_non_secret(StorageRecord(\n                TYPE_LINK_SECRET_LABEL,\n                label,\n                tags=None,\n                ident=str(int(time()))))  # indy requires str\n\n        LOGGER.debug('Wallet._write_link_secret_label <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets current link secret label from non - secret storage records ; return None for no match.", "response": "async def get_link_secret_label(self) -> str:\n        \"\"\"\n        Get current link secret label from non-secret storage records; return None for no match.\n\n        :return: latest non-secret storage record for link secret label\n        \"\"\"\n\n        LOGGER.debug('Wallet.get_link_secret_label >>>')\n\n        if not self.handle:\n            LOGGER.debug('Wallet.get_link_secret <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rv = None\n        records = await self.get_non_secret(TYPE_LINK_SECRET_LABEL)\n        if records:\n            rv = records[str(max(int(k) for k in records))].value  # str to int, max, and back again\n\n        LOGGER.debug('Wallet.get_link_secret_label <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def open(self) -> 'Wallet':\n\n        LOGGER.debug('Wallet.open >>>')\n\n        created = False\n        while True:\n            try:\n                self._handle = await wallet.open_wallet(\n                    json.dumps(self.config),\n                    json.dumps(self.access_creds))\n                LOGGER.info('Opened wallet %s on handle %s', self.name, self.handle)\n                break\n            except IndyError as x_indy:\n                if x_indy.error_code == ErrorCode.WalletNotFoundError:\n                    if created:\n                        LOGGER.debug('Wallet.open() <!< Wallet %s not found after creation', self.name)\n                        raise AbsentWallet('Wallet {} not found after creation'.format(self.name))\n                    if self.auto_create:\n                        await self.create()\n                        continue\n                    else:\n                        LOGGER.debug('Wallet.open() <!< Wallet %s not found', self.name)\n                        raise AbsentWallet('Wallet {} not found'.format(self.name))\n                elif x_indy.error_code == ErrorCode.WalletAlreadyOpenedError:\n                    LOGGER.debug('Wallet.open() <!< Wallet %s is already open', self.name)\n                    raise WalletState('Wallet {} is already open'.format(self.name))\n                elif x_indy.error_code == ErrorCode.WalletAccessFailed:\n                    LOGGER.debug('Wallet.open() <!< Bad access credentials value for wallet %s', self.name)\n                    raise BadAccess('Bad access credentials value for wallet {}'.format(self.name))\n\n                LOGGER.debug('Wallet %s open raised indy error %s', self.name, x_indy.error_code)\n                raise\n\n        self.did = await self.get_anchor_did()\n        self.verkey = await did.key_for_local_did(self.handle, self.did) if self.did else None\n        LOGGER.info('Wallet %s got verkey %s for existing DID %s', self.name, self.verkey, self.did)\n\n        LOGGER.debug('Wallet.open <<<')\n        return self", "response": "Open wallet and return current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def create(self) -> None:\n\n        LOGGER.debug('Wallet.create >>>')\n\n        try:\n            await wallet.create_wallet(\n                config=json.dumps(self.config),\n                credentials=json.dumps(self.access_creds))\n            LOGGER.info('Created wallet %s', self.name)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletAlreadyExistsError:\n                LOGGER.debug('Wallet.create <!< Wallet %s already exists', self.name)\n                raise ExtantWallet('Wallet {} already exists'.format(self.name))\n            LOGGER.debug(\n                'Wallet.create <!< indy error code %s on creation of wallet %s',\n                x_indy.error_code,\n                self.name)\n            raise\n\n        auto_remove = self.auto_remove\n        self.auto_remove = False  # defer past this creation process\n        async with self:\n            did_info = await self.create_local_did(\n                self._von_config.get('seed', None),\n                self._von_config.get('did', None),\n                {'anchor': True})\n            self.did = did_info.did\n            self.verkey = did_info.verkey\n            if 'link_secret_label' in self._von_config:\n                await self.create_link_secret(self._von_config['link_secret_label'])\n        self.auto_remove = auto_remove\n\n        LOGGER.debug('Wallet.create <<<')", "response": "Create the wallet and store it in self. did."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the wallet and delete if auto_remove is set to True.", "response": "async def close(self) -> None:\n        \"\"\"\n        Explicit exit. Close wallet (and delete if so configured).\n        \"\"\"\n\n        LOGGER.debug('Wallet.close >>>')\n\n        if not self.handle:\n            LOGGER.warning('Abstaining from closing wallet %s: already closed', self.name)\n        else:\n            LOGGER.debug('Closing wallet %s', self.name)\n            await wallet.close_wallet(self.handle)\n            self._handle = None\n            if self.auto_remove:\n                LOGGER.info('Automatically removing wallet %s', self.name)\n                await self.remove()\n        self._handle = None\n\n        LOGGER.debug('Wallet.close <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove serialized wallet return whether wallet absent after operation", "response": "async def remove(self) -> bool:\n        \"\"\"\n        Remove serialized wallet, best effort, if it exists. Return whether wallet absent after operation\n        (removal successful or else not present a priori).\n\n        Raise WalletState if wallet is open.\n\n        :return: whether wallet gone from persistent storage\n        \"\"\"\n\n        LOGGER.debug('Wallet.remove >>>')\n\n        if self.handle:\n            LOGGER.debug('Wallet.remove <!< Wallet %s is open', self.name)\n            raise WalletState('Wallet {} is open'.format(self.name))\n\n        rv = True\n        try:\n            LOGGER.info('Attempting to remove wallet: %s', self.name)\n            await wallet.delete_wallet(\n                json.dumps(self.config),\n                json.dumps(self.access_creds))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletNotFoundError:\n                LOGGER.info('Wallet %s not present; abstaining from removal', self.name)\n            else:\n                LOGGER.info('Failed wallet %s removal; indy-sdk error code %s', self.name, x_indy.error_code)\n                rv = False\n\n        LOGGER.debug('Wallet.remove <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def write_pairwise(\n            self,\n            their_did: str,\n            their_verkey: str = None,\n            my_did: str = None,\n            metadata: dict = None,\n            replace_meta: bool = False) -> PairwiseInfo:\n        \"\"\"\n        Store a pairwise DID for a secure connection. Use verification key for local DID in wallet if\n        supplied; otherwise, create one first. If local DID specified but not present, raise AbsentRecord.\n\n        With supplied metadata, replace or augment and overwrite any existing metadata for the pairwise\n        relation if one already exists in the wallet. Always include local and remote DIDs and keys in\n        metadata to allow for WQL search.\n\n        Raise AbsentRecord on call to update a non-existent record. Raise BadRecord if metadata does not\n        coerce into non-secrets API tags specification {str:str}.\n\n        :param their_did: remote DID\n        :param their_verkey: remote verification key (default None is OK if updating an existing pairwise DID)\n        :param my_did: local DID\n        :param metadata: metadata for pairwise connection\n        :param replace_meta: whether to (True) replace or (False) augment and overwrite existing metadata\n        :return: resulting PairwiseInfo\n        \"\"\"\n\n        LOGGER.debug(\n            'Wallet.write_pairwise >>> their_did: %s, their_verkey: %s, my_did: %s, metadata: %s, replace_meta: %s',\n            their_did,\n            their_verkey,\n            my_did,\n            metadata,\n            replace_meta)\n\n        if their_verkey is None:\n            match = await self.get_pairwise(their_did)\n            if not match:\n                LOGGER.debug(\n                    'Wallet.write_pairwise <!< Wallet %s has no pairwise DID on %s to update',\n                    self.name,\n                    their_did)\n                raise AbsentRecord('Wallet {} has no pairwise DID on {} to update'.format(self.name, their_did))\n            their_verkey = [pwise for pwise in match.values()][0].their_verkey\n\n        try:\n            await did.store_their_did(self.handle, json.dumps({'did': their_did, 'verkey': their_verkey}))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletItemAlreadyExists:\n                pass  # exists already, carry on\n            else:\n                LOGGER.debug(\n                    'Wallet.write_pairwise <!< Wallet %s write of their_did %s raised indy error code %s',\n                    self.name,\n                    their_did,\n                    x_indy.error_code)\n                raise\n\n        if my_did:\n            my_did_info = await self.get_local_did(my_did)  # raises AbsentRecord if no such local did\n        else:\n            my_did_info = await self.create_local_did(None, None, {'pairwise_for': their_did})\n\n        pairwise = PairwiseInfo(their_did, their_verkey, my_did_info.did, my_did_info.verkey, metadata)\n        try:\n            storec = await self.write_non_secret(\n                StorageRecord(TYPE_PAIRWISE, their_verkey, tags=pairwise_info2tags(pairwise), ident=their_did),\n                replace_meta)\n        except BadRecord:\n            LOGGER.debug(\n                'Wallet.write_pairwise <!< Pairwise metadata %s does not coerce into flat {str:str} tags dict',\n                pairwise.metadata)\n            raise\n\n        rv = storage_record2pairwise_info(storec)\n        LOGGER.debug('Wallet.write_pairwise <<< %s', rv)\n        return rv", "response": "Write a pairwise connection to the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_pairwise(self, pairwise_filt: str = None) -> dict:\n\n        LOGGER.debug('Wallet.get_pairwise >>> pairwise_filt: %s', pairwise_filt)\n\n        if not self.handle:\n            LOGGER.debug('Wallet.get_pairwise <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        storecs = await self.get_non_secret(\n            TYPE_PAIRWISE,\n            pairwise_filt if ok_did(pairwise_filt) or not pairwise_filt else json.loads(pairwise_filt),\n            canon_pairwise_wql)\n        rv = {k: storage_record2pairwise_info(storecs[k]) for k in storecs}  # touch up tags, mute leading ~\n\n        LOGGER.debug('Wallet.get_pairwise <<< %s', rv)\n        return rv", "response": "Get a dictionary mapping each remote DID of interest in wallet to its pairwise info or mapping them all. If wallet has no such item return empty dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def write_non_secret(self, storec: StorageRecord, replace_meta: bool = False) -> StorageRecord:\n\n        LOGGER.debug('Wallet.write_non_secret >>> storec: %s, replace_meta: %s', storec, replace_meta)\n\n        if not self.handle:\n            LOGGER.debug('Wallet.write_non_secret <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if not StorageRecord.ok_tags(storec.tags):\n            LOGGER.debug('Wallet.write_non_secret <!< bad storage record tags %s; use flat {str: str} dict', storec)\n            raise BadRecord('Bad storage record tags {}; use flat {{str:str}} dict'.format(storec))\n\n        try:\n            record = json.loads(await non_secrets.get_wallet_record(\n                self.handle,\n                storec.type,\n                storec.id,\n                json.dumps({\n                    'retrieveType': False,\n                    'retrieveValue': True,\n                    'retrieveTags': True\n                })))\n            if record['value'] != storec.value:\n                await non_secrets.update_wallet_record_value(\n                    self.handle,\n                    storec.type,\n                    storec.id,\n                    storec.value)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletItemNotFound:\n                await non_secrets.add_wallet_record(\n                    self.handle,\n                    storec.type,\n                    storec.id,\n                    storec.value,\n                    json.dumps(storec.tags) if storec.tags else None)\n            else:\n                LOGGER.debug(\n                    'Wallet.write_non_secret <!< Wallet lookup raised indy error code %s',\n                    x_indy.error_code)\n                raise\n        else:\n            if (record['tags'] or None) != storec.tags:  # record maps no tags to {}, not None\n                tags = (storec.tags or {}) if replace_meta else {**record['tags'], **(storec.tags or {})}\n\n                await non_secrets.update_wallet_record_tags(\n                    self.handle,\n                    storec.type,\n                    storec.id,\n                    json.dumps(tags))  # indy-sdk takes '{}' instead of None for null tags\n\n        record = json.loads(await non_secrets.get_wallet_record(\n            self.handle,\n            storec.type,\n            storec.id,\n            json.dumps({\n                'retrieveType': False,\n                'retrieveValue': True,\n                'retrieveTags': True\n            })))\n\n        rv = StorageRecord(storec.type, record['value'], tags=record.get('tags', None), ident=record['id'])\n        LOGGER.debug('Wallet.write_non_secret <<< %s', rv)\n        return rv", "response": "Write a non - secret storage record to the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_non_secret(\n            self,\n            typ: str,\n            filt: Union[dict, str] = None,\n            canon_wql: Callable[[dict], dict] = None,\n            limit: int = None) -> dict:\n        \"\"\"\n        Return dict mapping each non-secret storage record of interest by identifier or,\n        for no filter specified, mapping them all. If wallet has no such item, return empty dict.\n\n        :param typ: non-secret storage record type\n        :param filt: non-secret storage record identifier or WQL json (default all)\n        :param canon_wql: WQL canonicalization function (default von_anchor.canon.canon_non_secret_wql())\n        :param limit: maximum number of results to return (default no limit)\n        :return: dict mapping identifiers to non-secret storage records\n        \"\"\"\n\n        LOGGER.debug('Wallet.get_non_secret >>> typ: %s, filt: %s, canon_wql: %s', typ, filt, canon_wql)\n\n        if not self.handle:\n            LOGGER.debug('Wallet.get_non_secret <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        records = []\n        if isinstance(filt, str):  # ordinary lookup by value\n            try:\n                records = [json.loads(await non_secrets.get_wallet_record(\n                    self.handle,\n                    typ,\n                    filt,\n                    json.dumps({\n                        'retrieveType': False,\n                        'retrieveValue': True,\n                        'retrieveTags': True\n                    })))]\n            except IndyError as x_indy:\n                if x_indy.error_code == ErrorCode.WalletItemNotFound:\n                    pass\n                else:\n                    LOGGER.debug(\n                        'Wallet.get_non_secret <!< Wallet %s lookup raised indy exception %s',\n                        self.name,\n                        x_indy.error_code)\n                    raise\n        else:\n            canon = canon_wql or canon_non_secret_wql\n            s_handle = await non_secrets.open_wallet_search(\n                self.handle,\n                typ,\n                json.dumps(canon(filt or {})),\n                json.dumps({\n                    'retrieveRecords': True,\n                    'retrieveTotalCount': True,\n                    'retrieveType': False,\n                    'retrieveValue': True,\n                    'retrieveTags': True\n                }))\n\n            records = []\n            cardinality = int(json.loads(\n                await non_secrets.fetch_wallet_search_next_records(self.handle, s_handle, 0))['totalCount'])\n            chunk = min(cardinality, limit or cardinality, Wallet.DEFAULT_CHUNK)\n            if limit:\n                cardinality = min(limit, cardinality)\n            try:\n                while len(records) != cardinality:\n                    batch = json.loads(\n                        await non_secrets.fetch_wallet_search_next_records(self.handle, s_handle, chunk))['records']\n                    records.extend(batch)\n                    if len(batch) < chunk:\n                        break\n                if len(records) != cardinality:\n                    LOGGER.warning(\n                        'Non-secret search/limit indicated %s results but fetched %s',\n                        cardinality,\n                        len(records))\n            finally:\n                await non_secrets.close_wallet_search(s_handle)\n\n        rv = {record['id']: StorageRecord(typ, record['value'], record['tags'], record['id']) for record in records}\n        LOGGER.debug('Wallet.get_non_secret <<< %s', rv)\n        return rv", "response": "Get non - secret storage records for the specified type and filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencrypting plaintext for owner of DID anonymously or via authenticated encryption scheme.", "response": "async def encrypt(\n            self,\n            message: bytes,\n            authn: bool = False,\n            to_verkey: str = None,\n            from_verkey: str = None) -> bytes:\n        \"\"\"\n        Encrypt plaintext for owner of DID, anonymously or via authenticated encryption scheme.\n        Raise AbsentMessage for missing message, or WalletState if wallet is closed.\n\n        :param message: plaintext, as bytes\n        :param authn: whether to use authenticated encryption scheme\n        :param to_verkey: verification key of recipient, None for anchor's own\n        :param from_verkey: verification key of sender for authenticated encryption, None for anchor's own\n        :return: ciphertext, as bytes\n        \"\"\"\n\n        LOGGER.debug(\n            'Wallet.encrypt >>> message: %s, authn: %s, to_verkey: %s, from_verkey: %s',\n            message,\n            authn,\n            to_verkey,\n            from_verkey)\n\n        if not message:\n            LOGGER.debug('Wallet.encrypt <!< No message to encrypt')\n            raise AbsentMessage('No message to encrypt')\n\n        if not self.handle:\n            LOGGER.debug('Wallet.encrypt <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if authn:\n            rv = await crypto.auth_crypt(self.handle, from_verkey or self.verkey, to_verkey or self.verkey, message)\n        else:\n            rv = await crypto.anon_crypt(to_verkey or self.verkey, message)\n\n        LOGGER.debug('Wallet.auth_encrypt <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def pack(\n            self,\n            message: str,\n            recip_verkeys: Union[str, Sequence[str]] = None,\n            sender_verkey: str = None) -> bytes:\n        \"\"\"\n        Pack a message for one or more recipients (default anchor only).\n        Raise AbsentMessage for missing message, or WalletState if wallet is closed.\n\n        :param message: message to pack\n        :param recip_verkeys: verification keys of recipients (default anchor's own, only)\n        :param sender_verkey: sender verification key (default anonymous encryption)\n        :return: packed message\n        \"\"\"\n\n        LOGGER.debug(\n            'Wallet.pack >>> message: %s, recip_verkeys: %s, sender_verkey: %s',\n            message,\n            recip_verkeys,\n            sender_verkey)\n\n        if message is None:\n            LOGGER.debug('Wallet.pack <!< No message to pack')\n            raise AbsentMessage('No message to pack')\n\n        rv = await crypto.pack_message(\n            self.handle,\n            message,\n            [recip_verkeys] if isinstance(recip_verkeys, str) else list(recip_verkeys or [self.verkey]),\n            sender_verkey)\n\n        LOGGER.debug('Wallet.pack <<< %s', rv)\n        return rv", "response": "Pack a message for one or more recipients."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate new verification key. Raise WalletState if wallet is closed.", "response": "async def reseed_init(self, next_seed: str = None) -> str:\n        \"\"\"\n        Begin reseed operation: generate new key. Raise WalletState if wallet is closed.\n\n        :param next_seed: incoming replacement seed (default random)\n        :return: new verification key\n        \"\"\"\n\n        LOGGER.debug('Wallet.reseed_init >>> next_seed: [SEED]')\n\n        if not self.handle:\n            LOGGER.debug('Wallet.reseed_init <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        rv = await did.replace_keys_start(self.handle, self.did, json.dumps({'seed': next_seed} if next_seed else {}))\n        LOGGER.debug('Wallet.reseed_init <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn new DIDInfo with new verification key and metadata for DID.", "response": "async def reseed_apply(self) -> DIDInfo:\n        \"\"\"\n        Replace verification key with new verification key from reseed operation.\n        Raise WalletState if wallet is closed.\n\n        :return: DIDInfo with new verification key and metadata for DID\n        \"\"\"\n\n        LOGGER.debug('Wallet.reseed_apply >>>')\n\n        if not self.handle:\n            LOGGER.debug('Wallet.reseed_init <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        await did.replace_keys_apply(self.handle, self.did)\n        self.verkey = await did.key_for_local_did(self.handle, self.did)\n        now = int(time())\n        rv = DIDInfo(self.did, self.verkey, {'anchor': True, 'since': now, 'modified': now})\n        await did.set_did_metadata(self.handle, self.did, json.dumps(rv.metadata))\n\n        LOGGER.info('Wallet %s set seed hash metadata for DID %s', self.name, self.did)\n\n        LOGGER.debug('Wallet.reseed_apply <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend schema to ledger and retrieve it if it does not exist.", "response": "async def send_schema(self, schema_data_json: str) -> str:\n        \"\"\"\n        Send schema to ledger, then retrieve it as written to the ledger and return it.\n        Raise BadLedgerTxn on failure. Raise BadAttribute for attribute name with spaces or\n        reserved for indy-sdk.\n\n        If schema already exists on ledger, log error and return schema.\n\n        :param schema_data_json: schema data json with name, version, attribute names; e.g.,\n\n        ::\n\n            {\n                'name': 'my-schema',\n                'version': '1.234',\n                'attr_names': ['favourite_drink', 'height', 'last_visit_date']\n            }\n\n        :return: schema json as written to ledger (or existed a priori)\n        \"\"\"\n\n        LOGGER.debug('Origin.send_schema >>> schema_data_json: %s', schema_data_json)\n\n        schema_data = json.loads(schema_data_json)\n        for attr in schema_data['attr_names']:\n            if not (re.match(r'(?=[^- ])[-_a-zA-Z0-9 ]+(?<=[^- ])$', attr)) or attr.strip().lower() == 'hash':\n                LOGGER.debug('Origin.send_schema <!< Bad attribute name [%s]', attr)\n                raise BadAttribute('Bad attribute name [{}]'.format(attr))\n\n        s_id = schema_id(self.did, schema_data['name'], schema_data['version'])\n        s_key = schema_key(s_id)\n        rv_json = None\n        with SCHEMA_CACHE.lock:\n            try:\n                rv_json = await self.get_schema(s_key)\n                LOGGER.error(\n                    'Schema %s version %s already exists on ledger for origin-did %s: not sending',\n                    schema_data['name'],\n                    schema_data['version'],\n                    self.did)\n            except AbsentSchema:  # OK - about to create and send it\n                (_, schema_json) = await anoncreds.issuer_create_schema(\n                    self.did,\n                    schema_data['name'],\n                    schema_data['version'],\n                    json.dumps(schema_data['attr_names']))\n                req_json = await ledger.build_schema_request(self.did, schema_json)\n                await self._sign_submit(req_json)\n\n                for _ in range(16):  # reasonable timeout\n                    try:\n                        rv_json = await self.get_schema(s_key)  # adds to cache\n                        break\n                    except AbsentSchema:\n                        await sleep(1)\n                        LOGGER.info('Sent schema %s to ledger, waiting 1s for its appearance', s_id)\n\n                if not rv_json:\n                    LOGGER.debug('Origin.send_schema <!< timed out waiting on sent schema %s', s_id)\n                    raise BadLedgerTxn('Timed out waiting on sent schema {}'.format(s_id))\n\n        LOGGER.debug('Origin.send_schema <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the indy - sdk null role for a tails sync anchor which does not need write access.", "response": "def least_role() -> Role:\n        \"\"\"\n        Return the indy-sdk null role for a tails sync anchor, which does not need write access.\n\n        :return: USER role\n        \"\"\"\n\n        LOGGER.debug('NominalAnchor.least_role >>>')\n\n        rv = Role.USER\n\n        LOGGER.debug('NominalAnchor.least_role <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the current object.", "response": "async def close(self) -> None:\n        \"\"\"\n        Explicit exit. If so configured, populate cache to prove for any creds on schemata,\n        cred defs, and rev regs marked of interest in configuration at initialization,\n        archive cache, and purge prior cache archives.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('OrgHubAnchor.close >>>')\n\n        archive_caches = False\n        if self.config.get('archive-holder-prover-caches-on-close', False):\n            archive_caches = True\n            await self.load_cache_for_proof(False)\n        if self.config.get('archive-verifier-caches-on-close', {}):\n            archive_caches = True\n            await self.load_cache_for_verification(False)\n        if archive_caches:\n            ArchivableCaches.archive(self.dir_cache)\n            ArchivableCaches.purge_archives(self.dir_cache, True)\n\n        # Do not close wallet independently: allow for sharing open wallet over many anchor lifetimes\n        # await self.wallet.close() #1.7.8\n        # Do not close pool independently: let relying party decide when to go on-line and off-line\n\n        for path_rr_id in Tails.links(self._dir_tails):\n            rr_id = basename(path_rr_id)\n            try:\n                await HolderProver._sync_revoc_for_proof(self, rr_id)\n            except ClosedPool:\n                LOGGER.warning('OrgHubAnchor sync-revoc on close required ledger for %s but pool was closed', rr_id)\n\n        LOGGER.debug('OrgHubAnchor.close <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef canon_cred_wql(query: dict) -> dict:\n\n    for k in [qk for qk in query]:  # copy: iteration alters query keys\n        attr_match = re.match('attr::([^:]+)::(marker|value)$', k)\n        if isinstance(query[k], dict):  # only subqueries are dicts: recurse\n            query[k] = canon_cred_wql(query[k])\n        if k == '$or':\n            if not isinstance(query[k], list):\n                raise BadWalletQuery('Bad WQL; $or value must be a list in {}'.format(json.dumps(query)))\n            query[k] = [canon_cred_wql(subq) for subq in query[k]]\n        if attr_match:\n            qkey = 'attr::{}::{}'.format(canon(attr_match.group(1)), canon(attr_match.group(2)))\n            query[qkey] = query.pop(k)\n            tag_value = query[qkey]\n            if isinstance(tag_value, dict) and len(tag_value) == 1:\n                if '$in' in tag_value:\n                    tag_value['$in'] = [raw(val) for val in tag_value.pop('$in')]\n                else:\n                    wql_op = set(tag_value.keys()).pop()  # $neq, $gt, $gte, etc.\n                    tag_value[wql_op] = raw(tag_value[wql_op])\n            else:  # equality\n                query[qkey] = raw(query[qkey])\n\n    return query", "response": "Canonicalize WQL query to get all the keys that match the input to indy - sdk wallet credential filtration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef canon_pairwise_wql(query: dict = None) -> dict:\n\n    if not query:\n        return {\n            '~their_did': {\n                '$neq': ''\n            }\n        }\n\n    for k in [qk for qk in query]:  # copy: iteration alters query keys\n        if isinstance(query[k], dict):  # only subqueries are dicts: recurse\n            query[k] = canon_pairwise_wql(query[k])\n        if k == '$or':\n            if not isinstance(query[k], list):\n                raise BadWalletQuery('Bad WQL; $or value must be a list in {}'.format(json.dumps(query)))\n            query[k] = [canon_pairwise_wql(subq) for subq in query[k]]\n        elif k == '$not':\n            query[k] = canon_pairwise_wql(query.pop(k))\n        elif k not in WQL_1_OPS:\n            qkey = canon_pairwise_tag(k)\n            query[qkey] = query.pop(k)\n            tag_value = query[qkey]\n            if isinstance(tag_value, dict) and len(tag_value) == 1:\n                if '$in' in tag_value:\n                    tag_value['$in'] = [raw(val) for val in tag_value['$in']]\n                else:\n                    wql_op = set(tag_value.keys()).pop()  # $neq, $gt, $gt, etc.\n                    tag_value[wql_op] = raw(tag_value[wql_op])\n            else:\n                query[qkey] = raw(query.pop(qkey))\n\n    return query", "response": "Canonicalize WQL tags to unencrypted storage specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_requirements(filename):\n\n    try:\n        with open(filename) as fh_req:\n            return [line.strip() for line in fh_req if line.strip() and not line.startswith('#')]\n    except FileNotFoundError:\n        print('File not found: {}'.format(realpath(filename)), file=stderr)\n        raise", "response": "Load a pip requirements file and return a list of strings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def add_config(self, name: str, genesis: str = None) -> None:\n\n        LOGGER.debug('NodePoolManager.__init__ >>> name: %s, genesis: %s', name, genesis)\n\n        if name in await self.list():\n            LOGGER.debug('NodePoolManager.add_config: <!< Node pool %s configuration already present', name)\n            raise ExtantPool('Node pool {} configuration already present'.format(name))\n\n        genesis_tmp = None\n        path_gen = realpath(expanduser(expandvars(genesis)))\n        try:\n            if not isfile(path_gen):\n                genesis_tmp = NamedTemporaryFile(mode='w+b', buffering=0, delete=False)\n                with genesis_tmp:\n                    genesis_tmp.write(genesis.encode())\n            await pool.create_pool_ledger_config(\n                name,\n                json.dumps({\n                    'genesis_txn': path_gen if isfile(path_gen) else genesis_tmp.name\n                }))\n        finally:\n            if genesis_tmp:\n                remove(genesis_tmp.name)\n\n        LOGGER.debug('NodePoolManager.__init__ <<<')", "response": "Add node pool ledger configuration to indy home directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def list(self) -> List[str]:\n\n        LOGGER.debug('NodePoolManager.list >>>')\n\n        rv = [p['pool'] for p in await pool.list_pools()]\n\n        LOGGER.debug('NodePoolManager.list <<< %s', rv)\n        return rv", "response": "Return list of pool names configured empty list for none."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, name: str, config: dict = None) -> NodePool:\n\n        LOGGER.debug('NodePoolManager.node_pool >>>')\n\n        rv = NodePool(name, self.protocol, config)\n\n        LOGGER.debug('NodePoolManager.node_pool <<< %s', rv)\n        return rv", "response": "Return node pool in input name and optional configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove serialized pool info if it exists. Abstain from removing open node pool.", "response": "async def remove(self, name: str) -> None:\n        \"\"\"\n        Remove serialized pool info if it exists. Abstain from removing open node pool.\n        \"\"\"\n\n        LOGGER.debug('NodePoolManager.remove >>> name: %s', name)\n\n        try:\n            await pool.delete_pool_ledger_config(name)\n        except IndyError as x_indy:\n            LOGGER.info('Abstaining from node pool removal; indy-sdk error code %s', x_indy.error_code)\n\n        LOGGER.debug('NodePool.remove <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authnkey(self) -> dict:\n\n        return {k: self._pubkey[k] for k in self._pubkey if self._pubkey[k].authn}", "response": "Return a dict of public keys marked as authentication keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, item: Union[Service, PublicKey]) -> 'DIDDoc':\n\n        if isinstance(item, Service):\n            self.service[item.id] = item\n        elif isinstance(item, PublicKey):\n            self.pubkey[item.id] = item\n        else:\n            raise BadDIDDocItem('Cannot add item {} to DIDDoc on DID {}'.format(item, self.did))", "response": "Add or replace service or public key to current DIDDoc."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize(self) -> str:\n\n        return {\n            '@context': DIDDoc.CONTEXT,\n            'id': canon_ref(self.did, self.did),\n            'publicKey': [pubkey.to_dict() for pubkey in self.pubkey.values()],\n            'authentication': [{\n                'type': pubkey.type.authn_type,\n                'publicKey': canon_ref(self.did, pubkey.id)\n            } for pubkey in self.pubkey.values() if pubkey.authn],\n            'service': [service.to_dict() for service in self.service.values()]\n        }", "response": "Dump current object to a JSON - compatible dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_service_pubkeys(self, service: dict, tags: Union[Sequence[str], str]) -> List[PublicKey]:\n\n        rv = []\n        for tag in [tags] if isinstance(tags, str) else list(tags):\n\n            for svc_key in service.get(tag, {}):\n                canon_key = canon_ref(self.did, svc_key)\n                pubkey = None\n\n                if '#' in svc_key:\n                    if canon_key in self.pubkey:\n                        pubkey = self.pubkey[canon_key]\n                    else:  # service key refers to another DID doc\n                        LOGGER.debug(\n                            'DIDDoc.add_service_pubkeys <!< DID document %s has no public key %s',\n                            self.did,\n                            svc_key)\n                        raise AbsentDIDDocItem('DID document {} has no public key {}'.format(self.did, svc_key))\n                else:\n                    for existing_pubkey in self.pubkey.values():\n                        if existing_pubkey.value == svc_key:\n                            pubkey = existing_pubkey\n                            break\n                    else:\n                        pubkey = PublicKey(\n                            self.did,\n                            ident=svc_key[-9:-1],  # industrial-grade uniqueness\n                            value=svc_key)\n                        self._pubkey[pubkey.id] = pubkey\n\n                if pubkey and pubkey not in rv:  # perverse case: could specify same key multiple ways; append once\n                    rv.append(pubkey)\n\n        return rv", "response": "Add public keys specified in service. Return list of public keys so discovered. Raise AbsentDIDDocItem for public key reference not present in DID document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct DIDDoc object from dict representation.", "response": "def deserialize(cls, did_doc: dict) -> 'DIDDoc':\n        \"\"\"\n        Construct DIDDoc object from dict representation.\n\n        Raise BadIdentifier for bad DID.\n\n        :param did_doc: DIDDoc dict reprentation.\n        :return: DIDDoc from input json.\n        \"\"\"\n\n        rv = None\n        if 'id' in did_doc:\n            rv = DIDDoc(did_doc['id'])\n        else:  # get DID to serve as DID document identifier from first public key\n            if 'publicKey' not in did_doc:\n                LOGGER.debug('DIDDoc.deserialize <!< no identifier in DID document')\n                raise AbsentDIDDocItem('No identifier in DID document')\n            for pubkey in did_doc['publicKey']:\n                pubkey_did = canon_did(resource(pubkey['id']))\n                if ok_did(pubkey_did):\n                    rv = DIDDoc(pubkey_did)\n                    break\n            else:\n                LOGGER.debug('DIDDoc.deserialize <!< no identifier in DID document')\n                raise AbsentDIDDocItem('No identifier in DID document')\n\n        for pubkey in did_doc['publicKey']:  # include public keys and authentication keys by reference\n            pubkey_type = PublicKeyType.get(pubkey['type'])\n            authn = any(\n                canon_ref(rv.did, ak.get('publicKey', '')) == canon_ref(rv.did, pubkey['id'])\n                for ak in did_doc.get('authentication', {}) if isinstance(ak.get('publicKey', None), str))\n            key = PublicKey(  # initialization canonicalizes id\n                rv.did,\n                pubkey['id'],\n                pubkey[pubkey_type.specifier],\n                pubkey_type,\n                canon_did(pubkey['controller']),\n                authn)\n            rv.pubkey[key.id] = key\n\n        for akey in did_doc.get('authentication', {}):  # include embedded authentication keys\n            pk_ref = akey.get('publicKey', None)\n            if pk_ref:\n                pass  # got it already with public keys\n            else:\n                pubkey_type = PublicKeyType.get(akey['type'])\n                key = PublicKey(  # initialization canonicalized id\n                    rv.did,\n                    akey['id'],\n                    akey[pubkey_type.specifier],\n                    pubkey_type,\n                    canon_did(akey['controller']),\n                    True)\n                rv.pubkey[key.id] = key\n\n        for service in did_doc.get('service', {}):\n            endpoint = service['serviceEndpoint']\n            svc = Service(  # initialization canonicalizes id\n                rv.did,\n                service.get('id', canon_ref(rv.did, 'assigned-service-{}'.format(len(rv.service)), ';')),\n                service['type'],\n                rv.add_service_pubkeys(service, 'recipientKeys'),\n                rv.add_service_pubkeys(service, ['mediatorKeys', 'routingKeys']),\n                canon_ref(rv.did, endpoint, ';') if ';' in endpoint else endpoint,\n                service.get('priority', None))\n            rv.service[svc.id] = svc\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(version: str) -> 'Protocol':\n\n        return Protocol.V_13 if version == Protocol.V_13.value.name else Protocol.DEFAULT", "response": "Get the class corresponding to the input version value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the tag for the current version of node protocol.", "response": "def cd_id_tag(self, for_box_id: bool = False) -> str:\n        \"\"\"\n        Return (place-holder) credential definition identifier tag for current version of node protocol.\n        At present, von_anchor always uses the tag of 'tag' if the protocol calls for one.\n\n        :param for_box_id: whether to prefix a colon, if current protocol uses one, in constructing\n            a cred def id or rev reg id.\n        :return: cred def id tag\n        \"\"\"\n\n        if for_box_id:\n            return '' if self == Protocol.V_13 else ':tag'\n        return 'tag'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the identifier for input issuer DID and schema sequence number.", "response": "def cred_def_id(self, issuer_did: str, schema_seq_no: int) -> str:\n        \"\"\"\n        Return credential definition identifier for input issuer DID and schema sequence number.\n\n        :param issuer_did: DID of credential definition issuer\n        :param schema_seq_no: schema sequence number\n        :return: credential definition identifier\n        \"\"\"\n\n        return '{}:3:CL:{}{}'.format(  # 3 marks indy cred def id, CL is sig type\n            issuer_did,\n            schema_seq_no,\n            self.cd_id_tag(True))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef txn_data2schema_key(self, txn: dict) -> SchemaKey:\n\n        rv = None\n        if self == Protocol.V_13:\n            rv = SchemaKey(txn['identifier'], txn['data']['name'], txn['data']['version'])\n        else:\n            txn_txn = txn.get('txn', None) or txn  # may have already run this txn through txn2data() below\n            rv = SchemaKey(\n                txn_txn['metadata']['from'],\n                txn_txn['data']['data']['name'],\n                txn_txn['data']['data']['version'])\n\n        return rv", "response": "Return schema key from ledger transaction data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef txn2data(self, txn: dict) -> str:\n\n        rv_json = json.dumps({})\n        if self == Protocol.V_13:\n            rv_json = json.dumps(txn['result'].get('data', {}))\n        else:\n            rv_json = json.dumps((txn['result'].get('data', {}) or {}).get('txn', {}))  # \"data\": null for no such txn\n\n        return rv_json", "response": "Given ledger transaction return its data json."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives ledger transaction return its epoch time.", "response": "def txn2epoch(self, txn: dict) -> int:\n        \"\"\"\n        Given ledger transaction, return its epoch time.\n\n        :param txn: transaction as dict\n        :return: transaction time\n        \"\"\"\n\n        rv = None\n        if self == Protocol.V_13:\n            rv = txn['result']['txnTime']\n        else:\n            rv = txn['result']['txnMetadata']['txnTime']\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a genesis transaction return its node host and port.", "response": "def genesis_host_port(self, genesis_txn: dict) -> tuple:\n        \"\"\"\n        Given a genesis transaction, return its node host and port.\n\n        :param genesis_txn: genesis transaction as dict\n        :return: node host and port\n        \"\"\"\n\n        txn_data = genesis_txn['data'] if self == Protocol.V_13 else genesis_txn['txn']['data']['data']\n        return (txn_data['node_ip'], txn_data['node_port'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen reader handle and return current object.", "response": "async def open(self) -> 'Tails':\n        \"\"\"\n        Open reader handle and return current object.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Tails.open >>>')\n\n        self._reader_handle = await blob_storage.open_reader('default', self._tails_config_json)\n\n        LOGGER.debug('Tails.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether input token looks like a valid tails hash.", "response": "def ok_hash(token: str) -> bool:\n        \"\"\"\n        Whether input token looks like a valid tails hash.\n\n        :param token: candidate string\n        :return: whether input token looks like a valid tails hash\n        \"\"\"\n\n        LOGGER.debug('Tails.ok_hash >>> token: %s', token)\n\n        rv = re.match('[{}]{{42,44}}$'.format(B58), token) is not None\n        LOGGER.debug('Tails.ok_hash <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassociating tails file with a revocation registry id.", "response": "def associate(base_dir: str, rr_id: str, tails_hash: str) -> None:\n        \"\"\"\n        Create symbolic link to tails file named tails_hash for rev reg id rr_id.\n\n        :param rr_id: rev reg id\n        :param tails_hash: hash of tails file, serving as file name\n        \"\"\"\n\n        LOGGER.debug('Tails.associate >>> base_dir: %s, rr_id: %s, tails_hash: %s', base_dir, rr_id, tails_hash)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Tails.associate <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        if not Tails.ok_hash(tails_hash):\n            LOGGER.debug('Tails.associate <!< Bad tails hash %s', tails_hash)\n            raise BadIdentifier('Bad tails hash {}'.format(tails_hash))\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        directory = join(base_dir, cd_id)\n        cwd = getcwd()\n        makedirs(directory, exist_ok=True)\n        chdir(directory)\n        symlink(tails_hash, rr_id)\n        chdir(cwd)\n\n        LOGGER.debug('Tails.associate <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dir(base_dir: str, rr_id: str) -> str:\n\n        LOGGER.debug('Tails.dir >>> base_dir: %s, rr_id: %s', base_dir, rr_id)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Tails.dir <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        rv = join(base_dir, rev_reg_id2cred_def_id(rr_id))\n        LOGGER.debug('Tails.dir <<< %s', rv)\n        return rv", "response": "Return correct subdirectory of input base dir for tails files corresponding to input rev reg id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the path to the tails file associated with a revocation registry identifier.", "response": "def linked(base_dir: str, rr_id: str) -> str:\n        \"\"\"\n        Get, from the specified directory, the path to the tails file associated with\n        the input revocation registry identifier, or None for no such file.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param rr_id: rev reg id\n        :return: (stringified) path to tails file of interest, or None for no such file.\n        \"\"\"\n\n        LOGGER.debug('Tails.linked >>> base_dir: %s, rr_id: %s', base_dir, rr_id)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Tails.linked <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        link = join(base_dir, cd_id, rr_id)\n\n        rv = join(base_dir, cd_id, readlink(link)) if islink(link) else None\n        LOGGER.debug('Tails.linked <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning set of all symbolic links in specified base tails directory recursively.", "response": "def links(base_dir: str, issuer_did: str = None) -> set:\n        \"\"\"\n        Return set of all paths to symbolic links (rev reg ids) associating their\n        respective tails files, in specified base tails directory recursively\n        (omitting the .hopper subdirectory), on input issuer DID if specified.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param issuer_did: issuer DID of interest\n        :return: set of paths to symbolic links associating tails files\n        \"\"\"\n\n        LOGGER.debug('Tails.links >>> base_dir: %s, issuer_did: %s', base_dir, issuer_did)\n\n        if issuer_did and not ok_did(issuer_did):\n            LOGGER.debug('Tails.links <!< Bad DID %s', issuer_did)\n            raise BadIdentifier('Bad DID {}'.format(issuer_did))\n\n        rv = set()\n        for dir_path, dir_names, file_names in walk(base_dir, topdown=True):\n            dir_names[:] = [d for d in dir_names if not d.startswith('.')]\n            for file_name in file_names:\n                if islink(join(dir_path, file_name)) and (not issuer_did or ok_rev_reg_id(file_name, issuer_did)):\n                    rv.add(join(dir_path, file_name))\n\n        LOGGER.debug('Tails.links <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all tails files in specified base directory and without symbolic links associating .", "response": "def unlinked(base_dir: str) -> set:\n        \"\"\"\n        Return all paths to tails files, in specified tails base directory recursively\n        (omitting the .hopper subdirectory), without symbolic links associating\n        revocation registry identifiers.\n\n        At an Issuer, tails files should not persist long without revocation registry identifier\n        association via symbolic link. At a HolderProver, a newly downloaded tails file stays\n        unlinked until the anchor stores a credential or creates a proof needing it, or else the\n        anchor restarts.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :return: set of paths to tails files with no local symbolic links to them\n        \"\"\"\n\n        LOGGER.debug('Tails.unlinked >>> base_dir: %s', base_dir)\n\n        rv = set()\n        for dir_path, dir_names, file_names in walk(base_dir, topdown=True):\n            dir_names[:] = [d for d in dir_names if not d.startswith('.')]\n            for file_name in file_names:\n                if isfile(join(dir_path, file_name)) and Tails.ok_hash(file_name):\n                    rv.add(join(dir_path, file_name))\n        rv -= {join(dirname(path_link), readlink(path_link)) for path_link in Tails.links(base_dir)}\n\n        LOGGER.debug('Tails.unlinked <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn next available tag name for a new credential definition identifier.", "response": "def next_tag(base_dir: str, cd_id: str) -> (str, int):\n        \"\"\"\n        Return the next tag name available for a new rev reg id on input cred def id in base directory,\n        and suggested size of associated rev reg.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param cd_id: credential definition identifier of interest\n        :return: stringified least non-negative integer not yet used in a rev reg id associated with a tails file\n            in base directory, and recommendation for next size to use\n        \"\"\"\n\n        LOGGER.debug('Tails.next_tag >>> base_dir: %s, cd_id: %s', base_dir, cd_id)\n\n        if not ok_cred_def_id(cd_id):\n            LOGGER.debug('Tails.next_tag <!< Bad cred def id %s', cd_id)\n            raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n        tag = 1 + max([int(rev_reg_id2tag(basename(f)))\n            for f in Tails.links(base_dir) if cd_id in basename(f)] + [-1])  # -1: next tag is '0' if no tags so far\n        size = min(2**(tag + 6), Tails.MAX_SIZE)\n\n        rv = (tag, size)\n        LOGGER.debug('Tails.next_tag <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_rev_reg_id(base_dir: str, cd_id: str) -> str:\n\n        LOGGER.debug('Tails.current_rev_reg_id >>> base_dir: %s, cd_id: %s', base_dir, cd_id)\n\n        if not ok_cred_def_id(cd_id):\n            LOGGER.debug('Tails.current_rev_reg_id <!< Bad cred def id %s', cd_id)\n            raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n        tags = [int(rev_reg_id2tag(basename(f))) for f in Tails.links(base_dir)\n            if cd_id in basename(f)]\n        if not tags:\n            raise AbsentTails('No tails files present for cred def id {}'.format(cd_id))\n\n        rv = rev_reg_id(cd_id, str(max(tags)))  # ensure 10 > 9, not '9' > '10'\n        LOGGER.debug('Tails.current_rev_reg_id <<< %s', rv)\n        return rv", "response": "Return revocation registry identifier for input credential definition identifier."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path(self) -> str:\n\n        config = json.loads(self._tails_config_json)\n        return join(config['base_dir'], config['file'])", "response": "Returns the path to the current tails file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a credential attribute value.", "response": "def encode(orig: Any) -> str:\n    \"\"\"\n    Encode credential attribute value, purely stringifying any int32 and leaving numeric int32 strings alone,\n    but mapping any other input to a stringified 256-bit (but not 32-bit) integer. Predicates in indy-sdk operate\n    on int32 values properly only when their encoded values match their raw values.\n\n    :param orig: original value to encode\n    :return: encoded value\n    \"\"\"\n\n    if isinstance(orig, int) and -I32_BOUND <= orig < I32_BOUND:\n        return str(int(orig))  # python bools are ints\n\n    try:\n        i32orig = int(str(orig))  # don't encode floats as ints\n        if -I32_BOUND <= i32orig < I32_BOUND:\n            return str(i32orig)\n    except (ValueError, TypeError):\n        pass\n\n    rv = int.from_bytes(sha256(raw(orig).encode()).digest(), 'big')\n    while -I32_BOUND <= rv < I32_BOUND:\n        rv = int.from_bytes(sha256(rv.encode()).digest(), 'big')  # sha256 maps no 32-bit int to another: terminates\n\n    return str(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(relation: str) -> 'Predicate':\n\n        for pred in Predicate:\n            if relation.upper() in (pred.value.fortran, pred.value.wql.upper(), pred.value.math):\n                return pred\n        return None", "response": "Returns enum instance corresponding to input relation string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_int(value: Any) -> int:\n\n        if isinstance(value, (bool, int)):\n            return int(value)\n        return int(str(value))", "response": "Cast a value as its equivalent int for indy predicate argument. Raise ValueError for any input but\n int or boolean. Raise ValueError for any input but\n int or boolean. Raise ValueError for any input but\n int or boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(token: Union[str, int] = None) -> 'Role':\n\n        if token is None:\n            return Role.USER\n\n        for role in Role:\n            if role == Role.ROLE_REMOVE:\n                continue  # ROLE_REMOVE is not a sensible role to parse from any configuration\n            if isinstance(token, int) and token in role.value:\n                return role\n            if str(token).upper() == role.name or token in (str(v) for v in role.value):  # could be numeric string\n                return role\n\n        return None", "response": "Return the appropriate role for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef token(self) -> str:\n\n        return self.value[0] if self in (Role.USER, Role.ROLE_REMOVE) else self.name", "response": "Return token identifying role to indy - sdk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cached_get(timeout, *params):\n\n    def decorator(view_func):\n        @wraps(view_func, assigned=available_attrs(view_func))\n        def _wrapped_view(view_or_request, *args, **kwargs):\n\n            # The type of the request gets muddled when using a function based\n            # decorator. We must use a function based decorator so it can be\n            # used in urls.py.\n            request = getattr(view_or_request, \"request\", view_or_request)\n\n            if not hasattr(_thread_locals, \"ultracache_request\"):\n                setattr(_thread_locals, \"ultracache_request\", request)\n\n            # If request not GET or HEAD never cache\n            if request.method.lower() not in (\"get\", \"head\"):\n                return view_func(view_or_request, *args, **kwargs)\n\n            # If request contains messages never cache\n            l = 0\n            try:\n                l = len(request._messages)\n            except (AttributeError, TypeError):\n                pass\n            if l:\n                return view_func(view_or_request, *args, **kwargs)\n\n            # Compute a cache key\n            li = [str(view_or_request.__class__), view_func.__name__]\n\n            # request.get_full_path is implicitly added it no other request\n            # path is provided. get_full_path includes the querystring and is\n            # the more conservative approach but makes it trivially easy for a\n            # request to bust through the cache.\n            if not set(params).intersection(set((\n                \"request.get_full_path()\", \"request.path\", \"request.path_info\"\n            ))):\n                li.append(request.get_full_path())\n\n            if \"django.contrib.sites\" in settings.INSTALLED_APPS:\n                li.append(get_current_site_pk(request))\n\n            # Pre-sort kwargs\n            keys = list(kwargs.keys())\n            keys.sort()\n            for key in keys:\n                li.append(\"%s,%s\" % (key, kwargs[key]))\n\n            # Extend cache key with custom variables\n            for param in params:\n                if not isinstance(param, str):\n                    param = str(param)\n                li.append(eval(param))\n\n            s = \":\".join([str(l) for l in li])\n            hashed = hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n            cache_key = \"ucache-get-%s\" % hashed\n            cached = cache.get(cache_key, None)\n            if cached is None:\n                # The get view as outermost caller may bluntly set _ultracache\n                request._ultracache = []\n                response = view_func(view_or_request, *args, **kwargs)\n                content = None\n                if isinstance(response, TemplateResponse):\n                    content = response.render().rendered_content\n                elif isinstance(response, HttpResponse):\n                    content = response.content\n                if content is not None:\n                    headers = getattr(response, \"_headers\", {})\n                    cache.set(\n                        cache_key,\n                        {\"content\": content, \"headers\": headers},\n                        timeout\n                    )\n                    cache_meta(request, cache_key)\n            else:\n                response = HttpResponse(cached[\"content\"])\n                # Headers has a non-obvious format\n                for k, v in cached[\"headers\"].items():\n                    response[v[0]] = v[1]\n\n            return response\n\n        return _wrapped_view\n    return decorator", "response": "Decorator applied specifically to a view s get method."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms ancestor opening operations and synchronize revocation registry to tails tree content.", "response": "async def open(self) -> 'Issuer':\n        \"\"\"\n        Explicit entry. Perform ancestor opening operations,\n        then synchronize revocation registry to tails tree content.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Issuer.open >>>')\n\n        await super().open()\n        for path_rr_id in Tails.links(self.dir_tails, self.did):\n            await self._sync_revoc_for_issue(basename(path_rr_id))\n\n        LOGGER.debug('Issuer.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _send_rev_reg_def(self, rr_id: str) -> None:\n\n        LOGGER.debug('Issuer._send_rev_reg_def >>> rr_id: %s', rr_id)\n\n        dir_tails_rr_id = self.rrb.dir_tails_top(rr_id)\n        dir_target = self.rrb.dir_tails_target(rr_id)\n\n        if not Tails.linked(dir_tails_rr_id, rr_id):\n            LOGGER.debug(\n                'Issuer._send_rev_reg_def <!< Tails file for rev reg %s not ready in dir %s',\n                rr_id,\n                dir_target)\n            raise AbsentRevReg('Tails file for rev reg {} not ready in dir {}'.format(rr_id, dir_target))\n\n        file_rr_def = join(dir_target, 'rr_def.json')\n        if not isfile(file_rr_def):\n            LOGGER.debug('Issuer._send_rev_reg_def <!< Rev reg def file %s not present', file_rr_def)\n            raise AbsentRevReg('Rev reg def file {} not present'.format(file_rr_def))\n        with open(file_rr_def, 'r') as fh_rr_def:\n            rr_def_json = fh_rr_def.read()\n\n        file_rr_ent = join(dir_target, 'rr_ent.json')\n        if not isfile(file_rr_ent):\n            LOGGER.debug('Issuer._send_rev_reg_def <!< Rev reg entry file %s not present', file_rr_ent)\n            raise AbsentRevReg('Rev reg entry file {} not present'.format(file_rr_ent))\n        with open(file_rr_ent, 'r') as fh_rr_ent:\n            rr_ent_json = fh_rr_ent.read()\n\n        file_tails = Tails.linked(dir_tails_rr_id, rr_id)\n        if not file_tails:\n            LOGGER.debug('Issuer._send_rev_reg_def <!< Tails link %s not present in dir %s', rr_id, dir_target)\n            raise AbsentTails('Tails link {} not present in dir {}'.format(rr_id, dir_target))\n\n        if self.rrbx:\n            dir_cd_id = join(self.dir_tails, rev_reg_id2cred_def_id(rr_id))\n            makedirs(dir_cd_id, exist_ok=True)\n            rename(file_tails, join(dir_cd_id, basename(file_tails)))\n\n        with REVO_CACHE.lock:\n            rr_def_req_json = await ledger.build_revoc_reg_def_request(self.did, rr_def_json)\n            await self._sign_submit(rr_def_req_json)\n            await self.get_rev_reg_def(rr_id)  # add to cache en passant\n\n        rr_ent_req_json = await ledger.build_revoc_reg_entry_request(self.did, rr_id, 'CL_ACCUM', rr_ent_json)\n        await self._sign_submit(rr_ent_req_json)\n\n        if self.rrbx:\n            Tails.associate(self.dir_tails, rr_id, basename(file_tails))\n            rmtree(dir_tails_rr_id)\n        else:\n            remove(file_rr_def)\n            remove(file_rr_ent)\n\n        LOGGER.debug('Issuer._send_rev_reg_def <<<')", "response": "Sends revocation registry definition and initial entry to ledger and cache revocation registry definition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _set_rev_reg(self, rr_id: str, rr_size: int) -> None:\n\n        LOGGER.debug('Issuer._set_rev_reg >>> rr_id: %s, rr_size: %s', rr_id, rr_size)\n        assert self.rrbx\n\n        dir_hopper_rr_id = join(self.rrb.dir_tails_hopper, rr_id)\n\n        while Tails.linked(dir_hopper_rr_id, rr_id) is None:\n            await asyncio.sleep(1)\n        await self._send_rev_reg_def(rr_id)\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        (next_tag, rr_size_suggested) = Tails.next_tag(self.dir_tails, cd_id)\n        rr_id = rev_reg_id(cd_id, next_tag)\n        self.rrb.mark_in_progress(rr_id, rr_size or rr_size_suggested)\n\n        LOGGER.debug('Issuer._set_rev_reg <<<')", "response": "Set revocation registry data from hopper into place within tails directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _sync_revoc_for_issue(self, rr_id: str, rr_size: int = None) -> None:\n\n        LOGGER.debug('Issuer._sync_revoc_for_issue >>> rr_id: %s, rr_size: %s', rr_id, rr_size)\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Issuer._sync_revoc_for_issue <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        (cd_id, tag) = rev_reg_id2cred_def_id_tag(rr_id)\n\n        try:\n            await self.get_cred_def(cd_id)\n        except AbsentCredDef:\n            LOGGER.debug(\n                'Issuer._sync_revoc_for_issue <!< tails tree %s may be for another ledger; no cred def found on %s',\n                self.dir_tails,\n                cd_id)\n            raise AbsentCredDef('Tails tree {} may be for another ledger; no cred def found on {}'.format(\n                self.dir_tails,\n                cd_id))\n\n        with REVO_CACHE.lock:\n            revo_cache_entry = REVO_CACHE.get(rr_id, None)\n            tails = None if revo_cache_entry is None else revo_cache_entry.tails\n            if tails is None:  #  it's a new revocation registry, or not yet set in cache\n                try:\n                    tails = await Tails(self.dir_tails, cd_id, tag).open()\n                except AbsentTails:   # it's a new revocation registry\n                    if self.rrbx:\n                        await self._set_rev_reg(rr_id, rr_size)\n                    else:\n                        await self.rrb.create_rev_reg(rr_id, rr_size)\n                        await self._send_rev_reg_def(rr_id)\n                    tails = await Tails(self.dir_tails, cd_id, tag).open()  # symlink should exist now\n\n                if revo_cache_entry is None:\n                    REVO_CACHE[rr_id] = RevoCacheEntry(None, tails)\n                else:\n                    REVO_CACHE[rr_id].tails = tails\n\n        LOGGER.debug('Issuer._sync_revoc_for_issue <<<')", "response": "Synchronize revocation registry with tails file reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef path_tails(self, rr_id: str) -> str:\n\n        LOGGER.debug('Issuer.path_tails >>>')\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Issuer.path_tails <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        rv = Tails.linked(self.dir_tails, rr_id)\n        LOGGER.debug('Issuer.path_tails <<< %s', rv)\n        return rv", "response": "Return path to tails file for input revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _create_cred_def(self, schema: dict, ledger_cred_def: dict, revo: bool) -> (str, bool):\n\n        LOGGER.debug(\n            'Issuer._create_cred_def >>> schema: %s, ledger_cred_def: %s, revo: %s',\n            schema,\n            ledger_cred_def,\n            revo)\n\n        cred_def_json = '{}'\n        private_key_ok = True\n        try:\n            (_, cred_def_json) = await anoncreds.issuer_create_and_store_credential_def(\n                self.wallet.handle,\n                self.did,  # issuer DID\n                json.dumps(schema),\n                self.pool.protocol.cd_id_tag(False),  # expect only one cred def per schema and issuer\n                'CL',\n                json.dumps({'support_revocation': revo}))\n            if ledger_cred_def:\n                private_key_ok = False\n                LOGGER.warning(\n                    'New cred def on %s in wallet shadows existing one on ledger: private key not usable',\n                    cred_def_id(self.did, schema['seqNo'], self.pool.protocol))\n                    # carry on though, this anchor may have other capacities so public key may be good enough\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.AnoncredsCredDefAlreadyExistsError:\n                if ledger_cred_def:\n                    LOGGER.info(\n                        'Issuer wallet %s reusing existing cred def on schema %s version %s',\n                        self.name,\n                        schema['name'],\n                        schema['version'])\n                else:\n                    LOGGER.debug('Issuer._create_cred_def <!< corrupt wallet %s', self.name)\n                    raise CorruptWallet('Corrupt Issuer wallet {} has cred def on schema {} not on ledger'.format(\n                        self.name,\n                        schema['id']))\n            else:\n                LOGGER.debug(\n                    'Issuer._create_cred_def <!< cannot store cred def in wallet %s: indy error code %s',\n                    self.name,\n                    x_indy.error_code)\n                raise\n\n        rv = (cred_def_json, private_key_ok)\n        LOGGER.debug('Issuer._create_cred_def <<< %s', rv)\n        return rv", "response": "Create credential definition in wallet as part of the send_cred_def function sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a credential definition to the ledger.", "response": "async def send_cred_def(self, s_id: str, revo: bool = True, rr_size: int = None) -> str:\n        \"\"\"\n        Create a credential definition as Issuer, store it in its wallet, and send it to the ledger.\n\n        Raise CorruptWallet for wallet not pertaining to current ledger, BadLedgerTxn on failure\n        to send credential definition to ledger if need be, WalletState for closed wallet,\n        or IndyError for any other failure to create and store credential definition in wallet.\n\n        :param s_id: schema identifier\n        :param revo: whether to support revocation for cred def\n        :param rr_size: size of initial revocation registry (default as per RevRegBuilder.create_rev_reg()),\n            if revocation supported\n        :return: json credential definition as it appears on ledger\n        \"\"\"\n\n        LOGGER.debug('Issuer.send_cred_def >>> s_id: %s, revo: %s, rr_size: %s', s_id, revo, rr_size)\n\n        if not ok_schema_id(s_id):\n            LOGGER.debug('Issuer.send_cred_def <!< Bad schema id %s', s_id)\n            raise BadIdentifier('Bad schema id {}'.format(s_id))\n\n        if not self.wallet.handle:\n            LOGGER.debug('Issuer.send_cred_def <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if not self.pool:\n            LOGGER.debug('Issuer.send_cred_def <!< issuer %s has no pool', self.name)\n            raise AbsentPool('Issuer {} has no pool: cannot send cred def'.format(self.name))\n\n        rv_json = json.dumps({})\n        schema_json = await self.get_schema(schema_key(s_id))\n        schema = json.loads(schema_json)\n\n        cd_id = cred_def_id(self.did, schema['seqNo'], self.pool.protocol)\n        private_key_ok = True\n        with CRED_DEF_CACHE.lock:\n            try:\n                rv_json = await self.get_cred_def(cd_id)\n                LOGGER.info(\n                    'Cred def on schema %s version %s already exists on ledger; Issuer %s not sending another',\n                    schema['name'],\n                    schema['version'],\n                    self.name)\n            except AbsentCredDef:\n                pass  # OK - about to create, store, and send it\n\n            (cred_def_json, private_key_ok) = await self._create_cred_def(schema, json.loads(rv_json), revo)\n\n            if not json.loads(rv_json):  # checking the ledger returned no cred def: send it\n                req_json = await ledger.build_cred_def_request(self.did, cred_def_json)\n                await self._sign_submit(req_json)\n\n                for _ in range(16):  # reasonable timeout\n                    try:\n                        rv_json = await self.get_cred_def(cd_id)  # adds to cache\n                        break\n                    except AbsentCredDef:\n                        await asyncio.sleep(1)\n                        LOGGER.info('Sent cred def %s to ledger, waiting 1s for its appearance', cd_id)\n\n                if not rv_json:\n                    LOGGER.debug('Issuer.send_cred_def <!< timed out waiting on sent cred_def %s', cd_id)\n                    raise BadLedgerTxn('Timed out waiting on sent cred_def {}'.format(cd_id))\n\n                if revo:  # create new rev reg for tag '0'\n                    if self.rrbx:\n                        (_, rr_size_suggested) = Tails.next_tag(self.dir_tails, cd_id)\n                        self.rrb.mark_in_progress(rev_reg_id(cd_id, '0'), rr_size or rr_size_suggested)\n\n                    await self._sync_revoc_for_issue(rev_reg_id(cd_id, '0'), rr_size)  # sync rev reg on tag '0'\n\n        if revo and private_key_ok:\n            for tag in [str(t) for t in range(1, int(Tails.next_tag(self.dir_tails, cd_id)[0]))]:  # '1' to next-1\n                await self._sync_revoc_for_issue(rev_reg_id(cd_id, tag), rr_size if tag == '0' else None)\n\n        makedirs(join(self.dir_tails, cd_id), exist_ok=True)  # dir required for box id collection, revo or not\n\n        LOGGER.debug('Issuer.send_cred_def <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create_cred_offer(self, schema_seq_no: int) -> str:\n\n        LOGGER.debug('Issuer.create_cred_offer >>> schema_seq_no: %s', schema_seq_no)\n\n        if not self.wallet.handle:\n            LOGGER.debug('Issuer.create_cred_offer <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if not self.pool:\n            LOGGER.debug('Issuer.create_cred_offer <!< issuer %s has no pool', self.name)\n            raise AbsentPool('Issuer {} has no pool: cannot create cred offer'.format(self.name))\n\n        rv = None\n        cd_id = cred_def_id(self.did, schema_seq_no, self.pool.protocol)\n        try:\n            rv = await anoncreds.issuer_create_credential_offer(self.wallet.handle, cd_id)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletNotFoundError:\n                LOGGER.debug(\n                    'Issuer.create_cred_offer <!< did not issue cred definition from wallet %s',\n                    self.name)\n                raise CorruptWallet('Cannot create cred offer: did not issue cred definition from wallet {}'.format(\n                    self.name))\n            LOGGER.debug(\n                'Issuer.create_cred_offer <!< cannot create cred offer, indy error code %s',\n                x_indy.error_code)\n            raise\n\n        LOGGER.debug('Issuer.create_cred_offer <<< %s', rv)\n        return rv", "response": "Create credential offer for given schema sequence number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate new credential in revocation registry.", "response": "async def create_cred(\n            self,\n            cred_offer_json,\n            cred_req_json: str,\n            cred_attrs: dict,\n            rr_size: int = None) -> (str, str):\n        \"\"\"\n        Create credential as Issuer out of credential request and dict of key:value (raw, unencoded)\n        entries for attributes.\n\n        Return credential json, and if cred def supports revocation, credential revocation identifier.\n        Raise WalletState for closed wallet.\n\n        If the credential definition supports revocation, and the current revocation registry is full,\n        the processing creates a new revocation registry en passant. Depending on the revocation\n        registry size (by default starting at 64 and doubling iteratively through a maximum of 100000)\n        and the revocation registry builder posture (see RevRegBuilder.__init__()), this operation may\n        delay credential creation by several seconds. The use of an external revocation registry builder\n        runs a parallel process, skirting this delay, but is more costly at initialization.\n\n        :param cred_offer_json: credential offer json as created by Issuer\n        :param cred_req_json: credential request json as created by HolderProver\n        :param cred_attrs: dict mapping each attribute to its original value (the operation encodes it); e.g.,\n\n        ::\n\n            {\n                'favourite_drink': 'martini',\n                'height': 180,\n                'last_visit_date': '2017-12-31',\n                'weaknesses': None\n            }\n\n        :param rr_size: size of new revocation registry (default as per RevRegBuilder.create_rev_reg()) if necessary\n        :return: tuple with newly issued credential json, credential revocation identifier (if cred def\n            supports revocation, None otherwise).\n        \"\"\"\n\n        LOGGER.debug(\n            'Issuer.create_cred >>> cred_offer_json: %s, cred_req_json: %s, cred_attrs: %s, rr_size: %s',\n            cred_offer_json,\n            cred_req_json,\n            cred_attrs,\n            rr_size)\n\n        if not self.wallet.handle:\n            LOGGER.debug('Issuer.create_cred <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        cd_id = json.loads(cred_offer_json)['cred_def_id']\n        if not ok_cred_def_id(cd_id):\n            LOGGER.debug('Issuer.create_cred <!< Bad cred def id %s', cd_id)\n            raise BadIdentifier('Bad cred def id {}'.format(cd_id))\n\n        cred_def = json.loads(await self.get_cred_def(cd_id))  # ensure cred def is in cache\n\n        if 'revocation' in cred_def['value']:\n            with REVO_CACHE.lock:\n                rr_id = Tails.current_rev_reg_id(self.dir_tails, cd_id)\n                tails = REVO_CACHE[rr_id].tails\n                assert tails  # at (re)start, at cred def, Issuer sync_revoc_for_issue() sets this index in revo cache\n\n                try:\n                    (cred_json, cred_revoc_id, _) = await anoncreds.issuer_create_credential(  # issue by default to rr\n                        self.wallet.handle,\n                        cred_offer_json,\n                        cred_req_json,\n                        json.dumps({k: cred_attr_value(cred_attrs[k]) for k in cred_attrs}),\n                        rr_id,\n                        tails.reader_handle)\n                    rv = (cred_json, cred_revoc_id)\n\n                except IndyError as x_indy:\n                    if x_indy.error_code == ErrorCode.AnoncredsRevocationRegistryFullError:\n                        (tag, rr_size_suggested) = Tails.next_tag(self.dir_tails, cd_id)\n                        rr_id = rev_reg_id(cd_id, tag)\n                        if self.rrbx:\n                            await self._set_rev_reg(rr_id, rr_size)\n                        else:\n                            await self.rrb.create_rev_reg(rr_id, rr_size or rr_size_suggested)\n                            await self._send_rev_reg_def(rr_id)\n\n                        REVO_CACHE[rr_id].tails = await Tails(self.dir_tails, cd_id).open()  # symlink OK now\n                        return await self.create_cred(cred_offer_json, cred_req_json, cred_attrs)\n\n                    LOGGER.debug('Issuer.create_cred <!< cannot create cred, indy error code %s', x_indy.error_code)\n                    raise\n        else:\n            try:\n                (cred_json, _, _) = await anoncreds.issuer_create_credential(\n                    self.wallet.handle,\n                    cred_offer_json,\n                    cred_req_json,\n                    json.dumps({k: cred_attr_value(cred_attrs[k]) for k in cred_attrs}),\n                    None,\n                    None)\n                rv = (cred_json, None)\n            except IndyError as x_indy:\n                LOGGER.debug('Issuer.create_cred <!< cannot create cred, indy error code %s', x_indy.error_code)\n                raise\n\n        LOGGER.debug('Issuer.create_cred <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrevoking a revocation credential for the given revocation registry identifier and credential revocation identifier.", "response": "async def revoke_cred(self, rr_id: str, cr_id) -> int:\n        \"\"\"\n        Revoke credential that input revocation registry identifier and\n        credential revocation identifier specify.\n\n        Return (epoch seconds) time of revocation.\n\n        Raise AbsentTails if no tails file is available for input revocation registry identifier.\n        Raise WalletState for closed wallet.\n        Raise BadRevocation if issuer cannot revoke specified credential for any other reason\n        (e.g., did not issue it, already revoked it).\n\n        :param rr_id: revocation registry identifier\n        :param cr_id: credential revocation identifier\n        :return: time of revocation, in epoch seconds\n        \"\"\"\n\n        LOGGER.debug('Issuer.revoke_cred >>> rr_id: %s, cr_id: %s', rr_id, cr_id)\n\n        if not self.wallet.handle:\n            LOGGER.debug('Issuer.revoke_cred <!< Wallet %s is closed', self.name)\n            raise WalletState('Wallet {} is closed'.format(self.name))\n\n        if not ok_rev_reg_id(rr_id):\n            LOGGER.debug('Issuer.revoke_cred <!< Bad rev reg id %s', rr_id)\n            raise BadIdentifier('Bad rev reg id {}'.format(rr_id))\n\n        tails_reader_handle = (await Tails(\n            self.dir_tails,\n            *rev_reg_id2cred_def_id_tag(rr_id)).open()).reader_handle\n        try:\n            rrdelta_json = await anoncreds.issuer_revoke_credential(\n                self.wallet.handle,\n                tails_reader_handle,\n                rr_id,\n                cr_id)\n        except IndyError as x_indy:\n            LOGGER.debug(\n                'Issuer.revoke_cred <!< Could not revoke revoc reg id %s, cred rev id %s: indy error code %s',\n                rr_id,\n                cr_id,\n                x_indy.error_code)\n            raise BadRevocation(\n                'Could not revoke revoc reg id {}, cred rev id {}: indy error code {}'.format(\n                    rr_id,\n                    cr_id,\n                    x_indy.error_code))\n\n        rr_ent_req_json = await ledger.build_revoc_reg_entry_request(self.did, rr_id, 'CL_ACCUM', rrdelta_json)\n        resp_json = await self._sign_submit(rr_ent_req_json)  # raises AbsentPool or ClosedPool if applicable\n        resp = json.loads(resp_json)\n\n        rv = self.pool.protocol.txn2epoch(resp)\n        LOGGER.debug('Issuer.revoke_cred <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_box_ids_issued(self) -> str:\n\n        LOGGER.debug('Issuer.get_box_ids_issued >>>')\n\n        cd_ids = [\n            d for d in listdir(self.dir_tails) if isdir(join(self.dir_tails, d)) and ok_cred_def_id(d, self.did)]\n        s_ids = []\n        for cd_id in cd_ids:\n            try:\n                s_ids.append(json.loads(await self.get_schema(cred_def_id2seq_no(cd_id)))['id'])\n            except AbsentSchema:\n                LOGGER.error(\n                    'Issuer %s has issued cred def %s but no corresponding schema on ledger',\n                    self.name,\n                    cd_id)\n        rr_ids = [basename(link) for link in Tails.links(self.dir_tails, self.did)]\n\n        rv = json.dumps({\n            'schema_id': s_ids,\n            'cred_def_id': cd_ids,\n            'rev_reg_id': rr_ids\n        })\n        LOGGER.debug('Issuer.get_box_ids_issued <<< %s', rv)\n        return rv", "response": "Get the set of unique box identifiers for all credential definitions and credentials issued."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a URI reference return up to its delimiter or all of it if there is none.", "response": "def resource(ref: str, delimiter: str = None) -> str:\n    \"\"\"\n    Given a (URI) reference, return up to its delimiter (exclusively), or all of it if there is none.\n\n    :param ref: reference\n    :param delimiter: delimiter character (default None maps to '#', or ';' introduces identifiers)\n    \"\"\"\n\n    return ref.split(delimiter if delimiter else '#')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a URI into a DID if need be left - stripping did : if present.", "response": "def canon_did(uri: str) -> str:\n    \"\"\"\n    Convert a URI into a DID if need be, left-stripping 'did:sov:' if present.\n    Return input if already a DID. Raise BadIdentifier for invalid input.\n\n    :param uri: input URI or DID\n    :return: corresponding DID\n    \"\"\"\n\n    if ok_did(uri):\n        return uri\n\n    if uri.startswith('did:sov:'):\n        rv = uri[8:]\n        if ok_did(rv):\n            return rv\n    raise BadIdentifier('Bad specification {} does not correspond to a sovrin DID'.format(uri))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a DID and a reference in a DID document return a canonical form of a URI.", "response": "def canon_ref(did: str, ref: str, delimiter: str = None):\n    \"\"\"\n    Given a reference in a DID document, return it in its canonical form of a URI.\n\n    :param did: DID acting as the identifier of the DID document\n    :param ref: reference to canonicalize, either a DID or a fragment pointing to a location in the DID doc\n    :param delimiter: delimiter character marking fragment (default '#') or\n        introducing identifier (';') against DID resource\n    \"\"\"\n\n    if not ok_did(did):\n        raise BadIdentifier('Bad DID {} cannot act as DID document identifier'.format(did))\n\n    if ok_did(ref):  # e.g., LjgpST2rjsoxYegQDRm7EL\n        return 'did:sov:{}'.format(did)\n\n    if ok_did(resource(ref, delimiter)):  # e.g., LjgpST2rjsoxYegQDRm7EL#keys-1\n        return 'did:sov:{}'.format(ref)\n\n    if ref.startswith('did:sov:'):  # e.g., did:sov:LjgpST2rjsoxYegQDRm7EL, did:sov:LjgpST2rjsoxYegQDRm7EL#3\n        rv = ref[8:]\n        if ok_did(resource(rv, delimiter)):\n            return ref\n        raise BadIdentifier('Bad URI {} does not correspond to a sovrin DID'.format(ref))\n\n    if urlparse(ref).scheme:  # e.g., https://example.com/messages/8377464\n        return ref\n\n    return 'did:sov:{}{}{}'.format(did, delimiter if delimiter else '#', ref)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ok_tags(tags: dict) -> bool:\n\n        if not tags:\n            return True\n        depth = 0\n        queue = [(i, depth+1) for i in tags.values() if isinstance(i, dict)]\n        max_depth = 0\n        while queue and max_depth < 2:\n            sub, depth = queue.pop()\n            max_depth = max(max_depth, depth)\n            queue = queue + [(i, depth+1) for i in sub.values() if isinstance(i, dict)]\n\n        return max_depth < 2 and all(isinstance(k, str) and isinstance(tags[k], str) for k in tags)", "response": "Check if input tags dict is OK as an indy - sdk tags structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tags(self, val: str) -> None:\n\n        if not StorageRecord.ok_tags(val):\n            LOGGER.debug('StorageRecord.__init__ <!< Tags %s must map strings to strings', val)\n            raise BadRecord('Tags {} must map strings to strings'.format(val))\n\n        self._tags = val or {}", "response": "Set the tags of the record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_tags(self) -> dict:\n\n        return {t: self.tags[t] for t in (self.tags or {}) if t.startswith('~')} or None", "response": "Accessor for record tags stored in the clear."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encr_tags(self) -> dict:\n\n        return {t: self._tags[t] for t in self.tags or {} if not t.startswith('~')} or None", "response": "Accessor for record tags stored encrypted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching a dict of all hash suffixes from Pwned Passwords for a given a SHA - 1 prefix.", "response": "def _get_pwned(prefix):\n    \"\"\"\n    Fetches a dict of all hash suffixes from Pwned Passwords for a\n    given SHA-1 prefix.\n\n    \"\"\"\n    try:\n        response = requests.get(\n            url=API_ENDPOINT.format(prefix),\n            headers={'User-Agent': USER_AGENT},\n            timeout=getattr(\n                settings,\n                'PWNED_PASSWORDS_API_TIMEOUT',\n                REQUEST_TIMEOUT,\n            ),\n        )\n        response.raise_for_status()\n    except requests.RequestException as e:\n        # Gracefully handle timeouts and HTTP error response codes.\n        log.warning(\n            'Skipped Pwned Passwords check due to error: %r', e\n        )\n        return None\n\n    results = {}\n    for line in response.text.splitlines():\n        line_suffix, _, times = line.partition(':')\n        results[line_suffix] = int(times)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pwned_password(password):\n    if not isinstance(password, text_type):\n        raise TypeError('Password values to check must be Unicode strings.')\n    password_hash = hashlib.sha1(password.encode('utf-8')).hexdigest().upper()\n    prefix, suffix = password_hash[:5], password_hash[5:]\n    results = _get_pwned(prefix)\n    if results is None:\n        # Gracefully handle timeouts and HTTP error response codes.\n        return None\n    return results.get(suffix, 0)", "response": "Checks a password against the Pwned Passwords database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_regularizer(self, proxfun, **kwargs):\n\n        # if proxfun is a string, grab the corresponding function from operators.py\n        if isinstance(proxfun, str):\n            try:\n                proxfun_name = proxfun.split(None, 1)[0]\n                # Ignore everything after white space\n                op = getattr(operators, proxfun_name)\n                self.objectives.append(lambda theta, rho: op(theta.copy(), float(rho), **kwargs))\n\n            except AttributeError as e:\n                print(str(e) + '\\n' + 'Could not find the function ' + proxfun + ' in the operators module!')\n\n        # if proxfun is a function, add it as its own proximal operator\n        elif hasattr(proxfun, '__call__'):\n            self.objectives.append(lambda theta, rho: proxfun(theta.copy(), float(rho)))\n\n        # type of proxfun must be a string or a function\n        else:\n            raise TypeError('The argument \"proxfun\" must be a string or a function!')", "response": "Adds a regularizer to the objectives list for the given function in the operators module."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a set of regularizers to the set of regularizers.", "response": "def set_regularizers(self, regularizers, clear=True):\n        \"\"\"\n        Adds a set of regularizers\n\n        Parameters\n        ----------\n        regularizers : dict\n            Each key is the name of a corresponding proximal operator, and the\n            value associated with that key is a set of keyword arguments\n\n        clear : boolean, optional\n            Whether or not to clear the existing regularizers. (Default: True)\n\n        \"\"\"\n\n        # clear existing operators\n        if clear:\n            self.clear()\n\n        # add new regularizers\n        list([self.add_regularizer(proxfun, **regularizers[proxfun])\n              for proxfun in regularizers.keys()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minimize(self, theta_init, max_iter=50, callback=None, disp=0, tau=(10., 2., 2.), tol=1e-3):\n\n        # get list of objectives for this parameter\n        num_obj = len(self.objectives)\n        assert num_obj >= 1, \"There must be at least one objective!\"\n\n        # initialize lists of primal and dual variable copies, one for each objective\n        orig_shape = theta_init.shape\n        primals = [theta_init.flatten() for _ in range(num_obj)]\n        duals = [np.zeros(theta_init.size) for _ in range(num_obj)]\n        theta_avg = np.mean(primals, axis=0).ravel()\n\n        # initialize penalty parameter\n        tau = namedtuple('tau', ('init', 'inc', 'dec'))(*tau)\n        rho = tau.init\n\n        # store cumulative runtimes of each iteration, starting now\n        tstart = time.time()\n\n        # clear metadata\n        self.metadata = defaultdict(list)\n\n        # run ADMM iterations\n        self.converged = False\n        for cur_iter in range(max_iter):\n\n            # store the parameters from the previous iteration\n            theta_prev = theta_avg\n\n            # update each primal variable copy by taking a proximal step via each objective\n            for varidx, dual in enumerate(duals):\n                primals[varidx] = self.objectives[varidx]((theta_prev - dual).reshape(orig_shape), rho).ravel()\n\n            # average primal copies\n            theta_avg = np.mean(primals, axis=0)\n\n            # update the dual variables (after primal update has finished)\n            for varidx, primal in enumerate(primals):\n                duals[varidx] += primal - theta_avg\n\n            # compute primal and dual residuals\n            primal_resid = float(np.sum([np.linalg.norm(primal - theta_avg) for primal in primals]))\n            dual_resid = num_obj * rho ** 2 * np.linalg.norm(theta_avg - theta_prev)\n\n            # update penalty parameter according to primal and dual residuals\n            # (see sect. 3.4.1 of the Boyd and Parikh ADMM paper)\n            if primal_resid > tau.init * dual_resid:\n                rho *= float(tau.inc)\n            elif dual_resid > tau.init * primal_resid:\n                rho /= float(tau.dec)\n\n            # update metadata for this iteration\n            self.metadata['Primal resid'].append(primal_resid)\n            self.metadata['Dual resid'].append(dual_resid)\n            self.metadata['Time (s)'].append(time.time() - tstart)\n            self.metadata['rho'].append(rho)\n\n            # invoke the callback function with the current parameters and\n            # history\n            if callback is not None:\n\n                # get the metadata from this iteration\n                data = valmap(last, self.metadata)\n\n                callback(theta_avg.reshape(orig_shape), data)\n\n            # update the display\n            self.update_display(cur_iter + 1, disp)\n\n            # check for convergence\n            if (primal_resid <= tol) & (dual_resid <= tol):\n                self.converged = True\n                break\n\n        # clean up display\n        self.update_display(-1, disp)\n\n        # store and return final parameters\n        self.theta = theta_avg.reshape(orig_shape)\n        return self.theta", "response": "Minimize a list of objectives using a proximal consensus algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the display of the current iteration.", "response": "def update_display(self, iteration, disp_level, col_width=12):  # pragma: no cover\n        \"\"\"\n        Prints information about the optimization procedure to standard output\n\n        Parameters\n        ----------\n        iteration : int\n            The current iteration. Must either a positive integer or -1, which indicates the end of the algorithm\n\n        disp_level : int\n            An integer which controls how much information to display, ranging from 0 (nothing) to 3 (lots of stuff)\n\n        col_width : int\n            The width of each column in the data table, used if disp_level > 1\n        \"\"\"\n\n        # exit and print nothing if disp_level is zero\n        if disp_level == 0:\n            return\n\n        else:\n\n            # simple update, no table\n            if disp_level == 1 and iteration >= 0:\n                print('[Iteration %i]' % iteration)\n\n            # fancy table updates\n            if disp_level > 1:\n\n                # get the metadata from this iteration\n                data = valmap(last, self.metadata)\n\n                # choose what keys to use\n                keys = ['Time (s)', 'Primal resid', 'Dual resid', 'rho']\n\n                # initial update. print out table headers\n                if iteration == 1:\n                    print(tableprint.header(keys, width=col_width))\n\n                # print data\n                print(tableprint.row([data[k] for k in keys], width=col_width, format_spec='4g'))\n\n                if iteration == -1:\n                    print(tableprint.bottom(len(keys), width=col_width) + '\\n')\n\n            # print convergence statement\n            if iteration == -1 and self.converged:\n                print('Converged after %i iterations!' % len(self.metadata['Primal resid']))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef susvd(x, x_obs, rho, penalties):\n\n    assert type(x) == Tensor, \"Input array must be a Tensor\"\n\n    while True:\n\n        # proximal operator for the Fro. norm\n        x = squared_error(x, rho, x_obs)\n\n        # sequential singular value thresholding\n        for ix, penalty in enumerate(penalties):\n            x = x.unfold(ix).svt(penalty / rho).fold()\n\n        yield x", "response": "Sequential unfolding SVD generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the bytes from the python object.", "response": "def build(self, obj, context=None) -> bytes:\n        \"\"\"\n        Build bytes from the python object.\n\n        :param obj: Python object to build bytes from.\n        :param context: Optional context dictionary.\n        \"\"\"\n        stream = BytesIO()\n        self.build_stream(obj, stream, context)\n        return stream.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses some python object from the data.", "response": "def parse(self, data: bytes, context=None):\n        \"\"\"\n        Parse some python object from the data.\n\n        :param data: Data to be parsed.\n        :param context: Optional context dictionary.\n        \"\"\"\n        stream = BytesIO(data)\n        return self.parse_stream(stream, context)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_stream(self, obj, stream: BytesIO, context=None) -> None:\n        if context is None:\n            context = Context()\n        if not isinstance(context, Context):\n            context = Context(context)\n        try:\n            self._build_stream(obj, stream, context)\n        except Error:\n            raise\n        except Exception as exc:\n            raise BuildingError(str(exc))", "response": "Builds bytes from the python object into the stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_stream(self, stream: BytesIO, context=None):\n        if context is None:\n            context = Context()\n        if not isinstance(context, Context):\n            context = Context(context)\n        try:\n            return self._parse_stream(stream, context)\n        except Error:\n            raise\n        except Exception as exc:\n            raise ParsingError(str(exc))", "response": "Parse some python object from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the size of the construct in bytes.", "response": "def sizeof(self, context=None) -> int:\n        \"\"\"\n        Return the size of the construct in bytes.\n\n        :param context: Optional context dictionary.\n        \"\"\"\n        if context is None:\n            context = Context()\n        if not isinstance(context, Context):\n            context = Context(context)\n        try:\n            return self._sizeof(context)\n        except Error:\n            raise\n        except Exception as exc:\n            raise SizeofError(str(exc))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poissreg(x0, rho, x, y):\n\n    # objective and gradient\n    n = float(x.shape[0])\n    f = lambda w: np.mean(np.exp(x.dot(w)) - y * x.dot(w))\n    df = lambda w: (x.T.dot(np.exp(x.dot(w))) - x.T.dot(y)) / n\n\n    # minimize via BFGS\n    return bfgs(x0, rho, f, df)", "response": "Proximal operator for Poisson regression\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smooth(x0, rho, gamma, axis=0):\n\n    # Apply Laplacian smoothing\n    n = x0.shape[axis]\n    lap_op = spdiags([(2 + rho / gamma) * np.ones(n), -1 * np.ones(n), -1 * np.ones(n)], [0, -1, 1], n, n, format='csc')\n    x_out = np.rollaxis(spsolve(gamma * lap_op, rho * np.rollaxis(x0, axis, 0)), axis, 0)\n\n    return x_out", "response": "Proximal operator for a smoothing function enforced via the discrete laplacian operator\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nucnorm(x0, rho, gamma):\n\n    # compute SVD\n    u, s, v = np.linalg.svd(x0, full_matrices=False)\n\n    # soft threshold the singular values\n    sthr = np.maximum(s - (gamma / float(rho)), 0)\n\n    # reconstruct\n    x_out = (u.dot(np.diag(sthr)).dot(v))\n\n    return x_out", "response": "Proximal operator for the nuclear norm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tvd(x0, rho, gamma):\n    try:\n        from skimage.restoration import denoise_tv_bregman\n    except ImportError:\n        print('Error: scikit-image not found. TVD will not work.')\n        return x0\n\n    return denoise_tv_bregman(x0, rho / gamma)", "response": "Returns the TVD of the total variation of the TVD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sparse(x0, rho, gamma):\n\n    lmbda = float(gamma) / rho\n\n    return (x0 - lmbda) * (x0 >= lmbda) + (x0 + lmbda) * (x0 <= -lmbda)", "response": "This function computes the sparse operator for the l1 norm of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef linsys(x0, rho, P, q):\n    return np.linalg.solve(rho * np.eye(q.shape[0]) + P, rho * x0.copy() + q)", "response": "Proximal operator for the linear approximation of the function Ax = b."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef headerlist(self):\n        if 'Content-Type' not in self.headers:\n            self.headers.add_header('Content-Type', self.default_content_type)\n        if self._cookies:\n            for c in self._cookies.values():\n                self.headers.add_header('Set-Cookie', c.OutputString())\n        return self.headers.items()", "response": "WSGI conform list of ( header value ) tuples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_redirect(self, response, expected_url=None):\n        self.assertIn(\n            response.status_code,\n            self.redirect_codes,\n            self._get_redirect_assertion_message(response),\n        )\n        if expected_url:\n            location_header = response._headers.get('location', None)\n            self.assertEqual(\n                location_header,\n                ('Location', str(expected_url)),\n                'Response should redirect to {0}, but it redirects to {1} instead'.format(\n                    expected_url,\n                    location_header[1],\n                )\n            )", "response": "Assert that the response is redirected to the expected_url."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a model instance was created in the database.", "response": "def assert_instance_created(self, model_class, **kwargs):\n        \"\"\"\n        Checks if a model instance was created in the database.\n\n        For example::\n\n        >>> with self.assert_instance_created(Article, slug='lorem-ipsum'):\n        ...     Article.objects.create(slug='lorem-ipsum')\n        \"\"\"\n        return _InstanceContext(\n            self.assert_instance_does_not_exist,\n            self.assert_instance_exists,\n            model_class,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the model instance was deleted from the database.", "response": "def assert_instance_deleted(self, model_class, **kwargs):\n        \"\"\"\n        Checks if the model instance was deleted from the database.\n\n        For example::\n\n        >>> with self.assert_instance_deleted(Article, slug='lorem-ipsum'):\n        ...     Article.objects.get(slug='lorem-ipsum').delete()\n        \"\"\"\n        return _InstanceContext(\n            self.assert_instance_exists,\n            self.assert_instance_does_not_exist,\n            model_class,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _split_into_mimetype_and_priority(x):\n    if ';' in x:\n        content_type, priority = x.split(';')\n        casted_priority = float(priority.split('=')[1])\n    else:\n        content_type, casted_priority = x, 1.0\n\n    content_type = content_type.lstrip().rstrip()  # Replace ' text/html' to 'text/html'\n    return content_type, casted_priority", "response": "Split an accept header item into mimetype and priority."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and sort the accept header items.", "response": "def _parse_and_sort_accept_header(accept_header):\n    \"\"\"Parse and sort the accept header items.\n\n    >>> _parse_and_sort_accept_header('application/json;q=0.5, text/*')\n    [('text/*', 1.0), ('application/json', 0.5)]\n    \"\"\"\n    return sorted([_split_into_mimetype_and_priority(x) for x in accept_header.split(',')],\n                  key=lambda x: x[1], reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a mimetype best matched the accept headers.", "response": "def accept_best_match(accept_header, mimetypes):\n    \"\"\"Return a mimetype best matched the accept headers.\n\n    >>> accept_best_match('application/json, text/html', ['application/json', 'text/plain'])\n    'application/json'\n\n    >>> accept_best_match('application/json;q=0.5, text/*', ['application/json', 'text/plain'])\n    'text/plain'\n    \"\"\"\n    for mimetype_pattern, _ in _parse_and_sort_accept_header(accept_header):\n        matched_types = fnmatch.filter(mimetypes, mimetype_pattern)\n        if matched_types:\n            return matched_types[0]\n    return mimetypes[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmatching types of url vars.", "response": "def match_url_vars_type(url_vars, type_hints):\n    \"\"\" Match types of url vars.\n\n    >>> match_url_vars_type({'user_id': '1'}, {'user_id': int})\n    (True, {'user_id': 1})\n    >>> match_url_vars_type({'user_id': 'foo'}, {'user_id': int})\n    (False, {})\n    \"\"\"\n    typed_url_vars = {}\n    try:\n        for k, v in url_vars.items():\n            arg_type = type_hints.get(k)\n            if arg_type and arg_type != str:\n                typed_url_vars[k] = arg_type(v)\n            else:\n                typed_url_vars[k] = v\n    except ValueError:\n        return False, {}\n    return True, typed_url_vars"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_path(rule, path):\n    split_rule = split_by_slash(rule)\n    split_path = split_by_slash(path)\n    url_vars = {}\n\n    if len(split_rule) != len(split_path):\n        return False, {}\n\n    for r, p in zip(split_rule, split_path):\n        if r.startswith('{') and r.endswith('}'):\n            url_vars[r[1:-1]] = p\n            continue\n        if r != p:\n            return False, {}\n    return True, url_vars", "response": "Match a path to a rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(self, path, method):\n        if path != '/':\n            path = path.rstrip('/')\n        method = method.upper()\n\n        status = 404\n        for p, n, m in self.endpoints:\n            matched, url_vars = match_path(p, path)\n            if not matched:  # path: not matched\n                continue\n\n            if method not in m:  # path: matched, method: not matched\n                status = 405\n                raise HTTPError(status=status, body=f'Method not found: {path} {method}')  # it has security issue??\n\n            callback, type_hints = m[method]\n            type_matched, typed_url_vars = match_url_vars_type(url_vars, type_hints)\n            if not type_matched:\n                continue  # path: not matched (types are different)\n            return callback, typed_url_vars\n        raise HTTPError(status=status, body=f'Not found: {path}')", "response": "Match the given path and method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new rule or replace an existing rule.", "response": "def add(self, rule, method, name, callback):\n        \"\"\" Add a new rule or replace the target for an existing rule.\n\n        >>> from kobin import Response\n        >>> r = Router()\n        >>> def view(user_id: int) -> Response:\n        ...     return Response(f'You are {user_id}')\n        ...\n        >>> r.add('/users/{user_id}', 'GET', 'user-detail', view)\n        >>> path, name, methods = r.endpoints[0]\n        >>> path\n        '/users/{user_id}'\n        >>> name\n        'user-detail'\n        >>> callback, type_hints = methods['GET']\n        >>> view == callback\n        True\n        >>> type_hints['user_id'] == int\n        True\n        \"\"\"\n        if rule != '/':\n            rule = rule.rstrip('/')\n        method = method.upper()\n\n        for i, e in enumerate(self.endpoints):\n            r, n, callbacks = e\n            if r == rule:\n                assert name == n and n is not None, (\n                    \"A same path should set a same name for reverse routing.\"\n                )\n                callbacks[method] = (callback, get_type_hints(callback))\n                self.endpoints[i] = (r, name, callbacks)\n                break\n        else:\n            e = (rule, name, {method: (callback, get_type_hints(callback))})\n            self.endpoints.append(e)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreversing routing. >>> from kobin import Response >>> r = Router() >>> def view(user_id: int) -> Response: ... return Response(f'You are {user_id}') ... >>> r.add('/users/{user_id}', 'GET', 'user-detail', view) >>> r.reverse('user-detail', user_id=1) '/users/1'", "response": "def reverse(self, name, **kwargs):\n        \"\"\" Reverse routing.\n\n        >>> from kobin import Response\n        >>> r = Router()\n        >>> def view(user_id: int) -> Response:\n        ...     return Response(f'You are {user_id}')\n        ...\n        >>> r.add('/users/{user_id}', 'GET', 'user-detail', view)\n        >>> r.reverse('user-detail', user_id=1)\n        '/users/1'\n        \"\"\"\n        for p, n, _ in self.endpoints:\n            if name == n:\n                return p.format(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, value):\n        '''\n        Atomically sets the value to `value`.\n\n        :param value: The value to set.\n        '''\n        with self._lock.exclusive:\n            self._value = value\n            return value", "response": "Atomically sets the value to value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_and_set(self, value):\n        '''\n        Atomically sets the value to `value` and returns the old value.\n\n        :param value: The value to set.\n        '''\n        with self._lock.exclusive:\n            oldval = self._value\n            self._value = value\n            return oldval", "response": "Atomically sets the value to value and returns the old value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares the current value of the current object with the expected value and set the value to update if the current value is equal to the expect value.", "response": "def compare_and_set(self, expect, update):\n        '''\n        Atomically sets the value to `update` if the current value is equal to\n        `expect`.\n\n        :param expect: The expected current value.\n        :param update: The value to set if and only if `expect` equals the\n            current value.\n        '''\n        with self._lock.exclusive:\n            if self._value == expect:\n                self._value = update\n                return True\n\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_and_get(self, delta):\n        '''\n        Atomically adds `delta` to the current value.\n\n        :param delta: The delta to add.\n        '''\n        with self._lock.exclusive:\n            self._value += delta\n            return self._value", "response": "Atomically adds delta to the current value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_and_add(self, delta):\n        '''\n        Atomically adds `delta` to the current value and returns the old value.\n\n        :param delta: The delta to add.\n        '''\n        with self._lock.exclusive:\n            oldval = self._value\n            self._value += delta\n            return oldval", "response": "Atomically adds delta to the current value and returns the old value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubtract delta from the current value and returns the old value.", "response": "def subtract_and_get(self, delta):\n        '''\n        Atomically subtracts `delta` from the current value.\n\n        :param delta: The delta to subtract.\n        '''\n        with self._lock.exclusive:\n            self._value -= delta\n            return self._value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnotifies all the watches that a new value has changed.", "response": "def notify_watches(self, oldval, newval):\n        '''\n        Passes `oldval` and `newval` to each `fn` in the watches dictionary,\n        passing along its respective key and the reference to this object.\n\n        :param oldval: The old value which will be passed to the watch.\n        :param newval: The new value which will be passed to the watch.\n        '''\n        watches = self._watches.copy()\n        for k in watches:\n            fn = watches[k]\n            if isinstance(fn, collections.Callable):\n                fn(k, self, oldval, newval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef swap(self, fn, *args, **kwargs):\n        '''\n        Given a mutator `fn`, calls `fn` with the atom's current state, `args`,\n        and `kwargs`. The return value of this invocation becomes the new value\n        of the atom. Returns the new value.\n\n        :param fn: A function which will be passed the current state. Should\n            return a new state. This absolutely *MUST NOT* mutate the\n            reference to the current state! If it does, this function may loop\n            indefinitely.\n        :param \\*args: Arguments to be passed to `fn`.\n        :param \\*\\*kwargs: Keyword arguments to be passed to `fn`.\n        '''\n        while True:\n            oldval = self.deref()\n            newval = fn(oldval, *args, **kwargs)\n            if self._state.compare_and_set(oldval, newval):\n                self.notify_watches(oldval, newval)\n                return newval", "response": "Given a mutator fn calls the atom s current state with the current state args and kwargs returns the new value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self, newval):\n        '''\n        Resets the atom's value to `newval`, returning `newval`.\n\n        :param newval: The new value to set.\n        '''\n        oldval = self._state.get()\n        self._state.set(newval)\n        self.notify_watches(oldval, newval)\n        return newval", "response": "Resets the atom s value to newval returning the newval."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compare_and_set(self, oldval, newval):\n        '''\n        Given `oldval` and `newval`, sets the atom's value to `newval` if and\n        only if `oldval` is the atom's current value. Returns `True` upon\n        success, otherwise `False`.\n\n        :param oldval: The old expected value.\n        :param newval: The new value which will be set if and only if `oldval`\n            equals the current value.\n        '''\n        ret = self._state.compare_and_set(oldval, newval)\n        if ret:\n            self.notify_watches(oldval, newval)\n\n        return ret", "response": "Given oldval and newval sets the atom s value to newval if and only if and\n            equals the current value. Returns True upon success otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare the current value of the current key with the expected value and set the value to update if the current value is equal to the expect value.", "response": "def compare_and_set(self, expect, update):\n        '''\n        Atomically sets the value to `update` if the current value is equal to\n        `expect`.\n\n        :param expect: The expected current value.\n        :param update: The value to set if and only if `expect` equals the\n            current value.\n        '''\n        with self._reference.get_lock():\n            if self._reference.value == expect:\n                self._reference.value = update\n                return True\n\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_and_get(self, delta):\n        '''\n        Atomically adds `delta` to the current value.\n\n        :param delta: The delta to add.\n        '''\n        with self._reference.get_lock():\n            self._reference.value += delta\n            return self._reference.value", "response": "Atomically adds delta to the current value and returns the new value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subtract_and_get(self, delta):\n        '''\n        Atomically subtracts `delta` from the current value.\n\n        :param delta: The delta to subtract.\n        '''\n        with self._reference.get_lock():\n            self._reference.value -= delta\n            return self._reference.value", "response": "Atomically subtracts delta from the current value and returns the new value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_and_subtract(self, delta):\n        '''\n        Atomically subtracts `delta` from the current value and returns the\n        old value.\n\n        :param delta: The delta to subtract.\n        '''\n        with self._reference.get_lock():\n            oldval = self._reference.value\n            self._reference.value -= delta\n            return oldval", "response": "Atomically subtracts delta from the current value and returns the old value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef translate_zxcvbn_text(text):\n    i18n = {\n        \"Use a few words, avoid common phrases\": _(\n            \"Use a few words, avoid common phrases\"\n        ),\n        \"No need for symbols, digits, or uppercase letters\": _(\n            \"No need for symbols, digits, or uppercase letters\"\n        ),\n        \"Add another word or two. Uncommon words are better.\": _(\n            \"Add another word or two. Uncommon words are better.\"\n        ),\n        \"Straight rows of keys are easy to guess\": _(\n            \"Straight rows of keys are easy to guess\"\n        ),\n        \"Short keyboard patterns are easy to guess\": _(\n            \"Short keyboard patterns are easy to guess\"\n        ),\n        \"Use a longer keyboard pattern with more turns\": _(\n            \"Use a longer keyboard pattern with more turns\"\n        ),\n        'Repeats like \"aaa\" are easy to guess': _(\n            'Repeats like \"aaa\" are easy to guess'\n        ),\n        'Repeats like \"abcabcabc\" are only slightly harder to guess than \"abc\"': _(\n            'Repeats like \"abcabcabc\" are only slightly harder to guess than \"abc\"'\n        ),\n        \"Avoid repeated words and characters\": _(\"Avoid repeated words and characters\"),\n        'Sequences like \"abc\" or \"6543\" are easy to guess': _(\n            'Sequences like \"abc\" or \"6543\" are easy to guess'\n        ),\n        \"Avoid sequences\": _(\"Avoid sequences\"),\n        \"Recent years are easy to guess\": _(\"Recent years are easy to guess\"),\n        \"Avoid recent years\": _(\"Avoid recent years\"),\n        \"Avoid years that are associated with you\": _(\n            \"Avoid years that are associated with you\"\n        ),\n        \"Dates are often easy to guess\": _(\"Dates are often easy to guess\"),\n        \"Avoid dates and years that are associated with you\": _(\n            \"Avoid dates and years that are associated with you\"\n        ),\n        \"This is a top-10 common password\": _(\"This is a top-10 common password\"),\n        \"This is a top-100 common password\": _(\"This is a top-100 common password\"),\n        \"This is a very common password\": _(\"This is a very common password\"),\n        \"This is similar to a commonly used password\": _(\n            \"This is similar to a commonly used password\"\n        ),\n        \"A word by itself is easy to guess\": _(\"A word by itself is easy to guess\"),\n        \"Names and surnames by themselves are easy to guess\": _(\n            \"Names and surnames by themselves are easy to guess\"\n        ),\n        \"Common names and surnames are easy to guess\": _(\n            \"Common names and surnames are easy to guess\"\n        ),\n        \"Capitalization doesn't help very much\": _(\n            \"Capitalization doesn't help very much\"\n        ),\n        \"All-uppercase is almost as easy to guess as all-lowercase\": _(\n            \"All-uppercase is almost as easy to guess as all-lowercase\"\n        ),\n        \"Reversed words aren't much harder to guess\": _(\n            \"Reversed words aren't much harder to guess\"\n        ),\n        \"Predictable substitutions like '@' instead of 'a' don't help very much\": _(\n            \"Predictable substitutions like '@' instead of 'a' don't help very much\"\n        ),\n    }\n    translated_text = i18n.get(text)\n    if translated_text is None:\n        # zxcvbn is inconsistent, sometime there is a dot, sometime not\n        translated_text = i18n.get(text[:-1])\n    if translated_text is None:\n        LOGGER.warning(\n            \"No translation for '%s' or '%s', update the generatei18ndict command.\",\n            text,\n            text[:-1],\n        )\n        return text\n    return translated_text", "response": "Translate the text into a zxcvbn language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to find conda on the system", "response": "def find_conda():\n    \"\"\" Try to find conda on the system \"\"\"\n    USER_HOME = os.path.expanduser('~')\n    CONDA_HOME = os.environ.get('CONDA_HOME', '')\n    PROGRAMDATA = os.environ.get('PROGRAMDATA', '')\n\n    # Search common install paths and sys path\n    search_paths = [\n        # Windows\n        join(PROGRAMDATA, 'miniconda2', 'scripts'),\n        join(PROGRAMDATA, 'miniconda3', 'scripts'),\n        join(USER_HOME, 'miniconda2', 'scripts'),\n        join(USER_HOME, 'miniconda3', 'scripts'),\n        join(CONDA_HOME, 'scripts'),\n\n        # Linux\n        join(USER_HOME, 'miniconda2', 'bin'),\n        join(USER_HOME, 'miniconda3', 'bin'),\n        join(CONDA_HOME, 'bin'),\n        # TODO: OSX\n    ] + os.environ.get(\"PATH\", \"\").split(\";\" if 'win' in sys.path else \":\")\n\n    cmd = 'conda.exe' if IS_WIN else 'conda'\n    for conda_path in search_paths:\n        conda = join(conda_path, cmd)\n        if exists(conda):\n            return sh.Command(conda)\n\n    # Try to let the system find it\n    return sh.conda"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cp(src, dst):\n    print(\"[DEBUG]:   -> copying {} to {}\".format(src, dst))\n    if os.path.isfile(src):\n        if not exists(dirname(dst)):\n            os.makedirs(dirname(dst))\n        shutil.copy(src, dst)\n    else:\n        copy_tree(src, dst)", "response": "Like cp - R src dst"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all commands of the given class.", "response": "def find_commands(cls):\n    \"\"\" Finds commands by finding the subclasses of Command\"\"\"\n    cmds = []\n    for subclass in cls.__subclasses__():\n        cmds.append(subclass)\n        cmds.extend(find_commands(subclass))\n    return cmds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef link(self, path, pkg):\n        # Check if a custom linker exists to handle linking this package\n        #for ep in pkg_resources.iter_entry_points(group=\"enaml_native_linker\"):\n        #    if ep.name.replace(\"-\", '_') == pkg.replace(\"-\", '_'):\n        #        linker = ep.load()\n        #        print(\"Custom linker {} found for '{}'. Linking...\".format(\n        #            linker, pkg))\n        #        if linker(self.ctx, path):\n        #            return\n\n        #: Use the default builtin linker script\n        if exists(join(path, pkg, 'build.gradle')):\n            print(Colors.BLUE+\"[INFO] Linking {}/build.gradle\".format(\n                  pkg)+Colors.RESET)\n            self.link_android(path, pkg)\n        if exists(join(path, pkg, 'Podfile')):\n            print(Colors.BLUE+\"[INFO] Linking {}/Podfile\".format(\n                  pkg)+Colors.RESET)\n            self.link_ios(path, pkg)", "response": "Link the package in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_settings_linked(source, pkg):\n        for line in source.split(\"\\n\"):\n            if re.search(r\"include\\s*['\\\"]:{}['\\\"]\".format(pkg), line):\n                return True\n        return False", "response": "Returns true if the include line in the file \n        is linked to the specified package."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_build_linked(source, pkg):\n        for line in source.split(\"\\n\"):\n            if re.search(r\"(api|compile)\\s+project\\(['\\\"]:{}['\\\"]\\)\".format(pkg),\n                         line):\n                return True\n        return False", "response": "Returns true if the build is linked to the specified package"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding all java files matching the \"*Package. java\" pattern within the given enaml package directory relative to the java source path.", "response": "def find_packages(path):\n        \"\"\" Find all java files matching the \"*Package.java\" pattern within\n        the given enaml package directory relative to the java source path.\n        \"\"\"\n        matches = []\n        root = join(path, 'src', 'main', 'java')\n        for folder, dirnames, filenames in os.walk(root):\n            for filename in fnmatch.filter(filenames, '*Package.java'):\n                #: Open and make sure it's an EnamlPackage somewhere\n                with open(join(folder, filename)) as f:\n                    if \"implements EnamlPackage\" in f.read():\n                        package = os.path.relpath(folder, root)\n                        matches.append(os.path.join(package, filename))\n        return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_app_linked(source, pkg, java_package):\n        for line in source.split(\"\\n\"):\n            if java_package in line:\n                return True\n        return False", "response": "Returns true if the compile project line exists in the file \n "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlinking the android project to the new package", "response": "def link_android(self, path, pkg):\n        \"\"\" Link's the android project to this library.\n\n        1. Includes this project's directory in the app's \n            android/settings.gradle\n            It adds:\n                include ':<project-name>'\n                project(':<project-name>').projectDir = new File(\n                rootProject.projectDir, '../packages/<project-name>/android')\n\n        2. Add's this project as a dependency to the android/app/build.gradle\n            It adds:\n                compile project(':<project-name>')\n            to the dependencies.\n\n        3. If preset, adds the import and package statement\n           to the android/app/src/main/java/<bundle/id>/MainApplication.java\n\n        \"\"\"\n\n        bundle_id = self.ctx['bundle_id']\n        pkg_root = join(path, pkg)\n\n        #: Check if it's already linked\n        with open(join('android', 'settings.gradle')) as f:\n            settings_gradle = f.read()\n        with open(join('android', 'app', 'build.gradle')) as f:\n            build_gradle = f.read()\n\n        #: Find the MainApplication.java\n        main_app_java_path = join('android', 'app', 'src', 'main', 'java',\n                                  join(*bundle_id.split(\".\")),\n                                  'MainApplication.java')\n        with open(main_app_java_path) as f:\n            main_application_java = f.read()\n\n        try:\n            #: Now link all the EnamlPackages we can find in the new \"package\"\n            new_packages = Link.find_packages(join(path, pkg))\n            if not new_packages:\n                print(\"[Android] {} No EnamlPackages found to link!\".format(\n                      pkg))\n                return\n\n            #: Link settings.gradle\n            if not Link.is_settings_linked(settings_gradle, pkg):\n                #: Add two statements\n                new_settings = settings_gradle.split(\"\\n\")\n                new_settings.append(\"\")  # Blank line\n                new_settings.append(\"include ':{name}'\".format(name=pkg))\n                new_settings.append(\"project(':{name}').projectDir = \"\n                                    \"new File(rootProject.projectDir, \"\n                                    \"'../{path}/android/{name}')\"\n                                    .format(name=pkg, path=self.package_dir))\n\n                with open(join('android', 'settings.gradle'), 'w') as f:\n                    f.write(\"\\n\".join(new_settings))\n                print(\"[Android] {} linked in settings.gradle!\".format(pkg))\n            else:\n                print(\"[Android] {} was already linked in \"\n                      \"settings.gradle!\".format(pkg))\n\n            #: Link app/build.gradle\n            if not Link.is_build_linked(build_gradle, pkg):\n                #: Add two statements\n                new_build = build_gradle.split(\"\\n\")\n\n                #: Find correct line number\n                found = False\n                for i, line in enumerate(new_build):\n                    if re.match(r\"dependencies\\s*{\", line):\n                        found = True\n                        continue\n                    if found and \"}\" in line:\n                        #: Hackish way to find line of the closing bracket after\n                        #: the dependencies { block is found\n                        break\n                if not found:\n                    raise ValueError(\"Unable to find dependencies in \"\n                                     \"{pkg}/app/build.gradle!\".format(pkg=pkg))\n\n                #: Insert before the closing bracket\n                new_build.insert(i, \"    api project(':{name}')\".format(\n                    name=pkg))\n\n                with open(join('android', 'app', 'build.gradle'), 'w') as f:\n                    f.write(\"\\n\".join(new_build))\n                print(\"[Android] {} linked in app/build.gradle!\".format(pkg))\n            else:\n                print(\"[Android] {} was already linked in \"\n                      \"app/build.gradle!\".format(pkg))\n\n            new_app_java = []\n            for package in new_packages:\n                #: Add our import statement\n                javacls = os.path.splitext(package)[0].replace(\"/\", \".\")\n\n                if not Link.is_app_linked(main_application_java, pkg, javacls):\n                    #: Reuse previous if avialable\n                    new_app_java = (new_app_java or\n                                    main_application_java.split(\"\\n\"))\n\n                    #: Find last import statement\n                    j = 0\n                    for i, line in enumerate(new_app_java):\n                        if fnmatch.fnmatch(line, \"import *;\"):\n                            j = i\n\n                    new_app_java.insert(j+1, \"import {};\".format(javacls))\n\n                    #: Add the package statement\n                    j = 0\n                    for i, line in enumerate(new_app_java):\n                        if fnmatch.fnmatch(line.strip(), \"new *Package()\"):\n                            j = i\n                    if j == 0:\n                        raise ValueError(\"Could not find the correct spot to \"\n                                         \"add package {}\".format(javacls))\n                    else:\n                        #: Get indent and add to previous line\n                        #: Add comma to previous line\n                        new_app_java[j] = new_app_java[j]+ \",\"\n\n                        #: Insert new line\n                        new_app_java.insert(j+1, \"                new {}()\"\n                                            .format(javacls.split(\".\")[-1]))\n\n                else:\n                    print(\"[Android] {} was already linked in {}!\".format(\n                        pkg, main_app_java_path))\n\n            if new_app_java:\n                with open(main_app_java_path, 'w') as f:\n                    f.write(\"\\n\".join(new_app_java))\n\n            print(Colors.GREEN+\"[Android] {} linked successfully!\".format(\n                  pkg)+Colors.RESET)\n        except Exception as e:\n            print(Colors.GREEN+\"[Android] {} Failed to link. \"\n                               \"Reverting due to error: \"\n                               \"{}\".format(pkg, e)+Colors.RESET)\n\n            #: Undo any changes\n            with open(join('android', 'settings.gradle'), 'w') as f:\n                f.write(settings_gradle)\n            with open(join('android', 'app', 'build.gradle'), 'w') as f:\n                f.write(build_gradle)\n            with open(main_app_java_path, 'w') as f:\n                f.write(main_application_java)\n\n            #: Now blow up\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, args=None):\n        print(Colors.BLUE+\"[INFO] Unlinking {}...\".format(\n              args.names)+Colors.RESET)\n        for name in args.names:\n            self.unlink(Link.package_dir, name)", "response": "Unlink the specified resource classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unlink(self, path, pkg):\n        #: Check if a custom unlinker exists to handle unlinking this package\n        for ep in pkg_resources.iter_entry_points(\n                group=\"enaml_native_unlinker\"):\n            if ep.name.replace(\"-\", '_') == pkg.replace(\"-\", '_'):\n                unlinker = ep.load()\n                print(\"Custom unlinker {} found for '{}'. \"\n                      \"Unlinking...\".format(unlinker, pkg))\n                if unlinker(self.ctx, path):\n                    return\n\n        if exists(join(path, 'android', pkg, 'build.gradle')):\n            print(\"[Android] unlinking {}\".format(pkg))\n            self.unlink_android(path, pkg)\n\n        for target in ['iphoneos', 'iphonesimulator']:\n            if exists(join(path, target, pkg, 'Podfile')):\n                print(\"[iOS] unlinking {}\".format(pkg))\n                self.unlink_ios(path, pkg)", "response": "Unlink the package in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_tornado(self, args):\n        server = self\n        import tornado.ioloop\n        import tornado.web\n        import tornado.websocket\n\n        ioloop = tornado.ioloop.IOLoop.current()\n\n        class DevWebSocketHandler(tornado.websocket.WebSocketHandler):\n            def open(self):\n                super(DevWebSocketHandler, self).open()\n                server.on_open(self)\n\n            def on_message(self, message):\n                server.on_message(self, message)\n\n            def on_close(self):\n                super(DevWebSocketHandler, self).on_close()\n                server.on_close(self)\n\n        class MainHandler(tornado.web.RequestHandler):\n            def get(self):\n                self.write(server.index_page)\n\n        #: Set the call later method\n        server.call_later = ioloop.call_later\n        server.add_callback = ioloop.add_callback\n\n        app = tornado.web.Application([\n            (r\"/\", MainHandler),\n            (r\"/dev\", DevWebSocketHandler),\n        ])\n\n        app.listen(self.port)\n        print(\"Tornado Dev server started on {}\".format(self.port))\n        ioloop.start()", "response": "Tornado dev server implementation"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_twisted(self, args):\n        server = self\n\n        from twisted.internet import reactor\n        from twisted.web import resource\n        from twisted.web.static import File\n        from twisted.web.server import Site\n        from autobahn.twisted.websocket import (WebSocketServerFactory,\n                                                WebSocketServerProtocol)\n        from autobahn.twisted.resource import WebSocketResource\n\n        class DevWebSocketHandler(WebSocketServerProtocol):\n            def onConnect(self, request):\n                super(DevWebSocketHandler, self).onConnect(request)\n                server.on_open(self)\n\n            def onMessage(self, payload, isBinary):\n                server.on_message(self, payload)\n\n            def onClose(self, wasClean, code, reason):\n                super(DevWebSocketHandler,self).onClose(wasClean, code, reason)\n                server.on_close(self)\n\n            def write_message(self, message, binary=False):\n                self.sendMessage(message, binary)\n\n        #: Set the call later method\n        server.call_later = reactor.callLater\n        server.add_callback = reactor.callFromThread\n\n        factory = WebSocketServerFactory(u\"ws://0.0.0.0:{}\".format(self.port))\n        factory.protocol = DevWebSocketHandler\n\n        class MainHandler(resource.Resource):\n            def render_GET(self, req):\n                return str(server.index_page)\n\n        root = resource.Resource()\n        root.putChild(\"\", MainHandler())\n        root.putChild(\"dev\", WebSocketResource(factory))\n        reactor.listenTCP(self.port, Site(root))\n        print(\"Twisted Dev server started on {}\".format(self.port))\n        reactor.run()", "response": "Twisted dev server implementation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_message(self, handler, msg):\n        if self.remote_debugging:\n            #: Forward to other clients\n            for h in self.handlers:\n                if h != handler:\n                    h.write_message(msg, True)\n        else:\n            print(msg)", "response": "Called when a message is received from the remote client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to the client.", "response": "def send_message(self, msg):\n        \"\"\" Send a message to the client. This should not be used in\n        remote debugging mode.\n        \n        \"\"\"\n        if not self.handlers:\n            return  #: Client not connected\n        for h in self.handlers:\n            h.write_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the list of CLI commands by finding subclasses of the Command class and then loading them into the appropriate list of Command subclasses.", "response": "def _default_commands(self):\n        \"\"\" Build the list of CLI commands by finding subclasses of the Command \n        class\n\n        Also allows commands to be installed using the \"enaml_native_command\" \n        entry point. This entry point should return a Command subclass\n\n        \"\"\"\n        commands = [c() for c in find_commands(Command)]\n\n        #: Get commands installed via entry points\n        for ep in pkg_resources.iter_entry_points(\n                group=\"enaml_native_command\"):\n            c = ep.load()\n            if not issubclass(c, Command):\n                print(\"Warning: entry point {} did not return a valid enaml \"\n                      \"cli command! This command will be ignored!\".format(\n                    ep.name))\n            commands.append(c())\n\n        return commands"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _default_ctx(self):\n        if not self.in_app_directory:\n            print(\"Warning: {} does not exist. Using the default.\".format(\n                self.package))\n            ctx = {}\n\n        else:\n            with open(self.package) as f:\n                ctx = dict(yaml.load(f, Loader=yaml.RoundTripLoader))\n\n        if self.in_app_directory:\n            # Update the env for each platform\n            excluded = list(ctx.get('excluded', []))\n\n            for env in [ctx['ios'], ctx['android']]:\n                if 'python_build_dir' not in env:\n                    env['python_build_dir'] = expanduser(abspath('build/python'))\n                if 'conda_prefix' not in env:\n                    env['conda_prefix'] = os.environ.get(\n                        'CONDA_PREFIX', expanduser(abspath('venv')))\n\n                # Join the shared and local exclusions\n                env['excluded'] = list(env.get('excluded', [])) + excluded\n\n        return ctx", "response": "Return the package config or context and normalize some of the \n values \n "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a parser using the command list", "response": "def _default_parser(self):\n        \"\"\" Generate a parser using the command list \"\"\"\n        parser = ArgumentParser(prog='enaml-native')\n\n        #: Build commands by name\n        cmds = {c.title: c for c in self.commands}\n\n        #: Build parser, prepare commands\n        subparsers = parser.add_subparsers()\n        for c in self.commands:\n            p = subparsers.add_parser(c.title, help=c.help)\n            c.parser = p\n            for (flags, kwargs) in c.args:\n                p.add_argument(*flags.split(), **kwargs)\n            p.set_defaults(cmd=c)\n            c.ctx = self.ctx\n            c.cmds = cmds\n            c.cli = self\n\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_data(folder):\n    for (path, directories, filenames) in os.walk(folder):\n        for filename in filenames:\n            yield os.path.join('..', path, filename)", "response": "Find all the data in the folder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate(number=4, choice=SystemRandom().choice, words=words, joiner=\" \"):\n    return joiner.join(choice(words) for each in range(number))", "response": "Generate a random passphrase from the GSL."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redivmod(initial_value, factors):\n    result = []\n    value = initial_value\n    for divisor, label in factors:\n        if not divisor:\n            remainder = value\n            if not remainder:\n                break\n        else:\n            value, remainder = divmod(value, divisor)\n            if not value and not remainder:\n                break\n        if remainder == 1:\n            # depluralize\n            label = label[:-1]\n        if six.PY2:\n            addition = unicode(remainder) + ' ' + unicode(label)\n        else:\n            addition = str(remainder) + ' ' + str(label)\n        result.insert(0, addition)\n    if len(result) > 1:\n        result[-1] = \"and \" + result[-1]\n    if result:\n        return ', '.join(result)\n    else:\n        return \"instantly\"", "response": "Redivant the value of a single element of a sequence according to the list of factors and return a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nzip two sequences into a new set of points", "response": "def zip(self, other):\n        \"\"\"\n        zips two sequences unifying the corresponding points.\n        \"\"\"\n        return self.__class__(p1 % p2 for p1, p2 in zip(self, other))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays the sequence in a specific format.", "response": "def display(self, format=\"png\"):\n        \"\"\"\n        Return an object that can be used to display this sequence.\n        This is used for IPython Notebook.\n\n        :param format: \"png\" or \"svg\"\n        \"\"\"\n        from sebastian.core.transforms import lilypond\n        seq = HSeq(self) | lilypond()\n\n        lily_output = write_lilypond.lily_format(seq)\n        if not lily_output.strip():\n            #In the case of empty lily outputs, return self to get a textual display\n            return self\n\n        if format == \"png\":\n            suffix = \".preview.png\"\n            args = [\"lilypond\", \"--png\", \"-dno-print-pages\", \"-dpreview\"]\n        elif format == \"svg\":\n            suffix = \".preview.svg\"\n            args = [\"lilypond\", \"-dbackend=svg\", \"-dno-print-pages\", \"-dpreview\"]\n\n        f = tempfile.NamedTemporaryFile(suffix=suffix)\n        basename = f.name[:-len(suffix)]\n        args.extend([\"-o\" + basename, \"-\"])\n\n        #Pass shell=True so that if your $PATH contains ~ it will\n        #get expanded. This also changes the way the arguments get\n        #passed in. To work correctly, pass them as a string\n        p = sp.Popen(\" \".join(args), stdin=sp.PIPE, shell=True)\n        stdout, stderr = p.communicate(\"{ %s }\" % lily_output)\n        if p.returncode != 0:\n            # there was an error\n            #raise IOError(\"Lilypond execution failed: %s%s\" % (stdout, stderr))\n            return None\n\n        if not ipython:\n            return f.read()\n        if format == \"png\":\n            return Image(data=f.read(), filename=f.name, format=\"png\")\n        else:\n            return SVG(data=f.read(), filename=f.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, point):\n        point = Point(point)\n        self._elements.append(point)", "response": "Appends a copy of the given point to this sequence\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrepeats the sequence given number of times to produce a new sequence", "response": "def repeat(self, count):\n        \"\"\"\n        repeat sequence given number of times to produce a new sequence\n        \"\"\"\n        x = HSeq()\n        for i in range(count):\n            x = x.concatenate(self)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subseq(self, start_offset=0, end_offset=None):\n        from sebastian.core import DURATION_64\n\n        def subseq_iter(start_offset, end_offset):\n            cur_offset = 0\n            for point in self._elements:\n                try:\n                    cur_offset += point[DURATION_64]\n                except KeyError:\n                    raise ValueError(\"HSeq.subseq requires all points to have a %s attribute\" % DURATION_64)\n                #Skip until start\n                if cur_offset < start_offset:\n                    continue\n\n                #Yield points start_offset <=  point < end_offset\n                if end_offset is None or cur_offset < end_offset:\n                    yield point\n                else:\n                    raise StopIteration\n        return HSeq(subseq_iter(start_offset, end_offset))", "response": "Return a subset of the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arpeggio(pattern, point):\n    point['sequence'] = HSeq(point['sequence'][i] for i in pattern)\n    return point", "response": "Takes each subsequence into an arpeggio matching the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling the subsequence of the point with repetitions of its subsequence and sets the duration of each point.", "response": "def fill(duration, point):\n    \"\"\"\n    fills the subsequence of the point with repetitions of its subsequence and\n    sets the ``duration`` of each point.\n    \"\"\"\n    point['sequence'] = point['sequence'] * (point[DURATION_64] / (8 * duration)) | add({DURATION_64: duration})\n    return point"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand a tree of sequences into a single flat sequence recursively.", "response": "def expand(sequence):\n    \"\"\"\n    expands a tree of sequences into a single, flat sequence, recursively.\n    \"\"\"\n    expanse = []\n    for point in sequence:\n        if 'sequence' in point:\n            expanse.extend(expand(point['sequence']))\n        else:\n            expanse.append(point)\n    return sequence.__class__(expanse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new sequence with only the information that is available to the user.", "response": "def debug(sequence):\n    \"\"\"\n    adds information to the sequence for better debugging, currently only\n    an index property on each point in the sequence.\n    \"\"\"\n    points = []\n    for i, p in enumerate(sequence):\n        copy = Point(p)\n        copy['index'] = i\n        points.append(copy)\n    return sequence.__class__(points)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a portion of the input sequence", "response": "def subseq(start_offset=0, end_offset=None):\n    \"\"\"\n    Return a portion of the input sequence\n    \"\"\"\n    def _(sequence):\n        return sequence.subseq(start_offset, end_offset)\n    return _"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lilypond(point):\n    #If lilypond already computed, leave as is\n    if \"lilypond\" in point:\n        return point\n\n    #Defaults:\n    pitch_string = \"\"\n    octave_string = \"\"\n    duration_string = \"\"\n    preamble = \"\"\n    dynamic_string = \"\"\n    if \"pitch\" in point:\n        octave = point[\"octave\"]\n        pitch = point[\"pitch\"]\n        if octave > 4:\n            octave_string = \"'\" * (octave - 4)\n        elif octave < 4:\n            octave_string = \",\" * (4 - octave)\n        else:\n            octave_string = \"\"\n        m = modifiers(pitch)\n        if m > 0:\n            modifier_string = \"is\" * m\n        elif m < 0:\n            modifier_string = \"es\" * -m\n        else:\n            modifier_string = \"\"\n        pitch_string = letter(pitch).lower() + modifier_string\n\n    if DURATION_64 in point:\n        duration = point[DURATION_64]\n        if duration > 0:\n            if duration % 3 == 0:  # dotted note\n                duration_string = str(192 // (2 * duration)) + \".\"\n            else:\n                duration_string = str(64 // duration)\n        #TODO: for now, if we have a duration but no pitch, show a 'c' with an x note\n        if duration_string:\n            if not pitch_string:\n                pitch_string = \"c\"\n                octave_string = \"'\"\n                preamble = r'\\xNote '\n\n    if \"dynamic\" in point:\n        dynamic = point[\"dynamic\"]\n        if dynamic == \"crescendo\":\n            dynamic_string = \"\\<\"\n        elif dynamic == \"diminuendo\":\n            dynamic_string = \"\\>\"\n        else:\n            dynamic_string = \"\\%s\" % (dynamic,)\n\n    point[\"lilypond\"] = \"%s%s%s%s%s\" % (preamble, pitch_string, octave_string, duration_string, dynamic_string)\n\n    return point", "response": "Generates a lilypond representation of a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dynamics(start, end=None):\n    def _(sequence):\n        if start in _dynamic_markers_to_velocity:\n            start_velocity = _dynamic_markers_to_velocity[start]\n            start_marker = start\n        else:\n            raise ValueError(\"Unknown start dynamic: %s, must be in %s\" % (start, _dynamic_markers_to_velocity.keys()))\n\n        if end is None:\n            end_velocity = start_velocity\n            end_marker = start_marker\n        elif end in _dynamic_markers_to_velocity:\n            end_velocity = _dynamic_markers_to_velocity[end]\n            end_marker = end\n        else:\n            raise ValueError(\"Unknown end dynamic: %s, must be in %s\" % (start, _dynamic_markers_to_velocity.keys()))\n\n        retval = sequence.__class__([Point(point) for point in sequence._elements])\n\n        velocity_interval = (float(end_velocity) - float(start_velocity)) / (len(retval) - 1) if len(retval) > 1 else 0\n        velocities = [int(start_velocity + velocity_interval * pos) for pos in range(len(retval))]\n\n        # insert dynamics markers for lilypond\n        if start_velocity > end_velocity:\n            retval[0][\"dynamic\"] = \"diminuendo\"\n            retval[-1][\"dynamic\"] = end_marker\n        elif start_velocity < end_velocity:\n            retval[0][\"dynamic\"] = \"crescendo\"\n            retval[-1][\"dynamic\"] = end_marker\n        else:\n            retval[0][\"dynamic\"] = start_marker\n\n        for point, velocity in zip(retval, velocities):\n            point[\"velocity\"] = velocity\n\n        return retval\n    return _", "response": "Returns a new sequence with dynamics applied to the specified start and end notes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_blocks(user_text, session):\n    logger.debug(\"Getting blocks for {0}\".format(user_text))\n    doc = session.get(action='query', list='blocks', bkusers=user_text,\n                      bkprop=['id', 'timestamp'])\n    return [mwtypes.Timestamp(b['timestamp']) for b in doc['query']['blocks']]", "response": "Returns a list of blocks for a single user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of blocks for a single user", "response": "def get_user_blocks(session, user_text):\n    \"\"\"\n    Returns a list of blocks for a single user\n    \"\"\"\n    logger.debug(\"Getting user_blocks for {0}\".format(user_text))\n    doc = session.get(action='query', list='blocks', bkusers=user_text,\n                      bkprop=['id'])\n    return doc['query']['blocks']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a set of revisions by their IDs by repeatedly querying in batches.", "response": "def query_revisions_by_revids(session, revids, **params):\n    \"\"\"\n    Gets a set of revisions by their IDs by repeatedly querying in batches.\n    If an ID cannot be found, it is ignored.\n    \"\"\"\n    doc = session.get(action='query', prop='revisions',\n                      revids=revids, **params)\n\n    for page_doc in doc['query'].get('pages', {}).values():\n        revisions = page_doc.get('revisions', [])\n        if 'revisions' in page_doc:\n            del page_doc['revisions']\n\n        for revision_doc in revisions:\n            revision_doc['page'] = page_doc\n            yield revision_doc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a new tree structure from the given template.", "response": "def generate(variables, templates_path, main_template):\n    \"\"\"\n    :Parameters:\n        variables : dict\n            Template parameters, passed through.\n        templates_path : str\n            Root directory for transclusions.\n        main_template : str\n            Contents of the main template.\n\n    Returns the rendered output.\n    \"\"\"\n\n    env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(templates_path),\n        lstrip_blocks=True,\n        trim_blocks=True\n    )\n\n    def norm_alg_filename(alg_name):\n        if alg_name in variables['globals']['algorithm_filename_parts']:\n            return variables['globals']['algorithm_filename_parts'][alg_name]\n        else:\n            raise KeyError(\"{0} not found in globals.algorithm_filename_parts\"\n                           .format(alg_name))\n\n    env.globals.update(norm_alg_filename=norm_alg_filename)\n\n    template = env.from_string(main_template)\n    return template.render(variables) + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_metafile(filepath):\n    try:\n        with open(filepath, 'r', encoding='utf-8') as file:\n            return email.message_from_file(file)\n    except FileNotFoundError:\n        logger.warning(\"Category file %s not found\", filepath)\n        orm.delete(c for c in model.Category if c.file_path == filepath)\n        orm.commit()\n\n    return None", "response": "Load a metadata file from the filesystem"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscans a file and put it into the index", "response": "def scan_file(fullpath, relpath):\n    \"\"\" scan a file and put it into the index \"\"\"\n\n    load_metafile.cache_clear()\n\n    meta = load_metafile(fullpath)\n    if not meta:\n        return True\n\n    # update the category meta file mapping\n    category = meta.get('Category', utils.get_category(relpath))\n    values = {\n        'category': category,\n        'file_path': fullpath,\n        'sort_name': meta.get('Sort-Name', '')\n    }\n\n    logger.debug(\"setting category %s to metafile %s\", category, fullpath)\n    record = model.Category.get(category=category)\n    if record:\n        record.set(**values)\n    else:\n        record = model.Category(**values)\n\n    # update other relationships to the index\n    path_alias.remove_aliases(record)\n    for alias in meta.get_all('Path-Alias', []):\n        path_alias.set_alias(alias, category=record)\n\n    orm.commit()\n\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name(self):\n        if self._meta and self._meta.get('name'):\n            # get it from the meta file\n            return self._meta.get('name')\n        # infer it from the basename\n        return self.basename.replace('_', ' ').title()", "response": "Get the display name of the category"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the textual description of the category", "response": "def description(self):\n        \"\"\" Get the textual description of the category \"\"\"\n        if self._meta and self._meta.get_payload():\n            return utils.TrueCallableProxy(self._description)\n        return utils.CallableProxy(None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the category hierarchy leading up to this category and including the current category.", "response": "def breadcrumb(self):\n        \"\"\" Get the category hierarchy leading up to this category, including\n        root and self.\n\n        For example, path/to/long/category will return a list containing\n        Category('path'), Category('path/to'), and Category('path/to/long').\n        \"\"\"\n        ret = []\n        here = self\n        while here:\n            ret.append(here)\n            here = here.parent\n        return list(reversed(ret))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the sorting name of this category", "response": "def sort_name(self):\n        \"\"\" Get the sorting name of this category \"\"\"\n        if self._record and self._record.sort_name:\n            return self._record.sort_name\n        return self.name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the parent category", "response": "def parent(self):\n        \"\"\" Get the parent category \"\"\"\n        if self.path:\n            return Category(os.path.dirname(self.path))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the subcategories of this category", "response": "def _get_subcats(self, recurse=False):\n        \"\"\" Get the subcategories of this category\n\n        recurse -- whether to include their subcategories as well\n        \"\"\"\n\n        if recurse:\n            # No need to filter\n            return sorted([Category(e) for e in self._subcats_recursive],\n                          key=lambda c: c.sort_breadcrumb)\n\n        # get all the subcategories, with only the first subdir added\n\n        # number of path components to ingest\n        parts = len(self.path.split('/')) + 1 if self.path else 1\n\n        # convert the subcategories into separated pathlists with only 'parts'\n        # parts\n        subcats = [c.split('/')[:parts] for c in self._subcats_recursive]\n\n        # join them back into a path, and make unique\n        subcats = {'/'.join(c) for c in subcats}\n\n        # convert to a bunch of Category objects\n        return sorted([Category(c) for c in subcats], key=lambda c: c.sort_name or c.name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _first(self, **spec):\n        for record in self._entries(spec).order_by(model.Entry.local_date,\n                                                   model.Entry.id)[:1]:\n            return entry.Entry(record)\n        return None", "response": "Return the first entry in this category optionally including subcategories"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _last(self, **spec):\n        for record in self._entries(spec).order_by(orm.desc(model.Entry.local_date),\n                                                   orm.desc(model.Entry.id))[:1]:\n            return entry.Entry(record)\n        return None", "response": "Get the latest entry in this category optionally including subcategories"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_date(datestr):\n\n    match = re.match(\n        r'([0-9]{4})(-?([0-9]{1,2}))?(-?([0-9]{1,2}))?(_w)?$', datestr)\n    if not match:\n        return (arrow.get(datestr,\n                          tzinfo=config.timezone).replace(tzinfo=config.timezone),\n                'day', 'YYYY-MM-DD')\n\n    year, month, day, week = match.group(1, 3, 5, 6)\n    start = arrow.Arrow(year=int(year), month=int(\n        month or 1), day=int(day or 1), tzinfo=config.timezone)\n\n    if week:\n        return start.span('week')[0], 'week', WEEK_FORMAT\n    if day:\n        return start, 'day', DAY_FORMAT\n    if month:\n        return start, 'month', MONTH_FORMAT\n    if year:\n        return start, 'year', YEAR_FORMAT\n\n    raise ValueError(\"Could not parse date: {}\".format(datestr))", "response": "Parse a date expression into a tuple of start_date span_type span_format"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds a file by relative path.", "response": "def find_file(path, search_path):\n    \"\"\" Find a file by relative path. Arguments:\n\n    path -- the image's filename\n    search_path -- a list of directories to check in\n\n    Returns: the resolved file path\n    \"\"\"\n\n    if isinstance(search_path, str):\n        search_path = [search_path]\n    for relative in search_path:\n        candidate = os.path.normpath(os.path.join(relative, path))\n        if os.path.isfile(candidate):\n            return candidate\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_entry(rel_path, search_path):\n\n    from . import entry  # pylint:disable=cyclic-import\n\n    try:\n        entry_id = int(rel_path)\n        record = model.Entry.get(id=entry_id)\n        if record:\n            return entry.Entry(record)\n    except ValueError:\n        pass\n\n    if rel_path.startswith('/'):\n        search_path = [config.content_folder]\n        rel_path = '.' + rel_path\n\n    for where in search_path:\n        abspath = os.path.normpath(os.path.join(where, rel_path))\n        record = model.Entry.get(file_path=abspath)\n        if record:\n            return entry.Entry(record)\n    return None", "response": "Find an entry by relative path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef static_url(path, absolute=False):\n\n    if os.sep != '/':\n        path = '/'.join(path.split(os.sep))\n\n    return flask.url_for('static', filename=path, _external=absolute)", "response": "Shortcut for returning a URL for the requested static file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild an HTML tag from the given name and attributes.", "response": "def make_tag(name, attrs, start_end=False):\n    \"\"\" Build an HTML tag from the given name and attributes.\n\n    Arguments:\n\n    name -- the name of the tag (p, div, etc.)\n    attrs -- a dict of attributes to apply to the tag\n    start_end -- whether this tag should be self-closing\n    \"\"\"\n\n    text = '<' + name\n\n    if isinstance(attrs, dict):\n        attr_list = attrs.items()\n    elif isinstance(attrs, list):\n        attr_list = attrs\n    elif attrs is not None:\n        raise TypeError(\"Unhandled attrs type \" + str(type(attrs)))\n\n    for key, val in attr_list:\n        if val is not None:\n            escaped = html.escape(str(val), False).replace('\"', '&#34;')\n            text += ' {}=\"{}\"'.format(key, escaped)\n    if start_end:\n        text += ' /'\n    text += '>'\n    return flask.Markup(text)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a fingerprint for a file", "response": "def file_fingerprint(fullpath):\n    \"\"\" Get a metadata fingerprint for a file \"\"\"\n    stat = os.stat(fullpath)\n    return ','.join([str(value) for value in [stat.st_ino, stat.st_mtime, stat.st_size] if value])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a new argument list by remapping keys.", "response": "def remap_args(input_args, remap):\n    \"\"\" Generate a new argument list by remapping keys. The 'remap'\n    dict maps from destination key -> priority list of source keys\n    \"\"\"\n    out_args = input_args\n    for dest_key, src_keys in remap.items():\n        remap_value = None\n        if isinstance(src_keys, str):\n            src_keys = [src_keys]\n\n        for key in src_keys:\n            if key in input_args:\n                remap_value = input_args[key]\n                break\n\n        if remap_value is not None:\n            if out_args is input_args:\n                out_args = {**input_args}\n            out_args[dest_key] = remap_value\n\n    return out_args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remap_link_target(path, absolute=False):\n    if path.startswith('@'):\n        # static resource\n        return static_url(path[1:], absolute=absolute)\n\n    if absolute:\n        # absolute-ify whatever the URL is\n        return urllib.parse.urljoin(flask.request.url, path)\n\n    return path", "response": "remap a link target to a static URL if it s prefixed with @"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_category(filename):\n    return '/'.join(os.path.dirname(filename).split(os.sep))", "response": "Get a default category name from a filename in a cross - platform manner"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the default function return", "response": "def _default(self):\n        \"\"\" Get the default function return \"\"\"\n\n        if self._default_args:\n            return self._func(\n                *self._default_args,\n                **self._default_kwargs)\n\n        return self._func(**self._default_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting up the database", "response": "def setup():\n    \"\"\" Set up the database \"\"\"\n    try:\n        db.bind(**config.database_config)\n    except OSError:\n        # Attempted to connect to a file-based database where the file didn't\n        # exist\n        db.bind(**config.database_config, create_db=True)\n\n    rebuild = True\n\n    try:\n        db.generate_mapping(create_tables=True)\n        with orm.db_session:\n            version = GlobalConfig.get(key='schema_version')\n            if version and version.int_value != SCHEMA_VERSION:\n                logger.info(\"Existing database has schema version %d\",\n                            version.int_value)\n            else:\n                rebuild = False\n    except:  # pylint:disable=bare-except\n        logger.exception(\"Error mapping schema\")\n\n    if rebuild:\n        logger.info(\"Rebuilding schema\")\n        try:\n            db.drop_all_tables(with_all_data=True)\n            db.create_tables()\n        except:\n            raise RuntimeError(\"Unable to upgrade schema automatically; please \" +\n                               \"delete the existing database and try again.\")\n\n    with orm.db_session:\n        if not GlobalConfig.get(key='schema_version'):\n            logger.info(\"setting schema version to %d\", SCHEMA_VERSION)\n            GlobalConfig(key='schema_version',\n                         int_value=SCHEMA_VERSION)\n            orm.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning true if the entry should be viewable", "response": "def visible(self):\n        \"\"\" Returns true if the entry should be viewable \"\"\"\n        return self.status not in (PublishStatus.DRAFT.value,\n                                   PublishStatus.GONE.value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts card data based on the provided texts.", "response": "def extract_card(text, config, image_search_path):\n    \"\"\" Extract card data based on the provided texts. \"\"\"\n    card = CardData()\n    parser = CardParser(card, config, image_search_path)\n    misaka.Markdown(parser, extensions=markdown.ENABLED_EXTENSIONS)(text)\n\n    return card"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning the first paragraph of text into the summary text", "response": "def paragraph(self, content):\n        \"\"\" Turn the first paragraph of text into the summary text \"\"\"\n        if not self._out.description:\n            self._out.description = content\n        return ' '"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the images from the raw url", "response": "def image(self, raw_url, title='', alt=''):\n        ''' extract the images '''\n        max_images = self._config.get('count')\n        if max_images is not None and len(self._out.images) >= max_images:\n            # We already have enough images, so bail out\n            return ' '\n\n        image_specs = raw_url\n        if title:\n            image_specs += ' \"{}\"'.format(title)\n\n        alt, container_args = image.parse_alt_text(alt)\n\n        spec_list, _ = image.get_spec_list(image_specs, container_args)\n\n        for spec in spec_list:\n            if not spec:\n                continue\n\n            self._out.images.append(self._render_image(spec, alt))\n            if max_images is not None and len(self._out.images) >= max_images:\n                break\n\n        return ' '"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _render_image(self, spec, alt=''):\n        # pylint: disable=unused-argument\n\n        try:\n            path, image_args, _ = image.parse_image_spec(spec)\n        except Exception as err:  # pylint: disable=broad-except\n            # we tried\u2122\n            logger.exception(\"Got error on spec %s: %s\", spec, err)\n            return None\n\n        img = image.get_image(path, self._image_search_path)\n        if img:\n            image_config = {**image_args, **self._config, 'absolute': True}\n            return img.get_rendition(1, **image_config)[0]\n\n        return None", "response": "Given an image spec try to turn it into a card image per the configuration"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Publ application and configure it for use with Publ.", "response": "def publ(name, cfg):\n    \"\"\" Create a Flask app and configure it for use with Publ \"\"\"\n\n    config.setup(cfg)\n\n    app = _PublApp(name,\n                   template_folder=config.template_folder,\n                   static_folder=config.static_folder,\n                   static_url_path=config.static_url_path)\n\n    for route in [\n            '/',\n            '/<path:category>/',\n            '/<template>',\n            '/<path:category>/<template>',\n    ]:\n        app.add_url_rule(route, 'category', rendering.render_category)\n\n    for route in [\n            '/<int:entry_id>',\n            '/<int:entry_id>-',\n            '/<int:entry_id>-<slug_text>',\n            '/<path:category>/<int:entry_id>',\n            '/<path:category>/<int:entry_id>-',\n            '/<path:category>/<int:entry_id>-<slug_text>',\n    ]:\n        app.add_url_rule(route, 'entry', rendering.render_entry)\n\n    app.add_url_rule('/<path:path>.PUBL_PATHALIAS',\n                     'path_alias', rendering.render_path_alias)\n\n    app.add_url_rule('/_async/<path:filename>',\n                     'async', image.get_async)\n\n    app.add_url_rule('/_', 'chit', rendering.render_transparent_chit)\n\n    app.add_url_rule('/_file/<path:filename>',\n                     'asset', rendering.retrieve_asset)\n\n    app.config['TRAP_HTTP_EXCEPTIONS'] = True\n    app.register_error_handler(\n        werkzeug.exceptions.HTTPException, rendering.render_exception)\n\n    app.jinja_env.globals.update(  # pylint: disable=no-member\n        get_view=view.get_view,\n        arrow=arrow,\n        static=utils.static_url,\n        get_template=rendering.get_template\n    )\n\n    caching.init_app(app)\n\n    maint = maintenance.Maintenance()\n\n    if config.index_rescan_interval:\n        maint.register(functools.partial(index.scan_index,\n                                         config.content_folder),\n                       config.index_rescan_interval)\n\n    if config.image_cache_interval and config.image_cache_age:\n        maint.register(functools.partial(image.clean_cache,\n                                         config.image_cache_age),\n                       config.image_cache_interval)\n\n    app.before_request(maint.run)\n\n    if 'CACHE_THRESHOLD' in config.cache:\n        app.after_request(set_cache_expiry)\n\n    if app.debug:\n        # We're in debug mode so we don't want to scan until everything's up\n        # and running\n        app.before_first_request(startup)\n    else:\n        # In production, register the exception handler and scan the index\n        # immediately\n        app.register_error_handler(Exception, rendering.render_exception)\n        startup()\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the cache expiry headers", "response": "def set_cache_expiry(response):\n    \"\"\" Set the cache control headers \"\"\"\n    if response.cache_control.max_age is None and 'CACHE_DEFAULT_TIMEOUT' in config.cache:\n        response.cache_control.max_age = config.cache['CACHE_DEFAULT_TIMEOUT']\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the registered path - alias regular expressions", "response": "def get_path_regex(self, path):\n        \"\"\" Evaluate the registered path-alias regular expressions \"\"\"\n        for regex, func in self._regex_map:\n            match = re.match(regex, path)\n            if match:\n                return func(match)\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to guess the title from the filename", "response": "def guess_title(basename):\n    \"\"\" Attempt to guess the title from the filename \"\"\"\n\n    base, _ = os.path.splitext(basename)\n    return re.sub(r'[ _-]+', r' ', base).title()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_entry_id(entry, fullpath, assign_id):\n    warn_duplicate = False\n\n    if 'Entry-ID' in entry:\n        entry_id = int(entry['Entry-ID'])\n    else:\n        entry_id = None\n\n    # See if we've inadvertently duplicated an entry ID\n    if entry_id:\n        try:\n            other_entry = model.Entry.get(id=entry_id)\n            if (other_entry\n                    and os.path.isfile(other_entry.file_path)\n                    and not os.path.samefile(other_entry.file_path, fullpath)):\n                warn_duplicate = entry_id\n                entry_id = None\n        except FileNotFoundError:\n            # the other file doesn't exist, so just let it go\n            pass\n\n    # Do we need to assign a new ID?\n    if not entry_id and not assign_id:\n        # We're not assigning IDs yet\n        return None\n\n    if not entry_id:\n        # See if we already have an entry with this file path\n        by_filepath = model.Entry.get(file_path=fullpath)\n        if by_filepath:\n            entry_id = by_filepath.id\n\n    if not entry_id:\n        # We still don't have an ID; generate one pseudo-randomly, based on the\n        # entry file path. This approach averages around 0.25 collisions per ID\n        # generated while keeping the entry ID reasonably short. In general,\n        # count*N averages 1/(N-1) collisions per ID.\n\n        limit = max(10, orm.get(orm.count(e) for e in model.Entry) * 5)\n        attempt = 0\n\n        while not entry_id or model.Entry.get(id=entry_id):\n            # Stably generate a quasi-random entry ID from the file path\n            md5 = hashlib.md5()\n            md5.update(\"{} {}\".format(fullpath, attempt).encode('utf-8'))\n            entry_id = int.from_bytes(md5.digest(), byteorder='big') % limit\n            attempt = attempt + 1\n\n    if warn_duplicate is not False:\n        logger.warning(\"Entry '%s' had ID %d, which belongs to '%s'. Reassigned to %d\",\n                       fullpath, warn_duplicate, other_entry.file_path, entry_id)\n\n    return entry_id", "response": "Get or generate an entry ID for an entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving a message file out without mangling the headers", "response": "def save_file(fullpath, entry):\n    \"\"\" Save a message file out, without mangling the headers \"\"\"\n    with tempfile.NamedTemporaryFile('w', delete=False) as file:\n        tmpfile = file.name\n        # we can't just use file.write(str(entry)) because otherwise the\n        # headers \"helpfully\" do MIME encoding normalization.\n        # str(val) is necessary to get around email.header's encoding\n        # shenanigans\n        for key, val in entry.items():\n            print('{}: {}'.format(key, str(val)), file=file)\n        print('', file=file)\n        file.write(entry.get_payload())\n    shutil.move(tmpfile, fullpath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nscan a file and put it into the index", "response": "def scan_file(fullpath, relpath, assign_id):\n    \"\"\" scan a file and put it into the index \"\"\"\n    # pylint: disable=too-many-branches,too-many-statements,too-many-locals\n\n    # Since a file has changed, the lrucache is invalid.\n    load_message.cache_clear()\n\n    try:\n        entry = load_message(fullpath)\n    except FileNotFoundError:\n        # The file doesn't exist, so remove it from the index\n        record = model.Entry.get(file_path=fullpath)\n        if record:\n            expire_record(record)\n        return True\n\n    entry_id = get_entry_id(entry, fullpath, assign_id)\n    if entry_id is None:\n        return False\n\n    fixup_needed = False\n\n    basename = os.path.basename(relpath)\n    title = entry['title'] or guess_title(basename)\n\n    values = {\n        'file_path': fullpath,\n        'category': entry.get('Category', utils.get_category(relpath)),\n        'status': model.PublishStatus[entry.get('Status', 'SCHEDULED').upper()].value,\n        'entry_type': entry.get('Entry-Type', ''),\n        'slug_text': make_slug(entry.get('Slug-Text', title)),\n        'redirect_url': entry.get('Redirect-To', ''),\n        'title': title,\n        'sort_title': entry.get('Sort-Title', title),\n        'entry_template': entry.get('Entry-Template', '')\n    }\n\n    entry_date = None\n    if 'Date' in entry:\n        try:\n            entry_date = arrow.get(entry['Date'], tzinfo=config.timezone)\n        except arrow.parser.ParserError:\n            entry_date = None\n    if entry_date is None:\n        del entry['Date']\n        entry_date = arrow.get(\n            os.stat(fullpath).st_ctime).to(config.timezone)\n        entry['Date'] = entry_date.format()\n        fixup_needed = True\n\n    if 'Last-Modified' in entry:\n        last_modified_str = entry['Last-Modified']\n        try:\n            last_modified = arrow.get(\n                last_modified_str, tzinfo=config.timezone)\n        except arrow.parser.ParserError:\n            last_modified = arrow.get()\n            del entry['Last-Modified']\n            entry['Last-Modified'] = last_modified.format()\n            fixup_needed = True\n\n    values['display_date'] = entry_date.isoformat()\n    values['utc_date'] = entry_date.to('utc').datetime\n    values['local_date'] = entry_date.naive\n\n    logger.debug(\"getting entry %s with id %d\", fullpath, entry_id)\n    record = model.Entry.get(id=entry_id)\n    if record:\n        logger.debug(\"Reusing existing entry %d\", record.id)\n        record.set(**values)\n    else:\n        record = model.Entry(id=entry_id, **values)\n\n    # Update the entry ID\n    if str(record.id) != entry['Entry-ID']:\n        del entry['Entry-ID']\n        entry['Entry-ID'] = str(record.id)\n        fixup_needed = True\n\n    if 'UUID' not in entry:\n        entry['UUID'] = str(uuid.uuid5(\n            uuid.NAMESPACE_URL, 'file://' + fullpath))\n        fixup_needed = True\n\n    # add other relationships to the index\n    path_alias.remove_aliases(record)\n    if record.visible:\n        for alias in entry.get_all('Path-Alias', []):\n            path_alias.set_alias(alias, entry=record)\n\n    with orm.db_session:\n        set_tags = {\n            t.lower()\n            for t in entry.get_all('Tag', [])\n            + entry.get_all('Hidden-Tag', [])\n        }\n\n        for tag in record.tags:\n            if tag.key in set_tags:\n                set_tags.remove(tag.key)\n            else:\n                tag.delete()\n        for tag in set_tags:\n            model.EntryTag(entry=record, key=tag)\n        orm.commit()\n\n    if record.status == model.PublishStatus.DRAFT.value:\n        logger.info(\"Not touching draft entry %s\", fullpath)\n    elif fixup_needed:\n        logger.info(\"Fixing up entry %s\", fullpath)\n        save_file(fullpath, entry)\n\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expire_file(filepath):\n    load_message.cache_clear()\n    orm.delete(pa for pa in model.PathAlias if pa.entry.file_path == filepath)\n    orm.delete(item for item in model.Entry if item.file_path == filepath)\n    orm.commit()", "response": "Expire a record for a missing file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expire_record(record):\n    load_message.cache_clear()\n\n    # This entry no longer exists so delete it, and anything that references it\n    # SQLite doesn't support cascading deletes so let's just clean up\n    # manually\n    orm.delete(pa for pa in model.PathAlias if pa.entry == record)\n    record.delete()\n    orm.commit()", "response": "Expire a record for a missing entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a link potentially pre - redirected", "response": "def _link(self, *args, **kwargs):\n        \"\"\" Returns a link, potentially pre-redirected \"\"\"\n        if self._record.redirect_url:\n            return links.resolve(self._record.redirect_url,\n                                 self.search_path, kwargs.get('absolute'))\n\n        return self._permalink(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _permalink(self, absolute=False, expand=True, **kwargs):\n        return flask.url_for('entry',\n                             entry_id=self._record.id,\n                             category=self._record.category if expand else None,\n                             slug_text=self._record.slug_text if expand else None,\n                             _external=absolute,\n                             **kwargs)", "response": "Returns a canonical URL for the item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the message payload", "response": "def _message(self):\n        \"\"\" get the message payload \"\"\"\n        filepath = self._record.file_path\n        try:\n            return load_message(filepath)\n        except FileNotFoundError:\n            expire_file(filepath)\n            empty = email.message.Message()\n            empty.set_payload('')\n            return empty"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef body(self):\n        body, _, is_markdown = self._entry_content\n        return TrueCallableProxy(\n            self._get_markup,\n            body,\n            is_markdown) if body else CallableProxy(None)", "response": "Get the above - the - fold entry body text"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef more(self):\n        _, more, is_markdown = self._entry_content\n        return TrueCallableProxy(\n            self._get_markup,\n            more,\n            is_markdown) if more else CallableProxy(None)", "response": "Get the below - the - fold entry body text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the OpenGraph card for the entry.", "response": "def card(self):\n        \"\"\" Get the entry's OpenGraph card \"\"\"\n        body, more, is_markdown = self._entry_content\n        return TrueCallableProxy(\n            self._get_card,\n            body or more) if is_markdown else CallableProxy(None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the entry s summary text", "response": "def summary(self):\n        \"\"\" Get the entry's summary text \"\"\"\n        if self.get('Summary'):\n            return self.get('Summary')\n\n        body, more, is_markdown = self._entry_content\n        return TrueCallableProxy(\n            self._get_summary,\n            body or more) if is_markdown else CallableProxy(None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the date of last modification of the file", "response": "def last_modified(self):\n        \"\"\" Get the date of last file modification \"\"\"\n        if self.get('Last-Modified'):\n            return arrow.get(self.get('Last-Modified'))\n        return self.date"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_markup(self, text, is_markdown, **kwargs):\n        if is_markdown:\n            return markdown.to_html(\n                text,\n                config=kwargs,\n                search_path=self.search_path)\n\n        return html_entry.process(\n            text,\n            config=kwargs,\n            search_path=self.search_path)", "response": "get the rendered markup for an entry in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders out the tags for a Twitter card for this entry.", "response": "def _get_card(self, text, **kwargs):\n        \"\"\" Render out the tags for a Twitter/OpenGraph card for this entry. \"\"\"\n\n        def og_tag(key, val):\n            \"\"\" produce an OpenGraph tag with the given key and value \"\"\"\n            return utils.make_tag('meta', {'property': key, 'content': val}, start_end=True)\n\n        tags = og_tag('og:title', self.title(markup=False))\n        tags += og_tag('og:url', self.link(absolute=True))\n\n        card = cards.extract_card(text, kwargs, self.search_path)\n        for image in card.images:\n            tags += og_tag('og:image', image)\n        if card.description:\n            tags += og_tag('og:description',\n                           self.get('Summary', card.description))\n\n        return flask.Markup(tags)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_summary(self, text, **kwargs):\n\n        card = cards.extract_card(text, kwargs, self.search_path)\n        return flask.Markup((card.description or '').strip())", "response": "Render out just the summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the previous entry in any particular category", "response": "def _previous(self, **kwargs):\n        \"\"\" Get the previous item in any particular category \"\"\"\n        spec = self._pagination_default_spec(kwargs)\n        spec.update(kwargs)\n\n        query = queries.build_query(spec)\n        query = queries.where_before_entry(query, self._record)\n\n        for record in query.order_by(orm.desc(model.Entry.local_date),\n                                     orm.desc(model.Entry.id))[:1]:\n            return Entry(record)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _next(self, **kwargs):\n        spec = self._pagination_default_spec(kwargs)\n        spec.update(kwargs)\n\n        query = queries.build_query(spec)\n        query = queries.where_after_entry(query, self._record)\n\n        for record in query.order_by(model.Entry.local_date,\n                                     model.Entry.id)[:1]:\n            return Entry(record)\n        return None", "response": "Get the next entry in any particular category"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up the global configuration from an object", "response": "def setup(cfg):\n    \"\"\" set up the global configuration from an object \"\"\"\n\n    # copy the necessary configuration values over\n    this_module = sys.modules[__name__]\n    for name, value in cfg.items():\n        if hasattr(this_module, name):\n            setattr(this_module, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mime_type(template):\n    _, ext = os.path.splitext(template.filename)\n    return EXTENSION_MAP.get(ext, 'text/html; charset=utf-8')", "response": "infer the content - type from the extension"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map_template(category, template_list):\n\n    if isinstance(template_list, str):\n        template_list = [template_list]\n\n    for template in template_list:\n        path = os.path.normpath(category)\n        while path is not None:\n            for extension in ['', '.html', '.htm', '.xml', '.json']:\n                candidate = os.path.join(path, template + extension)\n                file_path = os.path.join(config.template_folder, candidate)\n                if os.path.isfile(file_path):\n                    return Template(template, candidate, file_path)\n            parent = os.path.dirname(path)\n            if parent != path:\n                path = parent\n            else:\n                path = None", "response": "Given a file path and an acceptable list of templates return the best - matching template s path relative to the configured template_folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an entry or a category return the path to a related template", "response": "def get_template(template, relation):\n    \"\"\" Given an entry or a category, return the path to a related template \"\"\"\n    if isinstance(relation, Entry):\n        path = relation.category.path\n    elif isinstance(relation, Category):\n        path = relation.path\n    else:\n        path = relation\n\n    tmpl = map_template(path, template)\n    return tmpl.filename if tmpl else None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a function that gets an image", "response": "def image_function(template=None, entry=None, category=None):\n    \"\"\" Get a function that gets an image \"\"\"\n\n    path = []\n\n    if entry is not None:\n        path += entry.search_path\n    if category is not None:\n        # Since the category might be different than the entry's category we add\n        # this too\n        path += category.search_path\n    if template is not None:\n        path.append(os.path.join(\n            config.content_folder,\n            os.path.dirname(template.filename)))\n\n    return lambda filename: image.get_image(filename, path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_publ_template(template, **kwargs):\n    text = render_template(\n        template.filename,\n        template=template,\n        image=image_function(\n            template=template,\n            category=kwargs.get('category'),\n            entry=kwargs.get('entry')),\n        **kwargs\n    )\n\n    return text, caching.get_etag(text)", "response": "Render out a template providing the image function based on the args."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_error(category, error_message, error_codes, exception=None):\n\n    if isinstance(error_codes, int):\n        error_codes = [error_codes]\n\n    error_code = error_codes[0]\n    template_list = [str(code) for code in error_codes]\n    template_list.append(str(int(error_code / 100) * 100))\n    template_list.append('error')\n\n    template = map_template(category, template_list)\n    if template:\n        return render_publ_template(\n            template,\n            _url_root=request.url_root,\n            category=Category(category),\n            error={'code': error_code, 'message': error_message},\n            exception=exception)[0], error_code\n\n    # no template found, so fall back to default Flask handler\n    return flask.abort(error_code)", "response": "Render an error page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncatch - all renderer for the top - level exception handler", "response": "def render_exception(error):\n    \"\"\" Catch-all renderer for the top-level exception handler \"\"\"\n    _, _, category = str.partition(request.path, '/')\n\n    qsize = index.queue_length()\n    if isinstance(error, http_error.NotFound) and qsize:\n        response = flask.make_response(render_error(\n            category, \"Site reindex in progress (qs={})\".format(qsize), 503))\n        response.headers['Retry-After'] = qsize\n        response.headers['Refresh'] = max(5, qsize / 5)\n        return response, 503\n\n    if isinstance(error, http_error.HTTPException):\n        return render_error(category, error.name, error.code, exception={\n            'type': type(error).__name__,\n            'str': error.description,\n            'args': error.args\n        })\n\n    return render_error(category, \"Exception occurred\", 500, exception={\n        'type': type(error).__name__,\n        'str': str(error),\n        'args': error.args\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders a known path - alias.", "response": "def render_path_alias(path):\n    \"\"\" Render a known path-alias (used primarily for forced .php redirects) \"\"\"\n\n    redir = path_alias.get_redirect('/' + path)\n    if not redir:\n        raise http_error.NotFound(\"Path redirection not found\")\n    return redir"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_category(category='', template=None):\n    # pylint:disable=too-many-return-statements\n\n    # See if this is an aliased path\n    redir = get_redirect()\n    if redir:\n        return redir\n\n    # Forbidden template types\n    if template and template.startswith('_'):\n        raise http_error.Forbidden(\"Template is private\")\n    if template in ['entry', 'error']:\n        raise http_error.BadRequest(\"Invalid view requested\")\n\n    if category:\n        # See if there's any entries for the view...\n        if not orm.select(e for e in model.Entry if e.category == category or\n                          e.category.startswith(category + '/')):\n            raise http_error.NotFound(\"No such category\")\n\n    if not template:\n        template = Category(category).get('Index-Template') or 'index'\n\n    tmpl = map_template(category, template)\n\n    if not tmpl:\n        # this might actually be a malformed category URL\n        test_path = '/'.join((category, template)) if category else template\n        logger.debug(\"Checking for malformed category %s\", test_path)\n        record = orm.select(\n            e for e in model.Entry if e.category == test_path).exists()\n        if record:\n            return redirect(url_for('category', category=test_path, **request.args))\n\n        # nope, we just don't know what this is\n        raise http_error.NotFound(\"No such view\")\n\n    view_spec = view.parse_view_spec(request.args)\n    view_spec['category'] = category\n    view_obj = view.View(view_spec)\n\n    rendered, etag = render_publ_template(\n        tmpl,\n        _url_root=request.url_root,\n        category=Category(category),\n        view=view_obj)\n\n    if request.if_none_match.contains(etag):\n        return 'Not modified', 304\n\n    return rendered, {'Content-Type': mime_type(tmpl),\n                      'ETag': etag}", "response": "Render a category page."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering an entry page.", "response": "def render_entry(entry_id, slug_text='', category=''):\n    \"\"\" Render an entry page.\n\n    Arguments:\n\n    entry_id -- The numeric ID of the entry to render\n    slug_text -- The expected URL slug text\n    category -- The expected category\n    \"\"\"\n\n    # pylint: disable=too-many-return-statements\n\n    # check if it's a valid entry\n    record = model.Entry.get(id=entry_id)\n    if not record:\n        # It's not a valid entry, so see if it's a redirection\n        path_redirect = get_redirect()\n        if path_redirect:\n            return path_redirect\n\n        logger.info(\"Attempted to retrieve nonexistent entry %d\", entry_id)\n        raise http_error.NotFound(\"No such entry\")\n\n    # see if the file still exists\n    if not os.path.isfile(record.file_path):\n        expire_record(record)\n\n        # See if there's a redirection\n        path_redirect = get_redirect()\n        if path_redirect:\n            return path_redirect\n\n        raise http_error.NotFound(\"No such entry\")\n\n    # Show an access denied error if the entry has been set to draft mode\n    if record.status == model.PublishStatus.DRAFT.value:\n        raise http_error.Forbidden(\"Entry not available\")\n    # Show a gone error if the entry has been deleted\n    if record.status == model.PublishStatus.GONE.value:\n        raise http_error.Gone()\n\n    # check if the canonical URL matches\n    if record.category != category or record.slug_text != slug_text:\n        # This could still be a redirected path...\n        path_redirect = get_redirect()\n        if path_redirect:\n            return path_redirect\n\n        # Redirect to the canonical URL\n        return redirect(url_for('entry',\n                                entry_id=entry_id,\n                                category=record.category,\n                                slug_text=record.slug_text))\n\n    # if the entry canonically redirects, do that now\n    entry_redirect = record.redirect_url\n    if entry_redirect:\n        return redirect(entry_redirect)\n\n    entry_template = (record.entry_template\n                      or Category(category).get('Entry-Template')\n                      or 'entry')\n\n    tmpl = map_template(category, entry_template)\n    if not tmpl:\n        raise http_error.BadRequest(\"Missing entry template\")\n\n    # Get the viewable entry\n    entry_obj = Entry(record)\n\n    # does the entry-id header mismatch? If so the old one is invalid\n    if int(entry_obj.get('Entry-ID')) != record.id:\n        expire_record(record)\n        return redirect(url_for('entry', entry_id=int(entry_obj.get('Entry-Id'))))\n\n    rendered, etag = render_publ_template(\n        tmpl,\n        _url_root=request.url_root,\n        entry=entry_obj,\n        category=Category(category))\n\n    if request.if_none_match.contains(etag):\n        return 'Not modified', 304\n\n    return rendered, {'Content-Type': mime_type(tmpl),\n                      'ETag': etag}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a transparent chit for external sized images", "response": "def render_transparent_chit():\n    \"\"\" Render a transparent chit for external, sized images \"\"\"\n\n    if request.if_none_match.contains('chit') or request.if_modified_since:\n        return 'Not modified', 304\n\n    out_bytes = base64.b64decode(\n        \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\")\n    return out_bytes, {'Content-Type': 'image/gif', 'ETag': 'chit',\n                       'Last-Modified': 'Tue, 31 Jul 1990 08:00:00 -0000'}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a non - image asset associated with an entry", "response": "def retrieve_asset(filename):\n    \"\"\" Retrieves a non-image asset associated with an entry \"\"\"\n\n    record = model.Image.get(asset_name=filename)\n    if not record:\n        raise http_error.NotFound(\"File not found\")\n    if not record.is_asset:\n        raise http_error.Forbidden()\n\n    return flask.send_file(record.file_path, conditional=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning all pending tasks ; force will run all pending tasks whether they re pending or not.", "response": "def run(self, force=False):\n        \"\"\" Run all pending tasks; 'force' will run all tasks whether they're\n        pending or not. \"\"\"\n        now = time.time()\n        for func, spec in self.tasks.items():\n            if force or now >= spec.get('next_run', 0):\n                func()\n                spec['next_run'] = now + spec['interval']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_asset(file_path):\n    record = model.Image.get(file_path=file_path)\n    fingerprint = ','.join((utils.file_fingerprint(file_path),\n                            str(RENDITION_VERSION)))\n    if not record or record.fingerprint != fingerprint:\n        # Reindex the file\n        logger.info(\"Updating image %s -> %s\", file_path, fingerprint)\n\n        # compute the md5sum; from https://stackoverflow.com/a/3431838/318857\n        md5 = hashlib.md5()\n        md5.update(bytes(RENDITION_VERSION))\n        with open(file_path, 'rb') as file:\n            for chunk in iter(lambda: file.read(16384), b\"\"):\n                md5.update(chunk)\n\n        values = {\n            'file_path': file_path,\n            'checksum': md5.hexdigest(),\n            'fingerprint': fingerprint,\n        }\n\n        try:\n            image = PIL.Image.open(file_path)\n            image = fix_orientation(image)\n        except IOError:\n            image = None\n\n        if image:\n            values['width'] = image.width\n            values['height'] = image.height\n            values['transparent'] = image.mode in ('RGBA', 'P')\n            values['is_asset'] = False\n        else:\n            # PIL could not figure out what file type this is, so treat it as\n            # an asset\n            values['is_asset'] = True\n            values['asset_name'] = os.path.join(values['checksum'][:5],\n                                                os.path.basename(file_path))\n        record = model.Image.get(file_path=file_path)\n        if record:\n            record.set(**values)\n        else:\n            record = model.Image(**values)\n        orm.commit()\n\n    return record", "response": "Get the database record for an asset file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_image(path, search_path):\n\n    if path.startswith('@'):\n        return StaticImage(path[1:], search_path)\n\n    if path.startswith('//') or '://' in path:\n        return RemoteImage(path, search_path)\n\n    if os.path.isabs(path):\n        file_path = utils.find_file(os.path.relpath(\n            path, '/'), config.content_folder)\n    else:\n        file_path = utils.find_file(path, search_path)\n    if not file_path:\n        return ImageNotFound(path, search_path)\n\n    record = _get_asset(file_path)\n    if record.is_asset:\n        return FileAsset(record, search_path)\n    return LocalImage(record, search_path)", "response": "Get an Image object from the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses an arglist into a dict of keyword arguments for Image.", "response": "def parse_arglist(args):\n    \"\"\" Parses an arglist into arguments for Image, as a kwargs dict \"\"\"\n    # per https://stackoverflow.com/a/49723227/318857\n\n    args = 'f({})'.format(args)\n    tree = ast.parse(args)\n    funccall = tree.body[0].value\n\n    args = [ast.literal_eval(arg) for arg in funccall.args]\n    kwargs = {arg.arg: ast.literal_eval(arg.value)\n              for arg in funccall.keywords}\n\n    if len(args) > 2:\n        raise TypeError(\n            \"Expected at most 2 positional args but {} were given\".format(len(args)))\n\n    if len(args) >= 1:\n        kwargs['width'] = int(args[0])\n    if len(args) >= 2:\n        kwargs['height'] = int(args[1])\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_alt_text(alt):\n    match = re.match(r'([^\\{]*)(\\{(.*)\\})$', alt)\n    if match:\n        alt = match.group(1)\n        args = parse_arglist(match.group(3))\n    else:\n        args = {}\n\n    return alt, args", "response": "Parses the arguments out from a Publ - Markdown alt text into a tuple of text and args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_image_spec(spec):\n\n    # I was having trouble coming up with a single RE that did it right,\n    # so let's just break it down into sub-problems. First, parse out the\n    # alt text...\n    match = re.match(r'(.+)\\s+\\\"(.*)\\\"\\s*$', spec)\n    if match:\n        spec, title = match.group(1, 2)\n    else:\n        title = None\n\n    # and now parse out the arglist\n    match = re.match(r'([^\\{]*)(\\{(.*)\\})\\s*$', spec)\n    if match:\n        spec = match.group(1)\n        args = parse_arglist(match.group(3))\n    else:\n        args = {}\n\n    return spec, args, (title and html.unescape(title))", "response": "Parses out a Publ - Markdown image spec into a tuple of path args title"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_spec_list(image_specs, container_args):\n\n    spec_list = [spec.strip() for spec in image_specs.split('|')]\n    original_count = len(spec_list)\n\n    if 'count' in container_args:\n        if 'count_offset' in container_args:\n            spec_list = spec_list[container_args['count_offset']:]\n        spec_list = spec_list[:container_args['count']]\n\n    return spec_list, original_count", "response": "Given a list of specs and a set of container args return a tuple of\n    the final container argument list and the original list size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_async(filename):\n\n    if os.path.isfile(os.path.join(config.static_folder, filename)):\n        return flask.redirect(flask.url_for('static', filename=filename))\n\n    retry_count = int(flask.request.args.get('retry_count', 0))\n    if retry_count < 10:\n        time.sleep(0.25)  # ghastly hack to get the client to backoff a bit\n        return flask.redirect(flask.url_for('async',\n                                            filename=filename,\n                                            cb=random.randint(0, 2**48),\n                                            retry_count=retry_count + 1))\n\n    # the image isn't available yet; generate a placeholder and let the\n    # client attempt to re-fetch periodically, maybe\n    vals = [int(b) for b in hashlib.md5(\n        filename.encode('utf-8')).digest()[0:12]]\n    placeholder = PIL.Image.new('RGB', (2, 2))\n    placeholder.putdata(list(zip(vals[0::3], vals[1::3], vals[2::3])))\n    outbytes = io.BytesIO()\n    placeholder.save(outbytes, \"PNG\")\n    outbytes.seek(0)\n\n    response = flask.make_response(\n        flask.send_file(outbytes, mimetype='image/png'))\n    response.headers['Refresh'] = 5\n    return response", "response": "Asynchronously fetch an image"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of attributes for the image.", "response": "def get_img_attrs(self, style=None, **kwargs):\n        \"\"\" Get an attribute list (src, srcset, style, et al) for the image.\n\n        style -- an optional list of CSS style fragments\n\n        Returns: a dict of attributes e.g. {'src':'foo.jpg','srcset':'foo.jpg 1x, bar.jpg 2x']\n        \"\"\"\n\n        add = {}\n        if 'prefix' in kwargs:\n            attr_prefixes = kwargs.get('prefix')\n            if isinstance(kwargs['prefix'], str):\n                attr_prefixes = [attr_prefixes]\n\n            for prefix in attr_prefixes:\n                for k, val in kwargs.items():\n                    if k.startswith(prefix):\n                        add[k[len(prefix):]] = val\n\n        return self._get_img_attrs(style, {**kwargs, **add})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds an image tag for the specified image options.", "response": "def get_img_tag(self, title='', alt_text='', **kwargs):\n        \"\"\" Build a <img> tag for the image with the specified options.\n\n        Returns: an HTML fragment. \"\"\"\n\n        try:\n            style = []\n\n            for key in ('img_style', 'style'):\n                if key in kwargs:\n                    if isinstance(kwargs[key], (list, tuple, set)):\n                        style += list(kwargs[key])\n                    else:\n                        style.append(kwargs[key])\n\n            if 'shape' in kwargs:\n                shape = self._get_shape_style(**kwargs)\n                if shape:\n                    style.append(\"shape-outside: url('{}')\".format(shape))\n\n            attrs = {\n                'alt': alt_text,\n                'title': title,\n                **self.get_img_attrs(style, **kwargs)\n            }\n\n            return flask.Markup(\n                self._wrap_link_target(\n                    kwargs,\n                    utils.make_tag(\n                        'img', attrs, start_end=kwargs.get('xhtml')),\n                    title))\n        except FileNotFoundError as error:\n            text = '<span class=\"error\">Image not found: <code>{}</code>'.format(\n                html.escape(error.filename))\n            if ' ' in error.filename:\n                text += ' (Did you forget a <code>|</code>?)'\n            text += '</span>'\n            return flask.Markup(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the CSS background attributes for an element.", "response": "def get_css_background(self, uncomment=False, **kwargs):\n        \"\"\" Get the CSS background attributes for an element.\n\n        Additional arguments:\n\n        uncomment -- surround the attributes with `*/` and `/*` so that the\n            template tag can be kept inside a comment block, to keep syntax\n            highlighters happy\n        \"\"\"\n\n        text = self._css_background(**kwargs)\n        if uncomment:\n            text = ' */ {} /* '.format(text)\n\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fullsize(self, kwargs):\n        fullsize_args = {}\n\n        if 'absolute' in kwargs:\n            fullsize_args['absolute'] = kwargs['absolute']\n\n        for key in ('width', 'height', 'quality', 'format', 'background', 'crop'):\n            fsk = 'fullsize_' + key\n            if fsk in kwargs:\n                fullsize_args[key] = kwargs[fsk]\n\n        img_fullsize, _ = self.get_rendition(1, **fullsize_args)\n        return img_fullsize", "response": "Get the fullsize rendition URL"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a link that points to the fullsize rendition specified", "response": "def _fullsize_link_tag(self, kwargs, title):\n        \"\"\" Render a <a href> that points to the fullsize rendition specified \"\"\"\n\n        return utils.make_tag('a', {\n            'href': self.get_fullsize(kwargs),\n            'data-lightbox': kwargs['gallery_id'],\n            'title': title\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_alias(alias, **kwargs):\n\n    spec = alias.split()\n    path = spec[0]\n\n    values = {**kwargs, 'path': path}\n\n    if len(spec) > 1:\n        values['template'] = spec[1]\n\n    record = model.PathAlias.get(path=path)\n    if record:\n        record.set(**values)\n    else:\n        record = model.PathAlias(**values)\n\n    orm.commit()\n    return record", "response": "Set a path alias."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an alias from the", "response": "def remove_alias(path):\n    \"\"\" Remove a path alias.\n\n    Arguments:\n\n    path -- the path to remove the alias of\n    \"\"\"\n    orm.delete(p for p in model.PathAlias if p.path == path)\n    orm.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_aliases(target):\n\n    if isinstance(target, model.Entry):\n        orm.delete(p for p in model.PathAlias if p.entry == target)\n    elif isinstance(target, model.Category):\n        orm.delete(p for p in model.PathAlias if p.category == target)\n    else:\n        raise TypeError(\"Unknown type {}\".format(type(target)))\n    orm.commit()", "response": "Remove all aliases to a destination"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_alias(path):\n    # pylint:disable=too-many-return-statements\n\n    record = model.PathAlias.get(path=path)\n\n    if not record:\n        return None, None\n\n    template = record.template if record.template != 'index' else None\n\n    if record.entry and record.entry.visible:\n        if record.template:\n            # a template was requested, so we go to the category page\n            category = (record.category.category\n                        if record.category else record.entry.category)\n            return url_for('category',\n                           start=record.entry.id,\n                           template=template,\n                           category=category), True\n\n        from . import entry  # pylint:disable=cyclic-import\n        outbound = entry.Entry(record.entry).get('Redirect-To')\n        if outbound:\n            # The entry has a Redirect-To (soft redirect) header\n            return outbound, False\n\n        return url_for('entry',\n                       entry_id=record.entry.id,\n                       category=record.entry.category,\n                       slug_text=record.entry.slug_text), True\n\n    if record.category:\n        return url_for('category',\n                       category=record.category.category,\n                       template=template), True\n\n    if record.url:\n        # This is an outbound URL that might be changed by the user, so\n        # we don't do a 301 Permanently moved\n        return record.url, False\n\n    return None, None", "response": "Get a path alias for a single path Returns a tuple of url is_permanent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_redirect(paths):\n\n    if isinstance(paths, str):\n        paths = [paths]\n\n    for path in paths:\n        url, permanent = get_alias(path)\n        if url:\n            return redirect(url, 301 if permanent else 302)\n\n        url, permanent = current_app.get_path_regex(path)\n        if url:\n            return redirect(url, 301 if permanent else 302)\n\n    return None", "response": "Get a redirect from a single path or a list of paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfixing the orientation of the image.", "response": "def fix_orientation(image):\n    \"\"\" adapted from https://stackoverflow.com/a/30462851/318857\n\n        Apply Image.transpose to ensure 0th row of pixels is at the visual\n        top of the image, and 0th column is the visual left-hand side.\n        Return the original image if unable to determine the orientation.\n\n        As per CIPA DC-008-2012, the orientation field contains an integer,\n        1 through 8. Other values are reserved.\n    \"\"\"\n\n    exif_orientation_tag = 0x0112\n    exif_transpose_sequences = [\n        [],\n        [],\n        [PIL.Image.FLIP_LEFT_RIGHT],\n        [PIL.Image.ROTATE_180],\n        [PIL.Image.FLIP_TOP_BOTTOM],\n        [PIL.Image.FLIP_LEFT_RIGHT, PIL.Image.ROTATE_90],\n        [PIL.Image.ROTATE_270],\n        [PIL.Image.FLIP_TOP_BOTTOM, PIL.Image.ROTATE_90],\n        [PIL.Image.ROTATE_90],\n    ]\n\n    try:\n        # pylint:disable=protected-access\n        orientation = image._getexif()[exif_orientation_tag]\n        sequence = exif_transpose_sequences[orientation]\n        return functools.reduce(type(image).transpose, sequence, image)\n    except (TypeError, AttributeError, KeyError):\n        # either no EXIF tags or no orientation tag\n        pass\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef thread_pool():\n        if not LocalImage._thread_pool:\n            logger.info(\"Starting LocalImage threadpool\")\n            LocalImage._thread_pool = concurrent.futures.ThreadPoolExecutor(\n                thread_name_prefix=\"Renderer\")\n        return LocalImage._thread_pool", "response": "Get the rendition threadpool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the rendition for this image generating it if necessary.", "response": "def get_rendition(self, output_scale=1, **kwargs):\n        # pylint:disable=too-many-locals\n        \"\"\"\n        Get the rendition for this image, generating it if necessary.\n        Returns a tuple of `(relative_path, width, height)`, where relative_path\n        is relative to the static file directory (i.e. what one would pass into\n        `get_static()`)\n\n        output_scale -- the upsample factor for the requested rendition\n\n        Keyword arguments:\n\n        scale -- the downsample factor for the base rendition\n        scale_min_width -- the minimum width after downsampling\n        scale_min_height -- the minimum height after downsampling\n        crop -- box to crop the original image into (left, top, right, bottom)\n        width -- the width to target\n        height -- the height to target\n        max_width -- the maximum width\n        max_height -- the maximum height\n        resize -- how to fit the width and height; \"fit\", \"fill\", or \"stretch\"\n        fill_crop_x -- horizontal offset fraction for resize=\"fill\"\n        fill_crop_y -- vertical offset fraction for resize=\"fill\"\n        format -- output format\n        background -- background color when converting transparent to opaque\n        quality -- the JPEG quality to save the image as\n        quantize -- how large a palette to use for GIF or PNG images\n        \"\"\"\n\n        basename, ext = os.path.splitext(\n            os.path.basename(self._record.file_path))\n        basename = utils.make_slug(basename)\n\n        if kwargs.get('format'):\n            ext = '.' + kwargs['format']\n\n        # The spec for building the output filename\n        out_spec = [basename, self._record.checksum[-10:]]\n\n        out_args = {}\n        if ext in ['.png', '.jpg', '.jpeg']:\n            out_args['optimize'] = True\n\n        crop = self._parse_tuple_string(kwargs.get('crop'))\n\n        size, box = self.get_rendition_size(kwargs, output_scale, crop)\n        box = self._adjust_crop_box(box, crop)\n\n        if size and (size[0] < self._record.width or size[1] < self._record.height):\n            out_spec.append('x'.join([str(v) for v in size]))\n\n        if box:\n            # pylint:disable=not-an-iterable\n            out_spec.append('-'.join([str(v) for v in box]))\n\n        # Set RGBA flattening options\n        flatten = self._record.transparent and ext not in ['.png', '.gif']\n        if flatten and 'background' in kwargs:\n            bg_color = kwargs['background']\n            if isinstance(bg_color, (tuple, list)):\n                out_spec.append('b' + '-'.join([str(a) for a in bg_color]))\n            else:\n                out_spec.append('b' + str(bg_color))\n\n        # Set JPEG quality\n        if ext in ('.jpg', '.jpeg') and kwargs.get('quality'):\n            out_spec.append('q' + str(kwargs['quality']))\n            out_args['quality'] = kwargs['quality']\n        if ext in ('.jpg', '.jpeg'):\n            out_args['optimize'] = True\n\n        # Build the output filename\n        out_basename = '_'.join([str(s) for s in out_spec]) + ext\n        out_rel_path = os.path.join(\n            config.image_output_subdir,\n            self._record.checksum[0:2],\n            self._record.checksum[2:6],\n            out_basename)\n        out_fullpath = os.path.join(config.static_folder, out_rel_path)\n\n        if os.path.isfile(out_fullpath):\n            os.utime(out_fullpath)\n            return utils.static_url(out_rel_path, kwargs.get('absolute')), size\n\n        LocalImage.thread_pool().submit(\n            self._render, out_fullpath, size, box, flatten, kwargs, out_args)\n\n        return flask.url_for('async', filename=out_rel_path, _external=kwargs.get('absolute')), size"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _adjust_crop_box(box, crop):\n\n        if crop and box:\n            # Both boxes are the same size; just line them up.\n            return (box[0] + crop[0], box[1] + crop[1],\n                    box[2] + crop[0], box[3] + crop[1])\n\n        if crop:\n            # We don't have a fit box, so just convert the crop box\n            return (crop[0], crop[1], crop[0] + crop[2], crop[1] + crop[3])\n\n        # We don't have a crop box, so return the fit box (even if it's None)\n        return box", "response": "Given a fit box and a crop box adjust one to the other"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a string into a tuple", "response": "def _parse_tuple_string(argument):\n        \"\"\" Return a tuple from parsing 'a,b,c,d' -> (a,b,c,d) \"\"\"\n        if isinstance(argument, str):\n            return tuple(int(p.strip()) for p in argument.split(','))\n        return argument"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps to determine the overall rendition size and cropping boxsize and image size", "response": "def get_rendition_size(self, spec, output_scale, crop):\n        \"\"\"\n        Wrapper to determine the overall rendition size and cropping box\n\n        Returns tuple of (size,box)\n        \"\"\"\n\n        if crop:\n            # Use the cropping rectangle size\n            _, _, width, height = crop\n        else:\n            # Use the original image size\n            width = self._record.width\n            height = self._record.height\n\n        mode = spec.get('resize', 'fit')\n        if mode == 'fit':\n            return self.get_rendition_fit_size(spec, width, height, output_scale)\n\n        if mode == 'fill':\n            return self.get_rendition_fill_size(spec, width, height, output_scale)\n\n        if mode == 'stretch':\n            return self.get_rendition_stretch_size(spec, width, height, output_scale)\n\n        raise ValueError(\"Unknown resize mode {}\".format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the size of the rendition based on the provided input size and the output size.", "response": "def get_rendition_fit_size(spec, input_w, input_h, output_scale):\n        \"\"\" Determine the scaled size based on the provided spec \"\"\"\n\n        width = input_w\n        height = input_h\n\n        scale = spec.get('scale')\n        if scale:\n            width = width / scale\n            height = height / scale\n\n        min_width = spec.get('scale_min_width')\n        if min_width and width < min_width:\n            height = height * min_width / width\n            width = min_width\n\n        min_height = spec.get('scale_min_height')\n        if min_height and height < min_height:\n            width = width * min_height / height\n            height = min_height\n\n        tgt_width, tgt_height = spec.get('width'), spec.get('height')\n\n        if tgt_width and width > tgt_width:\n            height = height * tgt_width / width\n            width = tgt_width\n\n        if tgt_height and height > tgt_height:\n            width = width * tgt_height / height\n            height = tgt_height\n\n        tgt_width, tgt_height = spec.get('max_width'), spec.get('max_height')\n\n        if tgt_width and width > tgt_width:\n            height = height * tgt_width / width\n            width = tgt_width\n\n        if tgt_height and height > tgt_height:\n            width = width * tgt_height / height\n            height = tgt_height\n\n        width = width * output_scale\n        height = height * output_scale\n\n        # Never scale to larger than the base rendition\n        width = min(round(width), input_w)\n        height = min(round(height), input_h)\n\n        return (width, height), None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the size of the rendition based on the input size and the input scale.", "response": "def get_rendition_fill_size(spec, input_w, input_h, output_scale):\n        \"\"\" Determine the scale-crop size given the provided spec \"\"\"\n\n        width = input_w\n        height = input_h\n\n        scale = spec.get('scale')\n        if scale:\n            width = width / scale\n            height = height / scale\n\n        if spec.get('scale_min_width'):\n            width = max(width, spec['spec_min_width'])\n\n        if spec.get('scale_min_height'):\n            height = max(height, spec['scale_min_height'])\n\n        if spec.get('width'):\n            width = min(width, spec['width'])\n        if spec.get('max_width'):\n            width = min(width, spec['max_width'])\n\n        if spec.get('height'):\n            height = min(height, spec['height'])\n        if spec.get('max_height'):\n            height = min(height, spec['max_height'])\n\n        width = width * output_scale\n        height = height * output_scale\n\n        # Never scale to larger than the base rendition (but keep the output\n        # aspect)\n        if width > input_w:\n            height = height * input_w / width\n            width = input_w\n\n        if height > input_h:\n            width = width * input_h / height\n            height = input_h\n\n        # Determine the box size\n        box_w = min(input_w, round(width * input_h / height))\n        box_h = min(input_h, round(height * input_w / width))\n\n        # Box offset\n        box_x = round((input_w - box_w) * spec.get('fill_crop_x', 0.5))\n        box_y = round((input_h - box_h) * spec.get('fill_crop_y', 0.5))\n\n        return (round(width), round(height)), (box_x, box_y, box_x + box_w, box_y + box_h)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the size of the rendition - stretch image given the provided spec.", "response": "def get_rendition_stretch_size(spec, input_w, input_h, output_scale):\n        \"\"\" Determine the scale-crop size given the provided spec \"\"\"\n\n        width = input_w\n        height = input_h\n\n        scale = spec.get('scale')\n        if scale:\n            width = width / scale\n            height = height / scale\n\n        min_width = spec.get('scale_min_width')\n        if min_width and width < min_width:\n            width = min_width\n\n        min_height = spec.get('scale_min_height')\n        if min_height and height < min_height:\n            height = min_height\n\n        tgt_width, tgt_height = spec.get('width'), spec.get('height')\n\n        if tgt_width and width > tgt_width:\n            width = tgt_width\n\n        tgt_height = spec.get('height')\n        if tgt_height and height > tgt_height:\n            height = tgt_height\n\n        tgt_width, tgt_height = spec.get('max_width'), spec.get('max_height')\n\n        if tgt_width and width > tgt_width:\n            width = tgt_width\n\n        tgt_height = spec.get('height')\n        if tgt_height and height > tgt_height:\n            height = tgt_height\n\n        width = width * output_scale\n        height = height * output_scale\n\n        return (round(width), round(height)), None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flatten(image, bgcolor=None):\n        if bgcolor:\n            background = PIL.Image.new('RGB', image.size, bgcolor)\n            background.paste(image, mask=image.split()[3])\n            return background\n\n        return image.convert('RGB')", "response": "Flatten an image with an optional background color"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_renditions(self, kwargs):\n        img_1x, size = self.get_rendition(\n            1, **utils.remap_args(kwargs, {\"quality\": \"quality_ldpi\"}))\n        img_2x, _ = self.get_rendition(\n            2, **utils.remap_args(kwargs, {\"quality\": \"quality_hdpi\"}))\n\n        return (img_1x, img_2x, size)", "response": "Get a bunch of renditions ; returns a tuple of 1x 2x size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_img_attrs(self, style, kwargs):\n\n        # Get the 1x and 2x renditions\n        img_1x, img_2x, size = self._get_renditions(kwargs)\n\n        return {\n            'src': img_1x,\n            'width': size[0],\n            'height': size[1],\n            'srcset': \"{} 1x, {} 2x\".format(img_1x, img_2x) if img_1x != img_2x else None,\n            'style': ';'.join(style) if style else None,\n            'class': kwargs.get('class', kwargs.get('img_class')),\n            'id': kwargs.get('img_id')\n        }", "response": "Get the attributes of an img tag for this image hidpi - aware"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _css_background(self, **kwargs):\n\n        # Get the 1x and 2x renditions\n        img_1x, img_2x, _ = self._get_renditions(kwargs)\n\n        tmpl = 'background-image: url(\"{s1x}\");'\n        if img_1x != img_2x:\n            image_set = 'image-set(url(\"{s1x}\") 1x, url(\"{s2x}\") 2x)'\n            tmpl += 'background-image: {ss};background-image: -webkit-{ss};'.format(\n                ss=image_set)\n        return tmpl.format(s1x=img_1x, s2x=img_2x)", "response": "Return the CSS specifiers for this as a hidpi - capable background image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process(text, config, search_path):\n    processor = HTMLEntry(config, search_path)\n    processor.feed(text)\n    text = processor.get_data()\n\n    if not config.get('no_smartquotes'):\n        text = misaka.smartypants(text)\n\n    return flask.Markup(text)", "response": "Process an HTML entry s HTML"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles a tag. attrs -- the attributes of the tag self_closing -- whether this is self-closing", "response": "def _handle_tag(self, tag, attrs, self_closing):\n        \"\"\" Handle a tag.\n\n        attrs -- the attributes of the tag\n        self_closing -- whether this is self-closing\n        \"\"\"\n\n        if tag.lower() == 'img':\n            attrs = self._image_attrs(attrs)\n\n        # Remap the attributes\n        out_attrs = []\n        for key, val in attrs:\n            if (key.lower() == 'href'\n                    or (key.lower() == 'src' and not tag.lower() == 'img')):\n                out_attrs.append((key, links.resolve(\n                    val, self._search_path, self._config.get('absolute'))))\n            else:\n                out_attrs.append((key, val))\n\n        self.append(\n            utils.make_tag(\n                tag,\n                out_attrs,\n                self_closing))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrewrites the width and height attributes on an image tag.", "response": "def _image_attrs(self, attrs):\n        \"\"\" Rewrite the SRC attribute on an <img> tag, possibly adding a SRCSET.\n        \"\"\"\n\n        path = None\n        config = {**self._config}\n\n        for key, val in attrs:\n            if key.lower() == 'width' or key.lower() == 'height':\n                try:\n                    config[key.lower()] = int(val)\n                except ValueError:\n                    pass\n            elif key.lower() == 'src':\n                path = val\n\n        img_path, img_args, _ = image.parse_image_spec(path)\n        img = image.get_image(img_path, self._search_path)\n\n        for key, val in img_args.items():\n            if val and key not in config:\n                config[key] = val\n\n        try:\n            img_attrs = img.get_img_attrs(**config)\n        except FileNotFoundError as error:\n            return [('data-publ-error', 'file not found: {}'.format(error.filename))]\n\n        # return the original attr list with the computed overrides in place\n        return [(key, val) for key, val in attrs\n                if key.lower() not in img_attrs] + list(img_attrs.items())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scan_file(fullpath, relpath, assign_id):\n\n    logger.debug(\"Scanning file: %s (%s) %s\", fullpath, relpath, assign_id)\n\n    def do_scan():\n        \"\"\" helper function to do the scan and gather the result \"\"\"\n        _, ext = os.path.splitext(fullpath)\n\n        try:\n            if ext in ENTRY_TYPES:\n                logger.info(\"Scanning entry: %s\", fullpath)\n                return entry.scan_file(fullpath, relpath, assign_id)\n\n            if ext in CATEGORY_TYPES:\n                logger.info(\"Scanning meta info: %s\", fullpath)\n                return category.scan_file(fullpath, relpath)\n\n            return None\n        except:  # pylint: disable=bare-except\n            logger.exception(\"Got error parsing %s\", fullpath)\n            return False\n\n    result = do_scan()\n    if result is False and not assign_id:\n        logger.info(\"Scheduling fixup for %s\", fullpath)\n        THREAD_POOL.submit(scan_file, fullpath, relpath, True)\n    else:\n        logger.debug(\"%s complete\", fullpath)\n        if result:\n            set_fingerprint(fullpath)\n        SCHEDULED_FILES.remove(fullpath)", "response": "Scan a file for the index and return the index of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_last_fingerprint(fullpath):\n    record = model.FileFingerprint.get(file_path=fullpath)\n    if record:\n        return record.fingerprint\n    return None", "response": "Get the last known modification time for a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the last known modification time for a file", "response": "def set_fingerprint(fullpath, fingerprint=None):\n    \"\"\" Set the last known modification time for a file \"\"\"\n    try:\n        fingerprint = fingerprint or utils.file_fingerprint(fullpath)\n\n        record = model.FileFingerprint.get(file_path=fullpath)\n        if record:\n            record.set(fingerprint=fingerprint,\n                       file_mtime=os.stat(fullpath).st_mtime)\n        else:\n            record = model.FileFingerprint(\n                file_path=fullpath,\n                fingerprint=fingerprint,\n                file_mtime=os.stat(fullpath).st_mtime)\n        orm.commit()\n    except FileNotFoundError:\n        orm.delete(fp for fp in model.FileFingerprint if fp.file_path == fullpath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart background scanning a directory for changes.", "response": "def background_scan(content_dir):\n    \"\"\" Start background scanning a directory for changes \"\"\"\n    observer = watchdog.observers.Observer()\n    observer.schedule(IndexWatchdog(content_dir),\n                      content_dir, recursive=True)\n    logging.info(\"Watching %s for changes\", content_dir)\n    observer.start()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npruning any files which are missing from the specified table.", "response": "def prune_missing(table):\n    \"\"\" Prune any files which are missing from the specified table \"\"\"\n    try:\n        for item in table.select():\n            if not os.path.isfile(item.file_path):\n                logger.info(\"File disappeared: %s\", item.file_path)\n                item.delete()\n    except:  # pylint:disable=bare-except\n        logger.exception(\"Error pruning %s\", table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scan_index(content_dir):\n\n    def scan_directory(root, files):\n        \"\"\" Helper function to scan a single directory \"\"\"\n        try:\n            for file in files:\n                fullpath = os.path.join(root, file)\n                relpath = os.path.relpath(fullpath, content_dir)\n\n                fingerprint = utils.file_fingerprint(fullpath)\n                last_fingerprint = get_last_fingerprint(fullpath)\n                if fingerprint != last_fingerprint and SCHEDULED_FILES.add(fullpath):\n                    scan_file(fullpath, relpath, False)\n        except:  # pylint:disable=bare-except\n            logger.exception(\"Got error parsing directory %s\", root)\n\n    for root, _, files in os.walk(content_dir, followlinks=True):\n        THREAD_POOL.submit(scan_directory, root, files)\n\n    for table in (model.Entry, model.Category, model.Image, model.FileFingerprint):\n        THREAD_POOL.submit(prune_missing, table)", "response": "Scan all files in a content directory and prune missing entries"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, item):\n        with self.lock:\n            if item in self.set:\n                return False\n            self.set.add(item)\n            return True", "response": "Add an item to the set and return whether it was newly added"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves an item from the set returning if it was present", "response": "def remove(self, item):\n        \"\"\" Remove an item from the set, returning if it was present \"\"\"\n        with self.lock:\n            if item in self.set:\n                self.set.remove(item)\n                return True\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a file in the content directory.", "response": "def update_file(self, fullpath):\n        \"\"\" Update a file \"\"\"\n        if SCHEDULED_FILES.add(fullpath):\n            logger.debug(\"Scheduling reindex of %s\", fullpath)\n            relpath = os.path.relpath(fullpath, self.content_dir)\n            THREAD_POOL.submit(scan_file, fullpath, relpath, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_modified(self, event):\n        logger.debug(\"file modified: %s\", event.src_path)\n        if not event.is_directory:\n            self.update_file(event.src_path)", "response": "on_modified handler for file modified event"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_deleted(self, event):\n        logger.debug(\"File deleted: %s\", event.src_path)\n        if not event.is_directory:\n            self.update_file(event.src_path)", "response": "Called when a file is deleted."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a view specification from a request arg list", "response": "def parse_view_spec(args):\n    \"\"\" Parse a view specification from a request arg list \"\"\"\n\n    view_spec = {}\n\n    if 'date' in args:\n        view_spec['date'] = args['date']\n    elif 'id' in args:\n        view_spec['start'] = args['id']\n\n    if 'tag' in args:\n        view_spec['tag'] = args.getlist('tag')\n        if len(view_spec['tag']) == 1:\n            view_spec['tag'] = args['tag']\n\n    return view_spec"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deleted(self):\n        query = queries.build_query({**self.spec,\n                                     'future': False,\n                                     '_deleted': True})\n        return [Entry(e) for e in query]", "response": "Gets the deleted entries from the view"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the most recent modification time for all entries in the view", "response": "def last_modified(self):\n        \"\"\" Gets the most recent modification time for all entries in the view \"\"\"\n        if self.entries:\n            latest = max(self.entries, key=lambda x: x.last_modified)\n            return arrow.get(latest.last_modified)\n        return arrow.get()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the previous page in the list", "response": "def previous(self):\n        \"\"\" Gets the previous page, respecting sort order \"\"\"\n        if self._order_by == 'oldest':\n            return self.older\n        if self._order_by == 'newest':\n            return self.newer\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the next page in the list", "response": "def next(self):\n        \"\"\" Gets the next page, respecting sort order \"\"\"\n        if self._order_by == 'oldest':\n            return self.newer\n        if self._order_by == 'newest':\n            return self.older\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the newest entry in the view regardless of sort order", "response": "def newest(self):\n        \"\"\" Gets the newest entry in the view, regardless of sort order \"\"\"\n        if self._order_by == 'newest':\n            return self.first\n        if self._order_by == 'oldest':\n            return self.last\n        return max(self.entries, key=lambda x: (x.date, x.id))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the oldest entry in the view regardless of sort order", "response": "def oldest(self):\n        \"\"\" Gets the oldest entry in the view, regardless of sort order \"\"\"\n        if self._order_by == 'newest':\n            return self.last\n        if self._order_by == 'oldest':\n            return self.first\n        return min(self.entries, key=lambda x: (x.date, -x.id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef paging(self):\n        if 'date' in self.spec:\n            _, date_span, _ = utils.parse_date(self.spec['date'])\n            return date_span\n        return 'offset'", "response": "Gets the pagination type ; compatible with entry. archive"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of all pages for this view", "response": "def pages(self):\n        \"\"\" Gets a list of all pages for this view \"\"\"\n        cur = self\n        pages = []\n        while cur.previous:\n            cur = cur.previous\n        while cur:\n            pages.append(cur)\n            cur = cur.next\n        return pages"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tags(self):\n        tag_list = self.spec.get('tag', [])\n        if isinstance(tag_list, (list, set, tuple)):\n            return list(tag_list)\n        return [tag_list]", "response": "Returns a list of all the tags applied to this view"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the neighboring pages from this view.", "response": "def _pagination(self):\n        \"\"\" Compute the neighboring pages from this view.\n\n        Returns a tuple of older page, newer page.\n        \"\"\"\n\n        oldest = self.oldest\n        newest = self.newest\n\n        base = {key: val for key, val in self.spec.items()\n                if key not in OFFSET_PRIORITY}\n\n        oldest_neighbor = View({\n            **base,\n            'before': oldest,\n            'order': 'newest'\n        }).first if oldest else None\n\n        newest_neighbor = View({\n            **base,\n            'after': newest,\n            'order': 'oldest'\n        }).first if newest else None\n\n        if 'date' in self.spec:\n            return self._get_date_pagination(base, oldest_neighbor, newest_neighbor)\n\n        if 'count' in self.spec:\n            return self._get_count_pagination(base, oldest_neighbor, newest_neighbor)\n\n        # we're not paginating\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the pagination for date - based views", "response": "def _get_date_pagination(self, base, oldest_neighbor, newest_neighbor):\n        \"\"\" Compute the pagination for date-based views \"\"\"\n        _, span, date_format = utils.parse_date(self.spec['date'])\n\n        if newest_neighbor:\n            newer_date = newest_neighbor.date.span(span)[0]\n            newer_view = View({**base,\n                               'order': self._order_by,\n                               'date': newer_date.format(date_format)})\n        else:\n            newer_view = None\n\n        if oldest_neighbor:\n            older_date = oldest_neighbor.date.span(span)[0]\n            older_view = View({**base,\n                               'order': self._order_by,\n                               'date': older_date.format(date_format)})\n        else:\n            older_view = None\n\n        return older_view, newer_view"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the pagination for count - based views", "response": "def _get_count_pagination(self, base, oldest_neighbor, newest_neighbor):\n        \"\"\" Compute the pagination for count-based views \"\"\"\n\n        count = self.spec['count']\n\n        out_spec = {**base, 'count': count, 'order': self._order_by}\n\n        if self._order_by == 'newest':\n            older_view = View({**out_spec,\n                               'last': oldest_neighbor}) if oldest_neighbor else None\n\n            newer_count = View({**base,\n                                'first': newest_neighbor,\n                                'order': 'oldest',\n                                'count': count}) if newest_neighbor else None\n            newer_view = View({**out_spec,\n                               'last': newer_count.last}) if newer_count else None\n\n            return older_view, newer_view\n\n        if self._order_by == 'oldest':\n            older_count = View({**base,\n                                'last': oldest_neighbor,\n                                'order': 'newest',\n                                'count': count}) if oldest_neighbor else None\n            older_view = View({**out_spec,\n                               'first': older_count.last}) if older_count else None\n\n            newer_view = View({**out_spec,\n                               'first': newest_neighbor}) if newest_neighbor else None\n\n            return older_view, newer_view\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a view with the specified tags added", "response": "def tag_add(self, *tags):\n        \"\"\" Return a view with the specified tags added \"\"\"\n        return View({**self.spec, 'tag': list(set(self.tags) | set(tags))})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tag_remove(self, *tags):\n        return View({**self.spec, 'tag': list(set(self.tags) - set(tags))})", "response": "Return a view with the specified tags removed"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a view with the specified tags toggled", "response": "def tag_toggle(self, *tags):\n        \"\"\" Return a view with the specified tags toggled \"\"\"\n        return View({**self.spec, 'tag': list(set(self.tags) ^ set(tags))})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting Markdown text to HTML", "response": "def to_html(text, config, search_path):\n    \"\"\" Convert Markdown text to HTML \"\"\"\n    processor = misaka.Markdown(HtmlRenderer(config, search_path),\n                                extensions=ENABLED_EXTENSIONS)\n\n    text = processor(text)\n    if not config.get('no_smartquotes'):\n        text = misaka.smartypants(text)\n\n    return flask.Markup(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a Markdown title to HTML", "response": "def render_title(text, markup=True, no_smartquotes=False):\n    \"\"\" Convert a Markdown title to HTML \"\"\"\n\n    # HACK: If the title starts with something that looks like a list, save it\n    # for later\n    pfx, text = re.match(r'([0-9. ]*)(.*)', text).group(1, 2)\n    text = pfx + misaka.Markdown(TitleRenderer(),\n                                 extensions=TITLE_EXTENSIONS)(text)\n\n    if not markup:\n        strip = HTMLStripper()\n        strip.feed(text)\n        text = strip.get_data()\n\n    if not no_smartquotes:\n        text = misaka.smartypants(text)\n\n    return flask.Markup(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image(self, raw_url, title='', alt=''):\n        # pylint: disable=too-many-locals\n\n        text = ''\n\n        image_specs = raw_url\n        if title:\n            image_specs += ' \"{}\"'.format(title)\n\n        alt, container_args = image.parse_alt_text(alt)\n\n        container_args = {**self._config, **container_args}\n\n        spec_list, original_count = image.get_spec_list(\n            image_specs, container_args)\n\n        for spec in spec_list:\n            text += self._render_image(spec,\n                                       container_args,\n                                       alt)\n\n        if original_count > len(spec_list) and 'more_text' in container_args:\n            more_text = container_args['more_text'].format(\n                count=original_count,\n                remain=original_count - len(spec_list))\n            if 'more_link' in container_args:\n                more_text = '{a}{text}</a>'.format(\n                    text=more_text,\n                    a=utils.make_tag('a', {'href': container_args['more_link']}))\n            if 'more_class' in container_args:\n                more_text = '{div}{text}</div>'.format(\n                    text=more_text,\n                    div=utils.make_tag('div', {'class': container_args['more_class']}))\n            text += flask.Markup(more_text)\n\n        if text and (container_args.get('div_class') or\n                     container_args.get('div_style')):\n            text = '{tag}{text}</div>'.format(\n                tag=utils.make_tag('div',\n                                   {'class': container_args.get('div_class'),\n                                    'style': container_args.get('div_style')}),\n                text=text)\n\n        # if text is ''/falsy then misaka interprets this as a failed parse...\n        return text or ' '", "response": "Adapt a standard Markdown image to a generated rendition set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npass a code fence through pygments", "response": "def blockcode(self, text, lang):\n        \"\"\" Pass a code fence through pygments \"\"\"\n        if lang and self._config.get('highlight_syntax', 'True'):\n            try:\n                lexer = pygments.lexers.get_lexer_by_name(lang, stripall=True)\n            except pygments.lexers.ClassNotFound:\n                lexer = None\n\n            if lexer:\n                formatter = pygments.formatters.HtmlFormatter()  # pylint: disable=no-member\n                return pygments.highlight(text, lexer, formatter)\n\n        return '\\n<div class=\"highlight\"><pre>{}</pre></div>\\n'.format(\n            flask.escape(text.strip()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nemit a link potentially remapped based on our embed or static rules.", "response": "def link(self, content, link, title=''):\n        \"\"\" Emit a link, potentially remapped based on our embed or static rules \"\"\"\n\n        link = links.resolve(link, self._search_path,\n                             self._config.get('absolute'))\n\n        return '{}{}</a>'.format(\n            utils.make_tag('a', {\n                'href': link,\n                'title': title if title else None\n            }),\n            content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paragraph(content):\n\n        # if the content contains a top-level div then don't wrap it in a <p>\n        # tag\n        if content.startswith('<div') and content.endswith('</div>'):\n            return '\\n' + content + '\\n'\n\n        text = '<p>' + content + '</p>'\n        text = re.sub(r'<p>\\s*</p>', r'', text)\n        return text or ' '", "response": "emit a paragraph stripping out any leading or following empty paragraphs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _render_image(self, spec, container_args, alt_text=None):\n\n        try:\n            path, image_args, title = image.parse_image_spec(spec)\n        except Exception as err:  # pylint: disable=broad-except\n            logger.exception(\"Got error on spec %s: %s\", spec, err)\n            return ('<span class=\"error\">Couldn\\'t parse image spec: ' +\n                    '<code>{}</code> {}</span>'.format(flask.escape(spec),\n                                                       flask.escape(str(err))))\n\n        composite_args = {**container_args, **image_args}\n\n        try:\n            img = image.get_image(path, self._search_path)\n        except Exception as err:  # pylint: disable=broad-except\n            logger.exception(\"Got error on image %s: %s\", path, err)\n            return ('<span class=\"error\">Error loading image {}: {}</span>'.format(\n                flask.escape(spec), flask.escape(str(err))))\n\n        return img.get_img_tag(title, alt_text, **composite_args)", "response": "Render an image specification into an img tag"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(path, search_path, absolute=False):\n\n    # Resolve external URLs\n    if re.match(r'([a-z][a-z0-9+.\\-]*:)?//', path, re.I):\n        return path\n\n    # Resolve static assets\n    if path.startswith('@'):\n        return utils.static_url(path[1:], absolute)\n\n    path, sep, anchor = path.partition('#')\n\n    # Resolve entries\n    entry = utils.find_entry(path, search_path)\n    if entry:\n        return entry.permalink(absolute=absolute) + sep + anchor\n\n    # Resolve images and assets\n    img_path, img_args, _ = image.parse_image_spec(path)\n    img = image.get_image(img_path, search_path)\n    if not isinstance(img, image.ImageNotFound):\n        path, _ = img.get_rendition(**img_args)\n    return path + sep + anchor", "response": "Resolve a link or source target to an appropriate entry or image rendition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether we should not cache a page render", "response": "def do_not_cache():\n    \"\"\" Return whether we should cache a page render \"\"\"\n\n    from . import index  # pylint: disable=cyclic-import\n\n    if index.in_progress():\n        # We are reindexing the site\n        return True\n\n    if request.if_none_match or request.if_modified_since:\n        # we might be returning a 304 NOT MODIFIED based on a client request,\n        # and we don't want to cache that as the result for *all* client\n        # requests to this URI\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a where clause for currently - visible entries in the system", "response": "def where_entry_visible(query, date=None):\n    \"\"\" Generate a where clause for currently-visible entries\n\n    Arguments:\n\n    date -- The date to generate it relative to (defaults to right now)\n    \"\"\"\n\n    return orm.select(\n        e for e in query\n        if e.status == model.PublishStatus.PUBLISHED.value or\n        (e.status == model.PublishStatus.SCHEDULED.value and\n         (e.utc_date <= (date or arrow.utcnow().datetime))\n         )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef where_entry_visible_future(query):\n\n    return orm.select(\n        e for e in query\n        if e.status in (model.PublishStatus.PUBLISHED.value,\n                        model.PublishStatus.SCHEDULED.value))", "response": "Generate a where clause for entries that are visible now or in the future"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef where_entry_deleted(query):\n    return orm.select(\n        e for e in query\n        if e.status == model.PublishStatus.GONE.value)", "response": "Generate a where clause for entries that have been deleted"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a where clause for a particular category", "response": "def where_entry_category(query, category, recurse=False):\n    \"\"\" Generate a where clause for a particular category \"\"\"\n\n    category = str(category)\n    if category and recurse:\n        # We're recursing and aren't in /, so add the prefix clause\n        return orm.select(\n            e for e in query\n            if e.category == category or e.category.startswith(category + '/')\n        )\n\n    if not recurse:\n        # We're not recursing, so we need an exact match on a possibly-empty\n        # category\n        return orm.select(e for e in query if e.category == category)\n\n    # We're recursing and have no category, which means we're doing nothing\n    return query"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a where clause for prior entries", "response": "def where_before_entry(query, ref):\n    \"\"\" Generate a where clause for prior entries\n\n    ref -- The entry of reference\n    \"\"\"\n    return orm.select(\n        e for e in query\n        if e.local_date < ref.local_date or\n        (e.local_date == ref.local_date and e.id < ref.id)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a where clause for later entries", "response": "def where_after_entry(query, ref):\n    \"\"\" Generate a where clause for later entries\n\n    ref -- the entry of reference\n    \"\"\"\n    return orm.select(\n        e for e in query\n        if e.local_date > ref.local_date or\n        (e.local_date == ref.local_date and\n         e.id > ref.id\n         )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a where clause where this is the last entry of the reference", "response": "def where_entry_last(query, ref):\n    \"\"\" Generate a where clause where this is the last entry\n\n    ref -- the entry of reference\n    \"\"\"\n    return orm.select(\n        e for e in query\n        if e.local_date < ref.local_date or\n        (e.local_date == ref.local_date and\n         e.id <= ref.id\n         )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a where clause where this is the first entry of the reference", "response": "def where_entry_first(query, ref):\n    \"\"\" Generate a where clause where this is the first entry\n\n    ref -- the entry of reference\n    \"\"\"\n    return orm.select(\n        e for e in query\n        if e.local_date > ref.local_date or\n        (e.local_date == ref.local_date and\n         e.id >= ref.id\n         )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a where clause for entries of certain types", "response": "def where_entry_type(query, entry_type):\n    \"\"\" Generate a where clause for entries of certain types\n\n    entry_type -- one or more entries to check against\n    \"\"\"\n    if isinstance(entry_type, (list, set, tuple)):\n        return orm.select(e for e in query if e.entry_type in entry_type)\n    return orm.select(e for e in query if e.entry_type == entry_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a where clause for entries with the given tag", "response": "def where_entry_tag(query, tag):\n    \"\"\" Generate a where clause for entries with the given tag \"\"\"\n    if isinstance(tag, (list, set, tuple)):\n        tags = [t.lower() for t in tag]\n        return orm.select(e for e in query for t in e.tags if t.key in tags)\n    return orm.select(e for e in query for t in e.tags if t.key == tag.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef where_entry_date(query, datespec):\n    date, interval, _ = utils.parse_date(datespec)\n    start_date, end_date = date.span(interval)\n\n    return orm.select(\n        e for e in query if\n        e.local_date >= start_date.naive and\n        e.local_date <= end_date.naive\n    )", "response": "Returns a where clause for entries which match a textual date spec"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the where clause based on a view specification", "response": "def build_query(spec):\n    \"\"\" build the where clause based on a view specification\n\n    spec -- The view specification. Contains the following possible values:\n        future -- Boolean; whether to include entries from the future\n        category -- Which category to limit to\n        recurse -- Whether to include subcategories\n        entry_type -- one or more entry types to include\n        entry_type_not -- one or more entry types to exclude\n        date -- a date spec\n        last -- the last entry to end a view on\n        first -- the first entry to start a view on\n        before -- get entries from before this one\n        after -- get entries from after this one\n    \"\"\"\n\n    query = model.Entry.select()\n\n    # primarily restrict by publication status\n    if spec.get('_deleted', False):\n        query = where_entry_deleted(query)\n    elif spec.get('future', False):\n        query = where_entry_visible_future(query)\n    else:\n        query = where_entry_visible(query)\n\n    # restrict by category\n    if spec.get('category') is not None:\n        path = str(spec.get('category', ''))\n        recurse = spec.get('recurse', False)\n        query = where_entry_category(query, path, recurse)\n\n    if spec.get('entry_type') is not None:\n        query = where_entry_type(query, spec['entry_type'])\n\n    if spec.get('entry_type_not') is not None:\n        query = where_entry_type_not(query, spec['entry_type_not'])\n\n    if spec.get('tag') is not None:\n        query = where_entry_tag(query, spec['tag'])\n\n    if spec.get('date') is not None:\n        query = where_entry_date(query, spec['date'])\n\n    if spec.get('last') is not None:\n        query = where_entry_last(query, get_entry(spec['last']))\n\n    if spec.get('first') is not None:\n        query = where_entry_first(query, get_entry(spec['first']))\n\n    if spec.get('before') is not None:\n        query = where_before_entry(query, get_entry(spec['before']))\n\n    if spec.get('after') is not None:\n        query = where_after_entry(query, get_entry(spec['after']))\n\n    return query.distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating templates and static files", "response": "def create_templates_static_files(app_path):\n    \"\"\"\n    create templates and static\n    \"\"\"\n    templates_path = os.path.join(app_path, 'templates')\n    static_path = os.path.join(app_path, 'static')\n    _mkdir_p(templates_path)\n    _mkdir_p(static_path)\n    # create {img, css, js}\n    os.chdir(static_path)\n    img_path = os.path.join(static_path, 'img')\n    css_path = os.path.join(static_path, 'css')\n    js_path = os.path.join(static_path, 'js')\n    _mkdir_p(img_path)\n    _mkdir_p(css_path)\n    _mkdir_p(js_path)\n\n    return css_path, templates_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_blueprint(app_path, blueprint, views_code, forms_code, templates_path):\n    blueprint_path = os.path.join(app_path, blueprint)\n    _mkdir_p(blueprint_path)\n    # create  blueprint files\n    os.chdir(blueprint_path)\n    init_code('__init__.py', _init_blueprint_code % (blueprint, blueprint))\n    init_code('views.py', views_code)\n    init_code('forms.py', forms_code)\n    # main blueprint templates\n    os.chdir(templates_path)\n    blueprint_templates_path = os.path.join(templates_path, blueprint)\n    _mkdir_p(blueprint_templates_path)\n\n    return blueprint_templates_path", "response": "create blueprint and create templates"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing a minimal flask project", "response": "def init(project_name):\n    \"\"\"\n    build a minimal flask project\n    \"\"\"\n    # the destination path\n    dst_path = os.path.join(os.getcwd(), project_name)\n\n    start_init_info(dst_path)\n\n    # create dst path\n    _mkdir_p(dst_path)\n\n    os.chdir(dst_path)\n    # create files\n    init_code('manage.py', _manage_basic_code)\n    init_code('requirement.txt', _requirement_code)\n\n    # create app/\n    app_path = os.path.join(dst_path, 'app')\n    _mkdir_p(app_path)\n\n    os.chdir(app_path)\n    # create files\n    init_code('views.py', _views_basic_code)\n    init_code('forms.py', _forms_basic_code)\n    init_code('__init__.py', _init_basic_code)\n\n    create_templates_static_files(app_path)\n\n    init_done_info()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blueprint(blueprint_name):\n    app = os.getcwd().split('/')[-1]\n    if app != 'app':\n        logger.warning('''\\033[31m{Warning}\\033[0m\n==> your current path is \\033[32m%s\\033[0m\\n\n==> please create your blueprint under app folder!''' % os.getcwd())\n        exit(1)\n\n    # destination path\n    dst_path = os.path.join(os.getcwd(), blueprint_name)\n    if os.path.isdir(dst_path):\n        logger.warning('''\\033[31m{Warning}\\033[0m\n==> bluprint \\033[32m%s\\033[0m\\n exist\n==> please try again !''' % dst_path)\n        exit(1)\n\n    # create dst_path\n    _mkdir_p(dst_path)\n\n    # change dir\n    os.chdir(dst_path)\n    # create files\n    init_code('__init__.py', _init_blueprint_code %\n        (blueprint_name, blueprint_name))\n    init_code('views.py', _views_blueprint_code %\n        (blueprint_name, blueprint_name))\n    init_code('forms.py', _forms_basic_code)\n\n    # register auth in app\n    os.chdir(os.path.join(dst_path, '..'))\n    with open('__init__.py', 'r+') as f:\n        prev = pos = 0\n        while f.readline():\n            prev, pos = pos, f.tell()\n        f.seek(prev)\n        f.write(\n            '\\nfrom %s import %s\\napp.register_blueprint(%s, url_prefix=\"/%s\")\\n\\n'\n            % (\n                blueprint_name, blueprint_name,\n                blueprint_name, blueprint_name\n            )\n        )\n\n    # create blueprint templates\n    templates_path = os.path.join(os.getcwd(), 'templates')\n    os.chdir(templates_path)\n    blueprint_templates_path = os.path.join(templates_path, blueprint_name)\n    _mkdir_p(blueprint_templates_path)\n\n    logger.info('''\\033[33m{Info}\\033[0m: create blueprint done!''')", "response": "create and register a blueprint"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef startproject(project_name):\n    # the destination path\n    dst_path = os.path.join(os.getcwd(), project_name)\n    start_init_info(dst_path)\n\n    # create dst path\n    _mkdir_p(dst_path)\n\n    # create project tree\n    os.chdir(dst_path)\n    # create files\n    init_code('manage.py', _manage_admin_code)\n    init_code('requirement.txt', _requirement_admin_code)\n    init_code('config.py', _config_sql_code)\n\n    # create app/\n    app_path = os.path.join(dst_path, 'app')\n    _mkdir_p(app_path)\n\n    # create files\n    os.chdir(app_path)\n    init_code('models.py', _models_admin_code)\n    init_code('__init__.py', _init_admin_code)\n\n    # create templates and static\n    css_path, templates_path = create_templates_static_files(app_path)\n    # create css files\n    os.chdir(css_path)\n    init_code('sign.css', _auth_login_css_code)\n\n    # create main blueprint\n    create_blueprint(\n        app_path,\n        'main',\n        _views_blueprint_code % ('main', 'main'),\n        _forms_basic_code,\n        templates_path\n    )\n\n    # create auth blueprint\n    auth_templates_path = create_blueprint(\n        app_path,\n        'auth',\n        _auth_views_code,\n        _auth_forms_code,\n        templates_path\n    )\n    # create auth templates files\n    os.chdir(auth_templates_path)\n    init_code('login.html', _auth_login_html_code)\n\n    # create admin site\n    admin_path = os.path.join(app_path, 'admin')\n    _mkdir_p(admin_path)\n\n    # create admin files\n    os.chdir(admin_path)\n    init_code('__init__.py', '')\n    init_code('views.py', _admin_views_code)\n\n    # create admin templates\n    os.chdir(templates_path)\n    admin_templates_path = os.path.join(templates_path, 'admin')\n    _mkdir_p(admin_templates_path)\n\n    # create admin templates files\n    os.chdir(admin_templates_path)\n    init_code('index.html', _admin_index_html_code)\n    init_code('logout.html', _admin_logout_html_code)\n\n    init_done_info()", "response": "build a full status project"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding sql modules into admin site", "response": "def admin(module):\n    \"\"\"add sql modules into admin site\"\"\"\n    # add module into admin site\n    app = os.getcwd().split('/')[-1]\n    if app != 'app':\n        logger.warning('''\\033[31m{Warning}\\033[0m\n==> your current path is \\033[32m%s\\033[0m\\n\n==> please add your sql module under app folder!''' % os.getcwd())\n        exit(1)\n\n    admin_path = os.path.join(os.getcwd(), 'admin')\n    os.chdir(admin_path)\n    with open('views.py', 'r+') as f:\n        prev = pos = 0\n        while f.readline():\n            prev, pos = pos, f.tell()\n        f.seek(prev)\n        f.write(\n            '\\nfrom app.models import %s\\nadmin.add_view(ModelView(%s, db.session))'\n            % (module, module)\n        )\n\n    logger.info('''\\033[33m{Info}\\033[0m: add module done!''')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _mkdir_p(abspath):\n    try:\n        os.makedirs(abspath)\n    except OSError as e:\n        if (e.errno == errno.EEXIST) and (os.path.isdir(abspath)):\n            pass\n        else: raise", "response": "Create the abspath\nCTYPE except the abspath exist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raise_error(error_type: str) -> None:\n    try:\n        error = next((v for k, v in ERROR_CODES.items() if k in error_type))\n    except StopIteration:\n        error = AirVisualError\n    raise error(error_type)", "response": "Raise the appropriate error based on error message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _raise_on_error(data: Union[str, dict]) -> None:\n    if isinstance(data, str):\n        raise_error(data)\n    elif 'status' in data and data['status'] != 'success':\n        raise_error(data['data']['message'])", "response": "Raise the appropriate exception on error."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a request against AirVisual.", "response": "async def request(\n            self,\n            method: str,\n            endpoint: str,\n            *,\n            base_url: str = API_URL_SCAFFOLD,\n            headers: dict = None,\n            params: dict = None,\n            json: dict = None) -> dict:\n        \"\"\"Make a request against AirVisual.\"\"\"\n        if not headers:\n            headers = {}\n        headers.update({'Content-Type': 'application/json'})\n\n        if not params:\n            params = {}\n\n        if self._api_key:\n            params.update({'key': self._api_key})\n\n        url = '{0}/{1}'.format(base_url, endpoint)\n        async with self.websession.request(method, url, headers=headers,\n                                           params=params, json=json) as resp:\n            data = await resp.json(content_type=None)\n            _raise_on_error(data)\n            return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def main() -> None:  # pylint: disable=too-many-statements\n    logging.basicConfig(level=logging.INFO)\n    async with ClientSession() as websession:\n        client = Client(websession, api_key='<API KEY>')\n\n        # Get supported locations (by location):\n        try:\n            _LOGGER.info(await client.supported.countries())\n            _LOGGER.info(await client.supported.states('USA'))\n            _LOGGER.info(await client.supported.cities('USA', 'Colorado'))\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get supported locations (by station):\n        try:\n            _LOGGER.info(\n                await client.supported.stations(\n                    'USA', 'Colorado', 'Denver'))\n        except UnauthorizedError as err:\n            _LOGGER.error(err)\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get data by nearest location (by IP):\n        try:\n            _LOGGER.info(await client.api.nearest_city())\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get data by nearest location (coordinates or explicit location):\n        try:\n            _LOGGER.info(\n                await client.api.nearest_city(\n                    latitude=39.742599, longitude=-104.9942557))\n            _LOGGER.info(\n                await client.api.city(\n                    city='Los Angeles', state='California', country='USA'))\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get data by nearest station (by IP):\n        try:\n            _LOGGER.info(await client.api.nearest_station())\n        except UnauthorizedError as err:\n            _LOGGER.error(err)\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get data by nearest station (by coordinates or explicit location):\n        try:\n            _LOGGER.info(\n                await client.api.nearest_station(\n                    latitude=39.742599, longitude=-104.9942557))\n            _LOGGER.info(\n                await client.api.station(\n                    station='US Embassy in Beijing',\n                    city='Beijing',\n                    state='Beijing',\n                    country='China'))\n        except UnauthorizedError as err:\n            _LOGGER.error(err)\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get data on AQI ranking:\n        try:\n            _LOGGER.info(await client.api.ranking())\n        except UnauthorizedError as err:\n            _LOGGER.error(err)\n        except AirVisualError as err:\n            _LOGGER.error('There was an error: %s', err)\n\n        # Get info on a AirVisual Pro node:\n        _LOGGER.info(await client.api.node('zEp8CifbnasWtToBc'))", "response": "Create the aiohttp session and run the example."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _nearest(\n            self,\n            kind: str,\n            latitude: Union[float, str] = None,\n            longitude: Union[float, str] = None) -> dict:\n        \"\"\"Return data from nearest city/station (IP or coordinates).\"\"\"\n        params = {}\n        if latitude and longitude:\n            params.update({'lat': str(latitude), 'lon': str(longitude)})\n\n        data = await self._request(\n            'get', 'nearest_{0}'.format(kind), params=params)\n        return data['data']", "response": "Return data from nearest city or station."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning data for the specified city.", "response": "async def city(self, city: str, state: str, country: str) -> dict:\n        \"\"\"Return data for the specified city.\"\"\"\n        data = await self._request(\n            'get',\n            'city',\n            params={\n                'city': city,\n                'state': state,\n                'country': country\n            })\n        return data['data']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def nearest_city(\n            self,\n            latitude: Union[float, str] = None,\n            longitude: Union[float, str] = None) -> dict:\n        \"\"\"Return data from nearest city (IP or coordinates).\"\"\"\n        return await self._nearest('city', latitude, longitude)", "response": "Return data from nearest city."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn data from a node by its ID.", "response": "async def node(self, node_id: str) -> dict:\n        \"\"\"Return data from a node by its ID.\"\"\"\n        return await self._request('get', node_id, base_url=NODE_URL_SCAFFOLD)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of supported cities in a country / state.", "response": "async def cities(self, country: str, state: str) -> list:\n        \"\"\"Return a list of supported cities in a country/state.\"\"\"\n        data = await self._request(\n            'get', 'cities', params={\n                'state': state,\n                'country': country\n            })\n        return [d['city'] for d in data['data']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of supported states in a country.", "response": "async def states(self, country: str) -> list:\n        \"\"\"Return a list of supported states in a country.\"\"\"\n        data = await self._request(\n            'get', 'states', params={'country': country})\n        return [d['state'] for d in data['data']]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def stations(self, city: str, state: str, country: str) -> list:\n        data = await self._request(\n            'get',\n            'stations',\n            params={\n                'city': city,\n                'state': state,\n                'country': country\n            })\n        return [station for station in data['data']]", "response": "Return a list of supported stations in a city."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a suitable column width to display one or more strings.", "response": "def column_width(tokens):\n    \"\"\"\n    Return a suitable column width to display one or more strings.\n    \"\"\"\n    get_len = tools.display_len if PY3 else len\n    lens = sorted(map(get_len, tokens or [])) or [0]\n    width = lens[-1]\n\n    # adjust for disproportionately long strings\n    if width >= 18:\n        most = lens[int(len(lens) * 0.9)]\n        if most < width + 6:\n            return most\n\n    return width"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\njustifying a string to fill one or more columns.", "response": "def justify_token(tok, col_width):\n    \"\"\"\n    Justify a string to fill one or more columns.\n    \"\"\"\n    get_len = tools.display_len if PY3 else len\n    tok_len = get_len(tok)\n    diff_len = tok_len - len(tok) if PY3 else 0\n\n    cols = (int(math.ceil(float(tok_len) / col_width))\n            if col_width < tok_len + 4 else 1)\n\n    if cols > 1:\n        return tok.ljust((col_width * cols) + (4 * cols) - diff_len)\n    else:\n        return tok.ljust(col_width + 4 - diff_len)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the display name of an object.", "response": "def display_name(name, obj, local):\n    \"\"\"\n    Get the display name of an object.\n\n    Keyword arguments (all required):\n\n    * ``name`` -- the name of the object as a string.\n    * ``obj`` -- the object itself.\n    * ``local`` -- a boolean value indicating whether the object is in local\n      scope or owned by an object.\n\n    \"\"\"\n    prefix = '' if local else '.'\n\n    if isinstance(obj, SeeError):\n        suffix = '?'\n    elif hasattr(obj, '__call__'):\n        suffix = '()'\n    else:\n        suffix = ''\n\n    return ''.join((prefix, name, suffix))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering the results using a pattern.", "response": "def filter(self, pattern):\n        \"\"\"\n        Filter the results using a pattern.\n\n        This accepts a shell-style wildcard pattern (as used by the fnmatch_\n        module)::\n\n            >>> see([]).filter('*op*')\n                .copy()    .pop()\n\n        It also accepts a regular expression. This may be a compiled regular\n        expression (from the re_ module) or a string that starts with a ``/``\n        (forward slash) character::\n\n            >>> see([]).filter('/[aeiou]{2}/')\n                .clear()    .count()\n\n        .. _fnmatch: https://docs.python.org/3/library/fnmatch.html\n        .. _re: https://docs.python.org/3/library/re.html\n        \"\"\"\n        if isinstance(pattern, REGEX_TYPE):\n            func = tools.filter_regex\n        elif pattern.startswith('/'):\n            pattern = re.compile(pattern.strip('/'))\n            func = tools.filter_regex\n        else:\n            func = tools.filter_wildcard\n\n        return SeeResult(func(self, pattern))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_ignoring_case(self, pattern):\n        return self.filter(re.compile(pattern, re.I))", "response": "Like filter but case - insensitive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef term_width():\n    if fcntl and termios:\n        try:\n            winsize = fcntl.ioctl(0, termios.TIOCGWINSZ, '    ')\n            _, width = struct.unpack('hh', winsize)\n            return width\n        except IOError:\n            pass\n    elif windll and create_string_buffer:  # pragma: no cover (windows)\n        stderr_handle, struct_size = -12, 22\n        handle = windll.kernel32.GetStdHandle(stderr_handle)\n        csbi = create_string_buffer(struct_size)\n        res = windll.kernel32.GetConsoleScreenBufferInfo(handle, csbi)\n        if res:\n            (_, _, _, _, _, left, _, right, _,\n             _, _) = struct.unpack('hhhhHhhhhhh', csbi.raw)\n            return right - left + 1\n        else:\n            return 0", "response": "Return the column width of the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the ideal column width for the output of a Travis CI command line.", "response": "def line_width(default_width=DEFAULT_LINE_WIDTH, max_width=MAX_LINE_WIDTH):\n    \"\"\"\n    Return the ideal column width for the output from :func:`see.see`, taking\n    the terminal width into account to avoid wrapping.\n    \"\"\"\n    width = term_width()\n    if width:  # pragma: no cover (no terminal info in Travis CI)\n        return min(width, max_width)\n    else:\n        return default_width"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the persistent ID of the object.", "response": "def persistent_id(self, obj):\n        \"\"\" Tags objects with a persistent ID, but do NOT emit it\n        \"\"\"\n        if getattr(obj, '_PERSIST_REFERENCES', None):\n            objid = id(obj)\n            obj._persistent_ref = objid\n            _weakmemos[objid] = obj\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_deprecated_args(tokens, args, kwargs):\n    num_args = len(args)\n    pattern = args[0] if num_args else kwargs.get('pattern', None)\n    regex = args[1] if num_args > 1 else kwargs.get('r', None)\n\n    if pattern is not None:\n        tokens = tools.filter_wildcard(tokens, pattern)\n        sys.stderr.write(\n            'Please use see().match() now. The \"pattern\" argument is '\n            'deprecated and will be removed in a later release. \\n')\n\n    if regex is not None:\n        tokens = tools.filter_regex(tokens, re.compile(regex))\n        sys.stderr.write(\n            'Please use see().match() now. The \"r\" argument is '\n            'deprecated and will be removed in a later release. \\n')\n\n    return tokens", "response": "Backwards compatibility with deprecated arguments pattern and r."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef see(obj=DEFAULT_ARG, *args, **kwargs):\n    use_locals = obj is DEFAULT_ARG\n\n    if use_locals:\n        # Get the local scope from the caller's stack frame.\n        # Typically this is the scope of an interactive Python session.\n        obj = Namespace(inspect.currentframe().f_back.f_locals)\n\n    tokens = []\n    attrs = dir(obj)\n\n    if not use_locals:\n\n        for name, func in INSPECT_FUNCS:\n            if func(obj):\n                tokens.append(name)\n\n        for feature in FEATURES:\n            if feature.match(obj, attrs):\n                tokens.append(feature.symbol)\n\n    for attr in filter(lambda a: not a.startswith('_'), attrs):\n        try:\n            prop = getattr(obj, attr)\n        except (AttributeError, Exception):  # pylint: disable=broad-except\n            prop = SeeError()\n        action = output.display_name(name=attr, obj=prop, local=use_locals)\n        tokens.append(action)\n\n    if args or kwargs:\n        tokens = handle_deprecated_args(tokens, args, kwargs)\n\n    return output.SeeResult(tokens)", "response": "A function that shows the features and attributes of an object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, mode='r', encoding=None):\n        access_type = self._get_access_type(mode)\n        return open(self.localpath, 'r'+access_type, encoding=encoding)", "response": "Return file - like object actually opens the file for this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _open_tmpfile(self, **kwargs):\n        self.tmpfile = get_tempfile(**kwargs)\n        path = self.tmpfile.name\n        return path", "response": "Open a temporary file and return its name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(self, mode='r', encoding=None):\n        access_type = self._get_access_type(mode)\n\n        if encoding is None:\n            encoding = self.encoding\n\n        # here, we face the task of returning the correct data type\n        if access_type == 'b':\n            if not self._isbytes:\n                content = self._contents.encode(encoding)  # unicode in, bytes out\n            else:\n                content = self._contents  # bytes in, bytes out\n            return io.BytesIO(content)\n        else:\n            assert access_type == 't'\n            if PYVERSION == 2 and self._isbytes:\n                return io.BytesIO(self._contents)  # bytes in, bytes out (python 2 only)\n            elif self._isbytes:\n                content = self._contents.decode(encoding)  # bytes in, unicode out\n            else:\n                content = self._contents  # unicode in, unicode out\n            return io.StringIO(content)", "response": "Return file - like object containing the contents of the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, filename, encoding=None):\n        from . import LocalFile\n\n        if os.path.isdir(filename) and self.source is None:\n            raise ValueError(\"Cannot write this object to \"\n                             \"directory %s without an explicit filename.\" % filename)\n\n        target = get_target_path(filename, self.source)\n\n        if encoding is None:\n            encoding = self.encoding\n\n        if self._isbytes:\n            kwargs = {'mode': 'wb'}\n        else:\n            kwargs = {'mode': 'w', 'encoding': encoding}\n\n        with open(target, **kwargs) as outfile:\n            outfile.write(self._contents)\n\n        return LocalFile(target, encoded_with=encoding)", "response": "Write the contents of this object to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring any methods or variables bound from the function s closure Returns a dictionary mapping of variable names to globally bound VARIABLES", "response": "def get_global_vars(func):\n    \"\"\" Store any methods or variables bound from the function's closure\n\n    Args:\n        func (function): function to inspect\n\n    Returns:\n        dict: mapping of variable names to globally bound VARIABLES\n    \"\"\"\n    closure = getclosurevars(func)\n    if closure['nonlocal']:\n        raise TypeError(\"Can't launch a job with closure variables: %s\" %\n                        closure['nonlocals'].keys())\n    globalvars = dict(modules={},\n                      functions={},\n                      vars={})\n    for name, value in closure['global'].items():\n        if inspect.ismodule(value):  # TODO: deal FUNCTIONS from closure\n            globalvars['modules'][name] = value.__name__\n        elif inspect.isfunction(value) or inspect.ismethod(value):\n            globalvars['functions'][name] = value\n        else:\n            globalvars['vars'][name] = value\n\n    return globalvars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the source code for a class or function.", "response": "def getsource(classorfunc):\n    \"\"\" Return the source code for a class or function.\n\n    Notes:\n        Returned source will not include any decorators for the object.\n        This will only return the explicit declaration of the object, not any dependencies\n\n    Args:\n        classorfunc (type or function): the object to get the source code for\n\n    Returns:\n        str: text of source code (without any decorators). Note: in python 2, this returns unicode\n    \"\"\"\n    if _isbuiltin(classorfunc):\n        return ''\n\n    try:\n        source = inspect.getsource(classorfunc)\n    except TypeError:  # raised if defined in __main__ - use fallback to get the source instead\n        source = getsourcefallback(classorfunc)\n\n    declaration = []\n\n    lines = source.splitlines()\n    if PY2 and not isinstance(source, unicode):\n        encoding = detect_encoding(iter(lines).next)[0]\n        sourcelines = (s.decode(encoding) for s in lines)\n    else:\n        sourcelines = iter(lines)\n\n    # First, get the declaration\n    found_keyword = False\n    for line in sourcelines:\n        words = line.split()\n        if not words:\n            continue\n        if words[0] in ('def', 'class'):\n            found_keyword = True\n        if found_keyword:\n            cind = line.find(':')\n            if cind > 0:\n                declaration.append(line[:cind + 1])\n                after_decl = line[cind + 1:].strip()\n                break\n            else:\n                declaration.append(line)\n\n    bodylines = list(sourcelines)  # the rest of the lines are body\n\n    # If it's a class, make sure we import its superclasses\n    # Unfortunately, we need to modify the code to make sure the\n    # parent classes have the correct names\n    # TODO: find a better way to do this without having to parse code\n    if type(classorfunc) == type:\n        cls = classorfunc\n        base_imports = {}\n        for base in cls.__bases__:\n            if base.__name__ == 'object' and base.__module__ == 'builtins':  # don't import `object`\n                continue\n            if base in base_imports:\n                continue\n            if base.__module__ == '__main__':\n                continue\n            base_imports[base] = 'from %s import %s' % (base.__module__, base.__name__)\n        cind = declaration[0].index('class ')\n\n        declstring = declaration[0][:cind] + 'class %s(%s):%s' % (\n            cls.__name__,\n            ','.join([base.__name__ for base in cls.__bases__]),\n            after_decl)\n        declaration = [impstring for c, impstring in base_imports.items()\n                       if c.__module__ != '__builtin__']\n        declaration.append(declstring)\n\n    else:\n        declaration[-1] += after_decl\n\n    return '\\n'.join(declaration + bodylines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getsourcefallback(cls):\n    for attr in cls.__dict__:\n        if inspect.ismethod(getattr(cls, attr)):\n            imethod = getattr(cls, attr)\n            break\n    else:\n        raise AttributeError(\n            \"Cannot get this class' source; it does not appear to have any methods\")\n\n    ### This part is derived from inspect.findsource ###\n    module = inspect.getmodule(cls)\n    file = inspect.getfile(imethod)\n    lines = linecache.getlines(file, module.__dict__)\n    name = cls.__name__\n    pat = re.compile(r'^(\\s*)class\\s*'+name+r'\\b')\n\n    # AMVMOD: find the encoding (necessary for python 2 only)\n    #if PY2:\n    #    with open(file, 'rb') as infile:\n    #        encoding = detect_encoding(infile.readline)[0]\n\n    # make some effort to find the best matching class definition:\n    # use the one with the least indentation, which is the one\n    # that's most probably not inside a function definition.\n    candidates = []\n    toplevel = False\n    for i in range(len(lines)):\n        match = pat.match(lines[i])\n        if match:\n            # if it's at toplevel, it's already the best one\n            if lines[i][0] == 'c':\n                flines, flnum = lines, i\n                toplevel = True\n                break\n            # else add whitespace to candidate list\n            candidates.append((match.group(1), i))\n    if candidates and not toplevel:\n        # this will sort by whitespace, and by line number,\n        # less whitespace first\n        candidates.sort()\n        flines, flnum = lines, candidates[0][1]\n    elif not candidates and not toplevel:\n        raise IOError('could not find class definition')\n    ### end modified inspect.findsource ###\n\n    # this is what inspect.getsourcelines does\n    glines = inspect.getblock(flines[flnum:])\n\n    # And this is what inspect.getsource does\n    if False: #if PY2:\n        return (\"\".join(glines)).decode(encoding)\n    else:\n        return \"\".join(glines)", "response": "This function is used to get the source of interactively defined classes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Job object for the requested job id.", "response": "def get_job(self, jobid):\n        \"\"\" Return a Job object for the requested job id.\n\n        The returned object will be suitable for retrieving output, but depending on the engine,\n        may not populate all fields used at launch time (such as `job.inputs`, `job.commands`, etc.)\n\n        Args:\n            jobid (str): container id\n\n        Returns:\n            pyccc.job.Job: job object for this container\n\n        Raises:\n            pyccc.exceptions.JobNotFound: if no job could be located for this jobid\n        \"\"\"\n        import shlex\n        from pyccc.job import Job\n\n        job = Job(engine=self)\n        job.jobid = job.rundata.containerid = jobid\n        try:\n            jobdata = self.client.inspect_container(job.jobid)\n        except docker.errors.NotFound:\n            raise exceptions.JobNotFound(\n                    'The daemon could not find containter \"%s\"' % job.jobid)\n\n        cmd = jobdata['Config']['Cmd']\n        entrypoint = jobdata['Config']['Entrypoint']\n\n        if len(cmd) == 3 and cmd[0:2] == ['sh', '-c']:\n            cmd = cmd[2]\n        elif entrypoint is not None:\n            cmd = entrypoint + cmd\n\n        if isinstance(cmd, list):\n            cmd = ' '.join(shlex.quote(x) for x in cmd)\n\n        job.command = cmd\n        job.env = jobdata['Config']['Env']\n        job.workingdir = jobdata['Config']['WorkingDir']\n        job.rundata.container = jobdata\n\n        return job"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submit(self, job):\n        self._check_job(job)\n\n        if job.workingdir is None:\n            job.workingdir = self.default_wdir\n        job.imageid = du.create_provisioned_image(self.client, job.image,\n                                                  job.workingdir, job.inputs)\n\n        container_args = self._generate_container_args(job)\n\n        job.rundata.container = self.client.create_container(job.imageid, **container_args)\n        self.client.start(job.rundata.container)\n        job.rundata.containerid = job.rundata.container['Id']\n        job.jobid = job.rundata.containerid", "response": "Submit the job to the engine"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump_all_outputs(self, job, target, abspaths=None):\n        import os\n        import shutil\n        from pathlib import Path\n\n        root = Path(native_str(target))\n        true_outputs = job.get_output()\n\n        if abspaths or len(true_outputs) < self.BULK_OUTPUT_FILE_THRESHOLD:\n            return super().dump_all_outputs(job, root, abspaths)\n\n        stagingdir = root / Path(native_str(job.workingdir)).name\n        workdir = job.get_directory(job.workingdir)\n        if not root.is_dir():\n            root.mkdir(parents=False)\n        if stagingdir.exists():\n            if PY2:\n                raise IOError('Path % exists' % stagingdir)\n            else:\n                raise FileExistsError(stagingdir)\n        workdir.put(str(root))\n        assert stagingdir.is_dir()\n        assert root in stagingdir.parents\n\n        for pathstr in true_outputs:\n            if os.path.isabs(pathstr):\n                continue\n            destpath = root / pathstr\n            currpath = stagingdir / pathstr\n            if not destpath.parent.is_dir():\n                destpath.parent.mkdir(parents=True)\n            currpath.rename(destpath)\n\n        shutil.rmtree(str(stagingdir))", "response": "Dump all outputs of the job to the target directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_build_context(image, inputs, wdir):\n    assert os.path.isabs(wdir)\n\n    dockerlines = [\"FROM %s\" % image,\n                    \"RUN mkdir -p %s\" % wdir]\n    build_context = {}\n\n    # This loop creates a Build Context for building the provisioned image\n    # We create a tar archive to be added to the root of the image filesystem\n    if inputs:\n        dockerlines.append('COPY root /')\n        for ifile, (path, obj) in enumerate(inputs.items()):\n            if not os.path.isabs(path):\n                path = os.path.join(wdir, path)\n            assert path[0] == '/'\n            build_context['root' + path] = obj\n\n    dockerstring = '\\n'.join(dockerlines)\n    build_context['Dockerfile'] = pyccc.BytesContainer(dockerstring.encode('utf-8'))\n    return build_context", "response": "Create a build context for the provisioned image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_tar_stream(build_context, buffer):\n    tf = tarfile.TarFile(fileobj=buffer, mode='w')\n    for context_path, fileobj in build_context.items():\n        if getattr(fileobj, 'localpath', None) is not None:\n            tf.add(fileobj.localpath, arcname=context_path)\n        else:\n            tar_add_bytes(tf, context_path, fileobj.read('rb'))\n    tf.close()", "response": "Create a tar stream of the build context to the provided buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a file to a tar archive containing the bytestring", "response": "def tar_add_bytes(tf, filename, bytestring):\n    \"\"\" Add a file to a tar archive\n\n    Args:\n        tf (tarfile.TarFile): tarfile to add the file to\n        filename (str): path within the tar file\n        bytestring (bytes or str): file contents. Must be :class:`bytes` or\n            ascii-encodable :class:`str`\n    \"\"\"\n    if not isinstance(bytestring, bytes):  # it hasn't been encoded yet\n        bytestring = bytestring.encode('ascii')\n    buff = io.BytesIO(bytestring)\n    tarinfo = tarfile.TarInfo(filename)\n    tarinfo.size = len(bytestring)\n    tf.addfile(tarinfo, buff)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kwargs_from_client(client, assert_hostname=False):\n    from docker import tls\n    if client.base_url in ('http+docker://localunixsocket', 'http+docker://localhost'):\n        return {'base_url': 'unix://var/run/docker.sock'}\n\n    params = {'base_url': client.base_url}\n    if client.cert:\n        # TODO: problem - client.cert is filepaths, and it would be insecure to send those files.\n        params['tls'] = tls.TLSConfig(\n            client_cert=client.cert,\n            ca_cert=client.verify,\n            verify=bool(client.verify),\n            assert_hostname=assert_hostname)\n\n    return params", "response": "Returns a dictionary of keyword arguments to pass to the Docker client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_class(self, module, name):\n        import pickle\n\n        modname = self.RENAMETABLE.get(module, module)\n\n        try:\n            # can't use ``super`` here (not 2/3 compatible)\n            klass = pickle.Unpickler.find_class(self, modname, name)\n\n        except (ImportError, RuntimeError):\n            definition = getattr(source, name)\n            newmod = _makemod(modname)\n            sys.modules[modname] = newmod\n            setattr(newmod, name, definition)\n            klass = pickle.Unpickler.find_class(self, newmod.__name__, name)\n            klass.__module__ = module\n        return klass", "response": "This override is here to help pickle find the class in the given module and name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gist_diff():\n    remote_file = wget(RAW_GIST)\n    proc = subprocess.Popen(('diff - %s'%MY_PATH).split(),\n                            stdin=subprocess.PIPE,\n                            stdout=subprocess.PIPE)\n    stdout, stderr = proc.communicate(remote_file)\n    return stdout", "response": "Diff this file with the gist on github"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload the page into a string", "response": "def wget(url):\n    \"\"\"\n    Download the page into a string\n    \"\"\"\n    import urllib.parse\n    request = urllib.request.urlopen(url)\n    filestring = request.read()\n    return filestring"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to decode bytes to text - try to autodetect Taxonomy first otherwise try to autodetect Taxonomy", "response": "def autodecode(b):\n    \"\"\" Try to decode ``bytes`` to text - try default encoding first, otherwise try to autodetect\n\n    Args:\n        b (bytes): byte string\n\n    Returns:\n        str: decoded text string\n    \"\"\"\n    import warnings\n    import chardet\n\n    try:\n        return b.decode()\n    except UnicodeError:\n        result = chardet.detect(b)\n        if result['confidence'] < 0.95:\n            warnings.warn('autodecode failed with utf-8; guessing %s' % result['encoding'])\n        return result.decode(result['encoding'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef can_use_widgets():\n    if 'IPython' not in sys.modules:\n        # IPython hasn't been imported, definitely not\n        return False\n    from IPython import get_ipython\n\n    # check for `kernel` attribute on the IPython instance\n    if getattr(get_ipython(), 'kernel', None) is None:\n        return False\n\n    try:\n        import ipywidgets as ipy\n        import traitlets\n    except ImportError:\n        return False\n\n    return True", "response": "Check if IPython is available and if so return True if IPython is available and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove non - leafs from a list of directory paths", "response": "def remove_directories(list_of_paths):\n    \"\"\"\n    Removes non-leafs from a list of directory paths\n    \"\"\"\n    found_dirs = set('/')\n    for path in list_of_paths:\n        dirs = path.strip().split('/')\n        for i in range(2,len(dirs)):\n            found_dirs.add( '/'.join(dirs[:i]) )\n\n    paths = [ path for path in list_of_paths if\n              (path.strip() not in found_dirs) and path.strip()[-1]!='/' ]\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the input file is under the working directory.", "response": "def _check_file_is_under_workingdir(filename, wdir):\n        \"\"\" Raise error if input is being staged to a location not underneath the working dir\n        \"\"\"\n        p = filename\n        if not os.path.isabs(p):\n            p = os.path.join(wdir, p)\n        targetpath = os.path.realpath(p)\n        wdir = os.path.realpath(wdir)\n        common = os.path.commonprefix([wdir, targetpath])\n        if len(common) < len(wdir):\n            raise exceptions.PathError(\n                    \"The subprocess engine does not support input files with absolute paths\")\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the file to the given path.", "response": "def put(self, filename, encoding=None):\n        \"\"\"Write the file to the given path\n\n        Args:\n            filename(str): path to write this file to\n\n        Returns:\n            LocalFile: reference to the copy of the file stored at ``filename``\n        \"\"\"\n        from . import LocalFile\n\n        if os.path.isdir(filename) and self.source is None:\n            raise ValueError(\"Cannot write this object to \"\n                             \"directory %s without an explicit filename.\" % filename)\n\n        target = get_target_path(filename, self.source)\n\n        if (encoding is not None) and (encoding != self.encoded_with):\n            raise ValueError('%s is already encoded as \"%s\"' % self, self.encoded_with)\n\n        with self.open('rb') as infile, open(target, 'wb') as outfile:\n            for line in infile:\n                outfile.write(line)\n        return LocalFile(target)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, mode='r', encoding=None):\n        access_type = self._get_access_type(mode)\n\n        if access_type == 't' and encoding is not None and encoding != self.encoded_with:\n            warnings.warn('Attempting to decode %s as \"%s\", but encoding is declared as \"%s\"'\n                          % (self, encoding, self.encoded_with))\n\n        if encoding is None:\n            encoding = self.encoded_with\n\n        buffer = io.BytesIO(self._contents)\n        if access_type == 'b':\n            return buffer\n        else:\n            return io.TextIOWrapper(buffer, encoding=encoding)", "response": "Return file - like object containing the contents of the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path to the target file.", "response": "def get_target_path(destination, origname):\n    \"\"\" Implements the directory/path semantics of linux mv/cp etc.\n\n    Examples:\n        >>> import os\n        >>> os.makedirs('./a')\n        >>> get_target_path('./a', '/tmp/myfile')\n        './myfile'\n        >>> get_target_path('./a/b', '/tmp/myfile')\n        './a/b'\n\n    Raises:\n        OSError: if neither destination NOR destination's parent exists OR it already exists\n    \"\"\"\n    if os.path.exists(destination):\n        if not os.path.isdir(destination):\n            raise OSError('Cannot write to requested destination %s - file exists' % destination)\n        return os.path.join(destination, os.path.basename(origname))\n    else:\n        destdir = os.path.abspath(os.path.join(destination, os.path.pardir))\n        if not os.path.isdir(destdir):\n            raise OSError(\n                    'Cannot write to requested destination %s - parent directory does not exist' %\n                    destination)\n        return os.path.join(destination)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put(self, filename):\n        from . import LocalFile\n        target = get_target_path(filename, self.source)\n        with self.open('rb') as infile, open(target, 'wb') as outfile:\n            shutil.copyfileobj(infile, outfile)\n        return LocalFile(target)", "response": "Write the file to the given path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the type of the file in mode.", "response": "def _get_access_type(self, mode):\n        \"\"\" Make sure mode is appropriate; return 'b' for binary access and 't' for text\n        \"\"\"\n        access_type = None\n        for char in mode:  # figure out whether it's binary or text access\n            if char in 'bt':\n                if access_type is not None:\n                    raise IOError('File mode \"%s\" contains contradictory flags' % mode)\n                access_type = char\n            elif char not in 'rbt':\n                raise NotImplementedError(\n                        '%s objects are read-only; unsupported mode \"%s\"'%\n                        (type(self), mode))\n\n        if access_type is None: access_type = 't'\n        return access_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the mapping of free variables to their current values.", "response": "def getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n\n    Note:\n        Modified function from the Python 3.5 inspect standard library module\n\n        Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n        2011, 2012, 2013, 2014, 2015, 2016, 2017 Python Software Foundation; All Rights\n        Reserved\"\n\n        See also py-cloud-compute-cannon/NOTICES.\n    \"\"\"\n    if inspect.ismethod(func):\n        func = func.__func__\n\n    elif not inspect.isroutine(func):\n        raise TypeError(\"'{!r}' is not a Python function\".format(func))\n\n    # AMVMOD: deal with python 2 builtins that don't define these\n    code = getattr(func, '__code__', None)\n    closure = getattr(func, '__closure__', None)\n    co_names = getattr(code, 'co_names', ())\n    glb = getattr(func, '__globals__', {})\n\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if closure is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {var: cell.cell_contents\n                         for var, cell in zip(code.co_freevars, func.__closure__)}\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = glb\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if inspect.ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    for name in co_names:\n        if name in (\"None\", \"True\", \"False\"):\n            # Because these used to be builtins instead of keywords, they\n            # may still show up as name references. We ignore them.\n            continue\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return {'nonlocal': nonlocal_vars,\n            'global': global_vars,\n            'builtin': builtin_vars,\n            'unbound': unbound_names}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(self, destination):\n        target = get_target_path(destination, self.localpath)\n        shutil.copytree(self.localpath, target)", "response": "Copy the referenced directory to the destination."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put(self, destination):\n        target = get_target_path(destination, self.dirname)\n        valid_paths = (self.dirname, './%s' % self.dirname)\n\n        with tarfile.open(self.archive_path, 'r:*') as tf:\n            members = []\n            for tarinfo in tf:\n                # Get only files under the directory `self.dirname`\n                pathsplit = os.path.normpath(tarinfo.path).split(os.sep)\n                if pathsplit[0] not in valid_paths:\n                    print('WARNING: skipped file \"%s\" in archive; not in directory \"%s\"' %\n                          (tarinfo.path, self.dirname))\n                    continue\n                if len(pathsplit) == 1:\n                    continue\n                tarinfo.name = os.path.join(*pathsplit[1:])\n                members.append(tarinfo)\n\n            if not members:\n                raise ValueError(\"No files under path directory '%s' in this tarfile\")\n\n            tf.extractall(target, members)", "response": "Copy the referenced directory to the destination directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put(self, destination):\n        if not self._fetched:\n            self._fetch()\n        DirectoryArchive.put(self, destination)", "response": "Copy the referenced directory to this path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump all outputs of the job to a directory.", "response": "def dump_all_outputs(self, job, target, abspaths=None):\n        \"\"\" Default dumping strategy - potentially slow for large numbers of files\n\n        Subclasses should offer faster implementations, if available\n        \"\"\"\n        from pathlib import Path\n        root = Path(native_str(target))\n\n        for outputpath, outputfile in job.get_output().items():\n            path = Path(native_str(outputpath))\n\n            # redirect absolute paths into the appropriate subdirectory\n            if path.is_absolute():\n                if abspaths:\n                    path = Path(native_str(abspaths), *path.parts[1:])\n                else:\n                    continue\n\n            dest = root / path\n            if not dest.parent.is_dir():\n                dest.parent.mkdir(parents=True)\n            if dest.is_file():\n                dest.unlink()\n            try:\n                outputfile.put(str(dest))\n            except IsADirectoryError:\n                if not dest.is_dir():\n                    dest.mkdir(parents=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch(self, image, command, **kwargs):\n        if isinstance(command, PythonCall):\n            return PythonJob(self, image, command, **kwargs)\n        else:\n            return Job(self, image, command, **kwargs)", "response": "Create a job on this engine"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef char_width(char):\n    if ord(char) < 128:\n        return 1\n    elif unicodedata.east_asian_width(char) in ('F', 'W'):\n        return 2\n    elif unicodedata.category(char) in ('Mn',):\n        return 0\n    else:\n        return 1", "response": "Get the display length of a unicode character."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display_len(text):\n    text = unicodedata.normalize('NFD', text)\n    return sum(char_width(char) for char in text)", "response": "Get the display length of a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_regex(names, regex):\n    return tuple(name for name in names\n                 if regex.search(name) is not None)", "response": "Filter a list of strings by a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_wildcard(names, pattern):\n    return tuple(name for name in names\n                 if fnmatch.fnmatch(name, pattern))", "response": "Filter a list of strings that match a shell - style wildcard pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(self, obj, attrs):\n        if '__doc__' in attrs:\n            lstrip = getattr(obj.__doc__, 'lstrip', False)\n            return lstrip and any(lstrip())", "response": "Match if the object contains a non - empty docstring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the packaged function as func.", "response": "def run(self, func=None):\n        \"\"\"\n        Evaluates the packaged function as func(*self.args,**self.kwargs)\n        If func is a method of an object, it's accessed as getattr(self.obj,__name__).\n        If it's a user-defined function, it needs to be passed in here because it can't\n        be serialized.\n\n        Returns:\n            object: function's return value\n        \"\"\"\n        to_run = self.prepare_namespace(func)\n        result = to_run(*self.args, **self.kwargs)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_namespace(self, func):\n        if self.is_imethod:\n            to_run = getattr(self.obj, self.imethod_name)\n        else:\n            to_run = func\n\n        for varname, modulename in self.global_modules.items():\n            to_run.__globals__[varname] = __import__(modulename)\n        if self.global_closure:\n            to_run.__globals__.update(self.global_closure)\n        if self.global_functions:\n            to_run.__globals__.update(self.global_functions)\n        return to_run", "response": "Prepares the function to be run after deserializing it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a list - table directive.", "response": "def make_list_table(headers, data, title='', columns=None):\n    \"\"\"Build a list-table directive.\n\n    :param headers: List of header values.\n    :param data: Iterable of row data, yielding lists or tuples with rows.\n    :param title: Optional text to show as the table title.\n    :param columns: Optional widths for the columns.\n    \"\"\"\n    results = []\n    add = results.append\n    add('.. list-table:: %s' % title)\n    add('   :header-rows: 1')\n    if columns:\n        add('   :widths: %s' % (','.join(str(c) for c in columns)))\n    add('')\n    add('   - * %s' % headers[0])\n    for h in headers[1:]:\n        add('     * %s' % h)\n    for row in data:\n        add('   - * %s' % row[0])\n        for r in row[1:]:\n            add('     * %s' % r)\n    add('')\n    return '\\n'.join(results)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_list_table_from_mappings(headers, data, title, columns=None):\n    header_names = [h[0] for h in headers]\n    header_keys = [h[1] for h in headers]\n    row_data = (\n        [d.get(k) for k in header_keys]\n        for d in data\n    )\n    return make_list_table(header_names, row_data, title, columns)", "response": "Builds a list - table directive from mappings."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat a decimal. Decimal like to 2 decimal places.", "response": "def decimal_format(value, TWOPLACES=Decimal(100) ** -2):\n    'Format a decimal.Decimal like to 2 decimal places.'\n    if not isinstance(value, Decimal):\n        value = Decimal(str(value))\n    return value.quantize(TWOPLACES)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying two radio buttons for turning notifications on or off.", "response": "def notification_preference(obj_type, profile):\n    '''Display two radio buttons for turning notifications on or off.\n    The default value is is have alerts_on = True.\n    '''\n    default_alert_value = True\n    if not profile:\n        alerts_on = True\n    else:\n        notifications = profile.get('notifications', {})\n        alerts_on = notifications.get(obj_type, default_alert_value)\n    return dict(alerts_on=alerts_on, obj_type=obj_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef committee_object(self):\n        '''If the committee id no longer exists in mongo for some reason,\n        this function returns None.\n        '''\n        if 'committee_id' in self:\n            _id = self['committee_id']\n            return self.document._old_roles_committees.get(_id)\n        else:\n            return self", "response": "Return the object that is associated with the committee id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntell this legislator object which session to use when calculating the legislator s context_role for a given bill or vote.", "response": "def context_role(self, bill=None, vote=None, session=None, term=None):\n        '''Tell this legislator object which session to use when calculating\n        the legisator's context_role for a given bill or vote.\n        '''\n        # If no hints were given about the context, look for a related bill,\n        # then for a related vote.\n        if not any([bill, vote, session, term]):\n            try:\n                bill = self.bill\n            except AttributeError:\n                # A vote?\n                try:\n                    vote = self.vote\n                except AttributeError:\n                    # If we're here, this method was called on a\n                    # Legislator was that doesn't have a related bill or vote.\n                    return ''\n\n        # If we still have to historical point of reference, figuring\n        # out the context role is impossible. Return emtpy string.\n        if not any([bill, vote, session, term]):\n            return ''\n\n        # First figure out the term.\n        if bill is not None:\n            term = bill['_term']\n\n        elif vote is not None:\n            try:\n                _bill = vote.bill\n            except AttributeError:\n                _bill = BillVote(vote).bill\n            if callable(_bill):\n                _bill = _bill()\n            term = _bill['_term']\n\n        if term is None and session is not None:\n            term = term_for_session(self[settings.LEVEL_FIELD], session)\n\n        # Use the term to get the related roles. First look in the current\n        # roles list, then fail over to the old_roles list.\n        roles = [r for r in self['roles']\n                 if r.get('type') == 'member' and r.get('term') == term]\n        roles = list(filter(None, roles))\n\n        if not roles:\n            roles = [r for r in self.get('old_roles', {}).get(term, [])\n                     if r.get('type') == 'member']\n        roles = list(filter(None, roles))\n\n        if not roles:\n            # Legislator had no roles for this term. If there is a related\n            # bill ro vote, this shouldn't happen, but could if the\n            # legislator's roles got deleted.\n            return ''\n\n        # If there's only one applicable role, we're done.\n        if len(roles) == 1:\n            role = roles.pop()\n            self['context_role'] = role\n            return role\n\n        # If only one of term or session is given and there are multiple roles:\n        if not list(filter(None, [bill, vote])):\n            if term is not None:\n                role = roles[0]\n                self['context_role'] = role\n                return role\n\n            # Below, use the date of the related bill or vote to determine\n            # which (of multiple) roles applies.\n            # Get the context date.\n            if session is not None:\n                # If we're here, we have multiple roles for a single session.\n                # Try to find the correct one in self.metadata,\n                # else give up.\n                session_data = self.metadata['session_details'][session]\n                for role in roles:\n                    role_start = role.get('start_date')\n                    role_end = role.get('end_date')\n\n                    # Return the first role that overlaps at all with the\n                    # session.\n                    session_start = session_data.get('start_date')\n                    session_end = session_data.get('end_date')\n                    if session_start and session_end:\n                        started_during = (role_start < session_start <\n                                          role_end)\n                        ended_during = (role_start < session_end < role_end)\n                        if started_during or ended_during:\n                            self['context_role'] = role\n                            return role\n                    else:\n                        continue\n\n                # Return first role from the session?\n                role = roles[0]\n                self['context_role'] = role\n                return role\n\n        if vote is not None:\n            date = vote['date']\n        if bill is not None:\n            date = bill['action_dates']['first']\n\n        dates_exist = False\n        for role in roles:\n            start_date = role.get('start_date')\n            end_date = role.get('end_date')\n            if start_date and end_date:\n                dates_exist = True\n                if start_date < date < end_date:\n                    self['context_role'] = role\n                    return role\n\n        if dates_exist:\n            # If we're here, the context date didn't fall into any of the\n            # legislator's role date ranges.\n            return ''\n\n        else:\n            # Here the roles didn't have date ranges. Return the last one?\n            role = roles.pop()\n            self['context_role'] = role\n            return role\n\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning old roles grouped first by term chamber then by type.", "response": "def old_roles_manager(self):\n        '''Return old roles, grouped first by term, then by chamber,\n        then by type.'''\n        wrapper = self._old_role_wrapper\n        chamber_getter = operator.methodcaller('get', 'chamber')\n        for term, roles in self.get('old_roles', {}).items():\n            chamber_roles = defaultdict(lambda: defaultdict(list))\n            for chamber, roles in itertools.groupby(roles, chamber_getter):\n                for role in roles:\n                    role = wrapper(role)\n                    typeslug = role['type'].lower().replace(' ', '_')\n                    chamber_roles[chamber][typeslug].append(role)\n            yield term, chamber_roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes a legislator name by stripping titles from the front and converting punctuation to lowercase.", "response": "def _normalize(self, name):\n        \"\"\"\n        Normalizes a legislator name by stripping titles from the front,\n        converting to lowercase and removing punctuation.\n        \"\"\"\n        name = re.sub(\n            r'^(Senator|Representative|Sen\\.?|Rep\\.?|'\n            'Hon\\.?|Right Hon\\.?|Mr\\.?|Mrs\\.?|Ms\\.?|L\\'hon\\.?|'\n            'Assembly(member|man|woman)) ',\n            '',\n            name)\n        return name.strip().lower().replace('.', '')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlearn a new entry in the database.", "response": "def _learn(self, legislator):\n        \"\"\"\n        Expects a dictionary with full_name, first_name, last_name and\n        middle_name elements as key.\n\n        While this can grow quickly, we should never be dealing with\n        more than a few hundred legislators at a time so don't worry about\n        it.\n        \"\"\"\n        name, obj = legislator, legislator['_id']\n\n        if (legislator['roles'] and legislator['roles'][0]['term'] ==\n                self._term and legislator['roles'][0]['type'] == 'member'):\n            chamber = legislator['roles'][0]['chamber']\n        else:\n            try:\n                chamber = legislator['old_roles'][self._term][0].get('chamber')\n            except KeyError:\n                raise ValueError(\"no role in legislator %s [%s] for term %s\" %\n                                 (legislator['full_name'], legislator['_id'],\n                                  self._term))\n\n        if '_code' in name:\n            code = name['_code']\n            if code in self._codes[chamber] or code in self._codes[None]:\n                raise ValueError(\"non-unique legislator code [%s] for %s\" %\n                                 (code, name['full_name']))\n            self._codes[chamber][code] = obj\n            self._codes[None][code] = obj\n\n        # We throw possible forms of this name into a set because we\n        # don't want to try to add the same form twice for the same\n        # name\n        forms = set()\n\n        def add_form(form):\n            forms.add(self._normalize(form))\n\n        add_form(name['full_name'])\n        add_form(name['_scraped_name'])\n        add_form(name['last_name'])\n\n        if name['first_name']:\n            add_form(\"%s, %s\" % (name['last_name'], name['first_name']))\n            add_form(\"%s %s\" % (name['first_name'], name['last_name']))\n            add_form(\"%s, %s\" % (name['last_name'], name['first_name'][0]))\n            add_form(\"%s (%s)\" % (name['last_name'], name['first_name']))\n            add_form(\"%s %s\" % (name['first_name'][0], name['last_name']))\n            add_form(\"%s (%s)\" % (name['last_name'], name['first_name'][0]))\n\n            if name['middle_name']:\n                add_form(\"%s, %s %s\" % (name['last_name'], name['first_name'],\n                                        name['middle_name']))\n                add_form(\"%s, %s %s\" % (name['last_name'],\n                                        name['first_name'][0],\n                                        name['middle_name']))\n                add_form(\"%s %s %s\" % (name['first_name'],\n                                       name['middle_name'],\n                                       name['last_name']))\n                add_form(\"%s, %s %s\" % (name['last_name'],\n                                        name['first_name'][0],\n                                        name['middle_name'][0]))\n                add_form(\"%s %s %s\" % (name['first_name'],\n                                       name['middle_name'][0],\n                                       name['last_name']))\n                add_form(\"%s, %s %s\" % (name['last_name'],\n                                        name['first_name'],\n                                        name['middle_name'][0]))\n                add_form(\"%s, %s.%s.\" % (name['last_name'],\n                                         name['first_name'][0],\n                                         name['middle_name'][0]))\n\n        for form in forms:\n            form = self._normalize(form)\n            if form in self._names[chamber]:\n                self._names[chamber][form] = None\n            else:\n                self._names[chamber][form] = obj\n\n            if form in self._names[None]:\n                self._names[None][form] = None\n            else:\n                self._names[None][form] = obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(self, name, chamber=None):\n        try:\n            return self._manual[chamber][name]\n        except KeyError:\n            pass\n\n        if chamber == 'joint':\n            chamber = None\n\n        try:\n            return self._codes[chamber][name]\n        except KeyError:\n            pass\n\n        if chamber not in self._names:\n            logger.warning(\"Chamber %s is invalid for a legislator.\" % (\n                chamber\n            ))\n            return None\n\n        name = self._normalize(name)\n        return self._names[chamber].get(name, None)", "response": "Returns the associated base - level object for the given name and chamber."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scraper(mod_path, scraper_type):\n\n    # act of importing puts it into the registry\n    try:\n        module = importlib.import_module(mod_path)\n    except ImportError as e:\n        raise ScrapeError(\"could not import %s\" % mod_path, e)\n\n    # now find the class within the module\n    ScraperClass = None\n\n    for k, v in module.__dict__.items():\n        if k.startswith('_'):\n            continue\n        if getattr(v, 'scraper_type', None) == scraper_type:\n            if ScraperClass:\n                raise ScrapeError(\"two %s scrapers found in module %s: %s %s\" %\n                                  (scraper_type, mod_path, ScraperClass, k))\n            ScraperClass = v\n\n    if not ScraperClass:\n        raise ScrapeError(\"no %s scraper found in module %s\" % (\n            scraper_type, mod_path))\n\n    return ScraperClass", "response": "import a scraper from the scraper registry"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading all schemas into schema dict", "response": "def _load_schemas(self):\n        \"\"\" load all schemas into schema dict \"\"\"\n\n        types = ('bill', 'committee', 'person', 'vote', 'event')\n\n        for type in types:\n            schema_path = os.path.join(os.path.split(__file__)[0],\n                                       '../schemas/%s.json' % type)\n            self._schema[type] = json.load(open(schema_path))\n            self._schema[type]['properties'][settings.LEVEL_FIELD] = {\n                'minLength': 2, 'type': 'string'}\n\n        # bills & votes\n        self._schema['bill']['properties']['session']['enum'] = \\\n            self.all_sessions()\n        self._schema['vote']['properties']['session']['enum'] = \\\n            self.all_sessions()\n\n        # legislators\n        terms = [t['name'] for t in self.metadata['terms']]\n        # ugly break here b/c this line is nearly impossible to split\n        self._schema['person']['properties']['roles'][\n            'items']['properties']['term']['enum'] = terms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that a session is present in the metadata dictionary.", "response": "def validate_session(self, session, latest_only=False):\n        \"\"\" Check that a session is present in the metadata dictionary.\n\n        raises :exc:`~billy.scrape.NoDataForPeriod` if session is invalid\n\n        :param session:  string representing session to check\n        \"\"\"\n        if latest_only:\n            if session != self.metadata['terms'][-1]['sessions'][-1]:\n                raise NoDataForPeriod(session)\n\n        for t in self.metadata['terms']:\n            if session in t['sessions']:\n                return True\n        raise NoDataForPeriod(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_term(self, term, latest_only=False):\n\n        if latest_only:\n            if term == self.metadata['terms'][-1]['name']:\n                return True\n            else:\n                raise NoDataForPeriod(term)\n\n        for t in self.metadata['terms']:\n            if term == t['name']:\n                return True\n        raise NoDataForPeriod(term)", "response": "Check that a term is present in the metadata dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_source(self, url, **kwargs):\n        self['sources'].append(dict(url=url, **kwargs))", "response": "Add a source URL from which data related to this object was scraped."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_column_ends(self):\n        '''Guess where the ends of the columns lie.\n        '''\n        ends = collections.Counter()\n        for line in self.text.splitlines():\n            for matchobj in re.finditer('\\s{2,}', line.lstrip()):\n                ends[matchobj.end()] += 1\n        return ends", "response": "Guess where the ends of the columns lie."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to guess the boundaries of the plain text columns.", "response": "def _get_column_boundaries(self):\n        '''Use the guessed ends to guess the boundaries of the plain\n        text columns.\n        '''\n        # Try to figure out the most common column boundaries.\n        ends = self._get_column_ends()\n        if not ends:\n            # If there aren't even any nontrivial sequences of whitespace\n            # dividing text, there may be just one column. In which case,\n            # Return a single span, effectively the whole line.\n            return [slice(None, None)]\n\n        most_common = []\n        threshold = self.threshold\n        for k, v in collections.Counter(ends.values()).most_common():\n            if k >= threshold:\n                most_common.append(k)\n\n        if most_common:\n            boundaries = []\n            for k, v in ends.items():\n                if v in most_common:\n                    boundaries.append(k)\n        else:\n            # Here there weren't enough boundaries to guess the most common\n            # ones, so just use the apparent boundaries. In other words, we\n            # have only 1 row. Potentially a source of inaccuracy.\n            boundaries = ends.keys()\n\n        # Convert the boundaries into a list of span slices.\n        boundaries.sort()\n        last_boundary = boundaries[-1]\n        boundaries = zip([0] + boundaries, boundaries)\n        boundaries = list(itertools.starmap(slice, boundaries))\n\n        # And get from the last boundary to the line ending.\n        boundaries.append(slice(last_boundary, None))\n        return boundaries"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract cells from the given line.", "response": "def getcells(self, line):\n        '''Using self.boundaries, extract cells from the given line.\n        '''\n        for boundary in self.boundaries:\n            cell = line.lstrip()[boundary].strip()\n            if cell:\n                for cell in re.split('\\s{3,}', cell):\n                    yield cell\n            else:\n                yield None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rows(self):\n        '''Returns an iterator of row tuples.\n        '''\n        for line in self.text.splitlines():\n            yield tuple(self.getcells(line))", "response": "Returns an iterator of row tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an interator of all cells in the table.", "response": "def cells(self):\n        '''Returns an interator of all cells in the table.\n        '''\n        for line in self.text.splitlines():\n            for cell in self.getcells(line):\n                yield cell"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the end of the range of the result set.", "response": "def range_end(self):\n        '''\"Showing 40 - 50 of 234 results\n                         ^\n        '''\n        count = self.count\n        range_end = self.range_start + self.limit - 1\n        if count < range_end:\n            range_end = count\n        return range_end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator of tuples where string page_number is the number of the html link and clickable is True if the link is clickable or False if the link is not clickable.", "response": "def pagination_data(self, max_number_of_links=7):\n        '''Returns a generator of tuples (string, page_number, clickable),\n        where `string` is the text of the html link, `page_number` is\n        the number of the page the link points to, and `clickable` is\n        a boolean indicating whether the link is clickable or not.\n        '''\n        div, mod = divmod(max_number_of_links, 2)\n        if not mod == 1:\n            msg = 'Max number of links must be odd, was %r.'\n            raise ValueError(msg % max_number_of_links)\n\n        midpoint = (max_number_of_links - 1) / 2\n        current_page = self.current_page\n        last_page = self.last_page\n\n        if current_page > last_page:\n            raise Http404('invalid page number')\n            current_page = last_page\n\n        show_link_firstpage = midpoint < current_page\n        show_link_previouspage = 1 < current_page\n        show_link_lastpage = current_page < (self.last_page - midpoint)\n        show_link_nextpage = current_page < last_page\n\n        extra_room_previous = midpoint - current_page\n        if extra_room_previous < 0:\n            extra_room_previous = 0\n        if not show_link_firstpage:\n            extra_room_previous += 1\n        if not show_link_previouspage:\n            extra_room_previous += 1\n\n        extra_room_subsequent = current_page - last_page + midpoint\n        extra_room_subsequent = max([extra_room_subsequent, 0])\n        if not show_link_nextpage:\n            extra_room_subsequent += 1\n        if not show_link_lastpage:\n            extra_room_subsequent += 1\n\n        if self.current_page == 1:\n            yield PageLink(string=1, page_number=1, clickable=False)\n        else:\n            # The  \"first page\" link.\n            if show_link_firstpage:\n                #[<<] [<] [7] [8] [9] 10 [11] ...'\n                #  ^\n                yield PageLink(string=u\"\\u00AB\", page_number=1, clickable=True)\n\n            if show_link_previouspage:\n                # The \"previous page\" link.\n                #[<<] [<] [7] [8] [9] 10 [11] ...'\n                #      ^\n                yield PageLink(string=u\"\\u2039\",\n                               page_number=self.previous_page,\n                               clickable=True)\n\n        # Up to `midpoint + extra_room_subsequent` previous page numbers.\n        previous = itertools.islice(self.previous_pages_numbers(),\n                                    midpoint + extra_room_subsequent)\n        previous = list(previous)[::-1]\n\n        for page_number in previous:\n            yield PageLink(string=page_number,\n                           page_number=page_number, clickable=True)\n\n        # The current page, clickable.\n        if current_page != 1:\n            yield PageLink(string=current_page,\n                           page_number=current_page, clickable=False)\n\n        # Up to `midpoint + extra_room_previous` subsequent page numbers.\n        subsequent_count = midpoint + extra_room_previous\n        _subsequent_pages_count = self._subsequent_pages_count\n        if _subsequent_pages_count < subsequent_count:\n            subsequent_count = _subsequent_pages_count\n\n        for page_number in itertools.islice(self.subsequent_pages_numbers(),\n                                            subsequent_count):\n            yield PageLink(string=page_number,\n                           page_number=page_number, clickable=True)\n\n        if show_link_nextpage:\n            yield PageLink(string=u\"\\u203A\",\n                           page_number=current_page + 1,\n                           clickable=True)\n\n        if show_link_lastpage:\n            yield PageLink(string=u\"\\u00BB\",\n                           page_number=last_page,\n                           clickable=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_scraper(scraper_type, options, metadata):\n    _clear_scraped_data(options.output_dir, scraper_type)\n\n    scraper = _get_configured_scraper(scraper_type, options, metadata)\n    ua_email = os.environ.get('BILLY_UA_EMAIL')\n    if ua_email and scraper:\n        scraper.user_agent += ' ({})'.format(ua_email)\n    if not scraper:\n        return [{\n            \"type\": scraper_type,\n            \"start_time\": dt.datetime.utcnow(),\n            \"noscraper\": True,\n            \"end_time\": dt.datetime.utcnow()\n        }]\n\n    runs = []\n\n    # Removed from the inner loop due to non-bicameral scrapers\n    scrape = {\n        \"type\": scraper_type\n    }\n    scrape['start_time'] = dt.datetime.utcnow()\n\n    if scraper_type in ('bills', 'votes', 'events'):\n        times = options.sessions\n        for time in times:\n            scraper.validate_session(time, scraper.latest_only)\n    elif scraper_type in ('committees', 'legislators'):\n        times = options.terms\n        for time in times:\n            scraper.validate_term(time, scraper.latest_only)\n\n    # run scraper against year/session/term\n    for time in times:\n        # old style\n        chambers = options.chambers\n        if scraper_type == 'events' and len(options.chambers) == 2:\n            chambers.append('other')\n\n        if _is_old_scrape(scraper.scrape):\n            for chamber in chambers:\n                scraper.scrape(chamber, time)\n        else:\n            scraper.scrape(time, chambers=chambers)\n\n        # error out if events or votes don't scrape anything\n        if not scraper.object_count and scraper_type not in ('events',\n                                                             'votes'):\n            raise ScrapeError(\"%s scraper didn't save any objects\" %\n                              scraper_type)\n\n    scrape['end_time'] = dt.datetime.utcnow()\n    runs.append(scrape)\n\n    return runs", "response": "Run scraper against the specified options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_bill(data, standalone_votes, categorizer):\n    abbr = data[settings.LEVEL_FIELD]\n\n    # clean up bill_ids\n    data['bill_id'] = fix_bill_id(data['bill_id'])\n    if 'alternate_bill_ids' in data:\n        data['alternate_bill_ids'] = [fix_bill_id(bid) for bid in\n                                      data['alternate_bill_ids']]\n\n    # move subjects to scraped_subjects\n    # NOTE: intentionally doesn't copy blank lists of subjects\n    # this avoids the problem where a bill is re-run but we can't\n    # get subjects anymore (quite common)\n    subjects = data.pop('subjects', None)\n    if subjects:\n        data['scraped_subjects'] = subjects\n\n    # update categorized subjects\n    if categorizer:\n        categorizer.categorize_bill(data)\n\n    # companions\n    for companion in data['companions']:\n        companion['bill_id'] = fix_bill_id(companion['bill_id'])\n        # query based on companion\n        spec = companion.copy()\n        spec[settings.LEVEL_FIELD] = abbr\n        if not spec['chamber']:\n            spec.pop('chamber')\n        companion_obj = db.bills.find_one(spec)\n        if companion_obj:\n            companion['internal_id'] = companion_obj['_id']\n        else:\n            logger.warning('Unknown companion: {chamber} {session} {bill_id}'\n                           .format(**companion))\n\n    # look for a prior version of this bill\n    bill = db.bills.find_one({settings.LEVEL_FIELD: abbr,\n                              'session': data['session'],\n                              'chamber': data['chamber'],\n                              'bill_id': data['bill_id']})\n\n    # keep doc ids consistent\n    doc_matcher = DocumentMatcher(abbr)\n    if bill:\n        doc_matcher.learn_ids(bill['versions'] + bill['documents'])\n    doc_matcher.set_ids(data['versions'] + data['documents'])\n\n    # match sponsor leg_ids\n    match_sponsor_ids(abbr, data)\n\n    # process votes ############\n\n    # pull votes off bill\n    bill_votes = data.pop('votes', [])\n\n    # grab the external bill votes if present\n    if metadata(abbr).get('_partial_vote_bill_id'):\n        # this is a hack initially added for Rhode Island where we can't\n        # determine the full bill_id, if this key is in the metadata\n        # we just use the numeric portion, not ideal as it won't work\n        # where HB/SBs overlap, but in RI they never do\n        # pull off numeric portion of bill_id\n        numeric_bill_id = data['bill_id'].split()[1]\n        bill_votes += standalone_votes.pop((data['chamber'], data['session'],\n                                            numeric_bill_id), [])\n    else:\n        # add loaded votes to data\n        bill_votes += standalone_votes.pop((data['chamber'], data['session'],\n                                            data['bill_id']), [])\n\n    # do id matching and other vote prep\n    if bill:\n        prepare_votes(abbr, data['session'], bill['_id'], bill_votes)\n    else:\n        prepare_votes(abbr, data['session'], None, bill_votes)\n\n    # process actions ###########\n\n    dates = {'first': None, 'last': None, 'passed_upper': None,\n             'passed_lower': None, 'signed': None}\n\n    vote_flags = {\n        \"bill:passed\",\n        \"bill:failed\",\n        \"bill:veto_override:passed\",\n        \"bill:veto_override:failed\",\n        \"amendment:passed\",\n        \"amendment:failed\",\n        \"committee:passed\",\n        \"committee:passed:favorable\",\n        \"committee:passed:unfavorable\",\n        \"committee:passed:failed\"\n    }\n    already_linked = set()\n    remove_vote = set()\n\n    for action in data['actions']:\n        adate = action['date']\n\n        def _match_committee(name):\n            return get_committee_id(abbr, action['actor'], name)\n\n        def _match_legislator(name):\n            return get_legislator_id(abbr,\n                                     data['session'],\n                                     action['actor'],\n                                     name)\n\n        resolvers = {\n            \"committee\": _match_committee,\n            \"legislator\": _match_legislator\n        }\n\n        if \"related_entities\" in action:\n            for entity in action['related_entities']:\n                try:\n                    resolver = resolvers[entity['type']]\n                except KeyError as e:\n                    # We don't know how to deal.\n                    logger.error(\"I don't know how to sort a %s\" % e)\n                    continue\n\n                id = resolver(entity['name'])\n                entity['id'] = id\n\n        # first & last dates\n        if not dates['first'] or adate < dates['first']:\n            dates['first'] = adate\n        if not dates['last'] or adate > dates['last']:\n            dates['last'] = adate\n\n        # passed & signed dates\n        if (not dates['passed_upper'] and action['actor'] == 'upper'\n                and 'bill:passed' in action['type']):\n            dates['passed_upper'] = adate\n        elif (not dates['passed_lower'] and action['actor'] == 'lower'\n                and 'bill:passed' in action['type']):\n            dates['passed_lower'] = adate\n        elif (not dates['signed'] and 'governor:signed' in action['type']):\n            dates['signed'] = adate\n\n        # vote-action matching\n        action_attached = False\n        # only attempt vote matching if action has a date and is one of the\n        # designated vote action types\n        if set(action['type']).intersection(vote_flags) and action['date']:\n            for vote in bill_votes:\n                if not vote['date']:\n                    continue\n\n                delta = abs(vote['date'] - action['date'])\n                if (delta < datetime.timedelta(hours=20) and\n                        vote['chamber'] == action['actor']):\n                    if action_attached:\n                        # multiple votes match, we can't guess\n                        action.pop('related_votes', None)\n                    else:\n                        related_vote = vote['vote_id']\n                        if related_vote in already_linked:\n                            remove_vote.add(related_vote)\n\n                        already_linked.add(related_vote)\n                        action['related_votes'] = [related_vote]\n                        action_attached = True\n\n    # remove related_votes that we linked to multiple actions\n    for action in data['actions']:\n        for vote in remove_vote:\n            if vote in action.get('related_votes', []):\n                action['related_votes'].remove(vote)\n\n    # save action dates to data\n    data['action_dates'] = dates\n\n    data['_term'] = term_for_session(abbr, data['session'])\n\n    alt_titles = set(data.get('alternate_titles', []))\n\n    for version in data['versions']:\n        # Merge any version titles into the alternate_titles list\n        if 'title' in version:\n            alt_titles.add(version['title'])\n        if '+short_title' in version:\n            alt_titles.add(version['+short_title'])\n    try:\n        # Make sure the primary title isn't included in the\n        # alternate title list\n        alt_titles.remove(data['title'])\n    except KeyError:\n        pass\n    data['alternate_titles'] = list(alt_titles)\n    data = apply_filters(filters, data)\n\n    if not bill:\n        insert_with_id(data)\n        git_add_bill(data)\n        save_votes(data, bill_votes)\n        return \"insert\"\n    else:\n        update(bill, data, db.bills)\n        git_add_bill(bill)\n        save_votes(bill, bill_votes)\n        return \"update\"", "response": "Import a single bill into the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_current_fields(abbr):\n    meta = db.metadata.find_one({'_id': abbr})\n    current_term = meta['terms'][-1]\n    current_session = current_term['sessions'][-1]\n\n    for bill in db.bills.find({settings.LEVEL_FIELD: abbr}):\n        if bill['session'] == current_session:\n            bill['_current_session'] = True\n        else:\n            bill['_current_session'] = False\n\n        if bill['session'] in current_term['sessions']:\n            bill['_current_term'] = True\n        else:\n            bill['_current_term'] = False\n\n        db.bills.save(bill, safe=True)", "response": "Set/update _current_term and _current_session fields on all bills\n    for a given location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading in already set ids on objects", "response": "def learn_ids(self, item_list):\n        \"\"\" read in already set ids on objects \"\"\"\n        self._reset_sequence()\n        for item in item_list:\n            key = self.nondup_key_for_item(item)\n            self.ids[key] = item[self.id_key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets ids on an object using internal mapping then new ids", "response": "def set_ids(self, item_list):\n        \"\"\" set ids on an object, using internal mapping then new ids \"\"\"\n        self._reset_sequence()\n        for item in item_list:\n            key = self.nondup_key_for_item(item)\n            item[self.id_key] = self.ids.get(key) or self._get_next_id()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_committees_from_legislators(current_term, abbr):\n\n    # for all current legislators\n    for legislator in db.legislators.find({'roles': {'$elemMatch': {\n            'term': current_term, settings.LEVEL_FIELD: abbr}}}):\n\n        # for all committee roles\n        for role in legislator['roles']:\n            if (role['type'] == 'committee member' and\n                    'committee_id' not in role):\n\n                spec = {settings.LEVEL_FIELD: abbr,\n                        'chamber': role['chamber'],\n                        'committee': role['committee']}\n                if 'subcommittee' in role:\n                    spec['subcommittee'] = role['subcommittee']\n\n                committee = db.committees.find_one(spec)\n\n                if not committee:\n                    committee = spec\n                    committee['_type'] = 'committee'\n                    # copy LEVEL_FIELD from legislator to committee\n                    committee[settings.LEVEL_FIELD] = \\\n                        legislator[settings.LEVEL_FIELD]\n                    committee['members'] = []\n                    committee['sources'] = []\n                    if 'subcommittee' not in committee:\n                        committee['subcommittee'] = None\n                    insert_with_id(committee)\n\n                for member in committee['members']:\n                    if member['leg_id'] == legislator['leg_id']:\n                        break\n                else:\n                    committee['members'].append(\n                        {'name': legislator['full_name'],\n                         'leg_id': legislator['leg_id'],\n                         'role': role.get('position') or 'member'})\n                    for source in legislator['sources']:\n                        if source not in committee['sources']:\n                            committee['sources'].append(source)\n                    db.committees.save(committee, safe=True)\n\n                    role['committee_id'] = committee['_id']\n\n        db.legislators.save(legislator, safe=True)", "response": "create committees from all legislators that have committee roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassociates a sponsor with this bill.", "response": "def add_sponsor(self, type, name, **kwargs):\n        \"\"\"\n        Associate a sponsor with this bill.\n\n        :param type: the type of sponsorship, e.g. 'primary', 'cosponsor'\n        :param name: the name of the sponsor as provided by the official source\n        \"\"\"\n        self['sponsors'].append(dict(type=type, name=name, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_version(self, name, url, mimetype=None, on_duplicate='error',\n                    **kwargs):\n        \"\"\"\n        Add a version of the text of this bill.\n\n        :param name: a name given to this version of the text, e.g.\n                     'As Introduced', 'Version 2', 'As amended', 'Enrolled'\n        :param url: the location of this version on the legislative website.\n        :param mimetype: MIME type of the document\n        :param on_duplicate: What to do if a duplicate is seen:\n            error - default option, raises a ValueError\n            ignore - add the document twice (rarely the right choice)\n            use_new - use the new name, removing the old document\n            use_old - use the old name, not adding the new document\n\n        If multiple formats are provided, a good rule of thumb is to\n        prefer text, followed by html, followed by pdf/word/etc.\n        \"\"\"\n        if not mimetype:\n            raise ValueError('mimetype parameter to add_version is required')\n        if on_duplicate != 'ignore':\n            if url in self._seen_versions:\n                if on_duplicate == 'error':\n                    raise ValueError('duplicate version url %s' % url)\n                elif on_duplicate == 'use_new':\n                    # delete the old version\n                    self['versions'] = [v for v in self['versions']\n                                        if v['url'] != url]\n                elif on_duplicate == 'use_old':\n                    return       # do nothing\n            self._seen_versions.add(url)\n\n        d = dict(name=name, url=url, mimetype=mimetype, **kwargs)\n        self['versions'].append(d)", "response": "Add a version of the text to the current bill."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an action that was performed on this bill.", "response": "def add_action(self, actor, action, date, type=None, committees=None,\n                   legislators=None, **kwargs):\n        \"\"\"\n        Add an action that was performed on this bill.\n\n        :param actor: a string representing who performed the action.\n          If the action is associated with one of the chambers this\n          should be 'upper' or 'lower'. Alternatively, this could be\n          the name of a committee, a specific legislator, or an outside\n          actor such as 'Governor'.\n        :param action: a string representing the action performed, e.g.\n                       'Introduced', 'Signed by the Governor', 'Amended'\n        :param date: the date/time this action was performed.\n        :param type: a type classification for this action\n        ;param committees: a committee or list of committees to associate with\n                           this action\n        \"\"\"\n\n        def _cleanup_list(obj, default):\n            if not obj:\n                obj = default\n            elif isinstance(obj, string_types):\n                obj = [obj]\n            elif not isinstance(obj, list):\n                obj = list(obj)\n            return obj\n\n        type = _cleanup_list(type, ['other'])\n        committees = _cleanup_list(committees, [])\n        legislators = _cleanup_list(legislators, [])\n\n        if 'committee' in kwargs:\n            raise ValueError(\"invalid param 'committee' passed to add_action, \"\n                             \"must use committees\")\n\n        if isinstance(committees, string_types):\n            committees = [committees]\n\n        related_entities = []         # OK, let's work some magic.\n        for committee in committees:\n            related_entities.append({\n                \"type\": \"committee\",\n                \"name\": committee\n            })\n\n        for legislator in legislators:\n            related_entities.append({\n                \"type\": \"legislator\",\n                \"name\": legislator\n            })\n\n        self['actions'].append(dict(actor=actor, action=action,\n                                    date=date, type=type,\n                                    related_entities=related_entities,\n                                    **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassociate another bill with this one.", "response": "def add_companion(self, bill_id, session=None, chamber=None):\n        \"\"\"\n        Associate another bill with this one.\n\n        If session isn't set it will be set to self['session'].\n        \"\"\"\n        companion = {'bill_id': bill_id,\n                     'session': session or self['session'],\n                     'chamber': chamber}\n        self['companions'].append(companion)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef homepage(request):\n    '''\n    Context:\n        all_metadata\n\n    Templates:\n        - billy/web/public/homepage.html\n    '''\n    all_metadata = db.metadata.find()\n\n    return render(request, templatename('homepage'),\n                  dict(all_metadata=all_metadata))", "response": "Returns the homepage. html."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef downloads(request):\n    '''\n    Context:\n        - all_metadata\n\n    Templates:\n        - billy/web/public/downloads.html\n    '''\n    all_metadata = sorted(db.metadata.find(), key=lambda x: x['name'])\n    return render(request, 'billy/web/public/downloads.html',\n                  {'all_metadata': all_metadata})", "response": "Returns a list of all downloads."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_your_legislator(request):\n    '''\n    Context:\n        - request\n        - lat\n        - long\n        - located\n        - legislators\n\n    Templates:\n        - billy/web/public/find_your_legislator_table.html\n    '''\n\n    # check if lat/lon are set\n    # if leg_search is set, they most likely don't have ECMAScript enabled.\n    # XXX: fallback behavior here for alpha.\n\n    get = request.GET\n    context = {}\n    template = 'find_your_legislator'\n\n    context['request'] = \"\"\n    if \"q\" in get:\n        context['request'] = get['q']\n\n    if \"lat\" in get and \"lon\" in get:\n        # We've got a passed lat/lon. Let's build off it.\n        lat = get['lat']\n        lon = get['lon']\n\n        context['lat'] = lat\n        context['lon'] = lon\n        context['located'] = True\n\n        qurl = \"%slegislators/geo/?long=%s&lat=%s&apikey=%s\" % (\n            billy_settings.API_BASE_URL,\n            lon,\n            lat,\n            getattr(billy_settings, 'API_KEY', '')\n        )\n        leg_resp = json.load(urllib2.urlopen(qurl, timeout=0.5))\n        # allow limiting lookup to region for region map views\n        if 'abbr' in get:\n            leg_resp = [leg for leg in leg_resp\n                        if leg[billy_settings.LEVEL_FIELD] == get['abbr']]\n            context['abbr'] = get['abbr']\n\n        # Also, allow filtering by chamber\n        if 'chamber' in get:\n            leg_resp = [leg for leg in leg_resp\n                        if leg['chamber'] == get['chamber']]\n            context['chamber'] = get['chamber']\n\n        if \"boundary\" in get:\n            return HttpResponse(json.dumps([]))\n\n        context['legislators'] = map(Legislator, leg_resp)\n        template = 'find_your_legislator_table'\n\n    return render(request, templatename(template), context)", "response": "Find the most likely legislator for a given location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef activate_legislators(current_term, abbr):\n    for legislator in db.legislators.find(\n        {'roles': {'$elemMatch':\n                   {settings.LEVEL_FIELD: abbr, 'term': current_term}}}):\n        active_role = legislator['roles'][0]\n\n        if not active_role.get('end_date') and active_role['type'] == 'member':\n            legislator['active'] = True\n            legislator['party'] = active_role.get('party', None)\n            legislator['district'] = active_role.get('district', None)\n            legislator['chamber'] = active_role.get('chamber', None)\n\n        legislator['updated_at'] = datetime.datetime.utcnow()\n        db.legislators.save(legislator, safe=True)", "response": "Activates all legislators with the given term."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_role(self, role, term, start_date=None, end_date=None,\n                 **kwargs):\n        \"\"\"\n        Examples:\n\n        leg.add_role('member', term='2009', chamber='upper',\n                     party='Republican', district='10th')\n        \"\"\"\n        self['roles'].append(dict(role=role, term=term,\n                                  start_date=start_date,\n                                  end_date=end_date, **kwargs))", "response": "Adds a role to the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_office(self, type, name, address=None, phone=None, fax=None,\n                   email=None, **kwargs):\n        \"\"\"\n        Allowed office types:\n            capitol\n            district\n        \"\"\"\n        office_dict = dict(type=type, address=address, name=name, phone=phone,\n                           fax=fax, email=email, **kwargs)\n        self['offices'].append(office_dict)", "response": "Add an office to the set of allowed offices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the metadata for the given two - letter abbreviation.", "response": "def metadata(abbr, __metadata=__metadata):\n    \"\"\"\n    Grab the metadata for the given two-letter abbreviation.\n    \"\"\"\n    # This data should change very rarely and is queried very often so\n    # cache it here\n    abbr = abbr.lower()\n    if abbr in __metadata:\n        return __metadata[abbr]\n    rv = db.metadata.find_one({'_id': abbr})\n\n    __metadata[abbr] = rv\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the path if it doesn t exist", "response": "def cd(path):\n    '''Creates the path if it doesn't exist'''\n    old_dir = os.getcwd()\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(old_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_property_dict(schema):\n    pdict = {}\n    for k, v in schema['properties'].items():\n        pdict[k] = {}\n        if 'items' in v and 'properties' in v['items']:\n            pdict[k] = _get_property_dict(v['items'])\n    pdict[settings.LEVEL_FIELD] = {}\n    return pdict", "response": "given a schema produce a nested dictionary of fields"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_with_id(obj):\n    if '_id' in obj:\n        raise ValueError(\"object already has '_id' field\")\n\n    # add created_at/updated_at on insert\n    obj['created_at'] = datetime.datetime.utcnow()\n    obj['updated_at'] = obj['created_at']\n\n    if obj['_type'] == 'person' or obj['_type'] == 'legislator':\n        collection = db.legislators\n        id_type = 'L'\n    elif obj['_type'] == 'committee':\n        collection = db.committees\n        id_type = 'C'\n    elif obj['_type'] == 'bill':\n        collection = db.bills\n        id_type = 'B'\n    else:\n        raise ValueError(\"unknown _type for object\")\n\n    # get abbr\n    abbr = obj[settings.LEVEL_FIELD].upper()\n\n    id_reg = re.compile('^%s%s' % (abbr, id_type))\n\n    # Find the next available _id and insert\n    id_prefix = '%s%s' % (abbr, id_type)\n    cursor = collection.find({'_id': id_reg}).sort('_id', -1).limit(1)\n\n    try:\n        new_id = int(next(cursor)['_id'][len(abbr) + 1:]) + 1\n    except StopIteration:\n        new_id = 1\n\n    while True:\n        if obj['_type'] == 'bill':\n            obj['_id'] = '%s%08d' % (id_prefix, new_id)\n        else:\n            obj['_id'] = '%s%06d' % (id_prefix, new_id)\n        obj['_all_ids'] = [obj['_id']]\n\n        if obj['_type'] in ['person', 'legislator']:\n            obj['leg_id'] = obj['_id']\n\n        try:\n            return collection.insert(obj, safe=True)\n        except pymongo.errors.DuplicateKeyError:\n            new_id += 1", "response": "Insert a new object into the appropriate collection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(old, new, collection, sneaky_update_filter=None):\n    # need_save = something has changed\n    need_save = False\n\n    locked_fields = old.get('_locked_fields', [])\n\n    for key, value in new.items():\n\n        # don't update locked fields\n        if key in locked_fields:\n            continue\n\n        if old.get(key) != value:\n            if sneaky_update_filter and key in sneaky_update_filter:\n                if sneaky_update_filter[key](old[key], value):\n                    old[key] = value\n                    need_save = True\n            else:\n                old[key] = value\n                need_save = True\n\n        # remove old +key field if this field no longer has a +\n        plus_key = '+%s' % key\n        if plus_key in old:\n            del old[plus_key]\n            need_save = True\n\n    if need_save:\n        old['updated_at'] = datetime.datetime.utcnow()\n        collection.save(old, safe=True)\n\n    return need_save", "response": "update an existing object with a new object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts unix timestamps in the scraper output to python datetimes.", "response": "def convert_timestamps(obj):\n    \"\"\"\n    Convert unix timestamps in the scraper output to python datetimes\n    so that they will be saved properly as Mongo datetimes.\n    \"\"\"\n    for key in ('date', 'when', 'end', 'start_date', 'end_date'):\n        value = obj.get(key)\n        if value:\n            try:\n                obj[key] = _timestamp_to_dt(value)\n            except TypeError:\n                raise TypeError(\"expected float for %s, got %s\" % (key, value))\n\n    for key in ('sources', 'actions', 'votes', 'roles'):\n        for child in obj.get(key, []):\n            convert_timestamps(child)\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_name(obj):\n    if obj['_type'] in ('person', 'legislator'):\n        for key in ('first_name', 'last_name'):\n            if key not in obj or not obj[key]:\n                # Need to split\n                (obj['first_name'], obj['last_name'],\n                 obj['suffixes']) = name_tools.split(obj['full_name'])[1:]\n                break\n\n    return obj", "response": "Split name into first_name and last_name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_plus_helper(obj, fields):\n    new_obj = {}\n\n    for key, value in obj.items():\n        if key in fields or key.startswith('_'):\n            # if there's a subschema apply it to a list or subdict\n            if fields.get(key):\n                if isinstance(value, list):\n                    value = [_make_plus_helper(item, fields[key])\n                             for item in value]\n            # assign the value (modified potentially) to the new_obj\n            new_obj[key] = value\n        else:\n            # values not in the fields dict get a +\n            new_obj['+%s' % key] = value\n\n    return new_obj", "response": "Add a + prefix to any fields in obj that aren t in fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a '+' to the key of non - standard fields.", "response": "def make_plus_fields(obj):\n    \"\"\"\n    Add a '+' to the key of non-standard fields.\n\n    dispatch to recursive _make_plus_helper based on _type field\n    \"\"\"\n    fields = standard_fields.get(obj['_type'], dict())\n    return _make_plus_helper(obj, fields)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object(cls, abbr):\n        '''\n        This particular model needs its own constructor in order to take\n        advantage of the metadata cache in billy.util, which would otherwise\n        return unwrapped objects.\n        '''\n        obj = get_metadata(abbr)\n        if obj is None:\n            msg = 'No metadata found for abbreviation %r' % abbr\n            raise DoesNotExist(msg)\n        return cls(obj)", "response": "Returns an object of the class cls with the given abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef committees_legislators(self, *args, **kwargs):\n        '''Return an iterable of committees with all the\n        legislators cached for reference in the Committee model.\n        So do a \"select_related\" operation on committee members.\n        '''\n        committees = list(self.committees(*args, **kwargs))\n        legislators = self.legislators({'active': True},\n                                       fields=['full_name',\n                                               settings.LEVEL_FIELD])\n        _legislators = {}\n\n        # This will be a cache of legislator objects used in\n        # the committees.html template. Includes ids in each\n        # legislator's _all_ids field (if it exists.)\n        for obj in legislators:\n            if 'all_ids' in obj:\n                for _id in obj['_all_ids']:\n                    _legislators[_id] = obj\n            else:\n                _legislators[obj['_id']] = obj\n        del legislators\n        for com in committees:\n            com._legislators = _legislators\n        return committees", "response": "Return an iterable of committees with all the\n        legislators cached for reference in the Committee model. So do a select_related operation on members."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_fields(d, fields, delimiter='|'):\n    rd = {}\n    for f in fields:\n        v = d.get(f, None)\n        if isinstance(v, (str, unicode)):\n            v = v.encode('utf8')\n        elif isinstance(v, list):\n            v = delimiter.join(v)\n        rd[f] = v\n    return rd", "response": "extracts values out of an object d for saving to a csv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of Bills for the given session and bill_id.", "response": "def bill(request, abbr, session, bill_id):\n    '''\n    Context:\n        - vote_preview_row_template\n        - abbr\n        - metadata\n        - bill\n        - show_all_sponsors\n        - sponsors\n        - sources\n        - nav_active\n\n    Templates:\n        - billy/web/public/bill.html\n        - billy/web/public/vote_preview_row.html\n    '''\n    # get fixed version\n    fixed_bill_id = fix_bill_id(bill_id)\n    # redirect if URL's id isn't fixed id without spaces\n    if fixed_bill_id.replace(' ', '') != bill_id:\n        return redirect('bill', abbr=abbr, session=session, bill_id=fixed_bill_id.replace(' ', ''))\n    bill = db.bills.find_one({settings.LEVEL_FIELD: abbr, 'session': session,\n                              'bill_id': fixed_bill_id})\n    if bill is None:\n        raise Http404(u'no bill found {0} {1} {2}'.format(abbr, session, bill_id))\n\n    show_all_sponsors = request.GET.get('show_all_sponsors')\n    if show_all_sponsors:\n        sponsors = bill.sponsors_manager\n    else:\n        sponsors = bill.sponsors_manager.first_fifteen\n\n    return render(\n        request, templatename('bill'),\n        dict(vote_preview_row_template=templatename('vote_preview_row'),\n             abbr=abbr,\n             metadata=Metadata.get_object(abbr),\n             bill=bill,\n             show_all_sponsors=show_all_sponsors,\n             sponsors=sponsors,\n             sources=bill['sources'],\n             nav_active='bills'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vote(request, abbr, vote_id):\n    '''\n    Context:\n        - abbr\n        - metadata\n        - bill\n        - vote\n        - nav_active\n\n    Templates:\n        - vote.html\n    '''\n    vote = db.votes.find_one(vote_id)\n    if vote is None:\n        raise Http404('no such vote: {0}'.format(vote_id))\n    bill = vote.bill()\n\n    return render(request, templatename('vote'),\n                  dict(abbr=abbr, metadata=Metadata.get_object(abbr),\n                       bill=bill,\n                       vote=vote,\n                       nav_active='bills'))", "response": "Return a single object with the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of documents that are available for the given document.", "response": "def document(request, abbr, session, bill_id, doc_id):\n    '''\n    Context:\n        - abbr\n        - session\n        - bill\n        - version\n        - metadata\n        - nav_active\n\n    Templates:\n        - billy/web/public/document.html\n    '''\n    # get fixed version\n    fixed_bill_id = fix_bill_id(bill_id)\n    # redirect if URL's id isn't fixed id without spaces\n    if fixed_bill_id.replace(' ', '') != bill_id:\n        return redirect('document', abbr=abbr, session=session,\n                        bill_id=fixed_bill_id.replace(' ', ''), doc_id=doc_id)\n\n    bill = db.bills.find_one({settings.LEVEL_FIELD: abbr, 'session': session,\n                              'bill_id': fixed_bill_id})\n\n    if not bill:\n        raise Http404('No such bill.')\n\n    for version in bill['versions']:\n        if version['doc_id'] == doc_id:\n            break\n    else:\n        raise Http404('No such document.')\n\n    if not settings.ENABLE_DOCUMENT_VIEW.get(abbr, False):\n        return redirect(version['url'])\n\n    return render(request, templatename('document'),\n                  dict(abbr=abbr, session=session, bill=bill, version=version,\n                       metadata=bill.metadata, nav_active='bills'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_all(key):\n    '''\n    Context:\n        - abbr\n        - metadata\n        - bill\n        - sources\n        - nav_active\n\n    Templates:\n        - billy/web/public/bill_all_{key}.html\n            - where key is passed in, like \"actions\", etc.\n    '''\n    def func(request, abbr, session, bill_id, key):\n        # get fixed version\n        fixed_bill_id = fix_bill_id(bill_id)\n        # redirect if URL's id isn't fixed id without spaces\n        if fixed_bill_id.replace(' ', '') != bill_id:\n            return redirect('bill', abbr=abbr, session=session,\n                            bill_id=fixed_bill_id.replace(' ', ''))\n        bill = db.bills.find_one({settings.LEVEL_FIELD: abbr,\n                                  'session': session,\n                                  'bill_id': fixed_bill_id})\n        if bill is None:\n            raise Http404('no bill found {0} {1} {2}'.format(abbr, session,\n                                                             bill_id))\n        return render(request, templatename('bill_all_%s' % key),\n                      dict(abbr=abbr, metadata=Metadata.get_object(abbr),\n                           bill=bill, sources=bill['sources'],\n                           nav_active='bills'))\n    return func", "response": "A view that returns a list of all bills."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a context item with the data from the filter bills form.", "response": "def get_context_data(self, *args, **kwargs):\n        '''\n        Context:\n            If GET parameters are given:\n            - search_text\n            - form (FilterBillsForm)\n            - long_description\n            - description\n            - get_params\n            Otherwise, the only context item is an unbound FilterBillsForm.\n\n        Templates:\n            - Are specified in subclasses.\n        '''\n        context = super(RelatedBillsList, self).get_context_data(*args,\n                                                                 **kwargs)\n        metadata = context['metadata']\n        FilterBillsForm = get_filter_bills_form(metadata)\n\n        if self.request.GET:\n            form = FilterBillsForm(self.request.GET)\n            search_text = form.data.get('search_text')\n            context.update(search_text=search_text)\n            context.update(form=FilterBillsForm(self.request.GET))\n\n            # human readable description of search\n            description = []\n            if metadata:\n                description.append(metadata['name'])\n            else:\n                description = ['Search All']\n            long_description = []\n            chamber = form.data.get('chamber')\n            session = form.data.get('session')\n            type = form.data.get('type')\n            status = form.data.getlist('status')\n            subjects = form.data.getlist('subjects')\n            sponsor = form.data.get('sponsor__leg_id')\n            if chamber:\n                if metadata:\n                    description.append(metadata['chambers'][chamber]['name']\n                                      )\n                else:\n                    description.extend([chamber.title(), 'Chamber'])\n            description.append((type or 'Bill') + 's')\n            if session:\n                description.append(\n                    '(%s)' %\n                    metadata['session_details'][session]['display_name']\n                )\n            if 'signed' in status:\n                long_description.append('which have been signed into law')\n            elif 'passed_upper' in status and 'passed_lower' in status:\n                long_description.append('which have passed both chambers')\n            elif 'passed_lower' in status:\n                chamber_name = (metadata['chambers']['lower']['name']\n                                if metadata else 'lower chamber')\n                long_description.append('which have passed the ' +\n                                        chamber_name)\n            elif 'passed_upper' in status:\n                chamber_name = (metadata['chambers']['upper']['name']\n                                if metadata else 'upper chamber')\n                long_description.append('which have passed the ' +\n                                        chamber_name)\n            if sponsor:\n                leg = db.legislators.find_one({'_all_ids': sponsor},\n                                              fields=('full_name', '_id'))\n                leg = leg['full_name']\n                long_description.append('sponsored by ' + leg)\n            if subjects:\n                long_description.append('related to ' + ', '.join(subjects))\n            if search_text:\n                long_description.append(u'containing the term \"{0}\"'.format(\n                    search_text))\n            context.update(long_description=long_description)\n        else:\n            if metadata:\n                description = [metadata['name'], 'Bills']\n            else:\n                description = ['All Bills']\n            context.update(form=FilterBillsForm())\n\n        context.update(description=' '.join(description))\n\n        # Add the correct path to paginated links.\n        params = list(self.request.GET.lists())\n        for k, v in params[:]:\n            if k == 'page':\n                params.remove((k, v))\n        get_params = urllib.urlencode(params, doseq=True)\n        context['get_params'] = get_params\n\n        # Add the abbr.\n        context['abbr'] = self.kwargs['abbr']\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle submission of the region selection form in the base template.", "response": "def region_selection(request):\n    '''Handle submission of the region selection form in the base template. '''\n    form = get_region_select_form(request.GET)\n    abbr = form.data.get('abbr')\n    if not abbr or len(abbr) != 2:\n        return redirect('homepage')\n    return redirect('region', abbr=abbr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef region(request, abbr):\n    '''\n    Context:\n        - abbr\n        - metadata\n        - sessions\n        - chambers\n        - joint_committee_count\n        - geo_bounds\n        - nav_active\n\n    Templates:\n        - bill/web/public/region.html\n    '''\n    report = db.reports.find_one({'_id': abbr})\n    try:\n        meta = Metadata.get_object(abbr)\n    except DoesNotExist:\n        raise Http404\n\n    fallback_bounds = GEO_BOUNDS['US']\n    geo_bounds = GEO_BOUNDS.get(abbr.upper(), fallback_bounds)\n\n    # count legislators\n    legislators = meta.legislators({'active': True}, {'party': True,\n                                                      'chamber': True})\n    # Maybe later, mapreduce instead?\n    party_counts = defaultdict(lambda: defaultdict(int))\n    for leg in legislators:\n        if 'chamber' in leg:    # exclude lt. governors\n            party_counts[leg['chamber']][leg['party']] += 1\n\n    chambers = []\n\n    for chamber_type, chamber in meta['chambers'].items():\n        res = {}\n\n        # chamber metadata\n        res['type'] = chamber_type\n        res['title'] = chamber['title']\n        res['name'] = chamber['name']\n\n        # legislators\n        res['legislators'] = {\n            'count': sum(party_counts[chamber_type].values()),\n            'party_counts': dict(party_counts[chamber_type]),\n        }\n\n        # committees\n        res['committees_count'] = meta.committees({'chamber': chamber_type}\n                                                 ).count()\n\n        res['latest_bills'] = meta.bills({'chamber': chamber_type}).sort(\n            [('action_dates.first', -1)]).limit(2)\n        res['passed_bills'] = meta.bills({'chamber': chamber_type}).sort(\n            [('action_dates.passed_' + chamber_type, -1)]).limit(2)\n\n        chambers.append(res)\n\n    joint_committee_count = meta.committees({'chamber': 'joint'}).count()\n\n    # add bill counts to session listing\n    sessions = meta.sessions()\n    for s in sessions:\n        try:\n            s['bill_count'] = (\n                report['bills']['sessions'][s['id']]['upper_count']\n                + report['bills']['sessions'][s['id']]['lower_count'])\n        except KeyError:\n            # there's a chance that the session had no bills\n            s['bill_count'] = 0\n\n    return render(request, templatename('region'),\n                  dict(abbr=abbr, metadata=meta, sessions=sessions,\n                       chambers=chambers,\n                       joint_committee_count=joint_committee_count,\n                       geo_bounds=geo_bounds,\n                       nav_active='home'))", "response": "Return a single region for the given abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for a specific entry in the list of legislators.", "response": "def search(request, abbr):\n    '''\n    Context:\n        - search_text\n        - abbr\n        - metadata\n        - found_by_id\n        - bill_results\n        - more_bills_available\n        - legislators_list\n        - nav_active\n\n    Tempaltes:\n        - billy/web/public/search_results_no_query.html\n        - billy/web/public/search_results_bills_legislators.html\n        - billy/web/public/bills_list_row_with_abbr_and_session.html\n    '''\n    if not request.GET:\n        return render(request, templatename('search_results_no_query'),\n                      {'abbr': abbr})\n\n    search_text = unicode(request.GET['search_text']).encode('utf8')\n\n    # First try to get by bill_id.\n    if re.search(r'\\d', search_text):\n        url = '/%s/bills?' % abbr\n        url += urllib.urlencode([('search_text', search_text)])\n        return redirect(url)\n\n    else:\n        found_by_id = False\n        kwargs = {}\n        if abbr != 'all':\n            kwargs['abbr'] = abbr\n        bill_results = Bill.search(search_text, sort='last', **kwargs)\n\n        # Limit the bills if it's a search.\n        bill_result_count = len(bill_results)\n        more_bills_available = (bill_result_count > 5)\n        bill_results = bill_results[:5]\n\n        # See if any legislator names match. First split up name to avoid\n        # the Richard S. Madaleno problem. See Jira issue OS-32.\n        textbits = search_text.split()\n        textbits = filter(lambda s: 2 < len(s), textbits)\n        textbits = filter(lambda s: '.' not in s, textbits)\n        andspec = []\n        for text in textbits:\n            andspec.append({'full_name': {'$regex': text, '$options': 'i'}})\n        if andspec:\n            spec = {'$and': andspec}\n        else:\n            spec = {'full_name': {'$regex': search_text, '$options': 'i'}}\n\n        # Run the query.\n        if abbr != 'all':\n            spec[settings.LEVEL_FIELD] = abbr\n        legislator_results = list(db.legislators.find(spec).sort(\n            [('active', -1)]))\n\n    if abbr != 'all':\n        metadata = Metadata.get_object(abbr)\n    else:\n        metadata = None\n\n    return render(\n        request, templatename('search_results_bills_legislators'),\n        dict(search_text=search_text,\n             abbr=abbr,\n             metadata=metadata,\n             found_by_id=found_by_id,\n             bill_results=bill_results,\n             bill_result_count=bill_result_count,\n             more_bills_available=more_bills_available,\n             legislators_list=legislator_results,\n             column_headers_tmplname=None,  # not used\n             rowtemplate_name=templatename('bills_list_row_with'\n                                           '_abbr_and_session'),\n             show_chamber_column=True,\n             nav_active=None))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_member(self, legislator, role='member', **kwargs):\n        self['members'].append(dict(name=legislator, role=role,\n                                    **kwargs))", "response": "Add a member to the committee object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _bytype(self, action_type, action_spec=None):\n        '''Return the most recent date on which action_type occurred.\n        Action spec is a dictionary of key-value attrs to match.'''\n        for action in reversed(self.bill['actions']):\n            if action_type in action['type']:\n                for k, v in action_spec.items():\n                    if action[k] == v:\n                        yield action", "response": "Return the most recent date on which action_type occurred."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the yes / total ratio as a percetage string suitable for use as css attribute.", "response": "def _ratio(self, key):\n        '''Return the yes/total ratio as a percetage string\n        suitable for use as as css attribute.'''\n        total = float(self._total_votes())\n        try:\n            return math.floor(self[key] / total * 100)\n        except ZeroDivisionError:\n            return float(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of this vote.", "response": "def legislator_vote_value(self):\n        '''If this vote was accessed through the legislator.votes_manager,\n        return the value of this legislator's vote.\n        '''\n        if not hasattr(self, 'legislator'):\n            msg = ('legislator_vote_value can only be called '\n                   'from a vote accessed by legislator.votes_manager.')\n            raise ValueError(msg)\n        leg_id = self.legislator.id\n        for k in ('yes', 'no', 'other'):\n            for leg in self[k + '_votes']:\n                if leg['leg_id'] == leg_id:\n                    return k"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _vote_legislators(self, yes_no_other):\n        '''Return all legislators who votes yes/no/other on this bill.\n        '''\n        #id_getter = operator.itemgetter('leg_id')\n        #ids = map(id_getter, self['%s_votes' % yes_no_other])\n        #return map(self._legislator_objects.get, ids)\n        result = []\n        for voter in self[yes_no_other + '_votes']:\n            if voter['leg_id']:\n                result.append(self._legislator_objects.get(voter['leg_id']))\n            else:\n                result.append(voter)\n        return result", "response": "Return all legislators who votes yes or no on this bill."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nguess whether this vote is a voice vote.", "response": "def is_probably_a_voice_vote(self):\n        '''Guess whether this vote is a \"voice vote\".'''\n        if '+voice_vote' in self:\n            return True\n        if '+vote_type' in self:\n            if self['+vote_type'] == 'Voice':\n                return True\n        if 'voice vote' in self['motion'].lower():\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bill_objects(self):\n        '''Returns a cursor of full bill objects for any bills that have\n        ids. Not in use anyware as of 12/18/12, but handy to have around.\n        '''\n        bills = []\n        for bill in self['related_bills']:\n            if 'id' in bill:\n                bills.append(bill['id'])\n        return db.bills.find({\"_id\": {\"$in\": bills}})", "response": "Returns a cursor of full bill objects for any bills that have\n        ids. Not in use anyware as of 12 / 18 / 12 but handy to have around."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef host(self):\n        '''Return the host committee.\n        '''\n        _id = None\n        for participant in self['participants']:\n            if participant['type'] == 'host':\n                if set(['participant_type', 'id']) < set(participant):\n                    # This event uses the id keyname \"id\".\n                    if participant['participant_type'] == 'committee':\n                        _id = participant['id']\n                        if _id is None:\n                            continue\n                        return self.committees_dict.get(_id)\n                else:\n                    return participant['participant']", "response": "Return the host committee."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of members that chair the host committee and including co - chair and chairperson. This could concievalby yield a false positive positive.", "response": "def host_chairs(self):\n        '''Returns a list of members that chair the host committee,\n        including \"co-chair\" and \"chairperson.\" This could concievalby\n        yield a false positive if the person's title is 'dunce chair'.\n        '''\n        chairs = []\n        # Host is guaranteed to be a committe or none.\n        host = self.host()\n        if host is None:\n            return\n        for member, full_member in host.members_objects:\n            if 'chair' in member.get('role', '').lower():\n                chairs.append((member, full_member))\n        return chairs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef host_members(self):\n        '''Return the members of the host committee.\n        '''\n        host = self.host()\n        if host is None:\n            return\n        for member, full_member in host.members_objects:\n            yield full_member", "response": "Return the members of the host committee."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new empty tree of committees for the given author.", "response": "def committees(request, abbr):\n    '''\n    Context:\n        chamber\n        committees\n        abbr\n        metadata\n        chamber_name\n        chamber_select_template\n        chamber_select_collection\n        chamber_select_chambers\n        committees_table_template\n        show_chamber_column\n        sort_order\n        nav_active\n\n    Templates:\n        - billy/web/public/committees.html\n        - billy/web/public/committees-pjax.html\n        - billy/web/public/chamber_select_form.html\n        - billy/web/public/committees_table.html\n    '''\n    try:\n        meta = Metadata.get_object(abbr)\n    except DoesNotExist:\n        raise Http404\n\n    chamber = request.GET.get('chamber', 'both')\n    if chamber in ('upper', 'lower'):\n        chamber_name = meta['chambers'][chamber]['name']\n        spec = {'chamber': chamber}\n        show_chamber_column = False\n    elif chamber == 'joint':\n        chamber_name = 'Joint'\n        spec = {'chamber': 'joint'}\n        show_chamber_column = False\n    else:\n        chamber = 'both'\n        spec = {}\n        show_chamber_column = True\n        chamber_name = ''\n\n    chambers = dict((k, v['name']) for k, v in meta['chambers'].items())\n    if meta.committees({'chamber': 'joint'}).count():\n        chambers['joint'] = 'Joint'\n\n    fields = mongo_fields('committee', 'subcommittee', 'members',\n                          settings.LEVEL_FIELD, 'chamber')\n\n    sort_key = request.GET.get('key', 'committee')\n    sort_order = int(request.GET.get('order', 1))\n\n    committees = meta.committees_legislators(spec, fields=fields,\n                                             sort=[(sort_key, sort_order)])\n\n    sort_order = -sort_order\n\n    return TemplateResponse(\n        request, templatename('committees'),\n        dict(chamber=chamber, committees=committees, abbr=abbr, metadata=meta,\n             chamber_name=chamber_name,\n             chamber_select_template=templatename('chamber_select_form'),\n             chamber_select_collection='committees',\n             chamber_select_chambers=chambers,\n             committees_table_template=templatename('committees_table'),\n             show_chamber_column=show_chamber_column, sort_order=sort_order,\n             nav_active='committees'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef committee(request, abbr, committee_id):\n    '''\n    Context:\n        - committee\n        - abbr\n        - metadata\n        - sources\n        - nav_active\n\n    Tempaltes:\n        - billy/web/public/committee.html\n    '''\n    committee = db.committees.find_one({'_id': committee_id})\n    if committee is None:\n        raise Http404\n\n    return render(request, templatename('committee'),\n                  dict(committee=committee, abbr=abbr,\n                       metadata=Metadata.get_object(abbr),\n                       sources=committee['sources'],\n                       nav_active='committees'))", "response": "Return a single committee."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_common(obj, report):\n    # updated checks\n    if obj['updated_at'] >= yesterday:\n        report['_updated_today_count'] += 1\n        if obj['updated_at'] >= last_month:\n            report['_updated_this_month_count'] += 1\n            if obj['updated_at'] >= last_year:\n                report['_updated_this_year_count'] += 1", "response": "Update the common statistics for the objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef legislators(request, abbr):\n    '''\n    Context:\n        - metadata\n        - chamber\n        - chamber_title\n        - chamber_select_template\n        - chamber_select_collection\n        - chamber_select_chambers\n        - show_chamber_column\n        - abbr\n        - legislators\n        - sort_order\n        - sort_key\n        - legislator_table\n        - nav_active\n\n    Templates:\n        - billy/web/public/legislators.html\n        - billy/web/public/chamber_select_form.html\n        - billy/web/public/legislator_table.html\n    '''\n    try:\n        meta = Metadata.get_object(abbr)\n    except DoesNotExist:\n        raise Http404\n\n    spec = {'active': True, 'district': {'$exists': True}}\n\n    chambers = dict((k, v['name']) for k, v in meta['chambers'].items())\n\n    chamber = request.GET.get('chamber', 'both')\n    if chamber in chambers:\n        spec['chamber'] = chamber\n        chamber_title = meta['chambers'][chamber]['title'] + 's'\n    else:\n        chamber = 'both'\n        chamber_title = 'Legislators'\n\n    fields = mongo_fields('leg_id', 'full_name', 'photo_url', 'district',\n                          'party', 'first_name', 'last_name', 'chamber',\n                          billy_settings.LEVEL_FIELD, 'last_name')\n\n    sort_key = 'district'\n    sort_order = 1\n\n    if request.GET:\n        sort_key = request.GET.get('key', sort_key)\n        sort_order = int(request.GET.get('order', sort_order))\n\n    legislators = meta.legislators(extra_spec=spec, fields=fields)\n\n    def sort_by_district(obj):\n        matchobj = re.search(r'\\d+', obj.get('district', '') or '')\n        if matchobj:\n            return int(matchobj.group())\n        else:\n            return obj.get('district', '')\n\n    legislators = sorted(legislators, key=sort_by_district)\n\n    if sort_key != 'district':\n        legislators = sorted(legislators, key=operator.itemgetter(sort_key),\n                             reverse=(sort_order == -1))\n    else:\n        legislators = sorted(legislators, key=sort_by_district,\n                             reverse=bool(0 > sort_order))\n\n    sort_order = {1: -1, -1: 1}[sort_order]\n    legislators = list(legislators)\n\n    return TemplateResponse(\n        request, templatename('legislators'),\n        dict(metadata=meta, chamber=chamber,\n             chamber_title=chamber_title,\n             chamber_select_template=templatename('chamber_select_form'),\n             chamber_select_collection='legislators',\n             chamber_select_chambers=chambers, show_chamber_column=True,\n             abbr=abbr, legislators=legislators, sort_order=sort_order,\n             sort_key=sort_key,\n             legislator_table=templatename('legislator_table'),\n             nav_active='legislators'))", "response": "Return a list of legislators for the given legislator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a single legislator.", "response": "def legislator(request, abbr, _id, slug=None):\n    '''\n    Context:\n        - vote_preview_row_template\n        - roles\n        - abbr\n        - district_id\n        - metadata\n        - legislator\n        - sources\n        - sponsored_bills\n        - legislator_votes\n        - has_votes\n        - nav_active\n\n    Templates:\n        - billy/web/public/legislator.html\n        - billy/web/public/vote_preview_row.html\n\n    '''\n    try:\n        meta = Metadata.get_object(abbr)\n    except DoesNotExist:\n        raise Http404\n\n    legislator = db.legislators.find_one({'_id': _id})\n    if legislator is None:\n        spec = {'_all_ids': _id}\n        cursor = db.legislators.find(spec)\n        msg = 'Two legislators returned for spec %r' % spec\n        assert cursor.count() < 2, msg\n        try:\n            legislator = next(cursor)\n        except StopIteration:\n            raise Http404('No legislator was found with leg_id = %r' % _id)\n        else:\n            return redirect(legislator.get_absolute_url(), permanent=True)\n\n    if not legislator['active']:\n        return legislator_inactive(request, abbr, legislator)\n\n    district = db.districts.find({'abbr': abbr, 'chamber': legislator['chamber'],\n                                  'name': legislator['district']})\n    if district:\n        district_id = district[0]['division_id']\n    else:\n        district_id = None\n\n    sponsored_bills = legislator.sponsored_bills(\n        limit=6, sort=[('action_dates.first', pymongo.DESCENDING)])\n\n    # Note to self: Another slow query\n    legislator_votes = legislator.votes_6_sorted()\n    has_votes = bool(legislator_votes)\n    return render(\n        request, templatename('legislator'),\n        dict(vote_preview_row_template=templatename('vote_preview_row'),\n             roles=legislator.roles_manager, abbr=abbr,\n             district_id=district_id, metadata=meta, legislator=legislator,\n             sources=legislator['sources'],\n             sponsored_bills=list(sponsored_bills),\n             legislator_votes=list(legislator_votes),\n             has_votes=has_votes,\n             nav_active='legislators'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef legislator_inactive(request, abbr, legislator):\n    '''\n    Context:\n        - vote_preview_row_template\n        - old_roles\n        - abbr\n        - metadata\n        - legislator\n        - sources\n        - sponsored_bills\n        - legislator_votes\n        - has_votes\n        - nav_active\n\n    Templates:\n        - billy/web/public/legislator.html\n        - billy/web/public/vote_preview_row.html\n    '''\n    sponsored_bills = legislator.sponsored_bills(\n        limit=6, sort=[('action_dates.first', pymongo.DESCENDING)])\n\n    legislator_votes = list(legislator.votes_6_sorted())\n    has_votes = bool(legislator_votes)\n\n    return render(\n        request, templatename('legislator'),\n        dict(vote_preview_row_template=templatename('vote_preview_row'),\n             old_roles=legislator.old_roles_manager,\n             abbr=abbr,\n             metadata=legislator.metadata,\n             legislator=legislator,\n             sources=legislator['sources'],\n             sponsored_bills=list(sponsored_bills),\n             legislator_votes=legislator_votes,\n             has_votes=has_votes,\n             nav_active='legislators'))", "response": "Return a new page with the active legislator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_rank(rank):\n    normalized_ranks = {\n        'BA': 'UNDERGRADUATE',\n        'BACHELOR': 'UNDERGRADUATE',\n        'BS': 'UNDERGRADUATE',\n        'BSC': 'UNDERGRADUATE',\n        'JUNIOR': 'JUNIOR',\n        'MAS': 'MASTER',\n        'MASTER': 'MASTER',\n        'MS': 'MASTER',\n        'MSC': 'MASTER',\n        'PD': 'POSTDOC',\n        'PHD': 'PHD',\n        'POSTDOC': 'POSTDOC',\n        'SENIOR': 'SENIOR',\n        'STAFF': 'STAFF',\n        'STUDENT': 'PHD',\n        'UG': 'UNDERGRADUATE',\n        'UNDERGRADUATE': 'UNDERGRADUATE',\n        'VISITING SCIENTIST': 'VISITOR',\n        'VISITOR': 'VISITOR',\n    }\n    if not rank:\n        return None\n    rank = rank.upper().replace('.', '')\n    return normalized_ranks.get(rank, 'OTHER')", "response": "Normalize a rank in order to be schema - compliant."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_recid_from_ref(ref_obj):\n    if not isinstance(ref_obj, dict):\n        return None\n    url = ref_obj.get('$ref', '')\n    return maybe_int(url.split('/')[-1])", "response": "Retrieve recid from jsonref reference object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an absolute URL from a URL relative to the server root.", "response": "def absolute_url(relative_url):\n    \"\"\"Returns an absolute URL from a URL relative to the server root.\n\n    The base URL is taken from the Flask app config if present, otherwise it\n    falls back to ``http://inspirehep.net``.\n    \"\"\"\n    default_server = 'http://inspirehep.net'\n    server = current_app.config.get('SERVER_NAME', default_server)\n    if not re.match('^https?://', server):\n        server = u'http://{}'.format(server)\n    return urllib.parse.urljoin(server, relative_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a file path to a URL pointing to its path on AFS.", "response": "def afs_url(file_path):\n    \"\"\"Convert a file path to a URL pointing to its path on AFS.\n\n    If ``file_path`` doesn't start with ``/opt/cds-invenio/``, and hence is not on\n    AFS, it returns it unchanged.\n\n    The base AFS path is taken from the Flask app config if present, otherwise\n    it falls back to ``/afs/cern.ch/project/inspire/PROD``.\n    \"\"\"\n    default_afs_path = '/afs/cern.ch/project/inspire/PROD'\n    afs_path = current_app.config.get('LEGACY_AFS_PATH', default_afs_path)\n\n    if file_path is None:\n        return\n\n    if file_path.startswith('/opt/cds-invenio/'):\n        file_path = os.path.relpath(file_path, '/opt/cds-invenio/')\n        file_path = os.path.join(afs_path, file_path)\n        return urllib.parse.urljoin('file://', urllib.request.pathname2url(file_path.encode('utf-8')))\n\n    return file_path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef strip_empty_values(obj):\n    if isinstance(obj, dict):\n        new_obj = {}\n        for key, val in obj.items():\n            new_val = strip_empty_values(val)\n            if new_val is not None:\n                new_obj[key] = new_val\n        return new_obj or None\n    elif isinstance(obj, (list, tuple, set)):\n        new_obj = []\n        for val in obj:\n            new_val = strip_empty_values(val)\n            if new_val is not None:\n                new_obj.append(new_val)\n        return type(obj)(new_obj) or None\n    elif obj or obj is False or obj == 0:\n        return obj\n    else:\n        return None", "response": "Recursively strips empty values from the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dedupe_all_lists(obj, exclude_keys=()):\n    squared_dedupe_len = 10\n    if isinstance(obj, dict):\n        new_obj = {}\n        for key, value in obj.items():\n            if key in exclude_keys:\n                new_obj[key] = value\n            else:\n                new_obj[key] = dedupe_all_lists(value)\n        return new_obj\n    elif isinstance(obj, (list, tuple, set)):\n        new_elements = [dedupe_all_lists(v) for v in obj]\n        if len(new_elements) < squared_dedupe_len:\n            new_obj = dedupe_list(new_elements)\n        else:\n            new_obj = dedupe_list_of_dicts(new_elements)\n        return type(obj)(new_obj)\n    else:\n        return obj", "response": "Recursively remove duplucates from all lists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_date_aggressively(date):\n    def _strip_last_part(date):\n        parts = date.split('-')\n        return '-'.join(parts[:-1])\n\n    fake_dates = {'0000', '9999'}\n    if date in fake_dates:\n        return None\n    try:\n        return normalize_date(date)\n    except ValueError:\n        if '-' not in date:\n            raise\n        else:\n            new_date = _strip_last_part(date)\n            return normalize_date_aggressively(new_date)", "response": "Normalize date stripping date parts until a valid date is obtained."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_country_name_to_its_code(country_name, city=''):\n    if country_name:\n        country_name = country_name.upper().replace('.', '').strip()\n\n        if country_to_iso_code.get(country_name):\n            return country_to_iso_code.get(country_name)\n        elif country_name == 'KOREA':\n            if city.upper() in south_korean_cities:\n                return 'KR'\n        else:\n            for c_code, spellings in countries_alternative_spellings.items():\n                for spelling in spellings:\n                    if country_name == spelling:\n                        return c_code\n\n    return None", "response": "Try to match country name with its code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match_us_state(state_string):\n    if state_string:\n        state_string = state_string.upper().replace('.', '').strip()\n        if us_state_to_iso_code.get(state_string):\n            return us_state_to_iso_code.get(state_string)\n        else:\n            for code, state_spellings in us_states_alternative_spellings.items():\n                for spelling in state_spellings:\n                    if state_string == spelling:\n                        return code\n    return None", "response": "Try to match a string with one of the states in the US."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a conference address.", "response": "def parse_conference_address(address_string):\n    \"\"\"Parse a conference address.\n\n    This is a pretty dummy address parser. It only extracts country\n    and state (for US) and should be replaced with something better,\n    like Google Geocoding.\n    \"\"\"\n\n    geo_elements = address_string.split(',')\n    city = geo_elements[0]\n    country_name = geo_elements[-1].upper().replace('.', '').strip()\n    us_state = None\n    state = None\n    country_code = None\n\n    # Try to match the country\n    country_code = match_country_name_to_its_code(country_name, city)\n\n    if country_code == 'US' and len(geo_elements) > 1:\n        us_state = match_us_state(geo_elements[-2].upper().strip()\n                                  .replace('.', ''))\n\n    if not country_code:\n        # Sometimes the country name stores info about U.S. state\n        us_state = match_us_state(country_name)\n\n    if us_state:\n        state = us_state\n        country_code = 'US'\n\n    return {\n        'cities': [\n            city,\n        ],\n        'country_code': country_code,\n        'postal_code': None,\n        'state': state,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_institution_address(address, city, state_province,\n                              country, postal_code, country_code):\n    \"\"\"Parse an institution address.\"\"\"\n    address_list = force_list(address)\n    state_province = match_us_state(state_province) or state_province\n\n    postal_code = force_list(postal_code)\n    country = force_list(country)\n    country_code = match_country_code(country_code)\n\n    if isinstance(postal_code, (tuple, list)):\n        postal_code = ', '.join(postal_code)\n\n    if isinstance(country, (tuple, list)):\n        country = ', '.join(set(country))\n\n    if not country_code and country:\n        country_code = match_country_name_to_its_code(country)\n\n    if not country_code and state_province and state_province in us_state_to_iso_code.values():\n        country_code = 'US'\n\n    return {\n        'cities': force_list(city),\n        'country_code': country_code,\n        'postal_address': address_list,\n        'postal_code': postal_code,\n        'state': state_province,\n    }", "response": "Parse an institution address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ids2marc(self, key, value):\n    def _is_schema_inspire_bai(id_, schema):\n        return schema == 'INSPIRE BAI'\n\n    def _is_schema_inspire_id(id_, schema):\n        return schema == 'INSPIRE ID'\n\n    def _is_schema_spires(id_, schema):\n        return schema == 'SPIRES'\n\n    def _is_schema_linkedin(id, schema):\n        return schema == 'LINKEDIN'\n\n    def _is_schema_twitter(id, schema):\n        return schema == 'TWITTER'\n\n    id_ = value.get('value')\n    schema = value.get('schema')\n\n    if _is_schema_spires(id_, schema):\n        self.setdefault('970', []).append({'a': id_})\n    elif _is_schema_linkedin(id_, schema):\n        self.setdefault('8564', []).append(\n            {\n                'u': u'https://www.linkedin.com/in/{id}'.format(id=quote_url(id_)),\n                'y': 'LINKEDIN',\n            }\n        )\n    elif _is_schema_twitter(id_, schema):\n        self.setdefault('8564', []).append(\n            {\n                'u': u'https://twitter.com/{id}'.format(id=id_),\n                'y': 'TWITTER',\n            }\n        )\n    elif _is_schema_inspire_id(id_, schema):\n        return {\n            'a': id_,\n            '9': 'INSPIRE',\n        }\n    elif _is_schema_inspire_bai(id_, schema):\n        return {\n            'a': id_,\n            '9': 'BAI',\n        }\n    else:\n        return {\n            'a': id_,\n            '9': schema,\n        }", "response": "Populate the 3035 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the name key.", "response": "def name(self, key, value):\n    \"\"\"Populate the ``name`` key.\n\n    Also populates the ``status``, ``birth_date`` and ``death_date`` keys through side effects.\n    \"\"\"\n    def _get_title(value):\n        c_value = force_single_element(value.get('c', ''))\n        if c_value != 'title (e.g. Sir)':\n            return c_value\n\n    def _get_value(value):\n        a_value = force_single_element(value.get('a', ''))\n        q_value = force_single_element(value.get('q', ''))\n        return a_value or normalize_name(q_value)\n\n    if value.get('d'):\n        dates = value['d']\n        try:\n            self['death_date'] = normalize_date(dates)\n        except ValueError:\n            dates = dates.split(' - ')\n            if len(dates) == 1:\n                dates = dates[0].split('-')\n            self['birth_date'] = normalize_date(dates[0])\n            self['death_date'] = normalize_date(dates[1])\n\n    self['status'] = force_single_element(value.get('g', '')).lower()\n\n    return {\n        'numeration': force_single_element(value.get('b', '')),\n        'preferred_name': force_single_element(value.get('q', '')),\n        'title': _get_title(value),\n        'value': _get_value(value),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef name2marc(self, key, value):\n    result = self.get('100', {})\n\n    result['a'] = value.get('value')\n    result['b'] = value.get('numeration')\n    result['c'] = value.get('title')\n    result['q'] = value.get('preferred_name')\n\n    if 'name_variants' in value:\n        self['400'] = [{'a': el} for el in value['name_variants']]\n    if 'native_names' in value:\n        self['880'] = [{'a': el} for el in value['native_names']]\n    if 'previous_names' in value:\n        prev_names = [\n            {'a': u'Formerly {}'.format(prev_name)}\n            for prev_name in value['previous_names']\n        ]\n        self['667'] = prev_names\n\n    return result", "response": "Populate the 100 field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef positions(self, key, value):\n    email_addresses = self.get(\"email_addresses\", [])\n    current = None\n    record = None\n\n    recid_or_status = force_list(value.get('z'))\n    for el in recid_or_status:\n        if el.lower() == 'current':\n            current = True if value.get('a') else None\n        else:\n            record = get_record_ref(maybe_int(el), 'institutions')\n\n    rank = normalize_rank(value.get('r'))\n\n    current_email_addresses = force_list(value.get('m'))\n    non_current_email_addresses = force_list(value.get('o'))\n\n    email_addresses.extend({\n        'value': address,\n        'current': True,\n    } for address in current_email_addresses)\n    email_addresses.extend({\n        'value': address,\n        'current': False,\n    } for address in non_current_email_addresses)\n\n    self['email_addresses'] = email_addresses\n\n    if 'a' not in value:\n        return None\n\n    return {\n        'institution': value['a'],\n        'record': record,\n        'curated_relation': True if record is not None else None,\n        'rank': rank,\n        'start_date': normalize_date(value.get('s')),\n        'end_date': normalize_date(value.get('t')),\n        'current': current,\n    }", "response": "Populate the positions field by side effect."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the 595 MARCXML field.", "response": "def email_addresses2marc(self, key, value):\n    \"\"\"Populate the 595 MARCXML field.\n\n    Also populates the 371 field as a side effect.\n    \"\"\"\n    m_or_o = 'm' if value.get('current') else 'o'\n    element = {\n        m_or_o: value.get('value')\n    }\n\n    if value.get('hidden'):\n        return element\n    else:\n        self.setdefault('371', []).append(element)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the email_addresses field using the 595 MARCXML field.", "response": "def email_addresses595(self, key, value):\n    \"\"\"Populates the ``email_addresses`` field using the 595 MARCXML field.\n\n    Also populates ``_private_notes`` as a side effect.\n    \"\"\"\n    emails = self.get('email_addresses', [])\n\n    if value.get('o'):\n        emails.append({\n            'value': value.get('o'),\n            'current': False,\n            'hidden': True,\n        })\n\n    if value.get('m'):\n        emails.append({\n            'value': value.get('m'),\n            'current': True,\n            'hidden': True,\n        })\n\n    notes = self.get('_private_notes', [])\n    new_note = (\n        {\n            'source': value.get('9'),\n            'value': _private_note,\n        } for _private_note in force_list(value.get('a'))\n    )\n    notes.extend(new_note)\n    self['_private_notes'] = notes\n\n    return emails"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef arxiv_categories(self, key, value):\n    def _is_arxiv(category):\n        return category in valid_arxiv_categories()\n\n    def _is_inspire(category):\n        schema = load_schema('elements/inspire_field')\n        valid_inspire_categories = schema['properties']['term']['enum']\n\n        return category in valid_inspire_categories\n\n    def _normalize(a_value):\n        for category in valid_arxiv_categories():\n            if a_value.lower() == category.lower():\n                return normalize_arxiv_category(category)\n\n        schema = load_schema('elements/inspire_field')\n        valid_inspire_categories = schema['properties']['term']['enum']\n\n        for category in valid_inspire_categories:\n            if a_value.lower() == category.lower():\n                return category\n\n        field_codes_to_inspire_categories = {\n            'a': 'Astrophysics',\n            'b': 'Accelerators',\n            'c': 'Computing',\n            'e': 'Experiment-HEP',\n            'g': 'Gravitation and Cosmology',\n            'i': 'Instrumentation',\n            'l': 'Lattice',\n            'm': 'Math and Math Physics',\n            'n': 'Theory-Nucl',\n            'o': 'Other',\n            'p': 'Phenomenology-HEP',\n            'q': 'General Physics',\n            't': 'Theory-HEP',\n            'x': 'Experiment-Nucl',\n        }\n\n        return field_codes_to_inspire_categories.get(a_value.lower())\n\n    arxiv_categories = self.get('arxiv_categories', [])\n    inspire_categories = self.get('inspire_categories', [])\n\n    for value in force_list(value):\n        for a_value in force_list(value.get('a')):\n            normalized_a_value = _normalize(a_value)\n\n            if _is_arxiv(normalized_a_value):\n                arxiv_categories.append(normalized_a_value)\n            elif _is_inspire(normalized_a_value):\n                inspire_categories.append({'term': normalized_a_value})\n\n    self['inspire_categories'] = inspire_categories\n    return arxiv_categories", "response": "Populate the arxiv_categories key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the 100__d MARC field.", "response": "def birth_and_death_date2marc(self, key, value):\n    \"\"\"Populate the ``100__d`` MARC field, which includes the birth and the death date.\n\n    By not using the decorator ```for_each_value```, the values of the fields\n    ```birth_date``` and ```death_date``` are both added to ```values``` as a list.\n    \"\"\"\n    name_field = self.get('100', {})\n\n    if 'd' in name_field:\n        if int(name_field['d'].split('-')[0]) > int(value.split('-')[0]):\n            dates_field = ' - '.join([value, name_field['d']])\n        else:\n            dates_field = ' - '.join([name_field['d'], value])\n    else:\n        dates_field = value\n\n    name_field['d'] = dates_field\n\n    return name_field"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the url key.", "response": "def urls(self, key, value):\n    \"\"\"Populate the ``url`` key.\n\n    Also populates the ``ids`` key through side effects.\n    \"\"\"\n    description = force_single_element(value.get('y'))\n    url = value.get('u')\n\n    linkedin_match = LINKEDIN_URL.match(url)\n    twitter_match = TWITTER_URL.match(url)\n    wikipedia_match = WIKIPEDIA_URL.match(url)\n    if linkedin_match:\n        self.setdefault('ids', []).append(\n            {\n                'schema': 'LINKEDIN',\n                'value': unquote_url(linkedin_match.group('page')),\n            }\n        )\n    elif twitter_match:\n        self.setdefault('ids', []).append(\n            {\n                'schema': 'TWITTER',\n                'value': twitter_match.group('handle'),\n            }\n        )\n    elif wikipedia_match:\n        lang = wikipedia_match.group('lang')\n        page = unquote_url(wikipedia_match.group('page'))\n        if lang != 'en':\n            page = ':'.join([lang, page])\n        self.setdefault('ids', []).append(\n            {\n                'schema': 'WIKIPEDIA',\n                'value': page,\n            }\n        )\n    else:\n        return {\n            'description': description,\n            'value': url,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_record(self, key, value):\n    new_record = self.get('new_record', {})\n    ids = self.get('ids', [])\n\n    for value in force_list(value):\n        for id_ in force_list(value.get('a')):\n            ids.append({\n                'schema': 'SPIRES',\n                'value': id_,\n            })\n\n        new_recid = force_single_element(value.get('d', ''))\n        if new_recid:\n            new_record = get_record_ref(new_recid, 'authors')\n\n    self['ids'] = ids\n    return new_record", "response": "Populate the new_record key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the deleted key.", "response": "def deleted(self, key, value):\n    \"\"\"Populate the ``deleted`` key.\n\n    Also populates the ``stub`` key through side effects.\n    \"\"\"\n    def _is_deleted(value):\n        return force_single_element(value.get('c', '')).upper() == 'DELETED'\n\n    def _is_stub(value):\n        return not (force_single_element(value.get('a', '')).upper() == 'USEFUL')\n\n    deleted = self.get('deleted')\n    stub = self.get('stub')\n\n    for value in force_list(value):\n        deleted = not deleted and _is_deleted(value)\n        stub = not stub and _is_stub(value)\n\n    self['stub'] = stub\n    return deleted"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authors2marc(self, key, value):\n    value = force_list(value)\n\n    def _get_ids(value):\n        ids = {\n            'i': [],\n            'j': [],\n        }\n        if value.get('ids'):\n            for _id in value.get('ids'):\n                if _id.get('schema') == 'INSPIRE ID':\n                    ids['i'].append(_id.get('value'))\n                elif _id.get('schema') == 'ORCID':\n                    ids['j'].append('ORCID:' + _id.get('value'))\n                elif _id.get('schema') == 'JACOW':\n                    ids['j'].append(_id.get('value'))\n                elif _id.get('schema') == 'CERN':\n                    ids['j'].append('CCID-' + _id.get('value')[5:])\n        return ids\n\n    def _get_affiliations(value):\n        return [\n            aff.get('value') for aff in value.get('affiliations', [])\n        ]\n\n    def _get_affiliations_identifiers(value):\n        return [\n            u'{}:{}'.format(aff.get('schema'), aff.get('value')) for aff in value.get('affiliations_identifiers', [])\n        ]\n\n    def _get_inspire_roles(value):\n        values = force_list(value.get('inspire_roles'))\n        return ['ed.' for role in values if role == 'editor']\n\n    def _get_raw_affiliations(value):\n        return [\n            aff.get('value') for aff in value.get('raw_affiliations', [])\n        ]\n\n    def get_value_100_700(value):\n        ids = _get_ids(value)\n        return {\n            'a': value.get('full_name'),\n            'e': _get_inspire_roles(value),\n            'q': value.get('alternative_names'),\n            'i': ids.get('i'),\n            'j': ids.get('j'),\n            'm': value.get('emails'),\n            't': _get_affiliations_identifiers(value),\n            'u': _get_affiliations(value),\n            'v': _get_raw_affiliations(value),\n        }\n\n    def get_value_701(value):\n        ids = _get_ids(value)\n        return {\n            'a': value.get('full_name'),\n            'q': value.get('alternative_names'),\n            'i': ids.get('i'),\n            'j': ids.get('j'),\n            'u': _get_affiliations(value),\n            'v': _get_raw_affiliations(value),\n        }\n\n    if len(value) > 1:\n        self[\"700\"] = []\n        self[\"701\"] = []\n\n    for author in value[1:]:\n        is_supervisor = 'supervisor' in author.get('inspire_roles', [])\n        if is_supervisor:\n            self[\"701\"].append(get_value_701(author))\n        else:\n            self[\"700\"].append(get_value_100_700(author))\n    return get_value_100_700(value[0])", "response": "Populate the 100 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isbns(self, key, value):\n    def _get_medium(value):\n        def _normalize(medium):\n            schema = load_schema('hep')\n            valid_media = schema['properties']['isbns']['items']['properties']['medium']['enum']\n\n            medium = medium.lower().replace('-', '').replace(' ', '')\n            if medium in valid_media:\n                return medium\n            elif medium == 'ebook':\n                return 'online'\n            elif medium == 'paperback':\n                return 'softcover'\n\n            return ''\n\n        medium = force_single_element(value.get('b', ''))\n        normalized_medium = _normalize(medium)\n\n        return normalized_medium\n\n    def _get_isbn(value):\n        a_value = force_single_element(value.get('a', ''))\n        normalized_a_value = a_value.replace('.', '')\n        if normalized_a_value:\n            return normalize_isbn(normalized_a_value)\n\n    return {\n        'medium': _get_medium(value),\n        'value': _get_isbn(value),\n    }", "response": "Populate the isbns key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dois(self, key, value):\n    def _get_first_non_curator_source(sources):\n        sources_without_curator = [el for el in sources if el.upper() != 'CURATOR']\n        return force_single_element(sources_without_curator)\n\n    def _get_material(value):\n        MATERIAL_MAP = {\n            'ebook': 'publication',\n        }\n\n        q_value = force_single_element(value.get('q', ''))\n        normalized_q_value = q_value.lower()\n\n        return MATERIAL_MAP.get(normalized_q_value, normalized_q_value)\n\n    def _is_doi(id_, type_):\n        return (not type_ or type_.upper() == 'DOI') and is_doi(id_)\n\n    def _is_handle(id_, type_):\n        return (not type_ or type_.upper() == 'HDL') and is_handle(id_)\n\n    dois = self.get('dois', [])\n    persistent_identifiers = self.get('persistent_identifiers', [])\n\n    values = force_list(value)\n    for value in values:\n        id_ = force_single_element(value.get('a', ''))\n        material = _get_material(value)\n        schema = force_single_element(value.get('2', ''))\n\n        sources = force_list(value.get('9'))\n        source = _get_first_non_curator_source(sources)\n\n        if _is_doi(id_, schema):\n            dois.append({\n                'material': material,\n                'source': source,\n                'value': normalize_doi(id_),\n            })\n        else:\n            schema = 'HDL' if _is_handle(id_, schema) else schema\n            persistent_identifiers.append({\n                'material': material,\n                'schema': schema,\n                'source': source,\n                'value': id_,\n            })\n\n    self['persistent_identifiers'] = persistent_identifiers\n    return dois", "response": "Populate the dois key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the 0247 MARC field.", "response": "def dois2marc(self, key, value):\n    \"\"\"Populate the ``0247`` MARC field.\"\"\"\n    return {\n        '2': 'DOI',\n        '9': value.get('source'),\n        'a': value.get('value'),\n        'q': value.get('material'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating the 0247 MARC field.", "response": "def persistent_identifiers2marc(self, key, value):\n    \"\"\"Populate the ``0247`` MARC field.\"\"\"\n    return {\n        '2': value.get('schema'),\n        '9': value.get('source'),\n        'a': value.get('value'),\n        'q': value.get('material'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef texkeys(self, key, value):\n    def _is_oai(id_, schema):\n        return id_.startswith('oai:')\n\n    def _is_desy(id_, schema):\n        return id_ and schema in ('DESY',)\n\n    def _is_texkey(id_, schema):\n        return id_ and schema in ('INSPIRETeX', 'SPIRESTeX')\n\n    texkeys = self.get('texkeys', [])\n    external_system_identifiers = self.get('external_system_identifiers', [])\n    _desy_bookkeeping = self.get('_desy_bookkeeping', [])\n\n    values = force_list(value)\n    for value in values:\n        ids = force_list(value.get('a', ''))\n        other_ids = force_list(value.get('z', ''))\n        schema = force_single_element(value.get('9', ''))\n\n        for id_ in ids:\n            id_ = id_.strip()\n            if not id_:\n                continue\n\n            if _is_texkey(id_, schema):\n                texkeys.insert(0, id_)\n            elif _is_oai(id_, schema):\n                continue  # XXX: ignored.\n            elif _is_desy(id_, schema):\n                _desy_bookkeeping.append({'identifier': id_})\n            else:\n                external_system_identifiers.insert(0, {\n                    'schema': schema,\n                    'value': id_,\n                })\n\n        for id_ in other_ids:\n            id_ = id_.strip()\n            if not id_:\n                continue\n\n            if _is_texkey(id_, schema):\n                texkeys.append(id_)\n            elif _is_oai(id_, schema):\n                continue  # XXX: ignored.\n            elif _is_desy(id_, schema):\n                _desy_bookkeeping.append({'identifier': id_})\n            else:\n                external_system_identifiers.append({\n                    'schema': schema,\n                    'value': id_,\n                })\n\n    self['external_system_identifiers'] = external_system_identifiers\n    self['_desy_bookkeeping'] = _desy_bookkeeping\n    return texkeys", "response": "Populate the texkeys key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef texkeys2marc(self, key, value):\n    result = []\n\n    values = force_list(value)\n    if values:\n        value = values[0]\n        result.append({\n            '9': 'INSPIRETeX',\n            'a': value,\n        })\n\n        for value in values[1:]:\n            result.append({\n                '9': 'INSPIRETeX',\n                'z': value,\n            })\n\n    return result", "response": "Populate the 3035 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef external_system_identifiers2marc(self, key, value):\n    def _is_scheme_cernkey(id_, schema):\n        return schema == 'CERNKEY'\n\n    def _is_scheme_spires(id_, schema):\n        return schema == 'SPIRES'\n\n    result_035 = self.get('035', [])\n    id_dict = self.get('id_dict', defaultdict(list))\n    result_970 = self.get('970', [])\n\n    values = force_list(value)\n    for value in values:\n        id_ = value.get('value')\n        schema = value.get('schema')\n\n        if _is_scheme_spires(id_, schema):\n            result_970.append({\n                'a': id_,\n            })\n        elif _is_scheme_cernkey(id_, schema):\n            result_035.append({\n                '9': 'CERNKEY',\n                'z': id_,\n            })\n        else:\n            id_dict[schema].append(id_)\n\n    self['970'] = result_970\n    self['id_dict'] = id_dict\n    return result_035", "response": "Populate the 970 MARC field based on external system identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arxiv_eprints(self, key, value):\n    def _get_clean_arxiv_eprint(id_):\n        return id_.split(':')[-1]\n\n    def _is_arxiv_eprint(id_, source):\n        return source.lower() == 'arxiv'\n\n    def _is_hidden_report_number(other_id, source):\n        return other_id\n\n    def _get_clean_source(source):\n        if source == 'arXiv:reportnumber':\n            return 'arXiv'\n        return source\n\n    arxiv_eprints = self.get('arxiv_eprints', [])\n    report_numbers = self.get('report_numbers', [])\n\n    values = force_list(value)\n    for value in values:\n        id_ = force_single_element(value.get('a', ''))\n        other_id = force_single_element(value.get('z', ''))\n        categories = [normalize_arxiv_category(category) for category\n                      in force_list(value.get('c'))]\n        source = force_single_element(value.get('9', ''))\n\n        if _is_arxiv_eprint(id_, source):\n            arxiv_eprints.append({\n                'categories': categories,\n                'value': _get_clean_arxiv_eprint(id_),\n            })\n        elif _is_hidden_report_number(other_id, source):\n            report_numbers.append({\n                'hidden': True,\n                'source': _get_clean_source(source),\n                'value': other_id,\n            })\n        else:\n            report_numbers.append({\n                'source': _get_clean_source(source),\n                'value': id_,\n            })\n\n    self['report_numbers'] = report_numbers\n    return arxiv_eprints", "response": "Populate the arxiv_eprints key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the 3037 MARC field.", "response": "def arxiv_eprints2marc(self, key, values):\n    \"\"\"Populate the ``037`` MARC field.\n\n    Also populates the ``035`` and the ``65017`` MARC fields through side effects.\n    \"\"\"\n    result_037 = self.get('037', [])\n    result_035 = self.get('035', [])\n    result_65017 = self.get('65017', [])\n\n    for value in values:\n        arxiv_id = value.get('value')\n        arxiv_id = 'arXiv:' + arxiv_id if is_arxiv_post_2007(arxiv_id) else arxiv_id\n        result_037.append({\n            '9': 'arXiv',\n            'a': arxiv_id,\n            'c': force_single_element(value.get('categories')),\n        })\n\n        result_035.append({\n            '9': 'arXiv',\n            'a': 'oai:arXiv.org:' + value.get('value'),\n        })\n\n        categories = force_list(value.get('categories'))\n        for category in categories:\n            result_65017.append({\n                '2': 'arXiv',\n                'a': category,\n            })\n\n    self['65017'] = result_65017\n    self['035'] = result_035\n    return result_037"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the 3037 MARC field.", "response": "def report_numbers2marc(self, key, value):\n    \"\"\"Populate the ``037`` MARC field.\"\"\"\n    def _get_mangled_source(source):\n        if source == 'arXiv':\n            return 'arXiv:reportnumber'\n        return source\n\n    source = _get_mangled_source(value.get('source'))\n\n    if value.get('hidden'):\n        return {\n            '9': source,\n            'z': value.get('value'),\n        }\n\n    return {\n        '9': source,\n        'a': value.get('value'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef languages(self, key, value):\n    languages = self.get('languages', [])\n\n    values = force_list(value.get('a'))\n    for value in values:\n        for language in RE_LANGUAGE.split(value):\n            try:\n                name = language.strip().capitalize()\n                languages.append(pycountry.languages.get(name=name).alpha_2)\n            except KeyError:\n                pass\n\n    return languages", "response": "Populate the languages key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef languages2marc(self, key, value):\n    return {'a': pycountry.languages.get(alpha_2=value).name.lower()}", "response": "Populate the 041 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record_affiliations(self, key, value):\n    record = get_record_ref(value.get('z'), 'institutions')\n\n    return {\n        'curated_relation': record is not None,\n        'record': record,\n        'value': value.get('a'),\n    }", "response": "Populate the record_affiliations key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the document_type key.", "response": "def document_type(self, key, value):\n    \"\"\"Populate the ``document_type`` key.\n\n    Also populates the ``_collections``, ``citeable``, ``core``, ``deleted``,\n    ``refereed``, ``publication_type``, and ``withdrawn`` keys through side\n    effects.\n    \"\"\"\n    schema = load_schema('hep')\n    publication_type_schema = schema['properties']['publication_type']\n    valid_publication_types = publication_type_schema['items']['enum']\n\n    document_type = self.get('document_type', [])\n    publication_type = self.get('publication_type', [])\n\n    a_values = force_list(value.get('a'))\n    for a_value in a_values:\n        normalized_a_value = a_value.strip().lower()\n\n        if normalized_a_value == 'arxiv':\n            continue  # XXX: ignored.\n        elif normalized_a_value == 'citeable':\n            self['citeable'] = True\n        elif normalized_a_value == 'core':\n            self['core'] = True\n        elif normalized_a_value == 'noncore':\n            self['core'] = False\n        elif normalized_a_value == 'published':\n            self['refereed'] = True\n        elif normalized_a_value == 'withdrawn':\n            self['withdrawn'] = True\n        elif normalized_a_value == 'deleted':\n            self['deleted'] = True\n        elif normalized_a_value in COLLECTIONS_MAP:\n            self.setdefault('_collections', []).append(COLLECTIONS_MAP[normalized_a_value])\n        elif normalized_a_value in DOCUMENT_TYPE_MAP:\n            document_type.append(DOCUMENT_TYPE_MAP[normalized_a_value])\n        elif normalized_a_value in valid_publication_types:\n            publication_type.append(normalized_a_value)\n\n    c_value = force_single_element(value.get('c', ''))\n    normalized_c_value = c_value.strip().lower()\n\n    if normalized_c_value == 'deleted':\n        self['deleted'] = True\n\n    self['publication_type'] = publication_type\n    return document_type"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef document_type2marc(self, key, value):\n    if value in DOCUMENT_TYPE_REVERSE_MAP and DOCUMENT_TYPE_REVERSE_MAP[value]:\n        return {'a': DOCUMENT_TYPE_REVERSE_MAP[value]}", "response": "Populate the 980 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the references key.", "response": "def references(self, key, value):\n    \"\"\"Populate the ``references`` key.\"\"\"\n    def _has_curator_flag(value):\n        normalized_nine_values = [el.upper() for el in force_list(value.get('9'))]\n        return 'CURATOR' in normalized_nine_values\n\n    def _is_curated(value):\n        return force_single_element(value.get('z')) == '1' and _has_curator_flag(value)\n\n    def _set_record(el):\n        recid = maybe_int(el)\n        record = get_record_ref(recid, 'literature')\n        rb.set_record(record)\n\n    rb = ReferenceBuilder()\n    mapping = [\n        ('0', _set_record),\n        ('a', rb.add_uid),\n        ('b', rb.add_uid),\n        ('c', rb.add_collaboration),\n        ('e', partial(rb.add_author, role='ed.')),\n        ('h', rb.add_refextract_authors_str),\n        ('i', rb.add_uid),\n        ('k', rb.set_texkey),\n        ('m', rb.add_misc),\n        ('o', rb.set_label),\n        ('p', rb.set_publisher),\n        ('q', rb.add_parent_title),\n        ('r', rb.add_report_number),\n        ('s', rb.set_pubnote),\n        ('t', rb.add_title),\n        ('x', rb.add_raw_reference),\n        ('y', rb.set_year),\n    ]\n\n    for field, method in mapping:\n        for el in force_list(value.get(field)):\n            if el:\n                method(el)\n\n    for el in dedupe_list(force_list(value.get('u'))):\n        if el:\n            rb.add_url(el)\n\n    if _is_curated(value):\n        rb.curate()\n\n    if _has_curator_flag(value):\n        rb.obj['legacy_curated'] = True\n\n    return rb.obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the 999C5 MARC field.", "response": "def references2marc(self, key, value):\n    \"\"\"Populate the ``999C5`` MARC field.\"\"\"\n    reference = value.get('reference', {})\n\n    pids = force_list(reference.get('persistent_identifiers'))\n    a_values = ['doi:' + el for el in force_list(reference.get('dois'))]\n    a_values.extend(['hdl:' + el['value'] for el in pids if el.get('schema') == 'HDL'])\n    a_values.extend(['urn:' + el['value'] for el in pids if el.get('schema') == 'URN'])\n\n    external_ids = force_list(reference.get('external_system_identifiers'))\n    u_values = force_list(get_value(reference, 'urls.value'))\n    u_values.extend(CDS_RECORD_FORMAT.format(el['value']) for el in external_ids if el.get('schema') == 'CDS')\n    u_values.extend(ADS_RECORD_FORMAT.format(el['value']) for el in external_ids if el.get('schema') == 'ADS')\n\n    authors = force_list(reference.get('authors'))\n    e_values = [el['full_name'] for el in authors if el.get('inspire_role') == 'editor']\n    h_values = [el['full_name'] for el in authors if el.get('inspire_role') != 'editor']\n\n    r_values = force_list(reference.get('report_numbers'))\n    if reference.get('arxiv_eprint'):\n        arxiv_eprint = reference['arxiv_eprint']\n        r_values.append('arXiv:' + arxiv_eprint if is_arxiv_post_2007(arxiv_eprint) else arxiv_eprint)\n\n    if reference.get('publication_info'):\n        reference['publication_info'] = convert_new_publication_info_to_old([reference['publication_info']])[0]\n    journal_title = get_value(reference, 'publication_info.journal_title')\n    journal_volume = get_value(reference, 'publication_info.journal_volume')\n    page_start = get_value(reference, 'publication_info.page_start')\n    page_end = get_value(reference, 'publication_info.page_end')\n    artid = get_value(reference, 'publication_info.artid')\n    s_value = build_pubnote(journal_title, journal_volume, page_start, page_end, artid)\n\n    m_value = ' / '.join(force_list(reference.get('misc')))\n\n    return {\n        '0': get_recid_from_ref(value.get('record')),\n        '9': 'CURATOR' if value.get('legacy_curated') else None,\n        'a': a_values,\n        'b': get_value(reference, 'publication_info.cnum'),\n        'c': reference.get('collaborations'),\n        'e': e_values,\n        'h': h_values,\n        'i': reference.get('isbn'),\n        'k': reference.get('texkey'),\n        'm': m_value,\n        'o': reference.get('label'),\n        'p': get_value(reference, 'imprint.publisher'),\n        'q': get_value(reference, 'publication_info.parent_title'),\n        'r': r_values,\n        's': s_value,\n        't': get_value(reference, 'title.title'),\n        'u': u_values,\n        'x': get_value(value, 'raw_refs.value'),\n        'y': get_value(reference, 'publication_info.year'),\n        'z': 1 if value.get('curated_relation') else 0,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef documents(self, key, value):\n    def _is_hidden(value):\n        return 'HIDDEN' in [val.upper() for val in value] or None\n\n    def _is_figure(value):\n        figures_extensions = ['.png']\n        return value.get('f') in figures_extensions\n\n    def _is_fulltext(value):\n        return value.get('d', '').lower() == 'fulltext' or None\n\n    def _get_index_and_caption(value):\n        match = re.compile(r'(^\\d{5})?\\s*(.*)').match(value)\n        if match:\n            return match.group(1), match.group(2)\n\n    def _get_key(value):\n        fname = value.get('n', 'document')\n        extension = value.get('f', '')\n\n        if fname.endswith(extension):\n            return fname\n        return fname + extension\n\n    def _get_source(value):\n        source = value.get('t', '')\n        if source in ('INSPIRE-PUBLIC', 'Main'):\n            source = None\n        elif source.lower() == 'arxiv':\n            return 'arxiv'\n\n        return source\n\n    figures = self.get('figures', [])\n    is_context = value.get('f', '').endswith('context')\n\n    if is_context:\n        return\n\n    if _is_figure(value):\n        index, caption = _get_index_and_caption(value.get('d', ''))\n        figures.append({\n            'key': _get_key(value),\n            'caption': caption,\n            'url': afs_url(value.get('a')),\n            'order': index,\n            'source': 'arxiv',  # XXX: we don't have any other figures on legacy\n        })\n        self['figures'] = figures\n    else:\n        return {\n            'description': value.get('d') if not _is_fulltext(value) else None,\n            'key': _get_key(value),\n            'fulltext': _is_fulltext(value),\n            'hidden': _is_hidden(force_list(value.get('o'))),\n            'url': afs_url(value.get('a')),\n            'source': _get_source(value),\n        }", "response": "Populate the documents key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef marcxml2record(marcxml):\n    marcjson = create_record(marcxml, keep_singletons=False)\n    collections = _get_collections(marcjson)\n\n    if 'conferences' in collections:\n        return conferences.do(marcjson)\n    elif 'data' in collections:\n        return data.do(marcjson)\n    elif 'experiment' in collections:\n        return experiments.do(marcjson)\n    elif 'hepnames' in collections:\n        return hepnames.do(marcjson)\n    elif 'institution' in collections:\n        return institutions.do(marcjson)\n    elif 'job' in collections or 'jobhidden' in collections:\n        return jobs.do(marcjson)\n    elif 'journals' in collections or 'journalsnew' in collections:\n        return journals.do(marcjson)\n    return hep.do(marcjson)", "response": "Convert a MARCXML string to a JSON record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a JSON record to a MARCXML string.", "response": "def record2marcxml(record):\n    \"\"\"Convert a JSON record to a MARCXML string.\n\n    Deduces which set of rules to use by parsing the ``$schema`` key, as\n    it unequivocally determines which kind of record we have.\n\n    Args:\n        record(dict): a JSON record.\n\n    Returns:\n        str: a MARCXML string converted from the record.\n\n    \"\"\"\n    schema_name = _get_schema_name(record)\n\n    if schema_name == 'hep':\n        marcjson = hep2marc.do(record)\n    elif schema_name == 'authors':\n        marcjson = hepnames2marc.do(record)\n    else:\n        raise NotImplementedError(u'JSON -> MARC rules missing for \"{}\"'.format(schema_name))\n\n    record = RECORD()\n\n    for key, values in sorted(iteritems(marcjson)):\n        tag, ind1, ind2 = _parse_key(key)\n        if _is_controlfield(tag, ind1, ind2):\n            value = force_single_element(values)\n            if not isinstance(value, text_type):\n                value = text_type(value)\n            record.append(CONTROLFIELD(_strip_invalid_chars_for_xml(value), {'tag': tag}))\n        else:\n            for value in force_list(values):\n                datafield = DATAFIELD({'tag': tag, 'ind1': ind1, 'ind2': ind2})\n                for code, els in sorted(iteritems(value)):\n                    for el in force_list(els):\n                        if not isinstance(el, text_type):\n                            el = text_type(el)\n                        datafield.append(SUBFIELD(_strip_invalid_chars_for_xml(el), {'code': code}))\n                record.append(datafield)\n\n    return tostring(record, encoding='utf8', pretty_print=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npopulate the number_of_pages key.", "response": "def number_of_pages(self, key, value):\n    \"\"\"Populate the ``number_of_pages`` key.\"\"\"\n    result = maybe_int(force_single_element(value.get('a', '')))\n    if result and result > 0:\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef secondary_report_numbers(self, key, value):\n    preliminary_results_prefixes = ['ATLAS-CONF-', 'CMS-PAS-', 'CMS-DP-', 'LHCB-CONF-']\n    note_prefixes = ['ALICE-INT-', 'ATL-', 'ATLAS-CONF-', 'CMS-DP-', 'CMS-PAS-', 'LHCB-CONF-', 'LHCB-PUB-']\n\n    result_037 = self.get('037__', [])\n    result_500 = self.get('500__', [])\n    result_595 = self.get('595__', [])\n    result_980 = self.get('980__', [])\n\n    report = force_single_element(value.get('a', ''))\n    hidden_report = force_single_element(value.get('9') or value.get('z', ''))\n    source = 'CDS' if not is_arxiv(report) else 'arXiv'\n\n    if any(report.upper().startswith(prefix) for prefix in note_prefixes):\n        result_980.append({'a': 'NOTE'})\n\n    if any(report.upper().startswith(prefix) for prefix in preliminary_results_prefixes):\n        result_500.append({'9': 'CDS', 'a': 'Preliminary results'})\n\n    is_barcode = hidden_report.startswith('P0') or hidden_report.startswith('CM-P0')\n    if not report.startswith('SIS-') and not is_barcode:\n        result_037.append({\n            '9': source,\n            'a': report,\n            'c': value.get('c'),\n            'z': hidden_report if source == 'CDS' else None,\n        })\n\n    self['500__'] = result_500\n    self['595__'] = result_595\n    self['980__'] = result_980\n    return result_037", "response": "Populate the 216 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the 701 MARC field.", "response": "def nonfirst_authors(self, key, value):\n    \"\"\"Populate ``700`` MARC field.\n\n    Also populates the ``701`` MARC field through side-effects.\n    \"\"\"\n    field_700 = self.get('700__', [])\n    field_701 = self.get('701__', [])\n\n    is_supervisor = any(el.lower().startswith('dir') for el in force_list(value.get('e', '')))\n    if is_supervisor:\n        field_701.append(_converted_author(value))\n    else:\n        field_700.append(_converted_author(value))\n\n    self['701__'] = field_701\n    return field_700"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef urls(self, key, value):\n    def _is_preprint(value):\n        return value.get('y', '').lower() == 'preprint'\n\n    def _is_fulltext(value):\n        return value['u'].endswith('.pdf') and value['u'].startswith('http://cds.cern.ch')\n\n    def _is_local_copy(value):\n        return 'local copy' in value.get('y', '')\n\n    def _is_ignored_domain(value):\n        ignored_domains = ['http://cdsweb.cern.ch', 'http://cms.cern.ch',\n                           'http://cmsdoc.cern.ch', 'http://documents.cern.ch',\n                           'http://preprints.cern.ch', 'http://cds.cern.ch',\n                           'http://arxiv.org']\n        return any(value['u'].startswith(domain) for domain in ignored_domains)\n\n    field_8564 = self.get('8564_', [])\n    field_FFT = self.get('FFT__', [])\n\n    if 'u' not in value:\n        return field_8564\n\n    url = escape_url(value['u'])\n\n    if _is_fulltext(value) and not _is_preprint(value):\n        if _is_local_copy(value):\n            description = value.get('y', '').replace('local copy', 'on CERN Document Server')\n            field_8564.append({\n                'u': url,\n                'y': description,\n            })\n        else:\n            _, file_name = os.path.split(urllib.parse.urlparse(value['u']).path)\n            _, extension = os.path.splitext(file_name)\n            field_FFT.append({\n                't': 'CDS',\n                'a': url,\n                'd': value.get('y', ''),\n                'n': file_name,\n                'f': extension,\n            })\n    elif not _is_ignored_domain(value):\n        field_8564.append({\n            'u': url,\n            'y': value.get('y'),\n        })\n\n    self['FFT__'] = field_FFT\n    return field_8564", "response": "Populate the 8564 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the titles key.", "response": "def titles(self, key, value):\n    \"\"\"Populate the ``titles`` key.\"\"\"\n    if not key.startswith('245'):\n        return {\n            'source': value.get('9'),\n            'subtitle': value.get('b'),\n            'title': value.get('a'),\n        }\n\n    self.setdefault('titles', []).insert(0, {\n        'source': value.get('9'),\n        'subtitle': value.get('b'),\n        'title': value.get('a'),\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef title_translations(self, key, value):\n    return {\n        'language': langdetect.detect(value.get('a')),\n        'source': value.get('9'),\n        'subtitle': value.get('b'),\n        'title': value.get('a'),\n    }", "response": "Populate the title_translations key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the 246 MARC field from a list of titles.", "response": "def titles2marc(self, key, values):\n    \"\"\"Populate the ``246`` MARC field.\n\n    Also populates the ``245`` MARC field through side effects.\n    \"\"\"\n    first, rest = values[0], values[1:]\n\n    self.setdefault('245', []).append({\n        'a': first.get('title'),\n        'b': first.get('subtitle'),\n        '9': first.get('source'),\n    })\n\n    return [\n        {\n            'a': value.get('title'),\n            'b': value.get('subtitle'),\n            '9': value.get('source'),\n        } for value in rest\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the 242 MARC field.", "response": "def title_translations2marc(self, key, value):\n    \"\"\"Populate the ``242`` MARC field.\"\"\"\n    return {\n        'a': value.get('title'),\n        'b': value.get('subtitle'),\n        '9': value.get('source'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the imprints key.", "response": "def imprints(self, key, value):\n    \"\"\"Populate the ``imprints`` key.\"\"\"\n    return {\n        'place': value.get('a'),\n        'publisher': value.get('b'),\n        'date': normalize_date_aggressively(value.get('c')),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the 260 MARC field.", "response": "def imprints2marc(self, key, value):\n    \"\"\"Populate the ``260`` MARC field.\"\"\"\n    return {\n        'a': value.get('place'),\n        'b': value.get('publisher'),\n        'c': value.get('date'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef public_notes(self, key, value):\n    def _means_not_curated(public_note):\n        return public_note in [\n            '*Brief entry*',\n            '* Brief entry *',\n            '*Temporary entry*',\n            '* Temporary entry *',\n            '*Temporary record*',\n            '* Temporary record *',\n        ]\n\n    public_notes = self.get('public_notes', [])\n    thesis_info = self.get('thesis_info', {})\n\n    source = force_single_element(value.get('9', ''))\n    for value in force_list(value):\n        for public_note in force_list(value.get('a')):\n            match = IS_DEFENSE_DATE.match(public_note)\n            if match:\n                try:\n                    thesis_info['defense_date'] = normalize_date(match.group('defense_date'))\n                except ValueError:\n                    public_notes.append({\n                        'source': source,\n                        'value': public_note,\n                    })\n            elif _means_not_curated(public_note):\n                self['curated'] = False\n            else:\n                public_notes.append({\n                    'source': source,\n                    'value': public_note,\n                })\n\n    self['thesis_info'] = thesis_info\n    return public_notes", "response": "Populate the public_notes key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the thesis_info key.", "response": "def thesis_info(self, key, value):\n    \"\"\"Populate the ``thesis_info`` key.\"\"\"\n    def _get_degree_type(value):\n        DEGREE_TYPES_MAP = {\n            'RAPPORT DE STAGE': 'other',\n            'INTERNSHIP REPORT': 'other',\n            'DIPLOMA': 'diploma',\n            'BACHELOR': 'bachelor',\n            'LAUREA': 'laurea',\n            'MASTER': 'master',\n            'THESIS': 'other',\n            'PHD': 'phd',\n            'PDF': 'phd',\n            'PH.D. THESIS': 'phd',\n            'HABILITATION': 'habilitation',\n        }\n\n        b_value = force_single_element(value.get('b', ''))\n        if b_value:\n            return DEGREE_TYPES_MAP.get(b_value.upper(), 'other')\n\n    def _get_institutions(value):\n        c_values = force_list(value.get('c'))\n        z_values = force_list(value.get('z'))\n\n        # XXX: we zip only when they have the same length, otherwise\n        #      we might match a value with the wrong recid.\n        if len(c_values) != len(z_values):\n            return [{'name': c_value} for c_value in c_values]\n        else:\n            return [{\n                'curated_relation': True,\n                'name': c_value,\n                'record': get_record_ref(z_value, 'institutions'),\n            } for c_value, z_value in zip(c_values, z_values)]\n\n    thesis_info = self.get('thesis_info', {})\n\n    thesis_info['date'] = normalize_date(force_single_element(value.get('d')))\n    thesis_info['degree_type'] = _get_degree_type(value)\n    thesis_info['institutions'] = _get_institutions(value)\n\n    return thesis_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npopulates the 502 MARC field.", "response": "def thesis_info2marc(self, key, value):\n    \"\"\"Populate the ``502`` MARC field.\n\n    Also populates the ``500`` MARC field through side effects.\n    \"\"\"\n    def _get_b_value(value):\n        DEGREE_TYPES_MAP = {\n            'bachelor': 'Bachelor',\n            'diploma': 'Diploma',\n            'habilitation': 'Habilitation',\n            'laurea': 'Laurea',\n            'master': 'Master',\n            'other': 'Thesis',\n            'phd': 'PhD',\n        }\n\n        degree_type = value.get('degree_type')\n        if degree_type:\n            return DEGREE_TYPES_MAP.get(degree_type)\n\n    result_500 = self.get('500', [])\n    result_502 = self.get('502', {})\n\n    if value.get('defense_date'):\n        result_500.append({\n            'a': u'Presented on {}'.format(value.get('defense_date')),\n        })\n\n    result_502 = {\n        'b': _get_b_value(value),\n        'c': [el['name'] for el in force_list(value.get('institutions'))],\n        'd': value.get('date'),\n    }\n\n    self['500'] = result_500\n    return result_502"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the abstracts key.", "response": "def abstracts(self, key, value):\n    \"\"\"Populate the ``abstracts`` key.\"\"\"\n    result = []\n\n    source = force_single_element(value.get('9'))\n\n    for a_value in force_list(value.get('a')):\n        result.append({\n            'source': source,\n            'value': a_value,\n        })\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef funding_info(self, key, value):\n    return {\n        'agency': value.get('a'),\n        'grant_number': value.get('c'),\n        'project_number': value.get('f'),\n    }", "response": "Populate the funding_info key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef funding_info2marc(self, key, value):\n    return {\n        'a': value.get('agency'),\n        'c': value.get('grant_number'),\n        'f': value.get('project_number'),\n    }", "response": "Populate the 536 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npopulate the license key.", "response": "def license(self, key, value):\n    \"\"\"Populate the ``license`` key.\"\"\"\n    def _get_license(value):\n        a_values = force_list(value.get('a'))\n\n        oa_licenses = [el for el in a_values if el == 'OA' or el == 'Open Access']\n        other_licenses = [el for el in a_values if el != 'OA' and el != 'Open Access']\n\n        if not other_licenses:\n            return force_single_element(oa_licenses)\n        return force_single_element(other_licenses)\n\n    def _get_material(value):\n        material = value.get('3', '').lower()\n        if material == 'article':\n            return 'publication'\n        return material\n\n    return {\n        'imposing': value.get('b'),\n        'license': _get_license(value),\n        'material': _get_material(value),\n        'url': value.get('u'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the 595 MARC field.", "response": "def license2marc(self, key, value):\n    \"\"\"Populate the ``540`` MARC field.\"\"\"\n    return {\n        'a': value.get('license'),\n        'b': value.get('imposing'),\n        'u': value.get('url'),\n        '3': value.get('material'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copyright(self, key, value):\n    MATERIAL_MAP = {\n        'Article': 'publication',\n        'Published thesis as a book': 'publication',\n    }\n\n    material = value.get('e') or value.get('3')\n\n    return {\n        'holder': value.get('d'),\n        'material': MATERIAL_MAP.get(material),\n        'statement': value.get('f'),\n        'url': value.get('u'),\n        'year': maybe_int(value.get('g')),\n    }", "response": "Populate the copyright key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copyright2marc(self, key, value):\n    E_MAP = {\n        'publication': 'Article',\n    }\n\n    e_value = value.get('material')\n\n    return {\n        'd': value.get('holder'),\n        'e': E_MAP.get(e_value),\n        'f': value.get('statement'),\n        'g': value.get('year'),\n        'u': value.get('url'),\n    }", "response": "Populate the 595 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _private_notes(self, key, value):\n    def _is_for_cds(value):\n        normalized_c_values = [el.upper() for el in force_list(value.get('c'))]\n        return 'CDS' in normalized_c_values\n\n    def _is_for_hal(value):\n        normalized_c_values = [el.upper() for el in force_list(value.get('c'))]\n        return 'HAL' in normalized_c_values\n\n    def _is_not_for_hal(value):\n        normalized_c_values = [el.upper() for el in force_list(value.get('c'))]\n        return 'NOT HAL' in normalized_c_values\n\n    _private_notes = self.get('_private_notes', [])\n    _export_to = self.get('_export_to', {})\n\n    for value in force_list(value):\n        if _is_for_cds(value):\n            _export_to['CDS'] = True\n\n        if _is_for_hal(value):\n            _export_to['HAL'] = True\n        elif _is_not_for_hal(value):\n            _export_to['HAL'] = False\n\n        source = force_single_element(value.get('9'))\n        for _private_note in force_list(value.get('a')):\n            _private_notes.append({\n                'source': source,\n                'value': _private_note,\n            })\n\n    self['_export_to'] = _export_to\n    return _private_notes", "response": "Populate the _private_notes key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _private_notes2marc(self, key, value):\n    def _is_from_hal(value):\n        return value.get('source') == 'HAL'\n\n    if not _is_from_hal(value):\n        return {\n            '9': value.get('source'),\n            'a': value.get('value'),\n        }\n\n    self.setdefault('595_H', []).append({'a': value.get('value')})", "response": "Populate the 695 MARC key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npopulate the 595 MARC field.", "response": "def _export_to2marc(self, key, value):\n    \"\"\"Populate the ``595`` MARC field.\"\"\"\n    def _is_for_cds(value):\n        return 'CDS' in value\n\n    def _is_for_hal(value):\n        return 'HAL' in value and value['HAL']\n\n    def _is_not_for_hal(value):\n        return 'HAL' in value and not value['HAL']\n\n    result = []\n\n    if _is_for_cds(value):\n        result.append({'c': 'CDS'})\n\n    if _is_for_hal(value):\n        result.append({'c': 'HAL'})\n    elif _is_not_for_hal(value):\n        result.append({'c': 'not HAL'})\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the _desy_bookkeeping key.", "response": "def _desy_bookkeeping(self, key, value):\n    \"\"\"Populate the ``_desy_bookkeeping`` key.\"\"\"\n    return {\n        'date': normalize_date(value.get('d')),\n        'expert': force_single_element(value.get('a')),\n        'status': value.get('s'),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _desy_bookkeeping2marc(self, key, value):\n    if 'identifier' not in value:\n        return {\n            'a': value.get('expert'),\n            'd': value.get('date'),\n            's': value.get('status'),\n        }\n\n    self.setdefault('035', []).append({\n        '9': 'DESY',\n        'z': value['identifier']\n    })", "response": "Populate the 595_D MARC field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dates(self, key, value):\n    if value.get('q'):\n        self['date_proposed'] = normalize_date(value['q'])\n    if value.get('r'):\n        self['date_approved'] = normalize_date(value['r'])\n    if value.get('s'):\n        self['date_started'] = normalize_date(value['s'])\n    if value.get('c'):\n        self['date_cancelled'] = normalize_date(value['c'])\n    if value.get('t'):\n        self['date_completed'] = normalize_date(value['t'])\n\n    raise IgnoreKey", "response": "Populate the dates key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the experiment key.", "response": "def experiment(self, key, values):\n    \"\"\"Populate the ``experiment`` key.\n\n    Also populates the ``legacy_name``, the ``accelerator``, and the\n    ``institutions`` keys through side effects.\n    \"\"\"\n    experiment = self.get('experiment', {})\n    legacy_name = self.get('legacy_name', '')\n    accelerator = self.get('accelerator', {})\n    institutions = self.get('institutions', [])\n\n    for value in force_list(values):\n        if value.get('c'):\n            experiment['value'] = value.get('c')\n        if value.get('d'):\n            experiment['short_name'] = value.get('d')\n\n        if value.get('a'):\n            legacy_name = value.get('a')\n\n        if value.get('b'):\n            accelerator['value'] = value.get('b')\n\n        institution = {}\n        if value.get('u'):\n            institution['value'] = value.get('u')\n        if value.get('z'):\n            record = get_record_ref(maybe_int(value.get('z')), 'institutions')\n            if record:\n                institution['curated_relation'] = True\n                institution['record'] = record\n        institutions.append(institution)\n\n    self['legacy_name'] = legacy_name\n    self['accelerator'] = accelerator\n    self['institutions'] = institutions\n    return experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the core key.", "response": "def core(self, key, value):\n    \"\"\"Populate the ``core`` key.\n\n    Also populates the ``deleted`` and ``project_type`` keys through side\n    effects.\n    \"\"\"\n    core = self.get('core')\n    deleted = self.get('deleted')\n    project_type = self.get('project_type', [])\n\n    if not core:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        if 'CORE' in normalized_a_values:\n            core = True\n\n    if not deleted:\n        normalized_c_values = [el.upper() for el in force_list(value.get('c'))]\n        if 'DELETED' in normalized_c_values:\n            deleted = True\n\n    if not project_type:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        if 'ACCELERATOR' in normalized_a_values:\n            project_type.append('accelerator')\n\n    self['project_type'] = project_type\n    self['deleted'] = deleted\n    return core"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the control_number key.", "response": "def control_number(endpoint):\n    \"\"\"Populate the ``control_number`` key.\n\n    Also populates the ``self`` key through side effects.\n    \"\"\"\n    def _control_number(self, key, value):\n        self['self'] = get_record_ref(int(value), endpoint)\n        return int(value)\n\n    return _control_number"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquisition_source(self, key, value):\n    def _get_datetime(value):\n        d_value = force_single_element(value.get('d', ''))\n        if d_value:\n            try:\n                date = PartialDate.loads(d_value)\n            except ValueError:\n                return d_value\n            else:\n                datetime_ = datetime(year=date.year, month=date.month, day=date.day)\n                return datetime_.isoformat()\n\n    internal_uid, orcid, source = None, None, None\n\n    a_values = force_list(value.get('a'))\n    for a_value in a_values:\n        if IS_INTERNAL_UID.match(a_value):\n            if a_value.startswith('inspire:uid:'):\n                internal_uid = int(a_value[12:])\n            else:\n                internal_uid = int(a_value)\n        elif IS_ORCID.match(a_value):\n            if a_value.startswith('orcid:'):\n                orcid = a_value[6:]\n            else:\n                orcid = a_value\n        else:\n            source = a_value\n\n    c_value = force_single_element(value.get('c', ''))\n    normalized_c_value = c_value.lower()\n\n    if normalized_c_value == 'batchupload':\n        method = 'batchuploader'\n    elif normalized_c_value == 'submission':\n        method = 'submitter'\n    else:\n        method = normalized_c_value\n\n    return {\n        'datetime': _get_datetime(value),\n        'email': value.get('b'),\n        'internal_uid': internal_uid,\n        'method': method,\n        'orcid': orcid,\n        'source': source,\n        'submission_number': value.get('e'),\n    }", "response": "Populate the acquisition_source key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the public_notes key.", "response": "def public_notes_500(self, key, value):\n    \"\"\"Populate the ``public_notes`` key.\"\"\"\n    return [\n        {\n            'source': value.get('9'),\n            'value': public_note,\n        } for public_note in force_list(value.get('a'))\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _private_notes_595(self, key, value):\n    return [\n        {\n            'source': value.get('9'),\n            'value': _private_note,\n        } for _private_note in force_list(value.get('a'))\n    ]", "response": "Populate the _private_notes key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef external_system_identifiers(endpoint):\n    @utils.flatten\n    @utils.for_each_value\n    def _external_system_identifiers(self, key, value):\n        new_recid = maybe_int(value.get('d'))\n        if new_recid:\n            self['new_record'] = get_record_ref(new_recid, endpoint)\n\n        return [\n            {\n                'schema': 'SPIRES',\n                'value': ext_sys_id,\n            } for ext_sys_id in force_list(value.get('a'))\n        ]\n\n    return _external_system_identifiers", "response": "Populate the external_system_identifiers key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the deleted_records key.", "response": "def deleted_records(endpoint):\n    \"\"\"Populate the ``deleted_records`` key.\"\"\"\n    @utils.for_each_value\n    def _deleted_records(self, key, value):\n        deleted_recid = maybe_int(value.get('a'))\n        if deleted_recid:\n            return get_record_ref(deleted_recid, endpoint)\n\n    return _deleted_records"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef accelerator_experiments(self, key, value):\n    result = []\n\n    a_value = force_single_element(value.get('a'))\n    e_values = [el for el in force_list(value.get('e')) if el != '-']\n    zero_values = force_list(value.get('0'))\n\n    if a_value and not e_values:\n        result.append({'accelerator': a_value})\n\n    # XXX: we zip only when they have the same length, otherwise\n    #      we might match a value with the wrong recid.\n    if len(e_values) == len(zero_values):\n        for e_value, zero_value in zip(e_values, zero_values):\n            result.append({\n                'legacy_name': e_value,\n                'record': get_record_ref(zero_value, 'experiments'),\n            })\n    else:\n        for e_value in e_values:\n            result.append({'legacy_name': e_value})\n\n    return result", "response": "Populate the accelerator_experiments key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keywords(self, key, values):\n    keywords = self.get('keywords', [])\n    energy_ranges = self.get('energy_ranges', [])\n\n    for value in force_list(values):\n        if value.get('a'):\n            schema = force_single_element(value.get('2', '')).upper()\n            sources = force_list(value.get('9'))\n\n            a_values = force_list(value.get('a'))\n\n            if 'conference' not in sources:\n                for a_value in a_values:\n                    keywords.append({\n                        'schema': schema,\n                        'source': force_single_element(sources),\n                        'value': a_value,\n                    })\n\n        if value.get('e'):\n            energy_ranges.append(ENERGY_RANGES_MAP.get(value.get('e')))\n\n    self['energy_ranges'] = energy_ranges\n    return keywords", "response": "Populate the keywords key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keywords2marc(self, key, values):\n    result_695 = self.get('695', [])\n    result_084 = self.get('084', [])\n    result_6531 = self.get('6531', [])\n\n    for value in values:\n        schema = value.get('schema')\n        source = value.get('source')\n        keyword = value.get('value')\n\n        if schema == 'PACS' or schema == 'PDG':\n            result_084.append({\n                '2': schema,\n                '9': source,\n                'a': keyword,\n            })\n        elif schema == 'JACOW':\n            result_6531.append({\n                '2': 'JACoW',\n                '9': source,\n                'a': keyword,\n            })\n        elif schema == 'INSPIRE':\n            result_695.append({\n                '2': 'INSPIRE',\n                '9': source,\n                'a': keyword,\n            })\n        elif schema == 'INIS':\n            result_695.append({\n                '2': 'INIS',\n                '9': source,\n                'a': keyword,\n            })\n        elif source != 'magpie':\n            result_6531.append({\n                '9': source,\n                'a': keyword,\n            })\n\n    self['6531'] = result_6531\n    self['084'] = result_084\n    return result_695", "response": "Populate the 695 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collaborations(self, key, value):\n    result = []\n\n    for g_value in force_list(value.get('g')):\n        collaborations = normalize_collaboration(g_value)\n        if len(collaborations) == 1:\n            result.append({\n                'record': get_record_ref(maybe_int(value.get('0')), 'experiments'),\n                'value': collaborations[0],\n            })\n        else:\n            result.extend({'value': collaboration} for collaboration in collaborations)\n\n    return result", "response": "Populate the collaborations key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publication_info(self, key, value):\n    def _get_cnum(value):\n        w_value = force_single_element(value.get('w', ''))\n        normalized_w_value = w_value.replace('/', '-').upper()\n\n        return normalized_w_value\n\n    def _get_material(value):\n        schema = load_schema('elements/material')\n        valid_materials = schema['enum']\n\n        m_value = force_single_element(value.get('m', ''))\n        normalized_m_value = m_value.lower()\n\n        if normalized_m_value in valid_materials:\n            return normalized_m_value\n\n    def _get_parent_isbn(value):\n        z_value = force_single_element(value.get('z', ''))\n        if z_value:\n            return normalize_isbn(z_value)\n\n    def _get_pubinfo_freetext(value):\n        x_value = force_single_element(value.get('x', ''))\n        if not x_value.startswith('#DONE'):\n            return x_value\n\n    page_start, page_end, artid = split_page_artid(value.get('c'))\n\n    parent_recid = maybe_int(force_single_element(value.get('0')))\n    parent_record = get_record_ref(parent_recid, 'literature')\n\n    journal_recid = maybe_int(force_single_element(value.get('1')))\n    journal_record = get_record_ref(journal_recid, 'journals')\n\n    conference_recid = maybe_int(force_single_element(value.get('2')))\n    conference_record = get_record_ref(conference_recid, 'conferences')\n\n    return {\n        'artid': artid,\n        'cnum': _get_cnum(value),\n        'conf_acronym': force_single_element(value.get('q')),\n        'conference_record': conference_record,\n        'hidden': key.startswith('7731') or None,\n        'journal_issue': force_single_element(value.get('n')),\n        'journal_record': journal_record,\n        'journal_title': force_single_element(value.get('p')),\n        'journal_volume': force_single_element(value.get('v')),\n        'material': _get_material(value),\n        'page_end': page_end,\n        'page_start': page_start,\n        'parent_isbn': _get_parent_isbn(value),\n        'parent_record': parent_record,\n        'parent_report_number': force_single_element(value.get('r')),\n        'pubinfo_freetext': _get_pubinfo_freetext(value),\n        'year': maybe_int(force_single_element(value.get('y'))),\n    }", "response": "Populate the publication_info key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef publication_info2marc(self, key, values):\n    result_773 = self.get('773', [])\n    result_7731 = self.get('7731', [])\n\n    for value in force_list(convert_new_publication_info_to_old(values)):\n        page_artid = []\n        if value.get('page_start') and value.get('page_end'):\n            page_artid.append(u'{page_start}-{page_end}'.format(**value))\n        elif value.get('page_start'):\n            page_artid.append(u'{page_start}'.format(**value))\n        elif value.get('artid'):\n            page_artid.append(u'{artid}'.format(**value))\n\n        result = {\n            '0': get_recid_from_ref(value.get('parent_record')),\n            'c': page_artid,\n            'm': value.get('material'),\n            'n': value.get('journal_issue'),\n            'p': value.get('journal_title'),\n            'q': value.get('conf_acronym'),\n            'r': value.get('parent_report_number'),\n            'v': value.get('journal_volume'),\n            'w': value.get('cnum'),\n            'x': value.get('pubinfo_freetext'),\n            'y': value.get('year'),\n            'z': value.get('parent_isbn'),\n        }\n\n        if value.get('hidden'):\n            result_7731.append(result)\n        else:\n            result_773.append(result)\n\n    self['7731'] = result_7731\n    return result_773", "response": "Populate the 773 MARC field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef related_records_78002(self, key, value):\n    record = get_record_ref(maybe_int(value.get('w')), 'literature')\n    if record:\n        return {\n            'curated_relation': record is not None,\n            'record': record,\n            'relation': 'predecessor',\n        }", "response": "Populate the related_records key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef related_records_78502(self, key, value):\n    record = get_record_ref(maybe_int(value.get('w')), 'literature')\n    if record:\n        return {\n            'curated_relation': record is not None,\n            'record': record,\n            'relation': 'successor',\n        }", "response": "Populate the related_records key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the related_records key.", "response": "def related_records_78708(self, key, value):\n    \"\"\"Populate the ``related_records`` key.\"\"\"\n    record = get_record_ref(maybe_int(value.get('w')), 'literature')\n    if record:\n        return {\n            'curated_relation': record is not None,\n            'record': record,\n            'relation_freetext': value.get('i'),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating the 78502 MARC field based on the related_records key.", "response": "def related_records2marc(self, key, value):\n    \"\"\"Populate the ``78708`` MARC field\n\n    Also populates the ``78002``, ``78502`` MARC fields through side effects.\n    \"\"\"\n    if value.get('relation_freetext'):\n        return {\n            'i': value.get('relation_freetext'),\n            'w': get_recid_from_ref(value.get('record')),\n        }\n    elif value.get('relation') == 'successor':\n        self.setdefault('78502', []).append({\n            'i': 'superseded by',\n            'w': get_recid_from_ref(value.get('record')),\n        })\n    elif value.get('relation') == 'predecessor':\n        self.setdefault('78002', []).append({\n            'i': 'supersedes',\n            'w': get_recid_from_ref(value.get('record')),\n        })\n    else:\n        raise NotImplementedError(u\"Unhandled relation in related_records: {}\".format(value.get('relation')))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the _private_notes key.", "response": "def _private_notes(self, key, value):\n    \"\"\"Populate the ``_private_notes`` key.\"\"\"\n    return [\n        {\n            'source': value.get('9'),\n            'value': _private_note,\n        } for _private_note in force_list(value.get('x'))\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the proceedings key.", "response": "def proceedings(self, key, value):\n    \"\"\"Populate the ``proceedings`` key.\n\n    Also populates the ``refereed`` key through side effects.\n    \"\"\"\n    proceedings = self.get('proceedings')\n    refereed = self.get('refereed')\n\n    if not proceedings:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        if 'PROCEEDINGS' in normalized_a_values:\n            proceedings = True\n\n    if not refereed:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        if 'PEER REVIEW' in normalized_a_values:\n            refereed = True\n        elif 'NON-PUBLISHED' in normalized_a_values:\n            refereed = False\n\n    self['refereed'] = refereed\n    return proceedings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the short_title key.", "response": "def short_title(self, key, value):\n    \"\"\"Populate the ``short_title`` key.\n\n    Also populates the ``title_variants`` key through side effects.\n    \"\"\"\n    short_title = value.get('a')\n    title_variants = self.get('title_variants', [])\n\n    if value.get('u'):\n        short_title = value.get('u')\n        title_variants.append(value.get('a'))\n\n    self['title_variants'] = title_variants\n    return short_title"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the deleted key.", "response": "def deleted(self, key, value):\n    \"\"\"Populate the ``deleted`` key.\n\n    Also populates the ``book_series`` key through side effects.\n    \"\"\"\n    deleted = self.get('deleted')\n    book_series = self.get('book_series')\n\n    if not deleted:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        normalized_c_values = [el.upper() for el in force_list(value.get('c'))]\n        if 'DELETED' in normalized_a_values or 'DELETED' in normalized_c_values:\n            deleted = True\n\n    if not book_series:\n        normalized_a_values = [el.upper() for el in force_list(value.get('a'))]\n        if 'BOOKSERIES' in normalized_a_values:\n            book_series = True\n\n    self['book_series'] = book_series\n    return deleted"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ranks(self, key, value):\n    return [normalize_rank(el) for el in force_list(value.get('a'))]", "response": "Populate the ranks key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a command line argument parser", "response": "def new_parser(self):\n        \"\"\" Create a command line argument parser\n\n        Add a few default flags, such as --version\n        for displaying the program version when invoked \"\"\"\n\n        parser = argparse.ArgumentParser(description=self.description)\n        parser.add_argument(\n            '--version', help='show version and exit',\n            default=False, action='store_true')\n        parser.add_argument(\n            '--debug', help='enable debugging',\n            default=False, action='store_true')\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_command(self, command, function, description=None):\n\n        self.registered[command] = {\n            'function': function, 'description': description\n        }", "response": "Register a new function with a the name command and optionally description."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing command line arguments if necessary then run program.", "response": "def run(self, args=None):\n        \"\"\" Parse command line arguments if necessary then run program.\n        By default this method will just take of the --version flag.\n\n        The logic for other flags should be handled by your subclass \"\"\"\n\n        args = args or self.parser.parse_args()\n\n        if args.debug:\n            logging.basicConfig(level=logging.DEBUG)\n\n        if args.version:\n            print(version.VERSION)\n            sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing help for all available commands or just a single one", "response": "def help_function(self, command=None):\n        \"\"\" Show help for all available commands or just a single one \"\"\"\n        if command:\n            return self.registered[command].get(\n                'description', 'No help available'\n            )\n        return ', '.join(sorted(self.registered))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a new function for command", "response": "def add_command(self, command, function, description=None):\n        \"\"\" Register a new function for command \"\"\"\n        super(Program, self).add_command(command, function, description)\n        self.service.register(command, function)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse comand line arguments and run program", "response": "def run(self, args=None):\n        \"\"\" Parse comand line arguments/flags and run program \"\"\"\n\n        args = args or self.parser.parse_args()\n        super(Program, self).run(args)\n\n        # Read configuration file if any\n        if args.config is not None:\n            filepath = args.config\n            self.config.read(filepath)\n\n        # Start workers then wait until they finish work\n        [w.start() for w in self.workers]\n        [w.join() for w in self.workers]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a client based on addr", "response": "def create_client(self, addr, timeout):\n        \"\"\" Create client(s) based on addr \"\"\"\n\n        def make(addr):\n            c = Client(addr)\n            c.socket._set_recv_timeout(timeout)\n            return c\n\n        if ',' in addr:\n            addrs = addr.split(',')\n            addrs = [a.strip() for a in addrs]\n            return {a: make(a) for a in addrs}\n        return make(addr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _call_single(self, client, command, *args):\n        try:\n            return client.call(command, *args)\n        except Exception as e:\n            return None, str(e)", "response": "Call a single command and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall multiple commands on a set of resources.", "response": "def _call_multi(self, clients, command, *args):\n        \"\"\" Call multi \"\"\"\n        responses, errors = {}, {}\n        for addr, client in clients.items():\n            res, err = self._call_single(client, command, *args)\n            responses[addr] = res\n            errors[addr] = err\n        return responses, errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall a remote service.", "response": "def call(self, command, *args):\n        \"\"\" Call remote service(s) \"\"\"\n        if isinstance(self.c, dict):\n            return self._call_multi(self.c, command, *args)\n        return self._call_single(self.c, command, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if isinstance(self.c, dict):\n            for client in self.c.values():\n                client.sock.close()\n            return\n        self.c.socket.close()", "response": "Close socket and all its related resources."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow result or error", "response": "def _show(self, res, err, prefix='', colored=False):\n        \"\"\" Show result or error \"\"\"\n\n        if self.kind is 'local':\n            what = res if not err else err\n            print(what)\n            return\n\n        if self.kind is 'remote':\n            if colored:\n                red, green, reset = Fore.RED, Fore.GREEN, Fore.RESET\n            else:\n                red = green = reset = ''\n            if err:\n                what = prefix + red + 'remote err: {}'.format(err) + reset\n            else:\n                what = prefix + green + str(res) + reset\n            print(what)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes local or remote command and show response", "response": "def call(self, command, *args):\n        \"\"\" Execute local OR remote command and show response \"\"\"\n\n        if not command:\n            return\n\n        # Look for local methods first\n        try:\n            res = self.registered[command]['function'](self, *args)\n            return Response('local', res, None)\n\n        # Method not found, try remote\n        except KeyError:\n\n            # Execute remote command\n            res, err = self.client.call(command, *args)\n            return Response('remote', res, err, self.client.is_multi())\n\n        # Local exception\n        except Exception as e:\n            return Response('local', res, str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing ctl user input. Double quotes are used to group together multi words arguments.", "response": "def parse_input(self, text):\n        \"\"\" Parse ctl user input. Double quotes are used\n        to group together multi words arguments. \"\"\"\n\n        parts = util.split(text)\n        command = parts[0] if text and parts else None\n        command = command.lower() if command else None\n        args = parts[1:] if len(parts) > 1 else []\n\n        return (command, args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nentering loop read user input then run command. Repeat", "response": "def loop(self):\n        \"\"\" Enter loop, read user input then run command. Repeat \"\"\"\n\n        while True:\n            text = compat.input('ctl > ')\n            command, args = self.parse_input(text)\n            if not command:\n                continue\n            response = self.call(command, *args)\n            response.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreversing namespace lookup. Note that returned namespace may not be unique :param uri: namespace URI :return: namespace", "response": "def namespace_for(uri: Union[URIRef, Namespace, str]) -> str:\n    \"\"\"\n    Reverse namespace lookup.  Note that returned namespace may not be unique\n    :param uri: namespace URI\n    :return: namespace\n    \"\"\"\n    uri = str(uri)\n    if uri not in namespaces.values():\n        namespaces[AnonNS().ns] = uri\n    return [k for k, v in namespaces.items() if uri == v][0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split(text):\n\n    # Cleanup text\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)  # collpse multiple spaces\n\n    space, quote, parts = ' ', '\"', []\n    part, quoted = '', False\n\n    for char in text:\n\n        # Encoutered beginning double quote\n        if char is quote and quoted is False:\n            quoted = True\n            continue\n\n        # Encountered the ending double quote\n        if char is quote and quoted is True:\n            quoted = False\n            parts.append(part.strip())\n            part = ''\n            continue\n\n        # Found space in quoted\n        if char is space and quoted is True:\n            part += char\n            continue\n\n        # Found space but not quoted\n        if char is space:\n            if part:\n                parts.append(part)\n                part = ''\n            continue\n\n        # Found other character\n        if char is not space:\n            part += char\n            continue\n\n    if part:\n        parts.append(part.strip())\n\n    return parts", "response": "Split text into arguments accounting for muti - word arguments"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all available formats in rdflib for the required task", "response": "def known_formats(use: Union[Serializer, Parser]=Serializer, include_mime_types: bool = False) -> List[str]:\n    \"\"\" Return a list of available formats in rdflib for the required task\n    :param use: task (typically Serializer or Parser)\n    :param include_mime_types: whether mime types are included in the return list\n    :return: list of formats\n    \"\"\"\n    return sorted([name for name, kind in plugin._plugins.keys()\n                   if kind == use and (include_mime_types or '/' not in name)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the signature for a file name.", "response": "def file_signature(file_name: str) -> Optional[Tuple]:\n    \"\"\"\n    Return an identity signature for file name\n    :param file_name: name of file\n    :return: mode, size, last modified time if file exists, otherwise none\n    \"\"\"\n    try:\n        st = os.stat(file_name)\n    except FileNotFoundError:\n        return None\n    return stat.S_IFMT(st.st_mode), st.st_size, st.st_mtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef url_signature(url: str) -> Optional[Tuple]:\n    request = urllib.request.Request(url)\n    request.get_method = lambda: 'HEAD'\n    response = None\n    try:\n        response = urllib.request.urlopen(request)\n    except urllib.error.HTTPError:\n        return None\n    return response.info()['Last-Modified'], response.info()['Content-Length'], response.info().get('ETag')", "response": "Return an identify signature for a given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef signature(name: str) -> Optional[Tuple]:\n    return url_signature(name) if is_url(name) else file_signature(name) if is_file(name) else None", "response": "Returns the file or URL signature for name\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_long_description(readme_file):\n    try:\n        import pypandoc\n    except (ImportError, OSError) as e:\n        print('No pypandoc or pandoc: %s' % (e,))\n        if is_py3:\n            fh = open(readme_file, encoding='utf-8')\n        else:\n            fh = open(readme_file)\n        long_description = fh.read()\n        fh.close()\n        return long_description\n    else:\n        return pypandoc.convert(readme_file, 'rst')", "response": "Read package long description from README file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the prefixes from the graph for aesthetics", "response": "def strip_prefixes(g: Graph):\n        \"\"\" Remove the prefixes from the graph for aesthetics \"\"\"\n        return re.sub(r'^@prefix .* .\\n', '',\n                      g.serialize(format=\"turtle\").decode(),\n                      flags=re.MULTILINE).strip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_prefixes(self, nsmap: Dict[str, Namespace]) -> None:\n        [self._g.bind(e[0], e[1]) for e in nsmap.items()]", "response": "Add the required prefix definitions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, subj: Node, pred: URIRef, obj: Node) -> \"FHIRResource\":\n        self._g.add((subj, pred, obj))\n        return self", "response": "Shortcut to rdflib add function\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a value node to the object.", "response": "def add_value_node(self, subj: Node, pred: URIRef, val: Union[JsonObj, str, List],\n                       valuetype: Optional[URIRef]= None) -> None:\n        \"\"\"\n        Expand val according to the range of pred and add it to the graph\n        :param subj: graph subject\n        :param pred: graph predicate\n        :param val: JSON representation of target object\n        :param valuetype: predicate type if it can't be directly determined\n        \"\"\"\n        pred_type = self._meta.predicate_type(pred) if not valuetype else valuetype\n        # Transform generic resources into specific types\n        if pred_type == FHIR.Resource:\n            pred_type = FHIR[val.resourceType]\n\n        val_meta = FHIRMetaVocEntry(self._vocabulary, pred_type)\n        for k, p in val_meta.predicates().items():\n            if k in val:\n                self.add_val(subj, p, val, k)\n                if pred == FHIR.CodeableConcept.coding:\n                    self.add_type_arc(subj, val)\n            elif k == \"value\" and val_meta.predicate_type(p) == FHIR.Element:\n                # value / Element is the wild card combination -- if there is a \"value[x]\" in val, emit it where the\n                # type comes from 'x'\n                for vk in val._as_dict.keys():\n                    if vk.startswith(k):\n                        self.add_val(subj, FHIR['Extension.' + vk], val, vk, self._meta.value_predicate_to_type(vk))\n            else:\n                # Can have an extension only without a primary value\n                self.add_extension_val(subj, val, k, p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_reference(self, subj: Node, val: str) -> None:\n        match = FHIR_RESOURCE_RE.match(val)\n        ref_uri_str = res_type = None\n        if match:\n            ref_uri_str = val if match.group(FHIR_RE_BASE) else (self._base_uri + urllib.parse.quote(val))\n            res_type = match.group(FHIR_RE_RESOURCE)\n        elif '://' in val:\n            ref_uri_str = val\n            res_type = \"Resource\"\n        elif self._base_uri and not val.startswith('#') and not val.startswith('/'):\n            ref_uri_str = self._base_uri + urllib.parse.quote(val)\n            res_type = val.split('/', 1)[0] if '/' in val else \"Resource\"\n        if ref_uri_str:\n            ref_uri = URIRef(ref_uri_str)\n            self.add(subj, FHIR.link, ref_uri)\n            self.add(ref_uri, RDF.type, FHIR[res_type])", "response": "Add a fhir : link and RDF type arc if it can be determined\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_val(self, subj: Node, pred: URIRef, json_obj: JsonObj, json_key: str,\n                valuetype: Optional[URIRef] = None) -> Optional[BNode]:\n        \"\"\"\n        Add the RDF representation of val to the graph as a target of subj, pred.  Note that FHIR lists are\n        represented as a list of BNODE objects with a fhir:index discrimanant\n        :param subj: graph subject\n        :param pred: predicate\n        :param json_obj: object containing json_key\n        :param json_key: name of the value in the JSON resource\n        :param valuetype: value type if NOT determinable by predicate\n        :return: value node if target is a BNode else None\n        \"\"\"\n        if json_key not in json_obj:\n            print(\"Expecting to find object named '{}' in JSON:\".format(json_key))\n            print(json_obj._as_json_dumps())\n            print(\"entry skipped\")\n            return None\n        val = json_obj[json_key]\n        if isinstance(val, List):\n            list_idx = 0\n            for lv in val:\n                entry_bnode = BNode()\n                # TODO: this is getting messy. Refactor and clean this up\n                if pred == FHIR.Bundle.entry:\n                    entry_subj = URIRef(lv.fullUrl)\n                    self.add(entry_bnode, FHIR.index, Literal(list_idx))\n                    self.add_val(entry_bnode, FHIR.Bundle.entry.fullUrl, lv, 'fullUrl')\n                    self.add(entry_bnode, FHIR.Bundle.entry.resource, entry_subj)\n                    self.add(subj, pred, entry_bnode)\n                    entry_mv = FHIRMetaVocEntry(self._vocabulary, FHIR.BundleEntryComponent)\n                    for k, p in entry_mv.predicates().items():\n                        if k not in ['resource', 'fullUrl'] and k in lv:\n                            print(\"---> adding {}\".format(k))\n                            self.add_val(subj, p, lv, k)\n                    FHIRResource(self._vocabulary, None,  self._base_uri, lv.resource, self._g,\n                                 False, self._replace_narrative_text, False, resource_uri=entry_subj)\n                else:\n                    self.add(entry_bnode, FHIR.index, Literal(list_idx))\n                    if isinstance(lv, JsonObj):\n                        self.add_value_node(entry_bnode, pred, lv, valuetype)\n                    else:\n                        vt = self._meta.predicate_type(pred)\n                        atom_type = self._meta.primitive_datatype_nostring(vt) if vt else None\n                        self.add(entry_bnode, FHIR.value, Literal(lv, datatype=atom_type))\n                    self.add(subj, pred, entry_bnode)\n                list_idx += 1\n        else:\n            vt = self._meta.predicate_type(pred) if not valuetype else valuetype\n            if self._meta.is_atom(pred):\n                if self._replace_narrative_text and pred == FHIR.Narrative.div and len(val) > 120:\n                    val = REPLACED_NARRATIVE_TEXT\n                self.add(subj, pred, Literal(val))\n            else:\n                v = BNode()\n                if self._meta.is_primitive(vt):\n                    self.add(v, FHIR.value, Literal(str(val), datatype=self._meta.primitive_datatype_nostring(vt, val)))\n                else:\n                    self.add_value_node(v, pred, val, valuetype)\n                self.add(subj, pred, v)\n                if pred == FHIR.Reference.reference:\n                    self.add_reference(subj, val)\n                self.add_extension_val(v, json_obj, json_key)\n                return v\n        return None", "response": "Add the RDF representation of val to the graph as a target of subj pred."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_extension_val(self,\n                          subj: Node,\n                          json_obj:\n                          Union[JsonObj, List[JsonObjTypes]],\n                          key: str,\n                          pred: Optional[URIRef] = None) -> None:\n        \"\"\"\n        Add any extensions for the supplied object. This can be called in following situations:\n        1) Single extended value \n                \"key\" : (value),\n                \"_key\" : {\n                    \"extension\": [\n                       {\n                        \"url\": \"http://...\",\n                        \"value[x]\": \"......\" \n                       }\n                    ]\n                }\n        2) Single extension only\n                \"_key\" : {\n                    \"extension\": [\n                       {\n                        \"url\": \"http://...\",\n                        \"value[x]\": \"......\" \n                       }\n                    ]\n                }\n        3) Multiple extended values:\n                (TBD)\n                \n        4) Multiple extensions only\n                \"_key\" : [\n                  { \n                    \"extension\": [\n                       {\n                        \"url\": \"http://...\",\n                        \"value[x]\": \"......\" \n                       }\n                    ]\n                  }\n                ]\n                    \n        :param subj: Node containing subject\n        :param json_obj: Object (potentially) containing \"_key\"\n        :param key: name of element that is possibly extended (as indicated by \"_\" prefix)\n        :param pred: predicate for the contained elements. Only used in situations 3) (?) and 4 \n        \"\"\"\n        extendee_name = \"_\" + key\n        if extendee_name in json_obj:\n            if not isinstance(subj, BNode):\n                raise NotImplementedError(\"Extension to something other than a simple BNode\")\n            if isinstance(json_obj[extendee_name], list):\n                if not pred:\n                    raise NotImplemented(\"Case 3 not implemented\")\n                entry_idx = 0\n                for extension in json_obj[extendee_name]:\n                    entry = BNode()\n                    self.add(entry, FHIR.index, Literal(entry_idx))\n                    self.add_val(entry, FHIR.Element.extension, extension, 'extension')\n                    self.add(subj, pred, entry)\n                    entry_idx += 1\n            elif 'fhir_comments' in json_obj[extendee_name] and len(json_obj[extendee_name]) == 1:\n                # TODO: determine whether and how fhir comments should be represented in RDF.\n                # for the moment we just drop them\n                print(\"fhir_comment ignored\")\n                print(json_obj[extendee_name]._as_json_dumps())\n                pass\n            else:\n                self.add_val(subj, FHIR.Element.extension, json_obj[extendee_name], 'extension')", "response": "Add any extensions for the supplied object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef link(g: Graph, subject: Node, predicate: URIRef) -> Tuple[Optional[URIRef], Optional[URIRef]]:\n    link_node = g.value(subject, predicate)\n    if link_node:\n        l = g.value(link_node, FHIR.link)\n        if l:\n            typ = g.value(l, RDF.type)\n            return l, typ\n    return None, None", "response": "Return the link URI and link type for subject and predicate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef codeable_concept_code(g: Graph, subject: Node, predicate: URIRef, system: Optional[str]=None) \\\n        -> List[CodeableConcept]:\n    \"\"\"\n    Return a list of CodeableConcept entries for the supplied subject and predicate in graph g\n    :param g: graph containing the data\n    :param subject: subject\n    :param predicate: predicate\n    :param system: coding system.  If present, only concepts in this system will be returned\n    :return: system, code and optional URI of matching concept(s)\n    \"\"\"\n    # EXAMPLE:\n    # fhir:Patient.maritalStatus [\n    #    fhir:CodeableConcept.coding [\n    #      fhir:index 0;\n    #      a sct:36629006;\n    #      fhir:Coding.system [ fhir:value \"http://snomed.info/sct\" ];\n    #      fhir:Coding.code [ fhir:value \"36629006\" ];\n    #      fhir:Coding.display [ fhir:value \"Legally married\" ]\n    #    ], [\n    #      fhir:index 1;\n    #      fhir:Coding.system [ fhir:value \"http://hl7.org/fhir/v3/MaritalStatus\" ];\n    #      fhir:Coding.code [ fhir:value \"M\" ]\n    #    ]\n    # ];\n    rval = []\n    coded_entry = g.value(subject, predicate, any=False)\n    if coded_entry:\n        for codeable_concept in list(g.objects(coded_entry, FHIR.CodeableConcept.coding)):\n            coding_system = value(g, codeable_concept, FHIR.Coding.system)\n            coding_code = value(g, codeable_concept, FHIR.Coding.code)\n            if coding_system and coding_code and (system is None or system == coding_system):\n                rval.append(CodeableConcept(coding_system, coding_code, g.value(codeable_concept, RDF.type, any=False)))\n    return rval", "response": "Returns a list of CodeableConcept objects for the supplied subject and predicate in the given RDF graph g."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _to_str(uri: URIRef) -> str:\n        local_name = str(uri).replace(str(FHIR), '')\n        return local_name.rsplit('.', 1)[1] if '.' in local_name else local_name", "response": "Convert a FHIR style URI into a JSON representation of a tag name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the tag names and corresponding URI s for all properties that can be associated with subject", "response": "def predicates(self) -> Dict[str, URIRef]:\n        \"\"\"\n        Return the tag names and corresponding URI's for all properties that can be associated with subject\n        :return: Map from tag name (JSON object identifier) to corresponding URI\n        \"\"\"\n        rval = dict()\n        for parent in self._o.objects(self._subj, RDFS.subClassOf):\n            if isinstance(parent, URIRef) and not str(parent).startswith(str(W5)):\n                rval.update(**FHIRMetaVocEntry(self._o, parent).predicates())\n        for s in self._o.subjects(RDFS.domain, self._subj):\n            rval[self._to_str(s)] = s\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the type of predicate pred", "response": "def predicate_type(self, pred: URIRef) -> URIRef:\n        \"\"\"\n        Return the type of pred\n        :param pred: predicate to map\n        :return:\n        \"\"\"\n        return self._o.value(pred, RDFS.range)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the given FHIR type is valid.", "response": "def is_valid(self, t: URIRef) -> bool:\n        \"\"\"\n        Raise an exception if 't' is unrecognized\n        :param t: metadata URI\n        \"\"\"\n        if not self.has_type(t):\n            raise TypeError(\"Unrecognized FHIR type: {}\".format(t))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine whether type t is a FHIR primitive type", "response": "def is_primitive(self, t: URIRef) -> bool:\n        \"\"\"\n        Determine whether type \"t\" is a FHIR primitive type\n        :param t: type to test\n        :return:\n        \"\"\"\n        return FHIR.Primitive in self._o.objects(t, RDFS.subClassOf)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a predicate in the form of fhir : [.... value [ type ] to fhir : type", "response": "def value_predicate_to_type(self, value_pred: str) -> URIRef:\n        \"\"\"\n        Convert a predicate in the form of \"fhir:[...].value[type] to fhir:type, covering the downshift on the\n        first character if necessary\n        :param value_pred: Predicate associated with the value\n        :return: corresponding type or None if not found\n        \"\"\"\n        if value_pred.startswith('value'):\n            vp_datatype = value_pred.replace('value', '')\n            if vp_datatype:\n                if self.has_type(FHIR[vp_datatype]):\n                    return FHIR[vp_datatype]\n                else:\n                    vp_datatype = vp_datatype[0].lower() + vp_datatype[1:]\n                    if self.has_type(FHIR[vp_datatype]):\n                        return FHIR[vp_datatype]\n        if self.is_valid(FHIR[value_pred]):\n            return FHIR[value_pred]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_atom(self, pred: URIRef) -> bool:\n        if not self.has_type(pred):\n            if '.value' in str(pred):               # synthetic values (valueString, valueDate, ...)\n                return False\n            else:\n                raise TypeError(\"Unrecognized FHIR predicate: {}\".format(pred))\n        return pred == FHIR.nodeRole or OWL.DatatypeProperty in set(self._o.objects(pred, RDF.type))", "response": "Determine whether the predicate is an atom type"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the data type for primitive type t.", "response": "def primitive_datatype(self, t: URIRef) -> Optional[URIRef]:\n        \"\"\"\n        Return the data type for primitive type t, if any\n        :param t: type\n        :return: corresponding data type\n        \"\"\"\n        for sco in self._o.objects(t, RDFS.subClassOf):\n            sco_type = self._o.value(sco, RDF.type)\n            sco_prop = self._o.value(sco, OWL.onProperty)\n            if sco_type == OWL.Restriction and sco_prop == FHIR.value:\n                # The older versions of fhir.ttl (incorrectly) referenced the datatype directly\n                restriction_type = self._o.value(sco, OWL.allValuesFrom)\n                if not restriction_type:\n                    restriction_dt_entry = self._o.value(sco, OWL.someValuesFrom)\n                    restriction_type = self._o.value(restriction_dt_entry, OWL.onDatatype)\n                return restriction_type\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the data type for primitive type t if any defaulting string to no type", "response": "def primitive_datatype_nostring(self, t: URIRef, v: Optional[str] = None) -> Optional[URIRef]:\n        \"\"\"\n        Return the data type for primitive type t, if any, defaulting string to no type\n        :param t: type\n        :param v: value - for munging dates if we're doing FHIR official output\n        :return: corresponding data type\n        \"\"\"\n        vt = self.primitive_datatype(t)\n        if self.fhir_dates and vt == XSD.dateTime and v:\n            return XSD.gYear if len(v) == 4 else XSD.gYearMonth if len(v) == 7 \\\n                else XSD.date if (len(v) == 10 or (len(v) > 10 and v[10] in '+-')) else XSD.dateTime\n        # For some reason the oid datatype is represented as a string as well\n        if self.fhir_oids and vt == XSD.anyURI:\n            vt = None\n        return None if vt == XSD.string else vt"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a file to the cache.", "response": "def add(self, name: str, sig: Tuple, obj: object) -> None:\n        \"\"\"\n        Add a file to the cache\n        :param name: name of the object to be pickled\n        :param sig: signature for object\n        :param obj: object to pickle\n        \"\"\"\n        if self._cache_directory is not None:\n            if name in self._cache:\n                os.remove(os.path.join(self._cache_directory, self._cache[name].loc))\n            fname = os.path.join(self._cache_directory, str(uuid.uuid4()))\n            with open(fname, 'wb') as f:\n                pickle.dump(obj, f)\n            self._cache[name] = _PickleJar.CacheEntry(sig, fname)\n            self._update()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, name: str, sig: Tuple) -> Optional[object]:\n        if name not in self._cache:\n            return None\n        if self._cache[name].sig != sig:\n            del self._cache[name]\n            self._update()\n            return None\n        with open(self._cache[name].loc, 'rb') as f:\n            return pickle.load(f)", "response": "Returns the object representing name if it exists and has a unique signature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing all cache entries for the current instance.", "response": "def clear(self) -> None:\n        \"\"\"\n        Clear all cache entries for directory and, if it is a 'pure' directory, remove the directory itself\n        \"\"\"\n        if self._cache_directory is not None:\n            # Safety - if there isn't a cache directory file, this probably isn't a valid cache\n            assert os.path.exists(self._cache_directory_index), \"Attempt to clear a non-existent cache\"\n            self._load()            # Shouldn't have any impact but...\n            for e in self._cache.values():\n                if os.path.exists(e.loc):\n                    os.remove(e.loc)\n            self._cache.clear()\n            self._update()\n        self._cache = {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a FHIR JSON file to RDF", "response": "def fhir_json_to_rdf(json_fname: str,\n                     base_uri: str = \"http://hl7.org/fhir/\",\n                     target_graph: Optional[Graph] = None,\n                     add_ontology_header: bool = True,\n                     do_continuations: bool = True,\n                     replace_narrative_text: bool = False,\n                     metavoc: Optional[Union[Graph, FHIRMetaVoc]] = None) -> Graph:\n    \"\"\"\n    Convert a FHIR JSON resource image to RDF\n    :param json_fname: Name or URI of the file to convert\n    :param base_uri: Base URI to use for relative references.\n    :param target_graph:  If supplied, add RDF to this graph. If not, start with an empty graph.\n    :param add_ontology_header:  True means add owl:Ontology declaration to output\n    :param do_continuations: True means follow continuation records on bundles and queries\n    :param replace_narrative_text: True means replace any narrative text longer than 120 characters with\n                '<div xmlns=\"http://www.w3.org/1999/xhtml\">(removed)</div>'\n    :param metavoc: FHIR Metadata Vocabulary (fhir.ttl) graph\n    :return: resulting graph\n    \"\"\"\n\n    def check_for_continuation(data_: JsonObj) -> Optional[str]:\n        if do_continuations and 'link' in data_ and isinstance(data_.link, list):\n            for link_e in data_.link:\n                if 'relation' in link_e and link_e.relation == 'next':\n                    return link_e.url\n        return None\n\n    if target_graph is None:\n        target_graph = Graph()\n\n    if metavoc is None:\n        metavoc = FHIRMetaVoc().g\n    elif isinstance(metavoc, FHIRMetaVoc):\n        metavoc = metavoc.g\n\n    page_fname = json_fname\n    while page_fname:\n        data = load(page_fname)\n        if 'resourceType' in data and data.resourceType != 'Bundle':\n            FHIRResource(metavoc, None, base_uri, data, target=target_graph, add_ontology_header=add_ontology_header,\n                         replace_narrative_text=replace_narrative_text)\n            page_fname = check_for_continuation(data)\n        elif 'entry' in data and isinstance(data.entry, list) and 'resource' in data.entry[0]:\n            FHIRCollection(metavoc, None, base_uri, data, target=target_graph,\n                           add_ontology_header=add_ontology_header if 'resourceType' in data else False,\n                           replace_narrative_text=replace_narrative_text)\n            page_fname = check_for_continuation(data)\n        else:\n            page_fname = None\n            target_graph = None\n    return target_graph"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subj_pred_idx_to_uri(s: URIRef, p: URIRef, idx: Optional[int] = None) -> URIRef:\n    return URIRef(str(s) + '.' + str(p).rsplit('/', 1)[1] + (\"_{}\".format(idx) if idx is not None else ''))", "response": "Converts FHIR subject predicate and entry index into a URI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform the BNode whose subject is s into its equivalent skolemized equivalent.", "response": "def map_node(s: Node, sk_s: URIRef, gin: Graph, gout: Graph) -> None:\n    \"\"\"\n    Transform the BNode whose subject is s into its equivalent, replacing s with its 'skolemized' equivalent\n    :param s: Actual subject\n    :param sk_s: Equivalent URI of subject in output graph\n    :param gin: Input graph\n    :param gout: Output graph\n    \"\"\"\n    for p, o in gin.predicate_objects(s):\n        if not isinstance(o, BNode):\n            gout.add((sk_s, p, o))\n        else:\n            sk_o = subj_pred_idx_to_uri(sk_s, p, gin.value(o, FHIR.index))\n            gout.add((sk_s, p, sk_o))\n            map_node(o, sk_o, gin, gout)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef skolemize(gin: Graph) -> Graph:\n    gout = Graph()\n\n    # Emit any unreferenced subject BNodes (boxes)\n    anon_subjs = [s for s in gin.subjects() if isinstance(s, BNode) and len([gin.subject_predicates(s)]) == 0]\n    if anon_subjs:\n        idx = None if len(anon_subjs) == 1 else 0\n        for s in anon_subjs:\n            map_node(s, FHIR['treeRoot' + ('_{}'.format(idx) if idx is not None else '')], gin, gout)\n            if idx is not None:\n                idx += 1\n\n    # Cover all other non-bnode entries\n    for subj in set(s for s in gin.subjects() if isinstance(s, URIRef)):\n        map_node(subj, subj, gin, gout)\n    return gout", "response": "Takes a Gin graph and returns a new graph containing all of the blank nodes in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef complete_definition(subj: Node,\n                        source_graph: Graph,\n                        target_graph: Optional[Graph]=None) -> PrettyGraph:\n    \"\"\"\n    Return the transitive closure of subject.\n    :param subj: URI or BNode for subject\n    :param source_graph: Graph containing defininition\n    :param target_graph: return graph (for recursion)\n    :return: target_graph\n    \"\"\"\n    if target_graph is None:\n        target_graph = PrettyGraph()\n    for p, o in source_graph.predicate_objects(subj):\n        target_graph.add((subj, p, o))\n        if isinstance(o, BNode):\n            complete_definition(o, source_graph, target_graph)\n    return target_graph", "response": "Complete the transitive closure of subject."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps a graph g in a sorted n3 format", "response": "def dump_nt_sorted(g: Graph) -> List[str]:\n    \"\"\"\n    Dump graph g in a sorted n3 format\n    :param g: graph to dump\n    :return: stringified representation of g\n    \"\"\"\n    return [l.decode('ascii') for l in sorted(g.serialize(format='nt').splitlines()) if l]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rdf_compare(g1: Graph, g2: Graph, ignore_owl_version: bool=False, ignore_type_arcs: bool = False,\n                compare_filter: Optional[Callable[[Graph, Graph, Graph], None]]=None) -> str:\n    \"\"\"\n    Compare graph g1 and g2\n    :param g1: first graph\n    :param g2: second graph\n    :param ignore_owl_version:\n    :param ignore_type_arcs:\n    :param compare_filter: Final adjustment for graph difference. Used, for example, to deal with FHIR decimal problems.\n    :return: List of differences as printable lines or blank if everything matches\n    \"\"\"\n    def graph_for_subject(g: Graph, subj: Node) -> Graph:\n        subj_in_g = complete_definition(subj, g)\n        if ignore_type_arcs:\n            for ta_s, ta_o in subj_in_g.subject_objects(RDF.type):\n                if isinstance(ta_s, BNode) and isinstance(ta_o, URIRef):\n                    subj_in_g.remove((ta_s, RDF.type, ta_o))\n        if ignore_owl_version:\n            subj_in_g.remove((subj, OWL.versionIRI, subj_in_g.value(subj, OWL.versionIRI)))\n        return subj_in_g\n\n    def primary_subjects(g: Graph) -> Set[Node]:\n        anon_subjs = set(anon_s for anon_s in g.subjects()\n                         if isinstance(anon_s, BNode) and len([g.subject_predicates(anon_s)]) == 0)\n        return set(s_ for s_ in g1.subjects() if isinstance(s_, URIRef)).union(anon_subjs)\n\n    rval = \"\"\n\n    # Step 1: Find any subjects in one graph that don't exist in the other\n    g1_subjs = primary_subjects(g1)\n    g2_subjs = primary_subjects(g2)\n    for s in g1_subjs - g2_subjs:\n        rval += \"\\n===== Subjects in Graph 1 but not Graph 2: \"\n        rval += PrettyGraph.strip_prefixes(complete_definition(s, g1))\n    for s in g2_subjs - g1_subjs:\n        rval += \"\\n===== Subjects in Graph 2 but not Graph 1: \"\n        rval += PrettyGraph.strip_prefixes(complete_definition(s, g2))\n\n    # Step 2: Iterate over all of the remaining subjects comparing their contents\n    for s in g1_subjs.intersection(g2_subjs):\n        s_in_g1 = graph_for_subject(g1, s)\n        s_in_g2 = graph_for_subject(g2, s)\n        in_both, in_first, in_second = graph_diff(skolemize(s_in_g1), skolemize(s_in_g2))\n        if compare_filter:\n            compare_filter(in_both, in_first, in_second)\n        if len(list(in_first)) or len(list(in_second)):\n            rval += \"\\n\\nSubject {} DIFFERENCE: \".format(s) + '=' * 30\n            if len(in_first):\n                rval += \"\\n\\t----> First: \\n\" + '\\n'.join(dump_nt_sorted(in_first))\n            if len(in_second):\n                rval += \"\\n\\t----> Second: \\n\" + '\\n'.join(dump_nt_sorted(in_second))\n            rval += '-' * 40\n    return rval", "response": "Compare two graphs g1 and g2 and returns a list of differences as printable lines or blank if everything matches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proc_file(infile: str, outfile: str, opts: Namespace) -> bool:\n    g = fhir_json_to_rdf(infile, opts.uribase, opts.graph, add_ontology_header=not opts.noontology,\n                         do_continuations=not opts.nocontinuation, replace_narrative_text=bool(opts.nonarrative),\n                         metavoc=opts.fhir_metavoc)\n\n    # If we aren't carrying graph in opts, we're doing a file by file transformation\n    if g:\n        if not opts.graph:\n            serialize_graph(g, outfile, opts)\n        return True\n    else:\n        print(\"{} : Not a FHIR collection or resource\".format(infile))\n        return False", "response": "Process infile.\n    :param infile: input file to be processed\n    :param outfile: target output file.\n    :param opts:\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine whether to process a file based on the input file name and directory.", "response": "def file_filter(ifn: str, indir: str, opts: Namespace) -> bool:\n    \"\"\"\n    Determine whether to process ifn.  We con't process:\n        1) Anything in a directory having a path element that begins with \"_\"\n        2) Really, really big files\n        3) Temporary lists of know errors\n    :param ifn: input file name\n    :param indir: input directory\n    :param opts: argparse options\n    :return: True if to be processed, false if to be skipped\n    \"\"\"\n    # If it looks like we're processing a URL as an input file, skip the suffix check\n    if '://' in ifn:\n        return True\n\n    if not ifn.endswith('.json'):\n        return False\n\n    if indir and (indir.startswith(\"_\") or \"/_\" in indir or any(dn in indir for dn in opts.skipdirs)):\n        return False\n\n    if opts.skipfns and any(sfn in ifn for sfn in opts.skipfns):\n        return False\n\n    infile = os.path.join(indir, ifn)\n    if not opts.infile and opts.maxsize and os.path.getsize(infile) > (opts.maxsize * 1000):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fhirtordf(argv: List[str], default_exit: bool = True) -> bool:\n    dlp = dirlistproc.DirectoryListProcessor(argv,\n                                             description=\"Convert FHIR JSON into RDF\",\n                                             infile_suffix=\".json\",\n                                             outfile_suffix=\".ttl\",\n                                             addargs=addargs,\n                                             noexit=not default_exit)\n\n    if not dlp.successful_parse:\n        return False\n\n    # Version\n    if dlp.opts.version:\n        print(\"FHIR to RDF Conversion Tool -- Version {}\".format(__version__))\n\n    # We either have to have an input file or an input directory\n    if not dlp.opts.infile and not dlp.opts.indir:\n        if not dlp.opts.version:\n            dlp.parser.error(\"Either an input file or an input directory must be supplied\")\n        return dlp.opts.version\n\n    # Create the output directory if needed\n    if dlp.opts.outdir and not os.path.exists(dlp.opts.outdir):\n        os.makedirs(dlp.opts.outdir)\n\n    # If we are going to a single output file or stdout, gather all the input\n    dlp.opts.graph = Graph() if (not dlp.opts.outfile and not dlp.opts.outdir) or\\\n                                (dlp.opts.outfile and len(dlp.opts.outfile) == 1) else None\n    dlp.opts.fhir_metavoc = load_fhir_ontology(dlp.opts)\n\n    # If it looks like we're processing a URL as an input file, skip the suffix check\n    if dlp.opts.infile and len(dlp.opts.infile) == 1 and not dlp.opts.indir and \"://\" in dlp.opts.infile[0]:\n        dlp.infile_suffix = \"\"\n    dlp.outfile_suffix = '.' + suffix_for(dlp.opts.format)\n    nfiles, nsuccess = dlp.run(proc=proc_file, file_filter_2=file_filter)\n    if nfiles:\n        if dlp.opts.graph:\n            serialize_graph(dlp.opts.graph, dlp.opts.outfile[0] if dlp.opts.outfile else None, dlp.opts)\n        return nsuccess > 0\n    return False", "response": "Entry point for FHIR to RDF conversion tool"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a resource URI into a tuple of namespace type and resource.", "response": "def parse_fhir_resource_uri(uri: Union[URIRef, str]) -> FHIR_RESOURCE:\n    \"\"\"\n    Use the FHIR Regular Expression for Resource URI's to determine the namespace and type\n    of a given URI.  As an example, \"http://hl7.org/fhir/Patient/p123\" maps to the tuple\n    ``('Patient', 'http://hl7.org/fhir')\n\n    :param uri:  URI to parse\n    :return: FHIR_RESOURCE (namespace, type, resource)\n    \"\"\"\n    uri_str = str(uri)\n    m = FHIR_RESOURCE_RE.match(uri_str)\n    if m:\n        return FHIR_RESOURCE(URIRef(m.group(FHIR_RE_BASE)), FHIR[m.group(FHIR_RE_RESOURCE)], m.group(FHIR_RE_ID))\n    else:\n        # Not in the FHIR format - we can only do namespace and name\n        namespace, name = uri_str.rsplit('#', 1) if '#' in uri_str \\\n            else uri_str.rsplit('/', 1) if '/' in uri_str else (None, uri_str)\n\n        return FHIR_RESOURCE(URIRef(namespace), None, name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dummy distribution object that can be used to instrument the setup environment before calling the actual setup function.", "response": "def get_dummy_distribution():\n    \"\"\"\n    Returns a distutils Distribution object used to instrument the setup\n    environment before calling the actual setup() function.\n    \"\"\"\n\n    from .setup_helpers import _module_state\n\n    if _module_state['registered_commands'] is None:\n        raise RuntimeError(\n            'astropy_helpers.setup_helpers.register_commands() must be '\n            'called before using '\n            'astropy_helpers.setup_helpers.get_dummy_distribution()')\n\n    # Pre-parse the Distutils command-line options and config files to if\n    # the option is set.\n    dist = Distribution({'script_name': os.path.basename(sys.argv[0]),\n                         'script_args': sys.argv[1:]})\n    dist.cmdclass.update(_module_state['registered_commands'])\n\n    with silence():\n        try:\n            dist.parse_config_files()\n            dist.parse_command_line()\n        except (DistutilsError, AttributeError, SystemExit):\n            # Let distutils handle DistutilsErrors itself AttributeErrors can\n            # get raise for ./setup.py --help SystemExit can be raised if a\n            # display option was used, for example\n            pass\n\n    return dist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_distutils_option(option, commands):\n\n    dist = get_dummy_distribution()\n\n    for cmd in commands:\n        cmd_opts = dist.command_options.get(cmd)\n        if cmd_opts is not None and option in cmd_opts:\n            return cmd_opts[option][1]\n    else:\n        return None", "response": "Returns the value of the given distutils option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a custom option to a command object.", "response": "def add_command_option(command, name, doc, is_bool=False):\n    \"\"\"\n    Add a custom option to a setup command.\n\n    Issues a warning if the option already exists on that command.\n\n    Parameters\n    ----------\n    command : str\n        The name of the command as given on the command line\n\n    name : str\n        The name of the build option\n\n    doc : str\n        A short description of the option, for the `--help` message\n\n    is_bool : bool, optional\n        When `True`, the option is a boolean option and doesn't\n        require an associated value.\n    \"\"\"\n\n    dist = get_dummy_distribution()\n    cmdcls = dist.get_command_class(command)\n\n    if (hasattr(cmdcls, '_astropy_helpers_options') and\n            name in cmdcls._astropy_helpers_options):\n        return\n\n    attr = name.replace('-', '_')\n\n    if hasattr(cmdcls, attr):\n        raise RuntimeError(\n            '{0!r} already has a {1!r} class attribute, barring {2!r} from '\n            'being usable as a custom option name.'.format(cmdcls, attr, name))\n\n    for idx, cmd in enumerate(cmdcls.user_options):\n        if cmd[0] == name:\n            log.warn('Overriding existing {0!r} option '\n                     '{1!r}'.format(command, name))\n            del cmdcls.user_options[idx]\n            if name in cmdcls.boolean_options:\n                cmdcls.boolean_options.remove(name)\n            break\n\n    cmdcls.user_options.append((name, None, doc))\n\n    if is_bool:\n        cmdcls.boolean_options.append(name)\n\n    # Distutils' command parsing requires that a command object have an\n    # attribute with the same name as the option (with '-' replaced with '_')\n    # in order for that option to be recognized as valid\n    setattr(cmdcls, attr, None)\n\n    # This caches the options added through add_command_option so that if it is\n    # run multiple times in the same interpreter repeated adds are ignored\n    # (this way we can still raise a RuntimeError if a custom option overrides\n    # a built-in option)\n    if not hasattr(cmdcls, '_astropy_helpers_options'):\n        cmdcls._astropy_helpers_options = set([name])\n    else:\n        cmdcls._astropy_helpers_options.add(name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a set of all distutils display options in their long and short form.", "response": "def get_distutils_display_options():\n    \"\"\" Returns a set of all the distutils display options in their long and\n    short forms.  These are the setup.py arguments such as --name or --version\n    which print the project's metadata and then exit.\n\n    Returns\n    -------\n    opts : set\n        The long and short form display option arguments, including the - or --\n    \"\"\"\n\n    short_display_opts = set('-' + o[1] for o in Distribution.display_options\n                             if o[1])\n    long_display_opts = set('--' + o[0] for o in Distribution.display_options)\n\n    # Include -h and --help which are not explicitly listed in\n    # Distribution.display_options (as they are handled by optparse)\n    short_display_opts.add('-h')\n    long_display_opts.add('--help')\n\n    # This isn't the greatest approach to hardcode these commands.\n    # However, there doesn't seem to be a good way to determine\n    # whether build *will be* run as part of the command at this\n    # phase.\n    display_commands = set([\n        'clean', 'register', 'setopt', 'saveopts', 'egg_info',\n        'alias'])\n\n    return short_display_opts.union(long_display_opts.union(display_commands))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_sphinx_astropy_installed():\n    # We've split out the Sphinx part of astropy-helpers into sphinx-astropy\n    # but we want it to be auto-installed seamlessly for anyone using\n    # build_docs. We check if it's already installed, and if not, we install\n    # it to a local .eggs directory and add the eggs to the path (these\n    # have to each be added to the path, we can't add them by simply adding\n    # .eggs to the path)\n    sys_path_inserts = []\n    sphinx_astropy_version = None\n    try:\n        from sphinx_astropy import __version__ as sphinx_astropy_version  # noqa\n    except ImportError:\n\n        from setuptools import Distribution\n        dist = Distribution()\n\n        # Numpydoc 0.9.0 requires sphinx 1.6+, we can limit the version here\n        # until we also makes our minimum required version Sphinx 1.6\n        if SPHINX_LT_16:\n            dist.fetch_build_eggs('numpydoc<0.9')\n\n        # This egg build doesn't respect python_requires, not aware of\n        # pre-releases. We know that mpl 3.1+ requires Python 3.6+, so this\n        # ugly workaround takes care of it until there is a solution for\n        # https://github.com/astropy/astropy-helpers/issues/462\n        if LooseVersion(sys.version) < LooseVersion('3.6'):\n            dist.fetch_build_eggs('matplotlib<3.1')\n\n        eggs = dist.fetch_build_eggs('sphinx-astropy')\n\n        # Find out the version of sphinx-astropy if possible. For some old\n        # setuptools version, eggs will be None even if sphinx-astropy was\n        # successfully installed.\n        if eggs is not None:\n            for egg in eggs:\n                if egg.project_name == 'sphinx-astropy':\n                    sphinx_astropy_version = egg.parsed_version.public\n                    break\n\n        eggs_path = os.path.abspath('.eggs')\n        for egg in glob.glob(os.path.join(eggs_path, '*.egg')):\n            sys_path_inserts.append(egg)\n\n    return sphinx_astropy_version, sys_path_inserts", "response": "Ensure that sphinx - astropy is available installing it temporarily if not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the name of the appropriate platform - specific build subdirectory directory.", "response": "def _get_platlib_dir(cmd):\n    \"\"\"\n    Given a build command, return the name of the appropriate platform-specific\n    build subdirectory directory (e.g. build/lib.linux-x86_64-2.7)\n    \"\"\"\n\n    plat_specifier = '.{0}-{1}'.format(cmd.plat_name, sys.version[0:3])\n    return os.path.join(cmd.build_base, 'lib' + plat_specifier)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the path to the numpy headers.", "response": "def get_numpy_include_path():\n    \"\"\"\n    Gets the path to the numpy headers.\n    \"\"\"\n    # We need to go through this nonsense in case setuptools\n    # downloaded and installed Numpy for us as part of the build or\n    # install, since Numpy may still think it's in \"setup mode\", when\n    # in fact we're ready to use it to build astropy now.\n\n    import builtins\n    if hasattr(builtins, '__NUMPY_SETUP__'):\n        del builtins.__NUMPY_SETUP__\n    import imp\n    import numpy\n    imp.reload(numpy)\n\n    try:\n        numpy_include = numpy.get_include()\n    except AttributeError:\n        numpy_include = numpy.get_numpy_include()\n    return numpy_include"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if a given file or directory is hidden.", "response": "def is_path_hidden(filepath):\n    \"\"\"\n    Determines if a given file or directory is hidden.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to a file or directory\n\n    Returns\n    -------\n    hidden : bool\n        Returns `True` if the file is hidden\n    \"\"\"\n\n    name = os.path.basename(os.path.abspath(filepath))\n    if isinstance(name, bytes):\n        is_dotted = name.startswith(b'.')\n    else:\n        is_dotted = name.startswith('.')\n    return is_dotted or _has_hidden_attribute(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_if_different(filename, data):\n\n    assert isinstance(data, bytes)\n\n    if os.path.exists(filename):\n        with open(filename, 'rb') as fd:\n            original_data = fd.read()\n    else:\n        original_data = None\n\n    if original_data != data:\n        with open(filename, 'wb') as fd:\n            fd.write(data)", "response": "Writes data to filename if the content of the file is different."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_file(filename, name=None):\n    # Specifying a traditional dot-separated fully qualified name here\n    # results in a number of \"Parent module 'astropy' not found while\n    # handling absolute import\" warnings.  Using the same name, the\n    # namespaces of the modules get merged together.  So, this\n    # generates an underscore-separated name which is more likely to\n    # be unique, and it doesn't really matter because the name isn't\n    # used directly here anyway.\n    mode = 'r'\n\n    if name is None:\n        basename = os.path.splitext(filename)[0]\n        name = '_'.join(os.path.relpath(basename).split(os.sep)[1:])\n\n    if not os.path.exists(filename):\n        raise ImportError('Could not import file {0}'.format(filename))\n\n    if import_machinery:\n        loader = import_machinery.SourceFileLoader(name, filename)\n        mod = loader.load_module()\n    else:\n        with open(filename, mode) as fd:\n            mod = imp.load_module(name, fd, filename, ('.py', mode, 1))\n\n    return mod", "response": "Imports a module from a single file as if it doesn t belong to a specific package."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extends_doc(extended_func):\n\n    def decorator(func):\n        if not (extended_func.__doc__ is None or func.__doc__ is None):\n            func.__doc__ = '\\n\\n'.join([extended_func.__doc__.rstrip('\\n'),\n                                        func.__doc__.lstrip('\\n')])\n        return func\n\n    return decorator", "response": "A function decorator for use when wrapping an existing function but adding additional functionality to the docstring of the original\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind data files in package that match pattern.", "response": "def find_data_files(package, pattern):\n    \"\"\"\n    Include files matching ``pattern`` inside ``package``.\n\n    Parameters\n    ----------\n    package : str\n        The package inside which to look for data files\n    pattern : str\n        Pattern (glob-style) to match for the data files (e.g. ``*.dat``).\n        This supports the``**``recursive syntax. For example, ``**/*.fits``\n        matches all files ending with ``.fits`` recursively. Only one\n        instance of ``**`` can be included in the pattern.\n    \"\"\"\n\n    return glob.glob(os.path.join(package, pattern), recursive=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a version string into major minor and micro.", "response": "def _version_split(version):\n    \"\"\"\n    Split a version string into major, minor, and bugfix numbers.  If any of\n    those numbers are missing the default is zero.  Any pre/post release\n    modifiers are ignored.\n\n    Examples\n    ========\n    >>> _version_split('1.2.3')\n    (1, 2, 3)\n    >>> _version_split('1.2')\n    (1, 2, 0)\n    >>> _version_split('1.2rc1')\n    (1, 2, 0)\n    >>> _version_split('1')\n    (1, 0, 0)\n    >>> _version_split('')\n    (0, 0, 0)\n    \"\"\"\n\n    parsed_version = pkg_resources.parse_version(version)\n\n    if hasattr(parsed_version, 'base_version'):\n        # New version parsing for setuptools >= 8.0\n        if parsed_version.base_version:\n            parts = [int(part)\n                     for part in parsed_version.base_version.split('.')]\n        else:\n            parts = []\n    else:\n        parts = []\n        for part in parsed_version:\n            if part.startswith('*'):\n                # Ignore any .dev, a, b, rc, etc.\n                break\n            parts.append(int(part))\n\n    if len(parts) < 3:\n        parts += [0] * (3 - len(parts))\n\n    # In principle a version could have more parts (like 1.2.3.4) but we only\n    # support <major>.<minor>.<micro>\n    return tuple(parts[:3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a git header for the version. py module.", "response": "def _generate_git_header(packagename, version, githash):\n    \"\"\"\n    Generates a header to the version.py module that includes utilities for\n    probing the git repository for updates (to the current git hash, etc.)\n    These utilities should only be available in development versions, and not\n    in release builds.\n\n    If this fails for any reason an empty string is returned.\n    \"\"\"\n\n    loader = pkgutil.get_loader(git_helpers)\n    source = loader.get_source(git_helpers.__name__) or ''\n    source_lines = source.splitlines()\n    if not source_lines:\n        log.warn('Cannot get source code for astropy_helpers.git_helpers; '\n                 'git support disabled.')\n        return ''\n\n    idx = 0\n    for idx, line in enumerate(source_lines):\n        if line.startswith('# BEGIN'):\n            break\n    git_helpers_py = '\\n'.join(source_lines[idx + 1:])\n\n    verstr = version\n\n    new_githash = git_helpers.get_git_devstr(sha=True, show_warning=False)\n\n    if new_githash:\n        githash = new_githash\n\n    return _FROZEN_VERSION_PY_WITH_GIT_HEADER.format(\n                git_helpers=git_helpers_py, packagename=packagename,\n                verstr=verstr, githash=githash)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_version_py(packagename=None, version=None, release=None, debug=None,\n                        uses_git=None, srcdir='.'):\n    \"\"\"\n    Generate a version.py file in the package with version information, and\n    update developer version strings.\n\n    This function should normally be called without any arguments. In this case\n    the package name and version is read in from the ``setup.cfg`` file (from\n    the ``name`` or ``package_name`` entry and the ``version`` entry in the\n    ``[metadata]`` section).\n\n    If the version is a developer version (of the form ``3.2.dev``), the\n    version string will automatically be expanded to include a sequential\n    number as a suffix (e.g. ``3.2.dev13312``), and the updated version string\n    will be returned by this function.\n\n    Based on this updated version string, a ``version.py`` file will be\n    generated inside the package, containing the version string as well as more\n    detailed information (for example the major, minor, and bugfix version\n    numbers, a ``release`` flag indicating whether the current version is a\n    stable or developer version, and so on.\n    \"\"\"\n\n    if packagename is not None:\n        warnings.warn('The packagename argument to generate_version_py has '\n                      'been deprecated and will be removed in future. Specify '\n                      'the package name in setup.cfg instead', AstropyDeprecationWarning)\n\n    if version is not None:\n        warnings.warn('The version argument to generate_version_py has '\n                      'been deprecated and will be removed in future. Specify '\n                      'the version number in setup.cfg instead', AstropyDeprecationWarning)\n\n    if release is not None:\n        warnings.warn('The release argument to generate_version_py has '\n                      'been deprecated and will be removed in future. We now '\n                      'use the presence of the \"dev\" string in the version to '\n                      'determine whether this is a release', AstropyDeprecationWarning)\n\n    # We use ConfigParser instead of read_configuration here because the latter\n    # only reads in keys recognized by setuptools, but we need to access\n    # package_name below.\n    conf = ConfigParser()\n    conf.read('setup.cfg')\n\n    if conf.has_option('metadata', 'name'):\n        packagename = conf.get('metadata', 'name')\n    elif conf.has_option('metadata', 'package_name'):\n        # The package-template used package_name instead of name for a while\n        warnings.warn('Specifying the package name using the \"package_name\" '\n                      'option in setup.cfg is deprecated - use the \"name\" '\n                      'option instead.', AstropyDeprecationWarning)\n        packagename = conf.get('metadata', 'package_name')\n    elif packagename is not None:  # deprecated\n        pass\n    else:\n        sys.stderr.write('ERROR: Could not read package name from setup.cfg\\n')\n        sys.exit(1)\n\n    if conf.has_option('metadata', 'version'):\n        version = conf.get('metadata', 'version')\n        add_git_devstr = True\n    elif version is not None:  # deprecated\n        add_git_devstr = False\n    else:\n        sys.stderr.write('ERROR: Could not read package version from setup.cfg\\n')\n        sys.exit(1)\n\n    if release is None:\n        release = 'dev' not in version\n\n    if not release and add_git_devstr:\n        version += get_git_devstr(False)\n\n    if uses_git is None:\n        uses_git = not release\n\n    # In some cases, packages have a - but this is a _ in the module. Since we\n    # are only interested in the module here, we replace - by _\n    packagename = packagename.replace('-', '_')\n\n    try:\n        version_module = get_pkg_version_module(packagename)\n\n        try:\n            last_generated_version = version_module._last_generated_version\n        except AttributeError:\n            last_generated_version = version_module.version\n\n        try:\n            last_githash = version_module._last_githash\n        except AttributeError:\n            last_githash = version_module.githash\n\n        current_release = version_module.release\n        current_debug = version_module.debug\n    except ImportError:\n        version_module = None\n        last_generated_version = None\n        last_githash = None\n        current_release = None\n        current_debug = None\n\n    if release is None:\n        # Keep whatever the current value is, if it exists\n        release = bool(current_release)\n\n    if debug is None:\n        # Likewise, keep whatever the current value is, if it exists\n        debug = bool(current_debug)\n\n    package_srcdir = os.path.join(srcdir, *packagename.split('.'))\n    version_py = os.path.join(package_srcdir, 'version.py')\n\n    if (last_generated_version != version or current_release != release or\n            current_debug != debug):\n        if '-q' not in sys.argv and '--quiet' not in sys.argv:\n            log.set_threshold(log.INFO)\n\n        if is_distutils_display_option():\n            # Always silence unnecessary log messages when display options are\n            # being used\n            log.set_threshold(log.WARN)\n\n        log.info('Freezing version number to {0}'.format(version_py))\n\n        with open(version_py, 'w') as f:\n            # This overwrites the actual version.py\n            f.write(_get_version_py_str(packagename, version, last_githash,\n                                        release, debug, uses_git=uses_git))\n\n    return version", "response": "Generates a version. py file in the package with version information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pkg_version_module(packagename, fromlist=None):\n\n    version = import_file(os.path.join(packagename, 'version.py'), name='version')\n\n    if fromlist:\n        return tuple(getattr(version, member) for member in fromlist)\n    else:\n        return version", "response": "Returns the. version module generated by astropy_helpers. version_helpers. generate_version_py."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if the current build is in debug mode.", "response": "def get_debug_option(packagename):\n    \"\"\" Determines if the build is in debug mode.\n\n    Returns\n    -------\n    debug : bool\n        True if the current build was started with the debug option, False\n        otherwise.\n\n    \"\"\"\n\n    try:\n        current_debug = get_pkg_version_module(packagename,\n                                               fromlist=['debug'])[0]\n    except (ImportError, AttributeError):\n        current_debug = None\n\n    # Only modify the debug flag if one of the build commands was explicitly\n    # run (i.e. not as a sub-command of something else)\n    dist = get_dummy_distribution()\n    if any(cmd in dist.commands for cmd in ['build', 'build_ext']):\n        debug = bool(get_distutils_build_option('debug'))\n    else:\n        debug = bool(current_debug)\n\n    if current_debug is not None and current_debug != debug:\n        build_ext_cmd = dist.get_command_class('build_ext')\n        build_ext_cmd._force_rebuild = True\n\n    return debug"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_commands(package=None, version=None, release=None, srcdir='.'):\n\n    if package is not None:\n        warnings.warn('The package argument to generate_version_py has '\n                      'been deprecated and will be removed in future. Specify '\n                      'the package name in setup.cfg instead', AstropyDeprecationWarning)\n\n    if version is not None:\n        warnings.warn('The version argument to generate_version_py has '\n                      'been deprecated and will be removed in future. Specify '\n                      'the version number in setup.cfg instead', AstropyDeprecationWarning)\n\n    if release is not None:\n        warnings.warn('The release argument to generate_version_py has '\n                      'been deprecated and will be removed in future. We now '\n                      'use the presence of the \"dev\" string in the version to '\n                      'determine whether this is a release', AstropyDeprecationWarning)\n\n    # We use ConfigParser instead of read_configuration here because the latter\n    # only reads in keys recognized by setuptools, but we need to access\n    # package_name below.\n    conf = ConfigParser()\n    conf.read('setup.cfg')\n\n    if conf.has_option('metadata', 'name'):\n        package = conf.get('metadata', 'name')\n    elif conf.has_option('metadata', 'package_name'):\n        # The package-template used package_name instead of name for a while\n        warnings.warn('Specifying the package name using the \"package_name\" '\n                      'option in setup.cfg is deprecated - use the \"name\" '\n                      'option instead.', AstropyDeprecationWarning)\n        package = conf.get('metadata', 'package_name')\n    elif package is not None:  # deprecated\n        pass\n    else:\n        sys.stderr.write('ERROR: Could not read package name from setup.cfg\\n')\n        sys.exit(1)\n\n    if _module_state['registered_commands'] is not None:\n        return _module_state['registered_commands']\n\n    if _module_state['have_sphinx']:\n        try:\n            from .commands.build_sphinx import (AstropyBuildSphinx,\n                                                AstropyBuildDocs)\n        except ImportError:\n            AstropyBuildSphinx = AstropyBuildDocs = FakeBuildSphinx\n    else:\n        AstropyBuildSphinx = AstropyBuildDocs = FakeBuildSphinx\n\n    _module_state['registered_commands'] = registered_commands = {\n        'test': generate_test_command(package),\n\n        # Use distutils' sdist because it respects package_data.\n        # setuptools/distributes sdist requires duplication of information in\n        # MANIFEST.in\n        'sdist': DistutilsSdist,\n\n        'build_ext': AstropyHelpersBuildExt,\n        'build_sphinx': AstropyBuildSphinx,\n        'build_docs': AstropyBuildDocs\n    }\n\n    # Need to override the __name__ here so that the commandline options are\n    # presented as being related to the \"build\" command, for example; normally\n    # this wouldn't be necessary since commands also have a command_name\n    # attribute, but there is a bug in distutils' help display code that it\n    # uses __name__ instead of command_name. Yay distutils!\n    for name, cls in registered_commands.items():\n        cls.__name__ = name\n\n    # Add a few custom options; more of these can be added by specific packages\n    # later\n    for option in [\n            ('use-system-libraries',\n             \"Use system libraries whenever possible\", True)]:\n        add_command_option('build', *option)\n        add_command_option('install', *option)\n\n    add_command_hooks(registered_commands, srcdir=srcdir)\n\n    return registered_commands", "response": "This function generates a dictionary containing customized commands that can be passed to the cmdclass argument in setup. cfg."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a list of distutils command hooks to the passed in dict.", "response": "def add_command_hooks(commands, srcdir='.'):\n    \"\"\"\n    Look through setup_package.py modules for functions with names like\n    ``pre_<command_name>_hook`` and ``post_<command_name>_hook`` where\n    ``<command_name>`` is the name of a ``setup.py`` command (e.g. build_ext).\n\n    If either hook is present this adds a wrapped version of that command to\n    the passed in ``commands`` `dict`.  ``commands`` may be pre-populated with\n    other custom distutils command classes that should be wrapped if there are\n    hooks for them (e.g. `AstropyBuildPy`).\n    \"\"\"\n\n    hook_re = re.compile(r'^(pre|post)_(.+)_hook$')\n\n    # Distutils commands have a method of the same name, but it is not a\n    # *classmethod* (which probably didn't exist when distutils was first\n    # written)\n    def get_command_name(cmdcls):\n        if hasattr(cmdcls, 'command_name'):\n            return cmdcls.command_name\n        else:\n            return cmdcls.__name__\n\n    packages = find_packages(srcdir)\n    dist = get_dummy_distribution()\n\n    hooks = collections.defaultdict(dict)\n\n    for setuppkg in iter_setup_packages(srcdir, packages):\n        for name, obj in vars(setuppkg).items():\n            match = hook_re.match(name)\n            if not match:\n                continue\n\n            hook_type = match.group(1)\n            cmd_name = match.group(2)\n\n            if hook_type not in hooks[cmd_name]:\n                hooks[cmd_name][hook_type] = []\n\n            hooks[cmd_name][hook_type].append((setuppkg.__name__, obj))\n\n    for cmd_name, cmd_hooks in hooks.items():\n        commands[cmd_name] = generate_hooked_command(\n            cmd_name, dist.get_command_class(cmd_name), cmd_hooks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a new command that runs pre - and post - command hooks for that command before and after the cmd_cls.", "response": "def generate_hooked_command(cmd_name, cmd_cls, hooks):\n    \"\"\"\n    Returns a generated subclass of ``cmd_cls`` that runs the pre- and\n    post-command hooks for that command before and after the ``cmd_cls.run``\n    method.\n    \"\"\"\n\n    def run(self, orig_run=cmd_cls.run):\n        self.run_command_hooks('pre_hooks')\n        orig_run(self)\n        self.run_command_hooks('post_hooks')\n\n    return type(cmd_name, (cmd_cls, object),\n                {'run': run, 'run_command_hooks': run_command_hooks,\n                 'pre_hooks': hooks.get('pre', []),\n                 'post_hooks': hooks.get('post', [])})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning all the command hooks registered for that command and phase.", "response": "def run_command_hooks(cmd_obj, hook_kind):\n    \"\"\"Run hooks registered for that command and phase.\n\n    *cmd_obj* is a finalized command object; *hook_kind* is either\n    'pre_hook' or 'post_hook'.\n    \"\"\"\n\n    hooks = getattr(cmd_obj, hook_kind, None)\n\n    if not hooks:\n        return\n\n    for modname, hook in hooks:\n        if isinstance(hook, str):\n            try:\n                hook_obj = resolve_name(hook)\n            except ImportError as exc:\n                raise DistutilsModuleError(\n                    'cannot find hook {0}: {1}'.format(hook, exc))\n        else:\n            hook_obj = hook\n\n        if not callable(hook_obj):\n            raise DistutilsOptionError('hook {0!r} is not callable' % hook)\n\n        log.info('running {0} from {1} for {2} command'.format(\n                 hook_kind.rstrip('s'), modname, cmd_obj.get_command_name()))\n\n        try:\n            hook_obj(cmd_obj)\n        except Exception:\n            log.error('{0} command hook {1} raised an exception: %s\\n'.format(\n                hook_obj.__name__, cmd_obj.get_command_name()))\n            log.error(traceback.format_exc())\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the package_data and packagenames with the contents of the packages.", "response": "def update_package_files(srcdir, extensions, package_data, packagenames,\n                         package_dirs):\n    \"\"\"\n    This function is deprecated and maintained for backward compatibility\n    with affiliated packages.  Affiliated packages should update their\n    setup.py to use `get_package_info` instead.\n    \"\"\"\n\n    info = get_package_info(srcdir)\n    extensions.extend(info['ext_modules'])\n    package_data.update(info['package_data'])\n    packagenames = list(set(packagenames + info['packages']))\n    package_dirs.update(info['package_dir'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_package_info(srcdir='.', exclude=()):\n    ext_modules = []\n    packages = []\n    package_dir = {}\n\n    # Read in existing package data, and add to it below\n    setup_cfg = os.path.join(srcdir, 'setup.cfg')\n    if os.path.exists(setup_cfg):\n        conf = read_configuration(setup_cfg)\n        if 'options' in conf and 'package_data' in conf['options']:\n            package_data = conf['options']['package_data']\n        else:\n            package_data = {}\n    else:\n        package_data = {}\n\n    if exclude:\n        warnings.warn(\n            \"Use of the exclude parameter is no longer supported since it does \"\n            \"not work as expected. Use add_exclude_packages instead. Note that \"\n            \"it must be called prior to any other calls from setup helpers.\",\n            AstropyDeprecationWarning)\n\n    # Use the find_packages tool to locate all packages and modules\n    packages = find_packages(srcdir, exclude=exclude)\n\n    # Update package_dir if the package lies in a subdirectory\n    if srcdir != '.':\n        package_dir[''] = srcdir\n\n    # For each of the setup_package.py modules, extract any\n    # information that is needed to install them.  The build options\n    # are extracted first, so that their values will be available in\n    # subsequent calls to `get_extensions`, etc.\n    for setuppkg in iter_setup_packages(srcdir, packages):\n        if hasattr(setuppkg, 'get_build_options'):\n            options = setuppkg.get_build_options()\n            for option in options:\n                add_command_option('build', *option)\n        if hasattr(setuppkg, 'get_external_libraries'):\n            libraries = setuppkg.get_external_libraries()\n            for library in libraries:\n                add_external_library(library)\n\n    for setuppkg in iter_setup_packages(srcdir, packages):\n        # get_extensions must include any Cython extensions by their .pyx\n        # filename.\n        if hasattr(setuppkg, 'get_extensions'):\n            ext_modules.extend(setuppkg.get_extensions())\n        if hasattr(setuppkg, 'get_package_data'):\n            package_data.update(setuppkg.get_package_data())\n\n    # Locate any .pyx files not already specified, and add their extensions in.\n    # The default include dirs include numpy to facilitate numerical work.\n    ext_modules.extend(get_cython_extensions(srcdir, packages, ext_modules,\n                                             ['numpy']))\n\n    # Now remove extensions that have the special name 'skip_cython', as they\n    # exist Only to indicate that the cython extensions shouldn't be built\n    for i, ext in reversed(list(enumerate(ext_modules))):\n        if ext.name == 'skip_cython':\n            del ext_modules[i]\n\n    # On Microsoft compilers, we need to pass the '/MANIFEST'\n    # commandline argument.  This was the default on MSVC 9.0, but is\n    # now required on MSVC 10.0, but it doesn't seem to hurt to add\n    # it unconditionally.\n    if get_compiler_option() == 'msvc':\n        for ext in ext_modules:\n            ext.extra_link_args.append('/MANIFEST')\n\n    return {\n        'ext_modules': ext_modules,\n        'packages': packages,\n        'package_dir': package_dir,\n        'package_data': package_data,\n        }", "response": "Returns a dictionary of keyword arguments that can be passed directly to distutils. setup. py."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_setup_packages(srcdir, packages):\n\n    for packagename in packages:\n        package_parts = packagename.split('.')\n        package_path = os.path.join(srcdir, *package_parts)\n        setup_package = os.path.relpath(\n            os.path.join(package_path, 'setup_package.py'))\n\n        if os.path.isfile(setup_package):\n            module = import_file(setup_package,\n                                 name=packagename + '.setup_package')\n            yield module", "response": "A generator that yields all of the setup_package. py modules in the source packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_pyx_files(package_dir, package_name):\n    for dirpath, dirnames, filenames in walk_skip_hidden(package_dir):\n        for fn in filenames:\n            if fn.endswith('.pyx'):\n                fullfn = os.path.relpath(os.path.join(dirpath, fn))\n                # Package must match file name\n                extmod = '.'.join([package_name, fn[:-4]])\n                yield (extmod, fullfn)\n\n        break", "response": "Iterate over the. pyx files in the specified package directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cython_extensions(srcdir, packages, prevextensions=tuple(),\n                          extincludedirs=None):\n    \"\"\"\n    Looks for Cython files and generates Extensions if needed.\n\n    Parameters\n    ----------\n    srcdir : str\n        Path to the root of the source directory to search.\n    prevextensions : list of `~distutils.core.Extension` objects\n        The extensions that are already defined.  Any .pyx files already here\n        will be ignored.\n    extincludedirs : list of str or None\n        Directories to include as the `include_dirs` argument to the generated\n        `~distutils.core.Extension` objects.\n\n    Returns\n    -------\n    exts : list of `~distutils.core.Extension` objects\n        The new extensions that are needed to compile all .pyx files (does not\n        include any already in `prevextensions`).\n    \"\"\"\n\n    # Vanilla setuptools and old versions of distribute include Cython files\n    # as .c files in the sources, not .pyx, so we cannot simply look for\n    # existing .pyx sources in the previous sources, but we should also check\n    # for .c files with the same remaining filename. So we look for .pyx and\n    # .c files, and we strip the extension.\n    prevsourcepaths = []\n    ext_modules = []\n\n    for ext in prevextensions:\n        for s in ext.sources:\n            if s.endswith(('.pyx', '.c', '.cpp')):\n                sourcepath = os.path.realpath(os.path.splitext(s)[0])\n                prevsourcepaths.append(sourcepath)\n\n    for package_name in packages:\n        package_parts = package_name.split('.')\n        package_path = os.path.join(srcdir, *package_parts)\n\n        for extmod, pyxfn in iter_pyx_files(package_path, package_name):\n            sourcepath = os.path.realpath(os.path.splitext(pyxfn)[0])\n            if sourcepath not in prevsourcepaths:\n                ext_modules.append(Extension(extmod, [pyxfn],\n                                             include_dirs=extincludedirs))\n\n    return ext_modules", "response": "Returns a list of extensions that can be used to compile all. pyx files in the source directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing pkg - config to update distutils. Extension arguments for pkg - config packages.", "response": "def pkg_config(packages, default_libraries, executable='pkg-config'):\n    \"\"\"\n    Uses pkg-config to update a set of distutils Extension arguments\n    to include the flags necessary to link against the given packages.\n\n    If the pkg-config lookup fails, default_libraries is applied to\n    libraries.\n\n    Parameters\n    ----------\n    packages : list of str\n        A list of pkg-config packages to look up.\n\n    default_libraries : list of str\n        A list of library names to use if the pkg-config lookup fails.\n\n    Returns\n    -------\n    config : dict\n        A dictionary containing keyword arguments to\n        `distutils.Extension`.  These entries include:\n\n        - ``include_dirs``: A list of include directories\n        - ``library_dirs``: A list of library directories\n        - ``libraries``: A list of libraries\n        - ``define_macros``: A list of macro defines\n        - ``undef_macros``: A list of macros to undefine\n        - ``extra_compile_args``: A list of extra arguments to pass to\n          the compiler\n    \"\"\"\n\n    flag_map = {'-I': 'include_dirs', '-L': 'library_dirs', '-l': 'libraries',\n                '-D': 'define_macros', '-U': 'undef_macros'}\n    command = \"{0} --libs --cflags {1}\".format(executable, ' '.join(packages)),\n\n    result = DistutilsExtensionArgs()\n\n    try:\n        pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n        output = pipe.communicate()[0].strip()\n    except subprocess.CalledProcessError as e:\n        lines = [\n            (\"{0} failed. This may cause the build to fail below.\"\n             .format(executable)),\n            \"  command: {0}\".format(e.cmd),\n            \"  returncode: {0}\".format(e.returncode),\n            \"  output: {0}\".format(e.output)\n            ]\n        log.warn('\\n'.join(lines))\n        result['libraries'].extend(default_libraries)\n    else:\n        if pipe.returncode != 0:\n            lines = [\n                \"pkg-config could not lookup up package(s) {0}.\".format(\n                    \", \".join(packages)),\n                \"This may cause the build to fail below.\"\n                ]\n            log.warn('\\n'.join(lines))\n            result['libraries'].extend(default_libraries)\n        else:\n            for token in output.split():\n                # It's not clear what encoding the output of\n                # pkg-config will come to us in.  It will probably be\n                # some combination of pure ASCII (for the compiler\n                # flags) and the filesystem encoding (for any argument\n                # that includes directories or filenames), but this is\n                # just conjecture, as the pkg-config documentation\n                # doesn't seem to address it.\n                arg = token[:2].decode('ascii')\n                value = token[2:].decode(sys.getfilesystemencoding())\n                if arg in flag_map:\n                    if arg == '-D':\n                        value = tuple(value.split('=', 1))\n                    result[flag_map[arg]].append(value)\n                else:\n                    result['extra_compile_args'].append(value)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a build option for selecting the internal or system copy of a library.", "response": "def add_external_library(library):\n    \"\"\"\n    Add a build option for selecting the internal or system copy of a library.\n\n    Parameters\n    ----------\n    library : str\n        The name of the library.  If the library is `foo`, the build\n        option will be called `--use-system-foo`.\n    \"\"\"\n\n    for command in ['build', 'build_ext', 'install']:\n        add_command_option(command, str('use-system-' + library),\n                           'Use the system {0} library'.format(library),\n                           is_bool=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the previously used Cython version or unknown if Cython is installed.", "response": "def should_build_with_cython(previous_cython_version, is_release):\n    \"\"\"\n    Returns the previously used Cython version (or 'unknown' if not\n    previously built) if Cython should be used to build extension modules from\n    pyx files.\n    \"\"\"\n\n    # Only build with Cython if, of course, Cython is installed, we're in a\n    # development version (i.e. not release) or the Cython-generated source\n    # files haven't been created yet (cython_version == 'unknown'). The latter\n    # case can happen even when release is True if checking out a release tag\n    # from the repository\n    have_cython = False\n    try:\n        from Cython import __version__ as cython_version  # noqa\n        have_cython = True\n    except ImportError:\n        pass\n\n    if have_cython and (not is_release or previous_cython_version == 'unknown'):\n        return cython_version\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_cython_sources(self, extension):\n\n        # Determine the compiler we'll be using\n        if self.compiler is None:\n            compiler = get_default_compiler()\n        else:\n            compiler = self.compiler\n\n        # Replace .pyx with C-equivalents, unless c files are missing\n        for jdx, src in enumerate(extension.sources):\n            base, ext = os.path.splitext(src)\n            pyxfn = base + '.pyx'\n            cfn = base + '.c'\n            cppfn = base + '.cpp'\n\n            if not os.path.isfile(pyxfn):\n                continue\n\n            if self._uses_cython:\n                extension.sources[jdx] = pyxfn\n            else:\n                if os.path.isfile(cfn):\n                    extension.sources[jdx] = cfn\n                elif os.path.isfile(cppfn):\n                    extension.sources[jdx] = cppfn\n                else:\n                    msg = (\n                        'Could not find C/C++ file {0}.(c/cpp) for Cython '\n                        'file {1} when building extension {2}. Cython '\n                        'must be installed to build from a git '\n                        'checkout.'.format(base, pyxfn, extension.name))\n                    raise IOError(errno.ENOENT, msg, cfn)\n\n            # Cython (at least as of 0.29.2) uses deprecated Numpy API features\n            # the use of which produces a few warnings when compiling.\n            # These additional flags should squelch those warnings.\n            # TODO: Feel free to remove this if/when a Cython update\n            # removes use of the deprecated Numpy API\n            if compiler == 'unix':\n                extension.extra_compile_args.extend([\n                    '-Wp,-w', '-Wno-unused-function'])", "response": "Check that the C ++ files associated with. pyx and. c files are present and that they are C - equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_flag_value_from_var(flag, var, delim=' '):\n\n    if sys.platform.startswith('win'):\n        return None\n\n    # Simple input validation\n    if not var or not flag:\n        return None\n    flag_length = len(flag)\n    if not flag_length:\n        return None\n\n    # Look for var in os.eviron then in get_config_var\n    if var in os.environ:\n        flags = os.environ[var]\n    else:\n        try:\n            flags = get_config_var(var)\n        except KeyError:\n            return None\n\n    # Extract flag from {var:value}\n    if flags:\n        for item in flags.split(delim):\n            if item.startswith(flag):\n                return item[flag_length:]", "response": "Returns the value of a flag from an environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_openmp_flags():\n\n    compile_flags = []\n    link_flags = []\n\n    if get_compiler_option() == 'msvc':\n        compile_flags.append('-openmp')\n    else:\n\n        include_path = _get_flag_value_from_var('-I', 'CFLAGS')\n        if include_path:\n            compile_flags.append('-I' + include_path)\n\n        lib_path = _get_flag_value_from_var('-L', 'LDFLAGS')\n        if lib_path:\n            link_flags.append('-L' + lib_path)\n            link_flags.append('-Wl,-rpath,' + lib_path)\n\n        compile_flags.append('-fopenmp')\n        link_flags.append('-fopenmp')\n\n    return {'compiler_flags': compile_flags, 'linker_flags': link_flags}", "response": "Utility for returning compiler and linker flags possibly needed for\n    OpenMP support."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking whether OpenMP test code can be compiled and run.", "response": "def check_openmp_support(openmp_flags=None):\n    \"\"\"\n    Check whether OpenMP test code can be compiled and run.\n\n    Parameters\n    ----------\n    openmp_flags : dict, optional\n        This should be a dictionary with keys ``compiler_flags`` and\n        ``linker_flags`` giving the compiliation and linking flags respectively.\n        These are passed as `extra_postargs` to `compile()` and\n        `link_executable()` respectively. If this is not set, the flags will\n        be automatically determined using environment variables.\n\n    Returns\n    -------\n    result : bool\n        `True` if the test passed, `False` otherwise.\n    \"\"\"\n\n    ccompiler = new_compiler()\n    customize_compiler(ccompiler)\n\n    if not openmp_flags:\n        # customize_compiler() extracts info from os.environ. If certain keys\n        # exist it uses these plus those from sysconfig.get_config_vars().\n        # If the key is missing in os.environ it is not extracted from\n        # sysconfig.get_config_var(). E.g. 'LDFLAGS' get left out, preventing\n        # clang from finding libomp.dylib because -L<path> is not passed to\n        # linker. Call get_openmp_flags() to get flags missed by\n        # customize_compiler().\n        openmp_flags = get_openmp_flags()\n\n    compile_flags = openmp_flags.get('compiler_flags')\n    link_flags = openmp_flags.get('linker_flags')\n\n    # Pass -coverage flag to linker.\n    # https://github.com/astropy/astropy-helpers/pull/374\n    if '-coverage' in compile_flags and '-coverage' not in link_flags:\n        link_flags.append('-coverage')\n\n    tmp_dir = tempfile.mkdtemp()\n    start_dir = os.path.abspath('.')\n\n    try:\n        os.chdir(tmp_dir)\n\n        # Write test program\n        with open('test_openmp.c', 'w') as f:\n            f.write(CCODE)\n\n        os.mkdir('objects')\n\n        # Compile, test program\n        ccompiler.compile(['test_openmp.c'], output_dir='objects',\n                          extra_postargs=compile_flags)\n\n        # Link test program\n        objects = glob.glob(os.path.join('objects', '*' + ccompiler.obj_extension))\n        ccompiler.link_executable(objects, 'test_openmp',\n                                  extra_postargs=link_flags)\n\n        # Run test program\n        output = subprocess.check_output('./test_openmp')\n        output = output.decode(sys.stdout.encoding or 'utf-8').splitlines()\n\n        if 'nthreads=' in output[0]:\n            nthreads = int(output[0].strip().split('=')[1])\n            if len(output) == nthreads:\n                is_openmp_supported = True\n            else:\n                log.warn(\"Unexpected number of lines from output of test OpenMP \"\n                         \"program (output was {0})\".format(output))\n                is_openmp_supported = False\n        else:\n            log.warn(\"Unexpected output from test OpenMP \"\n                     \"program (output was {0})\".format(output))\n            is_openmp_supported = False\n    except (CompileError, LinkError, subprocess.CalledProcessError):\n        is_openmp_supported = False\n\n    finally:\n        os.chdir(start_dir)\n\n    return is_openmp_supported"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_openmp_supported():\n    log_threshold = log.set_threshold(log.FATAL)\n    ret = check_openmp_support()\n    log.set_threshold(log_threshold)\n    return ret", "response": "Determines whether the build compiler has OpenMP support."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds OpenMP compilation flags if supported.", "response": "def add_openmp_flags_if_available(extension):\n    \"\"\"\n    Add OpenMP compilation flags, if supported (if not a warning will be\n    printed to the console and no flags will be added.)\n\n    Returns `True` if the flags were added, `False` otherwise.\n    \"\"\"\n\n    if _ASTROPY_DISABLE_SETUP_WITH_OPENMP_:\n        log.info(\"OpenMP support has been explicitly disabled.\")\n        return False\n\n    openmp_flags = get_openmp_flags()\n    using_openmp = check_openmp_support(openmp_flags=openmp_flags)\n\n    if using_openmp:\n        compile_flags = openmp_flags.get('compiler_flags')\n        link_flags = openmp_flags.get('linker_flags')\n        log.info(\"Compiling Cython/C/C++ extension with OpenMP support\")\n        extension.extra_compile_args.extend(compile_flags)\n        extension.extra_link_args.extend(link_flags)\n    else:\n        log.warn(\"Cannot compile Cython/C/C++ extension with OpenMP, reverting \"\n                 \"to non-parallel code\")\n\n    return using_openmp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_openmp_enabled_py(packagename, srcdir='.', disable_openmp=None):\n\n    if packagename.lower() == 'astropy':\n        packagetitle = 'Astropy'\n    else:\n        packagetitle = packagename\n\n    epoch = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))\n    timestamp = datetime.datetime.utcfromtimestamp(epoch)\n\n    if disable_openmp is not None:\n        import builtins\n        builtins._ASTROPY_DISABLE_SETUP_WITH_OPENMP_ = disable_openmp\n    if _ASTROPY_DISABLE_SETUP_WITH_OPENMP_:\n        log.info(\"OpenMP support has been explicitly disabled.\")\n    openmp_support = False if _ASTROPY_DISABLE_SETUP_WITH_OPENMP_ else is_openmp_supported()\n\n    src = _IS_OPENMP_ENABLED_SRC.format(packagetitle=packagetitle,\n                                        timestamp=timestamp,\n                                        return_bool=openmp_support)\n\n    package_srcdir = os.path.join(srcdir, *packagename.split('.'))\n    is_openmp_enabled_py = os.path.join(package_srcdir, 'openmp_enabled.py')\n    with open(is_openmp_enabled_py, 'w') as f:\n        f.write(src)", "response": "Generate a new package. openmp_enabled. is_openmp_enabled. py file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value of a key from a pandas DataFrame.", "response": "def GetValue(self, row, col):\n        \"\"\"\n        Find the matching value from pandas DataFrame,\n        return it.\n        \"\"\"\n        if len(self.dataframe):\n            return str(self.dataframe.iloc[row, col])\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetValue(self, row, col, value):\n        self.dataframe.iloc[row, col] = value", "response": "Set the value in the pandas DataFrame\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SetColumnValues(self, col, value):\n        try:\n            self.dataframe.iloc[:, col] = value\n        except ValueError:\n            self.dataframe.loc[:, col] = value", "response": "Set all values in a column to a list - like value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting col label from dataframe", "response": "def GetColLabelValue(self, col):\n        \"\"\"\n        Get col label from dataframe\n        \"\"\"\n        if len(self.dataframe):\n            return self.dataframe.columns[col]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting col label value in dataframe", "response": "def SetColLabelValue(self, col, value):\n        \"\"\"\n        Set col label value in dataframe\n        \"\"\"\n        if len(self.dataframe):\n            col_name = str(self.dataframe.columns[col])\n            self.dataframe.rename(columns={col_name: str(value)}, inplace=True)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting to always have vertical scrollbar. Have horizontal scrollbar unless grid has very few rows. Older versions of wxPython will choke on this, in which case nothing happens.", "response": "def set_scrollbars(self):\n        \"\"\"\n        Set to always have vertical scrollbar.\n        Have horizontal scrollbar unless grid has very few rows.\n        Older versions of wxPython will choke on this,\n        in which case nothing happens.\n        \"\"\"\n        try:\n            if len(self.row_labels) < 5:\n                show_horizontal = wx.SHOW_SB_NEVER\n            else:\n                show_horizontal = wx.SHOW_SB_DEFAULT\n            self.ShowScrollbars(show_horizontal, wx.SHOW_SB_DEFAULT)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding items and update existing items in grid", "response": "def add_items(self, dataframe, hide_cols=()):\n        \"\"\"\n        Add items and/or update existing items in grid\n        \"\"\"\n        # replace \"None\" values with \"\"\n        dataframe = dataframe.fillna(\"\")\n        # remove any columns that shouldn't be shown\n        for col in hide_cols:\n            if col in dataframe.columns:\n                del dataframe[col]\n        # add more rows\n        self.AppendRows(len(dataframe))\n        columns = dataframe.columns\n        row_num = -1\n        # fill in all rows with appropriate values\n        for ind, row in dataframe.iterrows():\n            row_num += 1\n            for col_num, col in enumerate(columns):\n                value = row[col]\n                self.SetCellValue(row_num, col_num, str(value))\n                # set citation default value\n                if col == 'citations':\n                    citation = row['citations']\n                    if (citation is None) or (citation is np.nan):\n                            self.SetCellValue(row_num, col_num, 'This study')\n                    else:\n                        if 'This study' not in citation:\n                            if len(citation):\n                                citation += ':'\n                            citation += 'This study'\n                            self.SetCellValue(row_num, col_num, citation)\n        self.row_labels.extend(dataframe.index)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_items(self, rows=None, verbose=False):\n        if rows:\n            rows = rows\n        else:\n            rows = list(range(self.GetNumberRows()))\n        cols = list(range(self.GetNumberCols()))\n        data = {}\n        for row in rows:\n            data[row] = {}\n            for col in cols:\n                col_name = self.GetColLabelValue(col)\n                if verbose:\n                    print(col_name, \":\", self.GetCellValue(row, col))\n                data[row][col_name] = self.GetCellValue(row, col)\n        return data", "response": "Save the items of the current table in a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting self. changes to true when user edits the grid", "response": "def on_edit_grid(self, event):\n        \"\"\"sets self.changes to true when user edits the grid.\n        provides down and up key functionality for exiting the editor\"\"\"\n        if not self.changes:\n            self.changes = {event.Row}\n        else:\n            self.changes.add(event.Row)\n        #self.changes = True\n        try:\n            editor = event.GetControl()\n            editor.Bind(wx.EVT_KEY_DOWN, self.onEditorKey)\n        except AttributeError:\n            # if it's a EVT_GRID_EDITOR_SHOWN, it doesn't have the GetControl method\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_paste(self, event):\n        # find where the user has clicked\n        col_ind = self.GetGridCursorCol()\n        row_ind = self.GetGridCursorRow()\n        # read in clipboard text\n        text_df = pd.read_clipboard(header=None, sep='\\t').fillna('')\n        # add extra rows if need to accomadate clipboard text\n        row_length_diff = len(text_df) - (len(self.row_labels) - row_ind)\n        if row_length_diff > 0:\n            for n in range(row_length_diff):\n                self.add_row()\n        # ignore excess columns if present\n        col_length_diff = len(text_df.columns) - (len(self.col_labels) - col_ind)\n        if col_length_diff > 0:\n            text_df = text_df.iloc[:, :-col_length_diff].copy()\n        # go through copied text and parse it into the grid rows\n        for label, row_data in text_df.iterrows():\n            col_range = list(range(col_ind, col_ind + len(row_data)))\n            if len(row_data) > 1:\n                cols = list(zip(col_range, row_data.index))\n                for column in cols:\n                    value = row_data[column[1]]\n                    this_col = column[0]\n                    self.SetCellValue(row_ind, this_col, str(value))\n            else:\n                value = row_data[0]\n                self.SetCellValue(row_ind, col_ind, str(value))\n            row_ind += 1\n        # could instead use wxPython clipboard here\n        # see old git history for that\n        self.size_grid()\n        event.Skip()", "response": "Read clipboard into dataframe and parse it into grid rows and columns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_changes_after_row_delete(self, row_num):\n        if row_num in self.changes.copy():\n            self.changes.remove(row_num)\n        updated_rows = []\n        for changed_row in self.changes:\n            if changed_row == -1:\n                updated_rows.append(-1)\n            if changed_row > row_num:\n                updated_rows.append(changed_row - 1)\n            if changed_row < row_num:\n                updated_rows.append(changed_row)\n        self.changes = set(updated_rows)", "response": "Update self. changes so that the row numbers for deleted rows are still correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paint_invalid_cell(self, row, col, color='MEDIUM VIOLET RED',\n                           skip_cell=False):\n        \"\"\"\n        Take row, column, and turn it color\n        \"\"\"\n        self.SetColLabelRenderer(col, MyColLabelRenderer('#1101e0'))\n        # SetCellRenderer doesn't work with table-based grid (HugeGrid class)\n        if not skip_cell:\n            self.SetCellRenderer(row, col, MyCustomRenderer(color))", "response": "This method is used to paint an invalid cell in the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a column to the table dataframe and returns the index of the new column.", "response": "def add_col(self, label):\n        \"\"\"\n        Update table dataframe, and append a new column\n\n        Parameters\n        ----------\n        label : str\n\n        Returns\n        ---------\n        last_col: int\n            index column number of added col\n        \"\"\"\n        self.table.dataframe[label] = ''\n        self.AppendCols(1, updateLabels=False)\n        last_col = self.table.GetNumberCols() - 1\n        self.SetColLabelValue(last_col, label)\n        self.col_labels.append(label)\n        self.size_grid()\n        return last_col"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a column from the table dataframe and resize grid to display correctly", "response": "def remove_col(self, col_num):\n        \"\"\"\n        update table dataframe, and remove a column.\n        resize grid to display correctly\n        \"\"\"\n        label_value = self.GetColLabelValue(col_num).strip('**').strip('^^')\n        self.col_labels.remove(label_value)\n        del self.table.dataframe[label_value]\n        result = self.DeleteCols(pos=col_num, numCols=1, updateLabels=True)\n        self.size_grid()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnames is the main function for the high level function plotdi_a. py", "response": "def main():\n    \"\"\"\n    NAME\n       plotdi_a.py\n\n    DESCRIPTION\n       plots equal area projection  from dec inc data and fisher mean, cone of confidence\n\n    INPUT FORMAT\n       takes dec, inc, alpha95 as first three columns in space delimited file\n\n    SYNTAX\n       plotdi_a.py [-i][-f FILE]\n\n    OPTIONS\n        -f FILE to read file name from command line\n        -fmt [png,jpg,eps,pdf,svg] set plot file format ['svg' is default]\n        -sav save plot and quit\n\n    \"\"\"\n    fmt,plot='svg',0\n    if len(sys.argv) > 0:\n        if '-h' in sys.argv: # check if help is needed\n            print(main.__doc__)\n            sys.exit() # graceful quit\n        if '-fmt' in sys.argv:\n            ind=sys.argv.index('-fmt')\n            fmt=sys.argv[ind+1]\n        if '-sav' in sys.argv:plot=1\n        if '-f' in sys.argv:\n            ind=sys.argv.index('-f')\n            file=sys.argv[ind+1]\n            f=open(file,'r')\n            data=f.readlines()\n        else:\n            data=sys.stdin.readlines() # read in data from standard input\n    DIs,Pars=[],[]\n    for line in data:   # read in the data from standard input\n        pars=[]\n        rec=line.split() # split each line on space to get records\n        DIs.append([float(rec[0]),float(rec[1])])\n        pars.append(float(rec[0]))\n        pars.append(float(rec[1]))\n        pars.append(float(rec[2]))\n        pars.append(float(rec[0]))\n        isign=abs(float(rec[1])) / float(rec[1])\n        pars.append(float(rec[1])-isign*90.) #Beta inc\n        pars.append(float(rec[2])) # gamma\n        pars.append(float(rec[0])+90.) # Beta dec\n        pars.append(0.) #Beta inc\n        Pars.append(pars)\n#\n    EQ={'eq':1} # make plot dictionary\n    pmagplotlib.plot_init(EQ['eq'],5,5)\n    title='Equal area projection'\n    pmagplotlib.plot_eq(EQ['eq'],DIs,title)# plot directions\n    for k in range(len(Pars)):\n        pmagplotlib.plot_ell(EQ['eq'],Pars[k],'b',0,1) # plot ellipses\n    files={}\n    for key in list(EQ.keys()):\n        files[key]=key+'.'+fmt\n    titles={}\n    titles['eq']='Equal Area Plot'\n    if pmagplotlib.isServer:\n        black     = '#000000'\n        purple    = '#800080'\n        EQ = pmagplotlib.add_borders(EQ,titles,black,purple)\n        pmagplotlib.save_plots(EQ,files)\n    elif plot==0:\n        pmagplotlib.draw_figs(EQ)\n        ans=input(\" S[a]ve to save plot, [q]uit, Return to continue:  \")\n        if ans==\"q\": sys.exit()\n        if ans==\"a\":\n            pmagplotlib.save_plots(EQ,files)\n    else:\n        pmagplotlib.save_plots(EQ,files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    D,I=0.,90.\n    outfile=\"\"\n    infile=\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        infile=sys.argv[ind+1]\n        data=numpy.loadtxt(infile)\n    else:\n        data=numpy.loadtxt(sys.stdin,dtype=numpy.float)\n    if '-F' in sys.argv:\n        ind=sys.argv.index('-F')\n        outfile=sys.argv[ind+1]\n        out=open(outfile,'w')\n    if '-D' in sys.argv:\n        ind=sys.argv.index('-D')\n        D=float(sys.argv[ind+1])\n    if '-I' in sys.argv:\n        ind=sys.argv.index('-I')\n        I=float(sys.argv[ind+1])\n    if len(data.shape)>1: # 2-D array\n        N=data.shape[0] \n        DipDir,Dip=numpy.ones(N,dtype=numpy.float).transpose()*(D-180.),numpy.ones(N,dtype=numpy.float).transpose()*(90.-I)\n        data=data.transpose()\n        data=numpy.array([data[0],data[1],DipDir ,Dip]).transpose()\n        drot,irot=pmag.dotilt_V(data)\n        drot=(drot-180.)%360.  # \n        for k in range(N): \n             if outfile==\"\":\n                print('%7.1f %7.1f ' % (drot[k],irot[k]))\n             else:\n                out.write('%7.1f %7.1f\\n' % (drot[k],irot[k]))\n    else: \n        d,i=pmag.dotilt(data[0],data[1],(D-180.),90.-I)\n        if outfile==\"\":\n            print('%7.1f %7.1f ' % ((d-180.)%360.,i))\n        else:\n            out.write('%7.1f %7.1f\\n' % ((d-180.)%360.,i))", "response": "NAME\n        di_rot.py\n\n    DESCRIPTION\n        rotates set of directions to new coordinate system\n\n    SYNTAX\n        di_rot.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f specify input file, default is standard input\n        -F specify output file, default is standard output\n        -D D specify  Dec of new coordinate system, default is 0\n        -I I specify  Inc of new coordinate system, default is 90\n    INTPUT/OUTPUT\n        dec  inc   [space delimited]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nname _2g_bin_magic.py DESCRIPTION takes the binary 2g format magnetometer files and converts them to magic_measurements, er_samples.txt and er_sites.txt file SYNTAX 2g_bin_magic.py [command line options] OPTIONS -f FILE: specify input 2g (binary) file -F FILE: specify magic_measurements output file, default is: magic_measurements.txt -Fsa FILE: specify output file, default is: er_samples.txt -Fsi FILE: specify output file, default is: er_sites.txt -ncn NCON: specify naming convention: default is #2 below -ocn OCON: specify orientation convention, default is #5 below -mcd: specify sampling method codes as a colon delimited string: [default is: FS-FD:SO-POM] FS-FD field sampling done with a drill FS-H field sampling done with hand samples FS-LOC-GPS field location done with GPS FS-LOC-MAP field location done with map SO-POM a Pomeroy orientation device was used SO-ASC an ASC orientation device was used SO-MAG orientation with magnetic compass SO-SUN orientation with sun compass -loc: location name, default=\"unknown\" -spc NUM : specify number of characters to designate a specimen, default = 0 -ins INST : specify instsrument name -a: average replicate measurements INPUT FORMAT Input files are horrible mag binary format (who knows why?) Orientation convention: [1] Lab arrow azimuth= mag_azimuth; Lab arrow dip=-field_dip i.e., field_dip is degrees from vertical down - the hade [default] [2] Lab arrow azimuth = mag_azimuth-90; Lab arrow dip = -field_dip i.e., mag_azimuth is strike and field_dip is hade [3] Lab arrow azimuth = mag_azimuth; Lab arrow dip = 90-field_dip i.e., lab arrow same as field arrow, but field_dip was a hade. [4] lab azimuth and dip are same as mag_azimuth, field_dip [5] lab azimuth is same as mag_azimuth,lab arrow dip=field_dip-90 [6] Lab arrow azimuth = mag_azimuth-90; Lab arrow dip = 90-field_dip [7] all others you will have to either customize your self or e-mail ltauxe@ucsd.edu for help. Magnetic declination convention: Az will use supplied declination to correct azimuth Sample naming convention: [1] XXXXY: where XXXX is an arbitrary length site designation and Y is the single character sample designation. e.g., TG001a is the first sample from site TG001. [default] [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length) [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length) [4-Z] XXXX[YYY]: YYY is sample designation with Z characters from site XXX [5] site name = sample name [6] site name entered in site_name column in the orient.txt format input file -- NOT CURRENTLY SUPPORTED [7-Z] [XXX]YYY: XXX is site designation with Z characters from samples XXXYYY NB: all others you will have to either customize your self or e-mail ltauxe@ucsd.edu for help. OUTPUT output saved in magic_measurements.txt & er_samples.txt formatted files will overwrite any existing files", "response": "def main(command_line=True, **kwargs):\n    \"\"\"\n    NAME\n        _2g_bin_magic.py\n\n    DESCRIPTION\n        takes the binary 2g format magnetometer files and converts them to magic_measurements, er_samples.txt and er_sites.txt file\n\n    SYNTAX\n        2g_bin_magic.py [command line options]\n\n    OPTIONS\n        -f FILE: specify input 2g (binary) file\n        -F FILE: specify magic_measurements output file, default is: magic_measurements.txt\n        -Fsa FILE: specify output file, default is: er_samples.txt\n        -Fsi FILE: specify output file, default is: er_sites.txt\n        -ncn NCON:  specify naming convention: default is #2 below\n        -ocn OCON:  specify orientation convention, default is #5 below\n        -mcd: specify sampling method codes as a colon delimited string:  [default is: FS-FD:SO-POM]\n             FS-FD field sampling done with a drill\n             FS-H field sampling done with hand samples\n             FS-LOC-GPS  field location done with GPS\n             FS-LOC-MAP  field location done with map\n             SO-POM   a Pomeroy orientation device was used\n             SO-ASC   an ASC orientation device was used\n             SO-MAG   orientation with magnetic compass\n             SO-SUN   orientation with sun compass\n        -loc: location name, default=\"unknown\"\n        -spc NUM : specify number of characters to designate a  specimen, default = 0\n        -ins INST : specify instsrument name\n        -a: average replicate measurements\n\n    INPUT FORMAT\n        Input files are horrible mag binary format (who knows why?)\n        Orientation convention:\n            [1] Lab arrow azimuth= mag_azimuth; Lab arrow dip=-field_dip\n                i.e., field_dip is degrees from vertical down - the hade [default]\n            [2] Lab arrow azimuth = mag_azimuth-90; Lab arrow dip = -field_dip\n                i.e., mag_azimuth is strike and field_dip is hade\n            [3] Lab arrow azimuth = mag_azimuth; Lab arrow dip = 90-field_dip\n                i.e.,  lab arrow same as field arrow, but field_dip was a hade.\n            [4] lab azimuth and dip are same as mag_azimuth, field_dip\n            [5] lab azimuth is same as mag_azimuth,lab arrow dip=field_dip-90\n            [6] Lab arrow azimuth = mag_azimuth-90; Lab arrow dip = 90-field_dip\n            [7] all others you will have to either customize your\n                self or e-mail ltauxe@ucsd.edu for help.\n\n         Magnetic declination convention:\n             Az will use supplied declination to correct azimuth\n\n       Sample naming convention:\n        [1] XXXXY: where XXXX is an arbitrary length site designation and Y\n            is the single character sample designation.  e.g., TG001a is the\n            first sample from site TG001.    [default]\n        [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length)\n        [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length)\n        [4-Z] XXXX[YYY]:  YYY is sample designation with Z characters from site XXX\n        [5] site name = sample name\n        [6] site name entered in site_name column in the orient.txt format input file  -- NOT CURRENTLY SUPPORTED\n        [7-Z] [XXX]YYY:  XXX is site designation with Z characters from samples  XXXYYY\n        NB: all others you will have to either customize your\n            self or e-mail ltauxe@ucsd.edu for help.\n\n    OUTPUT\n            output saved in magic_measurements.txt & er_samples.txt formatted files\n              will overwrite any existing files\n    \"\"\"\n    #\n    # initialize variables\n    #\n    mag_file = ''\n    specnum = 0\n    ub_file, samp_file, or_con, corr, meas_file = \"\", \"er_samples.txt\", \"3\", \"1\", \"magic_measurements.txt\"\n    pos_file, site_file = \"\", \"er_sites.txt\"\n    noave = 1\n    args = sys.argv\n    bed_dip, bed_dip_dir = \"\", \"\"\n    samp_con, Z, average_bedding = \"2\", 1, \"0\"\n    meths = 'FS-FD'\n    sclass, lithology, _type = \"\", \"\", \"\"\n    user, inst = \"\", \"\"\n    DecCorr = 0.\n    location_name = \"unknown\"\n    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    gmeths = \"\"\n    #\n    #\n    dir_path = '.'\n    if command_line:\n        if '-WD' in args:\n            ind = args.index(\"-WD\")\n            dir_path = sys.argv[ind + 1]\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if \"-f\" in args:\n            ind = args.index(\"-f\")\n            mag_file = sys.argv[ind + 1]\n        if \"-fpos\" in args:\n            ind = args.index(\"-fpos\")\n            pos_file = sys.argv[ind + 1]\n        if \"-F\" in args:\n            ind = args.index(\"-F\")\n            meas_file = sys.argv[ind + 1]\n        if \"-Fsa\" in args:\n            ind = args.index(\"-Fsa\")\n            samp_file = sys.argv[ind + 1]\n        if \"-Fsi\" in args:\n            ind = args.index(\"-Fsi\")\n            site_file = sys.argv[ind + 1]\n        if \"-ocn\" in args:\n            ind = args.index(\"-ocn\")\n            or_con = sys.argv[ind + 1]\n        if \"-ncn\" in args:\n            ind = args.index(\"-ncn\")\n            samp_con = sys.argv[ind + 1]\n        if \"-mcd\" in args:\n            ind = args.index(\"-mcd\")\n            gmeths = (sys.argv[ind + 1])\n        if \"-loc\" in args:\n            ind = args.index(\"-loc\")\n            location_name = (sys.argv[ind + 1])\n        if \"-spc\" in args:\n            ind = args.index(\"-spc\")\n            specnum = int(args[ind + 1])\n\n        if \"-ins\" in args:\n            ind = args.index(\"-ins\")\n            inst = args[ind + 1]\n        if \"-a\" in args:\n            noave = 0\n        #\n        ID = False\n        if '-ID' in args:\n            ind = args.index('-ID')\n            ID = args[ind + 1]\n        #\n\n    if not command_line:\n        dir_path = kwargs.get('dir_path', '.')\n        mag_file = kwargs.get('mag_file', '')\n        pos_file = kwargs.get('pos_file', '')\n        meas_file = kwargs.get('meas_file', 'magic_measurements.txt')\n        samp_file = kwargs.get('samp_file', 'er_samples.txt')\n        site_file = kwargs.get('site_file', 'er_sites.txt')\n        or_con = kwargs.get('or_con', '3')\n        samp_con = kwargs.get('samp_con', '2')\n        corr = kwargs.get('corr', '1')\n        gmeths = kwargs.get('gmeths', '')\n        location_name = kwargs.get('location_name', '')\n        specnum = int(kwargs.get('specnum', 0))\n        inst = kwargs.get('inst', '')\n        noave = kwargs.get('noave', 1)  # default is DO average\n        ID = kwargs.get('ID', '')\n\n    # format and fix variables acquired from command line args or input with\n    # **kwargs\n    if specnum != 0:\n        specnum = -specnum\n\n    if ID:\n        input_dir_path = ID\n    else:\n        input_dir_path = dir_path\n\n    if samp_con:\n        if \"4\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [4] must be in form 4-Z where Z is an integer\")\n                return False, \"option [4] must be in form 4-Z where Z is an integer\"\n            else:\n                Z = samp_con.split(\"-\")[1]\n                samp_con = \"4\"\n        if \"7\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                return False, \"option [7] must be in form 7-Z where Z is an integer\"\n            else:\n                Z = samp_con.split(\"-\")[1]\n                samp_con = \"7\"\n        if \"6\" in samp_con:\n            print('Naming convention option [6] not currently supported')\n            return False, 'Naming convention option [6] not currently supported'\n            try:\n                Samps, file_type = pmag.magic_read(\n                    os.path.join(input_dir_path, 'er_samples.txt'))\n            except:\n                print(\n                    \"there is no er_samples.txt file in your input directory - you can't use naming convention #6\")\n                return False, \"there is no er_samples.txt file in your input directory - you can't use naming convention #6\"\n            if file_type == 'bad_file':\n                print(\n                    \"there is no er_samples.txt file in your input directory - you can't use naming convention #6\")\n                return False, \"there is no er_samples.txt file in your input directory - you can't use naming convention #6\"\n\n    if not mag_file:\n        print(\"mag file is required input\")\n        return False, \"mag file is required input\"\n    output_dir_path = dir_path\n    mag_file = os.path.join(input_dir_path, mag_file)\n    samp_file = output_dir_path + '/' + samp_file\n    site_file = output_dir_path + '/' + site_file\n    meas_file = output_dir_path + '/' + meas_file\n    samplist = []\n    try:\n        Samps, file_type = pmag.magic_read(samp_file)\n        for samp in Samps:\n            if samp['er_sample_name'] not in samplist:\n                samplist.append(samp['er_sample_name'])\n    except:\n        Samps = []\n    MagRecs = []\n    try:\n        f = open(mag_file, 'br')\n        input = str(f.read()).strip(\"b '\")\n        f.close()\n    except Exception as ex:\n        print('ex', ex)\n        print(\"bad mag file\")\n        return False, \"bad mag file\"\n    firstline, date = 1, \"\"\n    d = input.split('\\\\xcd')\n    for line in d:\n        rec = line.split('\\\\x00')\n        if firstline == 1:\n            firstline = 0\n            spec, vol = \"\", 1\n            el = 51\n            while line[el:el + 1] != \"\\\\\":\n                spec = spec + line[el]\n                el += 1\n            # check for bad sample name\n            test = spec.split('.')\n            date = \"\"\n            if len(test) > 1:\n                spec = test[0]\n                kk = 24\n                while line[kk] != '\\\\x01' and line[kk] != '\\\\x00':\n                    kk += 1\n                vcc = line[24:kk]\n                el = 10\n                while rec[el].strip() != '':\n                    el += 1\n                date, comments = rec[el + 7], []\n            else:\n                el = 9\n                while rec[el] != '\\\\x01':\n                    el += 1\n                vcc, date, comments = rec[el - 3], rec[el + 7], []\n            specname = spec.lower()\n            print('importing ', specname)\n            el += 8\n            while rec[el].isdigit() == False:\n                comments.append(rec[el])\n                el += 1\n            while rec[el] == \"\":\n                el += 1\n            az = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            pl = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            bed_dip_dir = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            bed_dip = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            if rec[el] == '\\\\x01':\n                bed_dip = 180. - bed_dip\n                el += 1\n                while rec[el] == \"\":\n                    el += 1\n            fold_az = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            fold_pl = rec[el]\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            if rec[el] != \"\" and rec[el] != '\\\\x02' and rec[el] != '\\\\x01':\n                deccorr = float(rec[el])\n                az += deccorr\n                bed_dip_dir += deccorr\n                fold_az += deccorr\n                if bed_dip_dir >= 360:\n                    bed_dip_dir = bed_dip_dir - 360.\n                if az >= 360.:\n                    az = az - 360.\n                if fold_az >= 360.:\n                    fold_az = fold_az - 360.\n            else:\n                deccorr = 0\n            if specnum != 0:\n                sample = specname[:specnum]\n            else:\n                sample = specname\n            SampRec = {}\n            SampRec[\"er_sample_name\"] = sample\n            SampRec[\"er_location_name\"] = location_name\n            SampRec[\"er_citation_names\"] = \"This study\"\n            # convert to labaz, labpl\n            labaz, labdip = pmag.orient(az, pl, or_con)\n#\n# parse information common to all orientation methods\n#\n            SampRec[\"sample_bed_dip\"] = '%7.1f' % (bed_dip)\n            SampRec[\"sample_bed_dip_direction\"] = '%7.1f' % (bed_dip_dir)\n            SampRec[\"sample_dip\"] = '%7.1f' % (labdip)\n            SampRec[\"sample_azimuth\"] = '%7.1f' % (labaz)\n            if vcc.strip() != \"\":\n                vol = float(vcc) * 1e-6  # convert to m^3 from cc\n            SampRec[\"sample_volume\"] = '%10.3e' % (vol)\n            SampRec[\"sample_class\"] = sclass\n            SampRec[\"sample_lithology\"] = lithology\n            SampRec[\"sample_type\"] = _type\n            SampRec[\"sample_declination_correction\"] = '%7.1f' % (deccorr)\n            methods = gmeths.split(':')\n            if deccorr != \"0\":\n                if 'SO-MAG' in methods:\n                    del methods[methods.index('SO-MAG')]\n                methods.append('SO-CMD-NORTH')\n            meths = \"\"\n            for meth in methods:\n                meths = meths + meth + \":\"\n            meths = meths[:-1]\n            SampRec[\"magic_method_codes\"] = meths\n            if int(samp_con) < 6 or int(samp_con) == 7:\n                # parse out the site name\n                site = pmag.parse_site(SampRec[\"er_sample_name\"], samp_con, Z)\n                SampRec[\"er_site_name\"] = site\n            elif len(Samps) > 1:\n                site, location = \"\", \"\"\n                for samp in Samps:\n                    if samp[\"er_sample_name\"] == SampRec[\"er_sample_name\"]:\n                        site = samp[\"er_site_name\"]\n                        location = samp[\"er_location_name\"]\n                        break\n                SampRec[\"er_location_name\"] = samp[\"er_location_name\"]\n                SampRec[\"er_site_name\"] = samp[\"er_site_name\"]\n            if sample not in samplist:\n                samplist.append(sample)\n                Samps.append(SampRec)\n        else:\n            MagRec = {}\n            MagRec[\"treatment_temp\"] = '%8.3e' % (273)  # room temp in kelvin\n            MagRec[\"measurement_temp\"] = '%8.3e' % (273)  # room temp in kelvin\n            MagRec[\"treatment_ac_field\"] = '0'\n            MagRec[\"treatment_dc_field\"] = '0'\n            MagRec[\"treatment_dc_field_phi\"] = '0'\n            MagRec[\"treatment_dc_field_theta\"] = '0'\n            meas_type = \"LT-NO\"\n            MagRec[\"measurement_flag\"] = 'g'\n            MagRec[\"measurement_standard\"] = 'u'\n            MagRec[\"measurement_number\"] = '1'\n            MagRec[\"er_specimen_name\"] = specname\n            MagRec[\"er_sample_name\"] = SampRec['er_sample_name']\n            MagRec[\"er_site_name\"] = SampRec['er_site_name']\n            MagRec[\"er_location_name\"] = location_name\n            el, demag = 1, ''\n            treat = rec[el]\n            if treat[-1] == 'C':\n                demag = 'T'\n            elif treat != 'NRM':\n                demag = 'AF'\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            MagRec[\"measurement_dec\"] = rec[el]\n            cdec = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            MagRec[\"measurement_inc\"] = rec[el]\n            cinc = float(rec[el])\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            gdec = rec[el]\n            el += 1\n            while rec[el] == \"\":\n                el += 1\n            ginc = rec[el]\n            el = skip(2, el, rec)  # skip bdec,binc\n#                el=skip(4,el,rec) # skip gdec,ginc,bdec,binc\n#                print 'moment emu: ',rec[el]\n            MagRec[\"measurement_magn_moment\"] = '%10.3e' % (\n                float(rec[el]) * 1e-3)  # moment in Am^2 (from emu)\n            MagRec[\"measurement_magn_volume\"] = '%10.3e' % (\n                float(rec[el]) * 1e-3 / vol)  # magnetization in A/m\n            el = skip(2, el, rec)  # skip to xsig\n            MagRec[\"measurement_sd_x\"] = '%10.3e' % (\n                float(rec[el]) * 1e-3)  # convert from emu\n            el = skip(3, el, rec)  # skip to ysig\n            MagRec[\"measurement_sd_y\"] = '%10.3e' % (\n                float(rec[el]) * 1e-3)  # convert from emu\n            el = skip(3, el, rec)  # skip to zsig\n            MagRec[\"measurement_sd_z\"] = '%10.3e' % (\n                float(rec[el]) * 1e-3)  # convert from emu\n            el += 1  # skip to positions\n            MagRec[\"measurement_positions\"] = rec[el]\n#                    el=skip(5,el,rec) # skip to date\n#                    mm=str(months.index(date[0]))\n#                    if len(mm)==1:\n#                        mm='0'+str(mm)\n#                    else:\n#                        mm=str(mm)\n#                    dstring=date[2]+':'+mm+':'+date[1]+\":\"+date[3]\n#                    MagRec['measurement_date']=dstring\n            MagRec[\"magic_instrument_codes\"] = inst\n            MagRec[\"er_analyst_mail_names\"] = \"\"\n            MagRec[\"er_citation_names\"] = \"This study\"\n            MagRec[\"magic_method_codes\"] = meas_type\n            if demag == \"AF\":\n                MagRec[\"treatment_ac_field\"] = '%8.3e' % (\n                    float(treat[:-2]) * 1e-3)  # peak field in tesla\n                meas_type = \"LT-AF-Z\"\n                MagRec[\"treatment_dc_field\"] = '0'\n            elif demag == \"T\":\n                MagRec[\"treatment_temp\"] = '%8.3e' % (\n                    float(treat[:-1]) + 273.)  # temp in kelvin\n                meas_type = \"LT-T-Z\"\n            MagRec['magic_method_codes'] = meas_type\n            MagRecs.append(MagRec)\n    MagOuts = pmag.measurements_methods(MagRecs, noave)\n    MagOuts, keylist = pmag.fillkeys(MagOuts)\n    pmag.magic_write(meas_file, MagOuts, 'magic_measurements')\n    print(\"Measurements put in \", meas_file)\n    SampsOut, sampkeys = pmag.fillkeys(Samps)\n    pmag.magic_write(samp_file, SampsOut, \"er_samples\")\n    Sites = []\n    for samp in Samps:\n        SiteRec = {}\n        SiteRec['er_site_name'] = samp['er_site_name']\n        SiteRec['er_location_name'] = samp['er_location_name']\n        SiteRec['site_definition'] = 's'\n        SiteRec['er_citation_names'] = 'This study'\n        if 'sample_class' in list(samp.keys()):\n            SiteRec['site_class'] = samp['sample_class']\n        if 'sample_lithology' in list(samp.keys()):\n            SiteRec['site_lithology'] = samp['sample_lithology']\n        if 'sample_type' in list(samp.keys()):\n            SiteRec['site_lithology'] = samp['sample_lithology']\n        if 'sample_lat' in list(samp.keys()):\n            SiteRec['site_lat'] = samp['sample_lat']\n        else:\n            SiteRec['site_lat'] = \"-999\"\n        if 'sample_lon' in list(samp.keys()):\n            SiteRec['site_lon'] = samp['sample_lon']\n        else:\n            SiteRec['site_lon'] = \"-999\"\n        if 'sample_height' in list(samp.keys()):\n            SiteRec['site_height'] = samp['sample_height']\n        Sites.append(SiteRec)\n    pmag.magic_write(site_file, Sites, 'er_sites')\n    return True, meas_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nname unsquish. py DESCRIPTIONtake dec and inc data and unsquishes with specified flattening factor flt takes dec and inc data and unsquishes with specified flattening factor flt uses formula tan (Io )=(1/flt)*tan(Io ) *= tan (Io ) *= tan (Io ) *= tan (Io ) *= tan (Io ) *= tan (Io ) *= tan (Io ) *= tan (Io )", "response": "def main():\n    \"\"\"\n    NAME\n        unsquish.py\n    \n    DESCRIPTION\n      takes dec/inc data and \"unsquishes\" with specified flattening factor, flt\n      using formula tan(If)=(1/flt)*tan(Io)\n    \n    INPUT \n           declination inclination \n    OUTPUT\n           \"unsquished\" declincation inclination\n    \n    SYNTAX\n        unsquish.py [command line options] [< filename]\n    \n    OPTIONS\n        -h print help and quit\n        -f FILE, input file\n        -F FILE, output file\n        -flt FLT, flattening factor [required]\n    \n    \"\"\"\n    ofile=\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-F' in sys.argv:\n        ind=sys.argv.index('-F')\n        ofile=sys.argv[ind+1]  \n        out=open(ofile,'w')\n    if '-flt' in sys.argv:\n        ind=sys.argv.index('-flt')\n        flt=float(sys.argv[ind+1])\n    else:\n        print(main.__doc__)\n        sys.exit()  \n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]  \n        input=numpy.loadtxt(file)\n    else:\n        input=numpy.loadtxt(sys.stdin,dtype=numpy.float)\n# read in inclination data\n    for line in input: \n        dec=float(line[0])\n        inc=float(line[1])*numpy.pi/180.\n        tincnew=(old_div(1,flt))*numpy.tan(inc)\n        incnew=numpy.arctan(tincnew)*180./numpy.pi\n        if ofile==\"\":\n            print('%7.1f %7.1f'% (dec,incnew))\n        else:\n            out.write('%7.1f %7.1f'% (dec,incnew)+'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnaming iodp_jr6_magic.py DESCRIPTION converts shipboard .jr6 format files to magic_measurements format files SYNTAX iodp_jr6_magic.py [command line options] OPTIONS -h: prints the help message and quits. -f FILE: specify input file, or -F FILE: specify output file, default is magic_measurements.txt -fsa FILE: specify er_samples.txt file for sample name lookup , default is 'er_samples.txt' -loc HOLE : specify hole name (U1456A) -A: don't average replicate measurements INPUT JR6 .jr6 format file", "response": "def main(command_line=True, **kwargs):\n    \"\"\"\n    NAME\n        iodp_jr6_magic.py\n\n    DESCRIPTION\n        converts shipboard .jr6 format files to magic_measurements format files\n\n    SYNTAX\n        iodp_jr6_magic.py [command line options]\n\n    OPTIONS\n        -h: prints the help message and quits.\n        -f FILE: specify  input file, or\n        -F FILE: specify output file, default is magic_measurements.txt\n        -fsa FILE: specify  er_samples.txt file for sample name lookup ,\n           default is 'er_samples.txt'\n        -loc HOLE : specify hole name (U1456A)\n        -A: don't average replicate measurements\n\n    INPUT\n        JR6 .jr6 format file\n    \"\"\"\n\n\n    def fix_separation(filename, new_filename):\n        old_file = open(filename, 'r')\n        data = old_file.readlines()\n        new_data = []\n        for line in data:\n            new_line = line.replace('-', ' -')\n            new_line = new_line.replace('  ', ' ')\n            new_data.append(new_line)\n        new_file = open(new_filename, 'w')\n        for s in new_data:\n            new_file.write(s)\n        old_file.close()\n        new_file.close()\n        return new_filename\n\n\n    def old_fix_separation(filename, new_filename):\n        old_file = open(filename, 'r')\n        data = old_file.readlines()\n        new_data = []\n        for line in data:\n            new_line = []\n            for i in line.split():\n                if '-' in i[1:]:\n                    lead_char = '-' if i[0] == '-' else ''\n                    if lead_char:\n                        v = i[1:].split('-')\n                    else:\n                        v = i.split('-')\n                    new_line.append(lead_char + v[0])\n                    new_line.append('-' + v[1])\n                else:\n                    new_line.append(i)\n            new_line = (' '.join(new_line)) + '\\n'\n            new_data.append(new_line)\n        new_file = open(new_filename, 'w')\n        for s in new_data:\n            new_file.write(s)\n        new_file.close()\n        old_file.close()\n        return new_filename\n\n\n\n# initialize some stuff\n    noave=0\n    volume=2.5**3 #default volume is a 2.5cm cube\n    inst=\"\"\n    samp_con,Z='5',\"\"\n    missing=1\n    demag=\"N\"\n    er_location_name=\"unknown\"\n    citation='This study'\n    args=sys.argv\n    meth_code=\"LP-NO\"\n    version_num=pmag.get_version()\n    dir_path='.'\n    MagRecs=[]\n    samp_file = 'er_samples.txt'\n    meas_file = 'magic_measurements.txt'\n    mag_file = ''\n    #\n    # get command line arguments\n    #\n    if command_line:\n        if '-WD' in sys.argv:\n            ind = sys.argv.index('-WD')\n            dir_path=sys.argv[ind+1]\n        if '-ID' in sys.argv:\n            ind = sys.argv.index('-ID')\n            input_dir_path = sys.argv[ind+1]\n        else:\n            input_dir_path = dir_path\n        output_dir_path = dir_path\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if '-F' in args:\n            ind=args.index(\"-F\")\n            meas_file = args[ind+1]\n        if '-fsa' in args:\n            ind = args.index(\"-fsa\")\n            samp_file = args[ind+1]\n            if samp_file[0]!='/':\n                samp_file = os.path.join(input_dir_path, samp_file)\n            try:\n                open(samp_file,'r')\n                ErSamps,file_type=pmag.magic_read(samp_file)\n            except:\n                print(samp_file,' not found: ')\n                print('   download csv file and import to MagIC with iodp_samples_magic.py')\n        if '-f' in args:\n            ind = args.index(\"-f\")\n            mag_file= args[ind+1]\n        if \"-loc\" in args:\n            ind=args.index(\"-loc\")\n            er_location_name=args[ind+1]\n        if \"-A\" in args:\n            noave=1\n    if not command_line:\n        dir_path = kwargs.get('dir_path', '.')\n        input_dir_path = kwargs.get('input_dir_path', dir_path)\n        output_dir_path = dir_path\n        meas_file = kwargs.get('meas_file', 'magic_measurements.txt')\n        mag_file = kwargs.get('mag_file', '')\n        samp_file = kwargs.get('samp_file', 'er_samples.txt')\n        specnum = kwargs.get('specnum', 1)\n        samp_con = kwargs.get('samp_con', '1')\n        if len(str(samp_con)) > 1:\n            samp_con, Z = samp_con.split('-')\n        else:\n            Z = ''\n        er_location_name = kwargs.get('er_location_name', '')\n        noave = kwargs.get('noave', 0) # default (0) means DO average\n        meth_code = kwargs.get('meth_code', \"LP-NO\")\n\n    # format variables\n    meth_code=meth_code+\":FS-C-DRILL-IODP:SP-SS-C:SO-V\"\n    meth_code=meth_code.strip(\":\")\n    if mag_file:\n        mag_file = os.path.join(input_dir_path, mag_file)\n    samp_file = os.path.join(input_dir_path, samp_file)\n    meas_file = os.path.join(output_dir_path, meas_file)\n\n    # validate variables\n    if not mag_file:\n        print(\"You must provide an IODP_jr6 format file\")\n        return False, \"You must provide an IODP_jr6 format file\"\n    if not os.path.exists(mag_file):\n        print('The input file you provided: {} does not exist.\\nMake sure you have specified the correct filename AND correct input directory name.'.format(mag_file))\n        return False, 'The input file you provided: {} does not exist.\\nMake sure you have specified the correct filename AND correct input directory name.'.format(mag_file)\n    if not os.path.exists(samp_file):\n        print(\"Your input directory:\\n{}\\nmust contain an er_samples.txt file, or you must explicitly provide one\".format(input_dir_path))\n        return False, \"Your input directory:\\n{}\\nmust contain an er_samples.txt file, or you must explicitly provide one\".format(input_dir_path)\n\n    # parse data\n    temp = os.path.join(output_dir_path, 'temp.txt')\n    fix_separation(mag_file, temp)\n    samples, filetype = pmag.magic_read(samp_file)\n    with open(temp, 'r') as finput:\n        lines = finput.readlines()\n    os.remove(temp)\n    for line in lines:\n        MagRec = {}\n        line = line.split()\n        spec_text_id = line[0].split('_')[1]\n        SampRecs=pmag.get_dictitem(samples,'er_sample_alternatives',spec_text_id,'has')\n        if len(SampRecs)>0: # found one\n            MagRec['er_specimen_name']=SampRecs[0]['er_sample_name']\n            MagRec['er_sample_name']=MagRec['er_specimen_name']\n            MagRec['er_site_name']=MagRec['er_specimen_name']\n            MagRec[\"er_citation_names\"]=\"This study\"\n            MagRec['er_location_name']=er_location_name\n            MagRec['magic_software_packages']=version_num\n            MagRec[\"treatment_temp\"]='%8.3e' % (273) # room temp in kelvin\n            MagRec[\"measurement_temp\"]='%8.3e' % (273) # room temp in kelvin\n            MagRec[\"measurement_flag\"]='g'\n            MagRec[\"measurement_standard\"]='u'\n            MagRec[\"measurement_number\"]='1'\n            MagRec[\"treatment_ac_field\"]='0'\n\n            volume=float(SampRecs[0]['sample_volume'])\n            x = float(line[4])\n            y = float(line[3])\n            negz = float(line[2])\n            cart=np.array([x,y,-negz]).transpose()\n            direction = pmag.cart2dir(cart).transpose()\n            expon = float(line[5])\n            magn_volume = direction[2] * (10.0**expon)\n            moment = magn_volume * volume\n\n            MagRec[\"measurement_magn_moment\"]=str(moment)\n            MagRec[\"measurement_magn_volume\"]=str(magn_volume)#str(direction[2] * (10.0 ** expon))\n            MagRec[\"measurement_dec\"]='%7.1f'%(direction[0])\n            MagRec[\"measurement_inc\"]='%7.1f'%(direction[1])\n\n            step = line[1]\n            if step == 'NRM':\n                meas_type=\"LT-NO\"\n            elif step[0:2] == 'AD':\n                meas_type=\"LT-AF-Z\"\n                treat=float(step[2:])\n                MagRec[\"treatment_ac_field\"]='%8.3e' %(treat*1e-3) # convert from mT to tesla\n            elif step[0:2] == 'TD':\n                meas_type=\"LT-T-Z\"\n                treat=float(step[2:])\n                MagRec[\"treatment_temp\"]='%8.3e' % (treat+273.) # temp in kelvin\n            elif step[0:3]=='ARM': #\n                meas_type=\"LT-AF-I\"\n                treat=float(row['step'][3:])\n                MagRec[\"treatment_ac_field\"]='%8.3e' %(treat*1e-3) # convert from mT to tesla\n                MagRec[\"treatment_dc_field\"]='%8.3e' %(50e-6) # assume 50uT DC field\n                MagRec[\"measurement_description\"]='Assumed DC field - actual unknown'\n            elif step[0:3]=='IRM': #\n                meas_type=\"LT-IRM\"\n                treat=float(step[3:])\n                MagRec[\"treatment_dc_field\"]='%8.3e' %(treat*1e-3) # convert from mT to tesla\n            else:\n                print('unknown treatment type for ',row)\n                return False, 'unknown treatment type for ',row\n\n            MagRec['magic_method_codes']=meas_type\n            MagRecs.append(MagRec.copy())\n\n        else:\n            print('sample name not found: ',row['specname'])\n    MagOuts=pmag.measurements_methods(MagRecs,noave)\n    file_created, error_message = pmag.magic_write(meas_file,MagOuts,'magic_measurements')\n    if file_created:\n        return True, meas_file\n    else:\n        return False, 'Results not written to file'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_n_ptrm(tmin, tmax, ptrm_temps, ptrm_starting_temps):\n    # does not exclude ptrm checks that are less than tmin\n    ptrm_checks_included_temps= []\n    for num, check in enumerate(ptrm_temps):\n        if check > tmax:\n            pass\n        elif ptrm_starting_temps[num] > tmax: # or ptrm_starting_temps[num] < tmin:\n            pass\n        else:\n            ptrm_checks_included_temps.append(check)\n    return len(ptrm_checks_included_temps), ptrm_checks_included_temps", "response": "This function returns the number of ptrm checks included in best fit segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_max_ptrm_check(ptrm_checks_included_temps, ptrm_checks_all_temps, ptrm_x, t_Arai, x_Arai):\n    if not ptrm_checks_included_temps:\n        return [], float('nan'), float('nan'), float('nan'), float('nan')\n    diffs = []\n    abs_diffs = []\n    x_Arai_compare = []\n    ptrm_compare = []\n    check_percents = []\n    ptrm_checks_all_temps = list(ptrm_checks_all_temps)\n    for check in ptrm_checks_included_temps: # goes through each included temperature step\n        ptrm_ind = ptrm_checks_all_temps.index(check) # indexes the number of the check\n        ptrm_check = ptrm_x[ptrm_ind] # x value at that temperature step\n        ptrm_compare.append(ptrm_check) #\n        arai_ind = t_Arai.index(check)\n        ptrm_orig = x_Arai[arai_ind]\n        x_Arai_compare.append(ptrm_orig)\n        diff = ptrm_orig - ptrm_check\n        diffs.append(diff)\n        abs_diffs.append(abs(diff))\n        if ptrm_orig == 0:\n            check_percents.append(0)\n        else:\n            check_percents.append((old_div(abs(diff), ptrm_orig)) * 100)\n    max_diff = max(abs_diffs)\n    check_percent = max(check_percents)\n    sum_diffs = abs(sum(diffs))\n    sum_abs_diffs = sum(abs_diffs)\n    return diffs, max_diff, sum_diffs, check_percent, sum_abs_diffs", "response": "This function calculates the maximum ptrm check for a single element in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninput TRM length of best fit line (delta_x_prime), NRM length of best fit line, max_ptrm_check Output: DRAT (maximum difference produced by a ptrm check normed by best fit line), length best fit line", "response": "def get_DRAT(delta_x_prime, delta_y_prime, max_ptrm_check):\n    \"\"\"\n    Input: TRM length of best fit line (delta_x_prime),\n        NRM length of best fit line,\n        max_ptrm_check\n    Output: DRAT (maximum difference produced by a ptrm check normed by best fit line),\n        length best fit line\n    \"\"\"\n    L = numpy.sqrt(delta_x_prime**2 + delta_y_prime**2)\n    DRAT = (old_div(max_ptrm_check, L)) * 100\n    return DRAT, L"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_CDRAT(L, sum_ptrm_checks, sum_abs_ptrm_checks):\n    CDRAT = (old_div(sum_ptrm_checks, L)) * 100.\n    CDRAT_prime = (old_div(sum_abs_ptrm_checks, L)) * 100.\n    return CDRAT, CDRAT_prime", "response": "get CDRAT and CDRAT_prime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting DRATS from sum of ptrm check diffs and sum of absolute value of ptrm check diffs", "response": "def get_DRATS(sum_ptrm_checks, sum_abs_ptrm_checks, x_Arai, end):\n    \"\"\"\n    input: sum of ptrm check diffs, sum of absolute value of ptrm check diffs,\n        x_Arai set of points, end.\n    output: DRATS (uses sum of diffs), DRATS_prime (uses sum of absolute diffs)\n    \"\"\"\n    DRATS = (old_div(sum_ptrm_checks, x_Arai[end])) * 100.\n    DRATS_prime = (old_div(sum_abs_ptrm_checks, x_Arai[end])) * 100.\n    return DRATS, DRATS_prime"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninputting sum_ptrm_checks, sum_abs_ptrm_checks, n_pTRM, L output: mean DRAT (the average difference produced by a pTRM check, normalized by the length of the best-fit line)", "response": "def get_mean_DRAT(sum_ptrm_checks, sum_abs_ptrm_checks, n_pTRM, L):\n    \"\"\"\n    input: sum_ptrm_checks, sum_abs_ptrm_checks, n_pTRM, L\n    output: mean DRAT (the average difference produced by a pTRM check,\n    normalized by the length of the best-fit line)\n    \"\"\"\n    if not n_pTRM:\n        return float('nan'), float('nan')\n    mean_DRAT = ((old_div(1., n_pTRM)) * (old_div(sum_ptrm_checks, L))) * 100\n    mean_DRAT_prime = ((old_div(1., n_pTRM)) * (old_div(sum_abs_ptrm_checks, L))) * 100\n    return mean_DRAT, mean_DRAT_prime"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_mean_DEV(sum_ptrm_checks, sum_abs_ptrm_checks, n_pTRM, delta_x_prime):\n    if not n_pTRM:\n        return float('nan'), float('nan')\n    mean_DEV = ((old_div(1., n_pTRM)) * (old_div(sum_ptrm_checks, delta_x_prime))) * 100\n    mean_DEV_prime= ((old_div(1., n_pTRM)) * (old_div(sum_abs_ptrm_checks, delta_x_prime))) * 100\n    return mean_DEV, mean_DEV_prime", "response": "This function calculates the mean deviation of a checkon group"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the delta PTRM and PTRM_Checks vectors in vector form.", "response": "def get_delta_pal_vectors(PTRMS, PTRM_Checks, NRM):\n    \"\"\" takes in PTRM data in this format: [temp, dec, inc, moment, ZI or IZ] -- and PTRM_check data in this format: [temp, dec, inc, moment].  Returns them in vector form (cartesian). \"\"\"\n    PTRMS = numpy.array(PTRMS)\n    PTRM_Checks = numpy.array(PTRM_Checks)\n    TRM_1 = lib_direct.dir2cart(PTRMS[0,1:3])\n    PTRMS_cart = []\n    Checks_cart = []\n    for num, ptrm in enumerate(PTRMS):\n        ptrm_cart = lib_direct.dir2cart([PTRMS[num][1], PTRMS[num][2], old_div(PTRMS[num][3], NRM)])\n        PTRMS_cart.append(ptrm_cart)\n    for num, check in enumerate(PTRM_Checks):\n        check_cart = lib_direct.dir2cart([PTRM_Checks[num][1], PTRM_Checks[num][2], old_div(PTRM_Checks[num][3], NRM)])\n        Checks_cart.append(check_cart)\n    return PTRMS_cart, Checks_cart, TRM_1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_diffs(ptrms_vectors, ptrm_checks_vectors, ptrms_orig, checks_orig):\n    ptrm_temps = numpy.array(ptrms_orig)[:,0]\n    check_temps = numpy.array(checks_orig)[:,0]\n    index = numpy.zeros(len(ptrm_temps))\n    for num, temp in enumerate(ptrm_temps):\n        if len(numpy.where(check_temps == temp)[0]):\n            index[num] = numpy.where(check_temps == temp)[0][0]\n        else:\n            index[num] = float('nan')\n    diffs = numpy.zeros((len(ptrms_vectors), 3))\n    for num, ptrm in enumerate(ptrms_vectors):\n        if numpy.isnan(index[num]):\n            diffs[num] = numpy.array([0,0,0])\n        else:\n            diffs[num] = ptrm_checks_vectors[int(index[num])] - ptrm\n    C = numpy.cumsum(diffs, 0)\n    #print \"diffs (should be same as to_sum\"\n    #print diffs\n    #print \"C (should be same as dpal_sum)\"\n    #print C\n    return diffs, C", "response": "get the vector diffs between original and ptrm check"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting TRM star from ptrms_vectors start and end", "response": "def get_TRM_star(C, ptrms_vectors, start, end):\n    \"\"\"\n    input: C, ptrms_vectors, start, end\n    output: TRM_star, x_star (for delta_pal statistic)\n    \"\"\"\n    TRM_star = numpy.zeros([len(ptrms_vectors), 3])\n    TRM_star[0] = [0., 0., 0.]\n    x_star = numpy.zeros(len(ptrms_vectors))\n    for num, vec in enumerate(ptrms_vectors[1:]):\n        TRM_star[num+1] = vec + C[num]\n       # print 'vec', vec\n       # print 'C', C[num]\n    for num, trm in enumerate(TRM_star):\n        x_star[num] = numpy.linalg.norm(trm)\n    #print \"x_star (should match corr_TRM / NRM)\"\n    #print x_star[start:end+1]\n    return TRM_star[start:end+1], x_star[start:end+1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_b_star(x_star, y_err, y_mean, y_segment):\n    #print \"x_star, should be same as Xcorr / NRM\"\n    #print x_star\n    x_star_mean = numpy.mean(x_star)\n    x_err = x_star - x_star_mean\n    b_star = -1* numpy.sqrt( old_div(sum(numpy.array(y_err)**2), sum(numpy.array(x_err)**2)) )  # averaged slope\n    #print \"y_segment\", y_segment\n    b_star = numpy.sign(sum(x_err * y_err)) * numpy.std(y_segment, ddof=1) / numpy.std(x_star, ddof=1)\n    #print \"b_star (should be same as corr_slope)\"\n    #print b_star\n    return b_star", "response": "get b_star from x_star y_err y_mean y_segment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate delta_pal from b_star and b", "response": "def get_delta_pal(b, b_star):\n    \"\"\"\n    input: b, b_star (actual and corrected slope)\n    output: delta_pal\n    \"\"\"\n    delta_pal = numpy.abs(old_div((b - b_star), b)) * 100\n    return delta_pal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_full_delta_pal(PTRMS, PTRM_Checks, NRM, y_err, y_mean, b, start, end, y_segment):\n    #print \"-------\"\n    #print \"calling get_full_delta_pal in lib\"\n#    return 0\n    PTRMS_cart, checks, TRM_1 = get_delta_pal_vectors(PTRMS, PTRM_Checks, NRM)\n#    print \"PTRMS_Cart\", PTRMS_cart\n    diffs, C = get_diffs(PTRMS_cart, checks, PTRMS, PTRM_Checks)\n#    print \"C\", C\n    TRM_star, x_star = get_TRM_star(C, PTRMS_cart, start, end)\n#    print \"x_star\", x_star\n#    print type(x_star)\n    b_star = get_b_star(x_star, y_err, y_mean, y_segment)\n    delta_pal = get_delta_pal(b, b_star)\n    return delta_pal", "response": "input: PTRMS, PTRM_Checks, NRM, y_err, y_mean, b, start, end, y_segment\n    runs full sequence necessary to get delta_pal"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_segments(ptrms, ptrm_checks, tmax):\n    ptrms_included = []\n    checks_included = []\n    ptrms = numpy.array(ptrms)\n    for ptrm in ptrms:\n        if ptrm[0] <= tmax:\n            ptrms_included.append(ptrm)\n    for check in ptrm_checks:\n        if check[0] <= tmax:\n            checks_included.append(check)\n    #print \"checks\", ptrm_checks\n    #print \"checks_included\", checks_included\n    return ptrms_included, checks_included", "response": "get_segments - returns a list of all the segments in the order of the given ptrms and ptrm_checks"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the current fit on the GUI that is the parent of this one.", "response": "def select(self):\n        \"\"\"\n        Makes this fit the selected fit on the GUI that is it's parent\n        (Note: may be moved into GUI soon)\n        \"\"\"\n        if self.GUI==None: return\n        self.GUI.current_fit = self\n        if self.tmax != None and self.tmin != None:\n            self.GUI.update_bounds_boxes()\n        if self.PCA_type != None:\n            self.GUI.update_PCA_box()\n        try: self.GUI.zijplot\n        except AttributeError: self.GUI.draw_figure(self.GUI.s)\n        self.GUI.fit_box.SetStringSelection(self.name)\n        self.GUI.get_new_PCA_parameters(-1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the pmagpy paramters dictionary associated with this fit and the given coordinate system", "response": "def get(self,coordinate_system):\n        \"\"\"\n        Return the pmagpy paramters dictionary associated with this fit and the given\n        coordinate system\n        @param: coordinate_system -> the coordinate system who's parameters to return\n        \"\"\"\n        if coordinate_system == 'DA-DIR' or coordinate_system == 'specimen':\n            return self.pars\n        elif coordinate_system == 'DA-DIR-GEO' or coordinate_system == 'geographic':\n            return self.geopars\n        elif coordinate_system == 'DA-DIR-TILT' or coordinate_system == 'tilt-corrected':\n            return self.tiltpars\n        else:\n            print(\"-E- no such parameters to fetch for \" + coordinate_system + \" in fit: \" + self.name)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a coordinate system and a new parameters dictionary that follows pmagpy convention given by the pmag.py/domean function it alters this fit's bounds and parameters such that it matches the new data. @param: specimen -> None if fit is for a site or a sample or a valid specimen from self.GUI @param: coordinate_system -> the coordinate system to alter @param: new_pars -> the new paramters to change your fit to @alters: tmin, tmax, pars, geopars, tiltpars, PCA_type", "response": "def put(self,specimen,coordinate_system,new_pars):\n        \"\"\"\n        Given a coordinate system and a new parameters dictionary that follows pmagpy\n        convention given by the pmag.py/domean function it alters this fit's bounds and\n        parameters such that it matches the new data.\n        @param: specimen -> None if fit is for a site or a sample or a valid specimen from self.GUI\n        @param: coordinate_system -> the coordinate system to alter\n        @param: new_pars -> the new paramters to change your fit to\n        @alters: tmin, tmax, pars, geopars, tiltpars, PCA_type\n        \"\"\"\n\n        if specimen != None:\n            if type(new_pars)==dict:\n                if 'er_specimen_name' not in list(new_pars.keys()): new_pars['er_specimen_name'] = specimen\n                if 'specimen_comp_name' not in list(new_pars.keys()): new_pars['specimen_comp_name'] = self.name\n            if type(new_pars) != dict or 'measurement_step_min' not in list(new_pars.keys()) or 'measurement_step_max' not in list(new_pars.keys()) or 'calculation_type' not in list(new_pars.keys()):\n                print(\"-E- invalid parameters cannot assign to fit %s for specimen %s - was given:\\n%s\"%(self.name,specimen,str(new_pars)))\n                return self.get(coordinate_system)\n\n            self.tmin = new_pars['measurement_step_min']\n            self.tmax = new_pars['measurement_step_max']\n            self.PCA_type = new_pars['calculation_type']\n\n            if self.GUI!=None:\n                steps = self.GUI.Data[specimen]['zijdblock_steps']\n                tl = [self.tmin,self.tmax]\n                for i,t in enumerate(tl):\n                    if str(t) in steps: tl[i] = str(t)\n                    elif str(int(t)) in steps: tl[i] = str(int(t))\n                    elif \"%.1fmT\"%t in steps: tl[i] = \"%.1fmT\"%t\n                    elif \"%.0fC\"%t in steps: tl[i] = \"%.0fC\"%t\n                    else:\n                        print(\"-E- Step \" + str(tl[i]) + \" does not exsist (func: Fit.put)\")\n                        tl[i] = str(t)\n                self.tmin,self.tmax = tl\n            elif meas_data != None:\n                steps = meas_data[specimen]['zijdblock_steps']\n                tl = [self.tmin,self.tmax]\n                for i,t in enumerate(tl):\n                    if str(t) in steps: tl[i] = str(t)\n                    elif str(int(t)) in steps: tl[i] = str(int(t))\n                    elif \"%.1fmT\"%t in steps: tl[i] = \"%.1fmT\"%t\n                    elif \"%.0fC\"%t in steps: tl[i] = \"%.0fC\"%t\n                    else:\n                        print(\"-E- Step \" + str(tl[i]) + \" does not exsist (func: Fit.put)\")\n                        tl[i] = str(t)\n                self.tmin,self.tmax = tl\n            else: self.tmin,self.tmax = list(map(str, tl))\n\n        if coordinate_system == 'DA-DIR' or coordinate_system == 'specimen':\n            self.pars = new_pars\n        elif coordinate_system == 'DA-DIR-GEO' or coordinate_system == 'geographic':\n            self.geopars = new_pars\n        elif coordinate_system == 'DA-DIR-TILT' or coordinate_system == 'tilt-corrected':\n            self.tiltpars = new_pars\n        else:\n            print('-E- no such coordinate system could not assign those parameters to fit')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_values(self, name, tmin, tmax):\n        return str(self.name) == str(name) and str(self.tmin) == str(tmin) and str(self.tmax) == str(tmax)", "response": "A basic fit equality checker compares name and bounds of 2 fits\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining number of included tail checks in best fit segment", "response": "def get_n_tail(tmax, tail_temps):\n    \"\"\"determines number of included tail checks in best fit segment\"\"\"\n    #print \"tail_temps: {0}, tmax: {0}\".format(tail_temps, tmax)\n    t_index = 0\n    adj_tmax = 0\n    if tmax < tail_temps[0]:\n        return 0\n    try:\n        t_index = list(tail_temps).index(tmax)\n    except: # finds correct tmax if there was no tail check performed at tmax\n        for temp in tail_temps:\n            if temp <= tmax:\n                adj_tmax = temp\n        t_index = list(tail_temps).index(adj_tmax)\n    incl_temps = tail_temps[0:t_index+1] # b/c not inclusive\n    return len(incl_temps)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to get the max tail check for a single Arai", "response": "def get_max_tail_check(y_Arai, y_tail, t_Arai, tail_temps, n_tail):\n    \"\"\"\n    input: y_Arai, y_tail, t_Arai, tail_temps, n_tail\n    output: max_check, diffs\n    \"\"\"\n    if not n_tail:\n        return float('nan'), []\n    tail_compare = []\n    y_Arai_compare = []\n    for temp in tail_temps[:n_tail]:\n        tail_index = list(tail_temps).index(temp)\n        tail_check = y_tail[tail_index]\n        tail_compare.append(tail_check)\n        arai_index = list(t_Arai).index(temp)\n        nrm_orig = y_Arai[arai_index]\n        y_Arai_compare.append(nrm_orig)\n    diffs = numpy.array(y_Arai_compare) - numpy.array(tail_compare)\n    abs_diffs = abs(diffs)\n    max_check = max(abs_diffs)\n    return max_check, diffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets DRAT tail of a node", "response": "def get_DRAT_tail(max_check, L):\n    \"\"\"\n    input: tail_check_max, best fit line length\n    output: DRAT_tail\n    \"\"\"\n    if max_check == 0:\n        return float('nan')\n    DRAT_tail = (old_div(max_check, L)) * 100.\n    return DRAT_tail"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_delta_TR(tail_check_max, y_int):\n    if tail_check_max == 0 or numpy.isnan(tail_check_max):\n        return float('nan')\n    delta_TR = (old_div(tail_check_max, abs(y_int))) * 100.\n    return delta_TR", "response": "calculate delta TR from tail_check_max y_int"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_MD_VDS(tail_check_max, vds):\n    if tail_check_max == 0 or numpy.isnan(tail_check_max):\n        return float('nan')\n    MD_VDS = (old_div(tail_check_max, vds)) * 100\n    return MD_VDS", "response": "calculate MD VDS from vector difference sum"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_dm(self, num):\n        #enable or disable self.btn1a\n        if self.data_model_num == 3:\n            self.btn1a.Enable()\n        else:\n            self.btn1a.Disable()\n        #\n        # set pmag_gui_dialogs\n        global pmag_gui_dialogs\n        if self.data_model_num == 2:\n            pmag_gui_dialogs = pgd2\n            wx.CallAfter(self.get_wd_data2)\n        elif self.data_model_num == 3:\n            pmag_gui_dialogs = pgd3\n            wx.CallAfter(self.get_wd_data)\n\n        # do / re-do menubar\n        menubar = pmag_gui_menu.MagICMenu(self, data_model_num=self.data_model_num)\n        self.SetMenuBar(menubar)\n        self.menubar = menubar", "response": "Set the data model num and set the GUI to show it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_wd_data(self):\n        wait = wx.BusyInfo('Reading in data from current working directory, please wait...')\n        #wx.Yield()\n        print('-I- Read in any available data from working directory')\n        self.contribution = cb.Contribution(self.WD, dmodel=self.data_model)\n        del wait", "response": "Show dialog to get user input for which directory\n        to set as working directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting 2. 5 data from self. WD and put it into ErMagicBuilder object.", "response": "def get_wd_data2(self):\n        \"\"\"\n        Get 2.5 data from self.WD and put it into\n        ErMagicBuilder object.\n        Called by get_dm_and_wd\n        \"\"\"\n        wait = wx.BusyInfo('Reading in data from current working directory, please wait...')\n        #wx.Yield()\n        print('-I- Read in any available data from working directory (data model 2)')\n\n        self.er_magic = builder.ErMagicBuilder(self.WD,\n                                               data_model=self.data_model)\n        del wait"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dir(self):\n        if \"-WD\" in sys.argv and self.FIRST_RUN:\n            ind = sys.argv.index('-WD')\n            self.WD = os.path.abspath(sys.argv[ind+1])\n            os.chdir(self.WD)\n            self.WD = os.getcwd()\n            self.dir_path.SetValue(self.WD)\n        else:\n            self.on_change_dir_button(None)\n            #self.WD = os.getcwd()\n\n        self.FIRST_RUN = False", "response": "Choose a working directory dialog."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen dialog for rough conversion of 2.5 files to 3.0 files. Offer link to earthref for proper upgrade.", "response": "def on_btn_convert_3(self, event):\n        \"\"\"\n        Open dialog for rough conversion of\n        2.5 files to 3.0 files.\n        Offer link to earthref for proper upgrade.\n        \"\"\"\n        dia = pw.UpgradeDialog(None)\n        dia.Center()\n        res = dia.ShowModal()\n        if res == wx.ID_CANCEL:\n            webbrowser.open(\"https://www2.earthref.org/MagIC/upgrade\", new=2)\n            return\n        ## more nicely styled way, but doesn't link to earthref\n        #msg = \"This tool is meant for relatively simple upgrades (for instance, a measurement file, a sample file, and a criteria file).\\nIf you have a more complex contribution to upgrade, and you want maximum accuracy, use the upgrade tool at https://www2.earthref.org/MagIC/upgrade.\\n\\nDo you want to continue?\"\n        #result = pw.warning_with_override(msg)\n        #if result == wx.ID_NO:\n            #webbrowser.open(\"https://www2.earthref.org/MagIC/upgrade\", new=2)\n            #return\n        # turn files from 2.5 --> 3.0 (rough translation)\n        meas, upgraded, no_upgrade = pmag.convert_directory_2_to_3('magic_measurements.txt',\n                                                                   input_dir=self.WD, output_dir=self.WD,\n                                                                   data_model=self.contribution.data_model)\n        if not meas:\n            wx.MessageBox('2.5 --> 3.0 failed. Do you have a magic_measurements.txt file in your working directory?',\n                          'Info', wx.OK | wx.ICON_INFORMATION)\n            return\n\n        # create a contribution\n        self.contribution = cb.Contribution(self.WD)\n        # make skeleton files with specimen, sample, site, location data\n        self.contribution.propagate_measurement_info()\n        # pop up\n        upgraded_string = \", \".join(upgraded)\n        if no_upgrade:\n            no_upgrade_string = \", \".join(no_upgrade)\n            msg = '2.5 --> 3.0 translation completed!\\n\\nThese 3.0 format files were created: {}.\\n\\nHowever, these 2.5 format files could not be upgraded: {}.\\n\\nTo convert all 2.5 files, use the MagIC upgrade tool: https://www2.earthref.org/MagIC/upgrade\\n'.format(upgraded_string, no_upgrade_string)\n            if 'criteria.txt' in upgraded:\n                msg += '\\nNote: Please check your criteria file for completeness and accuracy, as not all 2.5 files will be fully upgraded.'\n            if 'pmag_criteria.txt' in no_upgrade:\n                msg += '\\nNote: Not all criteria files can be upgraded, even on the MagIC site.  You may need to recreate an old pmag_criteria file from scratch in Thellier GUI or Demag GUI.'\n            wx.MessageBox(msg, 'Warning', wx.OK | wx.ICON_INFORMATION)\n        else:\n            msg = '2.5 --> 3.0 translation completed!\\nThese files were converted: {}'.format(upgraded_string)\n            wx.MessageBox(msg, 'Info', wx.OK | wx.ICON_INFORMATION)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_btn_metadata(self, event):\n        # make sure we have a measurements file\n        if not self.check_for_meas_file():\n            return\n        # make sure all files of the same type have been combined\n        if not self.check_for_uncombined_files():\n            return\n        if self.data_model_num == 2:\n            wait = wx.BusyInfo('Compiling required data, please wait...')\n            wx.SafeYield()\n            self.ErMagic_frame = ErMagicBuilder.MagIC_model_builder(self.WD, self, self.er_magic)\n        elif self.data_model_num == 3:\n            wait = wx.BusyInfo('Compiling required data, please wait...')\n            wx.SafeYield()\n            self.ErMagic_frame = ErMagicBuilder.MagIC_model_builder3(self.WD, self, self.contribution)\n        #\n        self.ErMagic_frame.Show()\n        self.ErMagic_frame.Center()\n        # gets total available screen space - 10%\n        size = wx.DisplaySize()\n        size = (size[0] - 0.3 * size[0], size[1] - 0.3 * size[1])\n        self.ErMagic_frame.Raise()\n        del wait", "response": "Initiate the series of windows to add metadata to the contribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitiates the object that will control steps 1 - 6 of checking headers filling in cell values etc.", "response": "def init_check_window2(self):\n        \"\"\"\n        initiates the object that will control steps 1-6\n        of checking headers, filling in cell values, etc.\n        \"\"\"\n        self.check_dia = pmag_er_magic_dialogs.ErMagicCheckFrame(self, 'Check Data',\n                                                                 self.WD, self.er_magic)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitiate the object that will control steps 1 - 6 of checking headers filling in cell values etc.", "response": "def init_check_window(self):\n        \"\"\"\n        initiates the object that will control steps 1-6\n        of checking headers, filling in cell values, etc.\n        \"\"\"\n        self.check_dia = pmag_er_magic_dialogs.ErMagicCheckFrame3(self, 'Check Data',\n                                                                  self.WD, self.contribution)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and fill wxPython grid for entering the current ISO - 8601 data.", "response": "def on_btn_orientation(self, event):\n        \"\"\"\n        Create and fill wxPython grid for entering\n        orientation data.\n        \"\"\"\n        wait = wx.BusyInfo('Compiling required data, please wait...')\n        wx.SafeYield()\n        #dw, dh = wx.DisplaySize()\n        size = wx.DisplaySize()\n        size = (size[0]-0.1 * size[0], size[1]-0.1 * size[1])\n        if self.data_model_num == 3:\n            frame = pmag_gui_dialogs.OrientFrameGrid3(self, -1, 'demag_orient.txt',\n                                                        self.WD, self.contribution,\n                                                        size)\n        else:\n            frame = pmag_gui_dialogs.OrientFrameGrid(self, -1, 'demag_orient.txt',\n                                                        self.WD, self.er_magic, size)\n        frame.Show(True)\n        frame.Centre()\n        self.Hide()\n        del wait"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_btn_unpack(self, event):\n        dlg = wx.FileDialog(\n            None, message = \"choose txt file to unpack\",\n            defaultDir=self.WD,\n            defaultFile=\"\",\n            style=wx.FD_OPEN #| wx.FD_CHANGE_DIR\n            )\n        if dlg.ShowModal() == wx.ID_OK:\n            FILE = dlg.GetPath()\n            input_dir, f = os.path.split(FILE)\n        else:\n            return False\n\n        outstring=\"download_magic.py -f {} -WD {} -ID {} -DM {}\".format(f, self.WD, input_dir, self.data_model_num)\n\n        # run as module:\n        print(\"-I- running python script:\\n %s\"%(outstring))\n        wait = wx.BusyInfo(\"Please wait, working...\")\n        wx.SafeYield()\n        ex = None\n        try:\n            if ipmag.download_magic(f, self.WD, input_dir, overwrite=True, data_model=self.data_model):\n                text = \"Successfully ran download_magic.py program.\\nMagIC files were saved in your working directory.\\nSee Terminal/message window for details.\"\n            else:\n                text = \"Something went wrong.  Make sure you chose a valid file downloaded from the MagIC database and try again.\"\n\n        except Exception as ex:\n            text = \"Something went wrong.  Make sure you chose a valid file downloaded from the MagIC database and try again.\"\n            del wait\n            dlg = wx.MessageDialog(self, caption=\"Saved\", message=text, style=wx.OK)\n            result = dlg.ShowModal()\n            if result == wx.ID_OK:\n                dlg.Destroy()\n            if ex:\n                raise(ex)\n        self.contribution = cb.Contribution(self.WD)", "response": "Create dialog to choose a file to unpack and create a contribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_btn_upload(self, event):\n        if not self.check_for_uncombined_files():\n            return\n        outstring=\"upload_magic.py\"\n        print(\"-I- running python script:\\n %s\"%(outstring))\n        wait = wx.BusyInfo(\"Please wait, working...\")\n        wx.SafeYield()\n        self.contribution.tables['measurements'].add_measurement_names()\n        if self.data_model_num == 3:\n            res, error_message, has_problems, all_failing_items = ipmag.upload_magic(concat=False, dir_path=self.WD,\n                                                                                     vocab=self.contribution.vocab,\n                                                                                     contribution=self.contribution)\n        if self.data_model_num == 2:\n            res, error_message, errors = ipmag.upload_magic2(dir_path=self.WD, data_model=self.er_magic.data_model)\n            del wait\n\n        if res:\n            text = \"You are ready to upload!\\n{} was generated in {}\".format(os.path.split(res)[1], os.path.split(res)[0])\n            dlg = pw.ChooseOne(self, \"Go to MagIC for uploading\", \"Not ready yet\", text, \"Saved\")\n            del wait\n            #dlg = wx.MessageDialog(self, caption=\"Saved\", message=text, style=wx.OK)\n        else:\n            text = \"There were some problems with the creation of your upload file.\\nError message: {}\\nSee Terminal/message window for details\".format(error_message)\n            dlg = wx.MessageDialog(self, caption=\"Error\", message=text, style=wx.OK)\n\n        dlg.Centre()\n        result = dlg.ShowModal()\n        if result == wx.ID_OK:\n            dlg.Destroy()\n        if result == wx.ID_YES:\n            pw.on_database_upload(None)\n\n        if self.data_model_num == 3:\n            if not res:\n                from programs import magic_gui\n                self.Disable()\n                self.Hide()\n                self.magic_gui_frame = magic_gui.MainFrame(self.WD,\n                                                           dmodel=self.data_model,\n                                                           title=\"Validations\",\n                                                           contribution=self.contribution)\n\n                self.magic_gui_frame.validation_mode = ['specimens']\n                self.magic_gui_frame.failing_items = all_failing_items\n                self.magic_gui_frame.change_dir_button.Disable()\n                self.magic_gui_frame.Centre()\n                self.magic_gui_frame.Show()\n                self.magic_gui_frame.highlight_problems(has_problems)\n                #\n                # change name of upload button to 'exit validation mode'\n                self.magic_gui_frame.bSizer2.GetStaticBox().SetLabel('return to main GUI')\n                self.magic_gui_frame.btn_upload.SetLabel(\"exit validation mode\")\n                # bind that button to quitting magic gui and re-enabling Pmag GUI\n                self.magic_gui_frame.Bind(wx.EVT_BUTTON, self.on_end_validation, self.magic_gui_frame.btn_upload)\n                # do binding so that closing/quitting re-opens the main frame\n                self.magic_gui_frame.Bind(wx.EVT_CLOSE, self.on_end_validation)\n                # this makes it work with only the validation window open\n                self.magic_gui_frame.Bind(wx.EVT_MENU,\n                                          lambda event: self.menubar.on_quit(event, self.magic_gui_frame),\n                                          self.magic_gui_frame.menubar.file_quit)\n                # this makes it work if an additional grid is open\n                self.Bind(wx.EVT_MENU,\n                          lambda event: self.menubar.on_quit(event, self.magic_gui_frame),\n                          self.magic_gui_frame.menubar.file_quit)", "response": "Upload a new MagIC file to MagIC"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_end_validation(self, event):\n        self.Enable()\n        self.Show()\n        self.magic_gui_frame.Destroy()", "response": "Switch back from validation mode to main Pmag GUI mode and show main frame and hide validation frame and show main frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_for_uncombined_files(self):\n        wd_files = os.listdir(self.WD)\n        if self.data_model_num == 2:\n            ftypes = ['er_specimens.txt', 'er_samples.txt', 'er_sites.txt', 'er_locations.txt', 'pmag_specimens.txt', 'pmag_samples.txt', 'pmag_sites.txt', 'rmag_specimens.txt', 'rmag_results.txt', 'rmag_anisotropy.txt']\n        else:\n            ftypes = ['specimens.txt', 'samples.txt', 'sites.txt', 'locations.txt']\n        uncombined = set()\n        for ftype in ftypes:\n            if ftype not in wd_files:\n                for f in wd_files:\n                    if f.endswith('_' + ftype):\n                        uncombined.add(ftype)\n        if uncombined:\n            msg = 'It looks like you may have uncombined files of type(s) {} in your working directory.\\nYou may want to go back to Step 1 and finish combining all files.\\nIf you continue, the program will try to extract as much information as possible from your measurement file.'.format(\", \".join(list(uncombined)))\n            dlg = pw.ChooseOne(self, 'Continue anyway', 'Go back', msg, title=\"Warning!\")\n            res = dlg.ShowModal()\n            if res == wx.ID_NO:\n                return\n        return True", "response": "Check if uncombined files are found in the working directory and if not continue to extract as much information as possible."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_for_meas_file(self):\n        if self.data_model_num == 2:\n            meas_file_name = \"magic_measurements.txt\"\n            dm = \"2.5\"\n        else:\n            meas_file_name = \"measurements.txt\"\n            dm = \"3.0\"\n        if not os.path.isfile(os.path.join(self.WD, meas_file_name)):\n            pw.simple_warning(\"Your working directory must have a {} format {} file to run this step.  Make sure you have fully completed step 1 (import magnetometer file) and ALSO converted to 3.0., if necessary), then try again.\\n\\nIf you are trying to look at data downloaded from MagIC, you must unpack the txt file first. Some contributions do not contain measurement data, in which case you won't be able to use this function.\".format(dm, meas_file_name))\n            return False\n        return True", "response": "Check the working directory for a measurement file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    args = sys.argv\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    dir_path = pmag.get_named_arg('-WD', '.')\n    fmt = pmag.get_named_arg('-fmt', 'svg')\n    save_plots = False\n    interactive = True\n    if '-sav' in sys.argv:\n        save_plots = True\n        interactive = False\n    infile = pmag.get_named_arg(\"-f\", \"specimens.txt\")\n    ipmag.dayplot_magic(dir_path, infile, save=save_plots,\n                        fmt=fmt, interactive=interactive)", "response": "NAME\n        dayplot_magic.py\n\n    DESCRIPTION\n        makes 'day plots' (Day et al. 1977) and squareness/coercivity,\n        plots 'linear mixing' curve from Dunlop and Carter-Stiglitz (2006).\n          squareness coercivity of remanence (Neel, 1955) plots after\n          Tauxe et al. (2002)\n\n    SYNTAX\n        dayplot_magic.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f: specify input hysteresis file, default is specimens.txt\n        -fmt [svg,png,jpg] format for output plots, default svg\n        -sav saves plots and quits quietly"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_import1(self, event):\n        pmag_menu_dialogs.MoveFileIntoWD(self.parent, self.parent.WD)", "response": "initialize window to import an arbitrary file into the working directory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef orient_import2(self, event):\n        pmag_menu_dialogs.ImportAzDipFile(self.parent, self.parent.WD)", "response": "initialize window to import an AzDip format file into the working directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames plot_map_pts.py DESCRIPTION plots points on map SYNTAX plot_map_pts.py [command line options] OPTIONS -h prints help and quits -sym [ro, bs, g^, r., b-, etc.] [1,5,10] symbol and size for points colors are r=red,b=blue,g=green, etc. symbols are '.' for points, ^, for triangle, s for square, etc. -, for lines, -- for dotted lines, see matplotlib online documentation for plot() -eye ELAT ELON [specify eyeball location] -etp put on topography -cmap color map [default is jet] -f FILE, specify input file -o color ocean blue/land green (default is not) -res [c,l,i,h] specify resolution (crude, low, intermediate, high] -fmt [pdf,eps, png] specify output format (default is pdf) -R don't plot details of rivers -B don't plot national/state boundaries, etc. -pad [LAT LON] pad bounding box by LAT/LON (default is not) -grd SPACE specify grid spacing -sav save plot and quit -prj PROJ, specify one of the supported projections: pc = Plate Carree aea = Albers Equal Area aeqd = Azimuthal Equidistant lcc = Lambert Conformal lcyl = Lambert Cylindrical merc = Mercator mill = Miller Cylindrical moll = Mollweide [default] ortho = Orthographic robin = Robinson sinu = Sinusoidal stere = Stereographic tmerc = Transverse Mercator utm = UTM laea = Lambert Azimuthal Equal Area geos = Geostationary npstere = North-Polar Stereographic spstere = South-Polar Stereographic Special codes for MagIC formatted input files: -n -l INPUTS space or tab delimited LON LAT data OR: standard MagIC formatted er_sites or pmag_results table DEFAULTS res: c prj: mollweide; lcc for MagIC format files ELAT,ELON = 0,0 pad LAT,LON=0,0 NB: high resolution or lines can be very slow", "response": "def main():\n    \"\"\"\n    NAME \n        plot_map_pts.py \n\n    DESCRIPTION\n        plots points on map\n \n    SYNTAX\n        plot_map_pts.py [command line options]\n\n    OPTIONS\n        -h prints help and quits\n        -sym [ro, bs, g^, r., b-, etc.] [1,5,10] symbol and size for points\n           colors are r=red,b=blue,g=green, etc.\n           symbols are '.' for points, ^, for triangle, s for square, etc.\n            -, for lines, -- for dotted lines, see matplotlib online documentation for plot()\n        -eye  ELAT ELON [specify eyeball location]\n        -etp  put on topography\n        -cmap color map [default is jet]\n        -f FILE, specify input file\n        -o color ocean blue/land green (default is not)\n        -res [c,l,i,h] specify resolution (crude, low, intermediate, high]\n        -fmt [pdf,eps, png] specify output format (default is pdf)\n        -R don't plot details of rivers\n        -B don't plot national/state boundaries, etc.\n        -pad [LAT LON] pad bounding box by LAT/LON (default is not)\n        -grd SPACE specify grid spacing\n        -sav  save plot and quit\n        -prj PROJ,  specify one of the supported projections: \n            pc = Plate Carree\n            aea = Albers Equal Area\n            aeqd = Azimuthal Equidistant\n            lcc = Lambert Conformal\n            lcyl = Lambert Cylindrical\n            merc = Mercator\n            mill = Miller Cylindrical\n            moll = Mollweide [default]\n            ortho = Orthographic\n            robin = Robinson\n            sinu = Sinusoidal\n            stere = Stereographic\n            tmerc = Transverse Mercator\n            utm = UTM\n            laea = Lambert Azimuthal Equal Area\n            geos = Geostationary\n            npstere = North-Polar Stereographic\n            spstere = South-Polar Stereographic\n        Special codes for MagIC formatted input files:\n            -n\n            -l\n    \n    INPUTS\n        space or tab delimited LON LAT data\n        OR: \n           standard MagIC formatted er_sites or pmag_results table\n    DEFAULTS\n        res:  c\n        prj: mollweide;  lcc for MagIC format files \n        ELAT,ELON = 0,0\n        pad LAT,LON=0,0\n        NB: high resolution or lines can be very slow\n    \n    \"\"\"\n    dir_path='.'\n    plot=0\n    ocean=0\n    res='c'\n    proj='moll'\n    Lats,Lons=[],[]\n    fmt='pdf'\n    sym='ro'\n    symsize=5\n    fancy=0\n    rivers,boundaries,ocean=1,1,0\n    latmin,latmax,lonmin,lonmax,lat_0,lon_0=-90,90,0.,360.,0.,0.\n    padlat,padlon,gridspace=0,0,30\n    lat_0,lon_0=\"\",\"\"\n    basemap=1\n    prn_name,prn_loc,names,locs=0,0,[],[]\n    if '-WD' in sys.argv:\n        ind = sys.argv.index('-WD')\n        dir_path=sys.argv[ind+1]\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    if '-res' in sys.argv:\n        ind = sys.argv.index('-res')\n        res=sys.argv[ind+1]\n        if res!= 'c' and res!='l':\n            print('this resolution will take a while - be patient')\n    if '-etp' in sys.argv: \n        fancy=1\n        print ('-W- plotting will require patience!')\n    if '-ctp' in sys.argv: basemap=0\n    if '-sav' in sys.argv: plot=1\n    if '-R' in sys.argv:rivers=0\n    if '-B' in sys.argv:boundaries=0\n    if '-o' in sys.argv:ocean=1\n    if '-cmap' in sys.argv:\n        ind = sys.argv.index('-cmap')\n        cmap=float(sys.argv[ind+1])\n    else:\n        cmap='jet'\n    if '-grd' in sys.argv:\n        ind = sys.argv.index('-grd')\n        gridspace=float(sys.argv[ind+1])\n    if '-eye' in sys.argv:\n        ind = sys.argv.index('-eye')\n        lat_0=float(sys.argv[ind+1])\n        lon_0=float(sys.argv[ind+2])\n    if '-sym' in sys.argv:\n        ind = sys.argv.index('-sym')\n        sym=sys.argv[ind+1]\n        symsize=int(sys.argv[ind+2])\n    if '-pad' in sys.argv:\n        ind = sys.argv.index('-pad')\n        padlat=float(sys.argv[ind+1])\n        padlon=float(sys.argv[ind+2])\n    if '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        file=dir_path+'/'+sys.argv[ind+1]\n        header=open(file,'r').readlines()[0].split('\\t')\n        if 'tab' in header[0]:\n            proj='lcc'\n            if 'sites' in header[1]:\n                latkey='lat'\n                lonkey='lon'\n                namekey='site'\n                lockey=''\n            else:  \n                print('file type not supported')\n                print(main.__doc__)\n                sys.exit()\n            Sites,file_type=pmag.magic_read(file)\n            Lats=pmag.get_dictkey(Sites,latkey,'f')\n            Lons=pmag.get_dictkey(Sites,lonkey,'f')\n            if prn_name==1:names=pmag.get_dictkey(Sites,namekey,'')\n            if prn_loc==1:names=pmag.get_dictkey(Sites,lockey,'')\n        else:\n            ptdata=numpy.loadtxt(file)\n            Lons=ptdata.transpose()[0]\n            Lats=ptdata.transpose()[1]\n        latmin=numpy.min(Lats)-padlat\n        lonmin=numpy.min(Lons)-padlon\n        latmax=numpy.max(Lats)+padlat\n        lonmax=numpy.max(Lons)+padlon\n        if lon_0==\"\":\n            lon_0=0.5*(lonmin+lonmax)\n            lat_0=0.5*(latmin+latmax)\n    else:\n        print(\"input file must be specified\")\n        sys.exit()\n    if '-prj' in sys.argv:\n        ind = sys.argv.index('-prj')\n        proj=sys.argv[ind+1]\n    FIG={'map':1}\n    pmagplotlib.plot_init(FIG['map'],6,6)\n    cnt=0\n    Opts={'latmin':latmin,'latmax':latmax,'lonmin':lonmin,'lonmax':lonmax,'lat_0':lat_0,'lon_0':lon_0,'proj':proj,'sym':sym,'symsize':3,'pltgrid':1,'res':res,'boundinglat':0.,'padlon':padlon,'padlat':padlat,'gridspace':gridspace,'cmap':cmap}\n    Opts['details']={}\n    Opts['details']['coasts']=1\n    Opts['details']['rivers']=rivers\n    Opts['details']['states']=boundaries\n    Opts['details']['countries']=boundaries\n    Opts['details']['ocean']=ocean\n    Opts['details']['fancy']=fancy\n    if len(names)>0:Opts['names']=names\n    if len(locs)>0:Opts['loc_name']=locs\n    if proj=='merc':\n        Opts['latmin']=-70\n        Opts['latmax']=70\n        Opts['lonmin']=-180\n        Opts['lonmax']=180\n    print('please wait to draw points')\n    Opts['sym']=sym\n    Opts['symsize']=symsize\n    if basemap: \n        pmagplotlib.plot_map(FIG['map'],Lats,Lons,Opts)\n    else:\n        pmagplotlib.plot_map(FIG['map'],Lats,Lons,Opts)\n    files={}\n    titles={}\n    titles['map']='PT Map'\n    for key in list(FIG.keys()):\n        files[key]='map_pts'+'.'+fmt\n    if pmagplotlib.isServer:\n        black     = '#000000'\n        purple    = '#800080'\n        FIG = pmagplotlib.add_borders(FIG,titles,black,purple)\n        pmagplotlib.save_plots(FIG,files)\n    if plot==1:\n        pmagplotlib.save_plots(FIG,files)\n    else:\n        pmagplotlib.draw_figs(FIG)\n        ans=input(\" S[a]ve to save plot, Return to quit:  \")\n        if ans==\"a\": pmagplotlib.save_plots(FIG,files)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nname n_h is the main function of the pmag. dohext function.", "response": "def main():\n    \"\"\"\n    NAME\n        s_hext.py\n\n    DESCRIPTION\n     calculates Hext statistics for tensor data\n\n    SYNTAX\n        s_hext.py [-h][-i][-f file] [<filename]\n\n    OPTIONS\n        -h prints help message and quits\n        -f file specifies filename on command line\n        -l NMEAS do line by line instead of whole file, use number of measurements NMEAS for degrees of freedom\n        < filename, reads from standard input (Unix like operating systems only)\n\n    INPUT\n        x11,x22,x33,x12,x23,x13,sigma [sigma only if line by line]\n\n    OUTPUT\n       F  F12  F23  sigma\n       and three sets of:\n        tau dec inc Eij dec inc Eik dec inc\n    \n    DEFAULT\n       average whole file\n    \"\"\"\n    ave=1\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-l' in sys.argv:\n        ind=sys.argv.index('-l')\n        npts=int(sys.argv[ind+1])\n        ave=0\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines()\n        f.close()\n    else:\n        data=sys.stdin.readlines()\n    Ss=[]\n    for line in data:\n        s=[]\n        rec=line.split()\n        for i in range(6):\n            s.append(float(rec[i]))\n        if ave==0:\n            sig=float(rec[6])\n            hpars=pmag.dohext(npts-6,sig,s)\n            print('%s %4.2f %s %4.2f %s %4.2f'%('F = ',hpars['F'],'F12 = ',hpars['F12'],'F23 = ',hpars['F23']))\n            print('%s %i %s %14.12f'%('Nmeas = ',npts,' sigma = ',sig))\n            print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t1\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"],hpars[\"e12\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"],hpars[\"e13\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"] ))\n            print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t2\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"],hpars[\"e23\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"],hpars[\"e12\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"] ))\n            print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t3\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"],hpars[\"e13\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"],hpars[\"e23\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"] ))\n        else:\n            Ss.append(s)\n    if ave==1:\n        npts=len(Ss)\n        nf,sigma,avs=pmag.sbar(Ss)\n        hpars=pmag.dohext(nf,sigma,avs)\n        print('%s %4.2f %s %4.2f %s %4.2f'%('F = ',hpars['F'],'F12 = ',hpars['F12'],'F23 = ',hpars['F23']))\n        print('%s %i %s %14.12f'%('N = ',npts,' sigma = ',sigma))\n        print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t1\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"],hpars[\"e12\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"],hpars[\"e13\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"] ))\n        print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t2\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"],hpars[\"e23\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"],hpars[\"e12\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"] ))\n        print('%7.5f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f'%(hpars[\"t3\"],hpars[\"v3_dec\"],hpars[\"v3_inc\"],hpars[\"e13\"],hpars[\"v1_dec\"],hpars[\"v1_inc\"],hpars[\"e23\"],hpars[\"v2_dec\"],hpars[\"v2_inc\"] ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _UTMLetterDesignator(Lat):\n\n        if   84 >= Lat >= 72: return 'X'\n        elif 72 >  Lat >= 64: return 'W'\n        elif 64 >  Lat >= 56: return 'V'\n        elif 56 >  Lat >= 48: return 'U'\n        elif 48 >  Lat >= 40: return 'T'\n        elif 40 >  Lat >= 32: return 'S'\n        elif 32 >  Lat >= 24: return 'R'\n        elif 24 >  Lat >= 16: return 'Q'\n        elif 16 >  Lat >= 8:  return 'P'\n        elif  8 >  Lat >= 0:  return 'N'\n        elif  0 >  Lat >=-8:  return 'M'\n        elif -8 >  Lat >=-16: return 'L'\n        elif -16 > Lat >=-24: return 'K'\n        elif -24 > Lat >=-32: return 'J'\n        elif -32 > Lat >=-40: return 'H'\n        elif -40 > Lat >=-48: return 'G'\n        elif -48 > Lat >=-56: return 'F'\n        elif -56 > Lat >=-64: return 'E'\n        elif -64 > Lat >=-72: return 'D'\n        elif -72 > Lat >=-80: return 'C'\n        else:                 return 'Z'", "response": "This routine determines the correct UTM letter designator for the given latitude."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting UTM coordinates to Lat Long.", "response": "def UTMtoLL(ReferenceEllipsoid, easting, northing, zone):\n        \"\"\"\n        converts UTM coords to lat/long.  Equations from USGS Bulletin 1532\n        East Longitudes are positive, West longitudes are negative.\n        North latitudes are positive, South latitudes are negative\n        Lat and Long are in decimal degrees.\n        Written by Chuck Gantz- chuck.gantz@globalstar.com\n        Converted to Python by Russ Nelson <nelson@crynwr.com>\n        \"\"\"\n\n        k0 = 0.9996\n        a = _ellipsoid[ReferenceEllipsoid][_EquatorialRadius]\n        eccSquared = _ellipsoid[ReferenceEllipsoid][_eccentricitySquared]\n        e1 = old_div((1-sqrt(1-eccSquared)),(1+sqrt(1-eccSquared)))\n        #NorthernHemisphere; //1 for northern hemispher, 0 for southern\n\n        x = easting-500000.0 #remove 500,000 meter offset for longitude\n        y = northing\n\n        ZoneLetter = zone[-1]\n        if ZoneLetter == 'Z':\n                raise Exception(\"Latitude is outside the UTM limits\")\n\n        ZoneNumber = int(zone[:-1])\n        if ZoneLetter >= 'N':\n                NorthernHemisphere = 1  # point is in northern hemisphere\n        else:\n                NorthernHemisphere = 0  # point is in southern hemisphere\n                y-= 10000000.0          # remove 10,000,000 meter offset used for southern hemisphere\n\n        LongOrigin = (ZoneNumber-1)*6-180+3  # +3 puts origin in middle of zone\n\n        eccPrimeSquared = old_div((eccSquared),(1-eccSquared))\n\n        M = old_div(y, k0)\n        mu = old_div(M,(a*(1-old_div(eccSquared,4)-3*eccSquared*eccSquared/64-5*eccSquared*eccSquared*eccSquared/256)))\n\n        phi1Rad = (mu+(3*e1/2-27*e1*e1*e1/32)*sin(2*mu)\n                  +(21*e1*e1/16-55*e1*e1*e1*e1/32)*sin(4*mu)\n                  +(151*e1*e1*e1/96)*sin(6*mu))\n        phi1 = degrees(phi1Rad);\n\n        N1 = old_div(a,sqrt(1-eccSquared*sin(phi1Rad)*sin(phi1Rad)))\n        T1 = tan(phi1Rad)*tan(phi1Rad)\n        C1 = eccPrimeSquared*cos(phi1Rad)*cos(phi1Rad)\n        R1 = a*(1-eccSquared)/pow(1-eccSquared*sin(phi1Rad)*sin(phi1Rad), 1.5)\n        D = old_div(x,(N1*k0))\n\n        Lat = phi1Rad-(N1*tan(phi1Rad)/R1)*(D*D/2-(5+3*T1+10*C1-4*C1*C1-9*eccPrimeSquared)*D*D*D*D/24\n                     +(61+90*T1+298*C1+45*T1*T1-252*eccPrimeSquared-3*C1*C1)*D*D*D*D*D*D/720)\n        Lat = degrees(Lat)\n\n        Long = old_div((D-(1+2*T1+C1)*D*D*D/6+(5-2*C1+28*T1-3*C1*C1+8*eccPrimeSquared+24*T1*T1)*D*D*D*D*D/120),cos(phi1Rad))\n        Long = LongOrigin+degrees(Long)\n        return (Long, Lat)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the main user interface.", "response": "def InitUI(self):\n        \"\"\"\n        Make main user interface\n        \"\"\"\n        bSizer0 = wx.StaticBoxSizer(\n            wx.StaticBox(self.panel, wx.ID_ANY, \"Choose MagIC project directory\", name='bSizer0'), wx.HORIZONTAL\n        )\n        self.dir_path = wx.TextCtrl(self.panel, id=-1, size=(600, 25), style=wx.TE_READONLY)\n        self.dir_path.SetValue(self.WD)\n        self.change_dir_button = buttons.GenButton(\n            self.panel, id=-1, label=\"change directory\", size=(-1, -1), name='change_dir_btn'\n        )\n        self.change_dir_button.SetBackgroundColour(\"#F8F8FF\")\n        self.change_dir_button.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.on_change_dir_button, self.change_dir_button)\n        bSizer0.Add(self.change_dir_button, wx.ALIGN_LEFT)\n        bSizer0.AddSpacer(40)\n        bSizer0.Add(self.dir_path, wx.ALIGN_CENTER_VERTICAL)\n\n        self.bSizer_msg = wx.StaticBoxSizer(wx.StaticBox(\n            self.panel, wx.ID_ANY, \"Message\", name='bsizer_msg'),\n                                            wx.HORIZONTAL)\n        self.message = wx.StaticText(self.panel, -1,\n                                     label=\"Some text will be here\",\n                                     name='messages')\n        self.bSizer_msg.Add(self.message)\n\n        #---sizer 1 ----\n        bSizer1 = wx.StaticBoxSizer(wx.StaticBox(\n            self.panel, wx.ID_ANY, \"Add information to the data model\", name='bSizer1'),\n                                    wx.HORIZONTAL)\n\n        text = \"1. add location data\"\n        self.btn1 = buttons.GenButton(self.panel, id=-1, label=text,\n                                      size=(300, 50), name='locations_btn')\n        self.btn1.SetBackgroundColour(\"#FDC68A\")\n        self.btn1.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn1)\n\n        text = \"2. add site data\"\n        self.btn2 = buttons.GenButton(self.panel, id=-1, label=text,\n                                      size=(300, 50), name='sites_btn')\n        self.btn2.SetBackgroundColour(\"#6ECFF6\")\n        self.btn2.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn2)\n\n\n        text = \"3. add sample data\"\n        self.btn3 = buttons.GenButton(self.panel, id=-1, label=text,\n                                      size=(300, 50), name='samples_btn')\n        self.btn3.SetBackgroundColour(\"#C4DF9B\")\n        self.btn3.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn3)\n\n\n        text = \"4. add specimen data\"\n        self.btn4 = buttons.GenButton(self.panel, id=-1,\n                                      label=text, size=(300, 50), name='specimens_btn')\n        self.btn4.SetBackgroundColour(\"#FDC68A\")\n        self.btn4.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn4)\n\n\n        text = \"5. add age data\"\n        self.btn5 = buttons.GenButton(self.panel, id=-1, label=text,\n                                      size=(300, 50), name='ages_btn')\n        self.btn5.SetBackgroundColour(\"#6ECFF6\")\n        self.btn5.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn5)\n\n        text = \"6. add measurements data\"\n        self.btn6 = buttons.GenButton(self.panel, id=-1, label=text,\n                                      size=(300, 50), name='measurements_btn')\n        self.btn6.SetBackgroundColour(\"#C4DF9B\")\n        self.btn6.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.make_grid_frame, self.btn6)\n\n        bsizer1a = wx.BoxSizer(wx.VERTICAL)\n        bsizer1a.AddSpacer(20)\n        bsizer1a.Add(self.btn1, wx.ALIGN_TOP)\n        bsizer1a.AddSpacer(20)\n        bsizer1a.Add(self.btn2, wx.ALIGN_TOP)\n        bsizer1a.AddSpacer(20)\n        bsizer1a.Add(self.btn3, wx.ALIGN_TOP)\n        bsizer1a.AddSpacer(20)\n\n        bSizer1.Add(bsizer1a, wx.ALIGN_CENTER, wx.EXPAND)\n        bSizer1.AddSpacer(20)\n\n        #bSizer1.Add(OR, 0, wx.ALIGN_CENTER, 0)\n        bSizer1.AddSpacer(20)\n        bsizer1b = wx.BoxSizer(wx.VERTICAL)\n        #__init__(self, parent, id, label, pos, size, style, validator, name\n        bsizer1b.Add(self.btn4, flag=wx.ALIGN_CENTER|wx.BOTTOM, border=20)\n        bsizer1b.Add(self.btn5, 0, flag=wx.ALIGN_CENTER|wx.BOTTOM, border=20)\n        bsizer1b.Add(self.btn6, 0, wx.ALIGN_CENTER, 0)\n        bSizer1.Add(bsizer1b, 0, wx.ALIGN_CENTER, 0)\n        bSizer1.AddSpacer(20)\n\n        #---sizer 2 ----\n\n        self.bSizer2 = wx.StaticBoxSizer(wx.StaticBox(self.panel, wx.ID_ANY, \"Create file for upload to MagIC database\", name='bSizer2'), wx.HORIZONTAL)\n\n        text = \"prepare upload txt file\"\n        self.btn_upload = buttons.GenButton(self.panel, id=-1, label=text,\n                                            size=(300, 50), name='upload_btn')\n        self.btn_upload.SetBackgroundColour(\"#C4DF9B\")\n        self.btn_upload.InitColours()\n        self.Bind(wx.EVT_BUTTON, self.on_upload_file, self.btn_upload)\n\n        self.bSizer2.AddSpacer(20)\n        self.bSizer2.Add(self.btn_upload, 0, wx.ALIGN_CENTER, 0)\n        self.bSizer2.AddSpacer(20)\n        #self.Bind(wx.EVT_BUTTON, self.on_btn_upload, self.btn_upload)\n\n\n        #---arrange sizers ----\n\n        self.hbox = wx.BoxSizer(wx.HORIZONTAL)\n        vbox = wx.BoxSizer(wx.VERTICAL)\n        vbox.AddSpacer(5)\n        #vbox.Add(self.logo,0,wx.ALIGN_CENTER,0)\n        vbox.AddSpacer(5)\n        vbox.Add(bSizer0, 0, wx.ALIGN_CENTER, 0)\n        vbox.AddSpacer(10)\n        #vbox.Add(bSizer0_1, 0, wx.ALIGN_CENTER, 0)\n        #vbox.AddSpacer(10)\n        vbox.Add(self.bSizer_msg, 0, wx.ALIGN_CENTER, 0)\n        self.bSizer_msg.ShowItems(False)\n        vbox.Add(bSizer1, 0, wx.ALIGN_CENTER, 0)\n        vbox.AddSpacer(10)\n        vbox.AddSpacer(10)\n        self.hbox.AddSpacer(10)\n        vbox.Add(self.bSizer2, 0, wx.ALIGN_CENTER, 0)\n        vbox.AddSpacer(10)\n\n        self.hbox.Add(vbox, 0, wx.ALIGN_CENTER, 0)\n        self.hbox.AddSpacer(5)\n\n        self.panel.SetSizer(self.hbox)\n        self.hbox.Fit(self)\n\n        # do menu\n        print(\"-I- Initializing menu\")\n        menubar = MagICMenu(self)\n        self.SetMenuBar(menubar)\n        self.menubar = menubar"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_change_dir_button(self, event=None):\n        currentDirectory = self.WD #os.getcwd()\n        change_dir_dialog = wx.DirDialog(self.panel,\n                                         \"Choose your working directory to create or edit a MagIC contribution:\",\n                                         defaultPath=currentDirectory,\n                                         style=wx.DD_DEFAULT_STYLE | wx.DD_NEW_DIR_BUTTON | wx.DD_CHANGE_DIR)\n        result = change_dir_dialog.ShowModal()\n        if result == wx.ID_CANCEL:\n            return\n        if result == wx.ID_OK:\n            self.WD = change_dir_dialog.GetPath()\n            self.dir_path.SetValue(self.WD)\n        change_dir_dialog.Destroy()\n        self.get_wd_data()", "response": "create change directory frame"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_grid_frame(self, event):\n        if self.grid_frame:\n            print('-I- You already have a grid frame open')\n            pw.simple_warning(\"You already have a grid open\")\n            return\n\n        try:\n            grid_type = event.GetButtonObj().Name[:-4] # remove '_btn'\n        except AttributeError:\n            grid_type = self.FindWindowById(event.Id).Name[:-4] # remove ('_btn')\n        wait = wx.BusyInfo('Making {} grid, please wait...'.format(grid_type))\n        wx.SafeYield()\n        # propagate site lat/lon info into locations if necessary\n        if grid_type == 'locations' and 'sites' in self.contribution.tables:\n            self.contribution.get_min_max_lat_lon()\n            self.contribution.propagate_cols_up(['lithologies',\n                                                 'geologic_classes'],\n                                                'locations', 'sites')\n        # propagate lithologies/type/class information from sites to samples/specimens\n        if grid_type in ['specimens', 'samples']:\n            self.contribution.propagate_lithology_cols()\n        # propagate average lat/lon info from samples table if\n        # available in samples and missing in sites\n        if grid_type == 'sites':\n            self.contribution.propagate_average_up(cols=['lat', 'lon', 'height'],\n                                           target_df_name='sites',\n                                           source_df_name='samples')\n            self.contribution.propagate_lithology_cols()\n        # hide mainframe\n        self.on_open_grid_frame()\n        # choose appropriate size for grid\n        if grid_type == 'measurements':\n            huge = True\n        else:\n            huge = False\n        # make grid frame\n        self.grid_frame = grid_frame.GridFrame(self.contribution, self.WD,\n                                               grid_type, grid_type,\n                                               self.panel, huge=huge)\n        row_string = \"\"\n        # paint validations if appropriate\n        if self.validation_mode:\n            if grid_type in self.validation_mode:\n                if grid_type == 'measurements':\n                    skip_cell_render = True\n                else:\n                    skip_cell_render = False\n                self.grid_frame.toggle_help(None, \"open\")\n                row_problems = self.failing_items[grid_type][\"rows\"]\n                missing_columns = self.failing_items[grid_type][\"missing_columns\"]\n                missing_groups = self.failing_items[grid_type][\"missing_groups\"]\n                #all_cols = row_problems.columns\n                #col_nums = range(len(all_cols))\n                #col_pos = dict(zip(all_cols, col_nums))\n                if len(row_problems):\n                    row_string = \"Columns and rows with problem data have been highlighted in blue.\\n\"\n                    if not skip_cell_render:\n                        row_string += \"Cells with problem data are highlighted according to the type of problem.\\nRed: incorrect data\\n\"\n                    row_string += \"For full error messages, see {}.\".format(grid_type + \"_errors.txt\")\n                    # reset codes button to show error file instead\n                    self.grid_frame.toggle_codes_btn.SetLabel(\"Show errors\")\n                    self.grid_frame.Bind(wx.EVT_BUTTON, self.grid_frame.show_errors,\n                                         self.grid_frame.toggle_codes_btn)\n                    # paint cells\n                    for row in row_problems['num']:\n                        self.grid_frame.grid.paint_invalid_row(row)\n                        mask = row_problems[\"num\"] == row\n                        items = row_problems[mask]\n                        cols = items.dropna(how=\"all\", axis=1).drop([\"num\", \"issues\"], axis=1)\n                        for col in cols:\n                            pre, col_name = val_up3.extract_col_name(col)\n                            col_ind = self.grid_frame.grid.col_labels.index(col_name)\n                            self.grid_frame.grid.paint_invalid_cell(row, col_ind,\n                                                                    skip_cell=skip_cell_render)\n                current_label = self.grid_frame.msg_text.GetLabel()\n                if len(missing_columns):\n                    col_string = \"You are missing the following required columns: {}\\n\\n\".format(\", \".join(missing_columns))\n                else:\n                    col_string = \"\"\n                if len(missing_groups):\n                    group_string = \"You must have at least one column from each of the following groups: {}\\n\\n\".format(\", \".join(missing_groups))\n                else:\n                    group_string = \"\"\n                #\n                add_text = \"\"\"{}{}{}\"\"\".format(col_string, group_string, row_string)\n                self.grid_frame.msg_text.SetLabel(add_text)\n        #self.on_finish_change_dir(self.change_dir_dialog)\n        self.grid_frame.do_fit(None)\n        del wait", "response": "Create a grid frame for the current event"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhighlights grids in red if they have validation errors", "response": "def highlight_problems(self, has_problems):\n        \"\"\"\n        Outline grid buttons in red if they have validation errors\n        \"\"\"\n        if has_problems:\n            self.validation_mode = set(has_problems)\n            # highlighting doesn't work with Windows\n            if sys.platform in ['win32', 'win62']:\n                self.message.SetLabel('The following grid(s) have incorrect or incomplete data:\\n{}'.format(', '.join(self.validation_mode)))\n            # highlighting does work with OSX\n            else:\n                for dtype in [\"specimens\", \"samples\", \"sites\", \"locations\", \"ages\", \"measurements\"]:\n                    wind = self.FindWindowByName(dtype + '_btn')\n                    if dtype not in has_problems:\n                        wind.Unbind(wx.EVT_PAINT, handler=self.highlight_button)\n                    else:\n                        wind.Bind(wx.EVT_PAINT, self.highlight_button)\n                self.Refresh()\n                self.message.SetLabel('Highlighted grids have incorrect or incomplete data')\n            self.bSizer_msg.ShowItems(True)\n            # manually fire a paint event to make sure all buttons\n            # are highlighted/unhighlighted appropriately\n            paintEvent = wx.CommandEvent(wx.wxEVT_PAINT,\n                                         self.GetId())\n            self.GetEventHandler().ProcessEvent(paintEvent)\n\n        else:\n            self.message.SetLabel(\"Validated!\")\n            self.bSizer_msg.ShowItems(True)\n        self.hbox.Fit(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset_highlights(self):\n        for dtype in [\"specimens\", \"samples\", \"sites\", \"locations\", \"ages\"]:\n            wind = self.FindWindowByName(dtype + '_btn')\n            wind.Unbind(wx.EVT_PAINT, handler=self.highlight_button)\n        self.Refresh()\n        #self.message.SetLabel('Highlighted grids have incorrect or incomplete data')\n        self.bSizer_msg.ShowItems(False)\n        self.hbox.Fit(self)", "response": "Remove red outlines from all buttons and update the grids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw a red highlight line around the event object", "response": "def highlight_button(self, event):\n        \"\"\"\n        Draw a red highlight line around the event object\n        \"\"\"\n        wind = event.GetEventObject()\n        pos = wind.GetPosition()\n        size = wind.GetSize()\n        try:\n            dc = wx.PaintDC(self)\n        except wx._core.PyAssertionError:\n            # if it's not a native paint event, we can't us wx.PaintDC\n            dc = wx.ClientDC(self)\n        dc.SetPen(wx.Pen('red', 5, wx.SOLID))\n        dc.DrawRectangle(pos[0], pos[1], size[0], size[1])\n        event.Skip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_clear(self, event):\n        dia = pmag_menu_dialogs.ClearWD(self.parent, self.parent.WD)\n        clear = dia.do_clear()\n        if clear:\n            print('-I- Clear data object')\n            self.contribution = cb.Contribution(self.WD, dmodel=self.data_model)\n            self.edited = False", "response": "Clear the data object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_close_grid(self, event):\n        if self.parent.grid_frame:\n            self.parent.grid_frame.onSave(None)\n            self.parent.grid_frame.Destroy()", "response": "Save and close the grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    firstline,itilt,igeo,linecnt,key=1,0,0,0,\"\"\n    out=\"\"\n    data,k15=[],[]\n    dir='./'\n    ofile=\"\"\n    if '-WD' in sys.argv:\n        ind=sys.argv.index('-WD')\n        dir=sys.argv[ind+1]+'/'\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-i' in sys.argv:\n        file=input(\"Input file name [.k15 format]: \")\n        f=open(file,'r')\n        data=f.readlines()\n        f.close()\n        file=input(\"Output file name [.s format]: \")\n        out=open(file,'w')\n        print (\" [g]eographic, [t]ilt corrected, \")\n        tg=input(\" [return for specimen coordinates]: \")  \n        if tg=='g': \n            igeo=1\n        elif tg=='t':\n            igeo,itilt=1,1\n    elif '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=dir+sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines()\n        f.close()\n    else:\n        data= sys.stdin.readlines()\n    if len(data)==0:\n        print(main.__doc__)\n        sys.exit()\n    if '-F' in sys.argv:\n        ind=sys.argv.index('-F')\n        ofile=dir+sys.argv[ind+1]\n        out=open(ofile,'w')\n    if '-crd' in sys.argv:\n        ind=sys.argv.index('-crd')\n        tg=sys.argv[ind+1] \n        if tg=='g':igeo=1\n        if tg=='t': igeo,itilt=1,1\n    for line in data:\n        rec=line.split()\n        if firstline==1:\n            firstline=0\n            nam=rec[0]\n            if igeo==1: az,pl=float(rec[1]),float(rec[2])\n            if itilt==1: bed_az,bed_dip=90.+float(rec[3]),float(rec[4])\n        else: \n            linecnt+=1\n            for i in range(5):\n                k15.append(float(rec[i]))\n            if linecnt==3:\n                sbar,sigma,bulk=pmag.dok15_s(k15) \n                if igeo==1: sbar=pmag.dosgeo(sbar,az,pl) \n                if itilt==1: sbar=pmag.dostilt(sbar,bed_az,bed_dip) \n                outstring=\"\"\n                for s in sbar:outstring+='%10.8f '%(s)\n                outstring+='%10.8f'%(sigma)\n                if out==\"\":\n                    print(outstring)\n                else:\n                    out.write(outstring+'\\n')\n                linecnt,firstline,k15=0,1,[]\n    if ofile!=\"\":print ('Output saved in ',ofile)", "response": "NAME k15_s. py\n    \n       . py"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nname zeq_magic_redo.py DESCRIPTION Calculate principal components through demagnetization data using bounds and calculation type stored in \"redo\" file SYNTAX zeq_magic_redo.py [command line options] OPTIONS -h prints help message -usr USER: identify user, default is \"\" -f: specify input file, default is magic_measurements.txt -F: specify output file, default is zeq_specimens.txt -fre REDO: specify redo file, default is \"zeq_redo\" -fsa SAMPFILE: specify er_samples format file, default is \"er_samples.txt\" -A : don't average replicate measurements, default is yes -crd [s,g,t] : specify coordinate system [s,g,t] [default is specimen coordinates] are specimen, geographic, and tilt corrected respectively NB: you must have a SAMPFILE in this directory to rotate from specimen coordinates -leg: attaches \"Recalculated from original measurements; supercedes published results. \" to comment field INPUTS zeq_redo format file is: specimen_name calculation_type[DE-BFL,DE-BFL-A,DE-BFL-O,DE-BFP,DE-FM] step_min step_max component_name[A,B,C]", "response": "def main():\n    \"\"\"\n    NAME\n        zeq_magic_redo.py\n   \n    DESCRIPTION\n        Calculate principal components through demagnetization data using bounds and calculation type stored in \"redo\" file\n  \n    SYNTAX\n        zeq_magic_redo.py [command line options]\n\n    OPTIONS\n        -h prints help message\n        -usr USER:   identify user, default is \"\"\n        -f: specify input file, default is magic_measurements.txt\n        -F: specify output file, default is zeq_specimens.txt\n        -fre  REDO: specify redo file, default is \"zeq_redo\"\n        -fsa  SAMPFILE: specify er_samples format file, default is \"er_samples.txt\"\n        -A : don't average replicate measurements, default is yes\n        -crd [s,g,t] : \n             specify coordinate system [s,g,t]  [default is specimen coordinates]\n                 are specimen, geographic, and tilt corrected respectively\n             NB: you must have a SAMPFILE in this directory to rotate from specimen coordinates\n        -leg:  attaches \"Recalculated from original measurements; supercedes published results. \" to comment field\n    INPUTS\n        zeq_redo format file is:\n        specimen_name calculation_type[DE-BFL,DE-BFL-A,DE-BFL-O,DE-BFP,DE-FM]  step_min step_max component_name[A,B,C]\n    \"\"\"\n    dir_path='.'\n    INCL=[\"LT-NO\",\"LT-AF-Z\",\"LT-T-Z\",\"LT-M-Z\"] # looking for demag data\n    beg,end,pole,geo,tilt,askave,save=0,0,[],0,0,0,0\n    user,doave,comment= \"\",1,\"\"\n    geo,tilt=0,0\n    version_num=pmag.get_version()\n    args=sys.argv\n    if '-WD' in args:\n        ind=args.index('-WD')\n        dir_path=args[ind+1]\n    meas_file,pmag_file,mk_file= dir_path+\"/\"+\"magic_measurements.txt\",dir_path+\"/\"+\"zeq_specimens.txt\",dir_path+\"/\"+\"zeq_redo\"\n    samp_file,coord=dir_path+\"/\"+\"er_samples.txt\",\"\"\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    if \"-usr\" in args:\n        ind=args.index(\"-usr\")\n        user=sys.argv[ind+1]\n    if \"-A\" in args:doave=0\n    if \"-leg\" in args: comment=\"Recalculated from original measurements; supercedes published results. \"\n    if \"-f\" in args:\n        ind=args.index(\"-f\")\n        meas_file=dir_path+'/'+sys.argv[ind+1]\n    if \"-F\" in args:\n        ind=args.index(\"-F\")\n        pmag_file=dir_path+'/'+sys.argv[ind+1]\n    if \"-fre\" in args:\n        ind=args.index(\"-fre\")\n        mk_file=dir_path+\"/\"+args[ind+1]\n    try:\n        mk_f=open(mk_file,'r')\n    except:\n        print(\"Bad redo file\")\n        sys.exit()\n    mkspec,skipped=[],[]\n    speclist=[]\n    for line in mk_f.readlines():\n        tmp=line.split()\n        mkspec.append(tmp)\n        speclist.append(tmp[0])\n    if \"-fsa\" in args:\n        ind=args.index(\"-fsa\")\n        samp_file=dir_path+'/'+sys.argv[ind+1]\n    if \"-crd\" in args:\n        ind=args.index(\"-crd\")\n        coord=sys.argv[ind+1]\n        if coord==\"g\":geo,tilt=1,0\n        if coord==\"t\":geo,tilt=1,1\n#\n# now get down to bidness\n    if geo==1:\n        samp_data,file_type=pmag.magic_read(samp_file)\n        if file_type != 'er_samples':\n            print(file_type)\n            print(\"This is not a valid er_samples file \") \n            sys.exit()\n    #\n    #\n    #\n\n    meas_data,file_type=pmag.magic_read(meas_file)\n    if file_type != 'magic_measurements':\n        print(file_type)\n        print(file_type,\"This is not a valid magic_measurements file \") \n        sys.exit()\n    #\n    # sort the specimen names\n    #\n    k = 0\n    print('Processing ',len(speclist), ' specimens - please wait')\n    PmagSpecs=[]\n    while k < len(speclist):\n        s=speclist[k]\n        recnum=0\n        PmagSpecRec={}\n        method_codes,inst_codes=[],[]\n    # find the data from the meas_data file for this sample\n    #\n    #  collect info for the PmagSpecRec dictionary\n    #\n        meas_meth=[]\n        spec=pmag.get_dictitem(meas_data,'er_specimen_name',s,'T')   \n        if len(spec)==0:\n            print('no data found for specimen:  ',s)\n            print('delete from zeq_redo input file...., then try again')\n        else: \n          for rec in  spec: # copy of vital stats to PmagSpecRec from first spec record in demag block\n           skip=1\n           methods=rec[\"magic_method_codes\"].split(\":\")\n           if len(set(methods) & set(INCL))>0:\n                   PmagSpecRec[\"er_analyst_mail_names\"]=user\n                   PmagSpecRec[\"magic_software_packages\"]=version_num\n                   PmagSpecRec[\"er_specimen_name\"]=s\n                   PmagSpecRec[\"er_sample_name\"]=rec[\"er_sample_name\"]\n                   PmagSpecRec[\"er_site_name\"]=rec[\"er_site_name\"]\n                   PmagSpecRec[\"er_location_name\"]=rec[\"er_location_name\"]\n                   if \"er_expedition_name\" in list(rec.keys()):PmagSpecRec[\"er_expedition_name\"]=rec[\"er_expedition_name\"]\n                   PmagSpecRec[\"er_citation_names\"]=\"This study\"\n                   if \"magic_experiment_name\" not in list(rec.keys()): rec[\"magic_experiment_name\"]=\"\"\n                   PmagSpecRec[\"magic_experiment_names\"]=rec[\"magic_experiment_name\"]\n                   if \"magic_instrument_codes\" not in list(rec.keys()): rec[\"magic_instrument_codes\"]=\"\"\n                   inst=rec['magic_instrument_codes'].split(\":\")\n                   for I in inst:\n                       if I not in inst_codes:  # copy over instruments\n                           inst_codes.append(I)\n                   meths=rec[\"magic_method_codes\"].split(\":\")\n                   for meth in meths:\n                       if meth.strip() not in meas_meth:meas_meth.append(meth)\n                   if \"LP-DIR-AF\" in meas_meth or \"LT-AF-Z\" in meas_meth: \n                       PmagSpecRec[\"measurement_step_unit\"]=\"T\"\n                       if \"LP-DIR-AF\" not in method_codes:method_codes.append(\"LP-DIR-AF\") \n                   if \"LP-DIR-T\" in meas_meth or \"LT-T-Z\" in meas_meth: \n                       PmagSpecRec[\"measurement_step_unit\"]=\"K\"\n                       if \"LP-DIR-T\" not in method_codes:method_codes.append(\"LP-DIR-T\") \n                   if \"LP-DIR-M\" in meas_meth or \"LT-M-Z\" in meas_meth: \n                       PmagSpecRec[\"measurement_step_unit\"]=\"J\"\n                       if \"LP-DIR-M\" not in method_codes:method_codes.append(\"LP-DIR-M\") \n    #\n    #\n        datablock,units=pmag.find_dmag_rec(s,spec) # fish out the demag data for this specimen\n    #\n        if len(datablock) <2 or s not in speclist : \n            k+=1\n#            print 'skipping ', s,len(datablock)\n        else:\n        #\n        # find replicate measurements at given treatment step and average them\n        #\n#            step_meth,avedata=pmag.vspec(data)\n#\n#            if len(avedata) != len(datablock):\n#                if doave==1: \n#                    method_codes.append(\"DE-VM\")\n#                    datablock=avedata\n        #\n        # do geo or stratigraphic correction now\n        #\n            if geo==1 or tilt==1:\n       # find top priority orientation method\n                orient,az_type=pmag.get_orient(samp_data,PmagSpecRec[\"er_sample_name\"])\n                if az_type not in method_codes:method_codes.append(az_type)\n        #\n        #  if tilt selected,  get stratigraphic correction\n        #\n                tiltblock,geoblock=[],[]\n                for rec in datablock:\n                    if \"sample_azimuth\" in list(orient.keys()) and orient[\"sample_azimuth\"]!=\"\":\n                        d_geo,i_geo=pmag.dogeo(rec[1],rec[2],float(orient[\"sample_azimuth\"]),float(orient[\"sample_dip\"]))\n                        geoblock.append([rec[0],d_geo,i_geo,rec[3],rec[4],rec[5]])\n                        if tilt==1 and \"sample_bed_dip_direction\" in list(orient.keys()): \n                            d_tilt,i_tilt=pmag.dotilt(d_geo,i_geo,float(orient[\"sample_bed_dip_direction\"]),float(orient[\"sample_bed_dip\"]))\n                            tiltblock.append([rec[0],d_tilt,i_tilt,rec[3],rec[4],rec[5]])\n                        elif tilt==1:\n                            if PmagSpecRec[\"er_sample_name\"] not in skipped:\n                                print('no tilt correction for ', PmagSpecRec[\"er_sample_name\"],' skipping....')\n                                skipped.append(PmagSpecRec[\"er_sample_name\"])\n                    else:\n                        if PmagSpecRec[\"er_sample_name\"] not in skipped:\n                            print('no geographic correction for ', PmagSpecRec[\"er_sample_name\"],' skipping....')\n                            skipped.append(PmagSpecRec[\"er_sample_name\"])\n    #\n    #\tget beg_pca, end_pca, pca\n            if PmagSpecRec['er_sample_name'] not in skipped:\n                compnum=-1\n                for spec in mkspec:\n                    if spec[0]==s:\n                        CompRec={}\n                        for key in list(PmagSpecRec.keys()):CompRec[key]=PmagSpecRec[key]\n                        compnum+=1\n                        calculation_type=spec[1]\n                        beg=float(spec[2])\n                        end=float(spec[3])\n                        if len(spec)>4:\n                            comp_name=spec[4]\n                        else:\n                            comp_name=string.uppercase[compnum]\n                        CompRec['specimen_comp_name']=comp_name\n                        if beg < float(datablock[0][0]):beg=float(datablock[0][0])\n                        if end > float(datablock[-1][0]):end=float(datablock[-1][0])\n                        for l  in range(len(datablock)):\n                            if datablock[l][0]==beg:beg_pca=l\n                            if datablock[l][0]==end:end_pca=l\n                        if geo==1 and tilt==0:\n                            mpars=pmag.domean(geoblock,beg_pca,end_pca,calculation_type)\n                            if mpars[\"specimen_direction_type\"]!=\"Error\":\n                                CompRec[\"specimen_dec\"]='%7.1f ' %(mpars[\"specimen_dec\"])\n                                CompRec[\"specimen_inc\"]='%7.1f ' %(mpars[\"specimen_inc\"])\n                                CompRec[\"specimen_tilt_correction\"]='0'\n                        if geo==1 and tilt==1:\n                            mpars=pmag.domean(tiltblock,beg_pca,end_pca,calculation_type)\n                            if mpars[\"specimen_direction_type\"]!=\"Error\":\n                                CompRec[\"specimen_dec\"]='%7.1f ' %(mpars[\"specimen_dec\"])\n                                CompRec[\"specimen_inc\"]='%7.1f ' %(mpars[\"specimen_inc\"])\n                                CompRec[\"specimen_tilt_correction\"]='100'\n                        if geo==0 and tilt==0: \n                            mpars=pmag.domean(datablock,beg_pca,end_pca,calculation_type)\n                            if mpars[\"specimen_direction_type\"]!=\"Error\":\n                                CompRec[\"specimen_dec\"]='%7.1f ' %(mpars[\"specimen_dec\"])\n                                CompRec[\"specimen_inc\"]='%7.1f ' %(mpars[\"specimen_inc\"])\n                                CompRec[\"specimen_tilt_correction\"]='-1'\n                        if mpars[\"specimen_direction_type\"]==\"Error\": \n                            pass\n                        else: \n                            CompRec[\"measurement_step_min\"]='%8.3e '%(datablock[beg_pca][0])\n                            try:\n                                CompRec[\"measurement_step_max\"]='%8.3e '%(datablock[end_pca][0] )\n                            except:\n                                print('error in end_pca ',PmagSpecRec['er_specimen_name'])\n                            CompRec[\"specimen_correction\"]='u'\n                            if calculation_type!='DE-FM':\n                                CompRec[\"specimen_mad\"]='%7.1f '%(mpars[\"specimen_mad\"])\n                                CompRec[\"specimen_alpha95\"]=\"\"\n                            else:\n                                CompRec[\"specimen_mad\"]=\"\"\n                                CompRec[\"specimen_alpha95\"]='%7.1f '%(mpars[\"specimen_alpha95\"])\n                            CompRec[\"specimen_n\"]='%i '%(mpars[\"specimen_n\"])\n                            CompRec[\"specimen_dang\"]='%7.1f '%(mpars[\"specimen_dang\"])\n                            CompMeths=[]\n                            for meth in method_codes:\n                                if meth not in CompMeths:CompMeths.append(meth)\n                            if calculation_type not in CompMeths:CompMeths.append(calculation_type)\n                            if geo==1: CompMeths.append(\"DA-DIR-GEO\")\n                            if tilt==1: CompMeths.append(\"DA-DIR-TILT\")\n                            if \"DE-BFP\" not in calculation_type:\n                                CompRec[\"specimen_direction_type\"]='l'\n                            else:\n                                CompRec[\"specimen_direction_type\"]='p'\n                            CompRec[\"magic_method_codes\"]=\"\"\n                            if len(CompMeths) != 0:\n                                methstring=\"\"\n                                for meth in CompMeths:\n                                    methstring=methstring+ \":\" +meth\n                                CompRec[\"magic_method_codes\"]=methstring.strip(':')\n                            CompRec[\"specimen_description\"]=comment\n                            if len(inst_codes) != 0:\n                                inststring=\"\"\n                                for inst in inst_codes:\n                                    inststring=inststring+ \":\" +inst\n                                CompRec[\"magic_instrument_codes\"]=inststring.strip(':')\n                            PmagSpecs.append(CompRec)\n            k+=1\n    pmag.magic_write(pmag_file,PmagSpecs,'pmag_specimens')\n    print(\"Recalculated specimen data stored in \",pmag_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnaming dipole_plat. py - h prints help message and quits - i prints interactive entry of latitude", "response": "def main():\n    \"\"\"\n    NAME \n        dipole_plat.py\n\n    DESCRIPTION\t\n        gives paleolatitude from given inclination, assuming GAD field\n\n    SYNTAX\n        dipole_plat.py [command line options]<filename\n\n    OPTIONS\n        -h prints help message and quits\n        -i allows interactive entry of latitude\n        -f file, specifies file name on command line\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    elif '-f' in sys.argv:\n       ind=sys.argv.index('-f')\n       file=sys.argv[ind+1]\n       f=open(file,'r')\n       data=f.readlines()\n    elif '-i' not in sys.argv:\n       data=sys.stdin.readlines()\n    if '-i' not in sys.argv:\n        for line in data:\n            rec=line.split()\n            print('%7.1f'%(pmag.plat(float(rec[0]))))\n    else: \n       while 1:\n           try:\n               inc=input(\"Inclination for converting to paleolatitude: <cntl-D> to quit \")\n               print('%7.1f'%(pmag.plat(float(inc))))\n           except:\n               print('\\n Good-bye \\n')\n               sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnames zeq_magic.py DESCRIPTION reads in a MagIC measurements formatted file, makes plots of remanence decay during demagnetization experiments. Reads in prior interpretations saved in a specimens formatted file interpretations in a specimens file. interpretations are saved in the coordinate system used. SYNTAX zeq_magic.py [command line options] OPTIONS -h prints help message and quits -f MEASFILE: sets measurements format input file, default: measurements.txt -fsp SPECFILE: sets specimens format file with prior interpreations, default: specimens.txt -fsa SAMPFILE: sets samples format file sample=>site information, default: samples.txt -fsi SITEFILE: sets sites format file with site=>location informationprior interpreations, default: samples.txt -Fp PLTFILE: sets filename for saved plot, default is name_type.fmt (where type is zijd, eqarea or decay curve) -crd [s,g,t]: sets coordinate system, g=geographic, t=tilt adjusted, default: specimen coordinate system -spc SPEC plots single specimen SPEC, saves plot with specified format with optional -dir settings and quits -dir [L,P,F][beg][end]: sets calculation type for principal component analysis, default is none beg: starting step for PCA calculation end: ending step for PCA calculation [L,P,F]: calculation type for line, plane or fisher mean must be used with -spc option -fmt FMT: set format of saved plot [png,svg,jpg] -A: suppresses averaging of replicate measurements, default is to average -sav: saves all plots without review", "response": "def main():\n    \"\"\"\n    NAME\n        zeq_magic.py\n    DESCRIPTION\n        reads in a MagIC measurements formatted file, makes plots of remanence decay\n        during demagnetization experiments.  Reads in prior interpretations saved in\n        a specimens formatted file interpretations in a specimens file.\n        interpretations are saved in the coordinate system used.\n    SYNTAX\n        zeq_magic.py [command line options]\n    OPTIONS\n        -h prints help message and quits\n        -f  MEASFILE: sets measurements format input file, default: measurements.txt\n        -fsp SPECFILE: sets specimens format file with prior interpreations, default: specimens.txt\n        -fsa SAMPFILE: sets samples format file sample=>site information, default: samples.txt\n        -fsi SITEFILE: sets sites format file with site=>location informationprior interpreations, default: samples.txt\n        -Fp PLTFILE: sets filename for saved plot, default is name_type.fmt (where type is zijd, eqarea or decay curve)\n        -crd [s,g,t]: sets coordinate system,  g=geographic, t=tilt adjusted, default: specimen coordinate system\n        -spc SPEC  plots single specimen SPEC, saves plot with specified format\n              with optional -dir settings and quits\n        -dir [L,P,F][beg][end]: sets calculation type for principal component analysis, default is none\n             beg: starting step for PCA calculation\n             end: ending step for PCA calculation\n             [L,P,F]: calculation type for line, plane or fisher mean\n             must be used with -spc option\n        -fmt FMT: set format of saved plot [png,svg,jpg]\n        -A:  suppresses averaging of  replicate measurements, default is to average\n        -sav: saves all plots without review\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        return\n    dir_path = pmag.get_named_arg(\"-WD\", default_val=os.getcwd())\n    meas_file = pmag.get_named_arg(\n        \"-f\", default_val=\"measurements.txt\")\n    spec_file = pmag.get_named_arg(\n        \"-fsp\", default_val=\"specimens.txt\")\n    specimen = pmag.get_named_arg(\n        \"-spc\", default_val=\"\")\n    samp_file = pmag.get_named_arg(\"-fsa\", default_val=\"samples.txt\")\n    site_file = pmag.get_named_arg(\"-fsi\", default_val=\"sites.txt\")\n    plot_file = pmag.get_named_arg(\"-Fp\", default_val=\"\")\n    crd = pmag.get_named_arg(\"-crd\", default_val=\"s\")\n    fmt = pmag.get_named_arg(\"-fmt\", \"svg\")\n    specimen = pmag.get_named_arg(\"-spc\", default_val=\"\")\n    interactive = True\n    save_plots = False\n    if \"-sav\" in sys.argv:\n        interactive = False\n        save_plots = True\n    ipmag.zeq_magic(meas_file, spec_file, crd, dir_path, n_plots=\"all\",\n                    save_plots=save_plots, fmt=fmt, interactive=interactive, specimen=specimen)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(command_line=True, **kwargs):\n    #\n    # initialize defaults\n    version_num=pmag.get_version()\n    meas_file='magic_measurements.txt'\n    csv_file=''\n    MagRecs,Specs=[],[]\n    citation=\"This study\"\n    dir_path,demag='.','NRM'\n    args=sys.argv\n    noave=0\n    # get command line args\n    if command_line:\n        if '-WD' in args:\n            ind=args.index(\"-WD\")\n            dir_path=args[ind+1]\n        if '-ID' in args:\n            ind = args.index('-ID')\n            input_dir_path = args[ind+1]\n        else:\n            input_dir_path = dir_path\n        output_dir_path = dir_path\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if \"-A\" in args: noave=1\n        if '-f' in args:\n            ind=args.index(\"-f\")\n            csv_file=args[ind+1]\n        if '-F' in args:\n            ind=args.index(\"-F\")\n            meas_file=args[ind+1]\n\n    if not command_line:\n        dir_path = kwargs.get('dir_path', '.')\n        input_dir_path = kwargs.get('input_dir_path', dir_path)\n        output_dir_path = dir_path # rename dir_path after input_dir_path is set\n        noave = kwargs.get('noave', 0) # default (0) is DO average\n        csv_file = kwargs.get('csv_file', '')\n        meas_file = kwargs.get('meas_file', 'magic_measurements.txt')\n\n    # format variables\n\n    meas_file= os.path.join(output_dir_path, meas_file)\n    if csv_file==\"\":\n        filelist=os.listdir(input_dir_path) # read in list of files to import\n    else:\n        csv_file = os.path.join(input_dir_path, csv_file)\n        filelist=[csv_file]\n    # parsing the data\n    file_found = False\n    for fname in filelist: # parse each file\n        if fname[-3:].lower()=='csv':\n            file_found = True\n            print('processing: ',fname)\n            with open(fname, 'r') as finput:\n                data = list(finput.readlines())\n            keys = data[0].replace('\\n','').split(',') # splits on underscores\n            interval_key=\"Offset (cm)\"\n            demag_key=\"Demag level (mT)\"\n            offline_demag_key=\"Treatment Value (mT or &deg;C)\"\n            offline_treatment_type=\"Treatment type\"\n            run_key=\"Test No.\"\n            if \"Inclination background + tray corrected  (deg)\" in keys: inc_key=\"Inclination background + tray corrected  (deg)\"\n            if \"Inclination background &amp; tray corrected (deg)\" in keys: inc_key=\"Inclination background &amp; tray corrected (deg)\"\n            if \"Declination background + tray corrected (deg)\" in keys: dec_key=\"Declination background + tray corrected (deg)\"\n            if \"Declination background &amp; tray corrected (deg)\" in keys: dec_key=\"Declination background &amp; tray corrected (deg)\"\n            if \"Intensity background + tray corrected  (A/m)\" in keys: int_key=\"Intensity background + tray corrected  (A/m)\"\n            if \"Intensity background &amp; tray corrected (A/m)\" in keys: int_key=\"Intensity background &amp; tray corrected (A/m)\"\n            type=\"Type\"\n            sect_key=\"Sect\"\n            half_key=\"A/W\"\n# need to add volume_key to LORE format!\n            if \"Sample volume (cm^3)\" in keys:volume_key=\"Sample volume (cm^3)\"\n            if \"Sample volume (cc)\" in keys:volume_key=\"Sample volume (cc)\"\n            if \"Sample volume (cm&sup3;)\" in keys:volume_key=\"Sample volume (cm&sup3;)\"\n            for line in data[1:]:\n                InRec={}\n                for k in range(len(keys)):InRec[keys[k]]=line.split(',')[k]\n                inst=\"IODP-SRM\"\n                MagRec={}\n                expedition=InRec['Exp']\n                location=InRec['Site']+InRec['Hole']\n                offsets=InRec[interval_key].split('.') # maintain consistency with er_samples convention of using top interval\n                if len(offsets)==1:\n                    offset=int(offsets[0])\n                else:\n                    offset=int(offsets[0])-1\n                #interval=str(offset+1)# maintain consistency with er_samples convention of using top interval\n                interval=str(offset)# maintain consistency with er_samples convention of using top interval\n                specimen=expedition+'-'+location+'-'+InRec['Core']+InRec[type]+\"-\"+InRec[sect_key]+'_'+InRec[half_key]+'_'+interval\n                if specimen not in Specs:Specs.append(specimen)\n                MagRec['er_expedition_name']=expedition\n                MagRec['er_location_name']=location\n                MagRec['er_site_name']=specimen\n                MagRec['er_citation_names']=citation\n                MagRec['er_specimen_name']=specimen\n                MagRec['er_sample_name']=specimen\n                MagRec['er_site_name']=specimen\n# set up measurement record - default is NRM\n                MagRec['magic_software_packages']=version_num\n                MagRec[\"treatment_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"measurement_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"treatment_ac_field\"]='0'\n                MagRec[\"treatment_dc_field\"]='0'\n                MagRec[\"treatment_dc_field_phi\"]='0'\n                MagRec[\"treatment_dc_field_theta\"]='0'\n                MagRec[\"measurement_flag\"]='g' # assume all data are \"good\"\n                MagRec[\"measurement_standard\"]='u' # assume all data are \"good\"\n                MagRec[\"measurement_csd\"]='0' # assume all data are \"good\"\n                volume=InRec[volume_key]\n                MagRec[\"magic_method_codes\"]='LT-NO'\n                sort_by='treatment_ac_field' # set default to AF demag\n                if InRec[demag_key]!=\"0\":\n                    MagRec['magic_method_codes'] = 'LT-AF-Z'\n                    inst=inst+':IODP-SRM-AF' # measured on shipboard in-line 2G AF\n                    treatment_value=float(InRec[demag_key].strip('\"'))*1e-3 # convert mT => T\n                    if sort_by ==\"treatment_ac_field\":\n                        MagRec[\"treatment_ac_field\"]=treatment_value # AF demag in treat mT => T\n                    else:\n                        MagRec[\"treatment_ac_field\"]=str(treatment_value)# AF demag in treat mT => T\n                elif offline_treatment_type in list(InRec.keys()) and InRec[offline_treatment_type]!=\"\":\n                    if \"Lowrie\" in InRec['Comments']:\n                        MagRec['magic_method_codes'] = 'LP-IRM-3D'\n                        treatment_value=float(InRec[offline_demag_key].strip('\"'))+273. # convert C => K\n                        MagRec[\"treatment_temp\"]=treatment_value\n                        MagRec[\"treatment_ac_field\"]=\"0\"\n                        sort_by='treatment_temp'\n                    elif 'Isothermal' in InRec[offline_treatment_type]:\n                        MagRec['magic_method_codes'] = 'LT-IRM'\n                        treatment_value=float(InRec[offline_demag_key].strip('\"'))*1e-3 # convert mT => T\n                        MagRec[\"treatment_dc_field\"]=treatment_value\n                        MagRec[\"treatment_ac_field\"]=\"0\"\n                        sort_by='treatment_dc_field'\n                MagRec[\"measurement_standard\"]='u' # assume all data are \"good\"\n                vol=float(volume)*1e-6 # convert from cc to m^3\n                if run_key in list(InRec.keys()):\n                    run_number=InRec[run_key]\n                    MagRec['external_database_ids']=run_number\n                    MagRec['external_database_names']='LIMS'\n                else:\n                    MagRec['external_database_ids']=\"\"\n                    MagRec['external_database_names']=''\n                MagRec['measurement_description']='sample orientation: '+InRec['Sample orientation']\n                MagRec['measurement_inc']=InRec[inc_key].strip('\"')\n                MagRec['measurement_dec']=InRec[dec_key].strip('\"')\n                intens= InRec[int_key].strip('\"')\n                MagRec['measurement_magn_moment']='%8.3e'%(float(intens)*vol) # convert intensity from A/m to Am^2 using vol\n                MagRec['magic_instrument_codes']=inst\n                MagRec['measurement_number']='1'\n                MagRec['measurement_positions']=''\n                MagRecs.append(MagRec)\n    if not file_found:\n        print(\"No .csv files were found\")\n        return False, \"No .csv files were found\"\n    MagOuts=[]\n    for spec in Specs:\n        Speclist=pmag.get_dictitem(MagRecs,'er_specimen_name',spec,'T')\n        Meassorted=sorted(Speclist, key=lambda x,y=None: int(round(float(x[sort_by])-float(y[sort_by]))) if y!=None else 0)\n        for rec in Meassorted:\n            for key in list(rec.keys()): rec[key]=str(rec[key])\n            MagOuts.append(rec)\n    Fixed=pmag.measurements_methods(MagOuts,noave)\n    Out,keys=pmag.fillkeys(Fixed)\n    if pmag.magic_write(meas_file,Out,'magic_measurements'):\n        print('data stored in ',meas_file)\n        return True, meas_file\n    else:\n        print('no data found.  bad magfile?')\n        return False, 'no data found.  bad magfile?'", "response": "This function is the main function for the iodp_dscr_magic. py script."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames sio_magic.py DESCRIPTION converts SIO .mag format files to magic_measurements format files SYNTAX sio_magic.py [command line options] OPTIONS -h: prints the help message and quits. -usr USER: identify user, default is \"\" -f FILE: specify .mag format input file, required -fsa SAMPFILE : specify er_samples.txt file relating samples, site and locations names,default is none -- values in SAMPFILE will override selections for -loc (location), -spc (designate specimen), and -ncn (sample-site naming convention) -F FILE: specify output file, default is magic_measurements.txt -Fsy: specify er_synthetics file, default is er_sythetics.txt -LP [colon delimited list of protocols, include all that apply] AF: af demag T: thermal including thellier but not trm acquisition S: Shaw method I: IRM (acquisition) I3d: 3D IRM experiment N: NRM only TRM: trm acquisition ANI: anisotropy experiment D: double AF demag G: triple AF demag (GRM protocol) CR: cooling rate experiment. The treatment coding of the measurement file should be: XXX.00,XXX.10, XXX.20 ...XX.70 etc. (XXX.00 is optional) where XXX in the temperature and .10,.20... are running numbers of the cooling rates steps. XXX.00 is optional zerofield baseline. XXX.70 is alteration check. syntax in sio_magic is: -LP CR xxx,yyy,zzz,..... xxx -A where xxx, yyy, zzz...xxx are cooling time in [K/minutes], seperated by comma, ordered at the same order as XXX.10,XXX.20 ...XX.70 if you use a zerofield step then no need to specify the cooling rate for the zerofield It is important to add to the command line the -A option so the measurements will not be averaged. But users need to make sure that there are no duplicate measurements in the file -V [1,2,3] units of IRM field in volts using ASC coil #1,2 or 3 -spc NUM : specify number of characters to designate a specimen, default = 0 -loc LOCNAME : specify location/study name, must have either LOCNAME or SAMPFILE or be a synthetic -syn INST TYPE: sets these specimens as synthetics created at institution INST and of type TYPE -ins INST : specify which demag instrument was used (e.g, SIO-Suzy or SIO-Odette),default is \"\" -dc B PHI THETA: dc lab field (in micro tesla) and phi,theta, default is none NB: use PHI, THETA = -1 -1 to signal that it changes, i.e. in anisotropy experiment -ac B : peak AF field (in mT) for ARM acquisition, default is none -ncn NCON: specify naming convention: default is #1 below -A: don't average replicate measurements Sample naming convention: [1] XXXXY: where XXXX is an arbitrary length site designation and Y is the single character sample designation. e.g., TG001a is the first sample from site TG001. [default] [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length) [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length) [4-Z] XXXX[YYY]: YYY is sample designation with Z characters from site XXX [5] site name same as sample [6] site is entered under a separate column NOT CURRENTLY SUPPORTED [7-Z] [XXXX]YYY: XXXX is site designation with Z characters with sample name XXXXYYYY NB: all others you will have to customize your self or e-mail ltauxe@ucsd.edu for help. [8] synthetic - has no site name [9] ODP naming convention INPUT Best to put separate experiments (all AF, thermal, thellier, trm aquisition, Shaw, etc.) in seperate .mag files (eg. af.mag, thermal.mag, etc.) Format of SIO .mag files: Spec Treat CSD Intensity Declination Inclination [optional metadata string] Spec: specimen name Treat: treatment step XXX T in Centigrade XXX AF in mT for special experiments: Thellier: XXX.0 first zero field step XXX.1 first in field step [XXX.0 and XXX.1 can be done in any order] XXX.2 second in-field step at lower temperature (pTRM check) XXX.3 second zero-field step after infield (pTRM check step) XXX.3 MUST be done in this order [XXX.0, XXX.1 [optional XXX.2] XXX.3] AARM: X.00 baseline step (AF in zero bias field - high peak field) X.1 ARM step (in field step) where X is the step number in the 15 position scheme (see Appendix to Lecture 13 - http://magician.ucsd.edu/Essentials_2) ATRM: X.00 optional baseline X.1 ATRM step (+X) X.2 ATRM step (+Y) X.3 ATRM step (+Z) X.4 ATRM step (-X) X.5 ATRM step (-Y) X.6 ATRM step (-Z) X.7 optional alteration check (+X) TRM: XXX.YYY XXX is temperature step of total TRM YYY is dc field in microtesla Intensity assumed to be total moment in 10^3 Am^2 (emu) Declination: Declination in specimen coordinate system Inclination: Declination in specimen coordinate system Optional metatdata string: mm/dd/yy;hh:mm;[dC,mT];xx.xx;UNITS;USER;INST;NMEAS hh in 24 hours. dC or mT units of treatment XXX (see Treat above) for thermal or AF respectively xx.xxx DC field UNITS of DC field (microT, mT) INST: instrument code, number of axes, number of positions (e.g., G34 is 2G, three axes, measured in four positions) NMEAS: number of measurements in a single position (1,3,200...)", "response": "def main(command_line=True, **kwargs):\n    \"\"\"\n    NAME\n        sio_magic.py\n\n    DESCRIPTION\n        converts SIO .mag format files to magic_measurements format files\n\n    SYNTAX\n        sio_magic.py [command line options]\n\n    OPTIONS\n        -h: prints the help message and quits.\n        -usr USER:   identify user, default is \"\"\n        -f FILE: specify .mag format input file, required\n        -fsa SAMPFILE : specify er_samples.txt file relating samples, site and locations names,default is none -- values in SAMPFILE will override selections for -loc (location), -spc (designate specimen), and -ncn (sample-site naming convention)\n        -F FILE: specify output file, default is magic_measurements.txt\n        -Fsy: specify er_synthetics file, default is er_sythetics.txt\n        -LP [colon delimited list of protocols, include all that apply]\n            AF:  af demag\n            T: thermal including thellier but not trm acquisition\n            S: Shaw method\n            I: IRM (acquisition)\n            I3d: 3D IRM experiment\n            N: NRM only\n            TRM: trm acquisition\n            ANI: anisotropy experiment\n            D: double AF demag\n            G: triple AF demag (GRM protocol)\n            CR: cooling rate experiment.\n                The treatment coding of the measurement file should be: XXX.00,XXX.10, XXX.20 ...XX.70 etc. (XXX.00 is optional)\n                where XXX in the temperature and .10,.20... are running numbers of the cooling rates steps.\n                XXX.00 is optional zerofield baseline. XXX.70 is alteration check.\n                syntax in sio_magic is: -LP CR xxx,yyy,zzz,..... xxx -A\n                where xxx, yyy, zzz...xxx  are cooling time in [K/minutes], seperated by comma, ordered at the same order as XXX.10,XXX.20 ...XX.70\n                if you use a zerofield step then no need to specify the cooling rate for the zerofield\n                It is important to add to the command line the -A option so the measurements will not be averaged.\n                But users need to make sure that there are no duplicate measurements in the file\n        -V [1,2,3] units of IRM field in volts using ASC coil #1,2 or 3\n        -spc NUM : specify number of characters to designate a  specimen, default = 0\n        -loc LOCNAME : specify location/study name, must have either LOCNAME or SAMPFILE or be a synthetic\n        -syn INST TYPE:  sets these specimens as synthetics created at institution INST and of type TYPE\n        -ins INST : specify which demag instrument was used (e.g, SIO-Suzy or SIO-Odette),default is \"\"\n        -dc B PHI THETA: dc lab field (in micro tesla) and phi,theta, default is none\n              NB: use PHI, THETA = -1 -1 to signal that it changes, i.e. in anisotropy experiment\n        -ac B : peak AF field (in mT) for ARM acquisition, default is none\n        -ncn NCON:  specify naming convention: default is #1 below\n        -A: don't average replicate measurements\n       Sample naming convention:\n            [1] XXXXY: where XXXX is an arbitrary length site designation and Y\n                is the single character sample designation.  e.g., TG001a is the\n                first sample from site TG001.    [default]\n            [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [4-Z] XXXX[YYY]:  YYY is sample designation with Z characters from site XXX\n            [5] site name same as sample\n            [6] site is entered under a separate column NOT CURRENTLY SUPPORTED\n            [7-Z] [XXXX]YYY:  XXXX is site designation with Z characters with sample name XXXXYYYY\n            NB: all others you will have to customize your self\n                 or e-mail ltauxe@ucsd.edu for help.\n\n            [8] synthetic - has no site name\n            [9] ODP naming convention\n    INPUT\n        Best to put separate experiments (all AF, thermal, thellier, trm aquisition, Shaw, etc.) in\n           seperate .mag files (eg. af.mag, thermal.mag, etc.)\n\n        Format of SIO .mag files:\n        Spec Treat CSD Intensity Declination Inclination [optional metadata string]\n\n\n        Spec: specimen name\n        Treat:  treatment step\n            XXX T in Centigrade\n            XXX AF in mT\n            for special experiments:\n              Thellier:\n                XXX.0  first zero field step\n                XXX.1  first in field step [XXX.0 and XXX.1 can be done in any order]\n                XXX.2  second in-field step at lower temperature (pTRM check)\n                XXX.3  second zero-field step after infield (pTRM check step)\n                       XXX.3 MUST be done in this order [XXX.0, XXX.1 [optional XXX.2] XXX.3]\n              AARM:\n                X.00  baseline step (AF in zero bias field - high peak field)\n                X.1   ARM step (in field step)  where\n                   X is the step number in the 15 position scheme\n                      (see Appendix to Lecture 13 - http://magician.ucsd.edu/Essentials_2)\n              ATRM:\n                X.00 optional baseline\n                X.1 ATRM step (+X)\n                X.2 ATRM step (+Y)\n                X.3 ATRM step (+Z)\n                X.4 ATRM step (-X)\n                X.5 ATRM step (-Y)\n                X.6 ATRM step (-Z)\n                X.7 optional alteration check (+X)\n\n              TRM:\n                XXX.YYY  XXX is temperature step of total TRM\n                         YYY is dc field in microtesla\n\n\n         Intensity assumed to be total moment in 10^3 Am^2 (emu)\n         Declination:  Declination in specimen coordinate system\n         Inclination:  Declination in specimen coordinate system\n\n         Optional metatdata string:  mm/dd/yy;hh:mm;[dC,mT];xx.xx;UNITS;USER;INST;NMEAS\n             hh in 24 hours.\n             dC or mT units of treatment XXX (see Treat above) for thermal or AF respectively\n             xx.xxx   DC field\n             UNITS of DC field (microT, mT)\n             INST:  instrument code, number of axes, number of positions (e.g., G34 is 2G, three axes,\n                    measured in four positions)\n             NMEAS: number of measurements in a single position (1,3,200...)\n\n\n    \"\"\"\n    # initialize some stuff\n    mag_file = None\n    codelist = None\n    infile_type=\"mag\"\n    noave=0\n    methcode,inst=\"LP-NO\",\"\"\n    phi,theta,peakfield,labfield=0,0,0,0\n    pTRM,MD,samp_con,Z=0,0,'1',1\n    dec=[315,225,180,135,45,90,270,270,270,90,180,180,0,0,0]\n    inc=[0,0,0,0,0,-45,-45,0,45,45,45,-45,-90,-45,45]\n    tdec=[0,90,0,180,270,0,0,90,0]\n    tinc=[0,0,90,0,0,-90,0,0,90]\n    missing=1\n    demag=\"N\"\n    er_location_name=\"\"\n    citation='This study'\n    args=sys.argv\n    fmt='old'\n    syn=0\n    synfile='er_synthetics.txt'\n    samp_infile,Samps='',[]\n    trm=0\n    irm=0\n    specnum=0\n    coil=\"\"\n    mag_file=\"\"\n#\n# get command line arguments\n#\n    meas_file=\"magic_measurements.txt\"\n    user=\"\"\n    if not command_line:\n        user = kwargs.get('user', '')\n        meas_file = kwargs.get('meas_file', '')\n        syn_file = kwargs.get('syn_file', '')\n        mag_file = kwargs.get('mag_file', '')\n        labfield = kwargs.get('labfield', '')\n        if labfield:\n            labfield = float(labfield) *1e-6\n        else:\n            labfield = 0\n        phi = kwargs.get('phi', 0)\n        if phi:\n            phi = float(phi)\n        else:\n            phi = 0\n        theta = kwargs.get('theta', 0)\n        if theta:\n            theta=float(theta)\n        else:\n            theta = 0\n        peakfield = kwargs.get('peakfield', 0)\n        if peakfield:\n            peakfield=float(peakfield) *1e-3\n        else:\n            peakfield = 0\n        specnum = kwargs.get('specnum', 0)\n        samp_con = kwargs.get('samp_con', '1')\n        er_location_name = kwargs.get('er_location_name', '')\n        samp_infile = kwargs.get('samp_infile', '')\n        syn = kwargs.get('syn', 0)\n        institution = kwargs.get('institution', '')\n        syntype = kwargs.get('syntype', '')\n        inst = kwargs.get('inst', '')\n        noave = kwargs.get('noave', 0)\n        codelist = kwargs.get('codelist', '')\n        coil = kwargs.get('coil', '')\n        cooling_rates = kwargs.get('cooling_rates', '')\n    if command_line:\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if \"-usr\" in args:\n            ind=args.index(\"-usr\")\n            user=args[ind+1]\n        if '-F' in args:\n            ind=args.index(\"-F\")\n            meas_file=args[ind+1]\n        if '-Fsy' in args:\n            ind=args.index(\"-Fsy\")\n            synfile=args[ind+1]\n        if '-f' in args:\n            ind=args.index(\"-f\")\n            mag_file=args[ind+1]\n        if \"-dc\" in args:\n            ind=args.index(\"-dc\")\n            labfield=float(args[ind+1])*1e-6\n            phi=float(args[ind+2])\n            theta=float(args[ind+3])\n        if \"-ac\" in args:\n            ind=args.index(\"-ac\")\n            peakfield=float(args[ind+1])*1e-3\n        if \"-spc\" in args:\n            ind=args.index(\"-spc\")\n            specnum=int(args[ind+1])\n        if \"-loc\" in args:\n            ind=args.index(\"-loc\")\n            er_location_name=args[ind+1]\n        if \"-fsa\" in args:\n            ind=args.index(\"-fsa\")\n            samp_infile = args[ind+1]\n        if '-syn' in args:\n            syn=1\n            ind=args.index(\"-syn\")\n            institution=args[ind+1]\n            syntype=args[ind+2]\n            if '-fsy' in args:\n                ind=args.index(\"-fsy\")\n                synfile=args[ind+1]\n        if \"-ins\" in args:\n            ind=args.index(\"-ins\")\n            inst=args[ind+1]\n        if \"-A\" in args: noave=1\n        if \"-ncn\" in args:\n            ind=args.index(\"-ncn\")\n            samp_con=sys.argv[ind+1]\n        if '-LP' in args:\n            ind=args.index(\"-LP\")\n            codelist=args[ind+1]\n\n        if \"-V\" in args:\n            ind=args.index(\"-V\")\n            coil=args[ind+1]\n\n\n    # make sure all initial values are correctly set up (whether they come from the command line or a GUI)\n    if samp_infile:\n        Samps, file_type = pmag.magic_read(samp_infile)\n    if coil:\n        coil = str(coil)\n        methcode=\"LP-IRM\"\n        irmunits = \"V\"\n        if coil not in [\"1\",\"2\",\"3\"]:\n            print(main.__doc__)\n            print('not a valid coil specification')\n            return False, '{} is not a valid coil specification'.format(coil)\n    if mag_file:\n        try:\n            #with open(mag_file,'r') as finput:\n            #    lines = finput.readlines()\n            lines=pmag.open_file(mag_file)\n        except:\n            print(\"bad mag file name\")\n            return False, \"bad mag file name\"\n    if not mag_file:\n        print(main.__doc__)\n        print(\"mag_file field is required option\")\n        return False, \"mag_file field is required option\"\n    if specnum!=0:\n        specnum=-specnum\n    #print 'samp_con:', samp_con\n    if samp_con:\n        if \"4\" == samp_con[0]:\n            if \"-\" not in samp_con:\n                print(\"naming convention option [4] must be in form 4-Z where Z is an integer\")\n                print('---------------')\n                return False, \"naming convention option [4] must be in form 4-Z where Z is an integer\"\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"4\"\n        if \"7\" == samp_con[0]:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                return False, \"option [7] must be in form 7-Z where Z is an integer\"\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"7\"\n\n    if codelist:\n        codes=codelist.split(':')\n        if \"AF\" in codes:\n            demag='AF'\n            if'-dc' not in args: methcode=\"LT-AF-Z\"\n            if'-dc' in args: methcode=\"LT-AF-I\"\n        if \"T\" in codes:\n            demag=\"T\"\n            if '-dc' not in args: methcode=\"LT-T-Z\"\n            if '-dc' in args: methcode=\"LT-T-I\"\n        if \"I\" in codes:\n            methcode=\"LP-IRM\"\n            irmunits=\"mT\"\n        if \"I3d\" in codes:\n            methcode=\"LT-T-Z:LP-IRM-3D\"\n        if \"S\" in codes:\n            demag=\"S\"\n            methcode=\"LP-PI-TRM:LP-PI-ALT-AFARM\"\n            trm_labfield=labfield\n            ans=input(\"DC lab field for ARM step: [50uT] \")\n            if ans==\"\":\n                arm_labfield=50e-6\n            else:\n                arm_labfield=float(ans)*1e-6\n            ans=input(\"temperature for total trm step: [600 C] \")\n            if ans==\"\":\n                trm_peakT=600+273 # convert to kelvin\n            else:\n                trm_peakT=float(ans)+273 # convert to kelvin\n        if \"G\" in codes: methcode=\"LT-AF-G\"\n        if \"D\" in codes: methcode=\"LT-AF-D\"\n        if \"TRM\" in codes:\n            demag=\"T\"\n            trm=1\n        if \"CR\" in     codes:\n            demag=\"T\"\n            cooling_rate_experiment=1\n            if command_line:\n                ind=args.index(\"CR\")\n                cooling_rates=args[ind+1]\n                cooling_rates_list=cooling_rates.split(',')\n            else:\n                cooling_rates_list=str(cooling_rates).split(',')\n    if demag==\"T\" and \"ANI\" in codes:\n        methcode=\"LP-AN-TRM\"\n    if demag==\"T\" and \"CR\" in codes:\n        methcode=\"LP-CR-TRM\"\n    if demag==\"AF\" and \"ANI\" in codes:\n        methcode=\"LP-AN-ARM\"\n        if labfield==0: labfield=50e-6\n        if peakfield==0: peakfield=.180\n    SynRecs,MagRecs=[],[]\n    version_num=pmag.get_version()\n\n\n    ##################################\n\n    if 1:\n    #if infile_type==\"SIO format\":\n        for line in lines:\n            instcode=\"\"\n            if len(line)>2:\n                SynRec={}\n                MagRec={}\n                MagRec['er_location_name']=er_location_name\n                MagRec['magic_software_packages']=version_num\n                MagRec[\"treatment_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"measurement_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"treatment_ac_field\"]='0'\n                MagRec[\"treatment_dc_field\"]='0'\n                MagRec[\"treatment_dc_field_phi\"]='0'\n                MagRec[\"treatment_dc_field_theta\"]='0'\n                meas_type=\"LT-NO\"\n                rec=line.split()\n                if rec[1]==\".00\":rec[1]=\"0.00\"\n                treat=rec[1].split('.')\n                if methcode==\"LP-IRM\":\n                    if irmunits=='mT':\n                        labfield=float(treat[0])*1e-3\n                    else:\n                        labfield=pmag.getfield(irmunits,coil,treat[0])\n                    if rec[1][0]!=\"-\":\n                        phi,theta=0.,90.\n                    else:\n                        phi,theta=0.,-90.\n                    meas_type=\"LT-IRM\"\n                    MagRec[\"treatment_dc_field\"]='%8.3e'%(labfield)\n                    MagRec[\"treatment_dc_field_phi\"]='%7.1f'%(phi)\n                    MagRec[\"treatment_dc_field_theta\"]='%7.1f'%(theta)\n                if len(rec)>6:\n                  code1=rec[6].split(';') # break e.g., 10/15/02;7:45 indo date and time\n                  if len(code1)==2: # old format with AM/PM\n                    missing=0\n                    code2=code1[0].split('/') # break date into mon/day/year\n                    code3=rec[7].split(';') # break e.g., AM;C34;200  into time;instr/axes/measuring pos;number of measurements\n                    yy=int(code2[2])\n                    if yy <90:\n                        yyyy=str(2000+yy)\n                    else: yyyy=str(1900+yy)\n                    mm=int(code2[0])\n                    if mm<10:\n                        mm=\"0\"+str(mm)\n                    else: mm=str(mm)\n                    dd=int(code2[1])\n                    if dd<10:\n                        dd=\"0\"+str(dd)\n                    else: dd=str(dd)\n                    time=code1[1].split(':')\n                    hh=int(time[0])\n                    if code3[0]==\"PM\":hh=hh+12\n                    if hh<10:\n                        hh=\"0\"+str(hh)\n                    else: hh=str(hh)\n                    min=int(time[1])\n                    if min<10:\n                       min= \"0\"+str(min)\n                    else: min=str(min)\n                    MagRec[\"measurement_date\"]=yyyy+\":\"+mm+\":\"+dd+\":\"+hh+\":\"+min+\":00.00\"\n                    MagRec[\"measurement_time_zone\"]='SAN'\n                    if inst==\"\":\n                        if code3[1][0]=='C':instcode='SIO-bubba'\n                        if code3[1][0]=='G':instcode='SIO-flo'\n                    else:\n                        instcode=''\n                    MagRec[\"measurement_positions\"]=code3[1][2]\n                  elif len(code1)>2: # newest format (cryo7 or later)\n                    if \"LP-AN-ARM\" not in methcode:labfield=0\n                    fmt='new'\n                    date=code1[0].split('/') # break date into mon/day/year\n                    yy=int(date[2])\n                    if yy <90:\n                        yyyy=str(2000+yy)\n                    else: yyyy=str(1900+yy)\n                    mm=int(date[0])\n                    if mm<10:\n                        mm=\"0\"+str(mm)\n                    else: mm=str(mm)\n                    dd=int(date[1])\n                    if dd<10:\n                        dd=\"0\"+str(dd)\n                    else: dd=str(dd)\n                    time=code1[1].split(':')\n                    hh=int(time[0])\n                    if hh<10:\n                        hh=\"0\"+str(hh)\n                    else: hh=str(hh)\n                    min=int(time[1])\n                    if min<10:\n                       min= \"0\"+str(min)\n                    else:\n                        min=str(min)\n                    MagRec[\"measurement_date\"]=yyyy+\":\"+mm+\":\"+dd+\":\"+hh+\":\"+min+\":00.00\"\n                    MagRec[\"measurement_time_zone\"]='SAN'\n                    if inst==\"\":\n                        if code1[6][0]=='C':\n                            instcode='SIO-bubba'\n                        if code1[6][0]=='G':\n                            instcode='SIO-flo'\n                    else:\n                        instcode=''\n                    if len(code1)>1:\n                        MagRec[\"measurement_positions\"]=code1[6][2]\n                    else:\n                        MagRec[\"measurement_positions\"]=code1[7]   # takes care of awkward format with bubba and flo being different\n                    if user==\"\":user=code1[5]\n                    if code1[2][-1]=='C':\n                        demag=\"T\"\n                        if code1[4]=='microT' and float(code1[3])!=0. and \"LP-AN-ARM\" not in methcode: labfield=float(code1[3])*1e-6\n                    if code1[2]=='mT' and methcode!=\"LP-IRM\":\n                        demag=\"AF\"\n                        if code1[4]=='microT' and float(code1[3])!=0.: labfield=float(code1[3])*1e-6\n                    if code1[4]=='microT' and labfield!=0. and meas_type!=\"LT-IRM\":\n                        phi,theta=0.,-90.\n                        if demag==\"T\": meas_type=\"LT-T-I\"\n                        if demag==\"AF\": meas_type=\"LT-AF-I\"\n                        MagRec[\"treatment_dc_field\"]='%8.3e'%(labfield)\n                        MagRec[\"treatment_dc_field_phi\"]='%7.1f'%(phi)\n                        MagRec[\"treatment_dc_field_theta\"]='%7.1f'%(theta)\n                    if code1[4]=='' or labfield==0. and meas_type!=\"LT-IRM\":\n                        if demag=='T':meas_type=\"LT-T-Z\"\n                        if demag==\"AF\":meas_type=\"LT-AF-Z\"\n                        MagRec[\"treatment_dc_field\"]='0'\n                if syn==0:\n                    MagRec[\"er_specimen_name\"]=rec[0]\n                    MagRec[\"er_synthetic_name\"]=\"\"\n                    MagRec[\"er_site_name\"]=\"\"\n                    if specnum!=0:\n                        MagRec[\"er_sample_name\"]=rec[0][:specnum]\n                    else:\n                        MagRec[\"er_sample_name\"]=rec[0]\n                    if samp_infile and Samps: # if samp_infile was provided AND yielded sample data\n                        samp=pmag.get_dictitem(Samps,'er_sample_name',MagRec['er_sample_name'],'T')\n                        if len(samp)>0:\n                            MagRec[\"er_location_name\"]=samp[0][\"er_location_name\"]\n                            MagRec[\"er_site_name\"]=samp[0][\"er_site_name\"]\n                        else:\n                            MagRec['er_location_name']=''\n                            MagRec[\"er_site_name\"]=''\n                    elif int(samp_con)!=6:\n                        site=pmag.parse_site(MagRec['er_sample_name'],samp_con,Z)\n                        MagRec[\"er_site_name\"]=site\n                    if MagRec['er_site_name']==\"\":\n                        print('No site name found for: ',MagRec['er_specimen_name'],MagRec['er_sample_name'])\n                    if MagRec[\"er_location_name\"]==\"\":\n                        print('no location name for: ',MagRec[\"er_specimen_name\"])\n                else:\n                    MagRec[\"er_specimen_name\"]=rec[0]\n                    if specnum!=0:\n                        MagRec[\"er_sample_name\"]=rec[0][:specnum]\n                    else:\n                        MagRec[\"er_sample_name\"]=rec[0]\n                    MagRec[\"er_site_name\"]=\"\"\n                    MagRec[\"er_synthetic_name\"]=MagRec[\"er_specimen_name\"]\n                    SynRec[\"er_synthetic_name\"]=MagRec[\"er_specimen_name\"]\n                    site=pmag.parse_site(MagRec['er_sample_name'],samp_con,Z)\n                    SynRec[\"synthetic_parent_sample\"]=site\n                    SynRec[\"er_citation_names\"]=\"This study\"\n                    SynRec[\"synthetic_institution\"]=institution\n                    SynRec[\"synthetic_type\"]=syntype\n                    SynRecs.append(SynRec)\n                if float(rec[1])==0:\n                    pass\n                elif demag==\"AF\":\n                    if methcode != \"LP-AN-ARM\":\n                        MagRec[\"treatment_ac_field\"]='%8.3e' %(float(rec[1])*1e-3) # peak field in tesla\n                        if meas_type==\"LT-AF-Z\": MagRec[\"treatment_dc_field\"]='0'\n                    else: # AARM experiment\n                        if treat[1][0]=='0':\n                            meas_type=\"LT-AF-Z:LP-AN-ARM:\"\n                            MagRec[\"treatment_ac_field\"]='%8.3e' %(peakfield) # peak field in tesla\n                            MagRec[\"treatment_dc_field\"]='%8.3e'%(0)\n                            if labfield!=0 and methcode!=\"LP-AN-ARM\": print(\"Warning - inconsistency in mag file with lab field - overriding file with 0\")\n                        else:\n                            meas_type=\"LT-AF-I:LP-AN-ARM\"\n                            ipos=int(treat[0])-1\n                            MagRec[\"treatment_dc_field_phi\"]='%7.1f' %(dec[ipos])\n                            MagRec[\"treatment_dc_field_theta\"]='%7.1f'% (inc[ipos])\n                            MagRec[\"treatment_dc_field\"]='%8.3e'%(labfield)\n                            MagRec[\"treatment_ac_field\"]='%8.3e' %(peakfield) # peak field in tesla\n                elif demag==\"T\" and methcode == \"LP-AN-TRM\":\n                    MagRec[\"treatment_temp\"]='%8.3e' % (float(treat[0])+273.) # temp in kelvin\n                    if treat[1][0]=='0':\n                        meas_type=\"LT-T-Z:LP-AN-TRM\"\n                        MagRec[\"treatment_dc_field\"]='%8.3e'%(0)\n                        MagRec[\"treatment_dc_field_phi\"]='0'\n                        MagRec[\"treatment_dc_field_theta\"]='0'\n                    else:\n                        MagRec[\"treatment_dc_field\"]='%8.3e'%(labfield)\n                        if treat[1][0]=='7': # alteration check as final measurement\n                                meas_type=\"LT-PTRM-I:LP-AN-TRM\"\n                        else:\n                                meas_type=\"LT-T-I:LP-AN-TRM\"\n\n                        # find the direction of the lab field in two ways:\n                        # (1) using the treatment coding (XX.1=+x, XX.2=+y, XX.3=+z, XX.4=-x, XX.5=-y, XX.6=-z)\n                        ipos_code=int(treat[1][0])-1\n                        # (2) using the magnetization\n                        DEC=float(rec[4])\n                        INC=float(rec[5])\n                        if INC < 45 and INC > -45:\n                            if DEC>315  or DEC<45: ipos_guess=0\n                            if DEC>45 and DEC<135: ipos_guess=1\n                            if DEC>135 and DEC<225: ipos_guess=3\n                            if DEC>225 and DEC<315: ipos_guess=4\n                        else:\n                            if INC >45: ipos_guess=2\n                            if INC <-45: ipos_guess=5\n                        # prefer the guess over the code\n                        ipos=ipos_guess\n                        MagRec[\"treatment_dc_field_phi\"]='%7.1f' %(tdec[ipos])\n                        MagRec[\"treatment_dc_field_theta\"]='%7.1f'% (tinc[ipos])\n                        # check it\n                        if ipos_guess!=ipos_code and treat[1][0]!='7':\n                            print(\"-E- ERROR: check specimen %s step %s, ATRM measurements, coding does not match the direction of the lab field!\"%(rec[0],\".\".join(list(treat))))\n\n\n                elif demag==\"S\": # Shaw experiment\n                    if treat[1][1]=='0':\n                        if  int(treat[0])!=0:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' % (float(treat[0])*1e-3) # AF field in tesla\n                            MagRec[\"treatment_dc_field\"]='0'\n                            meas_type=\"LT-AF-Z\" # first AF\n                        else:\n                            meas_type=\"LT-NO\"\n                            MagRec[\"treatment_ac_field\"]='0'\n                            MagRec[\"treatment_dc_field\"]='0'\n                    elif treat[1][1]=='1':\n                        if int(treat[0])==0:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' %(peakfield) # peak field in tesla\n                            MagRec[\"treatment_dc_field\"]='%8.3e'%(arm_labfield)\n                            MagRec[\"treatment_dc_field_phi\"]='%7.1f'%(phi)\n                            MagRec[\"treatment_dc_field_theta\"]='%7.1f'%(theta)\n                            meas_type=\"LT-AF-I\"\n                        else:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' % ( float(treat[0])*1e-3) # AF field in tesla\n                            MagRec[\"treatment_dc_field\"]='0'\n                            meas_type=\"LT-AF-Z\"\n                    elif treat[1][1]=='2':\n                        if int(treat[0])==0:\n                            MagRec[\"treatment_ac_field\"]='0'\n                            MagRec[\"treatment_dc_field\"]='%8.3e'%(trm_labfield)\n                            MagRec[\"treatment_dc_field_phi\"]='%7.1f'%(phi)\n                            MagRec[\"treatment_dc_field_theta\"]='%7.1f'%(theta)\n                            MagRec[\"treatment_temp\"]='%8.3e' % (trm_peakT)\n                            meas_type=\"LT-T-I\"\n                        else:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' % ( float(treat[0])*1e-3) # AF field in tesla\n                            MagRec[\"treatment_dc_field\"]='0'\n                            meas_type=\"LT-AF-Z\"\n                    elif treat[1][1]=='3':\n                        if int(treat[0])==0:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' %(peakfield) # peak field in tesla\n                            MagRec[\"treatment_dc_field\"]='%8.3e'%(arm_labfield)\n                            MagRec[\"treatment_dc_field_phi\"]='%7.1f'%(phi)\n                            MagRec[\"treatment_dc_field_theta\"]='%7.1f'%(theta)\n                            meas_type=\"LT-AF-I\"\n                        else:\n                            MagRec[\"treatment_ac_field\"]='%8.3e' % ( float(treat[0])*1e-3) # AF field in tesla\n                            MagRec[\"treatment_dc_field\"]='0'\n                            meas_type=\"LT-AF-Z\"\n\n\n                # Cooling rate experient # added by rshaar\n                elif demag==\"T\" and methcode == \"LP-CR-TRM\":\n\n                    MagRec[\"treatment_temp\"]='%8.3e' % (float(treat[0])+273.) # temp in kelvin\n                    if treat[1][0]=='0':\n                        meas_type=\"LT-T-Z:LP-CR-TRM\"\n                        MagRec[\"treatment_dc_field\"]='%8.3e'%(0)\n                        MagRec[\"treatment_dc_field_phi\"]='0'\n                        MagRec[\"treatment_dc_field_theta\"]='0'\n                    else:\n                        MagRec[\"treatment_dc_field\"]='%8.3e'%(labfield)\n                        if treat[1][0]=='7': # alteration check as final measurement\n                                meas_type=\"LT-PTRM-I:LP-CR-TRM\"\n                        else:\n                                meas_type=\"LT-T-I:LP-CR-TRM\"\n                        MagRec[\"treatment_dc_field_phi\"]='%7.1f' % (phi) # labfield phi\n                        MagRec[\"treatment_dc_field_theta\"]='%7.1f' % (theta) # labfield theta\n\n                        indx=int(treat[1][0])-1\n                        # alteration check matjed as 0.7 in the measurement file\n                        if indx==6:\n                           cooling_time= cooling_rates_list[-1]\n                        else:\n                           cooling_time=cooling_rates_list[indx]\n                        MagRec[\"measurement_description\"]=\"cooling_rate\"+\":\"+cooling_time+\":\"+\"K/min\"\n\n\n                elif demag!='N':\n                  if len(treat)==1:treat.append('0')\n                  MagRec[\"treatment_temp\"]='%8.3e' % (float(treat[0])+273.) # temp in kelvin\n                  if trm==0:  # demag=T and not trmaq\n                    if treat[1][0]=='0':\n                        meas_type=\"LT-T-Z\"\n                    else:\n                        MagRec[\"treatment_dc_field\"]='%8.3e' % (labfield) # labfield in tesla (convert from microT)\n                        MagRec[\"treatment_dc_field_phi\"]='%7.1f' % (phi) # labfield phi\n                        MagRec[\"treatment_dc_field_theta\"]='%7.1f' % (theta) # labfield theta\n                        if treat[1][0]=='1':meas_type=\"LT-T-I\" # in-field thermal step\n                        if treat[1][0]=='2':\n                            meas_type=\"LT-PTRM-I\" # pTRM check\n                            pTRM=1\n                        if treat[1][0]=='3':\n                            MagRec[\"treatment_dc_field\"]='0'  # this is a zero field step\n                            meas_type=\"LT-PTRM-MD\" # pTRM tail check\n                  else:\n                    labfield=float(treat[1])*1e-6\n                    MagRec[\"treatment_dc_field\"]='%8.3e' % (labfield) # labfield in tesla (convert from microT)\n                    MagRec[\"treatment_dc_field_phi\"]='%7.1f' % (phi) # labfield phi\n                    MagRec[\"treatment_dc_field_theta\"]='%7.1f' % (theta) # labfield theta\n                    meas_type=\"LT-T-I:LP-TRM\" # trm acquisition experiment\n\n\n\n                MagRec[\"measurement_csd\"]=rec[2]\n                MagRec[\"measurement_magn_moment\"]='%10.3e'% (float(rec[3])*1e-3) # moment in Am^2 (from emu)\n                MagRec[\"measurement_dec\"]=rec[4]\n                MagRec[\"measurement_inc\"]=rec[5]\n                MagRec[\"magic_instrument_codes\"]=instcode\n                MagRec[\"er_analyst_mail_names\"]=user\n                MagRec[\"er_citation_names\"]=citation\n                if \"LP-IRM-3D\" in methcode : meas_type=methcode\n                #MagRec[\"magic_method_codes\"]=methcode.strip(':')\n                MagRec[\"magic_method_codes\"]=meas_type\n                MagRec[\"measurement_flag\"]='g'\n                MagRec[\"er_specimen_name\"]=rec[0]\n                if 'std' in rec[0]:\n                    MagRec[\"measurement_standard\"]='s'\n                else:\n                    MagRec[\"measurement_standard\"]='u'\n                MagRec[\"measurement_number\"]='1'\n                #print MagRec['treatment_temp']\n                MagRecs.append(MagRec)\n    MagOuts=pmag.measurements_methods(MagRecs,noave)\n    pmag.magic_write(meas_file,MagOuts,'magic_measurements')\n    print(\"results put in \",meas_file)\n    if len(SynRecs)>0:\n        pmag.magic_write(synfile,SynRecs,'er_synthetics')\n        print(\"synthetics put in \",synfile)\n    return True, meas_file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fitcircle(n, x, y): \n# n points, x points, y points\n    \"\"\"c Fit circle to arbitrary number of x,y pairs, based on the\nc modified least squares method of Umback and Jones (2000),\nc IEEE Transactions on Instrumentation and Measurement.\"\"\"\n    # adding in normalize vectors step\n    #x = numpy.array(x) / max(x)\n    #y = numpy.array(y) / max(y)\n    #\n    \n    sx, sx2, sx3, sy, sy2, sy3, sxy, sxy2, syx2 = (0,) * 9\n    print(type(sx), sx)\n    for i in range(n):\n        sx = sx + x[i]\n        sx2 = sx2 + x[i]**2\n        sx3 = sx3 + x[i]**3\n        sy = sy + y[i]\n        sy2 = sy2 + y[i]**2\n        sy3 = sy3 + y[i]**3\n        sxy = sxy + x[i] * y[i]\n        sxy2 = sxy2 + x[i] * y[i]**2\n        syx2 = syx2 + y[i] * x[i]**2\n\n    A = n * sx2 - sx**2\n    B = n * sxy - sx*sy\n    C = n * sy2 - sy**2\n    D = 0.5 * (n * sxy2 - sx * sy2 + n * sx3 - sx * sx2)\n    E = 0.5 * (n * syx2 - sy * sx2 + n * sy3 - sy * sy2)\n    # values check out up to here\n\n    xo = old_div((D * C - B * E), (A * C - B**2))\n    yo = old_div((A * E - B * D), (A * C - B**2))\n    print(\"xo\", xo)\n    print(\"yo\", yo)\n\n    r = 0\n    for z in range(n):\n        r = r + old_div(numpy.sqrt( (x[z]-xo)**2 + (y[z]-yo)**2 ), n)\n\n    if xo <= numpy.mean(x) and yo <= numpy.mean(y):\n        k = old_div(-1.,r)\n    else:\n        k = old_div(1.,r)\n\n    SSE = lib_k.get_SSE(xo, yo, r, x, y)\n    print(\"r\", r)\n    return k, xo, yo, SSE", "response": "c Fit circle to arbitrary number of x y pairs based on the\nc modified least squares method of Umback and Jones"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnaming vgp_di.py DESCRIPTION converts site latitude, longitude and pole latitude, longitude to declination, inclination SYNTAX vgp_di.py [-h] [-i] [-f FILE] [< filename] OPTIONS -h prints help message and quits -i interactive data entry -f FILE to specify file name on the command line INPUT for file entry: PLAT PLON SLAT SLON where: PLAT: pole latitude PLON: pole longitude (positive east) SLAT: site latitude (positive north) SLON: site longitude (positive east) OUTPUT D I where: D: declination I: inclination", "response": "def main():\n    \"\"\"\n    NAME\n        vgp_di.py\n    DESCRIPTION\n      converts site latitude, longitude and pole latitude, longitude to declination, inclination\n    \n    SYNTAX\n        vgp_di.py [-h] [-i] [-f FILE] [< filename]\n    \n    OPTIONS\n        -h prints help message and quits\n        -i interactive data entry\n        -f FILE to specify file name on the command line\n    \n    INPUT \n      for file entry:\n        PLAT PLON  SLAT SLON    \n      where:\n         PLAT: pole latitude \n         PLON: pole longitude (positive east)\n         SLAT: site latitude (positive north)\n         SLON: site longitude (positive east)\n               \n    OUTPUT\n        D I\n        where:\n           D: declination\n           I: inclination\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-i' in sys.argv: # if one is -i\n        while 1:\n            try:\n                ans=input(\"Input Pole Latitude [positive north]: <cntrl-D to quit>  \")\n                plat=float(ans)  # assign input to plat, after conversion to floating point\n                ans=input(\"Input Pole Longitude [positive east]:  \")\n                plon =float(ans)\n                ans=input(\"Input Site Latitude:  \")\n                slat =float(ans)\n                ans=input(\"Input Site Longitude:  \")\n                slong =float(ans)\n                dec,inc=pmag.vgp_di(plat,plon,slat,slong)  # call vgp_di function from pmag module\n                print('%7.1f %7.1f'%(dec,inc)) # print out returned stuff\n            except EOFError:\n                print(\"\\n Good-bye\\n\")\n                sys.exit()\n            \n    elif '-f' in sys.argv: # manual input of file name\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        f=open(file,'r')\n        inp = f.readlines()  # read from standard inp\n        for line in inp:   # read in the data (as string variable), line by line\n            dec,inc= spitout(line)\n    else:\n        inp = sys.stdin.readlines()  # read from standard input\n        for line in inp:   # read in the data (as string variable), line by line\n            spitout(line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls the user s do archive in a unix - style shell.", "response": "def unix_install():\n    \"\"\"\n    Edits or creates .bashrc, .bash_profile, and .profile files in the users\n    HOME directory in order to add your current directory (hopefully your\n    PmagPy directory) and assorted lower directories in the PmagPy/programs\n    directory to your PATH environment variable. It also adds the PmagPy and\n    the PmagPy/programs directories to PYTHONPATH.\n    \"\"\"\n    PmagPyDir = os.path.abspath(\".\")\n    COMMAND = \"\"\"\\n\nfor d in %s/programs/*/ \"%s/programs/\"; do\n  case \":$PATH:\" in\n    *\":$d:\"*) :;; # already there\n    *) PMAGPATHS=\"$PMAGPATHS:$d\";; # or PATH=\"$PATH:$new_entry\"\n  esac\ndone\nexport PYTHONPATH=\"$PYTHONPATH:%s:%s/programs/\"\nexport PATH=\"$PATH:$PMAGPATHS\" \"\"\" % (PmagPyDir, PmagPyDir, PmagPyDir, PmagPyDir)\n    frc_path = os.path.join(\n        os.environ[\"HOME\"], \".bashrc\")  # not recommended, but hey it freaking works\n    fbprof_path = os.path.join(os.environ[\"HOME\"], \".bash_profile\")\n    fprof_path = os.path.join(os.environ[\"HOME\"], \".profile\")\n    all_paths = [frc_path, fbprof_path, fprof_path]\n\n    for f_path in all_paths:\n        open_type = 'a'\n        if not os.path.isfile(f_path):\n            open_type = 'w+'\n            fout = open(f_path, open_type)\n            fout.write(COMMAND)\n            fout.close()\n        else:\n            fin = open(f_path, 'r')\n            current_f = fin.read()\n            fin.close()\n            if COMMAND not in current_f:\n                fout = open(f_path, open_type)\n                fout.write(COMMAND)\n                fout.close()\n\n    print(\"Install complete. Please restart the shell to complete install.\\nIf you are seeing strange or non-existent paths in your PATH or PYTHONPATH variable please manually check your .bashrc, .bash_profile, and .profile or attempt to reinstall.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninstalls the given Python file into the Windows environment.", "response": "def windows_install(path_to_python=\"\"):\n    \"\"\"\n    Sets the .py extension to be associated with the ftype Python which is\n    then set to the python.exe you provide in the path_to_python variable or\n    after the -p flag if run as a script. Once the python environment is set\n    up the function proceeds to set PATH and PYTHONPATH using setx.\n\n    Parameters\n    ----------\n    path_to_python : the path the python.exe you want windows to execute when\n    running .py files\n    \"\"\"\n    if not path_to_python:\n        print(\"Please enter the path to your python.exe you wish Windows to use to run python files. If you do not, this script will not be able to set up a full python environment in Windows. If you already have a python environment set up in Windows such that you can run python scripts from command prompt with just a file name then ignore this message. Otherwise, you will need to run dev_setup.py again with the command line option '-p' followed by the correct full path to python.\\nRun dev_setup.py with the -h flag for more details\")\n        print(\"Would you like to continue? [y/N] \")\n        ans = input()\n        if ans == 'y':\n            pass\n        else:\n            return\n\n    # be sure to add python.exe if the user forgets to include the file name\n    if os.path.isdir(path_to_python):\n        path_to_python = os.path.join(path_to_python, \"python.exe\")\n    if not os.path.isfile(path_to_python):\n        print(\"The path to python provided is not a full path to the python.exe file or this path does not exist, was given %s.\\nPlease run again with the command line option '-p' followed by the correct full path to python.\\nRun dev_setup.py with the -h flag for more details\" % path_to_python)\n        return\n\n    # make windows associate .py with python\n    subprocess.check_call('assoc .py=Python', shell=True)\n    subprocess.check_call('ftype Python=%s ' %\n                          path_to_python + '\"%1\" %*', shell=True)\n\n    PmagPyDir = os.path.abspath(\".\")\n    ProgramsDir = os.path.join(PmagPyDir, 'programs')\n    dirs_to_add = [ProgramsDir]\n    for d in next(os.walk(ProgramsDir))[1]:\n        dirs_to_add.append(os.path.join(ProgramsDir, d))\n    path = str(subprocess.check_output('echo %PATH%', shell=True)).strip('\\n')\n    if \"PATH\" in path:\n        path = ''\n    pypath = str(subprocess.check_output(\n        'echo %PYTHONPATH%', shell=True)).strip('\\n')\n    if \"PYTHONPATH\" in pypath:\n        pypath = PmagPyDir + ';' + ProgramsDir\n    else:\n        pypath += ';' + PmagPyDir + ';' + ProgramsDir\n    for d_add in dirs_to_add:\n        path += ';' + d_add\n    unique_path_list = []\n    for p in path.split(';'):\n        p = p.replace('\"', '')\n        if p not in unique_path_list:\n            unique_path_list.append(p)\n    unique_pypath_list = []\n    for p in pypath.split(';'):\n        p = p.replace('\"', '')\n        if p not in unique_pypath_list:\n            unique_pypath_list.append(p)\n    path = functools.reduce(lambda x, y: x + ';' + y, unique_path_list)\n    pypath = functools.reduce(lambda x, y: x + ';' + y, unique_pypath_list)\n    print('setx PATH \"%s\"' % path)\n    subprocess.call('setx PATH \"%s\"' % path, shell=True)\n    print('setx PYTHONPATH \"%s\"' % pypath)\n    subprocess.call('setx PYTHONPATH \"%s\"' % (pypath), shell=True)\n\n    print(\"Install complete. Please restart the command prompt to complete install\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    do_help = pmag.get_flag_arg_from_sys('-h')\n    if do_help:\n        print(main.__doc__)\n        return False\n    res_file = pmag.get_named_arg('-f', 'pmag_results.txt')\n    crit_file = pmag.get_named_arg('-fcr', '')\n    spec_file = pmag.get_named_arg('-fsp', '')\n    age_file = pmag.get_named_arg('-fa', '')\n    grade = pmag.get_flag_arg_from_sys('-g')\n    latex = pmag.get_flag_arg_from_sys('-tex')\n    WD = pmag.get_named_arg('-WD', os.getcwd())\n    ipmag.pmag_results_extract(res_file, crit_file, spec_file, age_file, latex, grade, WD)", "response": "main function for pmag_results_extract. py"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    dir_path='.'\n    tspec=\"thellier_specimens.txt\"\n    aspec=\"AC_specimens.txt\"\n    ofile=\"TorAC_specimens.txt\"\n    critfile=\"pmag_criteria.txt\"\n    ACSamplist,Samplist,sigmin=[],[],10000\n    GoodSamps,SpecOuts=[],[]\n# get arguments from command line\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-fu' in sys.argv:\n        ind=sys.argv.index('-fu')\n        tspec=sys.argv[ind+1]\n    if '-fc' in sys.argv:\n        ind=sys.argv.index('-fc')\n        aspec=sys.argv[ind+1]\n    if '-F' in sys.argv:\n        ind=sys.argv.index('-F')\n        ofile=sys.argv[ind+1]\n    if '-WD' in sys.argv:\n        ind=sys.argv.index('-WD')\n        dir_path=sys.argv[ind+1]\n         \n    # read in pmag_specimens file\n    tspec=dir_path+'/'+tspec\n    aspec=dir_path+'/'+aspec\n    ofile=dir_path+'/'+ofile\n    Specs,file_type=pmag.magic_read(tspec)\n    Specs,file_type=pmag.magic_read(tspec)\n    Speclist=pmag.get_specs(Specs)\n    ACSpecs,file_type=pmag.magic_read(aspec)\n    ACspeclist=pmag.get_specs(ACSpecs)\n    for spec in Specs:\n            if spec[\"er_sample_name\"] not in Samplist:Samplist.append(spec[\"er_sample_name\"])\n    for spec in ACSpecs:\n            if spec[\"er_sample_name\"] not in ACSamplist:ACSamplist.append(spec[\"er_sample_name\"])\n    #\n    for samp in Samplist:\n        useAC,Ints,ACInts,GoodSpecs,AC,UC=0,[],[],[],[],[]\n        for spec in Specs:\n            if spec[\"er_sample_name\"].lower()==samp.lower():\n                    UC.append(spec)\n        if samp in ACSamplist:\n            for spec in ACSpecs:\n                if spec[\"er_sample_name\"].lower()==samp.lower():\n                        AC.append(spec)\n        if len(AC)>0:\n            AClist=[]\n            for spec in AC: \n                SpecOuts.append(spec)\n                AClist.append(spec['er_specimen_name'])\n                print('using AC: ',spec['er_specimen_name'],'%7.1f'%(1e6*float(spec['specimen_int'])))\n            for spec in UC: \n                if spec['er_specimen_name'] not in AClist:\n                   SpecOuts.append(spec)\n#                   print 'using UC: ',spec['er_specimen_name'],'%7.1f'%(1e6*float(spec['specimen_int']))\n        else:\n            for spec in UC: \n                SpecOuts.append(spec)\n#                print 'using UC: ',spec['er_specimen_name'],'%7.1f'%(1e6*float(spec['specimen_int']))\n    SpecOuts,keys=pmag.fillkeys(SpecOuts)\n    pmag.magic_write(ofile,SpecOuts,'pmag_specimens')\n    print('thellier data assessed for AC correction put in ', ofile)", "response": "get an anisotropy corrected data and replace it with an anisotropy corrected data and create a new one"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_specimen_PI_criteria(pars,acceptance_criteria):\n    '''\n    # Check if specimen pass Acceptance criteria\n    '''\n    #if 'pars' not in self.Data[specimen].kes():\n    #    return\n\n    pars['specimen_fail_criteria']=[]\n    for crit in list(acceptance_criteria.keys()):\n        if crit not in list(pars.keys()):\n            continue\n        if acceptance_criteria[crit]['value']==-999:\n            continue\n        if acceptance_criteria[crit]['category']!='IE-SPEC':\n            continue\n        cutoff_value=acceptance_criteria[crit]['value']\n        if crit=='specimen_scat':\n            if pars[\"specimen_scat\"] in [\"Fail\",'b',0,'0','FALSE',\"False\",False,\"f\"]:\n                pars['specimen_fail_criteria'].append('specimen_scat')\n        elif crit=='specimen_k' or crit=='specimen_k_prime':\n            if abs(pars[crit])>cutoff_value:\n                pars['specimen_fail_criteria'].append(crit)\n        # high threshold value:\n        elif acceptance_criteria[crit]['threshold_type']==\"high\":\n            if pars[crit]>cutoff_value:\n                pars['specimen_fail_criteria'].append(crit)\n        elif acceptance_criteria[crit]['threshold_type']==\"low\":\n            if pars[crit]<cutoff_value:\n                pars['specimen_fail_criteria'].append(crit)\n    return pars", "response": "Check if specimen pass Acceptance criteria"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nname dia_vgp.py DESCRIPTION converts declination inclination alpha95 to virtual geomagnetic pole, dp and dm SYNTAX dia_vgp.py [-h] [-i] [-f FILE] [< filename] OPTIONS -h prints help message and quits -i interactive data entry -f FILE to specify file name on the command line INPUT for file entry: D I A95 SLAT SLON where: D: declination I: inclination A95: alpha_95 SLAT: site latitude (positive north) SLON: site longitude (positive east) OUTPUT PLON PLAT DP DM where: PLAT: pole latitude PLON: pole longitude (positive east) DP: 95% confidence angle in parallel DM: 95% confidence angle in meridian", "response": "def main():\n    \"\"\"\n    NAME\n        dia_vgp.py\n    DESCRIPTION\n      converts declination inclination alpha95 to virtual geomagnetic pole, dp and dm\n    \n    SYNTAX\n        dia_vgp.py [-h] [-i] [-f FILE] [< filename]\n    \n    OPTIONS\n        -h prints help message and quits\n        -i interactive data entry\n        -f FILE to specify file name on the command line\n    \n    INPUT \n      for file entry:\n        D I A95 SLAT SLON      \n      where:\n         D: declination\n         I: inclination\n         A95: alpha_95\n         SLAT: site latitude (positive north)\n         SLON: site longitude (positive east)\n               \n    OUTPUT\n        PLON PLAT DP DM\n        where:\n             PLAT: pole latitude \n             PLON: pole longitude (positive east)\n             DP: 95% confidence angle in parallel \n             DM: 95% confidence angle in meridian \n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-i' in sys.argv: # if one is -i\n        while 1:\n            try:\n                ans=input(\"Input Declination: <cntrl-D to quit>  \")\n                Dec=float(ans)  # assign input to Dec, after conversion to floating point\n                ans=input(\"Input Inclination:  \")\n                Inc =float(ans)\n                ans=input(\"Input Alpha 95:  \")\n                a95 =float(ans)\n                ans=input(\"Input Site Latitude:  \")\n                slat =float(ans)\n                ans=input(\"Input Site Longitude:  \")\n                slong =float(ans)\n                spitout(Dec,Inc,a95,slat,slong)  # call dia_vgp function from pmag module\n                print('%7.1f %7.1f %7.1f %7.1f'%(plong,plat,dp,dm)) # print out returned stuff\n            except:\n                print(\"\\n Good-bye\\n\")\n                sys.exit()\n            \n    elif '-f' in sys.argv: # manual input of file name\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        f=open(file,'r')\n        inlist  = []\n        for line in f.readlines():\n            inlist.append([])\n            # loop over the elements, split by whitespace\n            for el in line.split():\n                inlist[-1].append(float(el))\n        spitout(inlist)\n\n    else:\n        input = sys.stdin.readlines()  # read from standard input\n        inlist  = []\n        for line in input:   # read in the data (as string variable), line by line\n            inlist.append([])\n            # loop over the elements, split by whitespace\n            for el in line.split():\n                inlist[-1].append(float(el))\n        spitout(inlist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    fmt='svg'\n    title=\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n       ind=sys.argv.index('-f')\n       file=sys.argv[ind+1] \n       X=numpy.loadtxt(file)\n       file=sys.argv[ind+2] \n       X2=numpy.loadtxt(file)\n#    else:\n#       X=numpy.loadtxt(sys.stdin,dtype=numpy.float)\n    else:\n       print('-f option required')\n       print(main.__doc__)\n       sys.exit()\n    if '-fmt' in sys.argv:\n       ind=sys.argv.index('-fmt')\n       fmt=sys.argv[ind+1] \n    if '-t' in sys.argv:\n       ind=sys.argv.index('-t')\n       title=sys.argv[ind+1] \n    CDF={'X':1}\n    pmagplotlib.plot_init(CDF['X'],5,5)\n    pmagplotlib.plot_cdf(CDF['X'],X,'','r','')\n    pmagplotlib.plot_cdf(CDF['X'],X2,title,'b','')\n    D,p=scipy.stats.ks_2samp(X,X2)\n    if p>=.05:\n        print(D,p,' not rejected at 95%')\n    else:\n        print(D,p,' rejected at 95%')\n    pmagplotlib.draw_figs(CDF)\n    ans= input('S[a]ve  plot, <Return> to quit ')\n    if ans=='a':\n        files={'X':'CDF_.'+fmt}\n        pmagplotlib.save_plots(CDF,files)", "response": "NAME\n        plot_2cdfs.py\n\n    DESCRIPTION\n        makes plots of cdfs of data in input file \n\n    SYNTAX\n        plot_2cdfs.py [-h][command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE1 FILE2\n        -t TITLE\n        -fmt [svg,eps,png,pdf,jpg..] specify format of output figure, default is svg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if len(sys.argv) <= 1:\n        print(main.__doc__)\n        print('you must supply a file name')\n        sys.exit()\n    FIG = {}  # plot dictionary\n    FIG['lowrie'] = 1  # demag is figure 1\n    pmagplotlib.plot_init(FIG['lowrie'], 6, 6)\n    norm = 1  # default is to normalize by maximum axis\n    in_file = pmag.get_named_arg(\"-f\", \"measurements.txt\")\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    in_file = pmag.resolve_file_name(in_file, dir_path)\n    data_model = pmag.get_named_arg(\"-DM\", 3)\n    data_model = int(float(data_model))\n    fmt = pmag.get_named_arg(\"-fmt\", \"svg\")\n    if '-N' in sys.argv:\n        norm = 0  # don't normalize\n    if '-sav' in sys.argv:\n        plot = 1  # silently save and quit\n    else:\n        plot = 0 # generate plots\n    print(in_file)\n    # read in data\n    PmagRecs, file_type = pmag.magic_read(in_file)\n    if data_model == 2 and file_type != \"magic_measurements\":\n        print('bad input file', file_type)\n        sys.exit()\n    if data_model == 3 and file_type != \"measurements\":\n        print('bad input file', file_type)\n        sys.exit()\n\n    if data_model == 2:\n        meth_code_col = 'magic_method_codes'\n        spec_col = 'er_specimen_name'\n        dec_col = \"measurement_dec\"\n        inc_col = 'measurement_inc'\n        moment_col = 'measurement_magn_moment'\n        temp_col = 'treatment_temp'\n    else:\n        meth_code_col = 'method_codes'\n        spec_col = 'specimen'\n        dec_col = 'dir_dec'\n        inc_col = 'dir_inc'\n        moment_col = 'magn_moment'\n        temp_col = \"treat_temp\"\n\n    PmagRecs = pmag.get_dictitem(\n        PmagRecs, meth_code_col, 'LP-IRM-3D', 'has')  # get all 3D IRM records\n\n    if len(PmagRecs) == 0:\n        print('no records found with the method code LP-IRM-3D')\n        sys.exit()\n\n    specs = pmag.get_dictkey(PmagRecs, spec_col, '')\n    sids = []\n    for spec in specs:\n        if spec not in sids:\n            sids.append(spec)  # get list of unique specimen names\n    for spc in sids:  # step through the specimen names\n        print(spc)\n        specdata = pmag.get_dictitem(\n            PmagRecs, spec_col, spc, 'T')  # get all this one's data\n\n        DIMs, Temps = [], []\n        for dat in specdata:  # step through the data\n            DIMs.append([float(dat[dec_col]), float(\n                dat[inc_col]), float(dat[moment_col])])\n            Temps.append(float(dat[temp_col])-273.)\n        carts = pmag.dir2cart(DIMs).transpose()\n        if norm == 1:  # want to normalize\n            nrm = (DIMs[0][2])  # normalize by NRM\n            ylab = \"M/M_o\"\n        else:\n            nrm = 1.  # don't normalize\n            ylab = \"Magnetic moment (Am^2)\"\n        xlab = \"Temperature (C)\"\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[0]) / nrm, sym='r-')\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[0]) / nrm, sym='ro')  # X direction\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[1]) / nrm, sym='c-')\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[1]) / nrm, sym='cs')  # Y direction\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[2]) / nrm, sym='k-')\n        pmagplotlib.plot_xy(FIG['lowrie'], Temps, abs(carts[2]) / nrm, sym='k^', title=spc, xlab=xlab, ylab=ylab)  # Z direction\n        files = {'lowrie': 'lowrie:_'+spc+'_.'+fmt}\n        if plot == 0:\n            pmagplotlib.draw_figs(FIG)\n            ans = input('S[a]ve figure? [q]uit, <return> to continue   ')\n            if ans == 'a':\n                pmagplotlib.save_plots(FIG, files)\n            elif ans == 'q':\n                sys.exit()\n        else:\n            pmagplotlib.save_plots(FIG, files)\n        pmagplotlib.clearFIG(FIG['lowrie'])", "response": "main function for the main function of the main function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    infile='pmag_specimens.txt'\n    sampfile=\"er_samples.txt\"\n    outfile=\"er_samples.txt\"\n# get command line stuff\n    if \"-h\" in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-fsp' in sys.argv:\n        ind=sys.argv.index(\"-fsp\")\n        infile=sys.argv[ind+1]\n    if '-fsm' in sys.argv:\n        ind=sys.argv.index(\"-fsm\")\n        sampfile=sys.argv[ind+1]\n    if '-F' in sys.argv:\n        ind=sys.argv.index(\"-F\")\n        outfile=sys.argv[ind+1]\n    if '-WD' in sys.argv:\n        ind=sys.argv.index(\"-WD\")\n        dir_path=sys.argv[ind+1]\n        infile=dir_path+'/'+infile\n        sampfile=dir_path+'/'+sampfile\n        outfile=dir_path+'/'+outfile\n# now do re-ordering\n    pmag.ReorderSamples(infile,sampfile,outfile)", "response": "NAME\n        reorder_samples.py\n\n    DESCRIPTION\n        takes specimen file and reorders sample file with selected orientation methods placed first\n\n    SYNTAX\n        reorder_samples.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -fsp: specimen input pmag_specimens format file, default is \"pmag_specimens.txt\"\n        -fsm: sample input er_samples format file, default is \"er_samples.txt\"\n        -F: output er_samples format file, default is \"er_samples.txt\"\n    OUPUT\n        writes re-ordered er_samples.txt file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef not_null(val, zero_as_null=True):\n\n    def can_iter(x):\n        \"\"\"\n        Returns True for a non-empty iterable\n        \"\"\"\n        try:\n            any(x)\n            return True\n        except TypeError:\n            return False\n\n    def not_empty(x):\n        \"\"\"\n        Returns true if x has length\n        \"\"\"\n        if len(x):\n            return True\n        return False\n\n\n    def exists(x):\n        \"\"\"\n        Returns true if x\n        \"\"\"\n        if x:\n            return True\n        return False\n\n    def is_nan(x):\n        \"\"\"\n        Returns True if x is nan\n        \"\"\"\n        try:\n            if np.isnan(x):\n                return True\n        except TypeError:\n            return False\n        return False\n\n    # return True iff you have a non-empty iterable\n    # and False for an empty iterable (including an empty string)\n    if can_iter(val):\n        return not_empty(val)\n    # if value is not iterable, return False for np.nan, None, 0, or False\n    # & True for all else\n    else:\n        if is_nan(val):\n            return False\n        if not zero_as_null:\n            if val == 0:\n                return True\n        return exists(val)", "response": "Returns True if a value is not null or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_intensity_col(data):\n    # possible intensity columns\n    intlist = ['magn_moment', 'magn_volume', 'magn_mass','magn_uncal']\n    # intensity columns that are in the data\n    int_meths = [col_name for col_name in data.columns if col_name in intlist]\n    # drop fully null columns\n    data.dropna(axis='columns', how='all')\n    # ignore columns with only blank values (including \"\")\n    for col_name in int_meths[:]:\n        if not data[col_name].any():\n            int_meths.remove(col_name)\n    if len(int_meths):\n        if 'magn_moment' in int_meths:\n            return 'magn_moment'\n        return int_meths[0]\n    return \"\"", "response": "Returns the first intensity column that is in the dataframe and has data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_sites_to_meas_table(dir_path):\n    reqd_tables = ['measurements', 'specimens', 'samples', 'sites']\n    con = Contribution(dir_path, read_tables=reqd_tables)\n    # check that all required tables are available\n    missing_tables = []\n    for table in reqd_tables:\n        if table not in con.tables:\n            missing_tables.append(table)\n    if missing_tables:\n        return False, \"You are missing {} tables\".format(\", \".join(missing_tables))\n\n    # put sample column into the measurements table\n    con.propagate_name_down('sample', 'measurements')\n    # put site column into the measurements table\n    con.propagate_name_down('site', 'measurements')\n    # check that column propagation was successful\n    if 'site' not in con.tables['measurements'].df.columns:\n        return False, \"Something went wrong with propagating sites down to the measurement level\"\n    return True, con.tables['measurements'].df", "response": "Add site columns to the measurements table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prep_for_intensity_plot(data, meth_code, dropna=(), reqd_cols=()):\n    # initialize\n    dropna = list(dropna)\n    reqd_cols = list(reqd_cols)\n    # get intensity column\n    try:\n        magn_col = get_intensity_col(data)\n    except AttributeError:\n        return False, \"Could not get intensity method from data\"\n    # drop empty columns\n    if magn_col not in dropna:\n        dropna.append(magn_col)\n    data = data.dropna(axis=0, subset=dropna)\n    # add to reqd_cols list\n    if 'method_codes' not in reqd_cols:\n        reqd_cols.append('method_codes')\n    if magn_col not in reqd_cols:\n        reqd_cols.append(magn_col)\n    # drop non reqd cols, make sure all reqd cols are present\n    try:\n        data = data[reqd_cols]\n    except KeyError as ex:\n        print(ex)\n        missing = set(reqd_cols).difference(data.columns)\n        return False, \"missing these required columns: {}\".format(\", \".join(missing))\n    # filter out records without the correct method code\n    data = data[data['method_codes'].str.contains(meth_code).astype(bool)]\n    return True, data", "response": "Prepare the data for an intensity plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a dataframe and string - i - fy a column of values.", "response": "def stringify_col(df, col_name):\n    \"\"\"\n    Take a dataframe and string-i-fy a column of values.\n    Turn nan/None into \"\" and all other values into strings.\n\n    Parameters\n    ----------\n    df : dataframe\n    col_name : string\n    \"\"\"\n    df = df.copy()\n    df[col_name] = df[col_name].fillna(\"\")\n    df[col_name] = df[col_name].astype(str)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_empty_magic_table(self, dtype, col_names=None, groups=None):\n        if dtype not in self.table_names:\n            print(\"-W- {} is not a valid MagIC table name\".format(dtype))\n            print(\"-I- Valid table names are: {}\".format(\", \".join(self.table_names)))\n            return\n        data_container = MagicDataFrame(dtype=dtype, columns=col_names, groups=groups)\n        self.tables[dtype] = data_container", "response": "Add a blank MagicDataFrame to the contribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a MagIC table to the contribution from a data list", "response": "def add_magic_table_from_data(self, dtype, data):\n        \"\"\"\n        Add a MagIC table to the contribution from a data list\n\n        Parameters\n        ----------\n        dtype : str\n            MagIC table type, i.e. 'specimens'\n        data : list of dicts\n            data list with format [{'key1': 'val1', ...}, {'key1': 'val2', ...}, ... }]\n        \"\"\"\n        self.tables[dtype] = MagicDataFrame(dtype=dtype, data=data)\n        if dtype == 'measurements':\n            self.tables['measurements'].add_sequence()\n        return dtype, self.tables[dtype]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_magic_table(self, dtype, fname=None, df=None):\n        if df is None:\n            # if providing a filename but no data type\n            if dtype == \"unknown\":\n                filename = os.path.join(self.directory, fname)\n                if not os.path.exists(filename):\n                    return False, False\n                data_container = MagicDataFrame(filename, dmodel=self.data_model)\n                dtype = data_container.dtype\n                if dtype == 'empty':\n                    return False, False\n                else:\n                    self.tables[dtype] = data_container\n                    return dtype, data_container\n            # if providing a data type, use the canonical filename\n            elif dtype not in self.filenames:\n                print('-W- \"{}\" is not a valid MagIC table type'.format(dtype))\n                print(\"-I- Available table types are: {}\".format(\", \".join(self.table_names)))\n                return False, False\n            #filename = os.path.join(self.directory, self.filenames[dtype])\n            filename = pmag.resolve_file_name(self.filenames[dtype], self.directory)\n            if os.path.exists(filename):\n                data_container = MagicDataFrame(filename, dtype=dtype,\n                                                dmodel=self.data_model)\n                if data_container.dtype != \"empty\":\n                    self.tables[dtype] = data_container\n                    return dtype, data_container\n                else:\n                    return False, False\n            else:\n                #print(\"-W- No such file: {}\".format(filename))\n                return False, False\n        # df is not None\n        else:\n            if not dtype:\n                print(\"-W- Must provide dtype\")\n                return False, False\n            data_container = MagicDataFrame(dtype=dtype, df=df)\n            self.tables[dtype] = data_container\n        self.tables[dtype].sort_dataframe_cols()\n        return dtype, self.tables[dtype]", "response": "Read in a new file to add a new table to self. tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef propagate_measurement_info(self):\n        meas_df = self.tables['measurements'].df\n        names_list = ['specimen', 'sample', 'site', 'location']\n        # add in any tables that you can\n        for num, name in enumerate(names_list):\n            # don't replace tables that already exist\n            if (name + \"s\") in self.tables:\n                continue\n            elif name in meas_df.columns:\n                items = meas_df[name].unique()\n                df = pd.DataFrame(columns=[name], index=items)\n                df[name] = df.index\n                # add in parent name if possible\n                # (i.e., sample name to specimens table)\n                if num < (len(names_list) - 1):\n                    parent = names_list[num+1]\n                    if parent in meas_df.columns:\n                        meas_df = meas_df.where(meas_df.notnull(), \"\")\n                        df[parent] = meas_df.drop_duplicates(subset=[name])[parent].values.astype(str)\n                df = df.where(df != \"\", np.nan)\n                df = df.dropna(how='all', axis='rows')\n                if len(df):\n                    self.tables[name + \"s\"] = MagicDataFrame(dtype=name + \"s\", df=df)\n                    self.write_table_to_file(name + \"s\")", "response": "Take a contribution with a measurement table and create the index for the unique names in the measurement table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npropagate all items from all tables to the file.", "response": "def propagate_all_tables_info(self, write=True):\n        \"\"\"\n        Find any items (specimens, samples, sites, or locations) from\n        tables other than measurements and make sure they each have a\n        row in their own table.  For example, if a site name is in\n        the samples table but not in the sites table, create a row\n        for it in the sites table.\n        \"\"\"\n        for table_name in [\"specimens\", \"samples\", \"sites\", \"locations\"]:\n            if not table_name in self.tables:\n                continue\n            df = self.tables[table_name].df\n            parent_name, child_name = self.get_parent_and_child(table_name)\n            if parent_name:\n                if parent_name[:-1] in df.columns:\n                    parents = sorted(set(df[parent_name[:-1]].dropna().values.astype(str)))\n                    if parent_name in self.tables: # if there is a parent table, update it\n                        parent_df = self.tables[parent_name].df\n                        missing_parents = set(parents) - set(parent_df.index)\n                        if missing_parents: # add any missing values\n                            print(\"-I- Updating {} table with values from {} table\".format(parent_name, table_name))\n                            for item in missing_parents:\n                                self.add_item(parent_name, {parent_name[:-1]: item}, label=item)\n                            # save any changes to file\n                            if write:\n                                self.write_table_to_file(parent_name)\n\n                    else:  # if there is no parent table, create it if necessary\n                        if parents:\n                            # create a parent_df with the names you got from the child\n                            print(\"-I- Creating new {} table with data from {} table\".format(parent_name, table_name))\n                            # add in the grandparent if available\n                            grandparent_name = self.get_parent_and_child(parent_name)[0]\n                            if grandparent_name:\n                                grandparent = \"\"\n                                if grandparent_name in df.columns:\n                                    grandparent = df[df[parent_name] == item][grandparent_name].values[0]\n                                columns = [parent_name[:-1]]#, grandparent_name[:-1]]\n                            else:\n                                columns = [parent_name[:-1]]\n\n                            parent_df = pd.DataFrame(columns=columns, index=parents)\n                            parent_df[parent_name[:-1]] = parent_df.index\n                            if grandparent_name:\n                                if grandparent_name[:-1] in df.columns:\n                                    parent_df = pd.merge(df[[parent_name[:-1], grandparent_name[:-1]]], parent_df, on=parent_name[:-1])\n                            self.tables[parent_name] = MagicDataFrame(dtype=parent_name,\n                                                                      df=parent_df)\n                            if write:\n                                # save new table to file\n                                self.write_table_to_file(parent_name)\n            if child_name:\n                if child_name in df.columns:\n                    raw_children = df[child_name].dropna().str.split(':')\n                    # create dict of all children with parent info\n                    parent_of_child = {}\n                    for parent, children in raw_children.items():\n                        for child in children:\n                            # remove whitespace\n                            child = child.strip()\n                            old_parent = parent_of_child.get(child)\n                            if old_parent and parent and (old_parent != parent):\n                                print('-I- for {} {}, replacing: {} with: {}'.format(child_name[:-1], child,\n                                                                                     old_parent, parent))\n                            parent_of_child[child] = parent\n                    # old way:\n                    # flatten list, ignore duplicates\n                    #children = sorted(set([item.strip() for sublist in raw_children for item in sublist]))\n                    if child_name in self.tables: # if there is already a child table, update it\n                        child_df = self.tables[child_name].df\n                        missing_children = set(parent_of_child.keys()) - set(child_df.index)\n                        if missing_children: # add any missing values\n                            print(\"-I- Updating {} table with values from {} table\".format(child_name, table_name))\n                            for item in missing_children:\n                                data = {child_name[:-1]: item, table_name[:-1]: parent_of_child[item]}\n                                self.add_item(child_name, data, label=item)\n                            if write:\n                                # save any changes to file\n                                self.write_table_to_file(child_name)\n                    else: # if there is no child table, create it if necessary\n                        if children:\n                            # create a child_df with the names you got from the parent\n                            print(\"-I- Creating new {} table with data from {} table\".format(child_name, table_name))\n                            # old way to make new table:\n                            #child_df = pd.DataFrame(columns=[table_name[:-1]], index=children)\n                            # new way to make new table\n                            children_list = sorted(parent_of_child.keys())\n                            children_data = [[child_name, parent_of_child[c_name]] for c_name in children_list]\n                            child_df = pd.DataFrame(index=children_list, columns=[child_name[:-1], table_name[:-1]], data=children_data)\n\n                            self.tables[child_name] = MagicDataFrame(dtype=child_name, df=child_df)\n                            if write:\n                                # save new table to file\n                                self.write_table_to_file(child_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parent_and_child(self, table_name):\n        if table_name not in self.ancestry:\n            return None, None\n        parent_ind = self.ancestry.index(table_name) + 1\n        if parent_ind + 1 > len(self.ancestry):\n            parent_name = None\n        else:\n            parent_name = self.ancestry[parent_ind]\n        child_ind = self.ancestry.index(table_name) - 1\n        if child_ind < 0:\n            child_name = None\n        else:\n            child_name = self.ancestry[child_ind]\n        return parent_name, child_name", "response": "Get the name of the parent table and child table for a given MagIC table name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_min_max_lat_lon(self):\n        if 'sites' not in self.tables:\n            return\n        # get min/max lat/lon from sites table\n        site_container = self.tables['sites']\n        if not ('lat' in site_container.df.columns and 'lon' in site_container.df.columns):\n            return\n        # convert lat/lon columns to string type\n        # (this is necessary for consistency because they MAY be string type already)\n        site_container.df['lat'] = site_container.df['lat'].fillna('').astype(str)\n        site_container.df['lon'] = site_container.df['lon'].fillna('').astype(str)\n        # replace empty strings with np.nan\n        site_container.df['lat'] = np.where(site_container.df['lat'].str.len(), site_container.df['lat'], np.nan)\n        site_container.df['lon'] = np.where(site_container.df['lon'].str.len(), site_container.df['lon'], np.nan)\n        # convert lat/lon values to float (they make be string from grid)\n        try:\n            site_container.df['lat'] = site_container.df['lat'].astype(float)\n        except ValueError as ex:\n            print('-W- Improperly formatted numbers in sites.lat')\n            return\n        try:\n            site_container.df['lon'] = site_container.df['lon'].astype(float)\n        except ValueError as ex:\n            print('-W- Improperly formatted numbers in sites.lon')\n            return\n        # group lat/lon by location\n        grouped_lon = site_container.df[['lon', 'location']].groupby('location')\n        grouped_lat = site_container.df[['lat', 'location']].groupby('location')\n        # get min/max longitude by location\n        lon_w = grouped_lon.min()\n        lon_e = grouped_lon.max()\n        # get min/max latitude by location\n        lat_s = grouped_lat.min()\n        lat_n = grouped_lat.max()\n        # assign lat/lon to location table\n        locs = {}\n        if 'locations' not in self.tables:\n            return\n        loc_container = self.tables['locations']\n        for loc in lat_s.index:\n            coords = {}\n            coords['lat_s'] = lat_s.loc[loc]['lat']\n            coords['lat_n'] = lat_n.loc[loc]['lat']\n            coords['lon_e'] = lon_e.loc[loc]['lon']\n            coords['lon_w'] = lon_w.loc[loc]['lon']\n            locs[loc] = coords\n        loc_container = self.tables['locations']\n        for loc_name in locs:\n            if loc_name in loc_container.df.index:\n                coords = locs[loc_name]\n                for coord in locs[loc_name]:\n                    # warn user if an old value will be overwritten\n                    new_value = coords[coord]\n                    # if the new value is null, ignore it\n                    if is_null(new_value, zero_as_null=False):\n                        continue\n                    # set old value to None if it wasn't in table\n                    if coord not in loc_container.df.columns:\n                        loc_container.df[coord] = None\n                    old_value = loc_container.df.loc[loc_name, coord]\n                    # use first value if multiple values returned, but don't shorten a string\n                    if not (isinstance(old_value, str)):\n                        try:\n                            old_value = old_value.values.astype(str)[0]\n                        except (TypeError,IndexError,AttributeError) as e: # if only one value, or np.nan, or NoneType\n                            pass\n                    if is_null(old_value, zero_as_null=False):\n                        pass\n                    elif isinstance(old_value, str):\n                        try:\n                            old_value = float(old_value)\n                        except ValueError:\n                            print('-W- In {}, automatically generated {} value ({}) will overwrite previous value ({})'.format(loc_name, coord, new_value, old_value))\n                            old_value = None\n                    elif not math.isclose(new_value, old_value):\n                        print('-W- In {}, automatically generated {} value ({}) will overwrite previous value ({})'.format(loc_name, coord, new_value, old_value))\n                    # set new value\n                    new_value = round(float(new_value), 5)\n                    loc_container.df.loc[loc_name, coord] = new_value\n        self.write_table_to_file('locations')\n        return locs", "response": "Find latitude and longitude information from sites table\n            and return the minimum and maximum latitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef propagate_lithology_cols(self):\n        cols = ['lithologies', 'geologic_types', 'geologic_classes']\n        #for table in ['specimens', 'samples']:\n            # convert \"Not Specified\" to blank\n            #self.tables[table].df.replace(\"^[Nn]ot [Ss]pecified\", '',\n            #                              regex=True, inplace=True)\n        self.propagate_cols(cols, 'samples', 'sites')\n        cols = ['lithologies', 'geologic_types', 'geologic_classes']\n        self.propagate_cols(cols, 'specimens', 'samples')\n        # if sites table is missing any values,\n        # go ahead and propagate values UP as well\n        if 'sites' not in self.tables:\n            return\n        for col in cols:\n            if col not in self.tables['sites'].df.columns:\n                self.tables['sites'].df[col] = None\n        if not all(self.tables['sites'].df[cols].values.ravel()):\n            print('-I- Propagating values up from samples to sites...')\n            self.propagate_cols_up(cols, 'sites', 'samples')", "response": "Propagate any data from lithologies geologic_types or geologic_classes to the samples and specimens tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenames an item in a table.", "response": "def rename_item(self, table_name, item_old_name, item_new_name):\n        \"\"\"\n        Rename item (such as a site) everywhere that it occurs.\n        This change often spans multiple tables.\n        For example, a site name will occur in the sites table,\n        the samples table, and possibly in the locations/ages tables.\n        \"\"\"\n        # define some helper methods:\n\n        def put_together_if_list(item):\n            \"\"\"\n            String joining function\n            that doesn't break with None/np.nan\n            \"\"\"\n            try:\n                res = \":\".join(item)\n                return \":\".join(item)\n            except TypeError as ex:\n                #print ex\n                return item\n\n        def replace_colon_delimited_value(df, col_name, old_value, new_value):\n            \"\"\"\n            Col must contain list\n            \"\"\"\n            count = 1\n            for index, row in df[df[col_name].notnull()].iterrows():\n                names_list = row[col_name]\n                names_list = [name.strip() for name in names_list]\n                try:\n                    ind = names_list.index(old_value)\n                except ValueError as ex:\n                    count += 1\n                    continue\n                names_list[ind] = new_value\n                df.loc[count, col_name] = names_list\n                count += 1\n\n        # initialize some things\n        item_type = table_name\n        ###col_name = item_type[:-1] + \"_name\"\n        col_name = item_type[:-1]\n        col_name_plural = col_name + \"s\"\n        table_df = self.tables[item_type].df\n\n        if item_old_name == '':\n            # just add a new item\n            self.add_item(table_name, {col_name: item_new_name}, item_new_name)\n            return\n\n        # rename item in its own table\n        table_df.rename(index={item_old_name: item_new_name}, inplace=True)\n        # rename in any parent/child tables\n        for table_name in self.tables:\n            df = self.tables[table_name].df\n            col_names = df.columns\n            # change anywhere col_name (singular, i.e. site) is found\n            if col_name in col_names:\n                df[col_name].where(df[col_name] != item_old_name, item_new_name, inplace=True)\n                # change anywhere col_name (plural, i.e. sites) is found\n            if col_name_plural in col_names:\n                df[col_name_plural + \"_list\"] = df[col_name_plural].str.split(\":\")\n                replace_colon_delimited_value(df, col_name_plural + \"_list\", item_old_name, item_new_name)\n                df[col_name_plural] = df[col_name_plural + \"_list\"].apply(put_together_if_list)\n                df.drop(col_name_plural + \"_list\", axis=1, inplace=True)\n            self.tables[table_name].df = df"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns both the table_name and col_name for a given index in the ancestry.", "response": "def get_table_name(self, ind):\n        \"\"\"\n        Return both the table_name (i.e., 'specimens')\n        and the col_name (i.e., 'specimen')\n        for a given index in self.ancestry.\n        \"\"\"\n        if ind >= len(self.ancestry):\n            return \"\", \"\"\n        if ind > -1:\n            table_name = self.ancestry[ind]\n            ###name = table_name[:-1] + \"_name\"\n            name = table_name[:-1]\n            return table_name, name\n        return \"\", \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef propagate_name_down(self, col_name, df_name, verbose=False):\n        if df_name not in self.tables:\n            table = self.add_magic_table(df_name)[1]\n            if is_null(table):\n                return\n        df = self.tables[df_name].df\n        if col_name in df.columns:\n            if all(df[col_name].apply(not_null)):\n                #print('{} already in {}'.format(col_name, df_name))\n                return df\n\n        # otherwise, do necessary merges to get col_name into df\n        # get names for each level\n        grandparent_table_name = col_name.split('_')[0] + \"s\"\n        grandparent_name = grandparent_table_name[:-1]\n        ind = self.ancestry.index(grandparent_table_name) - 1\n        #\n        parent_table_name, parent_name = self.get_table_name(ind)\n        child_table_name, child_name = self.get_table_name(ind - 1)\n        bottom_table_name, bottom_name = self.get_table_name(ind - 2)\n\n        # merge in bottom level\n        if child_name not in df.columns:\n            # add child table if missing\n            if bottom_table_name not in self.tables:\n                result = self.add_magic_table(bottom_table_name)[1]\n                if not isinstance(result, MagicDataFrame):\n                    if verbose:\n                        print(\"-W- Couldn't read in {} data for data propagation\".format(bottom_table_name))\n                    return df\n            # add child_name to df\n            add_df = self.tables[bottom_table_name].df\n            # drop duplicate names\n            add_df = add_df.drop_duplicates(subset=bottom_name)\n            if child_name not in df.columns:\n                if verbose:\n                    print(\"-W- Cannot complete propagation, {} table is missing {} column\".format(df_name, child_name))\n            else:\n                add_df = stringify_col(add_df, child_name)\n                df = stringify_col(df, bottom_name)\n                df = df.merge(add_df[[child_name]],\n                              left_on=[bottom_name],\n                              right_index=True, how=\"left\")\n                self.tables[df_name].df = df\n\n        # merge in one level above\n        if parent_name not in df.columns:\n            # add parent_table if missing\n            if child_table_name not in self.tables:\n                result = self.add_magic_table(child_table_name)[1]\n                if not isinstance(result, MagicDataFrame):\n                    if verbose:\n                        print(\"-W- Couldn't read in {} data\".format(child_table_name))\n                        print(\"-I- Make sure you've provided the correct file name\")\n                    return df\n            # add parent_name to df\n            add_df = self.tables[child_table_name].df\n            # drop duplicate names\n            add_df = add_df.drop_duplicates(subset=child_name)\n            if parent_name not in add_df:\n                if verbose:\n                    print('-W- could not finish propagating names: {} table is missing {} column'.format(child_table_name, parent_name))\n            elif parent_name not in df:\n                if verbose:\n                    print('-W- could not finish propagating names: {} table is missing {} column'.format(df_name, parent_name))\n            else:\n                add_df = stringify_col(add_df, parent_name)\n                df = stringify_col(df, child_name)\n                df = df.merge(add_df[[parent_name]],\n                              left_on=[child_name],\n                              right_index=True, how=\"left\")\n                self.tables[df_name].df = df\n\n        # merge in two levels above\n        if grandparent_name not in df.columns:\n            # add grandparent table if it is missing\n            if parent_table_name not in self.tables:\n                result = self.add_magic_table(parent_table_name)[1]\n                if not isinstance(result, MagicDataFrame):\n                    if verbose:\n                        print(\"-W- Couldn't read in {} data\".format(parent_table_name))\n                        print(\"-I- Make sure you've provided the correct file name\")\n                    return df\n            # add grandparent name to df\n            add_df = self.tables[parent_table_name].df\n            # drop duplicate names\n            add_df = add_df.drop_duplicates(subset=parent_name)\n            if grandparent_name not in add_df.columns:\n                if verbose:\n                    print('-W- could not finish propagating names: {} table is missing {} column'.format(parent_table_name, grandparent_name))\n            elif parent_name not in df.columns:\n                if verbose:\n                    print('-W- could not finish propagating names: {} table is missing {} column'.format(df_name, parent_name))\n            else:\n                add_df = stringify_col(add_df, grandparent_name)\n                df = stringify_col(df, parent_name)\n                df = df.merge(add_df[[grandparent_name]],\n                              left_on=[parent_name],\n                              right_index=True, how=\"left\")\n                df = stringify_col(df, grandparent_name)\n        # update the Contribution\n        self.tables[df_name].df = df\n        return df", "response": "Given a column name and a dataframe of names return the data for col_name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of column names in the source dataframe and a list of columns in the target dataframe propagate them to the target dataframe.", "response": "def propagate_cols(self, col_names, target_df_name, source_df_name,\n                       down=True):\n        \"\"\"\n        Put the data for \"col_name\" from source_df into target_df\n        Used to get \"azimuth\" from sample table into measurements table\n        (for example).\n        Note: if getting data from the sample table, don't include \"sample\"\n        in the col_names list.  It is included automatically.\n        \"\"\"\n        # make sure target table is read in\n        if target_df_name not in self.tables:\n            self.add_magic_table(target_df_name)\n        if target_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(target_df_name))\n            return\n        # make sure source table is read in\n        if source_df_name not in self.tables:\n            self.add_magic_table(source_df_name)\n            print(\"-W- Couldn't read in {} table\".format(source_df_name))\n            return\n        # make sure col_names are all available in source table\n        source_df = self.tables[source_df_name].df\n        if not set(col_names).issubset(source_df.columns):\n            for col in col_names[:]:\n                if col not in source_df.columns:\n                    print(\"-W- Column '{}' isn't in {} table, skipping it\".format(col, source_df_name))\n                    col_names.remove(col)\n        if not col_names:\n            print(\"-W- Invalid or missing column names, could not propagate columns\")\n            return\n        #\n        if down:\n            add_name = source_df_name[:-1]\n            if 'measurements' in self.tables.keys():\n                self.propagate_location_to_measurements()\n            elif 'specimens' in self.tables.keys():\n                self.propagate_location_to_specimens()\n            else:\n                self.propagate_name_down('location', 'sites')\n        else:\n            add_name = target_df_name[:-1]\n\n        # get dataframes for merge\n        target_df = self.tables[target_df_name].df\n        source_df = self.tables[source_df_name].df\n        backup_source_df = source_df.copy()\n        # finesse source_df to make sure it has all the right columns\n        # and no unnecessary duplicates\n        if source_df_name[:-1] not in source_df.columns:\n            source_df[source_df_name[:-1]] = source_df.index\n        source_df = source_df.drop_duplicates(inplace=False, subset=col_names + [source_df_name[:-1]])\n        source_df = source_df.groupby(source_df.index, sort=False).fillna(method='ffill')\n        source_df = source_df.groupby(source_df.index, sort=False).fillna(method='bfill')\n        # if the groupby/fillna operation fails due to pandas bug, do the same by hand:\n        if not len(source_df):\n            new = []\n            grouped = backup_source_df.groupby(backup_source_df.index)\n            for label, group in grouped:\n                new_group = group.fillna(method=\"ffill\")\n                new_group = new_group.fillna(method=\"bfill\")\n                new.append(new_group)\n            source_df = pd.concat(new, sort=True)\n\n        # if the groupby/fillna still doesn't work, we are out of luck\n        if not len(source_df):\n            return target_df\n        # propagate down\n        if down:\n            # do merge\n            target_df[add_name] = target_df[add_name].astype(str)\n            target_df = target_df.merge(source_df[col_names], how='left',\n                                        left_on=add_name, right_index=True,\n                                        suffixes=[\"_target\", \"_source\"])\n        # propagate up\n        else:\n            # do merge\n            col_names.append(add_name)\n            source_df[add_name] = source_df[add_name].astype(str)\n            target_df = target_df.merge(source_df[col_names],\n                                        how='left', left_index=True,\n                                        right_on=add_name,\n                                        suffixes=['_target', '_source'])\n            target_df.index = target_df[add_name]\n            target_df.drop([add_name + \"_source\", add_name + \"_target\"], axis=1, inplace=True)\n\n        # ignore any duplicate rows\n        target_df.drop_duplicates(inplace=True)\n        # mess with target_df to remove un-needed merge columns\n        for col in col_names:\n            # if there has been a previous merge, consolidate and delete data\n            if col + \"_target\" in target_df.columns:\n                # prioritize values from target df\n                new_arr = np.where(target_df[col + \"_target\"],\n                                   target_df[col + \"_target\"],\n                                   target_df[col + \"_source\"])\n                target_df.rename(columns={col + \"_target\": col}, inplace=True)\n                target_df[col] = new_arr\n            if col + \"_source\" in target_df.columns:\n                # delete extra merge column\n                del target_df[col + \"_source\"]\n        #\n\n        # drop any duplicate rows\n        target_df.drop_duplicates(inplace=True)\n        self.tables[target_df_name].df = target_df\n        return target_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef propagate_cols_up(self, cols, target_df_name, source_df_name):\n        print(\"-I- Trying to propagate {} columns from {} table into {} table\".format(cols,\n                                                                                      source_df_name,\n                                                                                      target_df_name))\n        # make sure target table is read in\n        if target_df_name not in self.tables:\n            self.add_magic_table(target_df_name)\n        if target_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(target_df_name))\n            return\n        # make sure source table is read in\n        if source_df_name not in self.tables:\n            self.add_magic_table(source_df_name)\n            print(\"-W- Couldn't read in {} table\".format(source_df_name))\n            return\n        target_df = self.tables[target_df_name]\n        source_df = self.tables[source_df_name]\n        target_name = target_df_name[:-1]\n        # make sure source_df has relevant columns\n        for col in cols:\n            if col not in source_df.df.columns:\n                source_df.df[col] = None\n        # if target_df has info, propagate that into all rows\n        target_df.front_and_backfill(cols)\n        # make sure target_name is in source_df for merging\n        if target_name not in source_df.df.columns:\n            print(\"-W- You can't merge data from {} table into {} table\".format(source_df_name, target_df_name))\n            print(\"    Your {} table is missing {} column\".format(source_df_name, target_name))\n            self.tables[target_df_name] = target_df\n            return target_df\n        source_df.front_and_backfill([target_name])\n        # group source df by target_name\n        grouped = source_df.df.groupby(source_df.df[target_name])\n        if not len(grouped):\n            print(\"-W- Couldn't propagate from {} to {}\".format(source_df_name, target_df_name))\n            return target_df\n        # function to generate capitalized, sorted, colon-delimited list\n        # of unique, non-null values from a column\n        def func(group, col_name):\n            lst = group[col_name][group[col_name].notnull()].unique()\n            split_lst = [col.split(':') for col in lst if col]\n            sorted_lst = sorted(np.unique([item.capitalize() for sublist in split_lst for item in sublist]))\n            group_col = \":\".join(sorted_lst)\n            return group_col\n        # apply func to each column\n        for col in cols:\n            res = grouped.apply(func, col)\n            target_df.df['new_' + col] = res\n            target_df.df[col] = np.where(target_df.df[col], target_df.df[col], target_df.df['new_' + col])\n            target_df.df.drop(['new_' + col], axis='columns', inplace=True)\n        # set table\n        self.tables[target_df_name] = target_df\n        return target_df", "response": "Given a list of columns from source table and target table and a list of columns apply them to the target table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propagate_average_up(self, cols=['lat', 'lon'],\n                             target_df_name='sites', source_df_name='samples'):\n        \"\"\"\n        Propagate average values from a lower table to a higher one.\n        For example, propagate average lats/lons from samples to sites.\n        Pre-existing values will not be overwritten.\n\n        Parameters\n        ----------\n        cols : list-like\n            list of columns to propagate\n        target_df_name : str\n            name of table to propagate values into\n        source_df_name:\n            name of table to propagate values from\n\n        Returns\n        ---------\n        target_df : MagicDataFrame or None\n            returns table with propagated data,\n            or None if no propagation could be done\n        \"\"\"\n        # make sure target/source table are appropriate\n        target_ind = self.ancestry.index(target_df_name)\n        source_ind = self.ancestry.index(source_df_name)\n        if target_ind - source_ind != 1:\n            print('-W- propagate_average_up only works with tables that are spaced one apart, i.e. sites and samples.')\n            print('    Source table must be lower in the hierarchy than the target table.')\n            print('    You have provided \"{}\" as the target table and \"{}\" as the source table.'.format(target_df_name, source_df_name))\n            return None\n        # make sure target table is read in\n        if target_df_name not in self.tables:\n            self.add_magic_table(target_df_name)\n        if target_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(target_df_name))\n            return\n        # make sure source table is read in\n        if source_df_name not in self.tables:\n            self.add_magic_table(source_df_name)\n        if source_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(source_df_name))\n            return\n        # get tables\n        target_df = self.tables[target_df_name]\n        source_df = self.tables[source_df_name]\n        target_name = target_df_name[:-1]\n        # step 1: make sure columns exist in target_df\n        for col in cols:\n            if col not in target_df.df.columns:\n                target_df.df[col] = None\n        # step 2: propagate target_df columns forward & back\n        target_df.front_and_backfill(cols)\n        # step 3: see if any column values are missing\n        values = [not_null(val) for val in target_df.df[cols].values.ravel()]\n        if all(values):\n            print('-I- {} table already has {} filled column(s)'.format(target_df_name, cols))\n            self.tables[target_df_name] = target_df\n            return target_df\n        # step 4: make sure columns are in source table, also target name\n        if target_name not in source_df.df.columns:\n            print(\"-W- can't propagate from {} to {} table\".format(source_df_name, target_df_name))\n            print(\"    Missing {} column in {} table\".format(target_name, source_df_name))\n            self.tables[target_df_name] = target_df\n            return target_df\n        for col in cols:\n            if col not in target_df.df.columns:\n                target_df.df[col] = None\n        # step 5: if needed, average from source table and apply to target table\n        for col in cols:\n            if col not in source_df.df.columns:\n                source_df.df[col] = np.nan\n            else:\n                # make sure is numeric\n                source_df.df[col] = pd.to_numeric(source_df.df[col], errors='coerce')\n        grouped = source_df.df[cols + [target_name]].groupby(target_name)\n        grouped = grouped[cols].apply(np.mean)\n        for col in cols:\n            target_df.df['new_' + col] = grouped[col]\n            # use custom not_null\n            mask = [not_null(val) for val in target_df.df[col]]\n            target_df.df[col] = np.where(mask, #target_df.df[col].notnull(),\n                                         target_df.df[col],\n                                         target_df.df['new_' + col])\n            target_df.df.drop(['new_' + col], inplace=True, axis=1)\n            # round column to 5 decimal points\n            try:\n                target_df.df[col] = target_df.df[col].astype(float)\n                target_df.df = target_df.df.round({col: 5})\n            except ValueError: # if there are sneaky strings...\n                pass\n        self.tables[target_df_name] = target_df\n        return target_df", "response": "Propagate average values from a lower table to a higher one."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npropagates minimum and maximum values into a target table.", "response": "def propagate_min_max_up(self, cols=['age'],\n                             target_df_name='locations',\n                             source_df_name='sites',\n                             min_suffix='low',\n                             max_suffix='high'):\n        \"\"\"\n        Take minimum/maximum values for a set of columns in source_df,\n        and apply them to the target table.\n        This method won't overwrite values in the target table, it will only\n        supply values where they are missing.\n\n        Parameters\n        ----------\n        cols : list-like\n            list of columns to propagate, default ['age']\n        target_df_name : str\n            name of table to propagate values into, default 'locations'\n        source_df_name:\n            name of table to propagate values from, default 'sites'\n        min_suffix : str\n            suffix for minimum value, default 'low'\n        max_suffix : str\n            suffix for maximum value, default 'high'\n\n        Returns\n        ---------\n        target_df : MagicDataFrame\n            updated MagicDataFrame with propagated values\n        \"\"\"\n        # make sure target/source table are appropriate\n        target_ind = self.ancestry.index(target_df_name)\n        source_ind = self.ancestry.index(source_df_name)\n        if target_ind - source_ind != 1:\n            print('-W- propagate_min_max_up only works with tables that are spaced one apart, i.e. sites and samples.')\n            print('    Source table must be lower in the hierarchy than the target table.')\n            print('    You have provided \"{}\" as the target table and \"{}\" as the source table.'.format(target_df_name, source_df_name))\n            return None\n        # make sure target table is read in\n        if target_df_name not in self.tables:\n            self.add_magic_table(target_df_name)\n        if target_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(target_df_name))\n            return\n        # make sure source table is read in\n        if source_df_name not in self.tables:\n            self.add_magic_table(source_df_name)\n        if source_df_name not in self.tables:\n            print(\"-W- Couldn't read in {} table\".format(source_df_name))\n            return\n        # get tables\n        target_df = self.tables[target_df_name]\n        source_df = self.tables[source_df_name]\n        target_name = target_df_name[:-1]\n        # find and propagate min/max for each col in cols\n        for col in cols:\n            if col not in source_df.df.columns:\n                print('-W- {} table is missing \"{}\" column, skipping'.format(source_df_name, col))\n                continue\n            min_col = col + \"_\" + min_suffix\n            max_col = col + \"_\" + max_suffix\n            # add min/max cols to target_df if missing\n            if min_col not in target_df.df.columns:\n                target_df.df[min_col] = None\n            if max_col not in target_df.df.columns:\n                target_df.df[max_col] = None\n            # get min/max from source\n            if target_name not in source_df.df.columns:\n                print('-W- {} table missing {} column, cannot propagate age info'.format(target_name, source_df_name))\n                return\n            # make sure source is appropriately filled\n            source = source_df.front_and_backfill([col], inplace=False)\n            # add target_name back into front/backfilled source\n            source[target_name] = source_df.df[target_name]\n            grouped = source[[col, target_name]].groupby(target_name)\n            if len(grouped):\n                minimum, maximum = grouped.min(), grouped.max()\n                minimum = minimum.reindex(target_df.df.index)\n                maximum = maximum.reindex(target_df.df.index)\n                # update target_df without overwriting existing values\n                cond_min = target_df.df[min_col].apply(not_null)\n                cond_max = target_df.df[max_col].apply(not_null)\n                #\n                target_df.df[min_col] = np.where(cond_min,\n                                                 target_df.df[min_col],\n                                                 minimum[col])\n                target_df.df[max_col] = np.where(cond_max,\n                                                 target_df.df[max_col],\n                                                 maximum[col])\n        # update contribution\n        self.tables[target_df_name] = target_df\n        return target_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npropagates values from any age data into a specific age table.", "response": "def propagate_ages(self):\n        \"\"\"\n        Mine ages table for any age data, and write it into\n        specimens, samples, sites, locations tables.\n        Do not overwrite existing age data.\n        \"\"\"\n        # if there is no age table, skip\n        if 'ages' not in self.tables:\n            return\n        # if age table has no data, skip\n        if not len(self.tables['ages'].df):\n            return\n        # get levels in age table\n        self.get_age_levels()\n        # if age levels could not be determined, skip\n        if not \"level\" in self.tables[\"ages\"].df.columns:\n            return\n        if not any(self.tables[\"ages\"].df[\"level\"]):\n            return\n        # go through each level of age data\n        for level in self.tables['ages'].df['level'].unique():\n            table_name = level + 's'\n            age_headers = self.data_model.get_group_headers(table_name, 'Age')\n            # find age headers that are actually in table\n            actual_age_headers = list(set(self.tables[table_name].df.columns).intersection(age_headers))\n            # find site age headers that are available in ages table\n            available_age_headers = list(set(self.tables['ages'].df.columns).intersection(age_headers))\n            # fill in all available age info to all rows\n            self.tables[table_name].front_and_backfill(actual_age_headers)\n            # add any available headers to table\n            add_headers = set(available_age_headers).difference(actual_age_headers)\n            for header in add_headers:\n                self.tables[table_name].df[header] = None\n            # propagate values from ages into table\n            def move_values(ser, level, available_headers):\n                name = ser.name\n                cond1 = self.tables['ages'].df[level] == name\n                cond2 = self.tables['ages'].df['level'] == level\n                mask = cond1 & cond2\n                sli = self.tables['ages'].df[mask]\n                if len(sli):\n                    return list(sli[available_headers].values[0])\n                return [None] * len(available_headers)\n\n            res = self.tables[table_name].df.apply(move_values, axis=1,\n                                                   args=[level, available_age_headers])\n            # fill in table with values gleaned from ages\n            new_df = pd.DataFrame(data=list(res.values), index=res.index,\n                                  columns=available_age_headers)\n            age_values = np.where(self.tables[table_name].df[available_age_headers],\n                                  self.tables[table_name].df[available_age_headers],\n                                  new_df)\n            self.tables[table_name].df[available_age_headers] = age_values\n        #\n        # put age_high, age_low into locations table\n        print(\"-I- Adding age_high and age_low to locations table based on minimum/maximum ages found in sites table\")\n        self.propagate_min_max_up(cols=['age'], target_df_name='locations',\n                                  source_df_name='sites')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves all non - MagIC columns from all tables.", "response": "def remove_non_magic_cols(self):\n        \"\"\"\n        Remove all non-MagIC columns from all tables.\n        \"\"\"\n        for table_name in self.tables:\n            table = self.tables[table_name]\n            table.remove_non_magic_cols_from_table()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_table_to_file(self, dtype, custom_name=None, append=False, dir_path=None):\n        if custom_name:\n            fname = custom_name\n        else:\n            fname = self.filenames[dtype]\n        if not dir_path:\n            dir_path=self.directory\n        if dtype in self.tables:\n            write_df = self.remove_names(dtype)\n            outfile = self.tables[dtype].write_magic_file(custom_name=fname,\n                                                          dir_path=dir_path,\n                                                          append=append, df=write_df)\n        return outfile", "response": "Writes out a MagIC table to file using custom filename\n            as specified in self. filenames."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving unneeded name columns from the specified table.", "response": "def remove_names(self, dtype):\n        \"\"\"\n        Remove unneeded name columns ('specimen'/'sample'/etc)\n        from the specified table.\n\n        Parameters\n        ----------\n        dtype : str\n\n        Returns\n        ---------\n        pandas DataFrame without the unneeded columns\n\n        Example\n        ---------\n        Contribution.tables['specimens'].df = Contribution.remove_names('specimens')\n        # takes out 'location', 'site', and/or 'sample' columns from the\n        # specimens dataframe if those columns have been added\n        \"\"\"\n        if dtype not in self.ancestry:\n            return\n        if dtype in self.tables:\n            # remove extra columns here\n            self_ind = self.ancestry.index(dtype)\n            parent_ind = self_ind + 1 if self_ind < (len(self.ancestry) -1) else self_ind\n            remove = set(self.ancestry).difference([self.ancestry[self_ind], self.ancestry[parent_ind]])\n            remove = [dtype[:-1] for dtype in remove]\n            columns = self.tables[dtype].df.columns.difference(remove)\n            return self.tables[dtype].df[columns]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding any items that are referenced in a child table but are missing in their own table.", "response": "def find_missing_items(self, dtype):\n        \"\"\"\n        Find any items that are referenced in a child table\n        but are missing in their own table.\n        For example, a site that is listed in the samples table,\n        but has no entry in the sites table.\n\n        Parameters\n        ----------\n        dtype : str\n            table name, e.g. 'specimens'\n\n        Returns\n        ---------\n        set of missing values\n        \"\"\"\n        parent_dtype, child_dtype = self.get_parent_and_child(dtype)\n        if not child_dtype in self.tables:\n            return set()\n        items = set(self.tables[dtype].df.index.unique())\n        items_in_child_table = set(self.tables[child_dtype].df[dtype[:-1]].unique())\n        return {i for i in (items_in_child_table - items) if not_null(i)}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn contribution id if available", "response": "def get_con_id(self):\n        \"\"\"\n        Return contribution id if available\n        \"\"\"\n        con_id = \"\"\n        if \"contribution\" in self.tables:\n            if \"id\" in self.tables[\"contribution\"].df.columns:\n                con_id = str(self.tables[\"contribution\"].df[\"id\"].values[0])\n        return con_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_to_str(self):\n        def stringify(x):\n            # float --> string,\n            # truncating floats like 3.0 --> 3\n            if isinstance(x, float):\n                if x.is_integer():\n                    #print('{} --> {}'.format(x, str(x).rstrip('0').rstrip('.')))\n                    return str(x).rstrip('0').rstrip('.')\n                return(str(x))\n            # keep strings as they are,\n            # unless it is a string like \"3.0\",\n            # in which case truncate that too\n            if isinstance(x, str):\n                try:\n                    float(x)\n                    if x.endswith('0'):\n                        if x.rstrip('0').endswith('.'):\n                            #print('{} --> {}'.format(x, x.rstrip('0').rstrip('.')))\n                            return x.rstrip('0').rstrip('.')\n                except (ValueError, TypeError):\n                    pass\n            # integer --> string\n            if isinstance(x, int):\n                return str(x)\n            # if it is not int/str/float, just return as is\n            return x\n\n        def remove_extra_digits(x, prog):\n            \"\"\"\n            Remove extra digits\n            x is a string,\n            prog is always the following '_sre.SRE_Pattern':\n            prog = re.compile(\"\\d*[.]\\d*([0]{5,100}|[9]{5,100})\\d*\\Z\").\n            However, it is compiled outside of this sub-function\n            for performance reasons.\n            \"\"\"\n            if not isinstance(x, str):\n                return x\n            result = prog.match(x)\n            if result:\n                decimals = result.string.split('.')[1]\n                result = result.string\n                if decimals[-3] == '0':\n                    result = x[:-2].rstrip('0')\n                if decimals[-3] == '9':\n                    result = x[:-2].rstrip('9')\n                    try:\n                        last_digit = int(result[-1])\n                        result = result[:-1] + str(last_digit + 1)\n                    except ValueError:\n                        result = float(result[:-1]) + 1\n                #if result != x:\n                #    print('changing {} to {}'.format(x, result))\n                return result\n            return x\n\n        for col in self.df.columns:\n            self.df[col] = self.df[col].apply(stringify)\n\n        prog = re.compile(\"\\d*[.]\\d*([0]{5,100}|[9]{5,100})\\d*\\Z\")\n        for col in self.df.columns:\n            self.df[col] = self.df[col].apply(lambda x: remove_extra_digits(x, prog))", "response": "Convert all the columns of the log file into a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_non_magic_cols_from_table(self, ignore_cols=()):\n        unrecognized_cols = self.get_non_magic_cols()\n        for col in ignore_cols:\n            if col in unrecognized_cols:\n                unrecognized_cols.remove(col)\n        if unrecognized_cols:\n            print('-I- Removing non-MagIC column names from {}:'.format(self.dtype), end=' ')\n            for col in unrecognized_cols:\n                self.df.drop(col, axis='columns', inplace=True)\n                print(col, end=' ')\n            print(\"\\n\")\n        return unrecognized_cols", "response": "Removes all non - MagIC columns from the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_row(self, ind, row_data):\n        if sorted(row_data.keys()) != sorted(self.df.columns):\n            # add any new column names\n            for key in row_data:\n                if key not in self.df.columns:\n                    self.df[key] = None\n            # add missing column names into row_data\n            for col_label in self.df.columns:\n                if col_label not in list(row_data.keys()):\n                    row_data[col_label] = None\n        try:\n            self.df.iloc[ind] = pd.Series(row_data)\n        except IndexError:\n            return False\n        return self.df", "response": "Update a row with data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a row with data.", "response": "def add_row(self, label, row_data, columns=\"\"):\n        \"\"\"\n        Add a row with data.\n        If any new keys are present in row_data dictionary,\n        that column will be added to the dataframe.\n        This is done inplace\n        \"\"\"\n        # use provided column order, making sure you don't lose any values\n        # from self.df.columns\n        if len(columns):\n            if sorted(self.df.columns) == sorted(columns):\n                self.df.columns = columns\n            else:\n                new_columns = []\n                new_columns.extend(columns)\n                for col in self.df.columns:\n                    if col not in new_columns:\n                        new_columns.append(col)\n        # makes sure all columns have data or None\n        if sorted(row_data.keys()) != sorted(self.df.columns):\n            # add any new column names\n            for key in row_data:\n                if key not in self.df.columns:\n                    self.df[key] = None\n            # add missing column names into row_data\n            for col_label in self.df.columns:\n                if col_label not in list(row_data.keys()):\n                    row_data[col_label] = None\n\n        # (make sure you are working with strings)\n        self.df.index = self.df.index.astype(str)\n        label = str(label)\n\n        # create a new row with suffix \"new\"\n        # (this ensures that you get a unique, new row,\n        #  instead of adding on to an existing row with the same label)\n        self.df.loc[label + \"new\"] = pd.Series(row_data)\n        # rename it to be correct\n        self.df.rename(index={label + \"new\": label}, inplace=True)\n        # use next line to sort index inplace\n        #self.df.sort_index(inplace=True)\n        return self.df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding df to a MagicDataFrame using a data list.", "response": "def add_data(self, data):  # add append option later\n        \"\"\"\n        Add df to a MagicDataFrame using a data list.\n\n        Parameters\n        ----------\n        data : list of dicts\n            data list with format [{'key1': 'val1', ...}, {'key1': 'val2', ...}, ... }]\n        dtype : str\n            MagIC table type\n        \"\"\"\n        df = pd.DataFrame(data)\n        name, dtype = self.get_singular_and_plural_dtype(self.dtype)\n        if name in df.columns:\n            df.index = df[name]\n        df.index.name = name + \" name\"\n        self.df = df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_blank_row(self, label):\n        col_labels = self.df.columns\n        blank_item = pd.Series({}, index=col_labels, name=label)\n        # use .loc to add in place (append won't do that)\n        self.df.loc[blank_item.name] = blank_item\n        return self.df", "response": "Add a blank row with only an index value to self. df.\n        This is done inplace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the row at ind", "response": "def delete_row(self, ind):\n        \"\"\"\n        remove self.df row at ind\n        inplace\n        \"\"\"\n        self.df = pd.concat([self.df[:ind], self.df[ind+1:]], sort=True)\n        return self.df"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes all rows that meet the condition and return a new dataframe with the new rows deleted", "response": "def delete_rows(self, condition, info_str=None):\n        \"\"\"\n        delete all rows with  condition==True\n        inplace\n\n        Parameters\n        ----------\n        condition : pandas DataFrame indexer\n            all self.df rows that meet this condition will be deleted\n        info_str : str\n            description of the kind of rows to be deleted,\n            e.g \"specimen rows with blank method codes\"\n\n        Returns\n        --------\n        df_data : pandas DataFrame\n            updated self.df\n        \"\"\"\n        self.df['num'] = list(range(len(self.df)))\n        df_data = self.df\n        # delete all records that meet condition\n        if len(df_data[condition]) > 0:  #we have one or more records to delete\n            inds = df_data[condition]['num'] # list of all rows where condition is TRUE\n            for ind in inds[::-1]:\n                df_data = self.delete_row(ind)\n                if info_str:\n                    print(\"-I- Deleting {}. \".format(info_str), end=' ')\n                    print('deleting row {}'.format(str(ind)))\n        # sort so that all rows for an item are together\n        df_data.sort_index(inplace=True)\n        # redo temporary index\n        df_data['num'] = list(range(len(df_data)))\n        self.df = df_data\n        return df_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndrops rows that have only null values ignoring certain columns.", "response": "def drop_stub_rows(self, ignore_cols=('specimen',\n                                          'sample',\n                                          'software_packages',\n                                          'num')):\n        \"\"\"\n        Drop self.df rows that have only null values,\n        ignoring certain columns.\n\n        Parameters\n        ----------\n        ignore_cols : list-like\n            list of column names to ignore for\n\n        Returns\n        ---------\n        self.df : pandas DataFrame\n        \"\"\"\n        # ignore citations if they just say 'This study'\n        if 'citations' in self.df.columns:\n            if list(self.df['citations'].unique()) == ['This study']:\n                ignore_cols = ignore_cols + ('citations',)\n        drop_cols = self.df.columns.difference(ignore_cols)\n        self.df.dropna(axis='index', subset=drop_cols, how='all', inplace=True)\n        return self.df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drop_duplicate_rows(self, ignore_cols=['specimen', 'sample']):\n        # keep any row with a unique index\n        unique_index = self.df.index.unique()\n        cond1 = ~self.df.index.duplicated(keep=False)\n        # or with actual data\n        ignore_cols = [col for col in ignore_cols if col in self.df.columns]\n        relevant_df = self.df.drop(ignore_cols, axis=1)\n        cond2 = relevant_df.notnull().any(axis=1)\n        orig_len = len(self.df)\n        new_df = self.df[cond1 | cond2]\n        # make sure we haven't lost anything important\n        if any(unique_index.difference(new_df.index.unique())):\n                cond1 = ~self.df.index.duplicated(keep=\"first\")\n        self.df = self.df[cond1 | cond2]\n        end_len = len(self.df)\n        removed = orig_len - end_len\n        if removed:\n            print('-I- Removed {} redundant records from {} table'.format(removed, self.dtype))\n        return self.df", "response": "Drop duplicate rows from self. df."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_record(self, name, new_data, condition, update_only=False,\n                      debug=False):\n        \"\"\"\n        Find the first row in self.df with index == name\n        and condition == True.\n        Update that record with new_data, then delete any\n        additional records where index == name and condition == True.\n        Change is inplace\n        \"\"\"\n        # add numeric index column temporarily\n        self.df['num'] = list(range(len(self.df)))\n        df_data = self.df\n        condition2 = (df_data.index == name)\n        # edit first of existing data that meets condition\n        if len(df_data[condition & condition2]) > 0:  #we have one or more records to update or delete\n            # list of all rows where condition is true and index == name\n            inds = df_data[condition & condition2]['num']\n            #inds = df_data[condition]['num'] # list of all rows where condition is true\n            existing_data = dict(df_data.iloc[inds.iloc[0]]) # get first record of existing_data from dataframe\n            existing_data.update(new_data) # update existing data with new interpretations\n            # update row\n            self.update_row(inds.iloc[0], existing_data)\n            # now remove all the remaining records of same condition\n            if len(inds) > 1:\n                for ind in inds[1:]:\n                    print(\"deleting redundant records for:\", name)\n                    df_data = self.delete_row(ind)\n        else:\n            if update_only:\n                print(\"no record found for that condition, not updating \", name)\n            else:\n                print('no record found - creating new one for ', name)\n                # add new row\n                df_data = self.add_row(name, new_data)\n        # sort so that all rows for an item are together\n        df_data.sort_index(inplace=True)\n        # redo temporary index\n        df_data['num'] = list(range(len(df_data)))\n        self.df = df_data\n        return df_data", "response": "Update a record in the table with new data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a MagicDataFrame with the selected column names replaced null values in selected columns with front and backfilled values if available.", "response": "def front_and_backfill(self, cols, inplace=True):\n        \"\"\"\n        Groups dataframe by index name then replaces null values in selected\n        columns with front/backfilled values if available.\n        Changes self.df inplace.\n\n        Parameters\n        ----------\n        self : MagicDataFrame\n        cols : array-like\n            list of column names\n\n        Returns\n        ---------\n        self.df\n        \"\"\"\n        cols = list(cols)\n        for col in cols:\n            if col not in self.df.columns: self.df[col] = np.nan\n        short_df = self.df[cols]\n        # horrible, bizarre hack to test for pandas malfunction\n        tester = short_df.groupby(short_df.index, sort=False).fillna(method='ffill')\n        if not_null(tester):\n            short_df = short_df.groupby(short_df.index, sort=False).fillna(method='ffill').groupby(short_df.index, sort=False).fillna(method='bfill')\n        else:\n            print('-W- Was not able to front/back fill table {} with these columns: {}'.format(self.dtype, ', '.join(cols)))\n        if inplace:\n            self.df[cols] = short_df[cols]\n            return self.df\n        return short_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_filled_col(self, col_list):\n        for col in col_list:\n            if col in self.df.columns:\n                if not all([is_null(val, False) for val in self.df[col]]):\n                    return col", "response": "Returns the first col_name from the list that is both\nTracingEnabled and\nCOOKIEID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_to_pmag_data_list(self, lst_or_dict=\"lst\", df=None):\n\n        \"\"\"\n        Take MagicDataFrame and turn it into a list of dictionaries.\n        This will have the same format as reading in a 2.5 file\n        with pmag.magic_read(), i.e.:\n        if \"lst\":\n          [{\"sample\": \"samp_name\", \"azimuth\": 12, ...}, {...}]\n        if \"dict\":\n          {\"samp_name\": {\"azimuth\": 12, ...}, \"samp_name2\": {...}, ...}\n        NOTE: \"dict\" not recommended with 3.0, as one sample can have\n        many rows, which means that dictionary items can be overwritten\n        \"\"\"\n        if isinstance(df, type(None)):\n            df = self.df\n        # replace np.nan / None with \"\"\n        df = df.where(df.notnull(), \"\")\n        # string-i-fy everything\n        df = df.astype(str)\n\n        if lst_or_dict == \"lst\":\n            return list(df.T.apply(dict))\n        else:\n            return {str(i[df.index.name.split(' ')[0]]): dict(i) for i in list(df.T.apply(dict))}", "response": "Take MagicDataFrame and turn it into a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the name of the given column in a DataFrame or a list of index_names to slice self. df.", "response": "def get_name(self, col_name, df_slice=\"\", index_names=\"\"):\n        \"\"\"\n        Takes in a column name, and either a DataFrame slice or\n        a list of index_names to slice self.df using fancy indexing.\n        Then return the value for that column in the relevant slice.\n        (Assumes that all values for column will be the same in the\n         chosen slice, so return the first one.)\n        \"\"\"\n        # if slice is provided, use it\n        if any(df_slice):\n            df_slice = df_slice\n        # if given index_names, grab a slice using fancy indexing\n        elif index_names:\n            df_slice = self.df.loc[index_names]\n        # otherwise, use the full DataFrame\n        else:\n            df_slice = self.df\n        # if the slice is empty, return \"\"\n        if len(df_slice) == 0:\n            return \"\"\n        # if the column name isn't present in the slice, return \"\"\n        if col_name not in df_slice.columns:\n            return \"\"\n        # otherwise, return the first value from that column\n        first_val = list(df_slice[col_name].dropna())\n        if any(first_val):\n            return first_val[0]\n        else:\n            return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_di_block(self, df_slice=None, do_index=False,\n                     item_names=None, tilt_corr='100',\n                     excl=None, ignore_tilt=False):\n        \"\"\"\n        Input either a DataFrame slice\n        or\n        do_index=True and a list of index_names.\n        Optional arguments:\n        Provide tilt_corr (default 100).\n        Excl is a list of method codes to exclude.\n        Output dec/inc from the slice in this format:\n        [[dec1, inc1], [dec2, inc2], ...].\n        Not inplace\n        \"\"\"\n        tilt_corr = int(tilt_corr)\n        if isinstance(df_slice, str):\n            if df_slice.lower() == \"all\":\n                # use entire DataFrame\n                df_slice = self.df\n        elif do_index:\n            # use fancy indexing (but note this will give duplicates)\n            df_slice = self.df.loc[item_names]\n        elif not do_index:\n            # otherwise use the provided slice\n            df_slice = df_slice\n\n        # once you have the slice, fix up the data\n        # tilt correction must match\n        if not ignore_tilt:\n            if tilt_corr != 0:\n                df_slice = df_slice[df_slice['dir_tilt_correction'] == tilt_corr]\n            else:\n                # if geographic (\"0\"),\n                # use records with no tilt_corr and assume geographic\n                cond1 = df_slice['dir_tilt_correction'] == None\n                cond2 = df_slice['dir_tilt_correction'] == tilt_corr\n                df_slice = df_slice[cond1 | cond2]\n        # exclude data with unwanted codes\n        if excl:\n            for ex in excl:\n                df_slice = self.get_records_for_code(ex, incl=False,\n                                                     use_slice=True,\n                                                     sli=df_slice)\n\n        df_slice = df_slice[df_slice['dir_inc'].notnull() & df_slice['dir_dec'].notnull()]\n        # possible add in:\n        # split out di_block from this study from di_block from other studies (in citations column)\n        # previously just used \"This study\", but it is no longer required\n        #if 'citations' in df_slice.columns:\n        #    df_slice = df_slice[df_slice['citations'].str.contains(\"This study\")]\n\n        # convert values into DIblock format\n        di_block = [[float(row['dir_dec']), float(row['dir_inc'])] for ind, row in df_slice.iterrows()]\n        return di_block", "response": "Get the di_block from the slice or the entire DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_records_for_code(self, meth_code, incl=True, use_slice=False,\n                             sli=None, strict_match=True):\n        \"\"\"\n        Use regex to see if meth_code is in the method_codes \":\" delimited list.\n        If incl == True, return all records WITH meth_code.\n        If incl == False, return all records WITHOUT meth_code.\n        If strict_match == True, return only records with the exact meth_code.\n        If strict_match == False, return records that contain the meth_code partial string,\n        (i.e., \"DE-\").\n        Not inplace\n        \"\"\"\n        # (must use fillna to replace np.nan with False for indexing)\n        if use_slice:\n            df = sli.copy()\n        else:\n            df = self.df.copy()\n        # if meth_code not provided, return unchanged dataframe\n        if not meth_code:\n            return df\n        # get regex\n        if not strict_match:\n            # grab any record that contains any part of meth_code\n            cond = df['method_codes'].str.contains(meth_code).fillna(False)\n        else:\n            # grab only an exact match\n            pattern = re.compile('{}(?=:|\\s|\\Z)'.format(meth_code))\n            cond = df['method_codes'].str.contains(pattern).fillna(False)\n        if incl:\n            # return a copy of records with that method code:\n            return df[cond]\n        else:\n            # return a copy of records without that method code\n            return df[~cond]", "response": "Get the records for a given method code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_dfs(self, df1):\n\n        if self.df.empty:\n            return df1\n        elif df1.empty:\n            return self.df\n\n        #copy to prevent mutation\n        cdf2 = self.df.copy()\n\n        #split data into types and decide which to replace\n#        if replace_dir_or_int == 'dir' and 'method_codes' in cdf2.columns:\n#            cdf2 = cdf2[cdf2['method_codes'].notnull()]\n#            acdf2 = cdf2[cdf2['method_codes'].str.contains('LP-PI')]\n#            mcdf2 = cdf2[cdf2['method_codes'].str.contains('LP-DIR')]\n#        elif replace_dir_or_int == 'int' and 'method_codes' in cdf2.columns:\n#            cdf2 = cdf2[cdf2['method_codes'].notnull()]\n#            mcdf2 = cdf2[cdf2['method_codes'].str.contains('LP-PI')]\n#            acdf2 = cdf2[cdf2['method_codes'].str.contains('LP-DIR')]\n#        else:\n#            mcdf2 = cdf2\n#            acdf2 = pd.DataFrame(columns=mcdf2.columns)\n\n        #get rid of stupid duplicates\n#        [mcdf2.drop(cx,inplace=True,axis=1) for cx in mcdf2.columns if cx in df1.columns]\n\n        #join the new calculated data with the old data of same type\n        if self.dtype.endswith('s'): dtype = self.dtype[:-1]\n        else: dtype = self.dtype\n        index_name = dtype + \"_name\"\n        for df in [df1, cdf2]:\n            df.index.name = index_name\n        mdf = df1.join(cdf2, how='outer', rsuffix='_remove', on=index_name)\n        def keep_non_null_vals(column):\n            extra_column = column + \"_remove\"\n            if column in mdf.columns and extra_column in mdf.columns:\n                mdf[column] = np.where(mdf[column].apply(lambda x: not_null(x, False)), mdf[column], mdf[extra_column])\n        # merge values in the following columns\n        # e.g., combine info from specimen + specimen_remove into specimen column\n        for col in ['specimen', 'sample', 'site', 'location', 'lat', 'lon']:\n            keep_non_null_vals(col)\n        #drop duplicate columns if they were created\n        [mdf.drop(col,inplace=True,axis=1) for col in mdf.columns if col.endswith(\"_remove\")]\n        #duplicates rows for some freaking reason\n        mdf.drop_duplicates(inplace=True,subset=[col for col in mdf.columns if col != 'description'])\n        #merge the data of the other type with the new data\n#        mdf = mdf.merge(acdf2, how='outer')\n        if dtype in mdf.columns:\n            #fix freaking indecies because pandas\n            mdf = mdf.set_index(dtype)\n            #really? I wanted the index changed not a column deleted?!?\n            mdf[dtype] = mdf.index\n            mdf.index.name = index_name\n            mdf.sort_index(inplace=True)\n        return mdf", "response": "This method takes new calculated data and replaces the corresponding data in self. df with the new input data preserving most important metadata if they are not already saved."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_magic_file(self, custom_name=None, dir_path=\".\",\n                         append=False, multi_type=False, df=None):\n        \"\"\"\n        Write self.df out to tab-delimited file.\n        By default will use standard MagIC filenames (specimens.txt, etc.),\n        or you can provide a custom_name to write to instead.\n        By default will write to custom_name if custom_name is a full path,\n        or will write to dir_path + custom_name if custom_name\n        is not a full path.\n\n        Parameters\n        ----------\n        self : MagIC DataFrame\n        custom_name : str\n            custom file name\n        dir_path : str\n            dir_path (used if custom_name is not a full path), default \".\"\n        append : bool\n            append to existing file, default False\n        multi_type : bool\n            for creating upload file\n\n        Return\n        --------\n        fname : str\n            output file name\n        \"\"\"\n        # don't let custom name start with \"./\"\n        if custom_name:\n            if custom_name.startswith('.'):\n                custom_name = os.path.split(custom_name)[1]\n        # put columns in logical order (by group)\n        self.sort_dataframe_cols()\n        # if indexing column was put in, remove it\n        if \"num\" in self.df.columns:\n            self.df = self.df.drop(\"num\", axis=1)\n        #\n        # make sure name is a string\n        name = self.get_singular_and_plural_dtype(self.dtype)[0]\n        if name in self.df.columns:\n            self.df[name] = self.df[name].astype(str)\n        #\n        if df is None:\n            df = self.df\n        # get full file path\n        dir_path = os.path.realpath(dir_path)\n        if custom_name:\n            fname = pmag.resolve_file_name(custom_name, dir_path) # os.path.join(dir_path, custom_name)\n        elif self.magic_file:\n            fname = pmag.resolve_file_name(self.magic_file, dir_path)\n        else:\n            fname = os.path.join(dir_path, self.dtype + \".txt\")\n        # see if there's any data\n        if not len(df):\n            print('-W- No data to write to {}'.format(fname))\n            return False\n        # add to existing file\n        if append:\n            print('-I- appending {} data to {}'.format(self.dtype, fname))\n            mode = \"a\"\n        # overwrite existing file\n        elif os.path.exists(fname):\n            print('-I- overwriting {}'.format(fname))\n            mode = \"w\"\n        # or create new file\n        else:\n            print('-I- writing {} records to {}'.format(self.dtype, fname))\n            mode = \"w\"\n        f = open(fname, mode)\n        if append:\n            header = False\n            if multi_type:\n                header = True\n                f.write('tab\\t{}\\n'.format(self.dtype))\n            f.flush()\n            df.to_csv(f, sep=\"\\t\", header=header, index=False, mode='a')\n        else:\n            f.write('tab\\t{}\\n'.format(self.dtype))\n            f.flush()\n            df.to_csv(f, sep=\"\\t\", header=True, index=False, mode='a')\n        print('-I- {} records written to {} file'.format(len(df), self.dtype))\n        f.close()\n        return fname", "response": "Write out to a MagIC file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind all columns that are not real MagIC 3 columns.", "response": "def get_non_magic_cols(self):\n        \"\"\"\n        Find all columns in self.df that are not real MagIC 3 columns.\n\n        Returns\n        --------\n        unrecognized_cols : list\n        \"\"\"\n        table_dm = self.data_model.dm[self.dtype]\n        approved_cols = table_dm.index\n        unrecognized_cols = (set(self.df.columns) - set(approved_cols))\n        return unrecognized_cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the first non - null value in a given index and column.", "response": "def get_first_non_null_value(self, ind_name, col_name):\n        \"\"\"\n        For a given index and column, find the first non-null value.\n\n        Parameters\n        ----------\n        self : MagicDataFrame\n        ind_name : str\n            index name for indexing\n        col_name : str\n            column name for indexing\n\n        Returns\n        ---------\n        single value of str, float, or int\n        \"\"\"\n        short_df = self.df.loc[ind_name, col_name]\n        mask = pd.notnull(short_df)\n        print(short_df[mask])\n        try:\n            val = short_df[mask].unique()[0]\n        except IndexError:\n            val = None\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_singular_and_plural_dtype(self, dtype):\n        dtype = dtype.strip()\n        if dtype.endswith('s'):\n            return dtype[:-1], dtype\n        elif dtype == 'criteria':\n            return 'table_column', 'criteria'\n        elif dtype == 'contribution':\n            return 'doi', 'contribution'", "response": "Returns singular and plural dtype for a specific MagIC table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames is the main function of the chi_magic. py script.", "response": "def main():\n    \"\"\"\n    NAME\n        chi_magic.py\n\n    DESCRIPTION\n        plots magnetic susceptibility as a function of frequency and temperature and AC field\n\n    SYNTAX\n        chi_magic.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -i allows interactive setting of FILE and temperature step\n        -f FILE, specify magic_measurements format file\n        -T IND, specify temperature step to plot\n        -e EXP, specify experiment name to plot\n        -fmt [svg,jpg,png,pdf] set figure format [default is svg]\n        -sav save figure and quit\n\n    DEFAULTS\n         FILE: magic_measurements.txt\n         IND: first\n         SPEC: step through one by one\n    \"\"\"\n    cont, FTinit, BTinit, k = \"\", 0, 0, 0\n    meas_file = \"magic_measurements.txt\"\n    spec = \"\"\n    Tind, cont = 0, \"\"\n    EXP = \"\"\n    fmt = 'svg'  # default image type for saving\n    plot = 0\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-i' in sys.argv:\n        fname = input(\n            \"Input magic_measurements file name? [magic_measurements.txt]  \")\n        if fname != \"\":\n            meas_file = fname\n    if '-e' in sys.argv:\n        ind = sys.argv.index('-e')\n        EXP = sys.argv[ind+1]\n    if '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        meas_file = sys.argv[ind+1]\n    if '-T' in sys.argv:\n        ind = sys.argv.index('-T')\n        Tind = int(sys.argv[ind+1])\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt = sys.argv[ind+1]\n    if '-sav' in sys.argv:\n        plot = 1\n    #\n    meas_data, file_type = pmag.magic_read(meas_file)\n    #\n    # get list of unique experiment names\n    #\n    # initialize some variables (a continuation flag, plot initialization flags and the experiment counter\n    experiment_names = []\n    for rec in meas_data:\n        if rec['magic_experiment_name'] not in experiment_names:\n            experiment_names.append(rec['magic_experiment_name'])\n    #\n    # hunt through by experiment name\n    if EXP != \"\":\n        try:\n            k = experiment_names.index(EXP)\n        except:\n            print(\"Bad experiment name\")\n            sys.exit()\n    while k < len(experiment_names):\n        e = experiment_names[k]\n        if EXP == \"\":\n            print(e, k+1, 'out of ', len(experiment_names))\n    #\n    #  initialize lists of data, susceptibility, temperature, frequency and field\n        X, T, F, B = [], [], [], []\n        for rec in meas_data:\n            methcodes = rec['magic_method_codes']\n            meths = methcodes.strip().split(':')\n            if rec['magic_experiment_name'] == e and \"LP-X\" in meths:  # looking for chi measurement\n                if 'measurement_temp' not in list(rec.keys()):\n                    rec['measurement_temp'] = '300'  # set defaults\n                if 'measurement_freq' not in list(rec.keys()):\n                    rec['measurement_freq'] = '0'  # set defaults\n                if 'measurement_lab_field_ac' not in list(rec.keys()):\n                    rec['measurement_lab_field_ac'] = '0'  # set default\n                if 'measurement_x' in rec.keys():\n                    # backward compatibility\n                    X.append(float(rec['measurement_x']))\n                else:\n                    # data model 2.5\n                    X.append(float(rec['measurement_chi_volume']))\n                T.append(float(rec['measurement_temp']))\n                F.append(float(rec['measurement_freq']))\n                B.append(float(rec['measurement_lab_field_ac']))\n    #\n    # get unique list of Ts,Fs, and Bs\n    #\n        Ts, Fs, Bs = [], [], []\n        for k in range(len(X)):   # hunt through all the measurements\n            if T[k] not in Ts:\n                Ts.append(T[k])  # append if not in list\n            if F[k] not in Fs:\n                Fs.append(F[k])\n            if B[k] not in Bs:\n                Bs.append(B[k])\n        Ts.sort()  # sort list of temperatures, frequencies and fields\n        Fs.sort()\n        Bs.sort()\n        if '-x' in sys.argv:\n            k = len(experiment_names)+1  # just plot the one\n        else:\n            k += 1  # increment experiment number\n    #\n    # plot chi versus T and F holding B constant\n    #\n        plotnum = 1  # initialize plot number to 1\n        if len(X) > 2:  # if there are any data to plot, continue\n            b = Bs[-1]  # keeping field constant and at maximum\n            XTF = []  # initialize list of chi versus Temp and freq\n            for f in Fs:   # step through frequencies sequentially\n                XT = []  # initialize list of chi versus temp\n                for kk in range(len(X)):  # hunt through all the data\n                    if F[kk] == f and B[kk] == b:  # select data with given freq and field\n                        XT.append([X[kk], T[kk]])  # append to list\n                XTF.append(XT)  # append list to list of frequencies\n            if len(XT) > 1:  # if there are any temperature dependent data\n                pmagplotlib.plot_init(plotnum, 5, 5)  # initialize plot\n                # call the plotting function\n                pmagplotlib.plot_xtf(plotnum, XTF, Fs, e, b)\n                if plot == 0:\n                    pmagplotlib.draw_figs({'fig': plotnum})  # make it visible\n                plotnum += 1  # increment plot number\n            f = Fs[0]  # set frequency to minimum\n            XTB = []  # initialize list if chi versus Temp and field\n            for b in Bs:  # step through field values\n                XT = []  # initial chi versus temp list for this field\n                for kk in range(len(X)):  # hunt through all the data\n                    if F[kk] == f and B[kk] == b:  # select data with given freq and field\n                        XT.append([X[kk], T[kk]])  # append to list\n                XTB.append(XT)\n            if len(XT) > 1:  # if there are any temperature dependent data\n                pmagplotlib.plot_init(plotnum, 5, 5)  # set up plot\n                # call the plotting function\n                pmagplotlib.plot_xtb(plotnum, XTB, Bs, e, f)\n                if plot == 0:\n                    pmagplotlib.draw_figs({'fig': plotnum})\n                plotnum += 1  # increment plot number\n            if '-i' in sys.argv:\n                for ind in range(len(Ts)):  # print list of temperatures available\n                    print(ind, int(Ts[ind]))\n                cont = input(\n                    \"Enter index of desired temperature step, s[a]ve plots, [return] to quit \")\n                if cont == 'a':\n                    files = {}\n                    PLTS = {}\n                    for p in range(1, plotnum):\n                        key = str(p)\n                        files[key] = e+'_'+key+'.'+fmt\n                        PLTS[key] = key\n                    pmagplotlib.save_plots(PLTS, files)\n                    cont = input(\n                        \"Enter index of desired temperature step, s[a]ve plots, [return] to quit \")\n                if cont == \"\":\n                    cont = 'q'\n            while cont != \"q\":\n                if '-i' in sys.argv:\n                    Tind = int(cont)  # set temperature index\n                b = Bs[-1]  # set field to max available\n                XF = []  # initial chi versus frequency list\n                for kk in range(len(X)):  # hunt through the data\n                    if T[kk] == Ts[Tind] and B[kk] == b:  # if temperature and field match,\n                        XF.append([X[kk], F[kk]])  # append the data\n                if len(XF) > 1:  # if there are any data to plot\n                    if FTinit == 0:  # if not already initialized, initialize plot\n                        # print 'initializing ',plotnum\n                        pmagplotlib.plot_init(plotnum, 5, 5)\n                        FTinit = 1\n                        XFplot = plotnum\n                        plotnum += 1  # increment plotnum\n                    pmagplotlib.plot_xft(XFplot, XF, Ts[Tind], e, b)\n                    if plot == 0:\n                        pmagplotlib.draw_figs({'fig': plotnum})\n                else:\n                    print(\n                        '\\n *** Skipping susceptibitily-frequency plot as a function of temperature *** \\n')\n                f = Fs[0]  # set frequency to minimum available\n                XB = []  # initialize chi versus field list\n                for kk in range(len(X)):  # hunt through the data\n                    # if temperature and field match those desired\n                    if T[kk] == Ts[Tind] and F[kk] == f:\n                        XB.append([X[kk], B[kk]])  # append the data to list\n                if len(XB) > 4:  # if there are any data\n                    if BTinit == 0:  # if plot not already initialized\n                        pmagplotlib.plot_init(plotnum, 5, 5)  # do it\n                        BTinit = 1\n                    # and call plotting function\n                    pmagplotlib.plot_xbt(plotnum, XB, Ts[Tind], e, f)\n                    if plot == 0:\n                        pmagplotlib.draw_figs({'fig': plotnum})\n                else:\n                    print(\n                        'Skipping susceptibitily - AC field plot as a function of temperature')\n                files = {}\n                PLTS = {}\n                for p in range(1, plotnum):\n                    key = str(p)\n                    files[key] = e+'_'+key+'.'+fmt\n                    PLTS[key] = p\n                if '-i' in sys.argv:\n                    # just in case you forgot, print out a new list of temperatures\n                    for ind in range(len(Ts)):\n                        print(ind, int(Ts[ind]))\n                    # ask for new temp\n                    cont = input(\n                        \"Enter index of next temperature step, s[a]ve plots,  [return] to quit \")\n                    if cont == \"\":\n                        sys.exit()\n                    if cont == 'a':\n                        pmagplotlib.save_plots(PLTS, files)\n                        cont = input(\n                            \"Enter index of desired temperature step, s[a]ve plots, [return] to quit \")\n                        if cont == \"\":\n                            sys.exit()\n                elif plot == 0:\n                    ans = input(\n                        \"enter s[a]ve to save files,  [return] to quit \")\n                    if ans == 'a':\n                        pmagplotlib.save_plots(PLTS, files)\n                        sys.exit()\n                    else:\n                        sys.exit()\n                else:\n                    pmagplotlib.save_plots(PLTS, files)\n                    sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames hysteresis_magic.py DESCRIPTION calculates hystereis parameters and saves them in 3.0 specimen format file makes plots if option selected SYNTAX hysteresis_magic.py [command line options] OPTIONS -h prints help message and quits -f: specify input file, default is agm_measurements.txt -F: specify specimens.txt output file -WD: directory to output files to (default : current directory) Note: if using Windows, all figures will output to current directory -ID: directory to read files from (default : same as -WD) -P: do not make the plots -spc SPEC: specify specimen name to plot and quit -sav save all plots and quit -fmt [png,svg,eps,jpg]", "response": "def main():\n    \"\"\"\n    NAME\n        hysteresis_magic.py\n\n    DESCRIPTION\n        calculates hystereis parameters and saves them in 3.0 specimen format file\n        makes plots if option selected\n\n    SYNTAX\n        hysteresis_magic.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f: specify input file, default is agm_measurements.txt\n        -F: specify specimens.txt output file\n        -WD: directory to output files to (default : current directory)\n             Note: if using Windows, all figures will output to current directory\n        -ID: directory to read files from (default : same as -WD)\n        -P: do not make the plots\n        -spc SPEC: specify specimen name to plot and quit\n        -sav save all plots and quit\n        -fmt [png,svg,eps,jpg]\n    \"\"\"\n    args = sys.argv\n    fmt = pmag.get_named_arg('-fmt', 'svg')\n    output_dir_path = pmag.get_named_arg('-WD', '.')\n    input_dir_path = pmag.get_named_arg('-ID', \"\")\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    meas_file = pmag.get_named_arg('-f', 'measurements.txt')\n    spec_file = pmag.get_named_arg('-F', 'specimens.txt')\n    make_plots = True\n    save_plots = False\n    if '-P' in args:\n        make_plots = False\n    if '-sav' in args:\n        save_plots = True\n    pltspec = pmag.get_named_arg('-spc', 0)\n    ipmag.hysteresis_magic(output_dir_path, input_dir_path, spec_file, meas_file,\n                           fmt, save_plots, make_plots, pltspec)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data_files_dir():\n    if 'data_files' in os.listdir(sys.prefix):\n        return os.path.join(sys.prefix, 'data_files')\n    else:\n        return os.path.join(get_pmag_dir(), 'data_files')", "response": "Find the directory with data_files and return the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pmag_dir():\n    # this is correct for py2exe (DEPRECATED)\n    #win_frozen = is_frozen()\n    #if win_frozen:\n    #    path = os.path.abspath(unicode(sys.executable, sys.getfilesystemencoding()))\n    #    path = os.path.split(path)[0]\n    #    return path\n    # this is correct for py2app\n    try:\n        return os.environ['RESOURCEPATH']\n    # this works for everything else\n    except KeyError: pass\n    # new way:\n    # if we're in the local PmagPy directory:\n    if os.path.isfile(os.path.join(os.getcwd(), 'pmagpy', 'pmag.py')):\n        lib_dir = os.path.join(os.getcwd(), 'pmagpy')\n    # if we're anywhere else:\n    elif getattr(sys, 'frozen', False): #pyinstaller datafile directory\n        return sys._MEIPASS\n    else:\n        # horrible, hack-y fix\n        # (prevents namespace issue between\n        # local github PmagPy and pip-installed PmagPy).\n        # must reload because we may have\n        # changed directories since importing\n        temp = os.getcwd()\n        os.chdir('..')\n        reload(locator)\n        lib_file = resource_filename('locator', 'resource.py')\n        full_dir = os.path.split(lib_file)[0]\n        ind = full_dir.rfind(os.sep)\n        lib_dir = full_dir[:ind+1]\n        lib_dir = os.path.realpath(os.path.join(lib_dir, 'pmagpy'))\n        os.chdir(temp)\n        # end fix\n        # old way:\n        #lib_dir = os.path.dirname(os.path.realpath(__file__))\n    if not os.path.isfile(os.path.join(lib_dir, 'pmag.py')):\n        lib_dir = os.getcwd()\n    fname = os.path.join(lib_dir, 'pmag.py')\n    if not os.path.isfile(fname):\n        pmag_dir = os.path.split(os.path.split(__file__)[0])[0]\n        if os.path.isfile(os.path.join(pmag_dir,'pmagpy','pmag.py')):\n            return pmag_dir\n        else:\n            print('-W- Can\\'t find the data model!  Make sure you have installed pmagpy using pip: \"pip install pmagpy --upgrade\"')\n            return '.'\n    # strip \"/\" or \"\\\" and \"pmagpy\" to return proper PmagPy directory\n    if lib_dir.endswith(os.sep):\n        lib_dir = lib_dir[:-1]\n    if lib_dir.endswith('pmagpy'):\n        pmag_dir = os.path.split(lib_dir)[0]\n    else:\n        pmag_dir = lib_dir\n    return pmag_dir", "response": "Returns the directory in which PmagPy is installed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    cmap = 'RdYlBu'\n    date = 2016.\n    if not Basemap:\n        print(\n            \"-W- Cannot access the Basemap module, which is required to run plot_magmap.py\")\n        sys.exit()\n    dir_path = '.'\n    lincr = 1  # level increment for contours\n    if '-WD' in sys.argv:\n        ind = sys.argv.index('-WD')\n        dir_path = sys.argv[ind+1]\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt = sys.argv[ind+1]\n        if fmt == 'jpg':\n            print('jpg not a supported option')\n            print(main.__doc__)\n            sys.exit()\n    else:\n        fmt = 'png'\n    if '-cm' in sys.argv:\n        ind = sys.argv.index('-cm')\n        cmap = sys.argv[ind+1]\n    if '-el' in sys.argv:\n        ind = sys.argv.index('-el')\n        el = sys.argv[ind+1]\n    else:\n        el = 'B'\n    if '-alt' in sys.argv:\n        ind = sys.argv.index('-alt')\n        alt = sys.argv[ind+1]\n    else:\n        alt = 0\n    if '-lon0' in sys.argv:\n        ind = sys.argv.index('-lon0')\n        lon_0 = float(sys.argv[ind+1])\n    else:\n        lon_0 = 0\n    if '-mod' in sys.argv:\n        ind = sys.argv.index('-mod')\n        mod = sys.argv[ind+1]\n        ghfile = ''\n    elif '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        ghfile = sys.argv[ind+1]\n        mod = 'custom'\n        date = ''\n    else:\n        mod, ghfile = 'cals10k', ''\n    if '-age' in sys.argv:\n        ind = sys.argv.index('-age')\n        date = float(sys.argv[ind+1])\n    if '-alt' in sys.argv:\n        ind = sys.argv.index('-alt')\n        alt = float(sys.argv[ind+1])\n    else:\n        alt = 0\n    save = pmag.get_flag_arg_from_sys(\"-sav\")\n    if mod == 'custom':\n        d = 'Custom'\n    else:\n        d = str(date)\n    Ds, Is, Bs, Brs, lons, lats = pmag.do_mag_map(\n        date, mod=mod, lon_0=lon_0, alt=alt, file=ghfile)\n    if el == 'D':\n        element = Ds\n    elif el == 'I':\n        element = Is\n    elif el == 'B':\n        element = Bs\n    elif el == 'Br':\n        element = Brs\n    elif el == 'I':\n        element = Is\n    else:\n        print(main.__doc__)\n        sys.exit()\n    pmagplotlib.plot_mag_map(1, element, lons, lats, el, lon_0=0, date=date)\n    if not save:\n        pmagplotlib.draw_figs({'map': 1})\n        res = pmagplotlib.save_or_quit()\n        if res == 'a':\n            figname = 'igrf'+d+'.'+fmt\n            print(\"1 saved in \", figname)\n            plt.savefig('igrf'+d+'.'+fmt)\n        sys.exit()\n    plt.savefig('igrf'+d+'.'+fmt)\n    print('Figure saved as: ', 'igrf'+d+'.'+fmt)", "response": "This function is the main function for the basemap plotting. It is the main function for the basemap plotting."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    args = sys.argv\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    verbose = pmagplotlib.verbose\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    input_dir_path = pmag.get_named_arg(\"-ID\", \"\")\n    num_bootstraps = pmag.get_named_arg(\"-n\", 1000)\n    ipar = pmag.get_flag_arg_from_sys(\"-par\", true=1, false=0)\n    ihext = pmag.get_flag_arg_from_sys(\"-x\", true=1, false=0)\n    ivec = pmag.get_flag_arg_from_sys(\"-v\", true=1, false=0)\n    iplot = pmag.get_flag_arg_from_sys(\"-P\", true=0, false=1)\n    isite = pmag.get_flag_arg_from_sys(\"-sit\", true=1, false=0)\n    iboot, vec = 1, 0\n    infile = pmag.get_named_arg('-f', 'specimens.txt')\n    samp_file = pmag.get_named_arg('-fsa', 'samples.txt')\n    site_file = pmag.get_named_arg('-fsi', 'sites.txt')\n    #outfile = pmag.get_named_arg(\"-F\", \"rmag_results.txt\")\n    fmt = pmag.get_named_arg(\"-fmt\", \"png\")\n    crd = pmag.get_named_arg(\"-crd\", \"s\")\n    comp, Dir, PDir = 0, [], []\n    user = pmag.get_named_arg(\"-usr\", \"\")\n    if '-B' in args:\n        iboot, ihext = 0, 1\n    plots, verbose = 0, True\n    if '-sav' in args:\n        plots = 1\n        verbose = 0\n    if '-gtc' in args:\n        ind = args.index('-gtc')\n        d, i = float(args[ind+1]), float(args[ind+2])\n        PDir.append(d)\n        PDir.append(i)\n    if '-d' in args:\n        comp = 1\n        ind = args.index('-d')\n        vec = int(args[ind+1])-1\n        Dir = [float(args[ind+2]), float(args[ind+3])]\n\n    ipmag.aniso_magic(infile=infile, samp_file=samp_file, site_file=site_file,\n                      ipar=ipar, ihext=ihext, ivec=ivec, iplot=iplot, isite=isite, iboot=iboot, vec=vec,\n                      Dir=Dir, PDir=PDir, comp=comp, user=user,\n                      fmt=fmt, crd=crd, verbose=verbose, plots=plots,\n                      num_bootstraps=num_bootstraps, dir_path=dir_path,\n                      input_dir_path=input_dir_path)", "response": "NAME aniso_magic. py - h is the entry point for the anisotropy_base. py script"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new():\n\n    args = sys.argv\n    if '-h' in args:\n        print(new.__doc__)\n        return\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    if '-ID' in args and dir_path == '.':\n        dir_path = pmag.get_named_arg(\"-ID\", \".\")\n    iboot, vec = 1, 0\n    num_bootstraps = pmag.get_named_arg(\"-n\", 1000)\n    ipar = pmag.get_flag_arg_from_sys(\"-par\", true=1, false=0)\n    ihext = pmag.get_flag_arg_from_sys(\"-x\", true=1, false=0)\n    ivec = pmag.get_flag_arg_from_sys(\"-v\", true=1, false=0)\n    if ivec:\n        vec = 3\n    #iplot = pmag.get_flag_arg_from_sys(\"-P\", true=0, false=1)\n    isite = pmag.get_flag_arg_from_sys(\"-sit\", true=1, false=0)\n    infile = pmag.get_named_arg('-f', 'specimens.txt')\n    samp_file = pmag.get_named_arg('-fsa', 'samples.txt')\n    site_file = pmag.get_named_arg('-fsi', 'sites.txt')\n    #outfile = pmag.get_named_arg(\"-F\", \"rmag_results.txt\")\n    fmt = pmag.get_named_arg(\"-fmt\", \"png\")\n    crd = pmag.get_named_arg(\"-crd\", \"s\")\n    comp, Dir, PDir = 0, [], []\n    user = pmag.get_named_arg(\"-usr\", \"\")\n    if '-B' in args:\n        iboot, ihext = 0, 1\n    save_plots, verbose, interactive = False, True, True\n    if '-sav' in args:\n        save_plots = True\n        verbose = False\n        interactive = False\n    if '-gtc' in args:\n        ind = args.index('-gtc')\n        d, i = float(args[ind+1]), float(args[ind+2])\n        PDir.append(d)\n        PDir.append(i)\n    if '-d' in args:\n        comp = 1\n        ind = args.index('-d')\n        vec = int(args[ind+1])-1\n        Dir = [float(args[ind+2]), float(args[ind+3])]\n    ipmag.aniso_magic_nb(infile, samp_file, site_file, verbose,\n                         ipar, ihext, ivec, isite, False, iboot,\n                         vec, Dir, PDir, crd, num_bootstraps,\n                         dir_path, save_plots=save_plots, interactive=interactive,\n                         fmt=fmt)", "response": "A function to create a new empty set of anisotropy data structures for the current base level of the current site."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    coord, kappa, cutoff, n = 0, 0, 180., 0\n    nb, anti, spin, v, boot = 1000, 0, 0, 0, 0\n    data_model = 3\n    rev = 0\n    if '-dm' in sys.argv:\n        ind = sys.argv.index(\"-dm\")\n        data_model = int(sys.argv[ind+1])\n    if data_model == 2:\n        coord_key = 'tilt_correction'\n        in_file = 'pmag_results.txt'\n        k_key, n_key, lat_key = 'average_k', 'average_nn', 'average_lat'\n    else:\n        coord_key = 'dir_tilt_correction'\n        in_file = 'sites.txt'\n        k_key, n_key, lat_key = 'dir_k', 'dir_n_samples`', 'lat'\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind = sys.argv.index(\"-f\")\n        in_file = sys.argv[ind + 1]\n        vgp_df = pd.read_csv(in_file, sep='\\t', header=1)\n    else:\n        vgp_df = pd.read_csv(sys.stdin, sep='\\t', header=1)\n    if '-c' in sys.argv:\n        ind = sys.argv.index('-c')\n        cutoff = float(sys.argv[ind+1])\n    if '-k' in sys.argv:\n        ind = sys.argv.index('-k')\n        kappa = float(sys.argv[ind+1])\n    if '-n' in sys.argv:\n        ind = sys.argv.index('-n')\n        n = float(sys.argv[ind+1])\n    if '-crd' in sys.argv:\n        ind = sys.argv.index(\"-crd\")\n        coord = sys.argv[ind+1]\n        if coord == 's':\n            coord = -1\n        if coord == 'g':\n            coord = 0\n        if coord == 't':\n            coord = 100\n    if '-a' in sys.argv:\n        anti = 1\n    if '-r' in sys.argv:\n        rev = 1\n    if '-p' in sys.argv:\n        spin = 1\n    if '-v' in sys.argv:\n        v = 1\n    if '-b' in sys.argv:\n        boot = 1\n    if '-mm97' in sys.argv:\n        mm97 = 1\n    else:\n        mm97 = 0\n    #\n    # find desired vgp lat,lon, kappa,N_site data:\n    #\n    vgp_df.dropna(subset=['vgp_lat', 'vgp_lon'])\n    keys = [coord_key, k_key, n_key, lat_key]\n    for key in keys:\n        if key not in vgp_df.columns:\n            vgp_df[key] = 0\n    vgp_df = vgp_df[vgp_df[coord_key] == coord]\n    if data_model != 3:  # convert\n        vgp_df['dir_k'] = vgp_df[k_key]\n        vgp_df['dir_n_samples'] = vgp_df[n_key]\n        vgp_df['lat'] = vgp_df[lat_key]\n    N, S_B, low, high, cutoff = pmag.scalc_vgp_df(\n        vgp_df, anti=anti, rev=rev, cutoff=cutoff, kappa=kappa, n=n, spin=spin, v=v, boot=boot, mm97=mm97)\n    if high != 0:\n        print(N, '%7.1f %7.1f  %7.1f %7.1f ' % (S_B, low, high, cutoff))\n    else:\n        print(N, '%7.1f  %7.1f ' % (S_B, cutoff))", "response": "NAME\n        scalc_magic.py\n\n    DESCRIPTION\n       calculates Sb from pmag_results files\n\n    SYNTAX\n        scalc_magic -h [command line options]\n\n    INPUT\n       takes magic formatted pmag_results (2.5) or sites (3.0) table\n       pmag_result_name (2.5)  must start with \"VGP: Site\"\n       must have average_lat (2.5)  or lat (3.0) if spin axis is reference\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE: specify input results file, default is 'sites.txt'\n        -c cutoff:  specify VGP colatitude cutoff value, default is no cutoff\n        -k cutoff: specify kappa cutoff, default is 0\n        -crd [s,g,t]: specify coordinate system, default is geographic\n        -v : use the VanDammme criterion\n        -a: use antipodes of reverse data: default is to use only normal\n        -r:  use reverse data only\n        -p: do relative to principle axis\n        -b: do bootstrap confidence bounds\n        -n: set minimum n for samples (specimens) per site\n        -dm: data model [3.0 is default, otherwise, 2.5]\n        -mm97: correct for within site scatter (McElhinny & McFadden, 1997)\n    NOTES\n        if kappa, N_site, lat supplied, will consider within site scatter\n    OUTPUT\n        N Sb  Sb_lower Sb_upper Co-lat. Cutoff\n\n\n     OUTPUT:\n         if option -b used: N,  S_B, lower and upper bounds\n         otherwise: N,  S_B, cutoff"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnames picks out keys and makes and xy plot_magic_keys. py", "response": "def main():\n    \"\"\"\n    NAME\n        plot_magic_keys.py\n\n    DESCRIPTION\n        picks out keys and makes and xy plot\n\n    SYNTAX\n        plot_magic_keys.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE: specify input magic format file\n        -xkey KEY: specify key for X\n        -ykey KEY: specify key  for Y\n        -b xmin xmax ymin ymax, sets bounds\n\n    \"\"\"\n    dir_path=\"./\"\n    if '-WD' in sys.argv:\n        ind=sys.argv.index('-WD')\n        dir_path=sys.argv[ind+1]\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        magic_file=dir_path+'/'+sys.argv[ind+1]\n    else:\n        print(main.__doc__)\n        sys.exit()\n    if '-xkey' in sys.argv:\n        ind=sys.argv.index('-xkey')\n        xkey=sys.argv[ind+1]\n        if '-ykey' in sys.argv:\n            ind=sys.argv.index('-ykey')\n            ykey=sys.argv[ind+1]\n    else:\n        print(main.__doc__)\n        sys.exit()\n    if '-b' in sys.argv:\n        ind=sys.argv.index('-b')\n        xmin=float(sys.argv[ind+1])\n        xmax=float(sys.argv[ind+2])\n        ymin=float(sys.argv[ind+3])\n        ymax=float(sys.argv[ind+4])\n    #\n    #\n    # get data read in\n    X,Y=[],[]\n    Data,file_type=pmag.magic_read(magic_file)\n    if len(Data)>0:\n        for rec in Data:\n            if xkey in list(rec.keys()) and rec[xkey]!=\"\" and ykey in list(rec.keys()) and rec[ykey]!=\"\":\n                try:\n                    X.append(float(rec[xkey]))\n                    Y.append(float(rec[ykey]))\n                except:\n                    pass\n        FIG={'fig':1}\n        pmagplotlib.plot_init(FIG['fig'],5,5)\n        if '-b' in sys.argv:\n            pmagplotlib.plot_xy(FIG['fig'],X,Y,sym='ro',xlab=xkey,ylab=ykey,xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax )\n        else:\n            pmagplotlib.plot_xy(FIG['fig'],X,Y,sym='ro',xlab=xkey,ylab=ykey)\n        pmagplotlib.draw_figs(FIG)\n        ans=input(\" S[a]ve to save plot, [q]uit, Return to continue:  \")\n        if ans==\"q\": sys.exit()\n        if ans==\"a\":\n            files = {}\n            for key in list(FIG.keys()):\n                files[key]=str(key) + \".svg\"\n                pmagplotlib.save_plots(FIG,files)\n        sys.exit()\n    else:\n        print('no data to plot')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    title = \"\"\n    files, fmt = {}, 'svg'\n    sym = {'lower': ['o', 'r'], 'upper': ['o', 'w']}\n    plot = 0\n    if '-h' in sys.argv:  # check if help is needed\n        print(main.__doc__)\n        sys.exit()  # graceful quit\n    if '-sav' in sys.argv:\n        plot = 1\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt = sys.argv[ind + 1]\n    if '-s' in sys.argv:\n        ind = sys.argv.index('-s')\n        sym['size'] = int(sys.argv[ind + 1])\n    else:\n        sym['size'] = 20\n    if '-Lsym' in sys.argv:\n        ind = sys.argv.index('-Lsym')\n        sym['lower'][0] = sys.argv[ind + 1]\n        sym['lower'][1] = sys.argv[ind + 2]\n    if '-Usym' in sys.argv:\n        ind = sys.argv.index('-Usym')\n        sym['upper'][0] = sys.argv[ind + 1]\n        sym['upper'][1] = sys.argv[ind + 2]\n    if '-f' in sys.argv:  # ask for filename\n        ind = sys.argv.index('-f')\n        fname = sys.argv[ind + 1]\n    else:\n        print(main.__doc__)\n        print(' \\n   -f option required')\n        sys.exit()  # graceful quit\n    DI = numpy.loadtxt(fname)\n    EQ = {'eq': 1}\n    pmagplotlib.plot_init(EQ['eq'], 5, 5)\n    pmagplotlib.plot_eq_sym(EQ['eq'], DI, 'Equal Area Plot', sym)  # make plot\n    if plot == 0:\n        pmagplotlib.draw_figs(EQ)  # make it visible\n    for key in list(EQ.keys()):\n        files[key] = key + '.' + fmt\n    if pmagplotlib.isServer:\n        black = '#000000'\n        purple = '#800080'\n        titles = {}\n        titles['eq'] = 'Equal Area Plot'\n        EQ = pmagplotlib.add_borders(EQ, titles, black, purple)\n        pmagplotlib.save_plots(EQ, files)\n    elif plot == 1:\n        fname = os.path.split(fname)[1].split('.')[0]\n        files['eq'] = fname + '_eq.' + fmt\n        pmagplotlib.save_plots(EQ, files)\n    else:\n        ans = input(\" S[a]ve to save plot, [q]uit without saving:  \")\n        if ans == \"a\":\n            pmagplotlib.save_plots(EQ, files)", "response": "NAME eqarea. py\n    DESCRIPTION\n       makes equal area projections from declination and inclination data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    fmt,nb='svg',1000\n    plot=0\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    elif '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n    else:\n        print(main.__doc__)\n        sys.exit()\n    if '-n' in sys.argv:\n        ind=sys.argv.index('-n')\n        nb=int(sys.argv[ind+1])\n    if '-sc' in sys.argv:\n        site_correction = True\n    else:\n        site_correction = False\n    if '-fmt' in sys.argv:\n        ind=sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    if '-sav' in sys.argv:plot=1\n    data=numpy.loadtxt(file)\n    upper,lower=int(round(.975*nb)),int(round(.025*nb))\n    E,I=[],[]\n    PLTS={'eq':1,'ei':2,'cdf':3,'v2':4}\n    pmagplotlib.plot_init(PLTS['eq'],6,6)\n    pmagplotlib.plot_init(PLTS['ei'],5,5)\n    pmagplotlib.plot_init(PLTS['cdf'],5,5)\n    pmagplotlib.plot_init(PLTS['v2'],5,5)\n    pmagplotlib.plot_eq(PLTS['eq'],data,'Data')\n    # this is a problem\n    #if plot==0:pmagplotlib.draw_figs(PLTS)\n    ppars=pmag.doprinc(data)\n    Io=ppars['inc']\n    n=ppars[\"N\"]\n    Es,Is,Fs,V2s=pmag.find_f(data)\n    if site_correction:\n        Inc,Elong=Is[Es.index(min(Es))],Es[Es.index(min(Es))]\n        flat_f = Fs[Es.index(min(Es))]\n    else:\n        Inc,Elong=Is[-1],Es[-1]\n        flat_f = Fs[-1]\n    pmagplotlib.plot_ei(PLTS['ei'],Es,Is,flat_f)\n    pmagplotlib.plot_v2s(PLTS['v2'],V2s,Is,flat_f)\n    b=0\n    print(\"Bootstrapping.... be patient\")\n    while b<nb:\n        bdata=pmag.pseudo(data)\n        Esb,Isb,Fsb,V2sb=pmag.find_f(bdata)\n        if b<25:\n            pmagplotlib.plot_ei(PLTS['ei'],Esb,Isb,Fsb[-1])\n        if Esb[-1]!=0:\n            ppars=pmag.doprinc(bdata)\n            if site_correction:\n                I.append(abs(Isb[Esb.index(min(Esb))]))\n                E.append(Esb[Esb.index(min(Esb))])\n            else:\n                I.append(abs(Isb[-1]))\n                E.append(Esb[-1])\n            b+=1\n            if b%25==0:print(b,' out of ',nb)\n    I.sort()\n    E.sort()\n    Eexp=[]\n    for i in I:\n       Eexp.append(pmag.EI(i))\n    if Inc==0:\n        title= 'Pathological Distribution: '+'[%7.1f, %7.1f]' %(I[lower],I[upper])\n    else:\n        title= '%7.1f [%7.1f, %7.1f]' %( Inc, I[lower],I[upper])\n    pmagplotlib.plot_ei(PLTS['ei'],Eexp,I,1)\n    pmagplotlib.plot_cdf(PLTS['cdf'],I,'Inclinations','r',title)\n    pmagplotlib.plot_vs(PLTS['cdf'],[I[lower],I[upper]],'b','--')\n    pmagplotlib.plot_vs(PLTS['cdf'],[Inc],'g','-')\n    pmagplotlib.plot_vs(PLTS['cdf'],[Io],'k','-')\n    if plot==0:\n        print('%7.1f %s %7.1f _ %7.1f ^ %7.1f:  %6.4f _ %6.4f ^ %6.4f' %(Io, \" => \", Inc, I[lower],I[upper], Elong, E[lower],E[upper]))\n        print(\"Io Inc  I_lower, I_upper, Elon, E_lower, E_upper\")\n        pmagplotlib.draw_figs(PLTS)\n        ans = \"\"\n        while ans not in ['q', 'a']:\n            ans= input(\"S[a]ve plots - <q> to quit:  \")\n        if ans=='q':\n           print(\"\\n Good bye\\n\")\n           sys.exit()\n\n    files={}\n    files['eq']='findEI_eq.'+fmt\n    files['ei']='findEI_ei.'+fmt\n    files['cdf']='findEI_cdf.'+fmt\n    files['v2']='findEI_v2.'+fmt\n    pmagplotlib.save_plots(PLTS,files)", "response": "This is the main function for the unflattening_EI_tangent function. It is used to find the unflattening_EI_tangent function that is used to calculate the unflattening_EI_tangent function. It is used to calculate the unflattening_EI_tangent function and the unflattening_EI_tangent function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_change_dir_button(self, event):\n        currentDirectory = self.WD #os.getcwd()\n        change_dir_dialog = wx.DirDialog(self.panel,\n                                         \"Choose your working directory to create or edit a MagIC contribution:\",\n                                         defaultPath=currentDirectory,\n                                         style=wx.DD_DEFAULT_STYLE | wx.DD_NEW_DIR_BUTTON | wx.DD_CHANGE_DIR)\n        result = change_dir_dialog.ShowModal()\n        if result == wx.ID_CANCEL:\n            return\n        if result == wx.ID_OK:\n            self.WD = change_dir_dialog.GetPath()\n            self.dir_path.SetValue(self.WD)\n        change_dir_dialog.Destroy()\n        wait = wx.BusyInfo('Initializing data object in new directory, please wait...')\n        wx.SafeYield()\n        print('-I- Initializing magic data object')\n        # make new builder object, but reuse old data_model\n        self.er_magic = builder.ErMagicBuilder(self.WD, self.er_magic.data_model)\n        print('-I- Read in any available data from working directory')\n        self.er_magic.get_all_magic_info()\n        print('-I- Initializing headers')\n        self.er_magic.init_default_headers()\n        self.er_magic.init_actual_headers()\n        del wait", "response": "Create a new directory dialog and initialize the data object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_grid_frame(self, event):\n        if self.grid_frame:\n            print('-I- You already have a grid frame open')\n            pw.simple_warning(\"You already have a grid open\")\n            return\n\n        try:\n            grid_type = event.GetButtonObj().Name[:-4] # remove '_btn'\n        except AttributeError:\n            grid_type = self.FindWindowById(event.Id).Name[:-4] # remove ('_btn')\n        wait = wx.BusyInfo('Making {} grid, please wait...'.format(grid_type))\n        wx.SafeYield()\n        # hide mainframe\n        self.on_open_grid_frame()\n        self.grid_frame = grid_frame.GridFrame(self.er_magic, self.WD, grid_type, grid_type, self.panel)\n        if self.validation_mode:\n            if grid_type in self.validation_mode:\n                self.grid_frame.grid.paint_invalid_cells(self.warn_dict[grid_type])\n                #self.grid_frame.msg_boxsizer\n                current_label = self.grid_frame.msg_text.GetLabel()\n                add_text = \"\"\"\\n\\nColumns and rows with problem data have been highlighted in blue.\nCells with problem data are highlighted with different colors according to the type of problem.\nRed: missing required data\nGreen: missing or invalid parent\nBlue: non-numeric data provided in a numeric field\nGray: unrecognized column\nPurple: invalid result child\nYellow: Out-of-range latitude (should be -90 - 90) or longitude (should be 0-360)\nLight gray: Unrecognized term in controlled vocabulary\n\nNote: It is possible to have a row highlighted that has no highlighted column.\nThis means that you are missing information higher up in the data.\nFor example: a specimen could be missing a site name.\nHowever, you need to fix this in the sample grid, not the specimen grid.\nOnce each item in the data has its proper parent, validations will be correct.\n\"\"\"\n                self.grid_frame.msg_text.SetLabel(add_text)\n        #self.on_finish_change_dir(self.change_dir_dialog)\n        del wait", "response": "Create a grid frame for the data type of the button that was clicked"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting all data to appropriate er_* and pmag_* files. Then use those files to create a MagIC upload format file. Validate the upload file.", "response": "def on_upload_file(self, event):\n        \"\"\"\n        Write all data to appropriate er_* and pmag_* files.\n        Then use those files to create a MagIC upload format file.\n        Validate the upload file.\n        \"\"\"\n        # coherence validations\n        wait = wx.BusyInfo('Validating data, please wait...')\n        wx.SafeYield()\n        spec_warnings, samp_warnings, site_warnings, loc_warnings = self.er_magic.validate_data()\n        result_warnings = self.er_magic.validate_results(self.er_magic.results)\n        meas_warnings = self.er_magic.validate_measurements(self.er_magic.measurements)\n        self.warn_dict = {'specimen': spec_warnings, 'sample': samp_warnings,\n                          'site': site_warnings, 'location': loc_warnings,\n                          'result': result_warnings, 'age': {}, 'measurement': meas_warnings}\n        # done coherence validations\n        del wait\n        # write upload file and perform data validations\n        wait = wx.BusyInfo('Making upload file, please wait...')\n        wx.SafeYield()\n        self.er_magic.write_files()\n        upfile, error_message, errors = ipmag.upload_magic(dir_path=self.WD,\n                                                           data_model=self.data_model)\n        del wait\n        if upfile:\n            text = \"You are ready to upload.\\nYour file:\\n{}\\nwas generated in directory: \\n{}\\nDrag and drop this file in the MagIC database.\".format(os.path.split(upfile)[1], self.WD)\n            dlg = wx.MessageDialog(self, caption=\"Saved\", message=text, style=wx.OK)\n        else:\n            text = \"There were some problems with the creation of your upload file.\\nError message: {}\\nSee Terminal/Command Prompt for details\".format(error_message)\n            dlg = wx.MessageDialog(self, caption=\"Error\", message=text, style=wx.OK)\n        result = dlg.ShowModal()\n        if result == wx.ID_OK:\n            dlg.Destroy()\n        self.edited = False\n        ## add together data & coherence errors into one dictionary\n        if errors:\n            for item_type in errors:\n                for item_name in errors[item_type]:\n                    if item_name in self.warn_dict[item_type]:\n                        self.warn_dict[item_type][item_name].update(errors[item_type][item_name])\n                    else:\n                        self.warn_dict[item_type][item_name] = errors[item_type][item_name]\n\n        has_problems = []\n        for item_type, warnings in list(self.warn_dict.items()):\n            if warnings:\n                has_problems.append(item_type)\n        # for any dtypes with validation problems (data or coherence),\n        # highlight the button to the corresponding grid\n        # skip this step for Windows\n        if sys.platform in ['win32', 'win62']:\n            pass\n        else:\n            for dtype in self.warn_dict:\n                wind = self.FindWindowByName(dtype + '_btn')\n                if wind:\n                    if dtype in has_problems:\n                        wind.Bind(wx.EVT_PAINT, self.highlight_button)\n                    else:\n                        wind.Unbind(wx.EVT_PAINT, handler=self.highlight_button)\n            self.Refresh()\n        if has_problems:\n            self.validation_mode = set(has_problems)\n            if sys.platform in ['win32', 'win62']:\n                self.message.SetLabel('The following grid(s) have incorrect or incomplete data:\\n{}'.format(', '.join(self.validation_mode)))\n            else:\n                self.message.SetLabel('Highlighted grids have incorrect or incomplete data')\n            self.bSizer_msg.ShowItems(True)\n            self.hbox.Fit(self)\n        if not has_problems:\n            self.validation_mode = set()\n            self.message.SetLabel('')\n            self.bSizer_msg.ShowItems(False)\n            self.hbox.Fit(self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_clear(self, event):\n        dia = pmag_menu_dialogs.ClearWD(self.parent, self.parent.WD)\n        clear = dia.do_clear()\n        if clear:\n            print('-I- Clear data object')\n            self.parent.er_magic = builder.ErMagicBuilder(self.parent.WD, self.parent.data_model)\n            print('-I- Initializing headers')\n            self.parent.er_magic.init_default_headers()\n            self.parent.er_magic.init_actual_headers()", "response": "Clear the data object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    #\n    # initialize variables\n    #\n    version_num=pmag.get_version()\n    orient_file,samp_file = \"orient\",\"er_samples.txt\"\n    args=sys.argv\n    dir_path,out_path='.','.'\n    default_outfile = True\n    #\n    #\n    if '-WD' in args:\n        ind=args.index('-WD')\n        dir_path=args[ind+1]\n    if '-OD' in args:\n        ind=args.index('-OD')\n        out_path=args[ind+1]\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    if \"-F\" in args:\n        ind=args.index(\"-F\")\n        orient_file=sys.argv[ind+1]\n        default_outfile = False\n    if \"-f\" in args:\n        ind=args.index(\"-f\")\n        samp_file=sys.argv[ind+1]\n    orient_file=out_path+'/'+orient_file\n    samp_file=dir_path+'/'+samp_file\n    #\n    # read in file to convert\n    #\n    ErSamples=[]\n    Required=['sample_class','sample_type','sample_lithology','lat','long']\n    Samps,file_type=pmag.magic_read(samp_file)\n    Locs=[]\n    OrKeys=['sample_name','site_name','mag_azimuth','field_dip','sample_class','sample_type','sample_lithology','lat','long','stratigraphic_height','method_codes','site_description']\n    print(\"file_type\", file_type) # LJ\n    if file_type.lower()=='er_samples':\n        SampKeys=['er_sample_name','er_site_name','sample_azimuth','sample_dip','sample_class','sample_type','sample_lithology','sample_lat','sample_lon','sample_height','magic_method_codes','er_sample_description']\n    elif file_type.lower()=='magic_measurements':\n        SampKeys=['er_sample_name','er_site_name']\n    else:\n        print('wrong file format; must be er_samples or magic_measurements only')\n    for samp in Samps:\n            if samp['er_location_name'] not in Locs:Locs.append(samp['er_location_name']) # get all the location names\n    for location_name in Locs:\n        loc_samps=pmag.get_dictitem(Samps,'er_location_name',location_name,'T')\n        OrOut=[]\n        for samp in loc_samps:\n            if samp['er_sample_name'] not in ErSamples:\n                ErSamples.append(samp['er_sample_name'])\n                OrRec={}\n                if 'sample_date' in list(samp.keys()) and samp['sample_date'].strip()!=\"\":\n                    date=samp['sample_date'].split(':')\n                    OrRec['date']=date[1]+'/'+date[2]+'/'+date[0][2:4]\n                for i in range(len(SampKeys)): \n                    if SampKeys[i] in list(samp.keys()):OrRec[OrKeys[i]]=samp[SampKeys[i]]\n                for key in Required:\n                    if key not in list(OrRec.keys()):OrRec[key]=\"\" # fill in blank required keys \n                OrOut.append(OrRec)\n        loc=location_name.replace(\" \",\"_\") \n        if default_outfile:\n            outfile=orient_file+'_'+loc+'.txt'\n        else:\n            outfile=orient_file\n        pmag.magic_write(outfile,OrOut,location_name)\n        print(\"Data saved in: \", outfile)", "response": "A function that takes an er_samples or magic_measurements format file and creates an orient. txt file for the new sample set and returns a template for the new sample set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    if len(sys.argv) > 0:\n        if '-h' in sys.argv: # check if help is needed\n            print(main.__doc__)\n            sys.exit() # graceful quit\n        if '-f' in sys.argv: # ask for filename\n            ind=sys.argv.index('-f')\n            file=sys.argv[ind+1]\n            f=open(file,'r')\n            data=f.readlines()\n        else:\n            data=sys.stdin.readlines() # read in data from standard input\n    DIs= [] # set up list for dec inc data\n    ofile = \"\"\n    if '-F' in sys.argv: # set up output file\n        ind = sys.argv.index('-F')\n        ofile= sys.argv[ind+1]\n        out = open(ofile, 'w + a')\n    for line in data:   # read in the data from standard input\n        if '\\t' in line:\n            rec=line.split('\\t') # split each line on space to get records\n        else:\n            rec=line.split() # split each line on space to get records\n        DIs.append((float(rec[0]),float(rec[1])))\n#\n    bpars=pmag.dobingham(DIs)\n    output = '%7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %7.1f %i' % (bpars[\"dec\"],bpars[\"inc\"],bpars[\"Eta\"],bpars[\"Edec\"],bpars[\"Einc\"],bpars[\"Zeta\"],bpars[\"Zdec\"],bpars[\"Zinc\"],bpars[\"n\"])\n    if ofile == \"\":\n        print(output)\n    else:\n        out.write(output+'\\n')", "response": "NAME\nATTRIBS get_n_record_from_dec_inc_file DESCRIPTION get_n_record_from_dec_inc_file DESCRIPTION get_n_record_from_dec_inc_file DESCRIPTION get_n_record_from_dec_inc_file DESCRIPTION get_n_record_from_dec_inc_file DESCRIPTION get_n_record_from_dec_file DESCRIPTION get_n_record_from_dec_file DESCRIPTION get_n"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames atrm_magic.py DESCRIPTION Converts ATRM data to best-fit tensor (6 elements plus sigma) Original program ARMcrunch written to accomodate ARM anisotropy data collected from 6 axial directions (+X,+Y,+Z,-X,-Y,-Z) using the off-axis remanence terms to construct the tensor. A better way to do the anisotropy of ARMs is to use 9,12 or 15 measurements in the Hext rotational scheme. SYNTAX atrm_magic.py [-h][command line options] OPTIONS -h prints help message and quits -f FILE: specify input file, default is atrm_measurements.txt -fsp FILE: specimen input file, default is specimens.txt (optional) -Fsp FILE: specify output file, default is specimens.txt (MagIC 3 only) -DM DATA_MODEL: specify MagIC 2 or MagIC 3, default is 3 INPUT Input for the present program is a TRM acquisition data with an optional baseline. The order of the measurements is: Decs=[0,90,0,180,270,0,0,90,0] Incs=[0,0,90,0,0,-90,0,0,90] The last two measurements are optional", "response": "def main():\n    \"\"\"\n    NAME\n        atrm_magic.py\n\n    DESCRIPTION\n        Converts ATRM  data to best-fit tensor (6 elements plus sigma)\n         Original program ARMcrunch written to accomodate ARM anisotropy data\n          collected from 6 axial directions (+X,+Y,+Z,-X,-Y,-Z) using the\n          off-axis remanence terms to construct the tensor. A better way to\n          do the anisotropy of ARMs is to use 9,12 or 15 measurements in\n          the Hext rotational scheme.\n\n    SYNTAX\n        atrm_magic.py [-h][command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE: specify input file, default is atrm_measurements.txt\n        -fsp FILE: specimen input file, default is specimens.txt (optional)\n        -Fsp FILE: specify output file, default is specimens.txt (MagIC 3 only)\n        -DM DATA_MODEL: specify MagIC 2 or MagIC 3, default is 3\n\n    INPUT\n        Input for the present program is a TRM acquisition data with an optional baseline.\n      The order of the measurements is:\n    Decs=[0,90,0,180,270,0,0,90,0]\n    Incs=[0,0,90,0,0,-90,0,0,90]\n     The last two measurements are optional\n\n    \"\"\"\n    # initialize some parameters\n    args = sys.argv\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n\n\n    #if \"-Fa\" in args:\n    #    ind = args.index(\"-Fa\")\n    #    rmag_anis = args[ind + 1]\n    #if \"-Fr\" in args:\n    #    ind = args.index(\"-Fr\")\n    #    rmag_res = args[ind + 1]\n\n    #meas_file = \"atrm_measurements.txt\"\n    #rmag_anis = \"trm_anisotropy.txt\"\n    #rmag_res = \"atrm_results.txt\"\n\n\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    input_dir_path = pmag.get_named_arg(\"-ID\", \"\")\n    meas_file = pmag.get_named_arg(\"-f\", \"measurements.txt\")\n    data_model_num = int(pmag.get_named_arg(\"-DM\", 3))\n    spec_outfile = pmag.get_named_arg(\"-Fsp\", \"specimens.txt\")\n    spec_infile = pmag.get_named_arg(\"-fsp\", \"specimens.txt\")\n\n\n    ipmag.atrm_magic(meas_file, dir_path, input_dir_path,\n                     spec_infile, spec_outfile, data_model_num)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(command_line=True, **kwargs):\n    #\n    # initialize defaults\n    version_num=pmag.get_version()\n    meas_file='magic_measurements.txt'\n    spec_file='er_specimens.txt'\n    samp_file='er_samples.txt'\n    site_file='er_sites.txt'\n    csv_file=''\n    ErSpecs,ErSamps,ErSites,ErLocs,ErCits=[],[],[],[],[]\n    MagRecs=[]\n    citation=\"This study\"\n    dir_path,demag='.','NRM'\n    args=sys.argv\n    noave=0\n    depth_method='a'\n    # get command line args\n    if command_line:\n        if '-WD' in args:\n            ind=args.index(\"-WD\")\n            dir_path=args[ind+1]\n        if '-ID' in args:\n            ind = args.index('-ID')\n            input_dir_path = args[ind+1]\n        else:\n            input_dir_path = dir_path\n        output_dir_path = dir_path\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if \"-A\" in args: noave=1\n        if '-f' in args:\n            ind=args.index(\"-f\")\n            csv_file=args[ind+1]\n        if '-F' in args:\n            ind=args.index(\"-F\")\n            meas_file=args[ind+1]\n        if '-Fsp' in args:\n            ind=args.index(\"-Fsp\")\n            spec_file = args[ind+1]\n        if '-Fsi' in args:\n            ind=args.index(\"-Fsi\")\n            site_file=args[ind+1]\n        if '-Fsa' in args:\n            ind=args.index(\"-Fsa\")\n            samp_file = args[ind+1]\n\n    if not command_line:\n        dir_path = kwargs.get('dir_path', '.')\n        input_dir_path = kwargs.get('input_dir_path', dir_path)\n        output_dir_path = dir_path # rename dir_path after input_dir_path is set\n        noave = kwargs.get('noave', 0) # default (0) is DO average\n        csv_file = kwargs.get('csv_file', '')\n        meas_file = kwargs.get('meas_file', 'magic_measurements.txt')\n        spec_file = kwargs.get('spec_file', 'er_specimens.txt')\n        samp_file = kwargs.get('samp_file', 'er_samples.txt')\n        site_file = kwargs.get('site_file', 'er_sites.txt')\n\n    # format variables\n\n    meas_file = os.path.join(output_dir_path, meas_file)\n    spec_file = os.path.join(output_dir_path, spec_file)\n    Specs,file_type = pmag.magic_read(spec_file)\n    samp_file = os.path.join(output_dir_path, samp_file)\n    ErSamps,file_type = pmag.magic_read(samp_file)\n    site_file = os.path.join(output_dir_path, site_file)\n    if csv_file==\"\":\n        filelist=os.listdir(input_dir_path) # read in list of files to import\n    else:\n        csv_file = os.path.join(input_dir_path, csv_file)\n        filelist=[csv_file]\n\n\n    # parsing the data\n    specimens,samples,sites=[],[],[]\n    MagRecs,SpecRecs,SampRecs,SiteRecs=[],[],[],[]\n    for samp in ErSamps:\n        if samp['er_sample_name'] not in samples:\n            samples.append(samp['er_sample_name'])\n            SampRecs.append(samp)\n    file_found = False\n    for f in filelist: # parse each file\n        if f[-3:].lower()=='csv':\n            file_found = True\n            print('processing: ',f)\n            full_file = os.path.join(input_dir_path, f)\n            with open(full_file, 'r') as fin:\n                file_input = fin.readlines()\n            keys=file_input[0].replace('\\n','').split(',') # splits on underscores\n            if \"Interval Top (cm) on SHLF\" in keys:interval_key=\"Interval Top (cm) on SHLF\"\n            if \" Interval Bot (cm) on SECT\" in keys:interval_key=\" Interval Bot (cm) on SECT\"\n            if \"Offset (cm)\" in keys: interval_key=\"Offset (cm)\"\n            if \"Top Depth (m)\" in keys:depth_key=\"Top Depth (m)\"\n            if \"CSF-A Top (m)\" in keys:depth_key=\"CSF-A Top (m)\"\n            if \"Depth CSF-A (m)\" in keys:depth_key=\"Depth CSF-A (m)\"\n            if \"CSF-B Top (m)\" in keys:\n                comp_depth_key=\"CSF-B Top (m)\" # use this model if available\n            elif \"Depth CSF-B (m)\" in keys:\n                comp_depth_key=\"Depth CSF-B (m)\"\n            else:\n                comp_depth_key=\"\"\n            if \"Demag level (mT)\" in keys:demag_key=\"Demag level (mT)\"\n            if \"Demag Level (mT)\" in keys: demag_key=\"Demag Level (mT)\"\n            if \"Inclination (Tray- and Bkgrd-Corrected) (deg)\" in keys:inc_key=\"Inclination (Tray- and Bkgrd-Corrected) (deg)\"\n            if \"Inclination background + tray corrected  (deg)\" in keys:inc_key=\"Inclination background + tray corrected  (deg)\"\n            if \"Inclination background + tray corrected  (\\xc2\\xb0)\" in keys:inc_key=\"Inclination background + tray corrected  (\\xc2\\xb0)\"\n            if \"Inclination background &amp; tray corrected (deg)\" in keys:inc_key=\"Inclination background &amp; tray corrected (deg)\"\n            if \"Declination (Tray- and Bkgrd-Corrected) (deg)\" in keys:dec_key=\"Declination (Tray- and Bkgrd-Corrected) (deg)\"\n            if \"Declination background + tray corrected (deg)\" in keys:dec_key=\"Declination background + tray corrected (deg)\"\n            if \"Declination background + tray corrected (\\xc2\\xb0)\" in keys:dec_key=\"Declination background + tray corrected (\\xc2\\xb0)\"\n            if \"Declination background &amp; tray corrected (deg)\" in keys:dec_key=\"Declination background &amp; tray corrected (deg)\"\n            if \"Intensity (Tray- and Bkgrd-Corrected) (A/m)\" in keys:int_key=\"Intensity (Tray- and Bkgrd-Corrected) (A/m)\"\n            if \"Intensity background + tray corrected  (A/m)\" in keys:int_key=\"Intensity background + tray corrected  (A/m)\"\n            if \"Intensity background &amp; tray corrected (A/m)\" in keys:int_key=\"Intensity background &amp; tray corrected (A/m)\"\n            if \"Core Type\" in keys:\n                core_type=\"Core Type\"\n            else: core_type=\"Type\"\n            if 'Run Number' in keys: run_number_key='Run Number'\n            if 'Test No.' in keys: run_number_key='Test No.'\n            if 'Test Changed On' in keys: date_key='Test Changed On'\n            if \"Timestamp (UTC)\" in keys: date_key=\"Timestamp (UTC)\"\n            if \"Section\" in keys: sect_key=\"Section\"\n            if \"Sect\" in keys: sect_key=\"Sect\"\n            if 'Section Half' in keys: half_key='Section Half'\n            if \"A/W\" in keys: half_key=\"A/W\"\n            if \"Text ID\" in keys: text_id=\"Text ID\"\n            if \"Text Id\" in keys: text_id=\"Text Id\"\n            for line in file_input[1:]:\n              InRec={}\n              test=0\n              recs=line.split(',')\n              for k in range(len(keys)):\n                  if len(recs)==len(keys):\n                      InRec[keys[k]]=line.split(',')[k]\n              if InRec['Exp']!=\"\": test=1 # get rid of pesky blank lines\n              if test==1:\n                run_number=\"\"\n                inst=\"IODP-SRM\"\n                volume='15.59' # set default volume to this\n                MagRec,SpecRec,SampRec,SiteRec={},{},{},{}\n                expedition=InRec['Exp']\n                location=InRec['Site']+InRec['Hole']\n# Maintain backward compatibility for the ever-changing LIMS format (Argh!)\n                while len(InRec['Core'])<3:\n                    InRec['Core']='0'+InRec['Core']\n                if \"Last Tray Measurment\" in list(InRec.keys()) and \"SHLF\" not in InRec[text_id] or 'dscr' in csv_file :  # assume discrete sample\n                    specimen=expedition+'-'+location+'-'+InRec['Core']+InRec[core_type]+\"-\"+InRec[sect_key]+'-'+InRec[half_key]+'-'+str(InRec[interval_key])\n                else: # mark as continuous measurements\n                    specimen=expedition+'-'+location+'-'+InRec['Core']+InRec[core_type]+\"_\"+InRec[sect_key]+InRec[half_key]+'-'+str(InRec[interval_key])\n                SpecRec['er_expedition_name']=expedition\n                SpecRec['er_location_name']=location\n                SpecRec['er_site_name']=specimen\n                SpecRec['er_citation_names']=citation\n                for key in list(SpecRec.keys()):SampRec[key]=SpecRec[key]\n                for key in list(SpecRec.keys()):SiteRec[key]=SpecRec[key]\n                SampRec['sample_azimuth']='0'\n                SampRec['sample_dip']='0'\n                SampRec['sample_core_depth']=InRec[depth_key]\n                if comp_depth_key!='':\n                    SampRec['sample_composite_depth']=InRec[comp_depth_key]\n                if \"SHLF\" not in InRec[text_id]:\n                    SampRec['magic_method_codes']='FS-C-DRILL-IODP:SP-SS-C:SO-V'\n                else:\n                    SampRec['magic_method_codes']='FS-C-DRILL-IODP:SO-V'\n                SpecRec['er_specimen_name']=specimen\n                SpecRec['er_sample_name']=specimen\n                SampRec['er_sample_name']=specimen\n                SampRec['er_specimen_names']=specimen\n                SiteRec['er_specimen_names']=specimen\n\n                for key in list(SpecRec.keys()):MagRec[key]=SpecRec[key]\n# set up measurement record - default is NRM\n                #MagRec['er_analyst_mail_names']=InRec['Test Entered By']\n                MagRec['magic_software_packages']=version_num\n                MagRec[\"treatment_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"measurement_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"treatment_ac_field\"]=0\n                MagRec[\"treatment_dc_field\"]='0'\n                MagRec[\"treatment_dc_field_phi\"]='0'\n                MagRec[\"treatment_dc_field_theta\"]='0'\n                MagRec[\"measurement_flag\"]='g' # assume all data are \"good\"\n                MagRec[\"measurement_standard\"]='u' # assume all data are \"good\"\n                SpecRec['er_specimen_alternatives']=InRec[text_id]\n                if 'Sample Area (cm?)' in list(InRec.keys()) and  InRec['Sample Area (cm?)']!= \"\": volume=InRec['Sample Area (cm?)']\n                if InRec[run_number_key]!= \"\": run_number=InRec[run_number_key]\n                datestamp=InRec[date_key].split() # date time is second line of file\n                if '/' in datestamp[0]:\n                    mmddyy=datestamp[0].split('/') # break into month day year\n                    if len(mmddyy[0])==1: mmddyy[0]='0'+mmddyy[0] # make 2 characters\n                    if len(mmddyy[1])==1: mmddyy[1]='0'+mmddyy[1] # make 2 characters\n                    if len(datestamp[1])==1: datestamp[1]='0'+datestamp[1] # make 2 characters\n                    date='20'+mmddyy[2]+':'+mmddyy[0]+\":\"+mmddyy[1] +':' +datestamp[1]+\":00.00\"\n                if '-' in datestamp[0]:\n                    mmddyy=datestamp[0].split('-') # break into month day year\n                    date=mmddyy[0]+':'+mmddyy[1]+\":\"+mmddyy[2] +':' +datestamp[1]+\":00.00\"\n                MagRec[\"measurement_date\"]=date\n                MagRec[\"magic_method_codes\"]='LT-NO'\n                if InRec[demag_key]!=\"0\":\n                    MagRec['magic_method_codes'] = 'LT-AF-Z'\n                    inst=inst+':IODP-SRM-AF' # measured on shipboard in-line 2G AF\n                    treatment_value=float(InRec[demag_key].strip('\"'))*1e-3 # convert mT => T\n                    MagRec[\"treatment_ac_field\"]=treatment_value # AF demag in treat mT => T\n                if 'Treatment Type' in list(InRec.keys()) and InRec['Treatment Type']!=\"\":\n                    if 'Alternating Frequency' in InRec['Treatment Type']:\n                        MagRec['magic_method_codes'] = 'LT-AF-Z'\n                        inst=inst+':I`ODP-DTECH' # measured on shipboard Dtech D2000\n                        treatment_value=float(InRec['Treatment Value'])*1e-3 # convert mT => T\n                        MagRec[\"treatment_ac_field\"]=treatment_value # AF demag in treat mT => T\n                    elif 'Thermal' in InRec['Treatment Type']:\n                        MagRec['magic_method_codes'] = 'LT-T-Z'\n                        inst=inst+':IODP-TDS' # measured on shipboard Schonstedt thermal demagnetizer\n                        treatment_value=float(InRec['Treatment Value'])+273 # convert C => K\n                        MagRec[\"treatment_temp\"]='%8.3e'%(treatment_value) #\n                MagRec[\"measurement_standard\"]='u' # assume all data are \"good\"\n                vol=float(volume)*1e-6 # convert from cc to m^3\n                if run_number!=\"\":\n                    MagRec['external_database_ids']=run_number\n                    MagRec['external_database_names']='LIMS'\n                else:\n                    MagRec['external_database_ids']=\"\"\n                    MagRec['external_database_names']=''\n                MagRec['measurement_inc']=InRec[inc_key].strip('\"')\n                MagRec['measurement_dec']=InRec[dec_key].strip('\"')\n                intens= InRec[int_key].strip('\"')\n                MagRec['measurement_magn_moment']='%8.3e'%(float(intens)*vol) # convert intensity from A/m to Am^2 using vol\n                MagRec['magic_instrument_codes']=inst\n                MagRec['measurement_number']='1'\n                MagRec['measurement_csd']=''\n                MagRec['measurement_positions']=''\n                MagRecs.append(MagRec)\n                if specimen not in specimens:\n                    specimens.append(specimen)\n                    SpecRecs.append(SpecRec)\n                if MagRec['er_sample_name']  not in samples:\n                    samples.append(MagRec['er_sample_name'])\n                    SampRecs.append(SampRec)\n                if MagRec['er_site_name']  not in sites:\n                    sites.append(MagRec['er_site_name'])\n                    SiteRecs.append(SiteRec)\n              #except:\n              #   print 'Boo-boo somewhere - no idea where'\n    if not file_found:\n        print(\"No .csv files were found\")\n        return False, \"No .csv files were found\"\n    if len(SpecRecs)>0:\n        print('spec_file', spec_file)\n        pmag.magic_write(spec_file,SpecRecs,'er_specimens')\n        #print 'specimens stored in ',spec_file\n    if len(SampRecs)>0:\n        SampOut,keys=pmag.fillkeys(SampRecs)\n        pmag.magic_write(samp_file,SampOut,'er_samples')\n        #print 'samples stored in ',samp_file\n    if len(SiteRecs)>0:\n        pmag.magic_write(site_file,SiteRecs,'er_sites')\n        #print 'sites stored in ',site_file\n    MagSort=pmag.sortbykeys(MagRecs,[\"er_specimen_name\",\"treatment_ac_field\"])\n    MagOuts=[]\n    for MagRec in MagSort:\n       MagRec[\"treatment_ac_field\"]='%8.3e'%(MagRec['treatment_ac_field']) # convert to string\n       MagOuts.append(MagRec)\n    Fixed=pmag.measurements_methods(MagOuts,noave)\n    if pmag.magic_write(meas_file,Fixed,'magic_measurements'):\n        print('data stored in ',meas_file)\n        return True, meas_file\n    else:\n        print('no data found.  bad magfile?')\n        return False, 'no data found.  bad magfile?'", "response": "This function is the main function for the iodp_srm_magic. py script."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    citation='This study'\n    MeasRecs=[]\n    units='cgs'\n    meth=\"LP-HYS\"\n    version_num=pmag.get_version()\n    args=sys.argv\n    fmt='old'\n    er_sample_name,er_site_name,er_location_name=\"\",\"\",\"\"\n    inst=\"\"\n    er_location_name=\"unknown\"\n    er_synthetic_name=\"\"\n    user=\"\"\n    er_site_name=\"\"\n    dir_path='.'\n    dm=3\n    if \"-WD\" in args:\n        ind=args.index(\"-WD\")\n        dir_path=args[ind+1]\n    if \"-ID\" in args:\n        ind = args.index(\"-ID\")\n        input_dir_path = args[ind+1]\n    else:\n        input_dir_path = dir_path\n    output_dir_path = dir_path\n    specfile = output_dir_path+'/er_specimens.txt'\n    output = output_dir_path+\"/agm_measurements.txt\"\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    if \"-bak\" in args:\n        meth=\"LP-IRM-DCD\"\n        output = output_dir_path+\"/irm_measurements.txt\"\n    if \"-new\" in args: fmt='new'\n    if \"-usr\" in args:\n        ind=args.index(\"-usr\")\n        user=args[ind+1]\n    if '-F' in args:\n        ind=args.index(\"-F\")\n        output = output_dir_path+'/'+args[ind+1]\n    if '-f' in args:\n        ind=args.index(\"-f\")\n        agm_file= input_dir_path+'/'+args[ind+1]\n        er_specimen_name=args[ind+1].split('.')[0]\n    else:\n        print(\"agm_file field is required option\")\n        print(main.__doc__)\n        sys.exit()\n    if '-Fsp' in args:\n        ind=args.index(\"-Fsp\")\n        specfile= output_dir_path+'/'+args[ind+1]\n    specnum,samp_con,Z=0,'1',1\n    if \"-spc\" in args:\n        ind=args.index(\"-spc\")\n        specnum=int(args[ind+1])\n        if specnum!=0:specnum=-specnum\n    if \"-spn\" in args:\n        ind=args.index(\"-spn\")\n        er_specimen_name=args[ind+1]\n    #elif \"-syn\" not in args:\n    #    print \"you must specify a specimen name\"\n    #    sys.exit()\n    if \"-syn\" in args:\n        ind=args.index(\"-syn\")\n        er_synthetic_name=args[ind+1]\n        er_specimen_name=\"\"\n    if \"-loc\" in args:\n        ind=args.index(\"-loc\")\n        er_location_name=args[ind+1]\n    if \"-fsa\" in args:\n        ind=args.index(\"-fsa\")\n        sampfile = input_dir_path+'/'+args[ind+1]\n        Samps,file_type=pmag.magic_read(sampfile)\n        print('sample_file successfully read in')\n    if \"-ncn\" in args:\n        ind=args.index(\"-ncn\")\n        samp_con=sys.argv[ind+1]\n        if \"4\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [4] must be in form 4-Z where Z is an integer\")\n                sys.exit()\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"4\"\n        if \"7\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                sys.exit()\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"7\"\n    if \"-ins\" in args:\n        ind=args.index(\"-ins\")\n        inst=args[ind+1]\n    if \"-u\" in args:\n        ind=args.index(\"-u\")\n        units=args[ind+1]\n    dm = pmag.get_named_arg(\"-DM\", 2)\n    ErSpecRecs,filetype=pmag.magic_read(specfile)\n    ErSpecRec,MeasRec={},{}\n    ErSpecRec['er_citation_names']=\"This study\"\n    ErSpecRec['er_specimen_name']=er_specimen_name\n    ErSpecRec['er_synthetic_name']=er_synthetic_name\n    if specnum!=0:\n        ErSpecRec[\"er_sample_name\"]=er_specimen_name[:specnum]\n    else:\n        ErSpecRec[\"er_sample_name\"]=er_specimen_name\n    if \"-fsa\" in args and er_synthetic_name==\"\":\n        for samp in Samps:\n            if samp[\"er_sample_name\"] == ErSpecRec[\"er_sample_name\"]:\n                ErSpecRec[\"er_location_name\"]=samp[\"er_location_name\"]\n                ErSpecRec[\"er_site_name\"]=samp[\"er_site_name\"]\n                break\n    elif int(samp_con)!=6 and int(samp_con)!=8:\n        site=pmag.parse_site(ErSpecRec['er_sample_name'],samp_con,Z)\n        ErSpecRec[\"er_site_name\"]=site\n        ErSpecRec[\"er_location_name\"]=er_location_name\n    ErSpecRec['er_scientist_mail_names']=user.strip()\n    insert=1\n    for rec in ErSpecRecs:\n        if rec['er_specimen_name']==er_specimen_name:\n            insert=0\n            break\n    if insert==1:\n        ErSpecRecs.append(ErSpecRec)\n        ErSpecRecs,keylist=pmag.fillkeys(ErSpecRecs)\n        pmag.magic_write(specfile,ErSpecRecs,'er_specimens')\n        print(\"specimen name put in \",specfile)\n    f=open(agm_file,'r')\n    Data=f.readlines()\n    if \"ASCII\" not in Data[0]:fmt='new'\n    measnum,start=1,\"\"\n    if fmt=='new': # new Micromag formatted file\n        end=2\n        for skip in range(len(Data)):\n            line=Data[skip]\n            rec=line.split()\n            if 'Units' in line:units=rec[-1]\n            if \"Raw\" in rec:\n                start=skip+2\n            if \"Field\" in rec and \"Moment\" in rec and start==\"\":\n                start=skip+2\n                break\n    else:\n        start = 2\n        end=1\n    for i in range(start,len(Data)-end): # skip header stuff\n\n        MeasRec={}\n        for key in list(ErSpecRec.keys()):\n            MeasRec[key]=ErSpecRec[key]\n        MeasRec['magic_instrument_codes']=inst\n        MeasRec['magic_method_codes']=meth\n        if 'er_synthetic_name' in list(MeasRec.keys()) and MeasRec['er_synthetic_name']!=\"\":\n            MeasRec['magic_experiment_name']=er_synthetic_name+':'+meth\n        else:\n            MeasRec['magic_experiment_name']=er_specimen_name+':'+meth\n        line=Data[i]\n        rec=line.split(',') # data comma delimited\n        if rec[0]!='\\n':\n            if units=='cgs':\n                field =float(rec[0])*1e-4 # convert from oe to tesla\n            else:\n                field =float(rec[0]) # field in tesla\n            if meth==\"LP-HYS\":\n                MeasRec['measurement_lab_field_dc']='%10.3e'%(field)\n                MeasRec['treatment_dc_field']=''\n            else:\n                MeasRec['measurement_lab_field_dc']=''\n                MeasRec['treatment_dc_field']='%10.3e'%(field)\n            if units=='cgs':\n                MeasRec['measurement_magn_moment']='%10.3e'%(float(rec[1])*1e-3) # convert from emu to Am^2\n            else:\n                MeasRec['measurement_magn_moment']='%10.3e'%(float(rec[1])) # Am^2\n            MeasRec['treatment_temp']='273' # temp in kelvin\n            MeasRec['measurement_temp']='273' # temp in kelvin\n            MeasRec['measurement_flag']='g'\n            MeasRec['measurement_standard']='u'\n            MeasRec['measurement_number']='%i'%(measnum)\n            measnum+=1\n            MeasRec['magic_software_packages']=version_num\n            MeasRecs.append(MeasRec)\n# now we have to relabel LP-HYS method codes.  initial loop is LP-IMT, minor loops are LP-M  - do this in measurements_methods function\n    if meth=='LP-HYS':\n        recnum=0\n        while float(MeasRecs[recnum]['measurement_lab_field_dc'])<float(MeasRecs[recnum+1]['measurement_lab_field_dc']) and recnum+1<len(MeasRecs): # this is LP-IMAG\n            MeasRecs[recnum]['magic_method_codes']='LP-IMAG'\n            MeasRecs[recnum]['magic_experiment_name']=MeasRecs[recnum]['er_specimen_name']+\":\"+'LP-IMAG'\n            recnum+=1\n#\n    if int(dm)==2:\n        pmag.magic_write(output,MeasRecs,'magic_measurements')\n    else:\n        print ('MagIC 3 is not supported yet')\n        sys.exit()\n        pmag.magic_write(output,MeasRecs,'measurements')\n\n    print(\"results put in \", output)", "response": "This function is the main entry point for the magic command line tool. It is used to convert Micromag agm files to IRM backfield curves and return the resulting IRM backfield curves."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_okButton(self, event):\n        os.chdir(self.WD)\n        options = {}\n        HUJI_file = self.bSizer0.return_value()\n        if not HUJI_file:\n            pw.simple_warning(\"You must select a HUJI format file\")\n            return False\n        options['magfile'] = HUJI_file\n        dat_file = self.bSizer0A.return_value()\n        if os.path.isfile(dat_file): options['datafile'] = dat_file\n        else: dat_file=\"\"\n        magicoutfile=os.path.split(HUJI_file)[1]+\".magic\"\n        outfile=os.path.join(self.WD, magicoutfile)\n        options['meas_file'] = outfile\n        magicoutfile=os.path.split(HUJI_file)[1]+\"_specimens.txt\"\n        spec_outfile=os.path.join(self.WD, magicoutfile)\n        options['spec_file'] = spec_outfile\n        magicoutfile=os.path.split(HUJI_file)[1]+\"_samples.txt\"\n        samp_outfile=os.path.join(self.WD, magicoutfile)\n        options['samp_file'] = samp_outfile\n        magicoutfile=os.path.split(HUJI_file)[1]+\"_sites.txt\"\n        site_outfile=os.path.join(self.WD, magicoutfile)\n        options['site_file'] = site_outfile\n        magicoutfile=os.path.split(HUJI_file)[1]+\"_locations.txt\"\n        loc_outfile=os.path.join(self.WD, magicoutfile)\n        options['loc_file'] = loc_outfile\n        user = self.bSizer1.return_value()\n        options['user'] = user\n        if user:\n            user = '-usr ' + user\n        experiment_type = self.bSizer2.return_value()\n        options['codelist'] = experiment_type\n        if not experiment_type:\n            pw.simple_warning(\"You must select an experiment type\")\n            return False\n        cooling_rate = self.cooling_rate.GetValue() or 0\n        if cooling_rate:\n            experiment_type = experiment_type + \" \" + cooling_rate\n        lab_field = self.bSizer3.return_value()\n        if not lab_field:\n            lab_field = \"0 0 0\"\n        lab_field_list = lab_field.split()\n        options['labfield'] = lab_field_list[0]\n        options['phi'] = lab_field_list[1]\n        options['theta'] = lab_field_list[2]\n        lab_field = '-dc ' + lab_field\n        spc = self.bSizer4.return_value()\n        options['specnum'] = spc or 0\n        if not spc:\n            spc = '-spc 0'\n        else:\n            spc = '-spc ' + spc\n        ncn = self.bSizer5.return_value()\n        options['samp_con'] = ncn\n        loc_name = self.bSizer6.return_value()\n        options['location'] = loc_name\n        if loc_name:\n            loc_name = '-loc ' + loc_name\n        #peak_AF = self.bSizer7.return_value()\n        #options['peakfield'] = peak_AF\n\n        replicate = self.bSizer8.return_value()\n        if replicate:\n            options['noave'] = 0\n            replicate = ''\n        else:\n            options['noave'] = 1\n            replicate = '-A'\n\n        COMMAND = \"huji_magic_new.py -f {} -fd {} -F {} -Fsp {} -Fsa {} -Fsi {} -Flo {} {} -LP {} {} -ncn {} {} {} {}\".format(HUJI_file, dat_file, outfile, spec_outfile, samp_outfile, site_outfile, loc_outfile, user, experiment_type, loc_name, ncn, lab_field, spc, replicate)\n        program_ran, error_message = convert.huji(**options)\n        if program_ran:\n            pw.close_window(self, COMMAND, outfile)\n        else:\n            pw.simple_warning(error_message)", "response": "This method is called when the user selects a new HUJI file and runs huji_magic. py with the appropriate flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_sheet(self):\n        '''\n        create an editable grid showing demag_orient.txt\n        '''\n        #--------------------------------\n        # orient.txt supports many other headers\n        # but we will only initialize with\n        # the essential headers for\n        # sample orientation and headers present\n        # in existing demag_orient.txt file\n        #--------------------------------\n\n\n        #--------------------------------\n        # create the grid\n        #--------------------------------\n\n        samples_list = list(self.orient_data.keys())\n        samples_list.sort()\n        self.samples_list = [ sample for sample in samples_list if sample is not \"\" ]\n        #self.headers.extend(self.add_extra_headers(samples_list))\n        display_headers = [header[1] for header in self.headers]\n        self.grid = magic_grid.MagicGrid(self.panel, 'orient grid',\n                                         self.samples_list, display_headers)\n        self.grid.InitUI()\n\n        #--------------------------------\n        # color the columns by groups\n        #--------------------------------\n\n        for i in range(len(self.samples_list)):\n            self.grid.SetCellBackgroundColour(i, 0, \"LIGHT GREY\")\n            self.grid.SetCellBackgroundColour(i, 1, \"LIGHT STEEL BLUE\")\n            self.grid.SetCellBackgroundColour(i, 2, \"YELLOW\")\n            self.grid.SetCellBackgroundColour(i, 3, \"YELLOW\")\n            self.grid.SetCellBackgroundColour(i, 4, \"PALE GREEN\")\n            self.grid.SetCellBackgroundColour(i, 5, \"PALE GREEN\")\n            self.grid.SetCellBackgroundColour(i, 6, \"KHAKI\")\n            self.grid.SetCellBackgroundColour(i, 7, \"KHAKI\")\n            self.grid.SetCellBackgroundColour(i, 8, \"KHAKI\")\n            self.grid.SetCellBackgroundColour(i, 9, \"KHAKI\")\n            self.grid.SetCellBackgroundColour(i, 10, \"KHAKI\")\n            self.grid.SetCellBackgroundColour(i, 11, \"LIGHT MAGENTA\")\n            self.grid.SetCellBackgroundColour(i, 12, \"LIGHT MAGENTA\")\n\n\n        #--------------------------------\n        # fill data from self.orient_data\n        #--------------------------------\n\n        headers = [header[0] for header in self.headers]\n        for sample in self.samples_list:\n            for key in list(self.orient_data[sample].keys()):\n                if key in headers:\n                    sample_index = self.samples_list.index(sample)\n                    i = headers.index(key)\n                    val = str(self.orient_data[sample][key])\n                    # if it's a pmag_object, use its name\n                    try:\n                        val = val.name\n                    except AttributeError:\n                        pass\n                    if val and val != \"None\":\n                        self.grid.SetCellValue(sample_index, i, val)\n\n        #--------------------------------\n\n        #--------------------------------\n        # fill in some default values\n        #--------------------------------\n        for row in range(self.grid.GetNumberRows()):\n            col = 1\n            if not self.grid.GetCellValue(row, col):\n                self.grid.SetCellValue(row, col, 'g')\n\n        #--------------------------------\n\n        # temporary trick to get drop-down-menus to work\n        self.grid.changes = {'a'}\n\n        self.grid.AutoSize()\n        #self.drop_down_menu = drop_down_menus.Menus(\"orient\", self, self.grid, '')\n        self.drop_down_menu = drop_down_menus3.Menus(\"orient\", self.contribution, self.grid)\n        self.Bind(wx.grid.EVT_GRID_LABEL_LEFT_CLICK, self.onLeftClickLabel, self.grid)", "response": "create an editable grid showing the demag_orient. txt file and the headers present in the file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_m_open_file(self,event):\n        '''\n        open orient.txt\n        read the data\n        display the data from the file in a new grid\n        '''\n        dlg = wx.FileDialog(\n            self, message=\"choose orient file\",\n            defaultDir=self.WD,\n            defaultFile=\"\",\n            style=wx.FD_OPEN | wx.FD_CHANGE_DIR\n            )\n        if dlg.ShowModal() == wx.ID_OK:\n            orient_file = dlg.GetPath()\n            dlg.Destroy()\n            new_data, dtype, keys = pmag.magic_read_dict(orient_file,\n                                                         sort_by_this_name=\"sample_name\",\n                                                         return_keys=True)\n\n            if len(new_data) > 0:\n                self.orient_data={}\n                self.orient_data=new_data\n            #self.create_sheet()\n            self.update_sheet()\n            print(\"-I- If you don't see a change in the spreadsheet, you may need to manually re-size the window\")", "response": "open the file in the current working directory read the data from the file in a new grid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_extra_headers(self, sample_names):\n        if not sample_names:\n            return []\n        full_headers = list(self.orient_data[sample_names[0]].keys())\n        add_ons = []\n        for head in full_headers:\n            if head not in self.header_names:\n                add_ons.append((head, head))\n        return add_ons", "response": "Add any additional headers that are not in the default headers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens the file in the current working directory read the data from the file display the data in a new grid", "response": "def on_m_open_file(self,event):\n        '''\n        open orient.txt\n        read the data\n        display the data from the file in a new grid\n        '''\n        dlg = wx.FileDialog(\n            self, message=\"choose orient file\",\n            defaultDir=self.WD,\n            defaultFile=\"\",\n            style=wx.FD_OPEN | wx.FD_CHANGE_DIR\n            )\n        if dlg.ShowModal() == wx.ID_OK:\n            orient_file = dlg.GetPath()\n            dlg.Destroy()\n            new_data = self.er_magic_data.read_magic_file(orient_file, \"sample_name\")[0]\n            if len(new_data) > 0:\n                self.orient_data={}\n                self.orient_data=new_data\n            #self.create_sheet()\n            self.update_sheet()\n            print(\"-I- If you don't see a change in the spreadsheet, you may need to manually re-size the window\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_m_save_file(self,event):\n\n        '''\n        save demag_orient.txt\n        (only the columns that appear on the grid frame)\n        '''\n        fout = open(os.path.join(self.WD, \"demag_orient.txt\"), 'w')\n        STR = \"tab\\tdemag_orient\\n\"\n        fout.write(STR)\n        headers = [header[0] for header in self.headers]\n        STR = \"\\t\".join(headers) + \"\\n\"\n        fout.write(STR)\n        for sample in self.samples_list:\n            STR = \"\"\n            for header in headers:\n                sample_index = self.samples_list.index(sample)\n                i = headers.index(header)\n                value = self.grid.GetCellValue(sample_index, i)\n                STR = STR + value + \"\\t\"\n            fout.write(STR[:-1] + \"\\n\")\n        if event != None:\n            dlg1 = wx.MessageDialog(None,caption=\"Message:\", message=\"data saved in file demag_orient.txt\" ,style=wx.OK|wx.ICON_INFORMATION)\n            dlg1.ShowModal()\n            dlg1.Destroy()", "response": "save demag_orient.txt\n        (only the columns that appear on the grid frame)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nname mk_redo. py - h prints help message and quits countryCode prints a zeq_redo format file", "response": "def main():\n    \"\"\"\n    NAME\n        mk_redo.py\n\n    DESCRIPTION\n        Makes thellier_redo and zeq_redo files from existing pmag_specimens format file\n\n    SYNTAX\n        mk_redo.py [-h] [command line options]\n\n    INPUT\n        takes specimens.txt formatted input file\n\n    OPTIONS\n        -h: prints help message and quits\n        -f FILE: specify input file, default is 'specimens.txt'\n        -F REDO: specify output file suffix, default is redo so that\n            output filenames are 'thellier_redo' for thellier data and 'zeq_redo' for direction only data\n\n    OUTPUT\n        makes a thellier_redo or a zeq_redo format file\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    zfile, tfile = 'zeq_redo', 'thellier_redo'\n    zredo, tredo = \"\", \"\"\n    dir_path = pmag.get_named_arg('-WD', '.')\n    inspec = pmag.get_named_arg('-f', 'specimens.txt')\n    if '-F' in sys.argv:\n        ind = sys.argv.index('-F')\n        redo = sys.argv[ind + 1]\n        tfile = redo\n        zfile = redo\n    inspec = pmag.resolve_file_name(inspec, dir_path)\n    zfile = pmag.resolve_file_name(zfile, dir_path)\n    tfile = pmag.resolve_file_name(tfile, dir_path)\n#\n# read in data\n#\n    specs = []\n    prior_spec_data, file_type = pmag.magic_read(inspec)\n    if file_type != 'specimens':\n        print(file_type, \" this is not a valid pmag_specimens file\")\n        sys.exit()\n    outstrings = []\n    for spec in prior_spec_data:\n        tmp = spec[\"method_codes\"].split(\":\")\n        meths = []\n        for meth in tmp:\n            methods = meth.strip().split('-')\n            for m in methods:\n                if m not in meths:\n                    meths.append(m)\n        if 'DIR' in meths:  # DE-BFL, DE-BFP or DE-FM\n            specs.append(spec['specimen'])\n            if 'dir_comp' in list(spec.keys()) and spec['dir_comp'] != \"\" and spec['dir_comp'] != \" \":\n                comp_name = spec['dir_comp']\n            else:\n                comp_name = string.ascii_uppercase[specs.count(\n                    spec['specimen']) - 1]\n            calculation_type = \"DE-BFL\"  # assume default calculation type is best-fit line\n            if \"BFP\" in meths:\n                calculation_type = 'DE-BFP'\n            elif \"FM\" in meths:\n                calculation_type = 'DE-FM'\n            if zredo == \"\":\n                zredo = open(zfile, \"w\")\n            outstring = '%s %s %s %s %s \\n' % (\n                spec[\"specimen\"], calculation_type, spec[\"meas_step_min\"], spec[\"meas_step_max\"], comp_name)\n            if outstring not in outstrings:\n                zredo.write(outstring)\n            outstrings.append(outstring)  # only writes unique interpretions\n        elif \"PI\" in meths and \"TRM\" in meths:   # thellier record\n            if tredo == \"\":\n                tredo = open(tfile, \"w\")\n            outstring = '%s %i %i \\n' % (spec[\"specimen\"], float(\n                spec[\"meas_step_min\"]), float(spec[\"meas_step_max\"]))\n            if outstring not in outstrings:\n                tredo.write(outstring)\n            outstrings.append(outstring)  # only writes unique interpretions\n    print('Redo files saved to: ', zfile, tfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_side(ls, side):\n    minx, miny, maxx, maxy = ls.bounds\n    points = {'left': [(minx, miny), (minx, maxy)],\n              'right': [(maxx, miny), (maxx, maxy)],\n              'bottom': [(minx, miny), (maxx, miny)],\n              'top': [(minx, maxy), (maxx, maxy)],}\n    return sgeom.LineString(points[side])", "response": "Given a shapely LineString which is assumed to be rectangular return the\n    line corresponding to a given side of the rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw ticks on the bottom x - axis of a Lambert Conformal projection.", "response": "def lambert_xticks(ax, ticks):\n    \"\"\"Draw ticks on the bottom x-axis of a Lambert Conformal projection.\"\"\"\n    te = lambda xy: xy[0]\n    lc = lambda t, n, b: np.vstack((np.zeros(n) + t, np.linspace(b[2], b[3], n))).T\n    xticks, xticklabels = _lambert_ticks(ax, ticks, 'bottom', lc, te)\n    ax.xaxis.tick_bottom()\n    ax.set_xticks(xticks)\n    ax.set_xticklabels([ax.xaxis.get_major_formatter()(xtick) for xtick in xticklabels])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lambert_yticks(ax, ticks):\n    te = lambda xy: xy[1]\n    lc = lambda t, n, b: np.vstack((np.linspace(b[0], b[1], n), np.zeros(n) + t)).T\n    yticks, yticklabels = _lambert_ticks(ax, ticks, 'left', lc, te)\n    ax.yaxis.tick_left()\n    ax.set_yticks(yticks)\n    ax.set_yticklabels([ax.yaxis.get_major_formatter()(ytick) for ytick in yticklabels])", "response": "Draw ricks on the left y - axis of a Lamber Conformal projection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the tick locations and labels for an axis of a Lambert Conformal projection.", "response": "def _lambert_ticks(ax, ticks, tick_location, line_constructor, tick_extractor):\n    \"\"\"Get the tick locations and labels for an axis of a Lambert Conformal projection.\"\"\"\n    outline_patch = sgeom.LineString(ax.outline_patch.get_path().vertices.tolist())\n    axis = find_side(outline_patch, tick_location)\n    n_steps = 30\n    extent = ax.get_extent(ccrs.PlateCarree())\n    _ticks = []\n    for t in ticks:\n        xy = line_constructor(t, n_steps, extent)\n        proj_xyz = ax.projection.transform_points(ccrs.Geodetic(), xy[:, 0], xy[:, 1])\n        xyt = proj_xyz[..., :2]\n        ls = sgeom.LineString(xyt.tolist())\n        locs = axis.intersection(ls)\n        if not locs:\n            tick = [None]\n        else:\n            tick = tick_extractor(locs.xy)\n        _ticks.append(tick[0])\n    # Remove ticks that aren't visible:    \n    ticklabels = copy(ticks)\n    while True:\n        try:\n            index = _ticks.index(None)\n        except ValueError:\n            break\n        _ticks.pop(index)\n        ticklabels.pop(index)\n    return _ticks, ticklabels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnaming umich_magic.py DESCRIPTION converts UMICH .mag format files to magic_measurements format files SYNTAX umich_magic.py [command line options] OPTIONS -h: prints the help message and quits. -usr USER: identify user, default is \"\" -f FILE: specify .mag format input file, required -fsa SAMPFILE : specify er_samples.txt file relating samples, site and locations names,default is none -F FILE: specify output file, default is magic_measurements.txt -spc NUM : specify number of characters to designate a specimen, default = 0 -loc LOCNAME : specify location/study name, must have either LOCNAME or SAMPFILE or be a synthetic -ncn NCON: specify naming convention: default is #1 below -A: don't average replicate measurements Sample naming convention: [1] XXXXY: where XXXX is an arbitrary length site designation and Y is the single character sample designation. e.g., TG001a is the first sample from site TG001. [default] [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length) [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length) [4-Z] XXXX[YYY]: YYY is sample designation with Z characters from site XXX [5] site name same as sample [6] site is entered under a separate column -- NOT CURRENTLY SUPPORTED [7-Z] [XXXX]YYY: XXXX is site designation with Z characters with sample name XXXXYYYY NB: all others you will have to customize your self or e-mail ltauxe@ucsd.edu for help. Format of UMICH .mag files: Spec Treat CSD Intensity Declination Inclination metadata string Spec: specimen name Treat: treatment step XXX T in Centigrade XXX AF in mT Intensity assumed to be total moment in 10^3 Am^2 (emu) Declination: Declination in specimen coordinate system Inclination: Declination in specimen coordinate system metatdata string: mm/dd/yy;hh:mm;[dC,mT];xx.xx;UNITS;USER;INST;NMEAS hh in 24 hours. dC or mT units of treatment XXX (see Treat above) for thermal or AF respectively xx.xxx DC field UNITS of DC field (microT, mT) INST: instrument code, number of axes, number of positions (e.g., G34 is 2G, three axes, measured in four positions) NMEAS: number of measurements in a single position (1,3,200...)", "response": "def main():\n    \"\"\"\n    NAME\n        umich_magic.py\n \n    DESCRIPTION\n        converts UMICH .mag format files to magic_measurements format files\n\n    SYNTAX\n        umich_magic.py [command line options]\n\n    OPTIONS\n        -h: prints the help message and quits.\n        -usr USER:   identify user, default is \"\"\n        -f FILE: specify .mag format input file, required\n        -fsa SAMPFILE : specify er_samples.txt file relating samples, site and locations names,default is none\n        -F FILE: specify output file, default is magic_measurements.txt\n        -spc NUM : specify number of characters to designate a  specimen, default = 0\n        -loc LOCNAME : specify location/study name, must have either LOCNAME or SAMPFILE or be a synthetic\n        -ncn NCON:  specify naming convention: default is #1 below\n        -A: don't average replicate measurements\n       Sample naming convention:\n            [1] XXXXY: where XXXX is an arbitrary length site designation and Y\n                is the single character sample designation.  e.g., TG001a is the\n                first sample from site TG001.    [default]\n            [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [4-Z] XXXX[YYY]:  YYY is sample designation with Z characters from site XXX\n            [5] site name same as sample\n            [6] site is entered under a separate column -- NOT CURRENTLY SUPPORTED\n            [7-Z] [XXXX]YYY:  XXXX is site designation with Z characters with sample name XXXXYYYY\n            NB: all others you will have to customize your self\n                 or e-mail ltauxe@ucsd.edu for help.\n \n        Format of UMICH .mag files:   \n        Spec Treat CSD Intensity Declination Inclination metadata string\n        Spec: specimen name\n        Treat:  treatment step\n            XXX T in Centigrade\n            XXX AF in mT\n         Intensity assumed to be total moment in 10^3 Am^2 (emu)\n         Declination:  Declination in specimen coordinate system\n         Inclination:  Declination in specimen coordinate system\n\n         metatdata string:  mm/dd/yy;hh:mm;[dC,mT];xx.xx;UNITS;USER;INST;NMEAS\n             hh in 24 hours.  \n             dC or mT units of treatment XXX (see Treat above) for thermal or AF respectively\n             xx.xxx   DC field\n             UNITS of DC field (microT, mT)\n             INST:  instrument code, number of axes, number of positions (e.g., G34 is 2G, three axes, \n                    measured in four positions)\n             NMEAS: number of measurements in a single position (1,3,200...)\n    \"\"\"\n# initialize some stuff\n    dir_path='.'\n    infile_type=\"mag\"\n    noave=0\n    methcode,inst=\"\",\"\"\n    phi,theta,peakfield,labfield=0,0,0,0\n    pTRM,MD,samp_con,Z=0,0,'1',1\n    missing=1\n    demag=\"N\"\n    er_location_name=\"\"\n    citation='This study'\n    args=sys.argv\n    methcode=\"LP-NO\"\n    samp_file,ErSamps='',[]\n    specnum=0\n#\n# get command line arguments\n#\n    meas_file=\"magic_measurements.txt\"\n    user=\"\"\n    if '-WD' in args:\n        ind=args.index(\"-WD\")\n        dir_path=args[ind+1]\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    if \"-usr\" in args:\n        ind=args.index(\"-usr\")\n        user=args[ind+1]\n    if '-F' in args:\n        ind=args.index(\"-F\")\n        meas_file=dir_path+'/'+args[ind+1]\n    if '-f' in args:\n        ind=args.index(\"-f\")\n        magfile=dir_path+'/'+args[ind+1]\n        try:\n            input=open(magfile,'r')\n        except:\n            print(\"bad mag file name\")\n            sys.exit()\n    else: \n        print(\"mag_file field is required option\")\n        print(main.__doc__)\n        sys.exit()\n    if \"-spc\" in args:\n        ind=args.index(\"-spc\")\n        specnum=int(args[ind+1])\n        if specnum!=0:specnum=-specnum\n    if \"-loc\" in args:\n        ind=args.index(\"-loc\")\n        er_location_name=args[ind+1]\n    if \"-fsa\" in args:\n        ind=args.index(\"-fsa\")\n        samp_file=dir_path+'/'+args[ind+1]\n        Samps,file_type=pmag.magic_read(samp_file)\n    if \"-A\" in args: noave=1\n    if \"-ncn\" in args:\n        ind=args.index(\"-ncn\")\n        samp_con=sys.argv[ind+1]\n        if \"4\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [4] must be in form 4-Z where Z is an integer\")\n                sys.exit()\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"4\"\n            samp_con=sys.argv[ind+1]\n        if \"7\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                sys.exit()\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"7\"\n    MagRecs,specs=[],[]\n    version_num=pmag.get_version()\n    if infile_type==\"mag\":\n        for line in input.readlines():\n            instcode=\"\"\n            if len(line)>2:\n                MagRec={}\n                MagRec['er_location_name']=er_location_name\n                MagRec['magic_software_packages']=version_num\n                MagRec[\"treatment_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"measurement_temp\"]='%8.3e' % (273) # room temp in kelvin\n                MagRec[\"treatment_ac_field\"]='0'\n                MagRec[\"treatment_dc_field\"]='0'\n                MagRec[\"treatment_dc_field_phi\"]='0'\n                MagRec[\"treatment_dc_field_theta\"]='0'\n                meas_type=\"LT-NO\"\n                rec=line.split()\n                labfield=0\n                code1=rec[6].split(';')\n                date=code1[0].split('/') # break date into mon/day/year\n                yy=int(date[2])\n                if yy <90:\n                    yyyy=str(2000+yy)\n                else: yyyy=str(1900+yy)\n                mm=int(date[0])\n                if mm<10:\n                    mm=\"0\"+str(mm)\n                else: mm=str(mm)\n                dd=int(date[1])\n                if dd<10:\n                    dd=\"0\"+str(dd)\n                else: dd=str(dd)\n                time=code1[1].split(':')\n                hh=int(time[0])\n                if hh<10:\n                    hh=\"0\"+str(hh)\n                else: hh=str(hh)\n                min=int(time[1])\n                if min<10:\n                   min= \"0\"+str(min)\n                else: min=str(min)\n                MagRec[\"measurement_date\"]=yyyy+\":\"+mm+\":\"+dd+\":\"+hh+\":\"+min+\":00.00\"\n                MagRec[\"measurement_time_zone\"]=''\n                instcode=''\n                if len(code1)>1:\n                    MagRec[\"measurement_positions\"]=code1[6][2]\n                else:\n                    MagRec[\"measurement_positions\"]=code1[7]   # takes care of awkward format with bubba and flo being different\n                if user==\"\":user=code1[5]\n                if code1[2][-1]=='C': demag=\"T\"\n                if code1[2]=='mT': demag=\"AF\"\n                treat=rec[1].split('.')\n                if len(treat)==1:treat.append('0')\n                if demag=='T' and treat!=0:\n                    meas_type=\"LT-T-Z\"\n                    MagRec[\"treatment_temp\"]='%8.3e' % (float(treat[0])+273.) # temp in kelvin\n                if demag==\"AF\":\n                    meas_type=\"LT-AF-Z\"\n                    MagRec[\"treatment_ac_field\"]='%8.3e' % (float(treat[0])*1e-3) # Af field in T\n                MagRec[\"treatment_dc_field\"]='0'\n                MagRec[\"er_specimen_name\"]=rec[0]\n                if rec[0] not in specs:specs.append(rec[0]) # get a list of specimen names\n                experiment=rec[0]+\":\"\n                MagRec[\"er_site_name\"]=\"\"\n                if specnum!=0:\n                    MagRec[\"er_sample_name\"]=rec[0][:specnum]\n                else:\n                    MagRec[\"er_sample_name\"]=rec[0]\n                if \"-fsa\" in args:\n                    for samp in Samps:\n                        if samp[\"er_sample_name\"] == MagRec[\"er_sample_name\"]: \n                            MagRec[\"er_location_name\"]=samp[\"er_location_name\"]\n                            MagRec[\"er_site_name\"]=samp[\"er_site_name\"]\n                            break\n                elif int(samp_con)!=6:\n                    site=pmag.parse_site(MagRec['er_sample_name'],samp_con,Z)\n                    MagRec[\"er_site_name\"]=site\n                if MagRec['er_site_name']==\"\":\n                    print('No site name found for: ',MagRec['er_specimen_name'],MagRec['er_sample_name'])\n                if MagRec[\"er_location_name\"]==\"\":\n                    print('no location name for: ',MagRec[\"er_specimen_name\"]) \n                if rec[1]==\".00\":rec[1]=\"0.00\"\n                MagRec[\"measurement_csd\"]=rec[2]\n                MagRec[\"measurement_magn_moment\"]='%10.3e'% (float(rec[3])*1e-3) # moment in Am^2 (from emu)\n                MagRec[\"measurement_dec\"]=rec[4]\n                MagRec[\"measurement_inc\"]=rec[5]\n                MagRec[\"magic_instrument_codes\"]=instcode\n                MagRec[\"er_analyst_mail_names\"]=user\n                MagRec[\"er_citation_names\"]=citation\n                MagRec[\"magic_method_codes\"]=meas_type\n                MagRec[\"measurement_flag\"]='g'\n                MagRec[\"er_specimen_name\"]=rec[0]\n                MagRec[\"measurement_number\"]='1'\n                MagRecs.append(MagRec) \n    MagOuts=[]\n    for spec in specs:  # gather all demag types for this specimen\n        SpecRecs,meths,measnum=[],[],1\n        for rec in MagRecs:\n            if rec['er_specimen_name']==spec:\n                rec['measurement_number']=str(measnum)\n                measnum+=1\n                if rec['magic_method_codes'] not in meths:meths.append(rec['magic_method_codes'])\n                SpecRecs.append(rec)\n        expname=spec\n        if \"LT-AF-Z\" in meths:expname=expname+ ':LP-DIR-AF'\n        if \"LT-T-Z\" in meths:expname=expname+ ':LP-DIR-T'\n        for rec in SpecRecs:\n            rec['magic_experiment_name']=expname\n            MagOuts.append(rec)\n    pmag.magic_write(meas_file,MagOuts,'magic_measurements')\n    print(\"results put in \",meas_file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot intensity decay curves for demagnetization experiments Parameters ---------- in_file : str, default \"measurements.txt\" dir_path : str output directory, default \".\" input_dir_path : str input file directory (if different from dir_path), default \"\" spec_file : str input specimen file name, default \"specimens.txt\" samp_file: str input sample file name, default \"samples.txt\" site_file : str input site file name, default \"sites.txt\" loc_file : str input location file name, default \"locations.txt\" plot_by : str [spc, sam, sit, loc] (specimen, sample, site, location), default \"loc\" LT : str lab treatment [T, AF, M], default AF norm : bool normalize by NRM magnetization, default True XLP : str exclude specific lab protocols, (for example, method codes like LP-PI) default \"\" save_plots : bool plot and save non-interactively, default True fmt : str [\"png\", \"svg\", \"pdf\", \"jpg\"], default \"svg\" Returns --------- type - Tuple : (True or False indicating if conversion was sucessful, file name(s) written)", "response": "def dmag_magic(in_file=\"measurements.txt\", dir_path=\".\", input_dir_path=\"\",\n         spec_file=\"specimens.txt\", samp_file=\"samples.txt\",\n         site_file=\"sites.txt\", loc_file=\"locations.txt\",\n         plot_by=\"loc\", LT=\"AF\", norm=True, XLP=\"\",\n         save_plots=True, fmt=\"svg\"):\n\n    \"\"\"\n    plots intensity decay curves for demagnetization experiments\n\n    Parameters\n    ----------\n    in_file : str, default \"measurements.txt\"\n    dir_path : str\n        output directory, default \".\"\n    input_dir_path : str\n        input file directory (if different from dir_path), default \"\"\n    spec_file : str\n        input specimen file name, default \"specimens.txt\"\n    samp_file: str\n        input sample file name, default \"samples.txt\"\n    site_file : str\n        input site file name, default \"sites.txt\"\n    loc_file : str\n        input location file name, default \"locations.txt\"\n    plot_by : str\n        [spc, sam, sit, loc] (specimen, sample, site, location), default \"loc\"\n    LT : str\n        lab treatment [T, AF, M], default AF\n    norm : bool\n        normalize by NRM magnetization, default True\n    XLP : str\n        exclude specific  lab protocols, (for example, method codes like LP-PI)\n        default \"\"\n    save_plots : bool\n        plot and save non-interactively, default True\n    fmt : str\n        [\"png\", \"svg\", \"pdf\", \"jpg\"], default \"svg\"\n\n    Returns\n    ---------\n    type - Tuple : (True or False indicating if conversion was sucessful, file name(s) written)\n\n    \"\"\"\n    dir_path = os.path.realpath(dir_path)\n    if not input_dir_path:\n        input_dir_path = dir_path\n    input_dir_path = os.path.realpath(input_dir_path)\n\n    # format plot_key\n    name_dict = {'loc': 'location', 'sit': 'site',\n                 'sam': 'sample', 'spc': 'specimen'}\n    if plot_by not in name_dict.values():\n        try:\n            plot_key = name_dict[plot_by]\n        except KeyError:\n            print('Unrecognized plot_by {}, falling back to plot by location'.format(plot_by))\n            plot_key = \"loc\"\n    else:\n        plot_key = plot_by\n\n\n    # figure out what kind of experiment\n    LT = \"LT-\" + LT + \"-Z\"\n    print('LT', LT)\n    if LT == \"LT-T-Z\":\n        units, dmag_key = 'K', 'treat_temp'\n    elif LT == \"LT-AF-Z\":\n        units, dmag_key = 'T', 'treat_ac_field'\n    elif LT == 'LT-M-Z':\n        units, dmag_key = 'J', 'treat_mw_energy'\n    else:\n        units = 'U'\n\n\n    # init\n    FIG = {}  # plot dictionary\n    FIG['demag'] = 1  # demag is figure 1\n    # create contribution and add required headers\n    fnames = {\"specimens\": spec_file, \"samples\": samp_file,\n              'sites': site_file, 'locations': loc_file}\n    if not os.path.exists(pmag.resolve_file_name(in_file, input_dir_path)):\n        print('-E- Could not find {}'.format(in_file))\n        return False, []\n    contribution = cb.Contribution(input_dir_path, single_file=in_file,\n                                   custom_filenames=fnames)\n    file_type = list(contribution.tables.keys())[0]\n    print(len(contribution.tables['measurements'].df), ' records read from ', in_file)\n    # add plot_key into measurements table\n    if plot_key not in contribution.tables['measurements'].df.columns:\n        #contribution.propagate_name_down(plot_key, 'measurements')\n        contribution.propagate_location_to_measurements()\n    data_container = contribution.tables[file_type]\n    # pare down to only records with useful data\n    # grab records that have the requested code\n    data_slice = data_container.get_records_for_code(LT)\n    # and don't have the offending code\n    data = data_container.get_records_for_code(XLP, incl=False, use_slice=True,\n                                               sli=data_slice, strict_match=False)\n\n    # make sure quality is in the dataframe\n    if 'quality' not in data.columns:\n        data['quality'] = 'g'\n    # get intensity key and make sure intensity data is not blank\n    intlist = ['magn_moment', 'magn_volume', 'magn_mass']\n    IntMeths = [col_name for col_name in data.columns if col_name in intlist]\n    # get rid of any entirely blank intensity columns\n    for col_name in IntMeths:\n        if not data[col_name].any():\n            data.drop(col_name, axis=1, inplace=True)\n    IntMeths = [col_name for col_name in data.columns if col_name in intlist]\n    if len(IntMeths) == 0:\n        print('-E- No intensity headers found')\n        return False, []\n\n    int_key = IntMeths[0] # plot first intensity method found - normalized to initial value anyway - doesn't matter which used\n    data = data[data[int_key].notnull()]\n    # make list of individual plots\n    # by default, will be by location_name\n    plotlist = data[plot_key].unique()\n    plotlist.sort()\n    pmagplotlib.plot_init(FIG['demag'], 5, 5)\n    last_plot = False\n    # iterate through and plot the data\n    for plot in plotlist:\n        if plot == plotlist[-1]:\n            last_plot = True\n        plot_data = data[data[plot_key] == plot].copy()\n        if not save_plots:\n            print(plot, 'plotting by: ', plot_key)\n        if len(plot_data) > 2:\n            title = plot\n            spcs = []\n            spcs = plot_data['specimen'].unique()\n            for spc in spcs:\n                INTblock = []\n                spec_data = plot_data[plot_data['specimen'] == spc]\n                for ind, rec in spec_data.iterrows():\n                    INTblock.append([float(rec[dmag_key]), 0, 0, float(rec[int_key]), 1, rec['quality']])\n                if len(INTblock) > 2:\n                    pmagplotlib.plot_mag(FIG['demag'], INTblock,\n                                       title, 0, units, norm)\n\n            if save_plots:\n                files = {}\n                for key in list(FIG.keys()):\n                    if pmagplotlib.isServer:\n                        files[key] = title + '_' + LT + '.' + fmt\n                        incl_dir = False\n                    else: # if not server, include directory in output path\n                        files[key] = os.path.join(dir_path, title + '_' + LT + '.' + fmt)\n                        incl_dir = True\n\n                pmagplotlib.save_plots(FIG, files, incl_directory=incl_dir)\n            else:\n                pmagplotlib.draw_figs(FIG)\n                prompt = \" S[a]ve to save plot, [q]uit,  Return to continue:  \"\n                ans = input(prompt)\n                if ans == 'q':\n                    return True, []\n                if ans == \"a\":\n                    files = {}\n                    for key in list(FIG.keys()):\n                        if pmagplotlib.isServer:\n                            files[key] = title + '_' + LT + '.' + fmt\n                            incl_dir = False\n                        else: # if not server, include directory in output path\n                            files[key] = os.path.join(dir_path, title + '_' + LT + '.' + fmt)\n                            incl_dir = True\n                    pmagplotlib.save_plots(FIG, files, incl_directory=incl_dir)\n            pmagplotlib.clearFIG(FIG['demag'])\n    if last_plot:\n        return True, []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnames dmag_magic.py DESCRIPTION plots intensity decay curves for demagnetization experiments SYNTAX dmag_magic -h [command line options] INPUT takes magic formatted measurements.txt files OPTIONS -h prints help message and quits -f FILE: specify input file, default is: measurements.txt -obj OBJ: specify object [loc, sit, sam, spc] for plot, default is by location -LT [AF,T,M]: specify lab treatment type, default AF -XLP [PI]: exclude specific lab protocols, (for example, method codes like LP-PI) -N do not normalize by NRM magnetization -sav save plots silently and quit -fmt [svg,jpg,png,pdf] set figure format [default is svg] NOTE loc: location (study); sit: site; sam: sample; spc: specimen", "response": "def main():\n    \"\"\"\n    NAME\n        dmag_magic.py\n\n    DESCRIPTION\n       plots intensity decay curves for demagnetization experiments\n\n    SYNTAX\n        dmag_magic -h [command line options]\n\n    INPUT\n       takes magic formatted measurements.txt files\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE: specify input file, default is: measurements.txt\n        -obj OBJ: specify  object  [loc, sit, sam, spc] for plot,\n               default is by location\n        -LT [AF,T,M]: specify lab treatment type, default AF\n        -XLP [PI]: exclude specific  lab protocols,\n               (for example, method codes like LP-PI)\n        -N do not normalize by NRM magnetization\n        -sav save plots silently and quit\n        -fmt [svg,jpg,png,pdf] set figure format [default is svg]\n    NOTE\n        loc: location (study); sit: site; sam: sample; spc: specimen\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    # initialize variables from command line + defaults\n    dir_path = pmag.get_named_arg(\"-WD\", default_val=\".\")\n    input_dir_path = pmag.get_named_arg('-ID', '')\n    if not input_dir_path:\n        input_dir_path = dir_path\n    in_file = pmag.get_named_arg(\"-f\", default_val=\"measurements.txt\")\n    in_file = pmag.resolve_file_name(in_file, input_dir_path)\n    if \"-ID\" not in sys.argv:\n        input_dir_path = os.path.split(in_file)[0]\n    plot_by = pmag.get_named_arg(\"-obj\", default_val=\"loc\")\n    LT = pmag.get_named_arg(\"-LT\", \"AF\")\n    no_norm = pmag.get_flag_arg_from_sys(\"-N\")\n    norm = False if no_norm else True\n    save_plots = pmag.get_flag_arg_from_sys(\"-sav\")\n    fmt = pmag.get_named_arg(\"-fmt\", \"svg\")\n    XLP = pmag.get_named_arg(\"-XLP\", \"\")\n    spec_file = pmag.get_named_arg(\"-fsp\", default_val=\"specimens.txt\")\n    samp_file = pmag.get_named_arg(\"-fsa\", default_val=\"samples.txt\")\n    site_file = pmag.get_named_arg(\"-fsi\", default_val=\"sites.txt\")\n    loc_file = pmag.get_named_arg(\"-flo\", default_val=\"locations.txt\")\n    dmag_magic(in_file, dir_path, input_dir_path, spec_file, samp_file,\n         site_file, loc_file, plot_by, LT, norm, XLP,\n         save_plots, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames watsons_v.py DESCRIPTION calculates Watson's V statistic from input files INPUT FORMAT takes dec/inc as first two columns in two space delimited files SYNTAX watsons_v.py [command line options] OPTIONS -h prints help message and quits -f FILE (with optional second) -f2 FILE (second file) -ant, flip antipodal directions to opposite direction in first file if only one file or flip all in second, if two files -P (don't save or show plot) -sav save figure and quit silently -fmt [png,svg,eps,pdf,jpg] format for saved figure OUTPUT Watson's V and the Monte Carlo Critical Value Vc. in plot, V is solid and Vc is dashed.", "response": "def main():\n    \"\"\"\n    NAME\n       watsons_v.py\n\n    DESCRIPTION\n       calculates Watson's V statistic from input files\n\n    INPUT FORMAT\n       takes dec/inc as first two columns in two space delimited files\n\n    SYNTAX\n       watsons_v.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE (with optional second)\n        -f2 FILE (second file)\n        -ant,  flip antipodal directions to opposite direction\n           in first file if only one file or flip all in second, if two files\n        -P  (don't save or show plot)\n        -sav save figure and quit silently\n        -fmt [png,svg,eps,pdf,jpg] format for saved figure\n\n    OUTPUT\n        Watson's V and the Monte Carlo Critical Value Vc.\n        in plot, V is solid and Vc is dashed.\n\n    \"\"\"\n    Flip=0\n    show,plot=1,0\n    fmt='svg'\n    file2=\"\"\n    if '-h' in sys.argv: # check if help is needed\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    if '-ant' in  sys.argv: Flip=1\n    if '-sav' in sys.argv: show,plot=0,1 # don't display, but do save plot\n    if '-fmt' in sys.argv:\n        ind=sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    if '-P' in  sys.argv: show=0 # don't display or save plot\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file1=sys.argv[ind+1]\n        data=numpy.loadtxt(file1).transpose()\n        D1=numpy.array([data[0],data[1]]).transpose()\n        file1_name=os.path.split(file1)[1].split('.')[0]\n    else:\n        print(\"-f is required\")\n        print(main.__doc__)\n        sys.exit()\n    if '-f2' in sys.argv:\n        ind=sys.argv.index('-f2')\n        file2=sys.argv[ind+1]\n        data2=numpy.loadtxt(file2).transpose()\n        D2=numpy.array([data2[0],data2[1]]).transpose()\n        file2_name=os.path.split(file2)[1].split('.')[0]\n        if Flip==1:\n            D2,D=pmag.flip(D2) # D2 are now flipped\n            if len(D2)!=0:\n                if len(D)!=0:\n                    D2=numpy.concatenate(D,D2) # put all in D2\n            elif len(D)!=0:\n                D2=D\n            else:\n                print('length of second file is zero')\n                sys.exit()\n    elif Flip==1:D2,D1=pmag.flip(D1) # peel out antipodal directions, put in D2\n#\n    counter,NumSims=0,5000\n#\n# first calculate the fisher means and cartesian coordinates of each set of Directions\n#\n    pars_1=pmag.fisher_mean(D1)\n    pars_2=pmag.fisher_mean(D2)\n#\n# get V statistic for these\n#\n    V=pmag.vfunc(pars_1,pars_2)\n#\n# do monte carlo simulation of datasets with same kappas, but common mean\n#\n    Vp=[] # set of Vs from simulations\n    if show==1:print(\"Doing \",NumSims,\" simulations\")\n    for k in range(NumSims):\n        counter+=1\n        if counter==50:\n            if show==1:print(k+1)\n            counter=0\n        Dirp=[]\n# get a set of N1 fisher distributed vectors with k1, calculate fisher stats\n        for i in range(pars_1[\"n\"]):\n            Dirp.append(pmag.fshdev(pars_1[\"k\"]))\n        pars_p1=pmag.fisher_mean(Dirp)\n# get a set of N2 fisher distributed vectors with k2, calculate fisher stats\n        Dirp=[]\n        for i in range(pars_2[\"n\"]):\n            Dirp.append(pmag.fshdev(pars_2[\"k\"]))\n        pars_p2=pmag.fisher_mean(Dirp)\n# get the V for these\n        Vk=pmag.vfunc(pars_p1,pars_p2)\n        Vp.append(Vk)\n#\n# sort the Vs, get Vcrit (95th one)\n#\n    Vp.sort()\n    k=int(.95*NumSims)\n    if show==1:\n        print(\"Watson's V,  Vcrit: \")\n        print('   %10.1f %10.1f'%(V,Vp[k]))\n    if show==1 or plot==1:\n        print(\"Watson's V,  Vcrit: \")\n        print('   %10.1f %10.1f'%(V,Vp[k]))\n        CDF={'cdf':1}\n        pmagplotlib.plot_init(CDF['cdf'],5,5)\n        pmagplotlib.plot_cdf(CDF['cdf'],Vp,\"Watson's V\",'r',\"\")\n        pmagplotlib.plot_vs(CDF['cdf'],[V],'g','-')\n        pmagplotlib.plot_vs(CDF['cdf'],[Vp[k]],'b','--')\n        if plot==0:pmagplotlib.draw_figs(CDF)\n        files={}\n        if pmagplotlib.isServer: # use server plot naming convention\n            if file2!=\"\":\n                files['cdf']='watsons_v_'+file1+'_'+file2+'.'+fmt\n            else:\n                files['cdf']='watsons_v_'+file1+'.'+fmt\n        else: # use more readable plot naming convention\n            if file2!=\"\":\n                files['cdf']='watsons_v_'+file1_name+'_'+file2_name+'.'+fmt\n            else:\n                files['cdf']='watsons_v_'+file1_name+'.'+fmt\n\n        if pmagplotlib.isServer:\n            black     = '#000000'\n            purple    = '#800080'\n            titles={}\n            titles['cdf']='Cumulative Distribution'\n            CDF = pmagplotlib.add_borders(CDF,titles,black,purple)\n            pmagplotlib.save_plots(CDF,files)\n        elif plot==0:\n            ans=input(\" S[a]ve to save plot, [q]uit without saving:  \")\n            if ans==\"a\": pmagplotlib.save_plots(CDF,files)\n        if plot==1: # save and quit silently\n            pmagplotlib.save_plots(CDF,files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    #\n    file='magic_measurements.txt'\n    methx,methy,fmt=\"\",\"\",'.svg'\n    plot_key=''\n    norm_by=\"\"\n    #plot=0\n    no_plot = pmag.get_flag_arg_from_sys('-sav')\n    if not no_plot:\n        do_plot = True\n    else:\n        do_plot = False\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n    if '-fmt' in sys.argv:\n        ind=sys.argv.index('-fmt')\n        fmt='.'+sys.argv[ind+1]\n    if '-n' in sys.argv:\n        ind=sys.argv.index('-n')\n        norm_by=sys.argv[ind+1]\n    xtreat_key,ytreat_key,xstep,ystep=\"\",\"\",\"\",\"\"\n    if '-x' in sys.argv:\n        ind=sys.argv.index('-x')\n        meths=sys.argv[ind+1].split(':')\n        methx=meths[0]\n        if len(meths)>1:\n            xtreat_key=meths[1]\n            xstep=float(meths[2])\n    if '-y' in sys.argv:\n        ind=sys.argv.index('-y')\n        meths=sys.argv[ind+1].split(':')\n        methy=meths[0]\n        if len(meths)>1:\n            ytreat_key=meths[1]\n            ystep=float(meths[2])\n    if '-obj' in sys.argv:\n        ind=sys.argv.index('-obj')\n        plot_by=sys.argv[ind+1]\n        if plot_by=='loc':plot_key='er_location_name'\n        if plot_by=='sit':plot_key='er_site_name'\n        if plot_by=='sam':plot_key='er_sample_name'\n        if plot_by=='spc':plot_key='er_specimen_name'\n    if '-h' in sys.argv:\n        do_plot = False\n    if '-i' in sys.argv:\n    #\n    # get name of file from command line\n    #\n        file=input(\"Input magic_measurments file name? [magic_measurements.txt] \")\n        if file==\"\":file=\"magic_measurements.txt\"\n    #\n    #\n    FIG={'fig':1}\n    pmagplotlib.plot_init(FIG['fig'],5,5)\n    data,file_type=pmag.magic_read(file)\n    if file_type!=\"magic_measurements\":\n        print(file_type,' not correct format for magic_measurments file')\n        sys.exit()\n    #\n    # collect method codes\n    methods,plotlist=[],[]\n    for rec in  data:\n        if plot_key!=\"\":\n            if rec[plot_key] not in plotlist:plotlist.append(rec[plot_key])\n        elif len(plotlist)==0:\n            plotlist.append('All')\n        meths=rec['magic_method_codes'].split(':')\n        for meth in meths:\n            if meth.strip() not in methods and meth.strip()!=\"LP-\":\n                methods.append(meth.strip())\n    #\n    if '-i' in sys.argv:\n        print(methods)\n    elif methx ==\"\" or methy==\"\":\n        print(methods)\n        sys.exit()\n    GoOn=1\n    while GoOn==1:\n        if '-i' in sys.argv:methx=input('Select method for x axis: ')\n        if methx not in methods:\n            if '-i' in sys.argv:\n                print('try again! method not available')\n            else:\n                print(main.__doc__)\n                print('\\n must specify X axis method\\n')\n                sys.exit()\n        else:\n            if pmagplotlib.verbose: print(methx, ' selected for X axis')\n            GoOn=0\n    GoOn=1\n    while GoOn==1:\n        if '-i' in sys.argv:methy=input('Select method for y axis: ')\n        if methy not in methods:\n            if '-i' in sys.argv:\n                print('try again! method not available')\n            else:\n                print(main.__doc__)\n                print('\\n must specify Y axis method\\n')\n                sys.exit()\n        else:\n            if pmagplotlib.verbose: print(methy, ' selected for Y axis')\n            GoOn=0\n    if norm_by==\"\":\n        measkeys=['measurement_magn_mass','measurement_magn_volume','measurement_magn_moment','measurement_magnitude','measurement_chi_volume','measurement_chi_mass','measurement_chi']\n    elif norm_by==\"V\":\n        measkeys=['measurement_magn_volume','measurement_chi_volume']\n    elif norm_by==\"M\":\n        measkeys=['measurement_magn_mass','measurement_chi_mass']\n    xmeaskey,ymeaskey=\"\",\"\"\n    plotlist.sort()\n    for plot in plotlist: # go through objects\n        if pmagplotlib.verbose:\n            print(plot)\n        X,Y=[],[]\n        x,y='',''\n        for rec in data:\n            if plot_key!=\"\" and rec[plot_key]!=plot:\n                pass\n            else:\n                meths=rec['magic_method_codes'].split(':')\n                for meth in meths:\n                    if meth.strip()==methx:\n                        if xmeaskey==\"\":\n                            for key in measkeys:\n                                if key in list(rec.keys()) and rec[key]!=\"\":\n                                    xmeaskey=key\n                                    if pmagplotlib.verbose:\n                                        print(xmeaskey,' being used for plotting X.')\n                                    break\n                    if meth.strip()==methy:\n                        if ymeaskey==\"\":\n                            for key in measkeys:\n                                if key in list(rec.keys()) and rec[key]!=\"\":\n                                    ymeaskey=key\n                                    if pmagplotlib.verbose:\n                                        print(ymeaskey,' being used for plotting Y')\n                                    break\n        if ymeaskey!=\"\" and xmeaskey!=\"\":\n            for rec in data:\n                x,y='',''\n                spec=rec['er_specimen_name'] # get the ydata for this specimen\n                if rec[ymeaskey]!=\"\" and methy in rec['magic_method_codes'].split(':'):\n                    if ytreat_key==\"\" or (ytreat_key in list(rec.keys()) and float(rec[ytreat_key])==ystep):\n                        y=float(rec[ymeaskey])\n                        for rec in data: # now find the xdata\n                            if rec['er_specimen_name']==spec and rec[xmeaskey]!=\"\" and methx in rec['magic_method_codes'].split(':'):\n                                if xtreat_key==\"\" or (xtreat_key in list(rec.keys()) and float(rec[xtreat_key])==xstep):\n                                    x=float(rec[xmeaskey])\n                if x != '' and y!= '':\n                    X.append(x)\n                    Y.append(y)\n        if len(X)>0:\n            pmagplotlib.clearFIG(FIG['fig'])\n            pmagplotlib.plot_xy(FIG['fig'],X,Y,sym='ro',xlab=methx,ylab=methy,title=plot+':Biplot')\n            if not pmagplotlib.isServer and do_plot:\n                pmagplotlib.draw_figs(FIG)\n                ans=input('S[a]ve plots, [q]uit,  Return for next plot ' )\n                if ans=='a':\n                    files={}\n                    for key in list(FIG.keys()): files[key]=plot+'_'+key+fmt\n                    pmagplotlib.save_plots(FIG,files)\n                if ans=='q':\n                    print(\"Good-bye\\n\")\n                    sys.exit()\n            else:\n                files={}\n                for key in list(FIG.keys()): files[key]=plot+'_'+key+fmt\n                if pmagplotlib.isServer:\n                    black     = '#000000'\n                    purple    = '#800080'\n                    titles={}\n                    titles['fig']='X Y Plot'\n                    FIG = pmagplotlib.add_borders(FIG,titles,black,purple)\n                pmagplotlib.save_plots(FIG,files)\n        else:\n            print('nothing to plot for ',plot)", "response": "This function is the main function for the biplot_magic. py script. It is the main function for the main function. It is used to create a new object of the same type and is used to create a new object of the same type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    xaxis, xplotind, yplotind = \"\", 0, 0  # (0 for strat pos)\n    yaxis, Xinc = \"\", \"\"\n    plot = 0\n    obj = 'all'\n    data_model_num = int(pmag.get_named_arg(\"-DM\", 3))\n    # 2.5 keys\n    if data_model_num == 2:\n        supported = ['pmag_specimens', 'pmag_samples',\n                     'pmag_sites', 'pmag_results', 'magic_web'] # available file types\n        Depth_keys = ['specimen_core_depth', 'specimen_height', 'specimen_elevation',\n                      'specimen_composite_depth', 'sample_core_depth', 'sample_height',\n                      'sample_elevation', 'sample_composite_depth', 'site_core_depth',\n                      'site_height', 'site_elevation', 'site_composite_depth', 'average_height']\n        Age_keys = ['specimen_inferred_age', 'sample_inferred_age',\n                    'site_inferred_age', 'average_age']\n        Unit_keys = {'specimen_inferred_age': 'specimen_inferred_age_unit',\n                     'sample_inferred_age': 'sample_inferred_age_unit',\n                     'site_inferred_age': 'site_inferred_age_unit', 'average_age': 'average_age_unit'}\n        Dec_keys = ['measurement_dec', 'specimen_dec',\n                    'sample_dec', 'site_dec', 'average_dec']\n        Inc_keys = ['measurement_inc', 'specimen_inc',\n                    'sample_inc', 'site_inc', 'average_inc']\n        Int_keys = ['measurement_magnitude', 'measurement_magn_moment', 'measurement_magn_volume',\n                    'measurement_magn_mass', 'specimen_int', 'specimen_int_rel', 'sample_int',\n                    'sample_int_rel', 'site_int', 'site_int_rel', 'average_int', 'average_int_rel']\n        Chi_keys = ['measurement_chi_volume', 'measurement_chi_mass']\n        Lat_keys = ['sample_lat', 'site_lat', 'average_lat']\n        VLat_keys = ['vgp_lat']\n        VLon_keys = ['vgp_lon']\n        Vdm_keys = ['vdm']\n        Vadm_keys = ['vadm']\n        method_col_name = \"magic_method_codes\"\n    else:\n        # 3.0 keys\n        supported = [\"specimens\", \"samples\", \"sites\", \"locations\"] # available file types\n        Depth_keys = [ \"height\", \"core_depth\", \"elevation\", \"composite_depth\" ]\n        Age_keys = [ \"age\" ]\n        Unit_keys = { \"age\": \"age\" }\n        Chi_keys = [ \"susc_chi_volume\", \"susc_chi_mass\" ]\n        Int_keys = [ \"magn_moment\", \"magn_volume\", \"magn_mass\", \"int_abs\", \"int_rel\" ]\n        Inc_keys = [ \"dir_inc\" ]\n        Dec_keys = [ \"dir_dec\" ]\n        Lat_Keys = [ \"lat\" ]\n        VLat_keys = [ \"vgp_lat\", \"pole_lat\" ]\n        VLon_keys = [ \"vgp_lon\", \"pole_lon\" ]\n        Vdm_keys = [ \"vdm\", \"pdm\" ]\n        Vadm_keys = [ \"vadm\", \"padm\" ]\n        method_col_name = \"method_codes\"\n\n    #\n    X_keys = [Age_keys, Depth_keys]\n    Y_keys = [Dec_keys, Inc_keys, Int_keys, Chi_keys,\n              VLat_keys, VLon_keys, Vdm_keys, Vadm_keys]\n    method, fmt = \"\", 'svg'\n    FIG = {'strat': 1}\n    plotexp, pTS = 0, 0\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    # default files\n    if data_model_num == 3:\n        res_file = pmag.get_named_arg(\"-f\", \"sites.txt\")\n    else:\n        res_file = pmag.get_named_arg(\"-f\", \"pmag_results.txt\")\n    res_file = pmag.resolve_file_name(res_file, dir_path)\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt = sys.argv[ind+1]\n    if '-obj' in sys.argv:\n        ind = sys.argv.index('-obj')\n        obj = sys.argv[ind+1]\n    if '-x' in sys.argv:\n        ind = sys.argv.index('-x')\n        xaxis = sys.argv[ind+1]\n    if '-y' in sys.argv:\n        ind = sys.argv.index('-y')\n        yaxis = sys.argv[ind+1]\n        if yaxis == 'dec':\n            ykeys = Dec_keys\n        if yaxis == 'inc':\n            ykeys = Inc_keys\n        if yaxis == 'int':\n            ykeys = Int_keys\n        if yaxis == 'chi':\n            ykeys = Chi_keys\n        if yaxis == 'lat':\n            ykeys = VLat_keys\n        if yaxis == 'lon':\n            ykeys = VLon_keys\n        if yaxis == 'vdm':\n            ykeys = Vdm_keys\n        if yaxis == 'vadm':\n            ykeys = Vadm_keys\n    if '-mcd' in sys.argv:\n        ind = sys.argv.index('-mcd')\n        method = sys.argv[ind+1]\n    if '-ts' in sys.argv:\n        ind = sys.argv.index('-ts')\n        ts = sys.argv[ind+1]\n        amin = float(sys.argv[ind+2])\n        amax = float(sys.argv[ind+3])\n        pTS = 1\n    if '-Iex' in sys.argv:\n        plotexp = 1\n    if '-sav' in sys.argv:\n        plot = 1\n    #\n    #\n    # get data read in\n    Results, file_type = pmag.magic_read(res_file)\n    if file_type not in supported:\n        print(\"Unsupported file type ({}), try again\".format(file_type))\n        sys.exit()\n    PltObjs = ['all']\n    if data_model_num == 2:\n        if file_type == 'pmag_results':  # find out what to plot\n            for rec in Results:\n                resname = rec['pmag_result_name'].split()\n                if 'Sample' in resname and 'sam' not in PltObjs:\n                    PltObjs.append('sam')\n                if 'Site' in resname and 'sit' not in PltObjs:\n                    PltObjs.append('sit')\n\n\n    methcodes = []\n    # need to know all the measurement types from method_codes\n    if \"magic_method_codes\" in list(Results[0].keys()):\n        for rec in Results:\n            meths = rec[\"magic_method_codes\"].split(\":\")\n            for meth in meths:\n                if meth.strip() not in methcodes and 'LP' in meth:\n                    # look for the lab treatments\n                    methcodes.append(meth.strip())\n    #\n    # initialize some variables\n    X_unit = \"\"  # Unit for age or depth plotting (meters if depth)\n    Xplots, Yplots = [], []\n    Xunits = []\n    yplotind, xplotind = 0, 0\n    #\n    # step through possible plottable keys\n    #\n    if xaxis == \"\" or yaxis == \"\":\n        for key in list(Results[0].keys()):\n            for keys in X_keys:\n                for xkeys in keys:\n                    if key in xkeys:\n                        for ResRec in Results:\n                            if ResRec[key] != \"\":\n                                # only plot something if there is something to plot!\n                                Xplots.append(key)\n                                break\n            for keys in Y_keys:\n                for pkeys in keys:\n                    if key in pkeys:\n                        for ResRec in Results:\n                            if ResRec[key] != \"\":\n                                Yplots.append(key)\n                                break\n        X, Y = [], []\n        for plt in Xplots:\n            if plt in Age_keys and 'age' not in X:\n                X.append('age')\n            if plt in Depth_keys and 'pos' not in X:\n                X.append('pos')\n        for plt in Yplots:\n            if plt in Dec_keys and 'dec' not in Y:\n                Y.append('dec')\n            if plt in Inc_keys and 'inc' not in Y:\n                Y.append('inc')\n            if plt in Int_keys and 'int' not in Y:\n                Y.append('int')\n            if plt in Chi_keys and 'chi' not in Y:\n                Y.append('chi')\n            if plt in VLat_keys and 'lat' not in Y:\n                Y.append('lat')\n            if plt in VLon_keys and 'lon' not in Y:\n                Y.append('lon')\n            if plt in Vadm_keys and 'vadm' not in Y:\n                Y.append('vadm')\n            if plt in Vdm_keys and 'vdm' not in Y:\n                Y.append('vdm')\n        if file_type == 'pmag_results':\n            print('available objects for plotting: ', PltObjs)\n        print('available X plots: ', X)\n        print('available Y plots: ', Y)\n        print('available method codes: ', methcodes)\n        f = open(dir_path+'/.striprc', 'w')\n        for x in X:\n            f.write('x:'+x+'\\n')\n        for y in Y:\n            f.write('y:'+y+'\\n')\n        for m in methcodes:\n            f.write('m:'+m+'\\n')\n        for obj in PltObjs:\n            f.write('obj:'+obj+'\\n')\n        sys.exit()\n    if plotexp == 1:\n        for lkey in Lat_keys:\n            for key in list(Results[0].keys()):\n                if key == lkey:\n                    lat = float(Results[0][lkey])\n                    Xinc = [pmag.pinc(lat), -pmag.pinc(lat)]\n                    break\n        if Xinc == \"\":\n            print('can not plot expected inc for site - lat unknown')\n    if method != \"\" and method not in methcodes:\n        print('your method not available, but these are:  ')\n        print(methcodes)\n        print('use ', methcodes[0], '? ^D to quit')\n    if xaxis == 'age':\n        for akey in Age_keys:\n            for key in list(Results[0].keys()):\n                if key == akey:\n                    Xplots.append(key)\n                    Xunits.append(Unit_keys[key])\n    if xaxis == 'pos':\n        for dkey in Depth_keys:\n            for key in list(Results[0].keys()):\n                if key == dkey:\n                    Xplots.append(key)\n    if len(Xplots) == 0:\n        print('desired X axis  information not found')\n        sys.exit()\n    if xaxis == 'age':\n        age_unit = Results[0][Xunits[0]]\n    if len(Xplots) > 1:\n        print('multiple X axis  keys found, using: ', Xplots[xplotind])\n    for ykey in ykeys:\n        for key in list(Results[0].keys()):\n            if key == ykey:\n                Yplots.append(key)\n    if len(Yplots) == 0:\n        print('desired Y axis  information not found')\n        sys.exit()\n    if len(Yplots) > 1:\n        print('multiple Y axis  keys found, using: ', Yplots[yplotind])\n\n    # check if age or depth info\n    if len(Xplots) == 0:\n        print(\"Must have either age or height info to plot \")\n        sys.exit()\n    #\n    # check for variable to plot\n    #\n    #\n    # determine X axis (age or depth)\n    #\n    if xaxis == \"age\":\n        plotind = \"1\"\n    if method == \"\":\n        try:\n            method = methcodes[0]\n        except IndexError:\n            method = \"\"\n    if xaxis == 'pos':\n        xlab = \"Stratigraphic Height (meters)\"\n    else:\n        xlab = \"Age (\"+age_unit+\")\"\n    Xkey = Xplots[xplotind]\n    Ykey = Yplots[yplotind]\n    ylab = Ykey\n    #\n    # collect the data for plotting\n    XY = []\n    isign = 1.\n#    if float(Results[0][Xkey])/float(Results[-1][Xkey])>0 and float(Results[0][Xkey])<0:\n#        isign=-1. # x axis all same sign and negative, take positive (e.g.,for depth in core)\n#        xlab=\"Stratigraphic Position (meters)\"\n#    else:\n#        isign=1.\n    for rec in Results:\n        if \"magic_method_codes\" in list(rec.keys()):\n            meths = rec[\"magic_method_codes\"].split(\":\")\n            if method in meths:  # make sure it is desired lab treatment step\n                if obj == 'all' and rec[Xkey].strip() != \"\":\n                    XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n                elif rec[Xkey].strip() != \"\":\n                    name = rec['pmag_result_name'].split()\n                    if obj == 'sit' and \"Site\" in name:\n                        XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n                    if obj == 'sam' and \"Sample\" in name:\n                        XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n        elif method == \"\":\n            if obj == 'all' and rec[Xkey].strip() != \"\":\n                XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n            elif rec[Xkey].strip() != \"\":\n                name = rec['pmag_result_name'].split()\n                if obj == 'sit' and \"Site\" in name:\n                    XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n                if obj == 'sam' and \"Sample\" in name:\n                    XY.append([isign*float(rec[Xkey]), float(rec[Ykey])])\n        else:\n            print(\"Something wrong with your plotting choices\")\n            break\n    XY.sort()\n    title = \"\"\n    if \"er_locations_names\" in list(Results[0].keys()):\n        title = Results[0][\"er_location_names\"]\n    if \"er_locations_name\" in list(Results[0].keys()):\n        title = Results[0][\"er_location_name\"]\n    labels = [xlab, ylab, title]\n    pmagplotlib.plot_init(FIG['strat'], 10, 5)\n    pmagplotlib.plot_strat(FIG['strat'], XY, labels)  # plot them\n    if plotexp == 1:\n        pmagplotlib.plot_hs(FIG['strat'], Xinc, 'b', '--')\n    if yaxis == 'inc' or yaxis == 'lat':\n        pmagplotlib.plot_hs(FIG['strat'], [0], 'b', '-')\n        pmagplotlib.plot_hs(FIG['strat'], [-90, 90], 'g', '-')\n    if pTS == 1:\n        FIG['ts'] = 2\n        pmagplotlib.plot_init(FIG['ts'], 10, 5)\n        pmagplotlib.plot_ts(FIG['ts'], [amin, amax], ts)\n    files = {}\n    for key in list(FIG.keys()):\n        files[key] = key+'.'+fmt\n    if pmagplotlib.isServer:\n        black = '#000000'\n        purple = '#800080'\n        files = {}\n        files['strat'] = xaxis+'_'+yaxis+'_.'+fmt\n        files['ts'] = 'ts.'+fmt\n        titles = {}\n        titles['strat'] = 'Depth/Time Series Plot'\n        titles['ts'] = 'Time Series Plot'\n        FIG = pmagplotlib.add_borders(FIG, titles, black, purple)\n        pmagplotlib.save_plots(FIG, files)\n    elif plot == 1:\n        pmagplotlib.save_plots(FIG, files)\n    else:\n        pmagplotlib.draw_figs(FIG)\n        ans = input(\" S[a]ve to save plot, [q]uit without saving:  \")\n        if ans == \"a\":\n            pmagplotlib.save_plots(FIG, files)", "response": "NAME strip_magic. py is the entry point for the magic file strip_magic. py is the main function for the magic file strip_magic. py"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    import numpy\n    X=arange(.1,10.1,.2) #make a list of numbers\n    Y=myfunc(X) # calls myfunc with argument X\n    for i in range(len(X)):\n        print(X[i],Y[i])", "response": "This program prints doubled values!"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    d,i,file2=\"\",\"\",\"\"\n    fmt,plot='svg',0\n    if '-h' in sys.argv: # check if help is needed\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    if '-sav' in sys.argv: plot=1\n    if '-fmt'  in sys.argv:\n        ind=sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file1=sys.argv[ind+1]\n    if '-f2' in sys.argv:\n        ind=sys.argv.index('-f2')\n        file2=sys.argv[ind+1]\n    if '-dir' in sys.argv:\n        ind=sys.argv.index('-dir')\n        d=float(sys.argv[ind+1])\n        i=float(sys.argv[ind+2])\n    D1=numpy.loadtxt(file1,dtype=numpy.float)\n    if file2!=\"\": D2=numpy.loadtxt(file2,dtype=numpy.float)\n#\n    counter,NumSims=0,1000\n#\n# get bootstrapped means for first data set\n#\n    print(\"Doing first set of directions, please be patient..\")\n    BDI1=pmag.di_boot(D1)\n#\n#   convert to cartesian coordinates X1,X2, Y1,Y2 and Z1, Z2\n#\n    if d==\"\": # repeat for second data set\n        print(\"Doing second  set of directions, please be patient..\")\n        BDI2=pmag.di_boot(D2)\n    else:\n        BDI2=[]\n# set up plots\n    CDF={'X':1,'Y':2,'Z':3}\n    pmagplotlib.plot_init(CDF['X'],4,4)\n    pmagplotlib.plot_init(CDF['Y'],4,4)\n    pmagplotlib.plot_init(CDF['Z'],4,4)\n# draw the cdfs\n    pmagplotlib.plot_com(CDF,BDI1,BDI2,[d,i])\n    files={}\n    files['X']='CD_X.'+fmt\n    files['Y']='CD_Y.'+fmt\n    files['Z']='CD_Z.'+fmt\n    if plot==0:\n        pmagplotlib.draw_figs(CDF)\n        ans=input(\"S[a]ve plots, <Return> to quit \")\n        if ans==\"a\":\n            pmagplotlib.save_plots(CDF,files)\n        else:\n            sys.exit()\n        \n    else: \n        pmagplotlib.save_plots(CDF,files)\n        sys.exit()", "response": "NAME\n       common_mean.py\n\n    DESCRIPTION\n       calculates bootstrap statistics to test for common mean\n\n    INPUT FORMAT\n       takes dec/inc as first two columns in two space delimited files\n   \n    SYNTAX\n       common_mean.py [command line options]\n    \n    OPTIONS\n       -h prints help message and quits\n       -f FILE, input file \n       -f2 FILE, optional second file to compare with first file\n       -dir D I, optional direction to compare with input file\n       -fmt [svg,jpg,pnd,pdf] set figure format [default is svg]\n    NOTES\n       must have either F2 OR dir but not both"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninput list of x points, list of y points output: k, a, b, SSE. curvature, circle center, and SSE Function for calculating the radius of the best fit circle to a set of x-y coordinates. Paterson, G. A., (2011), A simple test for the presence of multidomain behaviour during paleointensity experiments, J. Geophys. Res., in press, doi: 10.1029/2011JB008369", "response": "def AraiCurvature(x=x,y=y):\n    \"\"\"\n    input: list of x points, list of y points\n    output: k, a, b, SSE.  curvature, circle center, and SSE\n    Function for calculating the radius of the best fit circle to a set of \n    x-y coordinates.\n    Paterson, G. A., (2011), A simple test for the presence of multidomain\n    behaviour during paleointensity experiments, J. Geophys. Res., in press,\n    doi: 10.1029/2011JB008369\n\n    \"\"\"\n    # makes sure all values are floats, then norms them by largest value\n    X = numpy.array(list(map(float, x)))\n    X = old_div(X, max(X))\n    Y = numpy.array(list(map(float, y)))\n    Y = old_div(Y, max(Y))\n    XY = numpy.array(list(zip(X, Y)))\n                  \n    #Provide the intitial estimate\n    E1=TaubinSVD(XY);\n\n    #Determine the iterative solution\n    E2=LMA(XY, E1);\n\n    estimates=[E2[2], E2[0], E2[1]];\n    \n    best_a = E2[0]\n    best_b = E2[1]\n    best_r = E2[2]\n\n    if best_a <= numpy.mean(X) and best_b <= numpy.mean(Y):\n        k = old_div(-1.,best_r)\n    else:\n        k = old_div(1.,best_r)\n\n    SSE = get_SSE(best_a, best_b, best_r, X, Y)\n    return k, best_a, best_b, SSE"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef TaubinSVD(XY):\n    XY = numpy.array(XY)\n    X = XY[:,0] - numpy.mean(XY[:,0]) # norming points by x avg\n    Y = XY[:,1] - numpy.mean(XY[:,1]) # norming points by y avg\n    centroid = [numpy.mean(XY[:,0]), numpy.mean(XY[:,1])]\n    Z = X * X + Y * Y  \n    Zmean = numpy.mean(Z)\n    Z0 = old_div((Z - Zmean), (2. * numpy.sqrt(Zmean)))\n    ZXY = numpy.array([Z0, X, Y]).T\n    U, S, V = numpy.linalg.svd(ZXY, full_matrices=False) # \n    V = V.transpose()\n    A = V[:,2]\n    A[0] = old_div(A[0], (2. * numpy.sqrt(Zmean)))\n    A = numpy.concatenate([A, [(-1. * Zmean * A[0])]], axis=0)\n    a, b = (-1 * A[1:3]) / A[0] / 2 + centroid \n    r = numpy.sqrt(A[1]*A[1]+A[2]*A[2]-4*A[0]*A[3])/abs(A[0])/2;\n    return a,b,r", "response": "TaubinSVD is a simple algorithm for calculating the center of the fitting circle and the radius of the fitting circle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef VarCircle(XY, Par):  # must have at least 4 sets of xy points or else division by zero occurs\n    if type(XY) != numpy.ndarray:\n        XY = numpy.array(XY)\n    n = len(XY)\n    if n < 4:\n        raise Warning(\"Circle cannot be calculated with less than 4 data points.  Please include more data\")\n    Dx = XY[:,0] - Par[0]\n    Dy = XY[:,1] - Par[1]\n    D = numpy.sqrt(Dx * Dx + Dy * Dy) - Par[2]\n    result = old_div(numpy.dot(D, D),(n-3))\n    return result", "response": "Calculates the sample variance of distances from data points XY to the circle Par is a list of numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef LMA(XY,ParIni):\n    factorUp=10\n    factorDown=0.04\n    lambda0=0.01\n    epsilon=0.000001\n    IterMAX = 50\n    AdjustMax = 20\n    Xshift=0  \n    Yshift=0  \n    dX=1  \n    dY=0;                                                                                    \n    n = len(XY);      # number of data points\n\n    anew = ParIni[0] + Xshift\n    bnew = ParIni[1] + Yshift\n    Anew = old_div(1.,(2.*ParIni[2]))                                                                              \n    aabb = anew*anew + bnew*bnew    \n    Fnew = (aabb - ParIni[2]*ParIni[2])*Anew \n    Tnew = numpy.arccos(old_div(-anew,numpy.sqrt(aabb))) \n    if bnew > 0:\n        Tnew = 2*numpy.pi - Tnew\n    VarNew = VarCircle(XY,ParIni) \n\n    VarLambda = lambda0;  \n    finish = 0;  \n                                                                                                      \n    for it in range(0,IterMAX):\n                                                                      \n        Aold = Anew  \n        Fold = Fnew\n        Told = Tnew\n        VarOld = VarNew\n\n        H = numpy.sqrt(1+4*Aold*Fold);                                                                 \n        aold = -H*numpy.cos(Told)/(Aold+Aold) - Xshift;\n        bold = -H*numpy.sin(Told)/(Aold+Aold) - Yshift;\n        Rold = old_div(1,abs(Aold+Aold)); \n\n        DD = 1 + 4*Aold*Fold; \n        D = numpy.sqrt(DD);  \n        CT = numpy.cos(Told); \n        ST = numpy.sin(Told);    \n        H11=0; \n        H12=0; \n        H13=0; \n        H22=0; \n        H23=0; \n        H33=0; \n        F1=0; \n        F2=0; \n        F3=0;           \n                                                            \n        for i in range(0,n):\n            Xi = XY[i,0] + Xshift;   \n            Yi = XY[i,1] + Yshift;       \n            Zi = Xi*Xi + Yi*Yi;  \n            Ui = Xi*CT + Yi*ST;             \n            Vi =-Xi*ST + Yi*CT;\n\n            ADF = Aold*Zi + D*Ui + Fold;    \n            SQ = numpy.sqrt(4*Aold*ADF + 1);           \n            DEN = SQ + 1;                                          \n            Gi = 2*ADF/DEN;   \n            FACT = 2/DEN*(1 - Aold*Gi/SQ);      \n            DGDAi = FACT*(Zi + 2*Fold*Ui/D) - Gi*Gi/SQ;                \n            DGDFi = FACT*(2*Aold*Ui/D + 1);\n            DGDTi = FACT*D*Vi;    \n                                                          \n            H11 = H11 + DGDAi*DGDAi;                 \n            H12 = H12 + DGDAi*DGDFi;                           \n            H13 = H13 + DGDAi*DGDTi;                                          \n            H22 = H22 + DGDFi*DGDFi;\n            H23 = H23 + DGDFi*DGDTi;                                     \n            H33 = H33 + DGDTi*DGDTi;                        \n                                                 \n            F1 = F1 + Gi*DGDAi; \n            F2 = F2 + Gi*DGDFi;    \n            F3 = F3 + Gi*DGDTi;\n\n\n        for adjust in range(1,AdjustMax):\n                                              \n#             Cholesly decomposition                                     \n                                                                       \n            G11 = numpy.sqrt(H11 + VarLambda);\n            G12 = old_div(H12,G11)                                                                              \n            G13 = old_div(H13,G11)\n            G22 = numpy.sqrt(H22 + VarLambda - G12*G12);                                                              \n            G23 = old_div((H23 - G12*G13),G22);                                             \n            G33 = numpy.sqrt(H33 + VarLambda - G13*G13 - G23*G23);                \n                                                                                   \n            D1 = old_div(F1,G11);                                            \n            D2 = old_div((F2 - G12*D1),G22);                                                              \n            D3 = old_div((F3 - G13*D1 - G23*D2),G33);                \n\n            dT = old_div(D3,G33);  \n            dF = old_div((D2 - G23*dT),G22) \n            dA = old_div((D1 - G12*dF - G13*dT),G11) \n                                                                                   \n#            updating the parameters\n                                                                                            \n            Anew = Aold - dA;  \n            Fnew = Fold - dF;                             \n            Tnew = Told - dT;\n\n            if 1+4*Anew*Fnew < epsilon and VarLambda>1:  \n                Xshift = Xshift + dX;                                          \n                Yshift = Yshift + dY;                                                                               \n                                                                                     \n                H = numpy.sqrt(1+4*Aold*Fold);                               \n                aTemp = -H*numpy.cos(Told)/(Aold+Aold) + dX;                                     \n                bTemp = -H*numpy.sin(Told)/(Aold+Aold) + dY;                                      \n                rTemp = old_div(1,abs(Aold+Aold));                                       \n                                                                             \n                Anew = old_div(1,(rTemp + rTemp));                         \n                aabb = aTemp*aTemp + bTemp*bTemp;                          \n                Fnew = (aabb - rTemp*rTemp)*Anew;                             \n                Tnew = numpy.arccos(old_div(-aTemp,numpy.sqrt(aabb)));                                       \n                if bTemp > 0:\n                    Tnew = 2*numpy.pi - Tnew;           \n                VarNew = VarOld;                                         \n                break;                               \n\n            \n            if 1+4*Anew*Fnew < epsilon:  \n                VarLambda = VarLambda * factorUp;             \n                continue;              \n\n            DD = 1 + 4*Anew*Fnew;                  \n            D = numpy.sqrt(DD);                                                         \n            CT = numpy.cos(Tnew);                                \n            ST = numpy.sin(Tnew);    \n                    \n            GG = 0;                \n                            \n\n            for i in range(0, n):\n                Xi = XY[i,0] + Xshift;          \n                Yi = XY[i,1] + Yshift;    \n                Zi = Xi*Xi + Yi*Yi; \n                Ui = Xi*CT + Yi*ST;            \n                                                 \n                ADF = Anew*Zi + D*Ui + Fnew;                    \n                SQ = numpy.sqrt(4*Anew*ADF + 1);               \n                DEN = SQ + 1;                   \n                Gi = 2*ADF/DEN; \n                GG = GG + Gi*Gi;\n                                   \n            VarNew = old_div(GG,(n-3));    \n         \n            H = numpy.sqrt(1+4*Anew*Fnew);               \n            anew = -H*numpy.cos(Tnew)/(Anew+Anew) - Xshift;  \n            bnew = -H*numpy.sin(Tnew)/(Anew+Anew) - Yshift;  \n            Rnew = old_div(1,abs(Anew+Anew)); \n\n            if VarNew <= VarOld: \n                progress = old_div((abs(anew-aold) + abs(bnew-bold) + abs(Rnew-Rold)),(Rnew+Rold));      \n                if progress < epsilon: \n                    Aold = Anew;          \n                    Fold = Fnew;      \n                    Told = Tnew;           \n                    VarOld = VarNew # %#ok<NASGU>  \n                    finish = 1;     \n                    break;  \n\n                VarLambda = VarLambda * factorDown\n                break  \n            else:                 #    %   no improvement  \n                VarLambda = VarLambda * factorUp;      \n                continue;     \n\n        if finish == 1:\n            break\n\n    H = numpy.sqrt(1+4*Aold*Fold);                                                                                        \n    result_a = -H*numpy.cos(Told)/(Aold+Aold) - Xshift;                                                      \n    result_b = -H*numpy.sin(Told)/(Aold+Aold) - Yshift;                                                 \n    result_r = old_div(1,abs(Aold+Aold));       \n\n    return result_a, result_b, result_r", "response": "A function to compute the least squares fitting of circles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the SSE of a set of pages", "response": "def get_SSE(a,b,r,x,y):\n    \"\"\"\n    input: a, b, r, x, y.  circle center, radius, xpts, ypts\n    output: SSE\n    \"\"\"\n    SSE = 0\n    X = numpy.array(x)\n    Y = numpy.array(y)\n    for i in range(len(X)):\n        x = X[i]\n        y = Y[i]\n        v = (numpy.sqrt( (x -a)**2 + (y - b)**2 ) - r )**2\n        SSE += v\n    return SSE"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines() # read in data from standard input\n        for line in data: # step through line by line\n            dec=spitout(line)\n        sys.exit()\n    if '-i' in sys.argv:\n        while 1: # repeat this block until program killed\n            sundata={}  # dictionary with sundata in it\n            print (\"Time difference between Greenwich Mean Time (hrs to subtract from local time to get GMT): \")\n            try:\n                sundata[\"delta_u\"]=input(\"<cntl-D> to quit \")\n            except:\n                print(\"\\n Good-bye\\n\")\n                sys.exit()\n            date=\"\"\n            date=date+input(\"Year:  <cntl-D to quit> \")\n            date=date+\":\"+input(\"Month:  \")\n            date=date+\":\"+input(\"Day:  \")\n            date=date+\":\"+input(\"hour:  \")\n            date=date+\":\"+input(\"minute:  \")\n            sundata[\"date\"]=date\n            sundata[\"lat\"]=input(\"Latitude of sampling site (negative in southern hemisphere): \")\n            sundata[\"lon\"]=input(\"Longitude of sampling site (negative for western hemisphere): \")\n            sundata[\"shadow_angle\"]=input(\"Shadow angle: \")\n            print('%7.1f'%(pmag.dosundec(sundata))) # call sundec function from pmag module and print\n    else:\n        data=sys.stdin.readlines() # read in data from standard input\n    for line in data: # step through line by line\n        dec=spitout(line)", "response": "This function is the entry point for sun. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the names locations and bounds of locations", "response": "def main():\n    \"\"\"\n    NAME\n        sites_locations.py\n\n    DESCRIPTION\n        reads in er_sites.txt file and finds all locations and bounds of locations\n        outputs er_locations.txt file\n\n    SYNTAX\n        sites_locations.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f: specimen input er_sites format file, default is \"er_sites.txt\"\n        -F: locations table: default is \"er_locations.txt\"\n    \"\"\"\n# set defaults\n    site_file=\"er_sites.txt\"\n    loc_file=\"er_locations.txt\"\n    Names,user=[],\"unknown\"\n    Done=[]\n    version_num=pmag.get_version()\n    args=sys.argv\n    dir_path='.'\n# get command line stuff\n    if '-WD' in args:\n        ind=args.index(\"-WD\")\n        dir_path=args[ind+1]\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in args:\n        ind=args.index(\"-f\")\n        site_file=args[ind+1]\n    if '-F' in args:\n        ind=args.index(\"-F\")\n        loc_file=args[ind+1]\n    #\n    site_file=dir_path+'/'+site_file\n    loc_file=dir_path+'/'+loc_file\n    Sites,file_type=pmag.magic_read(site_file)\n    if file_type != 'er_sites':\n        print(file_type)\n        print(file_type,\"This is not a valid er_sites file \")\n        sys.exit()\n    # read in site data\n    #\n    LocNames,Locations=[],[]\n    for site in Sites:\n        if site['er_location_name'] not in LocNames: # new location name\n            LocNames.append(site['er_location_name'])\n            sites_locs=pmag.get_dictitem(Sites,'er_location_name',site['er_location_name'],'T') # get all sites for this loc\n            lats=pmag.get_dictkey(sites_locs,'site_lat','f') # get all the latitudes as floats\n            lons=pmag.get_dictkey(sites_locs,'site_lon','f') # get all the longitudes as floats\n            LocRec={'er_citation_names':'This study','er_location_name':site['er_location_name'],'location_type':''}\n            LocRec['location_begin_lat']=str(min(lats))\n            LocRec['location_end_lat']=str(max(lats))\n            LocRec['location_begin_lon']=str(min(lons))\n            LocRec['location_end_lon']=str(max(lons))\n            Locations.append(LocRec)\n    if len(Locations)>0:\n        pmag.magic_write(loc_file,Locations,\"er_locations\")\n        print(\"Locations written to: \",loc_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all codes of one type", "response": "def get_one_meth_type(self, mtype, method_list):\n        \"\"\"\n        Get all codes of one type (i.e., 'anisotropy_estimation')\n        \"\"\"\n        cond = method_list['dtype'] == mtype\n        codes = method_list[cond]\n        return codes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_one_meth_category(self, category, all_codes, code_types):\n        categories = Series(code_types[code_types[category] == True].index)\n        cond = all_codes['dtype'].isin(categories)\n        codes = all_codes[cond]\n        return codes", "response": "Get all codes in one category"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n\n    args = sys.argv\n    if \"-h\" in args:\n        print(main.__doc__)\n        sys.exit()\n\n    dataframe = extractor.command_line_dataframe([['f', False, 'orient.txt'], ['Fsa', False, 'samples.txt'], ['ncn', False, \"1\"], ['mcd', False, 'FS-FD'], ['loc', False, 'unknown'], ['app', False, False], ['WD', False, '.'], ['ID', False, '.'], ['DM', False, 3]])\n    checked_args = extractor.extract_and_check_args(args, dataframe)\n    #print('checked_args:', checked_args)\n    orient_file, samp_file, samp_con, method_codes, location_name, append, output_dir, input_dir, data_model = extractor.get_vars(['f', 'Fsa', 'ncn', 'mcd', 'loc', 'app', 'WD', 'ID', 'DM'], checked_args)\n\n    if len(str(samp_con)) > 1:\n        samp_con, Z = samp_con.split('-')\n        Z = float(Z)\n    else:\n        Z = 1\n\n    ipmag.azdip_magic(orient_file, samp_file, samp_con, Z, method_codes, location_name, append, output_dir, input_dir, data_model)", "response": "This function is the main function of the AzDip Magic module. It takes space delimited AzDip file and converts to MagIC formatted tables and returns the base function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    dir_path='.'\n    names,res,proj,locs,padlon,padlat,fancy,gridspace,details=[],'l','lcc','',0,0,0,15,1\n    Age_bounds=[-5000,2000]\n    Lat_bounds=[20,45]\n    Lon_bounds=[15,55]\n    fmt='pdf'\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        sites_file=sys.argv[ind+1]\n    if '-res' in sys.argv:\n        ind = sys.argv.index('-res')\n        res=sys.argv[ind+1]\n    if '-etp' in sys.argv:fancy=1\n    if '-o' in sys.argv:ocean=1\n    if '-d' in sys.argv:details=1\n    if '-prj' in sys.argv:\n        ind = sys.argv.index('-prj')\n        proj=sys.argv[ind+1]\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    verbose=pmagplotlib.verbose\n    if '-sav' in sys.argv:\n        verbose=0\n    if '-pad' in sys.argv:\n        ind = sys.argv.index('-pad')\n        padlat=float(sys.argv[ind+1])\n        padlon=float(sys.argv[ind+2])\n    if '-grd' in sys.argv:\n        ind = sys.argv.index('-grd')\n        gridspace=float(sys.argv[ind+1])\n    if '-WD' in sys.argv:\n        ind = sys.argv.index('-WD')\n        dir_path=sys.argv[ind+1]\n    sites_file=dir_path+'/'+sites_file\n    geo_in=open(sites_file,'r').readlines()\n    Age,AgeErr,Vadm,VadmErr,slats,slons=[],[],[],[],[],[]\n    for line in geo_in[2:]: # skip top two rows`\n        rec=line.split()\n        if float(rec[0])>Age_bounds[0] and float(rec[0])<Age_bounds[1] \\\n           and float(rec[12])>Lat_bounds[0] and float(rec[12]) < Lat_bounds[1]\\\n            and float(rec[13])>Lon_bounds[0] and float(rec[13])<Lon_bounds[1]:\n            Age.append(float(rec[0]))\n            AgeErr.append(float(rec[1]))\n            Vadm.append(10.*float(rec[6]))\n            VadmErr.append(10.*float(rec[7]))\n            slats.append(float(rec[12]))\n            slons.append(float(rec[13]))\n    FIGS={'map':1,'vadms':2}\n    pmagplotlib.plot_init(FIGS['map'],6,6)\n    pmagplotlib.plot_init(FIGS['vadms'],6,6)\n    Opts={'res':res,'proj':proj,'loc_name':locs,'padlon':padlon,'padlat':padlat,'latmin':numpy.min(slats)-padlat,'latmax':numpy.max(slats)+padlat,'lonmin':numpy.min(slons)-padlon,'lonmax':numpy.max(slons)+padlon,'sym':'ro','boundinglat':0.,'pltgrid':1}\n    Opts['lon_0']=int(0.5*(numpy.min(slons)+numpy.max(slons)))\n    Opts['lat_0']=int(0.5*(numpy.min(slats)+numpy.max(slats)))\n    Opts['gridspace']=gridspace\n    if details==1:\n        Opts['details']={'coasts':1,'rivers':0,'states':1,'countries':1,'ocean':1}\n    else:\n        Opts['details']={'coasts':1,'rivers':0,'states':0,'countries':0,'ocean':1}\n    Opts['details']['fancy']=fancy\n    pmagplotlib.plot_map(FIGS['map'],slats,slons,Opts)\n    pmagplotlib.plot_xy(FIGS['vadms'],Age,Vadm,sym='bo',xlab='Age (Years CE)',ylab=r'VADM (ZAm$^2$)')\n    if verbose:pmagplotlib.draw_figs(FIGS)\n    files={}\n    for key in list(FIGS.keys()):\n        files[key]=key+'.'+fmt\n    if pmagplotlib.isServer:\n        black     = '#000000'\n        purple    = '#800080'\n        titles={}\n        titles['map']='Map'\n        titles['vadms']='VADMs'\n        FIG = pmagplotlib.add_borders(FIGS,titles,black,purple)\n        pmagplotlib.save_plots(FIGS,files)\n    elif verbose:\n        ans=input(\" S[a]ve to save plot, Return to quit:  \")\n        if ans==\"a\":\n            pmagplotlib.save_plots(FIGS,files)\n    else:\n        pmagplotlib.save_plots(FIGS,files)", "response": "This function is the main function for the main function of the main function. It is the main function for the main function. It is the main function for the main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    args = sys.argv\n    if '-h' in args:\n        print(do_help())\n        sys.exit()\n\n    # def k15_magic(k15file, specnum=0, sample_naming_con='1', er_location_name=\"unknown\", measfile='magic_measurements.txt', sampfile=\"er_samples.txt\", aniso_outfile='rmag_anisotropy.txt', result_file=\"rmag_results.txt\", input_dir_path='.', output_dir_path='.'):\n\n    dataframe = extractor.command_line_dataframe([['f', True, ''], ['F', False, 'measurements.txt'], ['Fsa', False, 'samples.txt'], ['Fa', False, 'specimens.txt'], [\n                                                 'Fr', False, 'rmag_results.txt'], ['spc', False, 0], ['ncn', False, '1'], ['loc', False, 'unknown'], ['WD', False, '.'], ['ID', False, '.'], ['DM', False, 3]])\n    checked_args = extractor.extract_and_check_args(args, dataframe)\n    k15file, measfile, sampfile, aniso_outfile, result_file, specnum, sample_naming_con, location_name, output_dir_path, input_dir_path, data_model_num = extractor.get_vars(\n        ['f', 'F', 'Fsa', 'Fa', 'Fr', 'spc', 'ncn', 'loc', 'WD', 'ID', 'DM'], checked_args)\n    program_ran, error_message = convert.k15(k15file, specnum=specnum, sample_naming_con=sample_naming_con, location=location_name, meas_file=measfile,\n                                                 samp_file=sampfile, aniso_outfile=aniso_outfile, result_file=result_file, input_dir_path=input_dir_path, dir_path=output_dir_path, data_model_num=data_model_num)", "response": "This function is the main entry point for the k15_magic. py script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(command_line=True, **kwargs):\n\n#\n#              NB: use PHI, THETA = -1 -1 to signal that it changes, i.e. in anisotropy experiment\n#        -ac B : peak AF field (in mT) for ARM acquisition, default is none\n    #\n    #initialize variables\n    norm='cc'\n    samp_con,Z='3',1\n    meas_file='magic_measurements.txt'\n    spec_file='er_specimens.txt'\n    samp_file='er_samples.txt'\n    site_file='er_sites.txt'\n    ErSpecs,ErSamps,ErSites,ErLocs,ErCits=[],[],[],[],[]\n    MeasRecs=[]\n    specnum,units,locname=0,\"1\",\"unknown\"\n    citation=\"This study\"\n    dir_path='.'\n    args=sys.argv\n    if command_line:\n        if '-WD' in args:\n            ind=args.index(\"-WD\")\n            dir_path=args[ind+1]\n        if \"-h\" in args:\n            print(main.__doc__)\n            return False\n        if \"-usr\" in args:\n            ind=args.index(\"-usr\")\n            user=args[ind+1]\n        if '-F' in args:\n            ind=args.index(\"-F\")\n            meas_file=args[ind+1]\n        if '-Fsp' in args:\n            ind=args.index(\"-Fsp\")\n            spec_file=args[ind+1]\n        if '-Fsa' in args:\n            ind=args.index(\"-Fsa\")\n            samp_file=args[ind+1]\n        if '-Fsi' in args:   # LORI addition\n            ind=args.index(\"-Fsi\")\n            site_file=args[ind+1]\n        if '-loc' in args:\n            ind=args.index(\"-loc\")\n            locname=args[ind+1]\n        if '-mcd' in args:\n            ind=args.index(\"-mcd\")\n            methods=args[ind+1]\n        else:\n            methods='SO-MAG'\n        if '-spc' in args:\n            ind=args.index(\"-spc\")\n            specnum=-int(args[ind+1])\n        if '-n' in args:\n            ind=args.index(\"-n\")\n            norm=args[ind+1]\n        if \"-A\" in args:\n            avg=1\n        else:\n            avg=0\n        if '-dc' in args:\n            ind=args.index('-dc')\n            DC_FIELD,DC_PHI,DC_THETA=list(map(float,args[ind+1].strip('( ) [ ]').split(',')))\n            DC_FIELD *= 1e-6\n            yn=''\n            GET_DC_PARAMS=False\n        else: GET_DC_PARAMS,DC_FIELD,DC_PHI,DC_THETA,yn=True,0,0,-90,''\n        if \"-ncn\" in args:\n            ind=args.index(\"-ncn\")\n            samp_con=sys.argv[ind+1]\n            if \"4\" in samp_con:\n                if \"-\" not in samp_con:\n                    print(\"option [4] must be in form 4-Z where Z is an integer\")\n                    return False, \"naming convention option [4] must be in form 4-Z where Z is an integer\"\n                else:\n                    Z=samp_con.split(\"-\")[1]\n                    samp_con=\"4\"\n            elif \"7\" in samp_con:\n                if \"-\" not in samp_con:\n                    print(\"option [7] must be in form 7-Z where Z is an integer\")\n                    return False, \"naming convention option [7] must be in form 7-Z where Z is an integer\"\n                else:\n                    Z=samp_con.split(\"-\")[1]\n                    samp_con=\"7\"\n        if '-f' in args:\n            ind=args.index(\"-f\")\n            magfile=args[ind+1]\n        if '-ID' in args:\n            ind = args.index('-ID')\n            input_dir_path = args[ind+1]\n        else:\n            input_dir_path = os.path.split(magfile)[0]\n        output_dir_path = dir_path\n        # LJ\n\n    # if you are running as a module:\n    elif not command_line:\n        dir_path = kwargs.get('dir_path', '.')\n        user = kwargs.get('user', '')\n        meas_file = kwargs.get('meas_file', 'magic_measurements.txt') # outfile\n        spec_file = kwargs.get('spec_file', 'er_specimens.txt') # specimen outfile\n        samp_file = kwargs.get('samp_file', 'er_samples.txt') # sample outfile\n        site_file = kwargs.get('site_file', 'er_sites.txt') # site outfile\n        locname = kwargs.get('locname', '')\n        methods = kwargs.get('methods', ['SO-MAG'])\n        specnum = -int(kwargs.get('specnum', 0))\n        norm = kwargs.get('norm', 'cc')\n        avg = kwargs.get('avg', 0)  # 0 means do average, 1 means don't\n        samp_con = kwargs.get('samp_con', '3')\n        magfile = kwargs.get('magfile', '')\n        input_dir_path = kwargs.get('input_dir_path', os.path.split(magfile)[0])\n        output_dir_path = dir_path\n        DC_FIELD,DC_PHI,DC_THETA = list(map(float, kwargs.get('dc_params', (0,0,-90))))\n        DC_FIELD *= 1e-6\n        yn = ''\n        if DC_FIELD==0 and DC_PHI==0 and DC_THETA==-90: GET_DC_PARAMS=True\n        else: GET_DC_PARAMS=False\n        # done with module-specific stuff\n\n        # formatting and checking variables\n        if \"4\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [4] must be in form 4-Z where Z is an integer\")\n                return False, \"naming convention option [4] must be in form 4-Z where Z is an integer\"\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"4\"\n        elif \"7\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                return False, \"naming convention option [7] must be in form 7-Z where Z is an integer\"\n            else:\n                Z=samp_con.split(\"-\")[1]\n                samp_con=\"7\"\n\n    magfile = os.path.join(input_dir_path, magfile)\n    spec_file = os.path.join(output_dir_path, spec_file)\n    samp_file = os.path.join(output_dir_path, samp_file)\n    site_file = os.path.join(output_dir_path, site_file)\n    meas_file= os.path.join(output_dir_path, meas_file)\n    FIRST_GET_DC=True\n    try:\n        with open(magfile,'r') as file_input:\n            File = file_input.readlines()\n    except Exception as ex:\n        print(\"bad sam file name: \", magfile)\n        return False, \"bad sam file name\"\n    if len(File) == 1: File = File[0].split('\\r'); File = [x+\"\\r\\n\" for x in File]\n    sids,ln,format=[],0,'CIT'\n    formats=['CIT','2G','APP','JRA']\n    if File[ln].strip()=='CIT': ln+=1\n    ErLocRec={}\n    ErLocRec[\"er_location_name\"]=locname\n    ErLocRec[\"er_citation_names\"]=citation\n    comment=File[ln]\n    if comment=='CIT':\n       format=comment\n       ln+=1\n    comment=File[ln]\n    print(comment)\n    ln+=1\n    specimens,samples,sites=[],[],[]\n    if format=='CIT':\n        line=File[ln].split()\n        site_lat=line[0]\n        site_lon=line[1]\n        ErLocRec[\"location_begin_lat\"]=site_lat\n        ErLocRec[\"location_begin_lon\"]=site_lon\n        ErLocRec[\"location_end_lat\"]=site_lat\n        ErLocRec[\"location_end_lon\"]=site_lon\n        ErLocs.append(ErLocRec)\n        try: Cdec=float(line[2])\n        except ValueError: pdb.set_trace()\n        for k in range(ln+1,len(File)):\n            line=File[k]\n            rec=line.split()\n            if rec == []: continue\n            specimen=rec[0]\n            specimens.append(specimen)\n    for specimen in specimens:\n        ErSpecRec,ErSampRec,ErSiteRec={},{},{}\n        if specnum!=0:\n            sample=specimen[:specnum]\n        else: sample=specimen\n        site=pmag.parse_site(sample,samp_con,Z)\n        ErSpecRec['er_specimen_name']=specimen\n        ErSpecRec['er_sample_name']=sample\n        ErSpecRec['er_site_name']=site\n        ErSpecRec['er_location_name']=locname\n        ErSpecRec['er_citation_names']=citation\n        ErSampRec['er_sample_name']=sample\n        ErSampRec['er_site_name']=site\n        ErSampRec['er_location_name']=locname\n        ErSampRec['er_citation_names']=citation\n        ErSampRec['magic_method_codes']=methods\n        ErSampRec['sample_declination_correction']='%7.1f'%(Cdec)\n        ErSiteRec['er_site_name']=site\n        ErSiteRec['er_location_name']=locname\n        ErSiteRec['er_citation_names']=citation\n        ErSiteRec['site_lat']=site_lat\n        ErSiteRec['site_lon']=site_lon\n        with open(os.path.join(input_dir_path,specimen),'r') as finput:\n            Lines = list(finput.readlines())\n        comment = \"\"\n        line=Lines[0].split()\n        if len(line)>2:\n            comment=line[2]\n        info=Lines[1].split()\n        vol=float(info[-1])\n        if vol!=1.0:\n            if norm=='cc':units=\"1\"\n            if norm=='m3':units=\"2\"\n            ErSpecRec['specimen_weight']=\"\"\n            if units==\"1\" or \"\":\n                ErSpecRec['specimen_volume']='%10.3e'%(vol*1e-6)\n            else:\n                ErSpecRec['specimen_volume']='%10.3e'%(vol)\n        else:\n            if norm=='cc':units=\"1\"\n            if norm=='m3':units=\"2\"\n            ErSpecRec['specimen_volume']=\"\"\n            if units==\"1\" or \"\":\n                ErSpecRec['specimen_weight']='%10.3e'%(vol*1e-3)\n            else:\n                ErSpecRec['specimen_weight']='%10.3e'%(vol)\n        dip=float(info[-2])\n        dip_direction=float(info[-3])+Cdec+90.\n        sample_dip=-float(info[-4])\n        sample_azimuth=float(info[-5])+Cdec-90.\n        if len(info)>5:\n            ErSampRec['sample_height']=info[-6]\n        else:\n            ErSampRec['sample_height']='0'\n        ErSampRec['sample_azimuth']='%7.1f'%(sample_azimuth)\n        ErSampRec['sample_dip']='%7.1f'%(sample_dip)\n        ErSampRec['sample_bed_dip']='%7.1f'%(dip)\n        ErSampRec['sample_bed_dip_direction']='%7.1f'%(dip_direction)\n        ErSampRec['sample_class']=''\n        ErSampRec['sample_type']=''\n        ErSampRec['sample_lithology']=''\n        if Cdec!=0 or Cdec!=\"\":\n            ErSampRec['magic_method_codes']='SO-CMD-NORTH'\n        else:\n            ErSampRec['magic_method_codes']='SO-MAG'\n        for line in Lines[2:len(Lines)]:\n            if line == '\\n': continue\n            MeasRec=ErSpecRec.copy()\n#           Remove specimen_volume and specimen_weight as they do not exits in the magic_measurement table\n            del MeasRec[\"specimen_volume\"]\n            del MeasRec[\"specimen_weight\"]\n            treat_type=line[0:3]\n            if treat_type[1] == '.':\n                treat_type = 'NRM'\n            treat=line[2:6]\n            try: float(treat)\n            except ValueError: treat = line[3:6]\n            if treat_type.startswith('NRM'):\n                MeasRec['magic_method_codes']='LT-NO'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']='273'\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            elif treat_type.startswith('AF'):\n                MeasRec['magic_method_codes']='LT-AF-Z'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']='273'\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                if treat.strip() == '':\n                    MeasRec['treatment_ac_field']='0'\n                else:\n                    MeasRec['treatment_ac_field']='%10.3e'%(float(treat)*1e-3)\n            elif treat_type.startswith('ARM'):\n                MeasRec['magic_method_codes']=\"LP-ARM\"\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']='273'\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                if treat.strip() == '':\n                    MeasRec['treatment_ac_field']='0'\n                else:\n                    MeasRec['magic_method_codes']=\"LP-ARM-AFD\"\n                    MeasRec['treatment_ac_field']='%10.3e'%(float(treat)*1e-3)\n            elif treat_type.startswith('TT'):\n                MeasRec['magic_method_codes']='LT-T-Z'\n                MeasRec['measurement_temp']='273'\n                if treat.strip() == '':\n                    MeasRec['treatment_temp']='273'\n                else:\n                    MeasRec['treatment_temp']='%7.1f'%(float(treat)+273)\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            elif treat_type.startswith('LT') or treat_type.startswith('LN2'):\n                MeasRec['magic_method_codes']='LT-LT-Z'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']='77'\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            elif line[4] == '0': #assume decimal IZZI format 0 field thus can hardcode the dc fields\n                MeasRec['magic_method_codes']='LT-T-Z'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']=str(int(treat_type) + 273)\n                MeasRec['treatment_dc_field']='0'\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            elif line[4] == '1': #assume decimal IZZI format in constant field\n                if GET_DC_PARAMS: GET_DC_PARAMS, FIRST_GET_DC, yn, DC_FIELD, DC_PHI, DC_THETA = get_dc_params(FIRST_GET_DC,specimen,treat_type,yn)\n                MeasRec['magic_method_codes']='LT-T-I'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']=str(int(treat_type) + 273)\n                MeasRec['treatment_dc_field']='%1.2e'%DC_FIELD\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            elif line[4] == '2': #assume decimal IZZI format PTRM step\n                if GET_DC_PARAMS: GET_DC_PARAMS, FIRST_GET_DC, yn, DC_FIELD, DC_PHI, DC_THETA = get_dc_params(FIRST_GET_DC,specimen,treat_type,yn)\n                MeasRec['magic_method_codes']='LT-PTRM-I'\n                MeasRec['measurement_temp']='273'\n                MeasRec['treatment_temp']=str(int(treat_type) + 273)\n                MeasRec['treatment_dc_field']='%1.2e'%DC_FIELD\n                MeasRec['treatment_dc_field_phi'] = '%1.2f'%DC_PHI\n                MeasRec['treatment_dc_field_theta'] = '%1.2f'%DC_THETA\n                MeasRec['treatment_ac_field']='0'\n            else:\n                print(\"trouble with your treatment steps\")\n            MeasRec['measurement_dec']=line[46:51]\n            MeasRec['measurement_inc']=line[52:58]\n            M='%8.2e'%(float(line[31:39])*vol*1e-3) # convert to Am2\n            MeasRec['measurement_magn_moment']=M\n            MeasRec['measurement_csd']='%7.1f'%(eval(line[41:46]))\n            MeasRec[\"measurement_positions\"]='1'\n            MeasRec['measurement_standard']='u'\n            if len(line)>60:\n                MeasRec['magic_instrument_codes']=line[85:]\n                MeasRec['measurement_sd_x']='%8.2e'%(float(line[58:67])*1e-8) #(convert e-5emu to Am2)\n                MeasRec['measurement_sd_y']='%8.2e'%(float(line[67:76])*1e-8)\n                MeasRec['measurement_sd_z']='%8.2e'%(float(line[76:85])*1e-8)\n            MeasRecs.append(MeasRec)\n        ErSpecs.append(ErSpecRec)\n        if sample not in samples:\n            samples.append(sample)\n            ErSamps.append(ErSampRec)\n        site=pmag.parse_site(sample,samp_con,Z)\n        if site not in sites:\n            sites.append(site)\n            ErSites.append(ErSiteRec)\n    pmag.magic_write(spec_file,ErSpecs,'er_specimens')\n    print('specimens stored in ',spec_file)\n    pmag.magic_write(samp_file,ErSamps,'er_samples')\n    print('samples stored in ',samp_file)\n    pmag.magic_write(site_file,ErSites,'er_sites')\n    print('sites stored in ', site_file)\n    Fixed=pmag.measurements_methods(MeasRecs,avg)\n    pmag.magic_write(meas_file,Fixed,'magic_measurements')\n    print('data stored in ',meas_file)\n    return True, meas_file", "response": "This function is the main entry point for the cit_magic. py script."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnaming nac_util. py is the main function of the demagnetization diagram.", "response": "def main():\n    \"\"\"\n    NAME\n       zeq.py\n  \n    DESCRIPTION\n       plots demagnetization data. The equal area projection has the X direction (usually North in geographic coordinates)\n          to the top.  The red line is the X axis of the Zijderveld diagram.  Solid symbols are lower hemisphere. \n          The solid (open) symbols in the Zijderveld diagram are X,Y (X,Z) pairs.  The demagnetization diagram plots the\n          fractional remanence remaining after each step. The green line is the fraction of the total remaence removed \n          between each step.        \n\n    INPUT FORMAT\n       takes specimen_name treatment intensity declination inclination  in space\n delimited file\n\n    SYNTAX\n        zeq.py [command line options\n\n    OPTIONS\n        -f FILE for reading from command line\n        -u [mT,C] specify units of mT OR C, default is unscaled\n        -sav save figure and quit\n        -fmt [svg,jpg,png,pdf] set figure format [default is svg]\n        -beg [step number] treatment step for beginning of PCA calculation, 0 is default\n        -end [step number] treatment step for end of PCA calculation, last step is default\n        -ct [l,p,f] Calculation Type: best-fit line,  plane or fisher mean; line is default\n\n    \"\"\"\n    files,fmt,plot={},'svg',0\n    end_pca,beg_pca=\"\",\"\"\n    calculation_type='DE-BFL' \n    if '-h' in sys.argv: # check if help is needed\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    else:\n        if '-f' in sys.argv:\n            ind=sys.argv.index('-f')\n            file=sys.argv[ind+1]\n        else:\n            print(main.__doc__)\n            sys.exit()\n        if '-u' in sys.argv:\n            ind=sys.argv.index('-u')\n            units=sys.argv[ind+1]\n            if units==\"C\":SIunits=\"K\"\n            if units==\"mT\":SIunits=\"T\"\n        else:\n            units=\"U\"\n            SIunits=\"U\"\n    if '-sav' in sys.argv:plot=1\n    if '-ct' in sys.argv:\n        ind=sys.argv.index('-ct')\n        ct=sys.argv[ind+1]\n        if ct=='f':calculation_type='DE-FM' \n        if ct=='p':calculation_type='DE-BFP' \n    if '-fmt' in sys.argv:\n        ind=sys.argv.index('-fmt')\n        fmt=sys.argv[ind+1]\n    if '-beg' in sys.argv:\n        ind=sys.argv.index('-beg')\n        beg_pca=int(sys.argv[ind+1])\n    if '-end' in sys.argv:\n        ind=sys.argv.index('-end')\n        end_pca=int(sys.argv[ind+1])\n    f=open(file,'r')\n    data=f.readlines()\n#\n    datablock= [] # set up list for data\n    s=\"\" # initialize specimen name\n    angle=0.\n    for line in data:   # read in the data from standard input\n        rec=line.split() # split each line on space to get records\n        if angle==\"\":angle=float(rec[3])\n        if s==\"\":s=rec[0]\n        if units=='mT':datablock.append([float(rec[1])*1e-3,float(rec[3]),float(rec[4]),1e-3*float(rec[2]),'','g']) # treatment, dec, inc, int # convert to T and Am^2 (assume emu)\n        if units=='C':datablock.append([float(rec[1])+273.,float(rec[3]),float(rec[4]),1e-3*float(rec[2]),'','g']) # treatment, dec, inc, int, convert to K and Am^2, assume emu\n        if units=='U':datablock.append([float(rec[1]),float(rec[3]),float(rec[4]),float(rec[2]),'','g']) # treatment, dec, inc, int, using unscaled units \n# define figure numbers in a dictionary for equal area, zijderveld,  \n#  and intensity vs. demagnetiztion step respectively\n    ZED={}\n    ZED['eqarea'],ZED['zijd'],  ZED['demag']=1,2,3 \n    pmagplotlib.plot_init(ZED['eqarea'],5,5) # initialize plots\n    pmagplotlib.plot_init(ZED['zijd'],5,5)\n    pmagplotlib.plot_init(ZED['demag'],5,5)\n#\n#\n    pmagplotlib.plot_zed(ZED,datablock,angle,s,SIunits) # plot the data\n    if plot==0:pmagplotlib.draw_figs(ZED)\n#\n# print out data for this sample to screen\n#\n    recnum=0\n    for plotrec in datablock:\n        if units=='mT':print('%i  %7.1f %8.3e %7.1f %7.1f ' % (recnum,plotrec[0]*1e3,plotrec[3],plotrec[1],plotrec[2]))\n        if units=='C':print('%i  %7.1f %8.3e %7.1f %7.1f ' % (recnum,plotrec[0]-273.,plotrec[3],plotrec[1],plotrec[2]))\n        if units=='U':print('%i  %7.1f %8.3e %7.1f %7.1f ' % (recnum,plotrec[0],plotrec[3],plotrec[1],plotrec[2]))\n        recnum += 1\n    if plot==0:\n      while 1:\n        if beg_pca!=\"\" and end_pca!=\"\" and calculation_type!=\"\":\n                pmagplotlib.plot_zed(ZED,datablock,angle,s,SIunits) # plot the data\n                mpars=pmag.domean(datablock,beg_pca,end_pca,calculation_type) # get best-fit direction/great circle\n                pmagplotlib.plot_dir(ZED,mpars,datablock,angle) # plot the best-fit direction/great circle\n                print('Specimen, calc_type, N, min, max, MAD, dec, inc')\n                if units=='mT':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]*1e3,mpars[\"measurement_step_max\"]*1e3,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n                if units=='C':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]-273,mpars[\"measurement_step_max\"]-273,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n                if units=='U':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"],mpars[\"measurement_step_max\"],mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n        if end_pca==\"\":end_pca=len(datablock)-1 # initialize end_pca, beg_pca to first and last measurement\n        if beg_pca==\"\":beg_pca=0\n        ans=input(\" s[a]ve plot, [b]ounds for pca and calculate, change [h]orizontal projection angle, [q]uit:   \")\n        if ans =='q':\n            sys.exit() \n        if  ans=='a':\n            files={}\n            for key in list(ZED.keys()):\n                files[key]=s+'_'+key+'.'+fmt \n            pmagplotlib.save_plots(ZED,files)\n        if ans=='h':\n            angle=float(input(\" Declination to project onto horizontal axis? \"))\n            pmagplotlib.plot_zed(ZED,datablock,angle,s,SIunits) # plot the data\n\n        if ans=='b':\n            GoOn=0\n            while GoOn==0: # keep going until reasonable bounds are set\n                print('Enter index of first point for pca: ','[',beg_pca,']')\n                answer=input('return to keep default  ')\n                if answer != \"\":beg_pca=int(answer)\n                print('Enter index  of last point for pca: ','[',end_pca,']')\n                answer=input('return to keep default  ')\n                if answer != \"\":\n                    end_pca=int(answer) \n                if beg_pca >=0 and beg_pca<=len(datablock)-2 and end_pca>0 and end_pca<len(datablock): \n                    GoOn=1\n                else:\n                    print(\"Bad entry of indices - try again\")\n                    end_pca=len(datablock)-1\n                    beg_pca=0\n            GoOn=0\n            while GoOn==0:\n                ct=input('Enter Calculation Type: best-fit line,  plane or fisher mean [l]/p/f :  ' )\n                if ct==\"\" or ct==\"l\": \n                    calculation_type=\"DE-BFL\"\n                    GoOn=1 # all good\n                elif ct=='p':\n                    calculation_type=\"DE-BFP\"\n                    GoOn=1 # all good\n                elif ct=='f':\n                    calculation_type=\"DE-FM\"\n                    GoOn=1 # all good\n                else: \n                    print(\"bad entry of calculation type: try again. \") # keep going\n                pmagplotlib.plot_zed(ZED,datablock,angle,s,SIunits) # plot the data\n                mpars=pmag.domean(datablock,beg_pca,end_pca,calculation_type) # get best-fit direction/great circle\n                pmagplotlib.plot_dir(ZED,mpars,datablock,angle) # plot the best-fit direction/great circle\n                print('Specimen, calc_type, N, min, max, MAD, dec, inc')\n                if units=='mT':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]*1e3,mpars[\"measurement_step_max\"]*1e3,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n                if units=='C':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]-273,mpars[\"measurement_step_max\"]-273,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n                if units=='U':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"],mpars[\"measurement_step_max\"],mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n        pmagplotlib.draw_figs(ZED)\n    else:\n        print(beg_pca,end_pca)\n        if beg_pca!=\"\" and end_pca!=\"\":\n            pmagplotlib.plot_zed(ZED,datablock,angle,s,SIunits) # plot the data\n            mpars=pmag.domean(datablock,beg_pca,end_pca,calculation_type) # get best-fit direction/great circle\n            pmagplotlib.plot_dir(ZED,mpars,datablock,angle) # plot the best-fit direction/great circle\n            print('Specimen, calc_type, N, min, max, MAD, dec, inc')\n            if units=='mT':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]*1e3,mpars[\"measurement_step_max\"]*1e3,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n            if units=='C':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"]-273,mpars[\"measurement_step_max\"]-273,mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n            if units=='U':print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"],mpars[\"measurement_step_max\"],mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n        files={}\n        for key in list(ZED.keys()):\n            files[key]=s+'_'+key+'.'+fmt \n        pmagplotlib.save_plots(ZED,files)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_for_reqd_cols(data, reqd_cols):\n    missing = []\n    for col in reqd_cols:\n        if col not in data[0]:\n            missing.append(col)\n    return missing", "response": "Check data for required columns"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    # reset log files\n    for fname in ['log.txt', 'errors.txt']:\n        f = os.path.join(os.getcwd(), fname)\n        if os.path.exists(f):\n            os.remove(f)\n    dirlist = ['./']\n    dir_path = os.getcwd()\n    #\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index(\"-fmt\")\n        fmt = sys.argv[ind + 1]\n    else:\n        fmt = 'png'\n    if '-f' in sys.argv:\n        ind = sys.argv.index(\"-f\")\n        filelist = [sys.argv[ind + 1]]\n    else:\n        filelist = os.listdir(dir_path)\n    ## initialize some variables\n    samp_file = 'samples.txt'\n    azimuth_key = 'azimuth'\n    meas_file = 'measurements.txt'\n    loc_key = 'location'\n    loc_file = 'locations.txt'\n    method_key = 'method_codes'\n    dec_key = 'dir_dec'\n    inc_key = 'dir_inc'\n    tilt_corr_key = \"dir_tilt_correction\"\n    aniso_tilt_corr_key = \"aniso_tilt_correction\"\n    hyst_bcr_key = \"hyst_bcr\"\n    hyst_mr_key = \"hyst_mr_moment\"\n    hyst_ms_key = \"hyst_ms_moment\"\n    hyst_bc_key = \"hyst_bc\"\n    Mkeys = ['magnitude', 'magn_moment', 'magn_volume', 'magn_mass']\n    results_file = 'sites.txt'\n    hyst_file = 'specimens.txt'\n    aniso_file = 'specimens.txt'\n    # create contribution and propagate data throughout\n    con = cb.Contribution()\n    con.propagate_location_to_measurements()\n    con.propagate_location_to_specimens()\n    con.propagate_location_to_samples()\n    if not con.tables:\n        print('-E- No MagIC tables could be found in this directory')\n        error_log(\"No MagIC tables found\")\n        return\n    # try to get the contribution id for error logging\n    con_id = \"\"\n    if 'contribution' in con.tables:\n        if 'id' in con.tables['contribution'].df.columns:\n            con_id = con.tables['contribution'].df.iloc[0]['id']\n    # check to see if propagation worked, otherwise you can't plot by location\n    lowest_table = None\n    for table in con.ancestry:\n        if table in con.tables:\n            lowest_table = table\n            break\n\n    do_full_directory = False\n    # check that locations propagated down to the lowest table in the contribution\n    if 'location' in con.tables[lowest_table].df.columns:\n        if 'locations' not in con.tables:\n            info_log('location names propagated to {}, but could not be validated'.format(lowest_table))\n        # are there any locations in the lowest table?\n        elif not all(con.tables[lowest_table].df['location'].isnull()):\n            locs = con.tables['locations'].df.index.unique()\n            lowest_locs = con.tables[lowest_table].df['location'].unique()\n            incorrect_locs = set(lowest_locs).difference(set(locs))\n            # are they actual locations?\n            if not incorrect_locs:\n                info_log('location names propagated to {}'.format(lowest_table))\n            else:\n                do_full_directory = True\n                error_log('location names did not propagate fully to {} table (looks like there are some naming inconsistencies between tables)'.format(lowest_table), con_id=con_id)\n        else:\n            do_full_directory = True\n            error_log('could not propagate location names down to {} table'.format(lowest_table), con_id=con_id)\n    else:\n        do_full_directory = True\n        error_log('could not propagate location names down to {} table'.format(lowest_table), con_id=con_id)\n\n    all_data = {}\n    all_data['measurements'] = con.tables.get('measurements', None)\n    all_data['specimens'] = con.tables.get('specimens', None)\n    all_data['samples'] = con.tables.get('samples', None)\n    all_data['sites'] = con.tables.get('sites', None)\n    all_data['locations'] = con.tables.get('locations', None)\n    if 'locations' in con.tables:\n        locations = con.tables['locations'].df.index.unique()\n    else:\n        locations = ['']\n    dirlist = [loc for loc in locations if cb.not_null(loc, False) and loc != 'nan']\n    if not dirlist:\n        dirlist = [\"./\"]\n    if do_full_directory:\n        dirlist = [\"./\"]\n\n    # plot the whole contribution as one location\n    if dirlist == [\"./\"]:\n        error_log('plotting the entire contribution as one location', con_id=con_id)\n        for fname in os.listdir(\".\"):\n            if fname.endswith(\".txt\"):\n                shutil.copy(fname, \"tmp_\" + fname)\n\n    # if possible, go through all data by location\n    # use tmp_*.txt files to separate out by location\n\n    for loc in dirlist:\n        print('\\nworking on: ', loc)\n\n        def get_data(dtype, loc_name):\n            \"\"\"\n            Extract data of type dtype for location loc_name.\n            Write tmp_dtype.txt files if possible.\n            \"\"\"\n            if cb.not_null(all_data[dtype], False):\n                data_container = all_data[dtype]\n                if loc_name == \"./\":\n                    data_df = data_container.df\n                else:\n                    # awkward workaround for chars like \"(\" and \"?\" that break in regex\n                    try:\n                        data_df = data_container.df[data_container.df['location'].astype(str).str.contains(loc_name, na=False)]\n                    except: #sre_constants.error:\n                        data_df = data_container.df[data_container.df['location'] == loc_name]\n\n                data = data_container.convert_to_pmag_data_list(df=data_df)\n                res = data_container.write_magic_file('tmp_{}.txt'.format(dtype), df=data_df)\n                if not res:\n                    return []\n                return data\n\n        meas_data = get_data('measurements', loc)\n        spec_data = get_data('specimens', loc)\n        samp_data = get_data('samples', loc)\n        site_data = get_data('sites', loc)\n        loc_data = get_data('locations', loc)\n\n        if loc == \"./\":  # if you can't sort by location, do everything together\n            try:\n                meas_data = con.tables['measurements'].convert_to_pmag_data_list()\n            except KeyError:\n                meas_data = None\n            try:\n                spec_data = con.tables['specimens'].convert_to_pmag_data_list()\n            except KeyError:\n                spec_data = None\n            try:\n                samp_data = con.tables['samples'].convert_to_pmag_data_list()\n            except KeyError:\n                samp_data = None\n            try:\n                site_data = con.tables['sites'].convert_to_pmag_data_list()\n            except KeyError:\n                site_data = None\n\n        crd = 's'\n        if samp_file in filelist and samp_data:  # find coordinate systems\n            samps = samp_data\n            file_type = \"samples\"\n            # get all non blank sample orientations\n            Srecs = pmag.get_dictitem(samps, azimuth_key, '', 'F')\n            if len(Srecs) > 0:\n                crd = 'g'\n                print('using geographic coordinates')\n            else:\n                print('using specimen coordinates')\n        else:\n            if VERBOSE:\n                print('-I- No sample data found')\n        if meas_file in filelist and meas_data:  # start with measurement data\n            print('working on measurements data')\n            data = meas_data\n            file_type = 'measurements'\n            # looking for  zeq_magic possibilities\n            # get all non blank method codes\n            AFZrecs = pmag.get_dictitem(data, method_key, 'LT-AF-Z', 'has')\n            # get all non blank method codes\n            TZrecs = pmag.get_dictitem(data, method_key, 'LT-T-Z', 'has')\n            # get all non blank method codes\n            MZrecs = pmag.get_dictitem(data, method_key, 'LT-M-Z', 'has')\n            # get all dec measurements\n            Drecs = pmag.get_dictitem(data, dec_key, '', 'F')\n            # get all inc measurements\n            Irecs = pmag.get_dictitem(data, inc_key, '', 'F')\n            for key in Mkeys:\n                Mrecs = pmag.get_dictitem(\n                    data, key, '', 'F')  # get intensity data\n                if len(Mrecs) > 0:\n                    break\n            # potential for stepwise demag curves\n            if len(AFZrecs) > 0 or len(TZrecs) > 0 or len(MZrecs) > 0 and len(Drecs) > 0 and len(Irecs) > 0 and len(Mrecs) > 0:\n                CMD = 'zeq_magic.py -f tmp_measurements.txt -fsp tmp_specimens.txt -fsa tmp_samples.txt -fsi tmp_sites.txt -sav -fmt ' + fmt + ' -crd ' + crd + \" -new\"\n                print(CMD)\n                info_log(CMD, loc)\n                os.system(CMD)\n            # looking for  thellier_magic possibilities\n            if len(pmag.get_dictitem(data, method_key, 'LP-PI-TRM', 'has')) > 0:\n                CMD = 'thellier_magic.py -f tmp_measurements.txt -fsp tmp_specimens.txt -sav -fmt ' + fmt\n                print(CMD)\n                info_log(CMD, loc)\n                os.system(CMD)\n            # looking for hysteresis possibilities\n            if len(pmag.get_dictitem(data, method_key, 'LP-HYS', 'has')) > 0:  # find hyst experiments\n                # check for reqd columns\n                missing = check_for_reqd_cols(data, ['treat_temp'])\n                if missing:\n                    error_log('LP-HYS method code present, but required column(s) [{}] missing'.format(\", \".join(missing)), loc, \"quick_hyst.py\", con_id=con_id)\n                else:\n                    CMD = 'quick_hyst.py -f tmp_measurements.txt -sav -fmt ' + fmt\n                    print(CMD)\n                    info_log(CMD, loc)\n                    os.system(CMD)\n            # equal area plots of directional data\n            # at measurment level (by specimen)\n\n            if data:\n                missing = check_for_reqd_cols(data, ['dir_dec', 'dir_inc'])\n                if not missing:\n                    CMD = \"eqarea_magic.py -f tmp_measurements.txt -obj spc -sav -no-tilt -fmt \" + fmt\n                    print(CMD)\n                    os.system(CMD)\n                    info_log(CMD, loc, \"eqarea_magic.py\")\n\n        else:\n            if VERBOSE:\n                print('-I- No measurement data found')\n\n        # site data\n        if results_file in filelist and site_data:\n            print('-I- result file found', results_file)\n            data = site_data\n            file_type = 'sites'\n            print('-I- working on site directions')\n            print('number of datapoints: ', len(data), loc)\n            dec_key = 'dir_dec'\n            inc_key = 'dir_inc'\n            int_key = 'int_abs'\n            SiteDIs = pmag.get_dictitem(data, dec_key, \"\", 'F')  # find decs\n            SiteDIs = pmag.get_dictitem(\n                SiteDIs, inc_key, \"\", 'F')  # find decs and incs\n            dir_data_found = len(SiteDIs)\n            print('{} Dec/inc pairs found'.format(dir_data_found))\n            if SiteDIs:\n                # then convert tilt_corr_key to correct format\n                old_SiteDIs = SiteDIs\n                SiteDIs = []\n                for rec in old_SiteDIs:\n                    if tilt_corr_key not in rec:\n                        rec[tilt_corr_key] = \"0\"\n                    # make sure tilt_corr_key is a correct format\n                    try:\n                        rec[tilt_corr_key] = str(int(float(rec[tilt_corr_key])))\n                    except ValueError:\n                        rec[tilt_corr_key] = \"0\"\n                    SiteDIs.append(rec)\n\n                print('number of individual directions: ', len(SiteDIs))\n                # tilt corrected coordinates\n                SiteDIs_t = pmag.get_dictitem(SiteDIs, tilt_corr_key, '100',\n                                              'T', float_to_int=True)\n                print('number of tilt corrected directions: ', len(SiteDIs_t))\n                SiteDIs_g = pmag.get_dictitem(\n                    SiteDIs, tilt_corr_key, '0', 'T', float_to_int=True)  # geographic coordinates\n                print('number of geographic  directions: ', len(SiteDIs_g))\n                SiteDIs_s = pmag.get_dictitem(\n                    SiteDIs, tilt_corr_key, '-1', 'T', float_to_int=True)  # sample coordinates\n                print('number of sample  directions: ', len(SiteDIs_s))\n                SiteDIs_x = pmag.get_dictitem(\n                    SiteDIs, tilt_corr_key, '', 'T')  # no coordinates\n                print('number of no coordinates  directions: ', len(SiteDIs_x))\n                if len(SiteDIs_t) > 0 or len(SiteDIs_g) > 0 or len(SiteDIs_s) > 0 or len(SiteDIs_x) > 0:\n                    CRD = \"\"\n                    if len(SiteDIs_t) > 0:\n                        CRD = ' -crd t'\n                    elif len(SiteDIs_g) > 0:\n                        CRD = ' -crd g'\n                    elif len(SiteDIs_s) > 0:\n                        CRD = ' -crd s'\n                    CMD = 'eqarea_magic.py -f tmp_sites.txt -fsp tmp_specimens.txt -fsa tmp_samples.txt -flo tmp_locations.txt -sav -fmt ' + fmt + CRD\n                    print(CMD)\n                    info_log(CMD, loc)\n                    os.system(CMD)\n                else:\n                    if dir_data_found:\n                        error_log('{} dec/inc pairs found, but no equal area plots were made'.format(dir_data_found), loc, \"equarea_magic.py\", con_id=con_id)\n            #\n            print('-I- working on VGP map')\n            VGPs = pmag.get_dictitem(\n                SiteDIs, 'vgp_lat', \"\", 'F')  # are there any VGPs?\n            if len(VGPs) > 0:  # YES!\n                CMD = 'vgpmap_magic.py -f tmp_sites.txt -prj moll -res c -sym ro 5 -sav -fmt png'\n                print(CMD)\n                info_log(CMD, loc, 'vgpmap_magic.py')\n                os.system(CMD)\n            else:\n                print('-I- No vgps found')\n\n            print('-I- Look for intensities')\n            # is there any intensity data?\n            if site_data:\n                if int_key in site_data[0].keys():\n                    # old way, wasn't working right:\n                    #CMD = 'magic_select.py  -key ' + int_key + ' 0. has -F tmp1.txt -f tmp_sites.txt'\n                    Selection = pmag.get_dictkey(site_data, int_key, dtype=\"f\")\n                    with open('intensities.txt', 'w') as out:\n                        for rec in Selection:\n                            if rec != 0:\n                                out.write(str(rec * 1e6) + \"\\n\")\n\n                    loc = loc.replace(\" \", \"_\")\n                    if loc == \"./\":\n                        loc_name = \"\"\n                    else:\n                        loc_name = loc\n                    histfile = 'LO:_' + loc_name + \\\n                        '_TY:_intensities_histogram:_.' + fmt\n                    # maybe run histplot.main here instead, so you can return an error message\n                    CMD = \"histplot.py -twin -b 1 -xlab 'Intensity (uT)' -sav -f intensities.txt -F \" + histfile\n                    os.system(CMD)\n                    info_log(CMD, loc)\n                    print(CMD)\n                else:\n                    print('-I- No intensities found')\n            else:\n                print('-I- No intensities found')\n\n        ##\n        if hyst_file in filelist and spec_data:\n            print('working on hysteresis', hyst_file)\n            data = spec_data\n            file_type = 'specimens'\n            hdata = pmag.get_dictitem(data, hyst_bcr_key, '', 'F')\n            hdata = pmag.get_dictitem(hdata, hyst_mr_key, '', 'F')\n            hdata = pmag.get_dictitem(hdata, hyst_ms_key, '', 'F')\n            # there are data for a dayplot\n            hdata = pmag.get_dictitem(hdata, hyst_bc_key, '', 'F')\n            if len(hdata) > 0:\n                CMD = 'dayplot_magic.py -f tmp_specimens.txt -sav -fmt ' + fmt\n                info_log(CMD, loc)\n                print(CMD)\n            else:\n                print('no hysteresis data found')\n        if aniso_file in filelist and spec_data:  # do anisotropy plots if possible\n            print('working on anisotropy', aniso_file)\n            data = spec_data\n            file_type = 'specimens'\n\n            # make sure there is some anisotropy data\n            if not data:\n                print('No anisotropy data found')\n            elif 'aniso_s' not in data[0]:\n                print('No anisotropy data found')\n            else:\n                # get specimen coordinates\n                if aniso_tilt_corr_key not in data[0]:\n                    sdata = data\n                else:\n                    sdata = pmag.get_dictitem(\n                        data, aniso_tilt_corr_key, '-1', 'T', float_to_int=True)\n                # get specimen coordinates\n                gdata = pmag.get_dictitem(\n                    data, aniso_tilt_corr_key, '0', 'T', float_to_int=True)\n                # get specimen coordinates\n                tdata = pmag.get_dictitem(\n                    data, aniso_tilt_corr_key, '100', 'T', float_to_int=True)\n                CRD = \"\"\n                CMD = 'aniso_magic.py -x -B -sav -fmt ' + fmt + \" -new\"\n                if len(sdata) > 3:\n                    CMD = CMD + ' -crd s'\n                    print(CMD)\n                    info_log(CMD, loc)\n                    os.system(CMD)\n                if len(gdata) > 3:\n                    CMD = CMD + ' -crd g'\n                    print(CMD)\n                    info_log(CMD, loc)\n                    os.system(CMD)\n                if len(tdata) > 3:\n                    CMD = CMD + ' -crd t'\n                    print(CMD)\n                    info_log(CMD, loc)\n                    os.system(CMD)\n        # remove temporary files\n        for fname in glob.glob('tmp*.txt'):\n            os.remove(fname)\n        try:\n            os.remove('intensities.txt')\n        except FileNotFoundError:\n            pass\n    if loc_file in filelist and loc_data:\n        #data, file_type = pmag.magic_read(loc_file)  # read in location data\n        data = loc_data\n        print('-I- working on pole map')\n        poles = pmag.get_dictitem(\n            data, 'pole_lat', \"\", 'F')  # are there any poles?\n        poles = pmag.get_dictitem(\n            poles, 'pole_lon', \"\", 'F')  # are there any poles?\n        if len(poles) > 0:  # YES!\n            CMD = 'polemap_magic.py -sav -fmt png -rev gv 40'\n            print(CMD)\n            info_log(CMD, \"all locations\", \"polemap_magic.py\")\n            os.system(CMD)\n        else:\n            print('-I- No poles found')\n    thumbnails.make_thumbnails(dir_path)", "response": "This function is the main function for the make_magic_plots. py script. It will take the magic directory and make plots and returns a dictionary of all the available data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnaming core_depthplot.py DESCRIPTION plots various measurements versus core_depth or age. plots data flagged as 'FS-SS-C' as discrete samples. SYNTAX core_depthplot.py [command line options] # or, for Anaconda users: core_depthplot_anaconda [command line options] OPTIONS -h prints help message and quits -f FILE: specify input measurments format file -fsum FILE: specify input LIMS database (IODP) core summary csv file -fwig FILE: specify input depth,wiggle to plot, in magic format with sample_core_depth key for depth -fsa FILE: specify input er_samples format file from magic for depth -fa FILE: specify input ages format file from magic for age NB: must have either -fsa OR -fa (not both) -fsp FILE sym size: specify input zeq_specimen format file from magic, sym and size NB: PCAs will have specified color, while fisher means will be white with specified color as the edgecolor -fres FILE specify input pmag_results file from magic, sym and size -LP [AF,T,ARM,IRM, X] step [in mT,C,mT,mT, mass/vol] to plot -S do not plot blanket treatment data (if this is set, you don't need the -LP) -sym SYM SIZE, symbol, size for continuous points (e.g., ro 5, bs 10, g^ 10 for red dot, blue square, green triangle), default is blue dot at 5 pt -D do not plot declination -M do not plot magnetization -log plot magnetization on a log scale -L do not connect dots with a line -I do not plot inclination -d min max [in m] depth range to plot -n normalize by weight in er_specimen table -Iex: plot the expected inc at lat - only available for results with lat info in file -ts TS amin amax: plot the GPTS for the time interval between amin and amax (numbers in Ma) TS: [ck95, gts04, gts12] -ds [mbsf,mcd] specify depth scale, mbsf default -fmt [svg, eps, pdf, png] specify output format for plot (default: svg) -sav save plot silently DEFAULTS: Measurements file: measurements.txt Samples file: samples.txt NRM step Summary file: none", "response": "def main():\n    \"\"\"\n    NAME\n        core_depthplot.py\n\n    DESCRIPTION\n        plots various measurements versus core_depth or age.  plots data flagged as 'FS-SS-C' as discrete samples.\n\n    SYNTAX\n        core_depthplot.py [command line options]\n        # or, for Anaconda users:\n        core_depthplot_anaconda [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -f FILE: specify input measurments format file\n        -fsum FILE: specify input LIMS database (IODP) core summary csv file\n        -fwig FILE: specify input depth,wiggle to plot, in magic format with sample_core_depth key for depth\n        -fsa FILE: specify input er_samples format file from magic for depth\n        -fa FILE: specify input ages format file from magic for age\n              NB: must have either -fsa OR -fa (not both)\n        -fsp FILE sym size: specify input zeq_specimen format file from magic, sym and size\n              NB: PCAs will have specified color, while fisher means will be white with specified color as the edgecolor\n        -fres FILE specify input pmag_results file from magic, sym and size\n        -LP [AF,T,ARM,IRM, X] step [in mT,C,mT,mT, mass/vol] to plot\n        -S do not plot blanket treatment data (if this is set, you don't need the -LP)\n        -sym SYM SIZE, symbol, size for continuous points (e.g., ro 5, bs 10, g^ 10 for red dot, blue square, green triangle), default is blue dot at 5 pt\n        -D do not plot declination\n        -M do not plot magnetization\n        -log  plot magnetization  on a log scale\n        -L do not connect dots with a line\n        -I do not plot inclination\n        -d min max [in m] depth range to plot\n        -n normalize by weight in er_specimen table\n        -Iex: plot the expected inc at lat - only available for results with lat info in file\n        -ts TS amin amax: plot the GPTS for the time interval between amin and amax (numbers in Ma)\n           TS: [ck95, gts04, gts12]\n        -ds [mbsf,mcd] specify depth scale, mbsf default\n        -fmt [svg, eps, pdf, png] specify output format for plot (default: svg)\n        -sav save plot silently\n\n     DEFAULTS:\n         Measurements file: measurements.txt\n         Samples file: samples.txt\n         NRM step\n         Summary file: none\n    \"\"\"\n\n    args = sys.argv\n    if '-h' in args:\n        print(main.__doc__)\n        sys.exit()\n\n    dataframe = extractor.command_line_dataframe([ ['f', False, 'measurements.txt'], ['fsum', False, ''],\n                                                   ['fwig', False, ''], ['fsa', False, ''],\n                                                   ['fa', False, ''], ['fsp', False, ''],\n                                                   ['fres', False, '' ],  ['fmt', False, 'svg'],\n                                                   ['LP', False,  ''], ['n', False, False],\n                                                   ['d', False, '-1 -1'], ['ts', False, ''],\n                                                   ['WD', False, '.'], ['L', False, True],\n                                                   ['S', False, True], ['D', False, True],\n                                                   ['I', False, True], ['M', False, True],\n                                                   ['log', False,  0],\n                                                   ['ds', False, 'sample_core_depth'],\n                                                   ['sym', False, 'bo 5'], ['ID', False, '.'],\n                                                   ['sav', False, False], ['DM', False, 3]])\n\n    checked_args = extractor.extract_and_check_args(args, dataframe)\n    meas_file, sum_file, wig_file, samp_file, age_file, spc_file, res_file, fmt, meth, norm, depth, timescale, dir_path, pltLine, pltSus, pltDec, pltInc, pltMag, logit, depth_scale, symbol, input_dir, save, data_model_num = extractor.get_vars(\n        ['f', 'fsum', 'fwig', 'fsa', 'fa', 'fsp', 'fres', 'fmt',  'LP', 'n', 'd', 'ts', 'WD', 'L', 'S', 'D', 'I', 'M', 'log', 'ds', 'sym', 'ID', 'sav', 'DM'], checked_args)\n\n    # format some variables\n    # format symbol/size\n    try:\n        sym, size = symbol.split()\n        size = int(size)\n    except:\n        print('you should provide -sym in this format: ro 5')\n        print('using defaults instead')\n        sym, size = 'ro', 5\n\n    # format result file, symbol, size\n    if res_file:\n        try:\n            res_file, res_sym, res_size = res_file.split()\n        except:\n            print('you must provide -fres in this format: -fres filename symbol size')\n            print(\n                'could not parse {}, defaulting to using no result file'.format(res_file))\n            res_file, res_sym, res_size = '', '', 0\n    else:\n        res_file, res_sym, res_size = '', '', 0\n\n    # format specimen file, symbol, size\n    if spc_file:\n        try:\n            spc_file, spc_sym, spc_size = spc_file.split()\n        except:\n            print('you must provide -fsp in this format: -fsp filename symbol size')\n            print(\n                'could not parse {}, defaulting to using no specimen file'.format(spc_file))\n            spc_file, spc_sym, spc_size = '', '', 0\n    else:\n        spc_file, spc_sym, spc_size = '', '', 0\n\n    # format min/max depth\n    try:\n        dmin, dmax = depth.split()\n    except:\n        print('you must provide -d in this format: -d dmin dmax')\n        print('could not parse {}, defaulting to plotting all depths'.format(depth))\n        dmin, dmax = -1, -1\n\n    # format timescale, min/max time\n    if timescale:\n        try:\n            timescale, amin, amax = timescale.split()\n            pltTime = True\n        except:\n            print(\n                'you must provide -ts in this format: -ts timescale minimum_age maximum_age')\n            print(\n                'could not parse {}, defaulting to using no timescale'.format(timescale))\n            timescale, amin, amax = None, -1, -1\n            pltTime = False\n    else:\n        timescale, amin, amax = None, -1, -1\n        pltTime = False\n\n    # format norm and wt_file\n    if norm and not isinstance(norm, bool):\n        wt_file = norm\n        norm = True\n    else:\n        norm = False\n        wt_file = ''\n\n    # format list of protcols and step\n    try:\n        method, step = meth.split()\n    except:\n        print(\n            'To use the -LP flag you must provide both the protocol and the step in this format:\\n-LP [AF,T,ARM,IRM, X] step [in mT,C,mT,mT, mass/vol] to plot')\n        print('Defaulting to using no protocol')\n        method, step = 'LT-NO', 0\n\n    # list of varnames\n    #['f', 'fsum', 'fwig', 'fsa', 'fa', 'fsp', 'fres', 'fmt',  'LP', 'n', 'd', 'ts', 'WD', 'L', 'S', 'D', 'I', 'M', 'log', 'ds', 'sym' ]\n    #meas_file, sum_file, wig_file, samp_file, age_file, spc_file, res_file, fmt, meth, norm, depth, timescale, dir_path, pltLine, pltSus, pltDec, pltInc, pltMag, logit, depth_scale, symbol\n\n    fig, figname = ipmag.core_depthplot(input_dir, meas_file, spc_file, samp_file, age_file, sum_file, wt_file, depth_scale, dmin, dmax, sym, size,\n                                        spc_sym, spc_size, method, step, fmt, pltDec, pltInc, pltMag, pltLine, pltSus, logit, pltTime, timescale, amin, amax, norm, data_model_num)\n\n    if not pmagplotlib.isServer:\n        figname = figname.replace(':', '_')\n\n    if fig and save:\n        print('-I- Created plot: {}'.format(figname))\n        plt.savefig(figname)\n        return\n\n    app = wx.App(redirect=False)\n    if not fig:\n        pw.simple_warning(\n            'No plot was able to be created with the data you provided.\\nMake sure you have given all the required information and try again')\n        return False\n\n    dpi = fig.get_dpi()\n    pixel_width = dpi * fig.get_figwidth()\n    pixel_height = dpi * fig.get_figheight()\n    figname = os.path.join(dir_path, figname)\n    plot_frame = pmag_menu_dialogs.PlotFrame((int(pixel_width), int(pixel_height + 50)),\n                                             fig, figname, standalone=True)\n\n    app.MainLoop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames calculate_bestfit_line_through_deagnetization DESCRIPTION calculate_bestfit_line_through_deagnetization is the main function for the demagnetization module.", "response": "def main():\n    \"\"\"\n    NAME\n       pca.py\n\n    DESCRIPTION\n       calculates best-fit line/plane through demagnetization data\n\n    INPUT FORMAT\n       takes specimen_name treatment intensity declination inclination  in space delimited file\n\n    SYNTAX\n       pca.py [command line options][< filename]\n\n    OPTIONS\n        -h prints help and quits\n        -f FILE\n        -dir [L,P,F][BEG][END] specify direction type, beginning and end\n          (L:line, P:plane or F:fisher mean of unit vectors)\n          BEG: first step (NRM = step zero)\n          END: last step (NRM = step zero)\n        < filename for reading from standard input\n    OUTPUT:\n        specimen_name calculation_type N beg end MAD dec inc\n        if calculation_type is 'p', dec and inc are pole to plane, otherwise, best-fit direction\n\n    EXAMPLE:\n        pca.py -dir L  1 5 <ex3.3\n        will calculate best-fit line through demagnetization steps 1 and 5 from file ex5.1\n\n    \"\"\"\n    if '-h' in sys.argv: # check if help is needed\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    if '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines()\n    else:\n        data=sys.stdin.readlines() # read in data from standard input\n    if '-dir' in sys.argv: # \n        ind=sys.argv.index('-dir')\n        typ=sys.argv[ind+1]\n        if typ=='L': calculation_type='DE-BFL'\n        if typ=='P': calculation_type='DE-BFP'\n        if typ=='F': calculation_type='DE-FM'\n        beg_pca = int(sys.argv[ind+2])\n        end_pca = int(sys.argv[ind+3])\n#\n#\n    datablock= [] # set up list for data\n    s=\"\"\n    ind=0\n    for line in data:   # read in the data from standard input\n        rec=line.split() # split each line on space to get records\n        if s==\"\":\n            s=rec[0]\n            print(s, calculation_type)\n        print(ind,rec[1],rec[3],rec[4],rec[2])\n        ind+=1\n        datablock.append([float(rec[1]),float(rec[3]),float(rec[4]),float(rec[2]),'0']) # treatment,dec,inc,int,dummy\n    mpars=pmag.domean(datablock,beg_pca,end_pca,calculation_type)\n    if calculation_type==\"DE-FM\":\n        print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"],mpars[\"measurement_step_max\"],mpars[\"specimen_a95\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))\n    else:\n        print('%s %s %i  %6.2f %6.2f %6.1f %7.1f %7.1f' % (s,calculation_type,mpars[\"specimen_n\"],mpars[\"measurement_step_min\"],mpars[\"measurement_step_max\"],mpars[\"specimen_mad\"],mpars[\"specimen_dec\"],mpars[\"specimen_inc\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames pt_rot.py DESCRIPTION rotates pt according to specified age and plate SYNTAX pt_rot.py [command line options] OPTIONS -h prints help and quits -f file with lon lat plate age Dplate as space delimited input Dplate is the destination plate coordinates desires - default is \"fixed south africa\" Dplate should be one of: [nwaf, neaf,saf,aus, eur, ind, sam, ant, grn, nam] -ff file Efile, file has lat lon data file and Efile has sequential rotation poles: Elat Elon Omega -F OFILE, output sites (pmag_results) formatted file with rotated points stored in pole_lon, pole_lat (vgp_lon, vgp_lat). (data_model=2.5) default is to print out rotated lon, lat to standard output -dm [2.5,3] set data model for output. Default is 3", "response": "def main():\n    \"\"\"\n    NAME \n        pt_rot.py \n\n    DESCRIPTION\n        rotates pt according to specified age and plate\n \n    SYNTAX\n        pt_rot.py [command line options]\n\n    OPTIONS\n        -h prints help and quits\n        -f file with lon lat plate age Dplate as space delimited input\n           Dplate is the destination plate coordinates desires \n           - default is \"fixed south africa\"\n           Dplate should be one of: [nwaf, neaf,saf,aus, eur, ind, sam, ant, grn, nam]\n        -ff file Efile,   file  has lat lon data file and Efile has sequential rotation poles: Elat Elon Omega \n        -F OFILE, output sites (pmag_results) formatted file with rotated points stored in pole_lon, pole_lat (vgp_lon, vgp_lat).  (data_model=2.5)\n           default is to print out rotated lon, lat to standard output\n        -dm [2.5,3] set data model for output.  Default is 3 \n    \"\"\"\n    dir_path='.'\n    PTS=[]\n    ResRecs=[]\n    ofile=\"\"\n    data_model=3\n    Dplates=['nwaf', 'neaf','saf','aus', 'eur', 'ind', 'sam', 'ant', 'grn', 'nam']\n    if '-WD' in sys.argv:\n        ind = sys.argv.index('-WD')\n        dir_path=sys.argv[ind+1]\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-F' in sys.argv:\n        ind = sys.argv.index('-F')\n        ofile=dir_path+'/'+sys.argv[ind+1]\n    if '-dm' in sys.argv:\n        ind = sys.argv.index('-dm')\n        data_model=dir_path+'/'+sys.argv[ind+1]\n    if '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        file=dir_path+'/'+sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines()\n    elif '-ff' in sys.argv:\n        ind = sys.argv.index('-ff')\n        file=dir_path+'/'+sys.argv[ind+1]\n        f=open(file,'r')\n        data=f.readlines()\n        Efile=dir_path+'/'+sys.argv[ind+2]\n        f=open(Efile,'r')\n        edata=f.readlines()\n        Poles=[]\n        for p in edata:\n             rec=p.split()\n             pole=[float(rec[0]),float(rec[1]),float(rec[2])] # pole is lat/lon/omega\n             Poles.append(pole)\n    else:\n        data=sys.stdin.readlines()\n    polelatkey,polelonkey='pole_lat','pole_lon'\n    if data_model!=3:\n        polelatkey,polelonkey='vgp_lat','vgp_lon'\n    for line in data:\n        PtRec={}\n        rec=line.split()\n        PtRec['site_lon']=rec[0]\n        PtRec['site_lat']=rec[1]\n        if '-ff' in sys.argv:\n            pt_lat,pt_lon=float(rec[1]),float(rec[0])\n            for pole in Poles:\n                ptrot= pmag.pt_rot(pole,[pt_lat],[pt_lon])\n                pt_lat=ptrot[0][0]\n                pt_lon=ptrot[1][0]\n            if ofile==\"\":\n                print(ptrot[1][0], ptrot[0][0])\n            else:\n                ResRec={polelonkey: '%7.1f'%(ptrot[0][0]),polelatkey:'%7.1f'%( ptrot[1][0])}\n                ResRecs.append(ResRec)\n        else:\n            PtRec['cont']=rec[2]\n            if PtRec['cont']=='af':PtRec['cont']='saf' # use fixed south africa\n            PtRec['age']=rec[3]\n            if len(rec)>4:\n               PtRec['dcont']=rec[4]\n            PTS.append(PtRec)\n    if '-ff' not in sys.argv:\n        for pt in PTS:\n            pole='not specified'\n            pt_lat=float(pt['site_lat'])\n            pt_lon=float(pt['site_lon'])\n            age=float(pt['age'])\n            ptrot=[[pt_lat],[pt_lon]]\n            if pt['cont']=='ib':\n                pole=frp.get_pole(pt['cont'],age)\n                ptrot= pmag.pt_rot(pole,[pt_lat],[pt_lon])\n                pt_lat=ptrot[0][0]\n                pt_lon=ptrot[1][0]\n                pt['cont']='eur'\n            if pt['cont']!='saf':\n                pole1=frp.get_pole(pt['cont'],age)\n                ptrot= pmag.pt_rot(pole1,[pt_lat],[pt_lon])\n                if 'dcont' in list(pt.keys()):\n                    pt_lat=ptrot[0][0]\n                    pt_lon=ptrot[1][0]\n                    pole=frp.get_pole(pt['dcont'],age)\n                    pole[2]=-pole[2] \n                    ptrot= pmag.pt_rot(pole,[pt_lat],[pt_lon])\n                if ofile==\"\":\n                    print(ptrot[1][0], ptrot[0][0])\n                else:\n                    ResRec={polelonkey: '%7.1f'%(ptrot[0][0]),polelatkey:'%7.1f'%( ptrot[1][0])}\n                    ResRecs.append(ResRec)\n            else:\n                if 'dcont' in list(pt.keys()):\n                    pole=frp.get_pole(pt['dcont'],age)\n                    pole[2]=-pole[2] \n                    ptrot= pmag.pt_rot(pole,[pt_lat],[pt_lon])\n                    print(ptrot)\n                    if ofile==\"\":\n                        print(ptrot[1][0], ptrot[0][0]) \n                    else:\n                        ResRec={polelonkey: '%7.1f'%(ptrot[0][0]),polelatkey:'%7.1f'%( ptrot[1][0])}\n                        ResRecs.append(ResRec)\n                else:\n                    if ofile==\"\":\n                        print(ptrot[1][0], ptrot[0][0])\n                    else:\n                        ResRec={polelonkey: '%7.1f'%(ptrot[0][0]),polelatkey:'%7.1f'%( ptrot[1][0])}\n                        ResRecs.append(ResRec)\n    if len(ResRecs)>0:\n        if data_model==3:\n            pmag.magic_write(ofile,ResRecs,'locations')\n        else:\n            pmag.magic_write(ofile,ResRecs,'pmag_results')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef requiredUnless(col_name, arg, dm, df, *args):\n    # if column name is present, no need to check if it is required\n    if col_name in df.columns:\n        return None\n    arg_list = arg.split(\",\")\n    arg_list = [argument.strip('\"') for argument in arg_list]\n    msg = \"\"\n    for a in arg_list:\n        # ignore validations that reference a different table\n        if \".\" in a:\n            continue\n        if a not in df.columns:\n            msg += \"{} column is required unless {} is present.  \".format(col_name, a)\n    if msg:\n        return msg\n    else:\n        return None\n    return None", "response": "Check if a column is required in df unless it is present in dm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef requiredUnlessTable(col_name, arg, dm, df, con=None):\n    table_name = arg\n    if col_name in df.columns:\n        return None\n    elif not con:\n        return None\n    elif table_name in con.tables:\n        return None\n    else:\n        return \"{} column is required unless table {} is present\".format(col_name, table_name)", "response": "This function checks that the given column is present in df unless it is present in contribution\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef required(col_name, arg, dm, df, *args):\n    if col_name in df.columns:\n        return None\n    else:\n        return '\"{}\" column is required'.format(col_name)", "response": "Check if a column is required in df."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a value in a column is in another table.", "response": "def isIn(row, col_name, arg, dm, df, con=None):\n    \"\"\"\n    row[col_name] must contain a value from another column.\n    If not, return error message.\n    \"\"\"\n    #grade = df.apply(func, args=(validation_name, arg, dm), axis=1)\n    cell_value = row[col_name]\n    cell_value = str(cell_value)\n    if not cell_value:\n        return None\n    elif cell_value == 'None':\n        return None\n    elif cell_value == 'nan':\n        return None\n    elif not con:\n        return None\n    # if it's in another table\n    cell_values = [v.strip(\" \") for v in cell_value.split(\":\")]\n    if \".\" in arg:\n        table_name, table_col_name = arg.split(\".\")\n        if table_name not in con.tables:\n            return None\n            #return \"Must contain a value from {} table. Missing {} table.\".format(table_name, table_name)\n        if table_col_name not in con.tables[table_name].df.columns:\n            return '{} table is missing \"{}\" column, which is required for validating \"{}\" column'.format(table_name, table_col_name, col_name)\n        possible_values = con.tables[table_name].df[table_col_name].unique()\n        for value in cell_values:\n            if value not in possible_values:\n                trunc_possible_values = [val.replace(' ', '') for val in possible_values if val]\n                trunc_cell_value = cell_value.replace(' ', '')\n                if trunc_cell_value not in trunc_possible_values:\n                    if trunc_cell_value != value:\n                        return 'This value (long): \"{}\" is not found in: {} column in {} table.  Also (short): {} is not in {}'.format(value, table_col_name, table_name, trunc_cell_value, arg)\n                    else:\n                        return 'This value: \"{}\" is not found in: {} column in {} table'.format(value, table_col_name, table_name)\n                    break\n    # if it's in the present table:\n    else:\n        possible_values = df[arg].unique()\n        for value in cell_values:\n            if value not in possible_values:\n                return 'This value: \"{}\" is not found in: {} column'.format(value, arg)\n                break\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkMax(row, col_name, arg, *args):\n    cell_value = row[col_name]\n    if not cell_value:\n        return None\n    elif isinstance(cell_value, float):\n        if np.isnan(cell_value):\n            return None\n    try:\n        arg_val = float(arg)\n    except ValueError:\n        if arg in row.index:\n            arg_val = row[arg]\n        else:\n            return None\n    if cb.is_null(arg_val):\n        return None\n    #arg = float(arg)\n    try:\n        if float(cell_value) <= float(arg_val):\n            return None\n        else:\n            return \"{} ({}) must be <= {} ({})\".format(str(cell_value), col_name, str(arg_val), str(arg))\n    # this happens when the value isn't a float (an error which will be caught elsewhere)\n    except ValueError:\n        return None", "response": "Check that the value of col_name is less than or equal to arg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cv(row, col_name, arg, current_data_model, df, con):\n    vocabulary = con.vocab.vocabularies\n    cell_value = str(row[col_name])\n    if not cell_value:\n        return None\n    elif cell_value == \"None\":\n        return None\n    cell_values = cell_value.split(\":\")\n    cell_values = [c.strip() for c in cell_values]\n    # get possible values for controlled vocabulary\n    # exclude weird unicode\n    possible_values = []\n    for val in vocabulary[col_name]:\n        try:\n            possible_values.append(str(val).lower())\n        except UnicodeEncodeError as ex:\n            print(val, ex)\n    for value in cell_values:\n        if str(value).lower() == \"nan\":\n            continue\n        elif str(value).lower() in possible_values:\n            continue\n        elif value.lower() == \"none\":\n            continue\n        else:\n            try:\n                if str(float(value)) in possible_values:\n                    continue\n            except:\n                pass\n            return '\"{}\" is not in controlled vocabulary for {}'.format(value, arg)\n    return None", "response": "This function checks if a controlled vocabulary value is in the appropriate controlled vocabulary for the given arg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_df(df, dm, con=None):\n    # check column validity\n    required_one = {}  # keep track of req'd one in group validations here\n    cols = df.columns\n    invalid_cols = [col for col in cols if col not in dm.index]\n    # go through and run all validations for the data type\n    for validation_name, validation in dm.iterrows():\n        value_type = validation['type']\n        if validation_name in df.columns:\n            output = df[validation_name].apply(test_type, args=(value_type,))\n            df[\"type_pass\" + \"_\" + validation_name + \"_\" + value_type] = output\n        #\n        val_list = validation['validations']\n        if not val_list or isinstance(val_list, float):\n            continue\n        for num, val in enumerate(val_list):\n            func_name, arg = split_func(val)\n            if arg == \"magic_table_column\":\n                continue\n            # first validate for presence\n            if func_name in presence_operations:\n                func = presence_operations[func_name]\n                #grade = func(validation_name, df, arg, dm)\n                grade = func(validation_name, arg, dm, df, con)\n                pass_col_name = \"presence_pass_\" + validation_name + \"_\" + func.__name__\n                df[pass_col_name] = grade\n            # then validate for correct values\n            elif func_name in value_operations:\n                func = value_operations[func_name]\n                if validation_name in df.columns:\n                    grade = df.apply(func, args=(validation_name, arg, dm, df, con), axis=1)\n                    col_name = \"value_pass_\" + validation_name + \"_\" + func.__name__\n                    if col_name in df.columns:\n                        num_range = list(range(1, 10))\n                        for num in num_range:\n                            if (col_name + str(num)) in df.columns:\n                                continue\n                            else:\n                                col_name = col_name + str(num)\n                                break\n                    df[col_name] = grade.astype(object)\n            # last, validate at the column group level\n            elif func_name in group_operations:\n                func = group_operations[func_name]\n                missing = func(validation_name, arg, dm, df)\n                if arg not in required_one:\n                    required_one[arg] = [missing]\n                else:\n                    required_one[arg].append(missing)\n        # format the group validation columns\n        for key, value in list(required_one.items()):\n            if None in value:\n                # this means at least one value from the required group is present,\n                # so the validation passes\n                continue\n            else:\n                # otherwise, all of the values from the required group are missing,\n                # so the validation fails\n                df[\"group_pass_{}\".format(key)] = \"you must have one column from group {}: {}\".format(key, \", \".join(value))\n\n    return df", "response": "Validate a DataFrame and return a new DataFrame with some required columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the names of all validation columns in a single node.", "response": "def get_validation_col_names(df):\n    \"\"\"\n    Input: validated pandas DataFrame (using validate_df)\n    Output: names of all value validation columns,\n            names of all presence validation columns,\n            names of all type validation columns,\n            names of all missing group columns,\n            names of all validation columns (excluding groups).\n    \"\"\"\n    value_cols = df.columns.str.match(\"^value_pass_\")\n    present_cols = df.columns.str.match(\"^presence_pass\")\n    type_cols = df.columns.str.match(\"^type_pass_\")\n    groups_missing = df.columns.str.match(\"^group_pass_\")\n    #\n    value_col_names = df.columns[value_cols]\n    present_col_names = df.columns[present_cols]\n    type_col_names = df.columns[type_cols]\n    group_missing_names = df.columns[groups_missing]\n    #\n    # all validation columns\n    validation_cols = np.where(value_cols, value_cols, present_cols)\n    validation_cols = np.where(validation_cols, validation_cols, type_cols)\n    validation_col_names = df.columns[validation_cols]\n    return value_col_names, present_col_names, type_col_names, group_missing_names, validation_col_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_row_failures(failing_items, verbose=False, outfile_name=None):\n    if outfile_name:\n        outfile = open(outfile_name, \"w\")\n        outfile.write(\"\\t\".join([\"name\", \"row_number\", \"problem_type\",\n                                 \"problem_col\", \"error_message\"]))\n        outfile.write(\"\\n\")\n    else:\n        outfile = None\n    for ind, row in failing_items.iterrows():\n        issues = row['issues']\n        string = \"{:10}  |  row number: {}\".format(ind, str(row[\"num\"]))\n        first_string = \"\\t\".join([str(ind), str(row[\"num\"])])\n        if verbose:\n            print(first_string)\n        #if outfile:\n        #    ofile.write(\"{}\\n\".format(string))\n        for key, issue in list(issues.items()):\n            issue_type, issue_col = extract_col_name(key)\n            string = \"{:10}  |  {:10}  |  {}\".format(issue_type, issue_col, issue)\n            string = \"\\t\".join([issue_type, issue_col, issue])\n            if verbose:\n                print(string)\n            if outfile:\n                outfile.write(first_string + \"\\t\" + string + \"\\n\")\n    if outfile:\n        outfile.close()", "response": "Print the failures of the given DataFrame to a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_row_failures(df, value_cols, type_cols, verbose=False, outfile=None):\n    # set temporary numeric index\n    df[\"num\"] = list(range(len(df)))\n    # get column names for value & type validations\n    names = value_cols.union(type_cols)\n    # drop all non validation columns\n    value_problems = df[names.union([\"num\"])]\n    # drop validation columns that contain no problems\n    failing_items = value_problems.dropna(how=\"all\", subset=names)\n    if not len(failing_items):\n        if verbose:\n            print(\"No problems\")\n        return []\n    failing_items = failing_items.dropna(how=\"all\", axis=1)\n    # get names of the failing items\n    bad_items = list(failing_items.index)\n    # get index numbers of the failing items\n    bad_indices = list(failing_items[\"num\"])\n    failing_items['issues'] = failing_items.drop(\"num\", axis=1).apply(make_row_dict, axis=1).values\n    # take output and print/write to file\n    print_row_failures(failing_items, verbose, outfile)\n    return failing_items", "response": "Get details on each detected issue in a row by row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bad_rows_and_cols(df, validation_names, type_col_names,\n                          value_col_names, verbose=False):\n    \"\"\"\n    Input: validated DataFrame, all validation names, names of the type columns,\n    names of the value columns, verbose (True or False).\n    Output: list of rows with bad values, list of columns with bad values,\n    list of missing (but required) columns.\n    \"\"\"\n    df[\"num\"] = list(range(len(df)))\n    problems = df[validation_names.union([\"num\"])]\n    all_problems = problems.dropna(how='all', axis=0, subset=validation_names)\n    value_problems = problems.dropna(how='all', axis=0, subset=type_col_names.union(value_col_names))\n    all_problems = all_problems.dropna(how='all', axis=1)\n    value_problems = value_problems.dropna(how='all', axis=1)\n    if not len(problems):\n        return None, None, None\n    #\n    bad_cols = all_problems.columns\n    prefixes = [\"value_pass_\", \"type_pass_\"]\n    missing_prefix = \"presence_pass_\"\n    problem_cols = []\n    missing_cols = []\n    long_missing_cols = []\n    problem_rows = []\n    for col in bad_cols:\n        pre, stripped_col = extract_col_name(col)\n        for prefix in prefixes:\n            if col.startswith(prefix):\n                problem_cols.append(stripped_col)\n                continue\n        if col.startswith(missing_prefix):\n            missing_cols.append(stripped_col)\n            long_missing_cols.append(col)\n    if len(value_problems):\n        bad_rows = list(zip(list(value_problems[\"num\"]), list(value_problems.index)))\n    else:\n        bad_rows = []\n    if verbose:\n        if bad_rows:\n            formatted_rows = [\"row: {}, name: {}\".format(row[0], row[1]) for row in bad_rows]\n            if len(bad_rows) > 5:\n                print(\"-W- these rows have problems:\\n\", \"\\n\".join(formatted_rows[:5]), \" ...\")\n                print(\"(for full error output see error file)\")\n            else:\n                print(\"-W- these rows have problems:\", \"\\n\".join(formatted_rows))\n        if problem_cols:\n            print(\"-W- these columns contain bad values:\", \", \".join(set(problem_cols)))\n        if missing_cols:\n            print(\"-W- these required columns are missing:\", \", \".join(missing_cols))\n    return bad_rows, problem_cols, missing_cols", "response": "Get bad rows and columns from a validated DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_table(the_con, dtype, verbose=False, output_dir=\".\"):\n    print(\"-I- Validating {}\".format(dtype))\n    # grab dataframe\n    current_df = the_con.tables[dtype].df\n    # grab data model\n    current_dm = the_con.tables[dtype].data_model.dm[dtype]\n    # run all validations (will add columns to current_df)\n    current_df = validate_df(current_df, current_dm, the_con)\n    # get names of the added columns\n    value_col_names, present_col_names, type_col_names, missing_groups, validation_col_names = get_validation_col_names(current_df)\n    # print out failure messages\n    ofile = os.path.join(output_dir, \"{}_errors.txt\".format(dtype))\n    failing_items = get_row_failures(current_df, value_col_names,\n                                     type_col_names, verbose, outfile=ofile)\n    bad_rows, bad_cols, missing_cols = get_bad_rows_and_cols(current_df, validation_col_names,\n                                                             value_col_names, type_col_names,\n                                                             verbose=True)\n    # delete all validation rows\n    current_df.drop(validation_col_names, axis=1, inplace=True)\n    current_df.drop(missing_groups, axis=1, inplace=True)\n    if len(failing_items):\n        print(\"-I- Complete list of row errors can be found in {}\".format(ofile))\n        return dtype, bad_rows, bad_cols, missing_cols, missing_groups, failing_items\n    elif len(missing_cols) or len(missing_groups):\n        print(\"-I- You are missing some required headers\")\n        if len(missing_cols):\n            print(\"-I- You are missing these required headers: {}\".format(\", \".join(missing_cols)))\n        if len(missing_groups):\n            formatted_groups = [group[11:] for group in missing_groups]\n            print('-I- You need at least one header from these groups: {}'.format(\", \".join(formatted_groups)))\n        else:\n            formatted_groups = []\n        return dtype, bad_rows, bad_cols, missing_cols, formatted_groups, failing_items\n    else:\n        print(\"-I- No row errors found!\")\n        return False", "response": "Validate a single table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_contribution(the_con):\n    passing = True\n    for dtype in list(the_con.tables.keys()):\n        print(\"validating {}\".format(dtype))\n        fail = validate_table(the_con, dtype)\n        if fail:\n            passing = False\n        print('--')", "response": "Validate a Contribution and return a list of all the tables that are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_func(string):\n    ind = string.index(\"(\")\n    return string[:ind], string[ind+1:-1].strip('\"')", "response": "Takes a string like requiredIf ( arg_name ) return the function name and the argument"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_degree_cols(df):\n    vals = ['lon_w', 'lon_e', 'lat_lon_precision', 'pole_lon',\n            'paleolon', 'paleolon_sigma',\n            'lon', 'lon_sigma', 'vgp_lon', 'paleo_lon', 'paleo_lon_sigma',\n            'azimuth', 'azimuth_dec_correction', 'dir_dec',\n            'geographic_precision', 'bed_dip_direction']\n    relevant_cols = list(set(vals).intersection(df.columns))\n    return relevant_cols", "response": "Take in a pandas DataFrame and return a list of columns that are in that DataFrame AND should be between 0 - 360 degrees."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_col_name(string):\n    prefixes = [\"presence_pass_\", \"value_pass_\", \"type_pass_\"]\n    end = string.rfind(\"_\")\n    for prefix in prefixes:\n        if string.startswith(prefix):\n            return prefix[:-6], string[len(prefix):end]\n    return string, string", "response": "Takes a string and splits it into a column name and the validation column name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_row_dict(row):\n    ind = row[row.notnull()].index\n    values = row[row.notnull()].values\n    # to transformation with extract_col_name here???\n    return dict(list(zip(ind, values)))", "response": "Takes in a DataFrame row and returns a dictionary with the row s index as key and the row s values as values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames eq_di. py . py", "response": "def main():\n    \"\"\"\n    NAME\n        eq_di.py\n    \n    DESCRIPTION\n      converts x,y pairs digitized from equal area projection to dec inc data\n    \n    SYNTAX\n        eq_di.py [command line options] [< filename]\n    \n    OPTIONS\n        -f FILE, input file\n        -F FILE, specifies output file name \n        -up if data are upper hemisphere\n    \"\"\"\n    out=\"\"\n    UP=0\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        dat=[]\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]  \n        f=open(file,'r')\n        input=f.readlines()\n    else:\n        input = sys.stdin.readlines()  # read from standard input\n    # NEW\n    ofile = \"\"\n    if '-F' in sys.argv:\n        ind=sys.argv.index('-F')\n        ofile=sys.argv[ind+1]\n        out=open(ofile, 'w + a')\n    # end NEW\n    if '-up' in sys.argv: UP=1\n    for line in input:\n        rec=line.split()\n        x,y=float(rec[1]),float(rec[0])  # swap x,y cartesian for x,y geographic\n        #d,i=pmag.doeqdi(x,y)\n        r=math.sqrt(x**2+y**2)\n        z=1.-r**2\n        t=math.asin(z)\n        if UP==1:t=-t\n        if x==0.:\n            if y<0:\n                p=3.*math.pi/2.\n            else:\n                p=old_div(math.pi,2.)\n        else:\n            p=math.atan2(y,x)\n        d,i=p*180./math.pi,t*180./math.pi\n        if d<0:d+=360.\n        # new\n        outstring = '%7.1f %7.1f'%(d,i)\n        if ofile == \"\":\n           # print '%7.1f %7.1f'%(d,i)\n            print(outstring)\n        else:\n            out.write(outstring+'\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    print(main.__doc__)\n    if '-h' in sys.argv:sys.exit() \n    cont,Int,Top=1,[],[]\n    while cont==1:\n        try: \n            Int.append(int(input(\" Enter desired treatment step interval: <return> to quit \")))\n            Top.append(int(input(\" Enter upper bound for this interval: \")))\n        except:\n            cont=0\n    pmag.chart_maker(Int,Top)", "response": "This function is called by the command line for the main function. It is used to generate a new chart for the given data entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames s_magic.py DESCRIPTION converts .s format data to measurements format. SYNTAX s_magic.py [command line options] OPTIONS -h prints help message and quits -DM DATA_MODEL_NUM data model number (default is 3) -f SFILE specifies the .s file name -sig last column has sigma -typ Anisotropy type: AMS,AARM,ATRM (default is AMS) -F FILE specifies the specimens formatted file name -usr USER specify username -loc location specify location/study name -spc NUM : specify number of characters to designate a specimen, default = 0 -spn SPECNAME, this specimen has the name SPECNAME -n first column has specimen name -crd [s,g,t], specify coordinate system of data s=specimen,g=geographic,t=tilt adjusted, default is 's' -ncn NCON: naming convention Sample naming convention: [1] XXXXY: where XXXX is an arbitrary length site designation and Y is the single character sample designation. e.g., TG001a is the first sample from site TG001. [default] [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length) [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length) [4-Z] XXXXYYY: YYY is sample designation with Z characters from site XXX [5] sample = site [6] sample, site, location info in er_samples.txt -- NOT CURRENTLY SUPPORTED [7-Z] [XXX]YYY: XXX is site designation with Z characters from samples XXXYYY NB: all others you will have to either customize your self or e-mail ltauxe@ucsd.edu for help. DEFAULT FILE: specimens.txt INPUT X11,X22,X33,X12,X23,X13 (.s format file) X11,X22,X33,X12,X23,X13,sigma (.s format file with -sig option) SID, X11,X22,X33,X12,X23,X13 (.s format file with -n option) OUTPUT specimens.txt format file NOTE because .s files do not have specimen names or location information, the output MagIC files will have to be changed prior to importing to data base.", "response": "def main():\n    \"\"\"\n    NAME\n        s_magic.py\n\n    DESCRIPTION\n        converts .s format data to measurements  format.\n\n    SYNTAX\n        s_magic.py [command line options]\n\n    OPTIONS\n        -h prints help message and quits\n        -DM DATA_MODEL_NUM data model number (default is 3)\n        -f SFILE specifies the .s file name\n        -sig last column has sigma\n        -typ Anisotropy type:  AMS,AARM,ATRM (default is AMS)\n        -F FILE specifies the specimens formatted file name\n        -usr USER specify username\n        -loc location specify location/study name\n        -spc NUM : specify number of characters to\n              designate a  specimen, default = 0\n        -spn SPECNAME, this specimen has the name SPECNAME\n        -n first column has specimen name\n        -crd [s,g,t], specify coordinate system of data\n           s=specimen,g=geographic,t=tilt adjusted, default is 's'\n        -ncn NCON: naming convention\n       Sample naming convention:\n            [1] XXXXY: where XXXX is an arbitrary length site designation and Y\n                is the single character sample designation.  e.g., TG001a is the\n                first sample from site TG001.    [default]\n            [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [4-Z] XXXXYYY:  YYY is sample designation with Z characters from site XXX\n            [5] sample = site\n            [6] sample, site, location info in er_samples.txt -- NOT CURRENTLY SUPPORTED\n            [7-Z] [XXX]YYY:  XXX is site designation with Z characters from samples  XXXYYY\n            NB: all others you will have to either customize your\n                self or e-mail ltauxe@ucsd.edu for help.\n\n\n    DEFAULT\n        FILE:  specimens.txt\n\n    INPUT\n        X11,X22,X33,X12,X23,X13  (.s format file)\n        X11,X22,X33,X12,X23,X13,sigma (.s format file with -sig option)\n        SID, X11,X22,X33,X12,X23,X13  (.s format file with -n option)\n\n    OUTPUT\n        specimens.txt format file\n\n    NOTE\n        because .s files do not have specimen names or location information, the output MagIC files\n        will have to be changed prior to importing to data base.\n    \"\"\"\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    data_model_num = pmag.get_named_arg(\"-DM\", 3)\n    data_model_num = int(float(data_model_num))\n    sfile = pmag.get_named_arg(\"-f\", reqd=True)\n    if data_model_num == 2:\n       anisfile = pmag.get_named_arg(\"-F\", \"rmag_anisotropy.txt\")\n    else:\n        anisfile = pmag.get_named_arg(\"-F\", \"specimens.txt\")\n    location = pmag.get_named_arg(\"-loc\", \"unknown\")\n    user = pmag.get_named_arg(\"-usr\", \"\")\n    sitename = pmag.get_named_arg(\"unknown\", \"\")\n    specnum = pmag.get_named_arg(\"-spc\", 0)\n    specnum = -int(specnum)\n    dir_path = pmag.get_named_arg(\"-WD\", \".\")\n    name = pmag.get_flag_arg_from_sys(\"-n\")\n    sigma = pmag.get_flag_arg_from_sys(\"-sig\")\n    spec = pmag.get_named_arg(\"-spn\", \"unknown\")\n    atype = pmag.get_named_arg(\"-typ\", 'AMS')\n    samp_con = pmag.get_named_arg(\"-ncn\", \"1\")\n    #if '-sig' in sys.argv:\n    #    sigma = 1\n    #if \"-n\" in sys.argv:\n    #    name = 1\n    coord_type = pmag.get_named_arg(\"-crd\", 's')\n    convert.s_magic(sfile, anisfile, dir_path, atype,\n            coord_type, sigma, samp_con, specnum,\n            location, spec, sitename, user, data_model_num, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    dir_path = '.'\n    sites_file = 'er_sites.txt'\n    ocean = 0\n    res = 'i'\n    proj = 'merc'\n    prn_name = 0\n    prn_loc = 0\n    fancy = 0\n    rivers, boundaries = 0, 0\n    padlon, padlat, gridspace, details = .5, .5, .5, 1\n    fmt = 'pdf'\n    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    if '-f' in sys.argv:\n        ind = sys.argv.index('-f')\n        sites_file = sys.argv[ind+1]\n    if '-res' in sys.argv:\n        ind = sys.argv.index('-res')\n        res = sys.argv[ind+1]\n    if '-etp' in sys.argv:\n        fancy = 1\n    if '-n' in sys.argv:\n        prn_name = 1\n    if '-l' in sys.argv:\n        prn_loc = 1\n    if '-o' in sys.argv:\n        ocean = 1\n    if '-R' in sys.argv:\n        rivers = 0\n    if '-B' in sys.argv:\n        boundaries = 0\n    if '-prj' in sys.argv:\n        ind = sys.argv.index('-prj')\n        proj = sys.argv[ind+1]\n    if '-fmt' in sys.argv:\n        ind = sys.argv.index('-fmt')\n        fmt = sys.argv[ind+1]\n    verbose = pmagplotlib.verbose\n    if '-sav' in sys.argv:\n        verbose = 0\n    if '-pad' in sys.argv:\n        ind = sys.argv.index('-pad')\n        padlat = float(sys.argv[ind+1])\n        padlon = float(sys.argv[ind+2])\n    if '-grd' in sys.argv:\n        ind = sys.argv.index('-grd')\n        gridspace = float(sys.argv[ind+1])\n    if '-WD' in sys.argv:\n        ind = sys.argv.index('-WD')\n        dir_path = sys.argv[ind+1]\n    sites_file = dir_path+'/'+sites_file\n    location = \"\"\n    FIG = {'map': 1}\n    pmagplotlib.plot_init(FIG['map'], 6, 6)\n    # read in er_sites file\n    Sites, file_type = pmag.magic_read(sites_file)\n    if 'results' in file_type:\n        latkey = 'average_lat'\n        lonkey = 'average_lon'\n        namekey = 'pmag_result_name'\n        lockey = 'er_location_names'\n    else:\n        latkey = 'site_lat'\n        lonkey = 'site_lon'\n        namekey = 'er_site_name'\n        lockey = 'er_location_name'\n    lats, lons = [], []\n    slats, slons = [], []\n    names, locs = [], []\n    for site in Sites:\n        if prn_loc == 1 and location == \"\":\n            location = site['er_location_name']\n        lats.append(float(site[latkey]))\n        l = float(site[lonkey])\n        if l < 0:\n            l = l+360.  # make positive\n        lons.append(l)\n        if prn_name == 1:\n            names.append(site[namekey])\n        if prn_loc == 1:\n            locs.append(site[lockey])\n    for lat in lats:\n        slats.append(lat)\n    for lon in lons:\n        slons.append(lon)\n    Opts = {'res': res, 'proj': proj, 'loc_name': locs, 'padlon': padlon, 'padlat': padlat, 'latmin': numpy.min(slats)-padlat, 'latmax': numpy.max(\n        slats)+padlat, 'lonmin': numpy.min(slons)-padlon, 'lonmax': numpy.max(slons)+padlon, 'sym': 'ro', 'boundinglat': 0., 'pltgrid': 1.}\n    Opts['lon_0'] = 0.5*(numpy.min(slons)+numpy.max(slons))\n    Opts['lat_0'] = 0.5*(numpy.min(slats)+numpy.max(slats))\n    Opts['names'] = names\n    Opts['gridspace'] = gridspace\n    Opts['details'] = {'coasts': 1, 'rivers': 1,\n                       'states': 1, 'countries': 1, 'ocean': 0}\n    if ocean == 1:\n        Opts['details']['ocean'] = 1\n    if rivers == 1:\n        Opts['details']['rivers'] = 0\n    if boundaries == 1:\n        Opts['details']['states'] = 0\n        Opts['details']['countries'] = 0\n    Opts['details']['fancy'] = fancy\n    pmagplotlib.plot_map(FIG['map'], lats, lons, Opts)\n    if verbose:\n        pmagplotlib.draw_figs(FIG)\n    files = {}\n    for key in list(FIG.keys()):\n        files[key] = 'Site_map'+'.'+fmt\n    if pmagplotlib.isServer:\n        black = '#000000'\n        purple = '#800080'\n        titles = {}\n        titles['map'] = 'Site Map'\n        FIG = pmagplotlib.add_borders(FIG, titles, black, purple)\n        pmagplotlib.save_plots(FIG, files)\n    elif verbose:\n        ans = input(\" S[a]ve to save plot, Return to quit:  \")\n        if ans == \"a\":\n            pmagplotlib.save_plots(FIG, files)\n    else:\n        pmagplotlib.save_plots(FIG, files)", "response": "This is the main function for the basemap_magic. py program."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    inc=[]\n    if '-h' in sys.argv: # check if help is needed\n        print(main.__doc__)\n        sys.exit() # graceful quit\n    if '-i' in sys.argv: # ask for filename\n        file=input(\"Enter file name with inc data: \")\n        inc=numpy.loadtxt(file)\n    elif '-f' in sys.argv:\n        ind=sys.argv.index('-f')\n        file=sys.argv[ind+1]\n        inc=numpy.loadtxt(file)\n    else:\n        inc = numpy.loadtxt(sys.stdin,dtype=numpy.float)\n    ofile=\"\"\n    if '-F' in sys.argv:\n        ind = sys.argv.index('-F')\n        ofile= sys.argv[ind+1]\n        out = open(ofile, 'w + a')\n    #\n    #get doincfish to do the dirty work:\n    fpars= pmag.doincfish(inc)\n    outstring='%7.1f %7.1f  %i %8.1f %7.1f %7.1f'%(fpars['ginc'],fpars['inc'],fpars['n'],fpars['r'],fpars['k'],fpars['alpha95'])\n    if ofile == \"\":\n        print(outstring)\n    else:\n        out.write(outstring+'\\n')", "response": "NAME incfish. py\nVIRTUAL. py DESCRIPTION get the fisher parameters from inc only data and return the mean of the fisher parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_diclist(undecorated, sort_on):\n    decorated = [(len(dict_[sort_on]) if hasattr(dict_[sort_on], '__len__') else dict_[\n                  sort_on], index) for (index, dict_) in enumerate(undecorated)]\n    decorated.sort()\n    return[undecorated[index] for (key, index) in decorated]", "response": "Sort a list of dictionaries by the value in each\n    dictionary for the sorting key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dictitem(In, k, v, flag, float_to_int=False):\n    if float_to_int:\n        try:\n            v = str(math.trunc(float(v)))\n        except ValueError:  # catches non floatable strings\n            pass\n        except TypeError:  # catches None\n            pass\n        fixed_In = []\n        for dictionary in In:\n            if k in dictionary:\n                val = dictionary[k]\n                try:\n                    val = str(math.trunc(float(val)))\n                except ValueError:  # catches non floatable strings\n                    pass\n                except TypeError:  # catches None\n                    pass\n                dictionary[k] = val\n            fixed_In.append(dictionary)\n        In = fixed_In\n    if flag == \"T\":\n        # return that which is\n        return [dictionary for dictionary in In if k in list(dictionary.keys()) and str(dictionary[k]).lower() == str(v).lower()]\n    if flag == \"F\":\n        # return that which is not\n        return [dictionary for dictionary in In if k in list(dictionary.keys()) and str(dictionary[k]).lower() != str(v).lower()]\n    if flag == \"has\":\n        # return that which is contained\n        return [dictionary for dictionary in In if k in list(dictionary.keys()) and str(v).lower() in str(dictionary[k]).lower()]\n    if flag == \"not\":\n        # return that which is not contained\n        return [dictionary for dictionary in In if k in list(dictionary.keys()) and str(v).lower() not in str(dictionary[k]).lower()]\n    if flag == \"eval\":\n        A = [dictionary for dictionary in In if k in list(dictionary.keys(\n        )) and dictionary[k] != '']  # find records with no blank values for key\n        # return that which is\n        return [dictionary for dictionary in A if k in list(dictionary.keys()) and float(dictionary[k]) == float(v)]\n    if flag == \"min\":\n        A = [dictionary for dictionary in In if k in list(dictionary.keys(\n        )) and dictionary[k] != '']  # find records with no blank values for key\n        # return that which is greater than\n        return [dictionary for dictionary in A if k in list(dictionary.keys()) and float(dictionary[k]) >= float(v)]\n    if flag == \"max\":\n        A = [dictionary for dictionary in In if k in list(dictionary.keys(\n        )) and dictionary[k] != '']  # find records with no blank values for key\n        # return that which is less than\n        return [dictionary for dictionary in A if k in list(dictionary.keys()) and float(dictionary[k]) <= float(v)]\n    if flag == 'not_null':\n        return [dictionary for dictionary in In if dictionary[k]]", "response": "returns a list of dictionaries from list In with key k = value v. CASE INSENSITIVE is allowed keywords"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dictkey(In, k, dtype):\n\n    Out = []\n    for d in In:\n        if dtype == '':\n            Out.append(d[k])\n        if dtype == 'f':\n            if d[k] == \"\":\n                Out.append(0)\n            elif d[k] == None:\n                Out.append(0)\n            else:\n                Out.append(float(d[k]))\n        if dtype == 'int':\n            if d[k] == \"\":\n                Out.append(0)\n            elif d[k] == None:\n                Out.append(0)\n            else:\n                Out.append(int(d[k]))\n    return Out", "response": "returns list of given key k from input list of dictionaries in data type dtype. uses command get_dictkey"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the orientation of a sample", "response": "def get_orient(samp_data, er_sample_name, **kwargs):\n    \"\"\"\n    samp_data : PmagPy list of dicts or pandas DataFrame\n    er_sample_name : sample name\n    \"\"\"\n    if isinstance(samp_data, pd.DataFrame):\n        samp_data = (samp_data.T.apply(dict))\n    # set orientation priorities\n    EX = [\"SO-ASC\", \"SO-POM\"]\n    samp_key, az_key, dip_key = 'er_sample_name', 'sample_azimuth', 'sample_dip'\n    disc_key, or_key, meth_key = 'sample_description', 'sample_orientation_flag',\\\n        'magic_method_codes'\n    if 'data_model' in list(kwargs.keys()) and kwargs['data_model'] == 3:\n        samp_key, az_key, dip_key = 'sample', 'azimuth', 'dip'\n        disc_key, or_key, meth_key = 'description', 'orientation_quality',\\\n            'method_codes'\n    orient = {samp_key: er_sample_name, az_key: \"\",\n              dip_key: \"\", disc_key: \"\"}\n    # get all the orientation data for this sample\n    orients = get_dictitem(samp_data, samp_key, er_sample_name, 'T')\n    if len(orients) > 0 and or_key in list(orients[0].keys()):\n        # exclude all samples with bad orientation flag\n        orients = get_dictitem(orients, or_key, 'b', 'F')\n    if len(orients) > 0:\n        orient = orients[0]  # re-initialize to first one\n    methods = get_dictitem(orients, meth_key, 'SO-', 'has')\n    # get a list of all orientation methods for this sample\n    methods = get_dictkey(methods, meth_key, '')\n    SO_methods = []\n    for methcode in methods:\n        meths = methcode.split(\":\")\n        for meth in meths:\n            if (meth.strip() not in EX) and meth.startswith('SO-'):\n                SO_methods.append(meth.strip())\n    # find top priority orientation method\n    if len(SO_methods) == 0:\n        print(\"no orientation data for sample \", er_sample_name)\n        # preserve meta-data anyway even though orientation is bad\n# get all the orientation data for this sample\n        orig_data = get_dictitem(samp_data, samp_key, er_sample_name, 'T')\n        if len(orig_data) > 0:\n            orig_data = orig_data[0]\n        else:\n            orig_data = []\n        az_type = \"SO-NO\"\n    else:\n        SO_priorities = set_priorities(SO_methods, 0)\n        az_type = SO_methods[SO_methods.index(SO_priorities[0])]\n        orient = get_dictitem(orients, meth_key, az_type, 'has')[\n            0]  # re-initialize to best one\n    return orient, az_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a mean inclination value of a distribution of directions, this function calculates the expected elongation of this distribution using a best-fit polynomial of the TK03 GAD secular variation model (Tauxe and Kent, 2004). Parameters ---------- inc : inclination in degrees (int or float) Returns --------- elongation : float Examples --------- >>> pmag.EI(20) 2.4863973732 >>> pmag.EI(90) 1.0241570135500004", "response": "def EI(inc):\n    \"\"\"\n    Given a mean inclination value of a distribution of directions, this\n    function calculates the expected elongation of this distribution using a\n    best-fit polynomial of the TK03 GAD secular variation model (Tauxe and\n    Kent, 2004).\n\n    Parameters\n    ----------\n    inc : inclination in degrees (int or float)\n\n    Returns\n    ---------\n    elongation : float\n\n    Examples\n    ---------\n    >>> pmag.EI(20)\n    2.4863973732\n    >>> pmag.EI(90)\n    1.0241570135500004\n    \"\"\"\n    poly_tk03 = [3.15976125e-06,  -3.52459817e-04,  -\n                 1.46641090e-02,   2.89538539e+00]\n    return poly_tk03[0] * inc**3 + poly_tk03[1] * inc**2 + poly_tk03[2] * inc + poly_tk03[3]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a distribution of directions, this function determines parameters (elongation, inclination, flattening factor, and elongation direction) that are consistent with the TK03 secular variation model. Parameters ---------- data : array of declination, inclination pairs (e.g. np.array([[140,21],[127,23],[142,19],[136,22]])) Returns --------- Es : list of elongation values Is : list of inclination values Fs : list of flattening factors V2s : list of elongation directions (relative to the distribution) The function will return a zero list ([0]) for each of these parameters if the directions constitute a pathological distribution. Examples --------- >>> directions = np.array([[140,21],[127,23],[142,19],[136,22]]) >>> Es, Is, Fs, V2s = pmag.find_f(directions)", "response": "def find_f(data):\n    \"\"\"\n    Given a distribution of directions, this function determines parameters\n    (elongation, inclination, flattening factor, and elongation direction) that\n    are consistent with the TK03 secular variation model.\n\n    Parameters\n    ----------\n    data : array of declination, inclination pairs\n        (e.g. np.array([[140,21],[127,23],[142,19],[136,22]]))\n\n    Returns\n    ---------\n    Es : list of elongation values\n    Is : list of inclination values\n    Fs : list of flattening factors\n    V2s : list of elongation directions (relative to the distribution)\n\n    The function will return a zero list ([0]) for each of these parameters if the directions constitute a pathological distribution.\n\n    Examples\n    ---------\n    >>> directions = np.array([[140,21],[127,23],[142,19],[136,22]])\n    >>> Es, Is, Fs, V2s = pmag.find_f(directions)\n    \"\"\"\n    rad = np.pi/180.\n    Es, Is, Fs, V2s = [], [], [], []\n    ppars = doprinc(data)\n    D = ppars['dec']\n    Decs, Incs = data.transpose()[0], data.transpose()[1]\n    Tan_Incs = np.tan(Incs * rad)\n    for f in np.arange(1., .2, -.01):\n        U = old_div(np.arctan((old_div(1., f)) * Tan_Incs), rad)\n        fdata = np.array([Decs, U]).transpose()\n        ppars = doprinc(fdata)\n        Fs.append(f)\n        Es.append(old_div(ppars[\"tau2\"], ppars[\"tau3\"]))\n        ang = angle([D, 0], [ppars[\"V2dec\"], 0])\n        if 180. - ang < ang:\n            ang = 180. - ang\n        V2s.append(ang)\n        Is.append(abs(ppars[\"inc\"]))\n        if EI(abs(ppars[\"inc\"])) <= Es[-1]:\n            del Es[-1]\n            del Is[-1]\n            del Fs[-1]\n            del V2s[-1]\n            if len(Fs) > 0:\n                for f in np.arange(Fs[-1], .2, -.005):\n                    U = old_div(np.arctan((old_div(1., f)) * Tan_Incs), rad)\n                    fdata = np.array([Decs, U]).transpose()\n                    ppars = doprinc(fdata)\n                    Fs.append(f)\n                    Es.append(old_div(ppars[\"tau2\"], ppars[\"tau3\"]))\n                    Is.append(abs(ppars[\"inc\"]))\n                    ang = angle([D, 0], [ppars[\"V2dec\"], 0])\n                    if 180. - ang < ang:\n                        ang = 180. - ang\n                    V2s.append(ang)\n                    if EI(abs(ppars[\"inc\"])) <= Es[-1]:\n                        return Es, Is, Fs, V2s\n    return [0], [0], [0], [0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert lat to a list of dicts.", "response": "def convert_lat(Recs):\n    \"\"\"\n    uses lat, for age<5Ma, model_lat if present, else tries to use average_inc to estimate plat.\n    \"\"\"\n    New = []\n    for rec in Recs:\n        if 'model_lat' in list(rec.keys()) and rec['model_lat'] != \"\":\n            New.append(rec)\n        elif 'average_age' in list(rec.keys()) and rec['average_age'] != \"\" and float(rec['average_age']) <= 5.:\n            if 'site_lat' in list(rec.keys()) and rec['site_lat'] != \"\":\n                rec['model_lat'] = rec['site_lat']\n                New.append(rec)\n        elif 'average_inc' in list(rec.keys()) and rec['average_inc'] != \"\":\n            rec['model_lat'] = '%7.1f' % (plat(float(rec['average_inc'])))\n            New.append(rec)\n    return New"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_ages(Recs, data_model=3):\n    if data_model == 3:\n        site_key = 'site'\n        agekey = \"age\"\n        keybase = \"\"\n    else:\n        site_key = 'er_site_names'\n        agekey = find('age', list(rec.keys()))\n        if agekey != \"\":\n            keybase = agekey.split('_')[0] + '_'\n\n    New = []\n    for rec in Recs:\n        age = ''\n        if rec[keybase + 'age'] != \"\":\n            age = float(rec[keybase + \"age\"])\n        elif rec[keybase + 'age_low'] != \"\" and rec[keybase + 'age_high'] != '':\n            age = np.mean([rec[keybase + 'age_high'],\n                           rec[keybase + \"age_low\"]])\n            # age = float(rec[keybase + 'age_low']) + old_div(\n            #    (float(rec[keybase + 'age_high']) - float(rec[keybase + 'age_low'])), 2.)\n        if age != '':\n            rec[keybase + 'age_unit']\n            if rec[keybase + 'age_unit'] == 'Ma':\n                rec[keybase + 'age'] = '%10.4e' % (age)\n            elif rec[keybase + 'age_unit'] == 'ka' or rec[keybase + 'age_unit'] == 'Ka':\n                rec[keybase + 'age'] = '%10.4e' % (age * .001)\n            elif rec[keybase + 'age_unit'] == 'Years AD (+/-)':\n                rec[keybase + 'age'] = '%10.4e' % ((2011 - age) * 1e-6)\n            elif rec[keybase + 'age_unit'] == 'Years BP':\n                rec[keybase + 'age'] = '%10.4e' % ((age) * 1e-6)\n            rec[keybase + 'age_unit'] = 'Ma'\n            New.append(rec)\n        else:\n            if 'site_key' in list(rec.keys()):\n                print('problem in convert_ages:', rec['site_key'])\n            elif 'er_site_name' in list(rec.keys()):\n                print('problem in convert_ages:', rec['site_key'])\n            else:\n                print('problem in convert_ages:', rec)\n        if len(New) == 0:\n            print('no age key:', rec)\n    return New", "response": "Convert ages to MaTrees"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert items from a list of dicts to a list of dicts.", "response": "def convert_items(data, mapping):\n    \"\"\"\n    Input: list of dicts (each dict a record for one item),\n    mapping with column names to swap into the records.\n    Output: updated list of dicts.\n    \"\"\"\n    new_recs = []\n    for rec in data:\n        new_rec = map_magic.mapping(rec, mapping)\n        new_recs.append(new_rec)\n    return new_recs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_directory_2_to_3(meas_fname=\"magic_measurements.txt\", input_dir=\".\",\n                             output_dir=\".\", meas_only=False, data_model=None):\n    \"\"\"\n    Convert 2.0 measurements file into 3.0 measurements file.\n    Merge and convert specimen, sample, site, and location data.\n    Also translates criteria data.\n\n    Parameters\n    ----------\n    meas_name : name of measurement file (do not include full path,\n        default is \"magic_measurements.txt\")\n    input_dir : name of input directory (default is \".\")\n    output_dir : name of output directory (default is \".\")\n    meas_only : boolean, convert only measurement data (default is False)\n    data_model : data_model3.DataModel object (default is None)\n\n    Returns\n    ---------\n    NewMeas : 3.0 measurements data (output of pmag.convert_items)\n    upgraded : list of files successfully upgraded to 3.0\n    no_upgrade: list of 2.5 files not upgraded to 3.0\n    \"\"\"\n    convert = {'specimens': map_magic.spec_magic2_2_magic3_map,\n               'samples': map_magic.samp_magic2_2_magic3_map,\n               'sites': map_magic.site_magic2_2_magic3_map,\n               'locations': map_magic.loc_magic2_2_magic3_map,\n               'ages': map_magic.age_magic2_2_magic3_map}\n    full_name = os.path.join(input_dir, meas_fname)\n    if not os.path.exists(full_name):\n        print(\"-W- {} is not a file\".format(full_name))\n        return False, False, False\n    # read in data model 2.5 measurements file\n    data2, filetype = magic_read(full_name)\n    # convert list of dicts to 3.0\n    NewMeas = convert_items(data2, map_magic.meas_magic2_2_magic3_map)\n    # write 3.0 output to file\n    ofile = os.path.join(output_dir, 'measurements.txt')\n    magic_write(ofile, NewMeas, 'measurements')\n    upgraded = []\n    if os.path.exists(ofile):\n        print(\"-I- 3.0 format measurements file was successfully created: {}\".format(ofile))\n        upgraded.append(\"measurements.txt\")\n    else:\n        print(\"-W- 3.0 format measurements file could not be created\")\n    #\n    no_upgrade = []\n    if not meas_only:\n        # try to convert specimens, samples, sites, & locations\n        for dtype in ['specimens', 'samples', 'sites', 'locations', 'ages']:\n            mapping = convert[dtype]\n            res = convert_and_combine_2_to_3(\n                dtype, mapping, input_dir, output_dir, data_model)\n            if res:\n                upgraded.append(res)\n        # try to upgrade criteria file\n        if os.path.exists(os.path.join(input_dir, 'pmag_criteria.txt')):\n            crit_file = convert_criteria_file_2_to_3(input_dir=input_dir,\n                                                     output_dir=output_dir,\n                                                     data_model=data_model)[0]\n            if crit_file:\n                upgraded.append(crit_file)\n            else:\n                no_upgrade.append(\"pmag_criteria.txt\")\n        # create list of all un-upgradeable files\n        for fname in os.listdir(input_dir):\n            if fname in ['measurements.txt', 'specimens.txt', 'samples.txt',\n                         'sites.txt', 'locations.txt']:\n                continue\n            elif 'rmag' in fname:\n                no_upgrade.append(fname)\n            elif fname in ['pmag_results.txt', 'er_synthetics.txt', 'er_images.txt',\n                           'er_plots.txt']:\n                no_upgrade.append(fname)\n\n    return NewMeas, upgraded, no_upgrade", "response": "Convert 2. 5 measurements file into 3. 0 measurements file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert 2. 5 format data to 3. 0 format data.", "response": "def convert_and_combine_2_to_3(dtype, map_dict, input_dir=\".\", output_dir=\".\", data_model=None):\n    \"\"\"\n    Read in er_*.txt file and pmag_*.txt file in working directory.\n    Combine the data, then translate headers from 2.5 --> 3.0.\n    Last, write out the data in 3.0.\n\n    Parameters\n    ----------\n    dtype : string for input type (specimens, samples, sites, etc.)\n    map_dict : dictionary with format {header2_format: header3_format, ...} (from mapping.map_magic module)\n    input_dir : input directory, default \".\"\n    output_dir : output directory, default \".\"\n    data_model : data_model3.DataModel object, default None\n\n    Returns\n    ---------\n    output_file_name with 3.0 format data (or None if translation failed)\n    \"\"\"\n    # read in er_ data & make DataFrame\n    er_file = os.path.join(input_dir, 'er_{}.txt'.format(dtype))\n    er_data, er_dtype = magic_read(er_file)\n    if len(er_data):\n        er_df = pd.DataFrame(er_data)\n        if dtype == 'ages':\n            pass\n            # remove records with blank ages\n            #er_data = get_dictitem(er_data, 'age', '', \"F\")\n            #er_df = pd.DataFrame(er_data)\n        else:\n            er_df.index = er_df['er_{}_name'.format(dtype[:-1])]\n    else:\n        er_df = pd.DataFrame()\n    #\n    if dtype == 'ages':\n        full_df = er_df\n    else:\n        # read in pmag_ data & make DataFrame\n        pmag_file = os.path.join(input_dir, 'pmag_{}.txt'.format(dtype))\n        pmag_data, pmag_dtype = magic_read(pmag_file)\n        if len(pmag_data):\n            pmag_df = pd.DataFrame(pmag_data)\n            pmag_df.index = pmag_df['er_{}_name'.format(dtype[:-1])]\n        else:\n            pmag_df = pd.DataFrame()\n        # combine the two Dataframes\n        full_df = pd.concat([er_df, pmag_df], sort=True)\n        # sort the DataFrame so that all records from one item are together\n        full_df.sort_index(inplace=True)\n\n    # fix the column names to be 3.0\n    full_df.rename(columns=map_dict, inplace=True)\n    # create a MagicDataFrame object, providing the dataframe and the data type\n    new_df = cb.MagicDataFrame(dtype=dtype, df=full_df, dmodel=data_model)\n    # write out the data to file\n    if len(new_df.df):\n        new_df.write_magic_file(dir_path=output_dir)\n        return dtype + \".txt\"\n    else:\n        print(\"-I- No {} data found.\".format(dtype))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a criteria file from 2. 5 format to 3. 0 format and write it out to file", "response": "def convert_criteria_file_2_to_3(fname=\"pmag_criteria.txt\", input_dir=\".\",\n                                 output_dir=\".\", data_model=None):\n    \"\"\"\n    Convert a criteria file from 2.5 to 3.0 format and write it out to file\n\n    Parameters\n    ----------\n    fname : string of filename (default \"pmag_criteria.txt\")\n    input_dir : string of input directory (default \".\")\n    output_dir : string of output directory (default \".\")\n    data_model : data_model.DataModel object (default None)\n\n    Returns\n    ---------\n    outfile : string output criteria filename, or False\n    crit_container : cb.MagicDataFrame with 3.0 criteria table\n    \"\"\"\n    # get criteria from infile\n    fname = os.path.join(input_dir, fname)\n    if not os.path.exists(fname):\n        return False, None\n    orig_crit, warnings = read_criteria_from_file(fname, initialize_acceptance_criteria(),\n                                                  data_model=2, return_warnings=True)\n    converted_crit = {}\n    # get data model including criteria map\n    if not data_model:\n        from . import data_model3 as dm3\n        DM = dm3.DataModel()\n    else:\n        DM = data_model\n    crit_map = DM.crit_map\n    # drop all empty mappings\n    stripped_crit_map = crit_map.dropna(axis='rows')\n    # go through criteria and get 3.0 name and criterion_operation\n    for crit in orig_crit:\n        if orig_crit[crit]['value'] in [-999, '-999', -999.]:\n            continue\n        if crit in stripped_crit_map.index:\n            criterion_operation = stripped_crit_map.loc[crit]['criteria_map']['criterion_operation']\n            table_col = stripped_crit_map.loc[crit]['criteria_map']['table_column']\n            orig_crit[crit]['criterion_operation'] = criterion_operation\n            converted_crit[table_col] = orig_crit[crit]\n        else:\n            print('-W- Could not convert {} to 3.0, skipping'.format(crit))\n    # switch axes\n    converted_df = pd.DataFrame(converted_crit).transpose()\n    # name the index\n    converted_df.index.name = \"table_column\"\n    # rename columns to 3.0 values\n    # 'category' --> criterion (uses defaults from initalize_default_criteria)\n    # 'pmag_criteria_code' --> criterion (uses what's actually in the translated file)\n    converted_df.rename(columns={'pmag_criteria_code': 'criterion', 'er_citation_names': 'citations',\n                                 'criteria_definition': 'description', 'value': 'criterion_value'},\n                        inplace=True)\n    # drop unused columns\n    valid_cols = DM.dm['criteria'].index\n    drop_cols = set(converted_df.columns) - set(valid_cols)\n    converted_df.drop(drop_cols, axis='columns', inplace=True)\n    # move 'table_column' from being the index to being a column\n    converted_df['table_column'] = converted_df.index\n    crit_container = cb.MagicDataFrame(dtype='criteria', df=converted_df)\n    crit_container.write_magic_file(dir_path=output_dir)\n    return \"criteria.txt\", crit_container"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef orient(mag_azimuth, field_dip, or_con):\n    or_con = str(or_con)\n    if mag_azimuth == -999:\n        return \"\", \"\"\n    if or_con == \"1\":  # lab_mag_az=mag_az;  sample_dip = -dip\n        return mag_azimuth, -field_dip\n    if or_con == \"2\":\n        return mag_azimuth - 90., -field_dip\n    if or_con == \"3\":  # lab_mag_az=mag_az;  sample_dip = 90.-dip\n        return mag_azimuth, 90. - field_dip\n    if or_con == \"4\":  # lab_mag_az=mag_az;  sample_dip = dip\n        return mag_azimuth, field_dip\n    if or_con == \"5\":  # lab_mag_az=mag_az;  sample_dip = dip-90.\n        return mag_azimuth, field_dip - 90.\n    if or_con == \"6\":  # lab_mag_az=mag_az-90.;  sample_dip = 90.-dip\n        return mag_azimuth - 90., 90. - field_dip\n    if or_con == \"7\":  # lab_mag_az=mag_az;  sample_dip = 90.-dip\n        return mag_azimuth - 90., 90. - field_dip\n    print(\"Error in orientation convention\")", "response": "converts user supplied orientations\n    to laboratory azimuth and plunge\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the Sf for a dataframe with VGP Lat. and optional Fisher s k site latitude and N information can be used to correct for within site scatter.", "response": "def get_sb_df(df, mm97=False):\n    \"\"\"\n    Calculates Sf for a dataframe with VGP Lat., and optional Fisher's k, site latitude and N information can be used to correct for within site scatter (McElhinny & McFadden, 1997)\n\n    Parameters\n    _________\n    df : Pandas Dataframe with columns\n        REQUIRED:\n        vgp_lat :  VGP latitude\n        ONLY REQUIRED for MM97 correction:\n        dir_k : Fisher kappa estimate\n        dir_n : number of specimens (samples) per site\n        lat : latitude of the site\n    mm97 : if True, will do the correction for within site scatter\n\n    Returns:\n    _______\n    Sf : Sf\n    \"\"\"\n    df['delta'] = 90.-df.vgp_lat\n    Sp2 = np.sum(df.delta**2)/(df.shape[0]-1)\n    if 'dir_k' in df.columns and mm97:\n        ks = df.dir_k\n        Ns = df.dir_n\n        Ls = np.radians(df.lat)\n        A95s = 140./np.sqrt(ks*Ns)\n        Sw2_n = 0.335*(A95s**2)*(2.*(1.+3.*np.sin(Ls)**2) /\n                                 (5.-3.*np.sin(Ls)**2))\n        return np.sqrt(Sp2-Sw2_n.mean())\n    else:\n        return np.sqrt(Sp2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the grade of a specimen given the acceptance criteria", "response": "def grade(PmagRec, ACCEPT, type, data_model=2.5):\n    \"\"\"\n    Finds the 'grade' (pass/fail; A/F) of a record (specimen,sample,site) given the acceptance criteria\n    \"\"\"\n    GREATERTHAN = ['specimen_q', 'site_k', 'site_n', 'site_n_lines', 'site_int_n', 'measurement_step_min', 'specimen_int_ptrm_n', 'specimen_fvds', 'specimen_frac', 'specimen_f', 'specimen_n', 'specimen_int_n', 'sample_int_n', 'average_age_min', 'average_k', 'average_r', 'specimen_magn_moment',\n                   'specimen_magn_volume', 'specimen_rsc', 'sample_n', 'sample_n_lines', 'sample_n_planes', 'sample_k', 'sample_r', 'site_magn_moment', 'site_magn_volume', 'site_magn_mass', 'site_r']  # these statistics must be exceede to pass, all others must be less than (except specimen_scat, which must be true)\n    ISTRUE = ['specimen_scat']\n    kill = []  # criteria that kill the record\n    sigma_types = ['sample_int_sigma', 'sample_int_sigma_perc', 'site_int_sigma',\n                   'site_int_sigma_perc', 'average_int_sigma', 'average_int_sigma_perc']\n    sigmas = []\n    accept = {}\n    if type == 'specimen_int':\n        USEKEYS = ['specimen_q', 'measurement_step_min', 'measurement_step_max', 'specimen_int_ptrm_n', 'specimen_fvds', 'specimen_frac', 'specimen_f', 'specimen_int_n', 'specimen_magn_moment',\n                   'specimen_magn_volume', 'specimen_rsc', 'specimen_scat', 'specimen_drats', 'specimen_int_mad', 'specimen_int_dang', 'specimen_md', 'specimen_b_beta', 'specimen_w', 'specimen_gmax']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.spec_magic2_2_magic3_map[k] for k in USEKEYS]\n    elif type == 'specimen_dir':\n        USEKEYS = ['measurement_step_min', 'measurement_step_max', 'specimen_mad',\n                   'specimen_n', 'specimen_magn_moment', 'specimen_magn_volume']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.spec_magic2_2_magic3_map[k] for k in USEKEYS]\n    elif type == 'sample_int':\n        USEKEYS = ['sample_int_n', 'sample_int_sigma', 'sample_int_sigma_perc']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.samp_magic2_2_magic3_map[k] for k in USEKEYS]\n    elif type == 'sample_dir':\n        USEKEYS = ['sample_alpha95', 'sample_n', 'sample_n_lines',\n                   'sample_n_planes', 'sample_k', 'sample_r']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.samp_magic2_2_magic3_map[k] for k in USEKEYS]\n    elif type == 'site_int':\n        USEKEYS = ['site_int_sigma', 'site_int_sigma_perc', 'site_int_n']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.site_magic2_2_magic3_map[k] for k in USEKEYS]\n    elif type == 'site_dir':\n        USEKEYS = ['site_alpha95', 'site_k', 'site_n',\n                   'site_n_lines', 'site_n_planes', 'site_r']\n        if data_model == 3.0:\n            USEKEYS = [map_magic.site_magic2_2_magic3_map[k] for k in USEKEYS]\n\n    for key in list(ACCEPT.keys()):\n        if ACCEPT[key] != \"\" and key in USEKEYS:\n            if key in ISTRUE and ACCEPT[key] == 'TRUE' or ACCEPT[key] == 'True':\n                # this is because Excel always capitalizes True to TRUE and\n                # python doesn't recognize that as a boolean.  never mind\n                ACCEPT[key] = '1'\n            elif ACCEPT[key] == 'FALSE' or ACCEPT[key] == 'False':\n                ACCEPT[key] = '0'\n            elif eval(ACCEPT[key]) == 0:\n                ACCEPT[key] = \"\"\n            accept[key] = ACCEPT[key]\n    for key in sigma_types:\n        if key in USEKEYS and key in list(accept.keys()) and key in list(PmagRec.keys()):\n            sigmas.append(key)\n    if len(sigmas) > 1:\n        if PmagRec[sigmas[0]] == \"\" or PmagRec[sigmas[1]] == \"\":\n            kill.append(sigmas[0])\n            kill.append(sigmas[1])\n        elif eval(PmagRec[sigmas[0]]) > eval(accept[sigmas[0]]) and eval(PmagRec[sigmas[1]]) > eval(accept[sigmas[1]]):\n            kill.append(sigmas[0])\n            kill.append(sigmas[1])\n    elif len(sigmas) == 1 and sigmas[0] in list(accept.keys()):\n        if PmagRec[sigmas[0]] > accept[sigmas[0]]:\n            kill.append(sigmas[0])\n    for key in list(accept.keys()):\n        if accept[key] != \"\":\n            if key not in list(PmagRec.keys()) or PmagRec[key] == '':\n                kill.append(key)\n            elif key not in sigma_types:\n                if key in ISTRUE:  # boolean must be true\n                    if PmagRec[key] != '1':\n                        kill.append(key)\n                if key in GREATERTHAN:\n                    if eval(str(PmagRec[key])) < eval(str(accept[key])):\n                        kill.append(key)\n                else:\n                    if eval(str(PmagRec[key])) > eval(str(accept[key])):\n                        kill.append(key)\n    return kill"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flip(di_block, combine=False):\n    ppars = doprinc(di_block)  # get principle direction\n    if combine:\n        D3 = []\n    D1, D2 = [], []\n    for rec in di_block:\n        ang = angle([rec[0], rec[1]], [ppars['dec'], ppars['inc']])\n        if ang > 90.:\n            d, i = (rec[0] - 180.) % 360., -rec[1]\n            D2.append([d, i])\n            if combine:\n                D3.append([d, i])\n        else:\n            D1.append([rec[0], rec[1]])\n            if combine:\n                D3.append([rec[0], rec[1]])\n    if combine:\n        return D3\n    else:\n        return D1, D2", "response": "Flips the reverse mode of a single DI block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to convert directional data at a given location to pole position dp dm", "response": "def dia_vgp(*args):  # new function interface by J.Holmes, SIO, 6/1/2011\n    \"\"\"\n    Converts directional data (declination, inclination, alpha95) at a given\n    location (Site latitude, Site longitude) to pole position (pole longitude,\n    pole latitude, dp, dm)\n\n    Parameters\n    ----------\n    Takes input as (Dec, Inc, a95, Site latitude, Site longitude)\n    Input can be as individual values (5 parameters)\n    or\n    as a list of lists: [[Dec, Inc, a95, lat, lon],[Dec, Inc, a95, lat, lon]]\n\n    Returns\n    ----------\n    if input is individual values for one pole the return is:\n    pole longitude, pole latitude, dp, dm\n\n    if input is list of lists the return is:\n    list of pole longitudes, list of pole latitude, list of dp, list of dm\n    \"\"\"\n    # test whether arguments are one 2-D list or 5 floats\n    if len(args) == 1:  # args comes in as a tuple of multi-dim lists.\n        largs = list(args).pop()  # scrap the tuple.\n        # reorganize the lists so that we get columns of data in each var.\n        (decs, dips, a95s, slats, slongs) = list(zip(*largs))\n    else:\n        # When args > 1, we are receiving five floats. This usually happens when the invoking script is\n        # executed in interactive mode.\n        (decs, dips, a95s, slats, slongs) = (args)\n\n    # We send all incoming data to numpy in an array form. Even if it means a\n    # 1x1 matrix. That's OKAY. Really.\n    (dec, dip, a95, slat, slong) = (np.array(decs), np.array(dips), np.array(a95s),\n                                    np.array(slats), np.array(slongs))  # package columns into arrays\n    rad = old_div(np.pi, 180.)  # convert to radians\n    dec, dip, a95, slat, slong = dec * rad, dip * \\\n        rad, a95 * rad, slat * rad, slong * rad\n    p = np.arctan2(2.0, np.tan(dip))\n    plat = np.arcsin(np.sin(slat) * np.cos(p) +\n                     np.cos(slat) * np.sin(p) * np.cos(dec))\n    beta = old_div((np.sin(p) * np.sin(dec)), np.cos(plat))\n\n    # -------------------------------------------------------------------------\n    # The deal with \"boolmask\":\n    # We needed a quick way to assign matrix values based on a logic decision, in this case setting boundaries\n    # on out-of-bounds conditions. Creating a matrix of boolean values the size of the original matrix and using\n    # it to \"mask\" the assignment solves this problem nicely. The downside to this is that Numpy complains if you\n    # attempt to mask a non-matrix, so we have to check for array type and do a normal assignment if the type is\n    # scalar. These checks are made before calculating for the rest of the function.\n    # -------------------------------------------------------------------------\n\n    boolmask = beta > 1.  # create a mask of boolean values\n    if isinstance(beta, np.ndarray):\n        beta[boolmask] = 1.  # assigns 1 only to elements that mask TRUE.\n    # Numpy gets upset if you try our masking trick with a scalar or a 0-D\n    # matrix.\n    else:\n        if boolmask:\n            beta = 1.\n    boolmask = beta < -1.\n    if isinstance(beta, np.ndarray):\n        beta[boolmask] = -1.  # assigns -1 only to elements that mask TRUE.\n    else:\n        if boolmask:\n            beta = -1.\n\n    beta = np.arcsin(beta)\n    plong = slong + np.pi - beta\n    if (np.cos(p) > np.sin(slat) * np.sin(plat)).any():\n        boolmask = (np.cos(p) > (np.sin(slat) * np.sin(plat)))\n        if isinstance(plong, np.ndarray):\n            plong[boolmask] = (slong + beta)[boolmask]\n        else:\n            if boolmask:\n                plong = slong + beta\n\n    boolmask = (plong < 0)\n    if isinstance(plong, np.ndarray):\n        plong[boolmask] = plong[boolmask] + 2 * np.pi\n    else:\n        if boolmask:\n            plong = plong + 2 * np.pi\n\n    boolmask = (plong > 2 * np.pi)\n    if isinstance(plong, np.ndarray):\n        plong[boolmask] = plong[boolmask] - 2 * np.pi\n    else:\n        if boolmask:\n            plong = plong - 2 * np.pi\n\n    dm = np.rad2deg(a95 * (old_div(np.sin(p), np.cos(dip))))\n    dp = np.rad2deg(a95 * (old_div((1 + 3 * (np.cos(p)**2)), 2)))\n    plat = np.rad2deg(plat)\n    plong = np.rad2deg(plong)\n    return plong.tolist(), plat.tolist(), dp.tolist(), dm.tolist()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the parameters of the York regression and Coe parameters", "response": "def int_pars(x, y, vds, **kwargs):\n    \"\"\"\n     calculates York regression and Coe parameters (with Tauxe Fvds)\n    \"\"\"\n    # first do linear regression a la York\n    # do Data Model 3 way:\n    if 'version' in list(kwargs.keys()) and kwargs['version'] == 3:\n        n_key = 'int_n_measurements'\n        b_key = 'int_b'\n        sigma_key = 'int_b_sigma'\n        f_key = 'int_f'\n        fvds_key = 'int_fvds'\n        g_key = 'int_g'\n        q_key = 'int_q'\n        b_beta_key = 'int_b_beta'\n\n    else:  # version 2\n        n_key = 'specimen_int_n'\n        b_key = 'specimen_b'\n        sigma_key = 'specimen_b_sigma'\n        f_key = 'specimen_f'\n        fvds_key = 'specimen_fvds'\n        g_key = 'specimen_g'\n        q_key = 'specimen_q'\n        b_beta_key = 'specimen_b_beta'\n\n    xx, yer, xer, xyer, yy, xsum, ysum, xy = 0., 0., 0., 0., 0., 0., 0., 0.\n    xprime, yprime = [], []\n    pars = {}\n    pars[n_key] = len(x)\n    n = float(len(x))\n    if n <= 2:\n        print(\"shouldn't be here at all!\")\n        return pars, 1\n    for i in range(len(x)):\n        xx += x[i]**2.\n        yy += y[i]**2.\n        xy += x[i] * y[i]\n        xsum += x[i]\n        ysum += y[i]\n    xsig = np.sqrt(old_div((xx - (old_div(xsum**2., n))), (n - 1.)))\n    ysig = np.sqrt(old_div((yy - (old_div(ysum**2., n))), (n - 1.)))\n    sum = 0\n    for i in range(int(n)):\n        yer += (y[i] - old_div(ysum, n))**2.\n        xer += (x[i] - old_div(xsum, n))**2.\n        xyer += (y[i] - old_div(ysum, n)) * (x[i] - old_div(xsum, n))\n    slop = -np.sqrt(old_div(yer, xer))\n    pars[b_key] = slop\n    s1 = 2. * yer - 2. * slop * xyer\n    s2 = (n - 2.) * xer\n    sigma = np.sqrt(old_div(s1, s2))\n    pars[sigma_key] = sigma\n    s = old_div((xy - (xsum * ysum / n)), (xx - old_div((xsum**2.), n)))\n    r = old_div((s * xsig), ysig)\n    pars[\"specimen_rsc\"] = r**2.\n    ytot = abs(old_div(ysum, n) - slop * xsum / n)\n    for i in range(int(n)):\n        xprime.append(old_div((slop * x[i] + y[i] - ytot), (2. * slop)))\n        yprime.append((old_div((slop * x[i] + y[i] - ytot), 2.)) + ytot)\n    sumdy, dy = 0, []\n    dyt = abs(yprime[0] - yprime[int(n) - 1])\n    for i in range((int(n) - 1)):\n        dy.append(abs(yprime[i + 1] - yprime[i]))\n        sumdy += dy[i]**2.\n    f = old_div(dyt, ytot)\n    pars[f_key] = f\n    pars[\"specimen_ytot\"] = ytot\n    ff = old_div(dyt, vds)\n    pars[fvds_key] = ff\n    ddy = (old_div(1., dyt)) * sumdy\n    g = 1. - old_div(ddy, dyt)\n    pars[g_key] = g\n    q = abs(slop) * f * g / sigma\n    pars[q_key] = q\n    pars[b_beta_key] = old_div(-sigma, slop)\n    return pars, 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates vector difference sum for demagnetization data", "response": "def dovds(data):\n    \"\"\"\n     calculates vector difference sum for demagnetization data\n    \"\"\"\n    vds, X = 0, []\n    for rec in data:\n        X.append(dir2cart(rec))\n    for k in range(len(X) - 1):\n        xdif = X[k + 1][0] - X[k][0]\n        ydif = X[k + 1][1] - X[k][1]\n        zdif = X[k + 1][2] - X[k][2]\n        vds += np.sqrt(xdif**2 + ydif**2 + zdif**2)\n    vds += np.sqrt(X[-1][0]**2 + X[-1][1]**2 + X[-1][2]**2)\n    return vds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vspec_magic(data):\n    vdata, Dirdata, step_meth = [], [], \"\"\n    if len(data) == 0:\n        return vdata\n    treat_init = [\"treatment_temp\", \"treatment_temp_decay_rate\", \"treatment_temp_dc_on\", \"treatment_temp_dc_off\", \"treatment_ac_field\", \"treatment_ac_field_decay_rate\", \"treatment_ac_field_dc_on\",\n                  \"treatment_ac_field_dc_off\", \"treatment_dc_field\", \"treatment_dc_field_decay_rate\", \"treatment_dc_field_ac_on\", \"treatment_dc_field_ac_off\", \"treatment_dc_field_phi\", \"treatment_dc_field_theta\"]\n    treats = []\n#\n# find keys that are used\n#\n    for key in treat_init:\n        if key in list(data[0].keys()):\n            treats.append(key)  # get a list of keys\n    stop = {}\n    stop[\"er_specimen_name\"] = \"stop\"\n    for key in treats:\n        stop[key] = \"\"  # tells program when to quit and go home\n    data.append(stop)\n#\n# set initial states\n#\n    DataState0, newstate = {}, 0\n    for key in treats:\n        DataState0[key] = data[0][key]  # set beginning treatment\n    k, R = 1, 0\n    for i in range(k, len(data)):\n        FDirdata, Dirdata, DataStateCurr, newstate = [], [], {}, 0\n        for key in treats:  # check if anything changed\n            DataStateCurr[key] = data[i][key]\n            if DataStateCurr[key].strip() != DataState0[key].strip():\n                newstate = 1  # something changed\n        if newstate == 1:\n            if i == k:  # sample is unique\n                vdata.append(data[i - 1])\n            else:  # measurement is not unique\n                # print \"averaging: records \" ,k,i\n                for l in range(k - 1, i):\n                    if 'orientation' in data[l]['measurement_description']:\n                        data[l]['measurement_description'] = \"\"\n                    Dirdata.append([float(data[l]['measurement_dec']), float(\n                        data[l]['measurement_inc']), float(data[l]['measurement_magn_moment'])])\n                    FDirdata.append(\n                        [float(data[l]['measurement_dec']), float(data[l]['measurement_inc'])])\n                dir, R = vector_mean(Dirdata)\n                Fpars = fisher_mean(FDirdata)\n                vrec = data[i - 1]\n                vrec['measurement_dec'] = '%7.1f' % (dir[0])\n                vrec['measurement_inc'] = '%7.1f' % (dir[1])\n                vrec['measurement_magn_moment'] = '%8.3e' % (\n                    old_div(R, (i - k + 1)))\n                vrec['measurement_csd'] = '%7.1f' % (Fpars['csd'])\n                vrec['measurement_positions'] = '%7.1f' % (Fpars['n'])\n                vrec['measurement_description'] = 'average of multiple measurements'\n                if \"magic_method_codes\" in list(vrec.keys()):\n                    meths = vrec[\"magic_method_codes\"].strip().split(\":\")\n                    if \"DE-VM\" not in meths:\n                        meths.append(\"DE-VM\")\n                    methods = \"\"\n                    for meth in meths:\n                        methods = methods + meth + \":\"\n                    vrec[\"magic_method_codes\"] = methods[:-1]\n                else:\n                    vrec[\"magic_method_codes\"] = \"DE-VM\"\n                vdata.append(vrec)\n# reset state to new one\n            for key in treats:\n                DataState0[key] = data[i][key]  # set beginning treatment\n            k = i + 1\n            if data[i][\"er_specimen_name\"] == \"stop\":\n                del data[-1]  # get rid of dummy stop sign\n                return vdata, treats", "response": "Takes average vector of replicate measurements and returns a list of vspecs that are unique"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_specs(data):\n    # sort the specimen names\n    speclist = []\n    for rec in data:\n        try:\n            spec = rec[\"er_specimen_name\"]\n        except KeyError as e:\n            spec = rec[\"specimen\"]\n        if spec not in speclist:\n            speclist.append(spec)\n    speclist.sort()\n    return speclist", "response": "Takes a magic format file and returns a list of unique specimen names"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vector_mean(data):\n    Xbar = np.zeros((3))\n    X = dir2cart(data).transpose()\n    for i in range(3):\n        Xbar[i] = X[i].sum()\n    R = np.sqrt(Xbar[0]**2+Xbar[1]**2+Xbar[2]**2)\n    Xbar = Xbar/R\n    dir = cart2dir(Xbar)\n    return dir, R", "response": "Calculates the vector mean of a given set of vectors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmarking demagnetization data for bad points with measurement_flag", "response": "def mark_dmag_rec(s, ind, data):\n    \"\"\"\n    Edits demagnetization data to mark \"bad\" points with measurement_flag\n    \"\"\"\n    datablock = []\n    for rec in data:\n        if rec['er_specimen_name'] == s:\n            meths = rec['magic_method_codes'].split(':')\n            if 'LT-NO' in meths or 'LT-AF-Z' in meths or 'LT-T-Z' in meths:\n                datablock.append(rec)\n    dmagrec = datablock[ind]\n    for k in range(len(data)):\n        meths = data[k]['magic_method_codes'].split(':')\n        if 'LT-NO' in meths or 'LT-AF-Z' in meths or 'LT-T-Z' in meths:\n            if data[k]['er_specimen_name'] == s:\n                if data[k]['treatment_temp'] == dmagrec['treatment_temp'] and data[k]['treatment_ac_field'] == dmagrec['treatment_ac_field']:\n                    if data[k]['measurement_dec'] == dmagrec['measurement_dec'] and data[k]['measurement_inc'] == dmagrec['measurement_inc'] and data[k]['measurement_magn_moment'] == dmagrec['measurement_magn_moment']:\n                        if data[k]['measurement_flag'] == 'g':\n                            flag = 'b'\n                        else:\n                            flag = 'g'\n                        data[k]['measurement_flag'] = flag\n                        break\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_dmag_rec(s, data, **kwargs):\n    if 'version' in list(kwargs.keys()) and kwargs['version'] == 3:\n        # convert dataframe to list of dictionaries\n        data = data.to_dict('records')\n        spec_key, dec_key, inc_key = 'specimen', 'dir_dec', 'dir_inc'\n        flag_key, temp_key, ac_key = 'flag', 'treat_temp', 'treat_ac_field'\n        meth_key = 'method_codes'\n        power_key, time_key = 'treat_mw_power', 'treat_mw_time'\n        Mkeys = ['magn_moment', 'magn_volume', 'magn_mass', 'magnitude']\n        # just look in the intensity column\n        inst_key = 'instrument_codes'\n    else:\n        spec_key, dec_key, inc_key = 'er_specimen_name', 'measurement_dec', 'measurement_inc'\n        flag_key = 'measurement_flag'\n        flag_key, temp_key, ac_key = 'measurement_flag', 'treatment_temp', 'treatment_ac_field'\n        meth_key = 'magic_method_codes'\n        power_key, time_key = 'treatment_mw_power', 'treatment_mw_time'\n        Mkeys = ['measurement_magn_moment', 'measurement_magn_volume',\n                 'measurement_magn_mass', 'measurement_magnitude']\n        inst_key = 'magic_instrument_codes'\n\n    EX = [\"LP-AN-ARM\", \"LP-AN-TRM\", \"LP-ARM-AFD\", \"LP-ARM2-AFD\", \"LP-TRM-AFD\",\n          \"LP-TRM\", \"LP-TRM-TD\", \"LP-X\"]  # list of excluded lab protocols\n    INC = [\"LT-NO\", \"LT-AF-Z\", \"LT-T-Z\",\n           \"LT-M-Z\", \"LP-PI-TRM-IZ\", \"LP-PI-M-IZ\"]\n    datablock, tr = [], \"\"\n    therm_flag, af_flag, mw_flag = 0, 0, 0\n    units = []\n    spec_meas = get_dictitem(data, spec_key, s, 'T')\n    for rec in spec_meas:\n        if flag_key not in list(rec.keys()):\n            rec[flag_key] = 'g'\n        skip = 1\n        tr = \"\"\n        meths = rec[meth_key].split(\":\")\n        methods = []\n        for m in meths:\n            methods.append(m.strip())  # get rid of the stupid spaces!\n        for meth in methods:\n            if meth.strip() in INC:\n                skip = 0\n        for meth in EX:\n            if meth in methods:\n                skip = 1\n        if skip == 0:\n            if \"LT-NO\" in methods:\n                tr = float(rec[temp_key])\n            if \"LT-AF-Z\" in methods:\n                af_flag = 1\n                try:\n                    tr = float(rec[ac_key])\n                except (KeyError, ValueError):\n                    tr = 0\n                if \"T\" not in units:\n                    units.append(\"T\")\n            if \"LT-T-Z\" in methods:\n                therm_flag = 1\n                tr = float(rec[temp_key])\n                if \"K\" not in units:\n                    units.append(\"K\")\n            if \"LT-M-Z\" in methods:\n                mw_flag = 1\n                tr = float(rec[power_key]) * float(rec[time_key])\n                if \"J\" not in units:\n                    units.append(\"J\")\n            # looking for in-field first thellier or microwave data -\n            # otherwise, just ignore this\n            if \"LP-PI-TRM-IZ\" in methods or \"LP-PI-M-IZ\" in methods:\n                ZI = 0\n            else:\n                ZI = 1\n            if tr != \"\":\n                dec, inc, int = \"\", \"\", \"\"\n                if dec_key in list(rec.keys()) and cb.not_null(rec[dec_key], False):\n                    dec = float(rec[dec_key])\n                if inc_key in list(rec.keys()) and cb.not_null(rec[inc_key], False):\n                    inc = float(rec[inc_key])\n                for key in Mkeys:\n                    if key in list(rec.keys()) and cb.not_null(rec[key], False):\n                        int = float(rec[key])\n                if inst_key not in list(rec.keys()):\n                    rec[inst_key] = ''\n                datablock.append(\n                    [tr, dec, inc, int, ZI, rec[flag_key], rec[inst_key]])\n    if therm_flag == 1:\n        for k in range(len(datablock)):\n            if datablock[k][0] == 0.:\n                datablock[k][0] = 273.\n    if af_flag == 1:\n        for k in range(len(datablock)):\n            if datablock[k][0] >= 273 and datablock[k][0] <= 323:\n                datablock[k][0] = 0.\n    meas_units = \"\"\n    if len(units) > 0:\n        for u in units:\n            meas_units = meas_units + u + \":\"\n        meas_units = meas_units[:-1]\n    return datablock, meas_units", "response": "Find demagnetization data for a specimen s."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a file and return a list of the file s lines.", "response": "def open_file(infile, verbose=True):\n    \"\"\"\n    Open file and return a list of the file's lines.\n    Try to use utf-8 encoding, and if that fails use Latin-1.\n\n    Parameters\n    ----------\n    infile : str\n        full path to file\n\n    Returns\n    ----------\n    data: list\n        all lines in the file\n    \"\"\"\n    try:\n        with codecs.open(infile, \"r\", \"utf-8\") as f:\n            lines = list(f.readlines())\n    # file might not exist\n    except FileNotFoundError:\n        if verbose:\n            print(\n                '-W- You are trying to open a file: {} that does not exist'.format(infile))\n        return []\n    # encoding might be wrong\n    except UnicodeDecodeError:\n        try:\n            with codecs.open(infile, \"r\", \"Latin-1\") as f:\n                print(\n                    '-I- Using less strict decoding for {}, output may have formatting errors'.format(infile))\n                lines = list(f.readlines())\n        # if file exists, and encoding is correct, who knows what the problem is\n        except Exception as ex:\n            print(\"-W- \", type(ex), ex)\n            return []\n    except Exception as ex:\n        print(\"-W- \", type(ex), ex)\n        return []\n    # don't leave a blank line at the end\n    i = 0\n    while i < 10:\n        if not len(lines[-1].strip(\"\\n\").strip(\"\\t\")):\n            lines = lines[:-1]\n            i += 1\n        else:\n            i = 10\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a MagIC template file and returns a list of dictionaries.", "response": "def magic_read(infile, data=None, return_keys=False, verbose=False):\n    \"\"\"\n    Reads  a Magic template file, returns  data in a list of dictionaries.\n\n    Parameters\n    ___________\n        Required:\n            infile : the MagIC formatted tab delimited data file\n                first line contains 'tab' in the first column and the data file type in the second (e.g., measurements, specimen, sample, etc.)\n        Optional:\n            data : data read in with, e.g., file.readlines()\n    Returns\n    _______\n        list of dictionaries, file type\n    \"\"\"\n    if infile:\n        if not os.path.exists(infile):\n            if return_keys:\n                return [], 'empty_file', []\n            return [], 'empty_file'\n    hold, magic_data, magic_record, magic_keys = [], [], {}, []\n    if data:\n        lines = list(data)\n    elif (not data) and (not infile):\n        if return_keys:\n            return [], 'empty_file', []\n        return [], 'empty_file'\n    else:\n        # if the file doesn't exist, end here\n        if not os.path.exists(infile):\n            if return_keys:\n                return [], 'bad_file', []\n            return [], 'bad_file'\n        # use custom pmagpy open_file\n        lines = open_file(infile, verbose=verbose)\n    if not lines:\n        if return_keys:\n            return [], 'bad_file', []\n        return [], 'bad_file'\n    d_line = lines[0][:-1].strip('\\n').strip('\\r').strip('\\t')\n    if not d_line:\n        if return_keys:\n            return [], 'empty_file', []\n        return [], 'empty_file'\n    if d_line[0] == \"s\" or d_line[1] == \"s\":\n        delim = 'space'\n    elif d_line[0] == \"t\" or d_line[1] == \"t\":\n        delim = 'tab'\n    else:\n        print('-W- error reading {}. Check that this is a MagIC-format file'.format(infile))\n        if return_keys:\n            return [], 'bad_file', []\n        return [], 'bad_file'\n    if delim == 'space':\n        file_type = d_line.split()[1]\n    if delim == 'tab':\n        file_type = d_line.split('\\t')[1]\n    if file_type == 'delimited':\n        if delim == 'space':\n            file_type = d_line.split()[2]\n        if delim == 'tab':\n            file_type = d_line.split('\\t')[2]\n    line = lines[1].strip('\\n').strip('\\r')\n    if delim == 'space':\n        line = line.split()  # lines[1][:-1].split()\n    if delim == 'tab':\n        line = line.split('\\t')  # lines[1][:-1].split('\\t')\n    for key in line:\n        magic_keys.append(key)\n    lines = lines[2:]\n    if len(lines) < 1:\n        if return_keys:\n            return [], 'empty_file', []\n        return [], 'empty_file'\n    for line in lines[:-1]:\n        line.replace('\\n', '')\n        if delim == 'space':\n            rec = line[:-1].split()\n        if delim == 'tab':\n            rec = line[:-1].split('\\t')\n        hold.append(rec)\n    line = lines[-1].replace('\\n', '').replace('\\r', '')\n    if delim == 'space':\n        rec = line[:-1].split()\n    if delim == 'tab':\n        rec = line.split('\\t')\n    hold.append(rec)\n    for rec in hold:\n        magic_record = {}\n        if len(magic_keys) > len(rec):\n            # pad rec with empty strings if needed\n            for i in range(len(magic_keys) - len(rec)):\n                rec.append('')\n        if len(magic_keys) != len(rec):\n            # ignores this warning when reading the dividers in an upload.txt\n            # composite file\n            if rec != ['>>>>>>>>>>'] and 'delimited' not in rec[0]:\n                print(\"Warning: Uneven record lengths detected in {}: \".format(infile))\n                print('keys:', magic_keys)\n                print('record:', rec)\n        # modified by Ron Shaar:\n        # add a health check:\n        # if len(magic_keys) > len(rec): take rec\n        # if len(magic_keys) < len(rec): take magic_keys\n        # original code: for k in range(len(rec)):\n        # channged to: for k in range(min(len(magic_keys),len(rec))):\n        for k in range(min(len(magic_keys), len(rec))):\n            magic_record[magic_keys[k]] = rec[k].strip('\\n').strip('\\r')\n        magic_data.append(magic_record)\n    magictype = file_type.lower().split(\"_\")\n    Types = ['er', 'magic', 'pmag', 'rmag']\n    if magictype in Types:\n        file_type = file_type.lower()\n    if return_keys:\n        return magic_data, file_type, magic_keys\n    return magic_data, file_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef magic_read_dict(path, data=None, sort_by_this_name=None, return_keys=False):\n    DATA = {}\n    #fin = open(path, 'r')\n    #first_line = fin.readline()\n    lines = open_file(path)\n    if not lines:\n        if return_keys:\n            return {}, 'empty_file', None\n        else:\n            return {}, 'empty_file'\n    first_line = lines.pop(0)\n    if first_line[0] == \"s\" or first_line[1] == \"s\":\n        delim = ' '\n    elif first_line[0] == \"t\" or first_line[1] == \"t\":\n        delim = '\\t'\n    else:\n        print('-W- error reading ', path)\n        if return_keys:\n            return {}, 'bad_file', None\n        else:\n            return {}, 'bad_file'\n\n    file_type = first_line.strip('\\n').strip('\\r').split(delim)[1]\n\n    item_type = file_type\n    #item_type = file_type.split('_')[1][:-1]\n    if sort_by_this_name:\n        pass\n    elif item_type == 'age':\n        sort_by_this_name = \"by_line_number\"\n    else:\n        sort_by_this_name = item_type\n    line = lines.pop(0)\n    header = line.strip('\\n').strip('\\r').split(delim)\n    counter = 0\n    for line in lines:\n        tmp_data = {}\n        tmp_line = line.strip('\\n').strip('\\r').split(delim)\n        for i in range(len(header)):\n            if i < len(tmp_line):\n                tmp_data[header[i]] = tmp_line[i].strip()\n            else:\n                tmp_data[header[i]] = \"\"\n        if sort_by_this_name == \"by_line_number\":\n            DATA[counter] = tmp_data\n            counter += 1\n        else:\n            if tmp_data[sort_by_this_name] != \"\":\n                DATA[tmp_data[sort_by_this_name]] = tmp_data\n    if return_keys:\n        return DATA, file_type, header\n    else:\n        return DATA, file_type", "response": "Read a magic - formatted tab - delimited file and return a dictionary of the data and file type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort_magic_data(magic_data, sort_name):\n    '''\n    Sort magic_data by header (like er_specimen_name for example)\n    '''\n    magic_data_sorted = {}\n    for rec in magic_data:\n        name = rec[sort_name]\n        if name not in list(magic_data_sorted.keys()):\n            magic_data_sorted[name] = []\n        magic_data_sorted[name].append(rec)\n    return magic_data_sorted", "response": "Sort magic_data by header name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload_read(infile, table):\n    delim = 'tab'\n    hold, magic_data, magic_record, magic_keys = [], [], {}, []\n    f = open(infile, \"r\")\n#\n# look for right table\n#\n    line = f.readline()[:-1]\n    file_type = line.split('\\t')[1]\n    if file_type == 'delimited':\n        file_type = line.split('\\t')[2]\n    if delim == 'tab':\n        line = f.readline()[:-1].split('\\t')\n    else:\n        f.close()\n        print(\"only tab delimitted files are supported now\")\n        return\n    while file_type != table:\n        while line[0][0:5] in f.readlines() != \">>>>>\":\n            pass\n        line = f.readline()[:-1]\n        file_type = line.split('\\t')[1]\n        if file_type == 'delimited':\n            file_type = line.split('\\t')[2]\n        ine = f.readline()[:-1].split('\\t')\n    while line[0][0:5] in f.readlines() != \">>>>>\":\n        for key in line:\n            magic_keys.append(key)\n        for line in f.readlines():\n            rec = line[:-1].split('\\t')\n            hold.append(rec)\n        for rec in hold:\n            magic_record = {}\n            if len(magic_keys) != len(rec):\n                print(\"Uneven record lengths detected: \", rec)\n                input(\"Return to continue.... \")\n            for k in range(len(magic_keys)):\n                magic_record[magic_keys[k]] = rec[k]\n            magic_data.append(magic_record)\n    f.close()\n    return magic_data", "response": "Reads a MagIC upload or downloaded txt file and puts data in a list of dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites out a magic format record to ofile", "response": "def putout(ofile, keylist, Rec):\n    \"\"\"\n    writes out a magic format record to ofile\n    \"\"\"\n    pmag_out = open(ofile, 'a')\n    outstring = \"\"\n    for key in keylist:\n        try:\n            outstring = outstring + '\\t' + str(Rec[key]).strip()\n        except:\n            print(key, Rec[key])\n            # raw_input()\n    outstring = outstring + '\\n'\n    pmag_out.write(outstring[1:])\n    pmag_out.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens the file ofile as a magic template file with headers as the keys to Rec", "response": "def first_rec(ofile, Rec, file_type):\n    \"\"\"\n    opens the file ofile as a magic template file with headers as the keys to Rec\n    \"\"\"\n    keylist = []\n    opened = False\n    # sometimes Windows needs a little extra time to open a file\n    # or else it throws an error\n    while not opened:\n        try:\n            pmag_out = open(ofile, 'w')\n            opened = True\n        except IOError:\n            time.sleep(1)\n    outstring = \"tab \\t\" + file_type + \"\\n\"\n    pmag_out.write(outstring)\n    keystring = \"\"\n    for key in list(Rec.keys()):\n        keystring = keystring + '\\t' + key.strip()\n        keylist.append(key)\n    keystring = keystring + '\\n'\n    pmag_out.write(keystring[1:])\n    pmag_out.close()\n    return keylist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef magic_write_old(ofile, Recs, file_type):\n    if len(Recs) < 1:\n        print ('nothing to write')\n        return\n    pmag_out = open(ofile, 'w')\n    outstring = \"tab \\t\" + file_type + \"\\n\"\n    pmag_out.write(outstring)\n    keystring = \"\"\n    keylist = []\n    for key in list(Recs[0].keys()):\n        keylist.append(key)\n    keylist.sort()\n    for key in keylist:\n        keystring = keystring + '\\t' + key.strip()\n    keystring = keystring + '\\n'\n    pmag_out.write(keystring[1:])\n    for Rec in Recs:\n        outstring = \"\"\n        for key in keylist:\n            try:\n                outstring = outstring + '\\t' + str(Rec[key].strip())\n            except:\n                if 'er_specimen_name' in list(Rec.keys()):\n                    print(Rec['er_specimen_name'])\n                elif 'er_specimen_names' in list(Rec.keys()):\n                    print(Rec['er_specimen_names'])\n                print(key, Rec[key])\n                # raw_input()\n        outstring = outstring + '\\n'\n        pmag_out.write(outstring[1:])\n    pmag_out.close()", "response": "This function writes out a MagIC formatted file from a list of dictionaries in MagIC format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dotilt(dec, inc, bed_az, bed_dip):\n    rad = old_div(np.pi, 180.)  # converts from degrees to radians\n    X = dir2cart([dec, inc, 1.])  # get cartesian coordinates of dec,inc\n# get some sines and cosines of new coordinate system\n    sa, ca = -np.sin(bed_az * rad), np.cos(bed_az * rad)\n    cdp, sdp = np.cos(bed_dip * rad), np.sin(bed_dip * rad)\n# do the rotation\n    xc = X[0] * (sa * sa + ca * ca * cdp) + X[1] * \\\n        (ca * sa * (1. - cdp)) + X[2] * sdp * ca\n    yc = X[0] * ca * sa * (1. - cdp) + X[1] * \\\n        (ca * ca + sa * sa * cdp) - X[2] * sa * sdp\n    zc = X[0] * ca * sdp - X[1] * sdp * sa - X[2] * cdp\n# convert back to direction:\n    Dir = cart2dir([xc, yc, -zc])\n    # return declination, inclination of rotated direction\n    return Dir[0], Dir[1]", "response": "Returns a tuple of declination inclination and rotation of a new tilt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dotilt_V(indat):\n    indat = indat.transpose()\n    # unpack input array into separate arrays\n    dec, inc, bed_az, bed_dip = indat[0], indat[1], indat[2], indat[3]\n    rad = old_div(np.pi, 180.)  # convert to radians\n    Dir = np.array([dec, inc]).transpose()\n    X = dir2cart(Dir).transpose()  # get cartesian coordinates\n    N = np.size(dec)\n\n# get some sines and cosines of new coordinate system\n    sa, ca = -np.sin(bed_az * rad), np.cos(bed_az * rad)\n    cdp, sdp = np.cos(bed_dip * rad), np.sin(bed_dip * rad)\n# do the rotation\n    xc = X[0] * (sa * sa + ca * ca * cdp) + X[1] * \\\n        (ca * sa * (1. - cdp)) + X[2] * sdp * ca\n    yc = X[0] * ca * sa * (1. - cdp) + X[1] * \\\n        (ca * ca + sa * sa * cdp) - X[2] * sa * sdp\n    zc = X[0] * ca * sdp - X[1] * sdp * sa - X[2] * cdp\n# convert back to direction:\n    cart = np.array([xc, yc, -zc]).transpose()\n    Dir = cart2dir(cart).transpose()\n    # return declination, inclination arrays of rotated direction\n    return Dir[0], Dir[1]", "response": "This function does a tilt correction on an array with rows of dec inc bed az direction and dip."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrotates declination and inclination into geographic coordinates using the Z direction of the lab arrow of the X direction.", "response": "def dogeo(dec, inc, az, pl):\n    \"\"\"\n    Rotates declination and inclination into geographic coordinates using the\n    azimuth and plunge of the X direction (lab arrow) of a specimen.\n\n    Parameters\n    ----------\n    dec : declination in specimen coordinates\n    inc : inclination in specimen coordinates\n\n    Returns\n    -------\n    rotated_direction : tuple of declination, inclination in geographic coordinates\n\n    Examples\n    --------\n    >>> pmag.dogeo(0.0,90.0,0.0,45.5)\n    (180.0, 44.5)\n    \"\"\"\n    A1, A2, A3 = [], [], []  # set up lists for rotation vector\n    # put dec inc in direction list and set  length to unity\n    Dir = [dec, inc, 1.]\n    X = dir2cart(Dir)  # get cartesian coordinates\n#\n#   set up rotation matrix\n#\n    A1 = dir2cart([az, pl, 1.])\n    A2 = dir2cart([az + 90., 0, 1.])\n    A3 = dir2cart([az - 180., 90. - pl, 1.])\n#\n# do rotation\n#\n    xp = A1[0] * X[0] + A2[0] * X[1] + A3[0] * X[2]\n    yp = A1[1] * X[0] + A2[1] * X[1] + A3[1] * X[2]\n    zp = A1[2] * X[0] + A2[2] * X[1] + A3[2] * X[2]\n#\n# transform back to dec,inc\n#\n    Dir_geo = cart2dir([xp, yp, zp])\n    return Dir_geo[0], Dir_geo[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dogeo_V(indat):\n    indat = indat.transpose()\n    # unpack input array into separate arrays\n    dec, inc, az, pl = indat[0], indat[1], indat[2], indat[3]\n    Dir = np.array([dec, inc]).transpose()\n    X = dir2cart(Dir).transpose()  # get cartesian coordinates\n    N = np.size(dec)\n    A1 = dir2cart(np.array([az, pl, np.ones(N)]).transpose()).transpose()\n    A2 = dir2cart(\n        np.array([az + 90., np.zeros(N), np.ones(N)]).transpose()).transpose()\n    A3 = dir2cart(\n        np.array([az - 180., 90. - pl, np.ones(N)]).transpose()).transpose()\n\n# do rotation\n#\n    xp = A1[0] * X[0] + A2[0] * X[1] + A3[0] * X[2]\n    yp = A1[1] * X[0] + A2[1] * X[1] + A3[1] * X[2]\n    zp = A1[2] * X[0] + A2[2] * X[1] + A3[2] * X[2]\n    cart = np.array([xp, yp, zp]).transpose()\n#\n# transform back to dec,inc\n#\n    Dir_geo = cart2dir(cart).transpose()\n    # send back declination and inclination arrays\n    return Dir_geo[0], Dir_geo[1]", "response": "This function takes a nested list of dec inc az and plunge and returns a list of arrays of dec inc az pl and X direction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrotate a single object in the order in which it is in the mean direction.", "response": "def dodirot(D, I, Dbar, Ibar):\n    \"\"\"\n    Rotate a direction (declination, inclination) by the difference between\n    dec=0 and inc = 90 and the provided desired mean direction\n\n    Parameters\n    ----------\n    D : declination to be rotated\n    I : inclination to be rotated\n    Dbar : declination of desired mean\n    Ibar : inclination of desired mean\n\n    Returns\n    ----------\n    drot, irot : rotated declination and inclination\n    \"\"\"\n    d, irot = dogeo(D, I, Dbar, 90. - Ibar)\n    drot = d - 180.\n    if drot < 360.:\n        drot = drot + 360.\n    if drot > 360.:\n        drot = drot - 360.\n    return drot, irot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrotate an array of dec and inc pairs to coordinate system with Dec Inc as 0 90", "response": "def dodirot_V(di_block, Dbar, Ibar):\n    \"\"\"\n    Rotate an array of dec/inc pairs to coordinate system with Dec,Inc as 0,90\n\n    Parameters\n    ___________________\n    di_block : array of [[Dec1,Inc1],[Dec2,Inc2],....]\n    Dbar : declination of desired center\n    Ibar : inclination of desired center\n\n    Returns\n    __________\n    array of rotated decs and incs: [[rot_Dec1,rot_Inc1],[rot_Dec2,rot_Inc2],....]\n    \"\"\"\n    N = di_block.shape[0]\n    DipDir, Dip = np.ones(N, dtype=np.float).transpose(\n    )*(Dbar-180.), np.ones(N, dtype=np.float).transpose()*(90.-Ibar)\n    di_block = di_block.transpose()\n    data = np.array([di_block[0], di_block[1], DipDir, Dip]).transpose()\n    drot, irot = dotilt_V(data)\n    drot = (drot-180.) % 360.  #\n    return np.column_stack((drot, irot))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the orientation info for a given samp", "response": "def find_samp_rec(s, data, az_type):\n    \"\"\"\n    find the orientation info for samp s\n    \"\"\"\n    datablock, or_error, bed_error = [], 0, 0\n    orient = {}\n    orient[\"sample_dip\"] = \"\"\n    orient[\"sample_azimuth\"] = \"\"\n    orient['sample_description'] = \"\"\n    for rec in data:\n        if rec[\"er_sample_name\"].lower() == s.lower():\n            if 'sample_orientation_flag' in list(rec.keys()) and rec['sample_orientation_flag'] == 'b':\n                orient['sample_orientation_flag'] = 'b'\n                return orient\n            if \"magic_method_codes\" in list(rec.keys()) and az_type != \"0\":\n                methods = rec[\"magic_method_codes\"].replace(\" \", \"\").split(\":\")\n                if az_type in methods and \"sample_azimuth\" in list(rec.keys()) and rec[\"sample_azimuth\"] != \"\":\n                    orient[\"sample_azimuth\"] = float(rec[\"sample_azimuth\"])\n                if \"sample_dip\" in list(rec.keys()) and rec[\"sample_dip\"] != \"\":\n                    orient[\"sample_dip\"] = float(rec[\"sample_dip\"])\n                if \"sample_bed_dip_direction\" in list(rec.keys()) and rec[\"sample_bed_dip_direction\"] != \"\":\n                    orient[\"sample_bed_dip_direction\"] = float(\n                        rec[\"sample_bed_dip_direction\"])\n                if \"sample_bed_dip\" in list(rec.keys()) and rec[\"sample_bed_dip\"] != \"\":\n                    orient[\"sample_bed_dip\"] = float(rec[\"sample_bed_dip\"])\n            else:\n                if \"sample_azimuth\" in list(rec.keys()):\n                    orient[\"sample_azimuth\"] = float(rec[\"sample_azimuth\"])\n                if \"sample_dip\" in list(rec.keys()):\n                    orient[\"sample_dip\"] = float(rec[\"sample_dip\"])\n                if \"sample_bed_dip_direction\" in list(rec.keys()):\n                    orient[\"sample_bed_dip_direction\"] = float(\n                        rec[\"sample_bed_dip_direction\"])\n                if \"sample_bed_dip\" in list(rec.keys()):\n                    orient[\"sample_bed_dip\"] = float(rec[\"sample_bed_dip\"])\n                if 'sample_description' in list(rec.keys()):\n                    orient['sample_description'] = rec['sample_description']\n        if orient[\"sample_azimuth\"] != \"\":\n            break\n    return orient"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Vdiff(D1, D2):\n    A = dir2cart([D1[0], D1[1], 1.])\n    B = dir2cart([D2[0], D2[1], 1.])\n    C = []\n    for i in range(3):\n        C.append(A[i] - B[i])\n    return cart2dir(C)", "response": "returns the vector difference between two directions D1 D2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cart2dir(cart):\n    cart = np.array(cart)\n    rad = old_div(np.pi, 180.)  # constant to convert degrees to radians\n    if len(cart.shape) > 1:\n        Xs, Ys, Zs = cart[:, 0], cart[:, 1], cart[:, 2]\n    else:  # single vector\n        Xs, Ys, Zs = cart[0], cart[1], cart[2]\n    if np.iscomplexobj(Xs):\n        Xs = Xs.real\n    if np.iscomplexobj(Ys):\n        Ys = Ys.real\n    if np.iscomplexobj(Zs):\n        Zs = Zs.real\n    Rs = np.sqrt(Xs**2 + Ys**2 + Zs**2)  # calculate resultant vector length\n    # calculate declination taking care of correct quadrants (arctan2) and\n    # making modulo 360.\n    Decs = (old_div(np.arctan2(Ys, Xs), rad)) % 360.\n    try:\n        # calculate inclination (converting to degrees) #\n        Incs = old_div(np.arcsin(old_div(Zs, Rs)), rad)\n    except:\n        print('trouble in cart2dir')  # most likely division by zero somewhere\n        return np.zeros(3)\n\n    return np.array([Decs, Incs, Rs]).transpose()", "response": "Converts a direction in cartesian coordinates into declination inclination intensity"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the orientation matrix of the archive X", "response": "def Tmatrix(X):\n    \"\"\"\n    gets the orientation matrix (T) from data in X\n    \"\"\"\n    T = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n    for row in X:\n        for k in range(3):\n            for l in range(3):\n                T[k][l] += row[k] * row[l]\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a list or array of vector directions in degrees ( declination inclination ) to an array of the direction in cartesian coordinates x y z", "response": "def dir2cart(d):\n    \"\"\"\n    Converts a list or array of vector directions in degrees (declination,\n    inclination) to an array of the direction in cartesian coordinates (x,y,z)\n\n    Parameters\n    ----------\n    d : list or array of [dec,inc] or [dec,inc,intensity]\n\n    Returns\n    -------\n    cart : array of [x,y,z]\n\n    Examples\n    --------\n    >>> pmag.dir2cart([200,40,1])\n    array([-0.71984631, -0.26200263,  0.64278761])\n    \"\"\"\n    ints = np.ones(len(d)).transpose(\n    )  # get an array of ones to plug into dec,inc pairs\n    d = np.array(d)\n    rad = np.pi/180.\n    if len(d.shape) > 1:  # array of vectors\n        decs, incs = d[:, 0] * rad, d[:, 1] * rad\n        if d.shape[1] == 3:\n            ints = d[:, 2]  # take the given lengths\n    else:  # single vector\n        decs, incs = np.array(float(d[0])) * rad, np.array(float(d[1])) * rad\n        if len(d) == 3:\n            ints = np.array(d[2])\n        else:\n            ints = np.array([1.])\n    cart = np.array([ints * np.cos(decs) * np.cos(incs), ints *\n                     np.sin(decs) * np.cos(incs), ints * np.sin(incs)]).transpose()\n    return cart"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef findrec(s, data):\n    datablock = []\n    for rec in data:\n        if s == rec[0]:\n            datablock.append([rec[1], rec[2], rec[3], rec[4]])\n    return datablock", "response": "finds all the records belonging to s in data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets average direction using Fisher or principal component analysis", "response": "def domean(data, start, end, calculation_type):\n    \"\"\"\n    Gets average direction using Fisher or principal component analysis (line\n    or plane) methods\n\n    Parameters\n    ----------\n    data : nest list of data: [[treatment,dec,inc,int,quality],...]\n    start : step being used as start of fit (often temperature minimum)\n    end : step being used as end of fit (often temperature maximum)\n    calculation_type : string describing type of calculation to be made\n    'DE-BFL' (line), 'DE-BFL-A' (line-anchored), 'DE-BFL-O' (line-with-origin),\n    'DE-BFP' (plane), 'DE-FM' (Fisher mean)\n\n    Returns\n    -------\n    mpars : dictionary with the keys \"specimen_n\",\"measurement_step_min\",\n    \"measurement_step_max\",\"specimen_mad\",\"specimen_dec\",\"specimen_inc\"\n    \"\"\"\n    mpars = {}\n    datablock = []\n    start0, end0 = start, end\n    # indata = [rec.append('g') if len(rec)<6 else rec for rec in indata] #\n    # this statement doesn't work!\n    indata = []\n    for rec in data:\n        if len(rec) < 6:\n            rec.append('g')\n        indata.append(rec)\n    if indata[start0][5] == 'b':\n        print(\"Can't select 'bad' point as start for PCA\")\n    flags = [x[5] for x in indata]\n    bad_before_start = flags[:start0].count('b')\n    bad_in_mean = flags[start0:end0 + 1].count('b')\n    start = start0 - bad_before_start\n    end = end0 - bad_before_start - bad_in_mean\n    datablock = [x for x in indata if x[5] == 'g']\n    if indata[start0] != datablock[start]:\n        print('problem removing bad data in pmag.domean start of datablock shifted:\\norigional: %d\\nafter removal: %d' % (\n            start0, indata.index(datablock[start])))\n    if indata[end0] != datablock[end]:\n        print('problem removing bad data in pmag.domean end of datablock shifted:\\norigional: %d\\nafter removal: %d' % (\n            end0, indata.index(datablock[end])))\n    mpars[\"calculation_type\"] = calculation_type\n    rad = old_div(np.pi, 180.)\n    if end > len(datablock) - 1 or end < start:\n        end = len(datablock) - 1\n    control, data, X, Nrec = [], [], [], float(end - start + 1)\n    cm = [0., 0., 0.]\n#\n#  get cartesian coordinates\n#\n    fdata = []\n    for k in range(start, end + 1):\n        if calculation_type == 'DE-BFL' or calculation_type == 'DE-BFL-A' or calculation_type == 'DE-BFL-O':  # best-fit line\n            data = [datablock[k][1], datablock[k][2], datablock[k][3]]\n        else:\n            data = [datablock[k][1], datablock[k][2], 1.0]  # unit weight\n        fdata.append(data)\n        cart = dir2cart(data)\n        X.append(cart)\n    if calculation_type == 'DE-BFL-O':  # include origin as point\n        X.append([0., 0., 0.])\n        # pass\n    if calculation_type == 'DE-FM':  # for fisher means\n        fpars = fisher_mean(fdata)\n        mpars[\"specimen_direction_type\"] = 'l'\n        mpars[\"specimen_dec\"] = fpars[\"dec\"]\n        mpars[\"specimen_inc\"] = fpars[\"inc\"]\n        mpars[\"specimen_alpha95\"] = fpars[\"alpha95\"]\n        mpars[\"specimen_n\"] = fpars[\"n\"]\n        mpars[\"specimen_r\"] = fpars[\"r\"]\n        mpars[\"measurement_step_min\"] = indata[start0][0]\n        mpars[\"measurement_step_max\"] = indata[end0][0]\n        mpars[\"center_of_mass\"] = cm\n        mpars[\"specimen_dang\"] = -1\n        return mpars\n#\n#   get center of mass for principal components (DE-BFL or DE-BFP)\n#\n    for cart in X:\n        for l in range(3):\n            cm[l] += old_div(cart[l], Nrec)\n    mpars[\"center_of_mass\"] = cm\n\n#\n#   transform to center of mass (if best-fit line)\n#\n    if calculation_type != 'DE-BFP':\n        mpars[\"specimen_direction_type\"] = 'l'\n    if calculation_type == 'DE-BFL' or calculation_type == 'DE-BFL-O':  # not for planes or anchored lines\n        for k in range(len(X)):\n            for l in range(3):\n                X[k][l] = X[k][l] - cm[l]\n    else:\n        mpars[\"specimen_direction_type\"] = 'p'\n\n#\n#   put in T matrix\n#\n    T = np.array(Tmatrix(X))\n#\n#   get sorted evals/evects\n#\n    t, V = tauV(T)\n    if t == []:\n        mpars[\"specimen_direction_type\"] = \"Error\"\n        print(\"Error in calculation\")\n        return mpars\n    v1, v3 = V[0], V[2]\n    if t[2] < 0:\n        t[2] = 0  # make positive\n    if calculation_type == 'DE-BFL-A':\n        Dir, R = vector_mean(fdata)\n        mpars[\"specimen_direction_type\"] = 'l'\n        mpars[\"specimen_dec\"] = Dir[0]\n        mpars[\"specimen_inc\"] = Dir[1]\n        mpars[\"specimen_n\"] = len(fdata)\n        mpars[\"measurement_step_min\"] = indata[start0][0]\n        mpars[\"measurement_step_max\"] = indata[end0][0]\n        mpars[\"center_of_mass\"] = cm\n        s1 = np.sqrt(t[0])\n        MAD = old_div(np.arctan(old_div(np.sqrt(t[1] + t[2]), s1)), rad)\n        if np.iscomplexobj(MAD):\n            MAD = MAD.real\n        # I think this is how it is done - i never anchor the \"PCA\" - check\n        mpars[\"specimen_mad\"] = MAD\n        return mpars\n    if calculation_type != 'DE-BFP':\n        #\n        #   get control vector for principal component direction\n        #\n        rec = [datablock[start][1], datablock[start][2], datablock[start][3]]\n        P1 = dir2cart(rec)\n        rec = [datablock[end][1], datablock[end][2], datablock[end][3]]\n        P2 = dir2cart(rec)\n#\n#   get right direction along principal component\n##\n        for k in range(3):\n            control.append(P1[k] - P2[k])\n        # changed by rshaar\n        # control is taken as the center of mass\n        # control=cm\n\n        dot = 0\n        for k in range(3):\n            dot += v1[k] * control[k]\n        if dot < -1:\n            dot = -1\n        if dot > 1:\n            dot = 1\n        if np.arccos(dot) > old_div(np.pi, 2.):\n            for k in range(3):\n                v1[k] = -v1[k]\n#   get right direction along principal component\n#\n        s1 = np.sqrt(t[0])\n        Dir = cart2dir(v1)\n        MAD = old_div(np.arctan(old_div(np.sqrt(t[1] + t[2]), s1)), rad)\n        if np.iscomplexobj(MAD):\n            MAD = MAD.real\n    if calculation_type == \"DE-BFP\":\n        Dir = cart2dir(v3)\n        MAD = old_div(\n            np.arctan(np.sqrt(old_div(t[2], t[1]) + old_div(t[2], t[0]))), rad)\n        if np.iscomplexobj(MAD):\n            MAD = MAD.real\n#\n#   get angle with  center of mass\n#\n    CMdir = cart2dir(cm)\n    Dirp = [Dir[0], Dir[1], 1.]\n    dang = angle(CMdir, Dirp)\n    mpars[\"specimen_dec\"] = Dir[0]\n    mpars[\"specimen_inc\"] = Dir[1]\n    mpars[\"specimen_mad\"] = MAD\n    # mpars[\"specimen_n\"]=int(Nrec)\n    mpars[\"specimen_n\"] = len(X)\n    mpars[\"specimen_dang\"] = dang[0]\n    mpars[\"measurement_step_min\"] = indata[start0][0]\n    mpars[\"measurement_step_max\"] = indata[end0][0]\n    return mpars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to calculate points on a circle about dec dip with angle alpha", "response": "def circ(dec, dip, alpha):\n    \"\"\"\n    function to calculate points on an circle about dec,dip with angle alpha\n    \"\"\"\n    rad = old_div(np.pi, 180.)\n    D_out, I_out = [], []\n    dec, dip, alpha = dec * rad, dip * rad, alpha * rad\n    dec1 = dec + old_div(np.pi, 2.)\n    isign = 1\n    if dip != 0:\n        isign = (old_div(abs(dip), dip))\n    dip1 = (dip - isign * (old_div(np.pi, 2.)))\n    t = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    v = [0, 0, 0]\n    t[0][2] = np.cos(dec) * np.cos(dip)\n    t[1][2] = np.sin(dec) * np.cos(dip)\n    t[2][2] = np.sin(dip)\n    t[0][1] = np.cos(dec) * np.cos(dip1)\n    t[1][1] = np.sin(dec) * np.cos(dip1)\n    t[2][1] = np.sin(dip1)\n    t[0][0] = np.cos(dec1)\n    t[1][0] = np.sin(dec1)\n    t[2][0] = 0\n    for i in range(101):\n        psi = float(i) * np.pi / 50.\n        v[0] = np.sin(alpha) * np.cos(psi)\n        v[1] = np.sin(alpha) * np.sin(psi)\n        v[2] = np.sqrt(abs(1. - v[0]**2 - v[1]**2))\n        elli = [0, 0, 0]\n        for j in range(3):\n            for k in range(3):\n                elli[j] = elli[j] + t[j][k] * v[k]\n        Dir = cart2dir(elli)\n        D_out.append(Dir[0])\n        I_out.append(Dir[1])\n    return D_out, I_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the paleointensity magic parameters", "response": "def PintPars(datablock, araiblock, zijdblock, start, end, accept, **kwargs):\n    \"\"\"\n     calculate the paleointensity magic parameters  make some definitions\n    \"\"\"\n    if 'version' in list(kwargs.keys()) and kwargs['version'] == 3:\n        meth_key = 'method_codes'\n        beta_key = 'int_b_beta'\n        temp_key, min_key, max_key = 'treat_temp', 'meas_step_min', 'meas_step_max'\n        dc_theta_key, dc_phi_key = 'treat_dc_field_theta', 'treat_dc_field_phi'\n        # convert dataframe to list of dictionaries\n        datablock = datablock.to_dict('records')\n        z_key = 'int_z'\n        drats_key = 'int_drats'\n        drat_key = 'int_drat'\n        md_key = 'int_md'\n        dec_key = 'dir_dec'\n        inc_key = 'dir_inc'\n        mad_key = 'int_mad_free'\n        dang_key = 'int_dang'\n        ptrm_key = 'int_n_ptrm'\n        theta_key = 'int_theta'\n        gamma_key = 'int_gamma'\n        delta_key = 'int_delta'\n        frac_key = 'int_frac'\n        gmax_key = 'int_gmax'\n        scat_key = 'int_scat'\n    else:\n        beta_key = 'specimen_b_beta'\n        meth_key = 'magic_method_codes'\n        temp_key, min_key, max_key = 'treatment_temp', 'measurement_step_min', 'measurement_step_max'\n        z_key = 'specimen_z'\n        drats_key = 'specimen_drats'\n        drat_key = 'specimen_drat'\n        md_key = 'specimen_md'\n        dec_key = 'specimen_dec'\n        inc_key = 'specimen_inc'\n        mad_key = 'specimen_int_mad'\n        dang_key = 'specimen_dang'\n        ptrm_key = 'specimen_int_ptrm_n'\n        theta_key = 'specimen_theta'\n        gamma_key = 'specimen_gamma'\n        delta_key = 'specimen_delta'\n        frac_key = 'specimen_frac'\n        gmax_key = 'specimen_gmax'\n        scat_key = 'specimen_scat'\n\n    first_Z, first_I, zptrm_check, ptrm_check, ptrm_tail = [], [], [], [], []\n    methcode, ThetaChecks, DeltaChecks, GammaChecks = \"\", \"\", \"\", \"\"\n    zptrm_check = []\n    first_Z, first_I, ptrm_check, ptrm_tail, zptrm_check, GammaChecks = araiblock[\n        0], araiblock[1], araiblock[2], araiblock[3], araiblock[4], araiblock[5]\n    if len(araiblock) > 6:\n        # used only for perpendicular method of paleointensity\n        ThetaChecks = araiblock[6]\n        # used only for perpendicular  method of paleointensity\n        DeltaChecks = araiblock[7]\n    xi, yi, diffcum = [], [], 0\n    xiz, xzi, yiz, yzi = [], [], [], []\n    Nptrm, dmax = 0, -1e-22\n# check if even zero and infield steps\n    if len(first_Z) > len(first_I):\n        maxe = len(first_I) - 1\n    else:\n        maxe = len(first_Z) - 1\n    if end == 0 or end > maxe:\n        end = maxe\n# get the MAD, DANG, etc. for directional data\n    bstep = araiblock[0][start][0]\n    estep = araiblock[0][end][0]\n    zstart, zend = 0, len(zijdblock)\n    for k in range(len(zijdblock)):\n        zrec = zijdblock[k]\n        if zrec[0] == bstep:\n            zstart = k\n        if zrec[0] == estep:\n            zend = k\n    PCA = domean(zijdblock, zstart, zend, 'DE-BFL')\n    D, Diz, Dzi, Du = [], [], [], []  # list of NRM vectors, and separated by zi and iz\n    for rec in zijdblock:\n        D.append((rec[1], rec[2], rec[3]))\n        Du.append((rec[1], rec[2]))\n        if rec[4] == 1:\n            Dzi.append((rec[1], rec[2]))  # if this is ZI step\n        else:\n            Diz.append((rec[1], rec[2]))  # if this is IZ step\n# calculate the vector difference sum\n    vds = dovds(D)\n    b_zi, b_iz = [], []\n# collect data included in ZigZag calculation\n    if end + 1 >= len(first_Z):\n        stop = end - 1\n    else:\n        stop = end\n    for k in range(start, end + 1):\n        for l in range(len(first_I)):\n            irec = first_I[l]\n            if irec[0] == first_Z[k][0]:\n                xi.append(irec[3])\n                yi.append(first_Z[k][3])\n    pars, errcode = int_pars(xi, yi, vds)\n    if errcode == 1:\n        return pars, errcode\n#    for k in range(start,end+1):\n    for k in range(len(first_Z) - 1):\n        for l in range(k):\n            # only go down to 10% of NRM.....\n            if old_div(first_Z[k][3], vds) > 0.1:\n                irec = first_I[l]\n                if irec[4] == 1 and first_I[l + 1][4] == 0:  # a ZI step\n                    xzi = irec[3]\n                    yzi = first_Z[k][3]\n                    xiz = first_I[l + 1][3]\n                    yiz = first_Z[k + 1][3]\n                    slope = np.arctan2((yzi - yiz), (xiz - xzi))\n                    r = np.sqrt((yzi - yiz)**2 + (xiz - xzi)**2)\n                    if r > .1 * vds:\n                        b_zi.append(slope)  # suppress noise\n                elif irec[4] == 0 and first_I[l + 1][4] == 1:  # an IZ step\n                    xiz = irec[3]\n                    yiz = first_Z[k][3]\n                    xzi = first_I[l + 1][3]\n                    yzi = first_Z[k + 1][3]\n                    slope = np.arctan2((yiz - yzi), (xzi - xiz))\n                    r = np.sqrt((yiz - yzi)**2 + (xzi - xiz)**2)\n                    if r > .1 * vds:\n                        b_iz.append(slope)  # suppress noise\n#\n    ZigZag, Frat, Trat = -1, 0, 0\n    if len(Diz) > 2 and len(Dzi) > 2:\n        ZigZag = 0\n        dizp = fisher_mean(Diz)  # get Fisher stats on IZ steps\n        dzip = fisher_mean(Dzi)  # get Fisher stats on ZI steps\n        dup = fisher_mean(Du)  # get Fisher stats on all steps\n#\n# if directions are TOO well grouped, can get false positive for ftest, so\n# angles must be > 3 degrees apart.\n#\n        if angle([dizp['dec'], dizp['inc']], [dzip['dec'], dzip['inc']]) > 3.:\n            F = (dup['n'] - 2.) * (dzip['r'] + dizp['r'] - dup['r']) / \\\n                (dup['n'] - dzip['r'] - dizp['r']\n                 )  # Watson test for common mean\n            nf = 2. * (dup['n'] - 2.)  # number of degees of freedom\n            ftest = fcalc(2, nf)\n            Frat = old_div(F, ftest)\n            if Frat > 1.:\n                ZigZag = Frat  # fails zigzag on directions\n                methcode = \"SM-FTEST\"\n# now do slopes\n    if len(b_zi) > 2 and len(b_iz) > 2:\n        bzi_m, bzi_sig = gausspars(b_zi)  # mean, std dev\n        biz_m, biz_sig = gausspars(b_iz)\n        n_zi = float(len(b_zi))\n        n_iz = float(len(b_iz))\n        b_diff = abs(bzi_m - biz_m)  # difference in means\n#\n# avoid false positives - set 3 degree slope difference here too\n        if b_diff > 3 * np.pi / 180.:\n            nf = n_zi + n_iz - 2.  # degrees of freedom\n            svar = old_div(((n_zi - 1.) * bzi_sig**2 +\n                            (n_iz - 1.) * biz_sig**2), nf)\n            T = old_div((b_diff), np.sqrt(\n                svar * (old_div(1.0, n_zi) + old_div(1.0, n_iz))))  # student's t\n            ttest = tcalc(nf, .05)  # t-test at 95% conf.\n            Trat = old_div(T, ttest)\n            if Trat > 1 and Trat > Frat:\n                ZigZag = Trat  # fails zigzag on directions\n                methcode = \"SM-TTEST\"\n    pars[z_key] = ZigZag\n    pars[meth_key] = methcode\n# do drats\n    if len(ptrm_check) != 0:\n        diffcum, drat_max = 0, 0\n        for prec in ptrm_check:\n            step = prec[0]\n            endbak = end\n            zend = end\n            while zend > len(zijdblock) - 1:\n                zend = zend - 2  # don't count alteration that happens after this step\n            if step < zijdblock[zend][0]:\n                Nptrm += 1\n                for irec in first_I:\n                    if irec[0] == step:\n                        break\n                diffcum += prec[3] - irec[3]\n                if abs(prec[3] - irec[3]) > drat_max:\n                    drat_max = abs(prec[3] - irec[3])\n        pars[drats_key] = (100 * abs(diffcum) / first_I[zend][3])\n        pars[drat_key] = (100 * abs(drat_max) / first_I[zend][3])\n    elif len(zptrm_check) != 0:\n        diffcum = 0\n        for prec in zptrm_check:\n            step = prec[0]\n            endbak = end\n            zend = end\n            while zend > len(zijdblock) - 1:\n                zend = zend - 1\n            if step < zijdblock[zend][0]:\n                Nptrm += 1\n                for irec in first_I:\n                    if irec[0] == step:\n                        break\n                diffcum += prec[3] - irec[3]\n        pars[drats_key] = (100 * abs(diffcum) / first_I[zend][3])\n    else:\n        pars[drats_key] = -1\n        pars[drat_key] = -1\n# and the pTRM tails\n    if len(ptrm_tail) != 0:\n        for trec in ptrm_tail:\n            step = trec[0]\n            for irec in first_I:\n                if irec[0] == step:\n                    break\n            if abs(trec[3]) > dmax:\n                dmax = abs(trec[3])\n        pars[md_key] = (100 * dmax / vds)\n    else:\n        pars[md_key] = -1\n    pars[min_key] = bstep\n    pars[max_key] = estep\n    pars[dec_key] = PCA[\"specimen_dec\"]\n    pars[inc_key] = PCA[\"specimen_inc\"]\n    pars[mad_key] = PCA[\"specimen_mad\"]\n    pars[dang_key] = PCA[\"specimen_dang\"]\n    pars[ptrm_key] = Nptrm\n# and the ThetaChecks\n    if ThetaChecks != \"\":\n        t = 0\n        for theta in ThetaChecks:\n            if theta[0] >= bstep and theta[0] <= estep and theta[1] > t:\n                t = theta[1]\n        pars[theta_key] = t\n    else:\n        pars[theta_key] = -1\n# and the DeltaChecks\n    if DeltaChecks != \"\":\n        d = 0\n        for delta in DeltaChecks:\n            if delta[0] >= bstep and delta[0] <= estep and delta[1] > d:\n                d = delta[1]\n        pars[delta_key]\n    else:\n        pars[delta_key] = -1\n    pars[gamma_key] = -1\n    if GammaChecks != \"\":\n        for gamma in GammaChecks:\n            if gamma[0] <= estep:\n                pars['specimen_gamma'] = gamma[1]\n\n    # --------------------------------------------------------------\n    # From here added By Ron Shaar 11-Dec 2012\n    # New parameters defined in Shaar and Tauxe (2012):\n    # FRAC (specimen_frac) - ranges from 0. to 1.\n    # SCAT (specimen_scat) - takes 1/0\n    # gap_max (specimen_gmax) - ranges from 0. to 1.\n    # --------------------------------------------------------------\n\n    # --------------------------------------------------------------\n    # FRAC is similar to Fvds, but the numerator is the vds fraction:\n    # FRAC= [ vds (start,end)] / total vds ]\n    # gap_max= max [ (vector difference) /  vds (start,end)]\n    # --------------------------------------------------------------\n\n    # collect all zijderveld data to arrays and calculate VDS\n\n    z_temperatures = [row[0] for row in zijdblock]\n    zdata = []                # array of zero-fields measurements in Cartezian coordinates\n    # array of vector differences (for vds calculation)\n    vector_diffs = []\n    NRM = zijdblock[0][3]     # NRM\n\n    for k in range(len(zijdblock)):\n        DIR = [zijdblock[k][1], zijdblock[k][2], old_div(zijdblock[k][3], NRM)]\n        cart = dir2cart(DIR)\n        zdata.append(np.array([cart[0], cart[1], cart[2]]))\n        if k > 0:\n            vector_diffs.append(\n                np.sqrt(sum((np.array(zdata[-2]) - np.array(zdata[-1]))**2)))\n    # last vector difference: from the last point to the origin.\n    vector_diffs.append(np.sqrt(sum(np.array(zdata[-1])**2)))\n    vds = sum(vector_diffs)  # vds calculation\n    zdata = np.array(zdata)\n    vector_diffs = np.array(vector_diffs)\n\n    # calculate the vds within the chosen segment\n    vector_diffs_segment = vector_diffs[zstart:zend]\n    # FRAC calculation\n    FRAC = old_div(sum(vector_diffs_segment), vds)\n    pars[frac_key] = FRAC\n\n    # gap_max calculation\n    max_FRAC_gap = max(\n        old_div(vector_diffs_segment, sum(vector_diffs_segment)))\n    pars[gmax_key] = max_FRAC_gap\n\n    # ---------------------------------------------------------------------\n    # Calculate the \"scat box\"\n    # all data-points, pTRM checks, and tail-checks, should be inside a \"scat box\"\n    # ---------------------------------------------------------------------\n\n    # intialization\n    # fail scat due to arai plot data points\n    pars[\"fail_arai_beta_box_scatter\"] = False\n    pars[\"fail_ptrm_beta_box_scatter\"] = False  # fail scat due to pTRM checks\n    pars[\"fail_tail_beta_box_scatter\"] = False  # fail scat due to tail checks\n    pars[scat_key] = \"t\"  # Pass by default\n\n    # --------------------------------------------------------------\n    # collect all Arai plot data points in arrays\n\n    x_Arai, y_Arai, t_Arai, steps_Arai = [], [], [], []\n    NRMs = araiblock[0]\n    PTRMs = araiblock[1]\n    ptrm_checks = araiblock[2]\n    ptrm_tail = araiblock[3]\n\n    PTRMs_temperatures = [row[0] for row in PTRMs]\n    NRMs_temperatures = [row[0] for row in NRMs]\n    NRM = NRMs[0][3]\n\n    for k in range(len(NRMs)):\n        index_pTRMs = PTRMs_temperatures.index(NRMs[k][0])\n        x_Arai.append(old_div(PTRMs[index_pTRMs][3], NRM))\n        y_Arai.append(old_div(NRMs[k][3], NRM))\n        t_Arai.append(NRMs[k][0])\n        if NRMs[k][4] == 1:\n            steps_Arai.append('ZI')\n        else:\n            steps_Arai.append('IZ')\n    x_Arai = np.array(x_Arai)\n    y_Arai = np.array(y_Arai)\n\n    # --------------------------------------------------------------\n    # collect all pTRM check to arrays\n\n    x_ptrm_check, y_ptrm_check, ptrm_checks_temperatures, = [], [], []\n    x_ptrm_check_starting_point, y_ptrm_check_starting_point, ptrm_checks_starting_temperatures = [], [], []\n\n    for k in range(len(ptrm_checks)):\n        if ptrm_checks[k][0] in NRMs_temperatures:\n            # find the starting point of the pTRM check:\n            for i in range(len(datablock)):\n                rec = datablock[i]\n                if \"LT-PTRM-I\" in rec[meth_key] and float(rec[temp_key]) == ptrm_checks[k][0]:\n                    starting_temperature = (float(datablock[i - 1][temp_key]))\n                    try:\n                        index = t_Arai.index(starting_temperature)\n                        x_ptrm_check_starting_point.append(x_Arai[index])\n                        y_ptrm_check_starting_point.append(y_Arai[index])\n                        ptrm_checks_starting_temperatures.append(\n                            starting_temperature)\n\n                        index_zerofield = zerofield_temperatures.index(\n                            ptrm_checks[k][0])\n                        x_ptrm_check.append(old_div(ptrm_checks[k][3], NRM))\n                        y_ptrm_check.append(\n                            old_div(zerofields[index_zerofield][3], NRM))\n                        ptrm_checks_temperatures.append(ptrm_checks[k][0])\n\n                        break\n                    except:\n                        pass\n\n    x_ptrm_check_starting_point = np.array(x_ptrm_check_starting_point)\n    y_ptrm_check_starting_point = np.array(y_ptrm_check_starting_point)\n    ptrm_checks_starting_temperatures = np.array(\n        ptrm_checks_starting_temperatures)\n    x_ptrm_check = np.array(x_ptrm_check)\n    y_ptrm_check = np.array(y_ptrm_check)\n    ptrm_checks_temperatures = np.array(ptrm_checks_temperatures)\n\n    # --------------------------------------------------------------\n    # collect tail checks to arrays\n\n    x_tail_check, y_tail_check, tail_check_temperatures = [], [], []\n    x_tail_check_starting_point, y_tail_check_starting_point, tail_checks_starting_temperatures = [], [], []\n\n    for k in range(len(ptrm_tail)):\n        if ptrm_tail[k][0] in NRMs_temperatures:\n\n            # find the starting point of the pTRM check:\n            for i in range(len(datablock)):\n                rec = datablock[i]\n                if \"LT-PTRM-MD\" in rec[meth_key] and float(rec[temp_key]) == ptrm_tail[k][0]:\n                    starting_temperature = (float(datablock[i - 1][temp_key]))\n                    try:\n\n                        index = t_Arai.index(starting_temperature)\n                        x_tail_check_starting_point.append(x_Arai[index])\n                        y_tail_check_starting_point.append(y_Arai[index])\n                        tail_checks_starting_temperatures.append(\n                            starting_temperature)\n\n                        index_infield = infield_temperatures.index(\n                            ptrm_tail[k][0])\n                        x_tail_check.append(\n                            old_div(infields[index_infield][3], NRM))\n                        y_tail_check.append(\n                            old_div(ptrm_tail[k][3], NRM) + old_div(zerofields[index_infield][3], NRM))\n                        tail_check_temperatures.append(ptrm_tail[k][0])\n\n                        break\n                    except:\n                        pass\n\n    x_tail_check = np.array(x_tail_check)\n    y_tail_check = np.array(y_tail_check)\n    tail_check_temperatures = np.array(tail_check_temperatures)\n    x_tail_check_starting_point = np.array(x_tail_check_starting_point)\n    y_tail_check_starting_point = np.array(y_tail_check_starting_point)\n    tail_checks_starting_temperatures = np.array(\n        tail_checks_starting_temperatures)\n\n    # --------------------------------------------------------------\n    # collect the chosen segment in the Arai plot to arrays\n\n    x_Arai_segment = x_Arai[start:end + 1]  # chosen segent in the Arai plot\n    y_Arai_segment = y_Arai[start:end + 1]  # chosen segent in the Arai plot\n\n    # --------------------------------------------------------------\n    # collect pTRM checks in segment to arrays\n    # notice, this is different than the conventional DRATS.\n    # for scat calculation we take only the pTRM checks which were carried out\n    # before reaching the highest temperature in the chosen segment\n\n    x_ptrm_check_for_SCAT, y_ptrm_check_for_SCAT = [], []\n    for k in range(len(ptrm_checks_temperatures)):\n        if ptrm_checks_temperatures[k] >= pars[min_key] and ptrm_checks_starting_temperatures <= pars[max_key]:\n            x_ptrm_check_for_SCAT.append(x_ptrm_check[k])\n            y_ptrm_check_for_SCAT.append(y_ptrm_check[k])\n\n    x_ptrm_check_for_SCAT = np.array(x_ptrm_check_for_SCAT)\n    y_ptrm_check_for_SCAT = np.array(y_ptrm_check_for_SCAT)\n\n    # --------------------------------------------------------------\n    # collect Tail checks in segment to arrays\n    # for scat calculation we take only the tail checks which were carried out\n    # before reaching the highest temperature in the chosen segment\n\n    x_tail_check_for_SCAT, y_tail_check_for_SCAT = [], []\n\n    for k in range(len(tail_check_temperatures)):\n        if tail_check_temperatures[k] >= pars[min_key] and tail_checks_starting_temperatures[k] <= pars[max_key]:\n            x_tail_check_for_SCAT.append(x_tail_check[k])\n            y_tail_check_for_SCAT.append(y_tail_check[k])\n\n    x_tail_check_for_SCAT = np.array(x_tail_check_for_SCAT)\n    y_tail_check_for_SCAT = np.array(y_tail_check_for_SCAT)\n\n    # --------------------------------------------------------------\n    # calculate the lines that define the scat box:\n\n    # if threshold value for beta is not defined, then scat cannot be calculated (pass)\n    # in this case, scat pass\n    if beta_key in list(accept.keys()) and accept[beta_key] != \"\":\n        b_beta_threshold = float(accept[beta_key])\n        b = pars[b_key]             # best fit line\n        cm_x = np.mean(np.array(x_Arai_segment))  # x center of mass\n        cm_y = np.mean(np.array(y_Arai_segment))  # y center of mass\n        a = cm_y - b * cm_x\n\n        # lines with slope = slope +/- 2*(specimen_b_beta)\n\n        two_sigma_beta_threshold = 2 * b_beta_threshold\n        two_sigma_slope_threshold = abs(two_sigma_beta_threshold * b)\n\n        # a line with a  shallower  slope  (b + 2*beta*b) passing through the center of mass\n        # y=a1+b1x\n        b1 = b + two_sigma_slope_threshold\n        a1 = cm_y - b1 * cm_x\n\n        # bounding line with steeper  slope (b - 2*beta*b) passing through the center of mass\n        # y=a2+b2x\n        b2 = b - two_sigma_slope_threshold\n        a2 = cm_y - b2 * cm_x\n\n        # lower bounding line of the 'beta box'\n        # y=intercept1+slop1x\n        slop1 = old_div(a1, ((old_div(a2, b2))))\n        intercept1 = a1\n\n        # higher bounding line of the 'beta box'\n        # y=intercept2+slop2x\n\n        slop2 = old_div(a2, ((old_div(a1, b1))))\n        intercept2 = a2\n\n        pars['specimen_scat_bounding_line_high'] = [intercept2, slop2]\n        pars['specimen_scat_bounding_line_low'] = [intercept1, slop1]\n\n        # --------------------------------------------------------------\n        # check if the Arai data points are in the 'box'\n\n        # the two bounding lines\n        ymin = intercept1 + x_Arai_segment * slop1\n        ymax = intercept2 + x_Arai_segment * slop2\n\n        # arrays of \"True\" or \"False\"\n        check_1 = y_Arai_segment > ymax\n        check_2 = y_Arai_segment < ymin\n\n        # check if at least one \"True\"\n        if (sum(check_1) + sum(check_2)) > 0:\n            pars[\"fail_arai_beta_box_scatter\"] = True\n\n        # --------------------------------------------------------------\n        # check if the pTRM checks data points are in the 'box'\n\n        if len(x_ptrm_check_for_SCAT) > 0:\n\n            # the two bounding lines\n            ymin = intercept1 + x_ptrm_check_for_SCAT * slop1\n            ymax = intercept2 + x_ptrm_check_for_SCAT * slop2\n\n            # arrays of \"True\" or \"False\"\n            check_1 = y_ptrm_check_for_SCAT > ymax\n            check_2 = y_ptrm_check_for_SCAT < ymin\n\n            # check if at least one \"True\"\n            if (sum(check_1) + sum(check_2)) > 0:\n                pars[\"fail_ptrm_beta_box_scatter\"] = True\n\n        # --------------------------------------------------------------\n        # check if the tail checks data points are in the 'box'\n\n        if len(x_tail_check_for_SCAT) > 0:\n\n            # the two bounding lines\n            ymin = intercept1 + x_tail_check_for_SCAT * slop1\n            ymax = intercept2 + x_tail_check_for_SCAT * slop2\n\n            # arrays of \"True\" or \"False\"\n            check_1 = y_tail_check_for_SCAT > ymax\n            check_2 = y_tail_check_for_SCAT < ymin\n\n            # check if at least one \"True\"\n            if (sum(check_1) + sum(check_2)) > 0:\n                pars[\"fail_tail_beta_box_scatter\"] = True\n\n        # --------------------------------------------------------------\n        # check if specimen_scat is PASS or FAIL:\n\n        if pars[\"fail_tail_beta_box_scatter\"] or pars[\"fail_ptrm_beta_box_scatter\"] or pars[\"fail_arai_beta_box_scatter\"]:\n            pars[scat_key] = 'f'\n        else:\n            pars[scat_key] = 't'\n\n    return pars, 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncustomizes by commenting out unwanted keys", "response": "def getkeys(table):\n    \"\"\"\n    customize by commenting out unwanted keys\n    \"\"\"\n    keys = []\n    if table == \"ER_expedition\":\n        pass\n    if table == \"ER_citations\":\n        keys.append(\"er_citation_name\")\n        keys.append(\"long_authors\")\n        keys.append(\"year\")\n        keys.append(\"title\")\n        keys.append(\"citation_type\")\n        keys.append(\"doi\")\n        keys.append(\"journal\")\n        keys.append(\"volume\")\n        keys.append(\"pages\")\n        keys.append(\"book_title\")\n        keys.append(\"book_editors\")\n        keys.append(\"publisher\")\n        keys.append(\"city\")\n    if table == \"ER_locations\":\n        keys.append(\"er_location_name\")\n        keys.append(\"er_scientist_mail_names\")\n#        keys.append(\"er_location_alternatives\" )\n        keys.append(\"location_type\")\n        keys.append(\"location_begin_lat\")\n        keys.append(\"location_begin_lon\")\n#        keys.append(\"location_begin_elevation\" )\n        keys.append(\"location_end_lat\")\n        keys.append(\"location_end_lon\")\n#        keys.append(\"location_end_elevation\" )\n        keys.append(\"continent_ocean\")\n        keys.append(\"country\")\n        keys.append(\"region\")\n        keys.append(\"plate_block\")\n        keys.append(\"terrane\")\n        keys.append(\"tectonic_setting\")\n#        keys.append(\"er_citation_names\")\n    if table == \"ER_Formations\":\n        keys.append(\"er_formation_name\")\n        keys.append(\"formation_class\")\n        keys.append(\"formation_lithology\")\n        keys.append(\"formation_paleo_environment\")\n        keys.append(\"formation_thickness\")\n        keys.append(\"formation_description\")\n    if table == \"ER_sections\":\n        keys.append(\"er_section_name\")\n        keys.append(\"er_section_alternatives\")\n        keys.append(\"er_expedition_name\")\n        keys.append(\"er_location_name\")\n        keys.append(\"er_formation_name\")\n        keys.append(\"er_member_name\")\n        keys.append(\"section_definition\")\n        keys.append(\"section_class\")\n        keys.append(\"section_lithology\")\n        keys.append(\"section_type\")\n        keys.append(\"section_n\")\n        keys.append(\"section_begin_lat\")\n        keys.append(\"section_begin_lon\")\n        keys.append(\"section_begin_elevation\")\n        keys.append(\"section_begin_height\")\n        keys.append(\"section_begin_drill_depth\")\n        keys.append(\"section_begin_composite_depth\")\n        keys.append(\"section_end_lat\")\n        keys.append(\"section_end_lon\")\n        keys.append(\"section_end_elevation\")\n        keys.append(\"section_end_height\")\n        keys.append(\"section_end_drill_depth\")\n        keys.append(\"section_end_composite_depth\")\n        keys.append(\"section_azimuth\")\n        keys.append(\"section_dip\")\n        keys.append(\"section_description\")\n        keys.append(\"er_scientist_mail_names\")\n        keys.append(\"er_citation_names\")\n    if table == \"ER_sites\":\n        keys.append(\"er_location_name\")\n        keys.append(\"er_site_name\")\n#        keys.append(\"er_site_alternatives\")\n#        keys.append(\"er_formation_name\")\n#        keys.append(\"er_member_name\")\n#        keys.append(\"er_section_name\")\n        keys.append(\"er_scientist_mail_names\")\n        keys.append(\"site_class\")\n#        keys.append(\"site_type\")\n#        keys.append(\"site_lithology\")\n#        keys.append(\"site_height\")\n#        keys.append(\"site_drill_depth\")\n#        keys.append(\"site_composite_depth\")\n#        keys.append(\"site_lithology\")\n#        keys.append(\"site_description\")\n        keys.append(\"site_lat\")\n        keys.append(\"site_lon\")\n#        keys.append(\"site_location_precision\")\n#        keys.append(\"site_elevation\")\n    if table == \"ER_samples\":\n        keys.append(\"er_location_name\")\n        keys.append(\"er_site_name\")\n#       keys.append(\"er_sample_alternatives\")\n        keys.append(\"sample_azimuth\")\n        keys.append(\"sample_dip\")\n        keys.append(\"sample_bed_dip\")\n        keys.append(\"sample_bed_dip_direction\")\n#       keys.append(\"sample_cooling_rate\")\n#       keys.append(\"sample_type\")\n#       keys.append(\"sample_lat\")\n#       keys.append(\"sample_lon\")\n        keys.append(\"magic_method_codes\")\n    if table == \"ER_ages\":\n        #       keys.append(\"er_location_name\")\n        #       keys.append(\"er_site_name\")\n        #       keys.append(\"er_section_name\")\n        #       keys.append(\"er_formation_name\")\n        #       keys.append(\"er_member_name\")\n        #       keys.append(\"er_site_name\")\n        #       keys.append(\"er_sample_name\")\n        #       keys.append(\"er_specimen_name\")\n        #       keys.append(\"er_fossil_name\")\n        #       keys.append(\"er_mineral_name\")\n        #       keys.append(\"tiepoint_name\")\n        keys.append(\"age\")\n        keys.append(\"age_sigma\")\n        keys.append(\"age_unit\")\n        keys.append(\"age_range_low\")\n        keys.append(\"age_range_hi\")\n        keys.append(\"timescale_eon\")\n        keys.append(\"timescale_era\")\n        keys.append(\"timescale_period\")\n        keys.append(\"timescale_epoch\")\n        keys.append(\"timescale_stage\")\n        keys.append(\"biostrat_zone\")\n        keys.append(\"conodont_zone\")\n        keys.append(\"magnetic_reversal_chron\")\n        keys.append(\"astronomical_stage\")\n#       keys.append(\"age_description\")\n#       keys.append(\"magic_method_codes\")\n#       keys.append(\"er_timescale_citation_names\")\n#       keys.append(\"er_citation_names\")\n    if table == \"MAGIC_measurements\":\n        keys.append(\"er_location_name\")\n        keys.append(\"er_site_name\")\n        keys.append(\"er_sample_name\")\n        keys.append(\"er_specimen_name\")\n        keys.append(\"measurement_positions\")\n        keys.append(\"treatment_temp\")\n        keys.append(\"treatment_ac_field\")\n        keys.append(\"treatment_dc_field\")\n        keys.append(\"treatment_dc_field_phi\")\n        keys.append(\"treatment_dc_field_theta\")\n        keys.append(\"magic_experiment_name\")\n        keys.append(\"magic_instrument_codes\")\n        keys.append(\"measurement_temp\")\n        keys.append(\"magic_method_codes\")\n        keys.append(\"measurement_inc\")\n        keys.append(\"measurement_dec\")\n        keys.append(\"measurement_magn_moment\")\n        keys.append(\"measurement_csd\")\n    return keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a help message for a give magic keyhelp", "response": "def magic_help(keyhelp):\n    \"\"\"\n    returns a help message for a give magic key\n    \"\"\"\n    helpme = {}\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_location_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"location_type\"] = \"Location type\"\n    helpme[\"location_begin_lat\"] = \"Begin of section or core or outcrop -- latitude\"\n    helpme[\"location_begin_lon\"] = \"Begin of section or core or outcrop -- longitude\"\n    helpme[\"location_begin_elevation\"] = \"Begin of section or core or outcrop -- elevation relative to sealevel\"\n    helpme[\"location_end_lat\"] = \"Ending of section or core -- latitude \"\n    helpme[\"location_end_lon\"] = \"Ending of section or core -- longitude \"\n    helpme[\"location_end_elevation\"] = \"Ending of section or core -- elevation relative to sealevel\"\n    helpme[\"location_geoid\"] = \"Geoid used in determination of latitude and longitude:  WGS84, GEOID03, USGG2003, GEOID99, G99SSS , G99BM, DEFLEC99 \"\n    helpme[\"continent_ocean\"] = \"Name for continent or ocean island region\"\n    helpme[\"ocean_sea\"] = \"Name for location in an ocean or sea\"\n    helpme[\"country\"] = \"Country name\"\n    helpme[\"region\"] = \"Region name\"\n    helpme[\"plate_block\"] = \"Plate or tectonic block name\"\n    helpme[\"terrane\"] = \"Terrane name\"\n    helpme[\"tectonic_setting\"] = \"Tectonic setting\"\n    helpme[\"location_description\"] = \"Detailed description\"\n    helpme[\"location_url\"] = \"Website URL for the location explicitly\"\n    helpme[\"er_scientist_mail_names\"] = \"Colon-delimited list of names for scientists who described location\"\n    helpme[\"er_citation_names\"] = \"Colon-delimited list of citations\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_formation_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"formation_class\"] = \"General lithology class: igneous, metamorphic or sedimentary\"\n    helpme[\"formation_lithology\"] = \"Lithology: e.g., basalt, sandstone, etc.\"\n    helpme[\"formation_paleo_enviroment\"] = \"Depositional environment\"\n    helpme[\"formation_thickness\"] = \"Formation thickness\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"er_member_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"member_class\"] = \"General lithology type\"\n    helpme[\"member_lithology\"] = \"Lithology\"\n    helpme[\"member_paleo_environment\"] = \"Depositional environment\"\n    helpme[\"member_thickness\"] = \"Member thickness\"\n    helpme[\"member_description\"] = \"Detailed description\"\n    helpme[\"er_section_name\"] = \"Name for section or core\"\n    helpme[\"er_section_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"er_expedition_name\"] = \"Name for seagoing or land expedition\"\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"section_definition\"] = \"General definition of section\"\n    helpme[\"section_class\"] = \"General lithology type\"\n    helpme[\"section_lithology\"] = \"Section lithology or archeological classification\"\n    helpme[\"section_type\"] = \"Section type\"\n    helpme[\"section_n\"] = \"Number of subsections included composite (stacked) section\"\n    helpme[\"section_begin_lat\"] = \"Begin of section or core -- latitude\"\n    helpme[\"section_begin_lon\"] = \"Begin of section or core -- longitude\"\n    helpme[\"section_begin_elevation\"] = \"Begin of section or core -- elevation relative to sealevel\"\n    helpme[\"section_begin_height\"] = \"Begin of section or core -- stratigraphic height\"\n    helpme[\"section_begin_drill_depth\"] = \"Begin of section or core -- depth in MBSF as used by ODP\"\n    helpme[\"section_begin_composite_depth\"] = \"Begin of section or core -- composite depth in MBSF as used by ODP\"\n    helpme[\"section_end_lat\"] = \"End of section or core -- latitude \"\n    helpme[\"section_end_lon\"] = \"End of section or core -- longitude \"\n    helpme[\"section_end_elevation\"] = \"End of section or core -- elevation relative to sealevel\"\n    helpme[\"section_end_height\"] = \"End of section or core -- stratigraphic height\"\n    helpme[\"section_end_drill_depth\"] = \"End of section or core -- depth in MBSF as used by ODP\"\n    helpme[\"section_end_composite_depth\"] = \"End of section or core -- composite depth in MBSF as used by ODP\"\n    helpme[\"section_azimuth\"] = \"Section azimuth as measured clockwise from the north\"\n    helpme[\"section_dip\"] = \"Section dip as measured into the outcrop\"\n    helpme[\"section_description\"] = \"Detailed description\"\n    helpme[\"er_site_name\"] = \"Name for site\"\n    helpme[\"er_site_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"er_expedition_name\"] = \"Name for seagoing or land expedition\"\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_section_name\"] = \"Name for section or core\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"site_definition\"] = \"General definition of site\"\n    helpme[\"site_class\"] = \"[A]rchaeologic,[E]xtrusive,[I]ntrusive,[M]etamorphic,[S]edimentary\"\n    helpme[\"site_lithology\"] = \"Site lithology or archeological classification\"\n    helpme[\"site_type\"] = \"Site type: slag, lava flow, sediment layer, etc.\"\n    helpme[\"site_lat\"] = \"Site location -- latitude\"\n    helpme[\"site_lon\"] = \"Site location -- longitude\"\n    helpme[\"site_location_precision\"] = \"Site location -- precision in latitude and longitude\"\n    helpme[\"site_elevation\"] = \"Site location -- elevation relative to sealevel\"\n    helpme[\"site_height\"] = \"Site location -- stratigraphic height\"\n    helpme[\"site_drill_depth\"] = \"Site location -- depth in MBSF as used by ODP\"\n    helpme[\"site_composite_depth\"] = \"Site location -- composite depth in MBSF as used by ODP\"\n    helpme[\"site_description\"] = \"Detailed description\"\n    helpme[\"magic_method_codes\"] = \"Colon-delimited list of method codes\"\n    helpme[\"er_sample_name\"] = \"Name for sample\"\n    helpme[\"er_sample_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"er_expedition_name\"] = \"Name for seagoing or land expedition\"\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_section_name\"] = \"Name for section or core\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"er_site_name\"] = \"Name for site\"\n    helpme[\"sample_class\"] = \"General lithology type\"\n    helpme[\"sample_lithology\"] = \"Sample lithology or archeological classification\"\n    helpme[\"sample_type\"] = \"Sample type\"\n    helpme[\"sample_texture\"] = \"Sample texture\"\n    helpme[\"sample_alteration\"] = \"Sample alteration grade\"\n    helpme[\"sample_alteration_type\"] = \"Sample alteration type\"\n    helpme[\"sample_lat\"] = \"Sample location -- latitude\"\n    helpme[\"sample_lon\"] = \"Sample location -- longitude\"\n    helpme[\"sample_location_precision\"] = \"Sample location -- precision in latitude and longitude\"\n    helpme[\"sample_elevation\"] = \"Sample location -- elevation relative to sealevel\"\n    helpme[\"sample_height\"] = \"Sample location -- stratigraphic height\"\n    helpme[\"sample_drill_depth\"] = \"Sample location -- depth in MBSF as used by ODP\"\n    helpme[\"sample_composite_depth\"] = \"Sample location -- composite depth in MBSF as used by ODP\"\n    helpme[\"sample_date\"] = \"Sampling date\"\n    helpme[\"sample_time_zone\"] = \"Sampling time zone\"\n    helpme[\"sample_azimuth\"] = \"Sample azimuth as measured clockwise from the north\"\n    helpme[\"sample_dip\"] = \"Sample dip as measured into the outcrop\"\n    helpme[\"sample_bed_dip_direction\"] = \"Direction of the dip of a paleo-horizontal plane in the bedding\"\n    helpme[\"sample_bed_dip\"] = \"Dip of the bedding as measured to the right of strike direction\"\n    helpme[\"sample_cooling_rate\"] = \"Estimated ancient in-situ cooling rate per Ma\"\n    helpme[\"er_specimen_name\"] = \"Name for specimen\"\n    helpme[\"er_specimen_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"er_expedition_name\"] = \"Name for seagoing or land expedition\"\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_section_name\"] = \"Name for section or core\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"er_site_name\"] = \"Name for site\"\n    helpme[\"er_sample_name\"] = \"Name for sample\"\n    helpme[\"specimen_class\"] = \"General lithology type\"\n    helpme[\"specimen_lithology\"] = \"Specimen lithology or archeological classification\"\n    helpme[\"specimen_type\"] = \"Specimen type\"\n    helpme[\"specimen_texture\"] = \"Specimen texture\"\n    helpme[\"specimen_alteration\"] = \"Specimen alteration grade\"\n    helpme[\"specimen_alteration_type\"] = \"Specimen alteration type\"\n    helpme[\"specimen_elevation\"] = \"Specimen location -- elevation relative to sealevel\"\n    helpme[\"specimen_height\"] = \"Specimen location -- stratigraphic height\"\n    helpme[\"specimen_drill_depth\"] = \"Specimen location -- depth in MBSF as used by ODP\"\n    helpme[\"specimen_composite_depth\"] = \"Specimen location -- composite depth in MBSF as used by ODP\"\n    helpme[\"specimen_azimuth\"] = \"Specimen azimuth as measured clockwise from the north\"\n    helpme[\"specimen_dip\"] = \"Specimen dip as measured into the outcrop\"\n    helpme[\"specimen_volume\"] = \"Specimen volume\"\n    helpme[\"specimen_weight\"] = \"Specimen weight\"\n    helpme[\"specimen_density\"] = \"Specimen density\"\n    helpme[\"specimen_size\"] = \"Specimen grain size fraction\"\n    helpme[\"er_expedition_name\"] = \"Name for seagoing or land expedition\"\n    helpme[\"er_location_name\"] = \"Name for location or drill site\"\n    helpme[\"er_formation_name\"] = \"Name for formation\"\n    helpme[\"er_member_name\"] = \"Name for member\"\n    helpme[\"er_site_name\"] = \"Name for site\"\n    helpme[\"er_sample_name\"] = \"Name for sample\"\n    helpme[\"er_specimen_name\"] = \"Name for specimen\"\n    helpme[\"er_fossil_name\"] = \"Name for fossil\"\n    helpme[\"er_mineral_name\"] = \"Name for mineral\"\n    helpme[\"GM-ALPHA\"] = \"Age determination by using alpha counting\"\n    helpme[\"GM-ARAR\"] = \"40Ar/39Ar age determination\"\n    helpme[\"GM-ARAR-AP\"] = \"40Ar/39Ar age determination: Age plateau\"\n    helpme[\"GM-ARAR-II\"] = \"40Ar/39Ar age determination: Inverse isochron\"\n    helpme[\"GM-ARAR-NI\"] = \"40Ar/39Ar age determination: Normal isochron\"\n    helpme[\"GM-ARAR-TF\"] = \"40Ar/39Ar age determination: Total fusion or recombined age\"\n    helpme[\"GM-C14\"] = \"Radiocarbon age determination\"\n    helpme[\"GM-C14-AMS\"] = \"Radiocarbon age determination: AMS\"\n    helpme[\"GM-C14-BETA\"] = \"Radiocarbon age determination: Beta decay counting\"\n    helpme[\"GM-C14-CAL\"] = \"Radiocarbon age determination: Calibrated\"\n    helpme[\"GM-CC\"] = \"Correlation chronology\"\n    helpme[\"GM-CC-ARCH\"] = \"Correlation chronology: Archeology\"\n    helpme[\"GM-CC-ARM\"] = \"Correlation chronology: ARM\"\n    helpme[\"GM-CC-ASTRO\"] = \"Correlation chronology: Astronomical\"\n    helpme[\"GM-CC-CACO3\"] = \"Correlation chronology: Calcium carbonate\"\n    helpme[\"GM-CC-COLOR\"] = \"Correlation chronology: Color or reflectance\"\n    helpme[\"GM-CC-GRAPE\"] = \"Correlation chronology: Gamma Ray Polarimeter Experiment\"\n    helpme[\"GM-CC-IRM\"] = \"Correlation chronology: IRM\"\n    helpme[\"GM-CC-ISO\"] = \"Correlation chronology: Stable isotopes\"\n    helpme[\"GM-CC-REL\"] = \"Correlation chronology: Relative chronology other than stratigraphic successions\"\n    helpme[\"GM-CC-STRAT\"] = \"Correlation chronology: Stratigraphic succession\"\n    helpme[\"GM-CC-TECT\"] = \"Correlation chronology: Tectites and microtectites\"\n    helpme[\"GM-CC-TEPH\"] = \"Correlation chronology: Tephrochronology\"\n    helpme[\"GM-CC-X\"] = \"Correlation chronology: Susceptibility\"\n    helpme[\"GM-CHEM\"] = \"Chemical chronology\"\n    helpme[\"GM-CHEM-AAR\"] = \"Chemical chronology: Amino acid racemization\"\n    helpme[\"GM-CHEM-OH\"] = \"Chemical chronology: Obsidian hydration\"\n    helpme[\"GM-CHEM-SC\"] = \"Chemical chronology: Stoan coatings CaCO3\"\n    helpme[\"GM-CHEM-TH\"] = \"Chemical chronology: Tephra hydration\"\n    helpme[\"GM-COSMO\"] = \"Cosmogenic age determination\"\n    helpme[\"GM-COSMO-AL26\"] = \"Cosmogenic age determination: 26Al\"\n    helpme[\"GM-COSMO-AR39\"] = \"Cosmogenic age determination: 39Ar\"\n    helpme[\"GM-COSMO-BE10\"] = \"Cosmogenic age determination: 10Be\"\n    helpme[\"GM-COSMO-C14\"] = \"Cosmogenic age determination: 14C\"\n    helpme[\"GM-COSMO-CL36\"] = \"Cosmogenic age determination: 36Cl\"\n    helpme[\"GM-COSMO-HE3\"] = \"Cosmogenic age determination: 3He\"\n    helpme[\"GM-COSMO-KR81\"] = \"Cosmogenic age determination: 81Kr\"\n    helpme[\"GM-COSMO-NE21\"] = \"Cosmogenic age determination: 21Ne\"\n    helpme[\"GM-COSMO-NI59\"] = \"Cosmogenic age determination: 59Ni\"\n    helpme[\"GM-COSMO-SI32\"] = \"Cosmogenic age determination: 32Si\"\n    helpme[\"GM-DENDRO\"] = \"Dendrochronology\"\n    helpme[\"GM-ESR\"] = \"Electron Spin Resonance\"\n    helpme[\"GM-FOSSIL\"] = \"Age determined from fossil record\"\n    helpme[\"GM-FT\"] = \"Fission track age determination\"\n    helpme[\"GM-HIST\"] = \"Historically recorded geological event\"\n    helpme[\"GM-INT\"] = \"Age determination through interpolation between at least two geological units of known age\"\n    helpme[\"GM-INT-L\"] = \"Age determination through interpolation between at least two geological units of known age: Linear\"\n    helpme[\"GM-INT-S\"] = \"Age determination through interpolation between at least two geological units of known age: Cubic spline\"\n    helpme[\"GM-ISO\"] = \"Age determined by isotopic dating, but no further details available\"\n    helpme[\"GM-KAR\"] = \"40K-40Ar age determination\"\n    helpme[\"GM-KAR-I\"] = \"40K-40Ar age determination: Isochron\"\n    helpme[\"GM-KAR-MA\"] = \"40K-40Ar age determination: Model age\"\n    helpme[\"GM-KCA\"] = \"40K-40Ca age determination\"\n    helpme[\"GM-KCA-I\"] = \"40K-40Ca age determination: Isochron\"\n    helpme[\"GM-KCA-MA\"] = \"40K-40Ca age determination: Model age\"\n    helpme[\"GM-LABA\"] = \"138La-138Ba age determination\"\n    helpme[\"GM-LABA-I\"] = \"138La-138Ba age determination: Isochron\"\n    helpme[\"GM-LABA-MA\"] = \"138La-138Ba age determination: Model age\"\n    helpme[\"GM-LACE\"] = \"138La-138Ce age determination\"\n    helpme[\"GM-LACE-I\"] = \"138La-138Ce age determination: Isochron\"\n    helpme[\"GM-LACE-MA\"] = \"138La-138Ce age determination: Model age\"\n    helpme[\"GM-LICHE\"] = \"Lichenometry\"\n    helpme[\"GM-LUHF\"] = \"176Lu-176Hf age determination\"\n    helpme[\"GM-LUHF-I\"] = \"176Lu-176Hf age determination: Isochron\"\n    helpme[\"GM-LUHF-MA\"] = \"176Lu-176Hf age determination: Model age\"\n    helpme[\"GM-LUM\"] = \"Luminescence\"\n    helpme[\"GM-LUM-IRS\"] = \"Luminescence: Infrared stimulated luminescence\"\n    helpme[\"GM-LUM-OS\"] = \"Luminescence: Optically stimulated luminescence\"\n    helpme[\"GM-LUM-TH\"] = \"Luminescence: Thermoluminescence\"\n    helpme[\"GM-MOD\"] = \"Model curve fit to available age dates\"\n    helpme[\"GM-MOD-L\"] = \"Model curve fit to available age dates: Linear\"\n    helpme[\"GM-MOD-S\"] = \"Model curve fit to available age dates: Cubic spline\"\n    helpme[\"GM-MORPH\"] = \"Geomorphic chronology\"\n    helpme[\"GM-MORPH-DEF\"] = \"Geomorphic chronology: Rate of deformation\"\n    helpme[\"GM-MORPH-DEP\"] = \"Geomorphic chronology: Rate of deposition\"\n    helpme[\"GM-MORPH-POS\"] = \"Geomorphic chronology: Geomorphology position\"\n    helpme[\"GM-MORPH-WEATH\"] = \"Geomorphic chronology: Rock and mineral weathering\"\n    helpme[\"GM-NO\"] = \"Unknown geochronology method\"\n    helpme[\"GM-O18\"] = \"Oxygen isotope dating\"\n    helpme[\"GM-PBPB\"] = \"207Pb-206Pb age determination\"\n    helpme[\"GM-PBPB-C\"] = \"207Pb-206Pb age determination: Common Pb\"\n    helpme[\"GM-PBPB-I\"] = \"207Pb-206Pb age determination: Isochron\"\n    helpme[\"GM-PLEO\"] = \"Pleochroic haloes\"\n    helpme[\"GM-PMAG-ANOM\"] = \"Paleomagnetic age determination: Magnetic anomaly identification\"\n    helpme[\"GM-PMAG-APWP\"] = \"Paleomagnetic age determination: Comparing paleomagnetic data to APWP\"\n    helpme[\"GM-PMAG-ARCH\"] = \"Paleomagnetic age determination: Archeomagnetism\"\n    helpme[\"GM-PMAG-DIR\"] = \"Paleomagnetic age determination: Directions\"\n    helpme[\"GM-PMAG-POL\"] = \"Paleomagnetic age determination: Polarities\"\n    helpme[\"GM-PMAG-REGSV\"] = \"Paleomagnetic age determination: Correlation to a regional secular variation curve\"\n    helpme[\"GM-PMAG-RPI\"] = \"Paleomagnetic age determination: Relative paleointensity\"\n    helpme[\"GM-PMAG-VEC\"] = \"Paleomagnetic age determination: Full vector\"\n    helpme[\"GM-RATH\"] = \"226Ra-230Th age determination\"\n    helpme[\"GM-RBSR\"] = \"87Rb-87Sr age determination\"\n    helpme[\"GM-RBSR-I\"] = \"87Rb-87Sr age determination: Isochron\"\n    helpme[\"GM-RBSR-MA\"] = \"87Rb-87Sr age determination: Model age\"\n    helpme[\"GM-REOS\"] = \"187Re-187Os age determination\"\n    helpme[\"GM-REOS-I\"] = \"187Re-187Os age determination: Isochron\"\n    helpme[\"GM-REOS-MA\"] = \"187Re-187Os age determination: Model age\"\n    helpme[\"GM-REOS-PT\"] = \"187Re-187Os age determination: Pt normalization of 186Os\"\n    helpme[\"GM-SCLERO\"] = \"Screlochronology\"\n    helpme[\"GM-SHRIMP\"] = \"SHRIMP age dating\"\n    helpme[\"GM-SMND\"] = \"147Sm-143Nd age determination\"\n    helpme[\"GM-SMND-I\"] = \"147Sm-143Nd age determination: Isochron\"\n    helpme[\"GM-SMND-MA\"] = \"147Sm-143Nd age determination: Model age\"\n    helpme[\"GM-THPB\"] = \"232Th-208Pb age determination\"\n    helpme[\"GM-THPB-I\"] = \"232Th-208Pb age determination: Isochron\"\n    helpme[\"GM-THPB-MA\"] = \"232Th-208Pb age determination: Model age\"\n    helpme[\"GM-UPA\"] = \"235U-231Pa age determination\"\n    helpme[\"GM-UPB\"] = \"U-Pb age determination\"\n    helpme[\"GM-UPB-CC-T0\"] = \"U-Pb age determination: Concordia diagram age, upper intersection\"\n    helpme[\"GM-UPB-CC-T1\"] = \"U-Pb age determination: Concordia diagram age, lower intersection\"\n    helpme[\"GM-UPB-I-206\"] = \"U-Pb age determination: 238U-206Pb isochron\"\n    helpme[\"GM-UPB-I-207\"] = \"U-Pb age determination: 235U-207Pb isochron\"\n    helpme[\"GM-UPB-MA-206\"] = \"U-Pb age determination: 238U-206Pb model age\"\n    helpme[\"GM-UPB-MA-207\"] = \"U-Pb age determination: 235U-207Pb model age\"\n    helpme[\"GM-USD\"] = \"Uranium series disequilibrium age determination\"\n    helpme[\"GM-USD-PA231-TH230\"] = \"Uranium series disequilibrium age determination: 231Pa-230Th\"\n    helpme[\"GM-USD-PA231-U235\"] = \"Uranium series disequilibrium age determination: 231Pa-235U\"\n    helpme[\"GM-USD-PB210\"] = \"Uranium series disequilibrium age determination: 210Pb\"\n    helpme[\"GM-USD-RA226-TH230\"] = \"Uranium series disequilibrium age determination: 226Ra-230Th\"\n    helpme[\"GM-USD-RA228-TH232\"] = \"Uranium series disequilibrium age determination: 228Ra-232Th\"\n    helpme[\"GM-USD-TH228-TH232\"] = \"Uranium series disequilibrium age determination: 228Th-232Th\"\n    helpme[\"GM-USD-TH230\"] = \"Uranium series disequilibrium age determination: 230Th\"\n    helpme[\"GM-USD-TH230-TH232\"] = \"Uranium series disequilibrium age determination: 230Th-232Th\"\n    helpme[\"GM-USD-TH230-U234\"] = \"Uranium series disequilibrium age determination: 230Th-234U\"\n    helpme[\"GM-USD-TH230-U238\"] = \"Uranium series disequilibrium age determination: 230Th-238U\"\n    helpme[\"GM-USD-U234-U238\"] = \"Uranium series disequilibrium age determination: 234U-238U\"\n    helpme[\"GM-UTH\"] = \"238U-230Th age determination\"\n    helpme[\"GM-UTHHE\"] = \"U-Th-He age determination\"\n    helpme[\"GM-UTHPB\"] = \"U-Th-Pb age determination\"\n    helpme[\"GM-UTHPB-CC-T0\"] = \"U-Th-Pb age determination: Concordia diagram intersection age, upper intercept\"\n    helpme[\"GM-UTHPB-CC-T1\"] = \"U-Th-Pb age determination: Concordia diagram intersection age, lower intercept\"\n    helpme[\"GM-VARVE\"] = \"Age determined by varve counting\"\n    helpme[\"tiepoint_name\"] = \"Name for tiepoint horizon\"\n    helpme[\"tiepoint_alternatives\"] = \"Colon-delimited list of alternative names and abbreviations\"\n    helpme[\"tiepoint_height\"] = \"Tiepoint stratigraphic height relative to reference tiepoint\"\n    helpme[\"tiepoint_height_sigma\"] = \"Tiepoint stratigraphic height uncertainty\"\n    helpme[\"tiepoint_elevation\"] = \"Tiepoint elevation relative to sealevel\"\n    helpme[\"tiepoint_type\"] = \"Tiepoint type\"\n    helpme[\"age\"] = \"Age\"\n    helpme[\"age_sigma\"] = \"Age -- uncertainty\"\n    helpme[\"age_range_low\"] = \"Age -- low range\"\n    helpme[\"age_range_high\"] = \"Age -- high range\"\n    helpme[\"age_unit\"] = \"Age -- unit\"\n    helpme[\"timescale_eon\"] = \"Timescale eon\"\n    helpme[\"timescale_era\"] = \"Timescale era\"\n    helpme[\"timescale_period\"] = \"Timescale period\"\n    helpme[\"timescale_epoch\"] = \"Timescale epoch\"\n    helpme[\"timescale_stage\"] = \"Timescale stage\"\n    helpme[\"biostrat_zone\"] = \"Biostratigraphic zone\"\n    helpme[\"conodont_zone\"] = \"Conodont zone\"\n    helpme[\"magnetic_reversal_chron\"] = \"Magnetic reversal chron\"\n    helpme[\"astronomical_stage\"] = \"Astronomical stage name\"\n    helpme[\"oxygen_stage\"] = \"Oxygen stage name\"\n    helpme[\"age_culture_name\"] = \"Age culture name\"\n    return helpme[keyhelp]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the declination for a given set of suncompass data Parameters __________ sundata : dictionary with these keys: date: time string with the format 'yyyy:mm:dd:hr:min' delta_u: time to SUBTRACT from local time for Universal time lat: latitude of location (negative for south) lon: longitude of location (negative for west) shadow_angle: shadow angle of the desired direction with respect to the sun. Returns ________ sunaz : the declination of the desired direction wrt true north.", "response": "def dosundec(sundata):\n    \"\"\"\n    returns the declination for a given set of suncompass data\n    Parameters\n    __________\n      sundata : dictionary with these keys:\n          date: time string with the format 'yyyy:mm:dd:hr:min'\n          delta_u: time to SUBTRACT from local time for Universal time\n          lat: latitude of location (negative for south)\n          lon: longitude of location (negative for west)\n          shadow_angle: shadow angle of the desired direction with respect to the sun.\n    Returns\n    ________\n       sunaz : the declination of the desired direction wrt true north.\n    \"\"\"\n    iday = 0\n    timedate = sundata[\"date\"]\n    timedate = timedate.split(\":\")\n    year = int(timedate[0])\n    mon = int(timedate[1])\n    day = int(timedate[2])\n    hours = float(timedate[3])\n    min = float(timedate[4])\n    du = int(sundata[\"delta_u\"])\n    hrs = hours - du\n    if hrs > 24:\n        day += 1\n        hrs = hrs - 24\n    if hrs < 0:\n        day = day - 1\n        hrs = hrs + 24\n    julian_day = julian(mon, day, year)\n    utd = old_div((hrs + old_div(min, 60.)), 24.)\n    greenwich_hour_angle, delta = gha(julian_day, utd)\n    H = greenwich_hour_angle + float(sundata[\"lon\"])\n    if H > 360:\n        H = H - 360\n    lat = float(sundata[\"lat\"])\n    if H > 90 and H < 270:\n        lat = -lat\n# now do spherical trig to get azimuth to sun\n    lat = np.radians(lat)\n    delta = np.radians(delta)\n    H = np.radians(H)\n    ctheta = np.sin(lat) * np.sin(delta) + np.cos(lat) * \\\n        np.cos(delta) * np.cos(H)\n    theta = np.arccos(ctheta)\n    beta = np.cos(delta) * np.sin(H) / np.sin(theta)\n#\n#       check which beta\n#\n    beta = np.degrees(np.arcsin(beta))\n    if delta < lat:\n        beta = 180 - beta\n    sunaz = 180 - beta\n    sunaz = (sunaz + float(sundata[\"shadow_angle\"])) % 360.  # mod 360\n    return sunaz"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns greenwich hour angle", "response": "def gha(julian_day, f):\n    \"\"\"\n    returns greenwich hour angle\n    \"\"\"\n    rad = old_div(np.pi, 180.)\n    d = julian_day - 2451545.0 + f\n    L = 280.460 + 0.9856474 * d\n    g = 357.528 + 0.9856003 * d\n    L = L % 360.\n    g = g % 360.\n# ecliptic longitude\n    lamb = L + 1.915 * np.sin(g * rad) + .02 * np.sin(2 * g * rad)\n# obliquity of ecliptic\n    epsilon = 23.439 - 0.0000004 * d\n# right ascension (in same quadrant as lambda)\n    t = (np.tan(old_div((epsilon * rad), 2)))**2\n    r = old_div(1, rad)\n    rl = lamb * rad\n    alpha = lamb - r * t * np.sin(2 * rl) + \\\n        (old_div(r, 2)) * t * t * np.sin(4 * rl)\n#       alpha=mod(alpha,360.0)\n# declination\n    delta = np.sin(epsilon * rad) * np.sin(lamb * rad)\n    delta = old_div(np.arcsin(delta), rad)\n# equation of time\n    eqt = (L - alpha)\n#\n    utm = f * 24 * 60\n    H = old_div(utm, 4) + eqt + 180\n    H = H % 360.0\n    return H, delta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling the keys of dictionaries within Recs", "response": "def fillkeys(Recs):\n    \"\"\"\n    reconciles keys of dictionaries within Recs.\n    \"\"\"\n    keylist, OutRecs = [], []\n    for rec in Recs:\n        for key in list(rec.keys()):\n            if key not in keylist:\n                keylist.append(key)\n    for rec in Recs:\n        for key in keylist:\n            if key not in list(rec.keys()):\n                rec[key] = \"\"\n        OutRecs.append(rec)\n    return OutRecs, keylist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the Fisher mean and associated parameter from a nested list of data points.", "response": "def fisher_mean(data):\n    \"\"\"\n    Calculates the Fisher mean and associated parameter from a di_block\n\n    Parameters\n    ----------\n    di_block : a nested list of [dec,inc] or [dec,inc,intensity]\n\n    Returns\n    -------\n    fpars : dictionary containing the Fisher mean and statistics\n        dec : mean declination\n        inc : mean inclination\n        r : resultant vector length\n        n : number of data points\n        k : Fisher k value\n        csd : Fisher circular standard deviation\n        alpha95 : Fisher circle of 95% confidence\n    \"\"\"\n    R, Xbar, X, fpars = 0, [0, 0, 0], [], {}\n    N = len(data)\n    if N < 2:\n        return fpars\n    X = dir2cart(data)\n    for i in range(len(X)):\n        for c in range(3):\n            Xbar[c] += X[i][c]\n    for c in range(3):\n        R += Xbar[c]**2\n    R = np.sqrt(R)\n    for c in range(3):\n        Xbar[c] = Xbar[c]/R\n    dir = cart2dir(Xbar)\n    fpars[\"dec\"] = dir[0]\n    fpars[\"inc\"] = dir[1]\n    fpars[\"n\"] = N\n    fpars[\"r\"] = R\n    if N != R:\n        k = (N - 1.) / (N - R)\n        fpars[\"k\"] = k\n        csd = 81./np.sqrt(k)\n    else:\n        fpars['k'] = 'inf'\n        csd = 0.\n    b = 20.**(1./(N - 1.)) - 1\n    a = 1 - b * (N - R) / R\n    if a < -1:\n        a = -1\n    a95 = np.degrees(np.arccos(a))\n    fpars[\"alpha95\"] = a95\n    fpars[\"csd\"] = csd\n    if a < 0:\n        fpars[\"alpha95\"] = 180.0\n    return fpars"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate gaussian statistics for data", "response": "def gausspars(data):\n    \"\"\"\n    calculates gaussian statistics for data\n    \"\"\"\n    N, mean, d = len(data), 0., 0.\n    if N < 1:\n        return \"\", \"\"\n    if N == 1:\n        return data[0], 0\n    for j in range(N):\n        mean += old_div(data[j], float(N))\n    for j in range(N):\n        d += (data[j] - mean)**2\n    stdev = np.sqrt(d * (1./(float(N - 1))))\n    return mean, stdev"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef weighted_mean(data):\n    W, N, mean, d = 0, len(data), 0, 0\n    if N < 1:\n        return \"\", \"\"\n    if N == 1:\n        return data[0][0], 0\n    for x in data:\n        W += x[1]  # sum of the weights\n    for x in data:\n        mean += old_div((float(x[1]) * float(x[0])), float(W))\n    for x in data:\n        d += (old_div(float(x[1]), float(W))) * (float(x[0]) - mean)**2\n    stdev = np.sqrt(d * (old_div(1., (float(N - 1)))))\n    return mean, stdev", "response": "Calculates weighted mean of data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to calculate fisher mean of a single set of data in a single set of polarity domains", "response": "def fisher_by_pol(data):\n    \"\"\"\n    input:    as in dolnp (list of dictionaries with 'dec' and 'inc')\n    description: do fisher mean after splitting data into two polarity domains.\n    output: three dictionaries:\n        'A'= polarity 'A'\n        'B = polarity 'B'\n        'ALL'= switching polarity of 'B' directions, and calculate fisher mean of all data\n    code modified from eqarea_ell.py b rshaar 1/23/2014\n    \"\"\"\n    FisherByPoles = {}\n    DIblock, nameblock, locblock = [], [], []\n    for rec in data:\n        if 'dec' in list(rec.keys()) and 'inc' in list(rec.keys()):\n            # collect data for fisher calculation\n            DIblock.append([float(rec[\"dec\"]), float(rec[\"inc\"])])\n        else:\n            continue\n        if 'name' in list(rec.keys()):\n            nameblock.append(rec['name'])\n        else:\n            nameblock.append(\"\")\n        if 'loc' in list(rec.keys()):\n            locblock.append(rec['loc'])\n        else:\n            locblock.append(\"\")\n\n    ppars = doprinc(np.array(DIblock))  # get principal directions\n    # choose the northerly declination principe component (\"normal\")\n    reference_DI = [ppars['dec'], ppars['inc']]\n    # make reference direction in northern hemisphere\n    if reference_DI[0] > 90 and reference_DI[0] < 270:\n        reference_DI[0] = (reference_DI[0] + 180.) % 360\n        reference_DI[1] = reference_DI[1] * -1.\n    nDIs, rDIs, all_DI, npars, rpars = [], [], [], [], []\n    nlist, rlist, alllist = \"\", \"\", \"\"\n    nloclist, rloclist, allloclist = \"\", \"\", \"\"\n    for k in range(len(DIblock)):\n        if angle([DIblock[k][0], DIblock[k][1]], reference_DI) > 90.:\n            rDIs.append(DIblock[k])\n            rlist = rlist + \":\" + nameblock[k]\n            if locblock[k] not in rloclist:\n                rloclist = rloclist + \":\" + locblock[k]\n            all_DI.append([(DIblock[k][0] + 180.) % 360., -1. * DIblock[k][1]])\n            alllist = alllist + \":\" + nameblock[k]\n            if locblock[k] not in allloclist:\n                allloclist = allloclist + \":\" + locblock[k]\n        else:\n            nDIs.append(DIblock[k])\n            nlist = nlist + \":\" + nameblock[k]\n            if locblock[k] not in nloclist:\n                nloclist = nloclist + \":\" + locblock[k]\n            all_DI.append(DIblock[k])\n            alllist = alllist + \":\" + nameblock[k]\n            if locblock[k] not in allloclist:\n                allloclist = allloclist + \":\" + locblock[k]\n\n    for mode in ['A', 'B', 'All']:\n        if mode == 'A' and len(nDIs) > 2:\n            fpars = fisher_mean(nDIs)\n            fpars['sites'] = nlist.strip(':')\n            fpars['locs'] = nloclist.strip(':')\n            FisherByPoles[mode] = fpars\n        elif mode == 'B' and len(rDIs) > 2:\n            fpars = fisher_mean(rDIs)\n            fpars['sites'] = rlist.strip(':')\n            fpars['locs'] = rloclist.strip(':')\n            FisherByPoles[mode] = fpars\n        elif mode == 'All' and len(all_DI) > 2:\n            fpars = fisher_mean(all_DI)\n            fpars['sites'] = alllist.strip(':')\n            fpars['locs'] = allloclist.strip(':')\n            FisherByPoles[mode] = fpars\n    return FisherByPoles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dolnp3_0(Data):\n    if len(Data) == 0:\n        print(\"This function requires input Data have at least 1 entry\")\n        return {}\n    if len(Data) == 1:\n        ReturnData = {}\n        ReturnData[\"dec\"] = Data[0]['dir_dec']\n        ReturnData[\"inc\"] = Data[0]['dir_inc']\n        ReturnData[\"n_total\"] = '1'\n        if \"DE-BFP\" in Data[0]['method_codes']:\n            ReturnData[\"n_lines\"] = '0'\n            ReturnData[\"n_planes\"] = '1'\n        else:\n            ReturnData[\"n_planes\"] = '0'\n            ReturnData[\"n_lines\"] = '1'\n        ReturnData[\"alpha95\"] = \"\"\n        ReturnData[\"R\"] = \"\"\n        ReturnData[\"K\"] = \"\"\n        return ReturnData\n    else:\n        LnpData = []\n        for n, d in enumerate(Data):\n            LnpData.append({})\n            LnpData[n]['dec'] = d['dir_dec']\n            LnpData[n]['inc'] = d['dir_inc']\n            LnpData[n]['tilt_correction'] = d['dir_tilt_correction']\n            if 'method_codes' in list(d.keys()):\n                if \"DE-BFP\" in d['method_codes']:\n                    LnpData[n]['dir_type'] = 'p'\n                else:\n                    LnpData[n]['dir_type'] = 'l'\n        # get a sample average from all specimens\n        ReturnData = dolnp(LnpData, 'dir_type')\n        return ReturnData", "response": "This function takes a list of dicts with the controlled vocabulary of 3_0 and calls dolnp on them."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns fisher mean, a95 for data using method of Mcfadden and Mcelhinny '88 for lines and planes Parameters __________ Data : nested list of dictionaries with keys Data model 3.0: dir_dec dir_inc dir_tilt_correction method_codes Data model 2.5: dec inc tilt_correction magic_method_codes direction_type_key : ['specimen_direction_type'] Returns ------- ReturnData : dictionary with keys dec : fisher mean dec of data in Data inc : fisher mean inc of data in Data n_lines : number of directed lines [method_code = DE-BFL or DE-FM] n_planes : number of best fit planes [method_code = DE-BFP] alpha95 : fisher confidence circle from Data R : fisher R value of Data K : fisher k value of Data Effects prints to screen in case of no data", "response": "def dolnp(data, direction_type_key):\n    \"\"\"\n    Returns fisher mean, a95 for data  using method of Mcfadden and Mcelhinny '88 for lines and planes\n\n    Parameters\n    __________\n    Data : nested list of dictionaries with keys\n        Data model 3.0:\n            dir_dec\n            dir_inc\n            dir_tilt_correction\n            method_codes\n        Data model 2.5:\n            dec\n            inc\n            tilt_correction\n            magic_method_codes\n         direction_type_key :  ['specimen_direction_type']\n    Returns\n    -------\n        ReturnData : dictionary with keys\n            dec : fisher mean dec of data in Data\n            inc : fisher mean inc of data in Data\n            n_lines : number of directed lines [method_code = DE-BFL or DE-FM]\n            n_planes : number of best fit planes [method_code = DE-BFP]\n            alpha95  : fisher confidence circle from Data\n            R : fisher R value of Data\n            K : fisher k value of Data\n    Effects\n        prints to screen in case of no data\n    \"\"\"\n\n    if 'dir_dec' in data[0].keys():\n        tilt_key = 'dir_tilt_correction'  # this is data model 3.0\n    else:\n        tilt_key = 'tilt_correction'  # this is data model 3.0\n    if tilt_key in list(data[0].keys()):\n        tc = str(data[0][tilt_key])\n    else:\n        tc = '-1'\n    dirV = [0, 0, 0]\n    fpars = {}\n\n    # sort data  into lines and planes and collect cartesian coordinates\n    fdata, n_lines, L, n_planes, E = process_data_for_mean(\n        data, direction_type_key)\n# set up initial points on the great circles\n    V, XV = [], []\n    if n_planes != 0:\n        if n_lines == 0:\n            # set the initial direction arbitrarily\n            V = dir2cart([180., -45., 1.])\n        else:\n            R = np.sqrt(E[0]**2 + E[1]**2 + E[2]**2)\n            for c in E:\n                # set initial direction as mean of lines\n                V.append(old_div(c, R))\n        XV = calculate_best_fit_vectors(L, E, V, n_planes)\n# calculating overall mean direction and R\n        U = E[:]\n        for dir in XV:\n            for c in range(3):\n                U[c] = U[c] + dir[c]\n        R = np.sqrt(U[0]**2 + U[1]**2 + U[2]**2)\n        for c in range(3):\n            U[c] = old_div(U[c], R)\n# get dec and inc of solution points on gt circles\n        dirV = cart2dir(U)\n# calculate modified Fisher stats fo fit\n        n_total = n_lines + n_planes\n        NP = n_lines + 0.5 * n_planes\n        if NP < 1.1:\n            NP = 1.1\n        if n_total - R != 0:\n            K = old_div((NP - 1.), (n_total - R))\n            fac = (20.**(old_div(1., (NP - 1.))) - 1.)\n            fac = fac * (NP - 1.) / K\n            a = 1. - old_div(fac, R)\n            a95 = a\n            if abs(a) > 1.0:\n                a95 = 1.\n            if a < 0:\n                a95 = -a95\n            a95 = np.arccos(a95) * 180. / np.pi\n        else:\n            a95 = 0.\n            K = 'inf'\n    else:\n        fdir = fisher_mean(fdata)\n        n_total, R, K, a95 = fdir[\"n\"], fdir[\"r\"], fdir[\"k\"], fdir[\"alpha95\"]\n        dirV[0], dirV[1] = fdir[\"dec\"], fdir[\"inc\"]\n    fpars[\"tilt_correction\"] = tc\n    fpars[\"n_total\"] = '%i ' % (n_total)\n    fpars[\"n_lines\"] = '%i ' % (n_lines)\n    fpars[\"n_planes\"] = '%i ' % (n_planes)\n    fpars[\"R\"] = '%5.4f ' % (R)\n    if K != 'inf':\n        fpars[\"K\"] = '%6.0f ' % (K)\n    else:\n        fpars[\"K\"] = K\n    fpars[\"alpha95\"] = '%7.1f ' % (a95)\n    fpars[\"dec\"] = '%7.1f ' % (dirV[0])\n    fpars[\"inc\"] = '%7.1f ' % (dirV[1])\n    return fpars"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vclose(L, V):\n    lam, X = 0, []\n    for k in range(3):\n        lam = lam + V[k] * L[k]\n    beta = np.sqrt(1. - lam**2)\n    for k in range(3):\n        X.append((old_div((V[k] - lam * L[k]), beta)))\n    return X", "response": "returns the closest vector in the tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the best fit vectors for a set of plane interpretations used in fisher mean calculations.", "response": "def calculate_best_fit_vectors(L, E, V, n_planes):\n    \"\"\"\n    Calculates the best fit vectors for a set of plane interpretations used in fisher mean calculations\n    @param: L - a list of the \"EL, EM, EN\" array of MM88 or the cartisian form of dec and inc of the plane interpretation\n    @param: E - the sum of the cartisian coordinates of all the line fits to be used in the mean\n    @param: V - inital direction to start iterating from to get plane best fits\n    @returns: nested list of n_plane by 3 dimension where the 3 are the cartisian dimension of the best fit vector\n    \"\"\"\n\n    U, XV = E[:], []  # make a copy of E to prevent mutation\n    for pole in L:\n        XV.append(vclose(pole, V))  # get some points on the great circle\n        for c in range(3):\n            U[c] = U[c] + XV[-1][c]\n# iterate to find best agreement\n    angle_tol = 1.\n    while angle_tol > 0.1:\n        angles = []\n        for k in range(n_planes):\n            for c in range(3):\n                U[c] = U[c] - XV[k][c]\n            R = np.sqrt(U[0]**2 + U[1]**2 + U[2]**2)\n            for c in range(3):\n                V[c] = old_div(U[c], R)\n            XX = vclose(L[k], V)\n            ang = XX[0] * XV[k][0] + XX[1] * XV[k][1] + XX[2] * XV[k][2]\n            angles.append(np.arccos(ang) * 180. / np.pi)\n            for c in range(3):\n                XV[k][c] = XX[c]\n                U[c] = U[c] + XX[c]\n            amax = -1\n            for ang in angles:\n                if ang > amax:\n                    amax = ang\n            angle_tol = amax\n\n    return XV"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_data_for_mean(data, direction_type_key):\n    dec_key, inc_key, meth_key = 'dec', 'inc', 'magic_method_codes'  # data model 2.5\n    if 'dir_dec' in data[0].keys():  # this is data model 3.0\n        dec_key, inc_key, meth_key = 'dir_dec', 'dir_inc', 'method_codes'\n\n    n_lines, n_planes = 0, 0\n    L, fdata = [], []\n    E = [0, 0, 0]\n\n    # sort data  into lines and planes and collect cartesian coordinates\n    for rec in data:\n        cart = dir2cart([float(rec[dec_key]), float(rec[inc_key])])[0]\n        if direction_type_key in list(rec.keys()):\n            if rec[direction_type_key] == 'p':  # this is a pole to a plane\n                n_planes += 1\n                L.append(cart)  # this is the \"EL, EM, EN\" array of MM88\n            else:  # this is a line\n                n_lines += 1\n                # collect data for fisher calculation\n                fdata.append([float(rec[dec_key]), float(rec[inc_key]), 1.])\n                E[0] += cart[0]\n                E[1] += cart[1]\n                E[2] += cart[2]\n        elif 'method_codes' in list(rec.keys()):\n            if \"DE-BFP\" in rec[meth_key]:  # this is a pole to a plane\n                n_planes += 1\n                L.append(cart)  # this is the \"EL, EM, EN\" array of MM88\n            else:  # this is a line\n                n_lines += 1\n                # collect data for fisher calculation\n                fdata.append([rec[dec_key], rec[inc_key], 1.])\n                E[0] += cart[0]\n                E[1] += cart[1]\n                E[2] += cart[2]\n        elif meth_key in list(rec.keys()):\n            if \"DE-BFP\" in rec[meth_key]:  # this is a pole to a plane\n                n_planes += 1\n                L.append(cart)  # this is the \"EL, EM, EN\" array of MM88\n            else:  # this is a line\n                n_lines += 1\n                # collect data for fisher calculation\n                fdata.append([rec[dec_key], rec[inc_key], 1.])\n                E[0] += cart[0]\n                E[1] += cart[1]\n                E[2] += cart[2]\n        else:\n                # EVERYTHING IS A LINE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n            n_lines += 1\n            # collect data for fisher calculation\n            fdata.append([rec[dec_key], rec[inc_key], 1.])\n            E[0] += cart[0]\n            E[1] += cart[1]\n            E[2] += cart[2]\n\n    return fdata, n_lines, L, n_planes, E", "response": "process data for fisher means"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scoreit(pars, PmagSpecRec, accept, text, verbose):\n    s = PmagSpecRec[\"er_specimen_name\"]\n    PmagSpecRec[\"measurement_step_min\"] = '%8.3e' % (\n        pars[\"measurement_step_min\"])\n    PmagSpecRec[\"measurement_step_max\"] = '%8.3e' % (\n        pars[\"measurement_step_max\"])\n    PmagSpecRec[\"measurement_step_unit\"] = pars[\"measurement_step_unit\"]\n    PmagSpecRec[\"specimen_int_n\"] = '%i' % (pars[\"specimen_int_n\"])\n    PmagSpecRec[\"specimen_lab_field_dc\"] = '%8.3e' % (\n        pars[\"specimen_lab_field_dc\"])\n    PmagSpecRec[\"specimen_int\"] = '%8.3e ' % (pars[\"specimen_int\"])\n    PmagSpecRec[\"specimen_b\"] = '%5.3f ' % (pars[\"specimen_b\"])\n    PmagSpecRec[\"specimen_q\"] = '%5.1f ' % (pars[\"specimen_q\"])\n    PmagSpecRec[\"specimen_f\"] = '%5.3f ' % (pars[\"specimen_f\"])\n    PmagSpecRec[\"specimen_fvds\"] = '%5.3f' % (pars[\"specimen_fvds\"])\n    PmagSpecRec[\"specimen_b_beta\"] = '%5.3f' % (pars[\"specimen_b_beta\"])\n    PmagSpecRec[\"specimen_int_mad\"] = '%7.1f' % (pars[\"specimen_int_mad\"])\n    PmagSpecRec[\"specimen_dec\"] = '%7.1f' % (pars[\"specimen_dec\"])\n    PmagSpecRec[\"specimen_inc\"] = '%7.1f' % (pars[\"specimen_inc\"])\n    PmagSpecRec[\"specimen_int_dang\"] = '%7.1f ' % (pars[\"specimen_int_dang\"])\n    PmagSpecRec[\"specimen_drats\"] = '%7.1f ' % (pars[\"specimen_drats\"])\n    PmagSpecRec[\"specimen_int_ptrm_n\"] = '%i ' % (pars[\"specimen_int_ptrm_n\"])\n    PmagSpecRec[\"specimen_rsc\"] = '%6.4f ' % (pars[\"specimen_rsc\"])\n    PmagSpecRec[\"specimen_md\"] = '%i ' % (int(pars[\"specimen_md\"]))\n    PmagSpecRec[\"specimen_b_sigma\"] = '%5.3f ' % (pars[\"specimen_b_sigma\"])\n    if 'specimen_scat' in list(pars.keys()):\n        PmagSpecRec['specimen_scat'] = pars['specimen_scat']\n    if 'specimen_gmax' in list(pars.keys()):\n        PmagSpecRec['specimen_gmax'] = '%5.3f' % (pars['specimen_gmax'])\n    if 'specimen_frac' in list(pars.keys()):\n        PmagSpecRec['specimen_frac'] = '%5.3f' % (pars['specimen_frac'])\n    # PmagSpecRec[\"specimen_Z\"]='%7.1f'%(pars[\"specimen_Z\"])\n  # check score\n    #\n    kill = grade(PmagSpecRec, accept, 'specimen_int')\n    Grade = \"\"\n    if len(kill) == 0:\n        Grade = 'A'\n    else:\n        Grade = 'F'\n    pars[\"specimen_grade\"] = Grade\n    if verbose == 0:\n        return pars, kill\n    diffcum = 0\n    if pars['measurement_step_unit'] == 'K':\n        outstr = \"specimen     Tmin  Tmax  N  lab_field  B_anc  b  q  f(coe)  Fvds  beta  MAD  Dang  Drats  Nptrm  Grade  R  MD%  sigma  Gamma_max \\n\"\n        pars_out = (s, (pars[\"measurement_step_min\"] - 273), (pars[\"measurement_step_max\"] - 273), (pars[\"specimen_int_n\"]), 1e6 * (pars[\"specimen_lab_field_dc\"]), 1e6 * (pars[\"specimen_int\"]), pars[\"specimen_b\"], pars[\"specimen_q\"], pars[\"specimen_f\"], pars[\"specimen_fvds\"],\n                    pars[\"specimen_b_beta\"], pars[\"specimen_int_mad\"], pars[\"specimen_int_dang\"], pars[\"specimen_drats\"], pars[\"specimen_int_ptrm_n\"], pars[\"specimen_grade\"], np.sqrt(pars[\"specimen_rsc\"]), int(pars[\"specimen_md\"]), pars[\"specimen_b_sigma\"], pars['specimen_gamma'])\n        outstring = '%s %4.0f %4.0f %i %4.1f %4.1f %5.3f %5.1f %5.3f %5.3f %5.3f  %7.1f %7.1f %7.1f %s %s %6.3f %i %5.3f %7.1f' % pars_out + '\\n'\n    elif pars['measurement_step_unit'] == 'J':\n        outstr = \"specimen     Wmin  Wmax  N  lab_field  B_anc  b  q  f(coe)  Fvds  beta  MAD  Dang  Drats  Nptrm  Grade  R  MD%  sigma  ThetaMax DeltaMax GammaMax\\n\"\n        pars_out = (s, (pars[\"measurement_step_min\"]), (pars[\"measurement_step_max\"]), (pars[\"specimen_int_n\"]), 1e6 * (pars[\"specimen_lab_field_dc\"]), 1e6 * (pars[\"specimen_int\"]), pars[\"specimen_b\"], pars[\"specimen_q\"], pars[\"specimen_f\"], pars[\"specimen_fvds\"], pars[\"specimen_b_beta\"],\n                    pars[\"specimen_int_mad\"], pars[\"specimen_int_dang\"], pars[\"specimen_drats\"], pars[\"specimen_int_ptrm_n\"], pars[\"specimen_grade\"], np.sqrt(pars[\"specimen_rsc\"]), int(pars[\"specimen_md\"]), pars[\"specimen_b_sigma\"], pars[\"specimen_theta\"], pars[\"specimen_delta\"], pars[\"specimen_gamma\"])\n        outstring = '%s %4.0f %4.0f %i %4.1f %4.1f %5.3f %5.1f %5.3f %5.3f %5.3f  %7.1f %7.1f %7.1f %s %s %6.3f %i %5.3f %7.1f %7.1f %7.1f' % pars_out + '\\n'\n    if pars[\"specimen_grade\"] != \"A\":\n        print('\\n killed by:')\n        for k in kill:\n            print(k, ':, criterion set to: ',\n                  accept[k], ', specimen value: ', pars[k])\n        print('\\n')\n    print(outstr)\n    print(outstring)\n    return pars, kill", "response": "This function generates a scoreit formula for a given set of data and returns a new set of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef b_vdm(B, lat):\n    # changed radius of the earth from 3.367e6 3/12/2010\n    fact = ((6.371e6)**3) * 1e7\n    colat = np.radians(90. - lat)\n    return fact * B / (np.sqrt(1 + 3 * (np.cos(colat)**2)))", "response": "Converts a magnetic field value to a virtual dipole moment or a virtual axial dipole moment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vdm_b(vdm, lat):\n    rad = old_div(np.pi, 180.)\n    # changed radius of the earth from 3.367e6 3/12/2010\n    fact = ((6.371e6)**3) * 1e7\n    colat = (90. - lat) * rad\n    return vdm * (np.sqrt(1 + 3 * (np.cos(colat)**2))) / fact", "response": "Converts a virtual dipole moment or a virtual axial dipole moment to a local magnetic field value in tesla."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef binglookup(w1i, w2i):\n    K = {'0.06': {'0.02': ['-25.58', '-8.996'], '0.06': ['-9.043', '-9.043'], '0.04': ['-13.14', '-9.019']}, '0.22': {'0.08': ['-6.944', '-2.644'], '0.02': ['-25.63', '-2.712'], '0.20': ['-2.649', '-2.354'], '0.06': ['-9.027', '-2.673'], '0.04': ['-13.17', '-2.695'], '0.14': ['-4.071', '-2.521'], '0.16': ['-3.518', '-2.470'], '0.10': ['-5.658', '-2.609'], '0.12': ['-4.757', '-2.568'], '0.18': ['-3.053', '-2.414'], '0.22': ['-2.289', '-2.289']}, '0.46': {'0.02': ['-25.12', '-0.250'], '0.08': ['-6.215', '0.000'], '0.06': ['-8.371', '-0.090'], '0.04': ['-12.58', '-0.173']}, '0.44': {'0.08': ['-6.305', '-0.186'], '0.02': ['-25.19', '-0.418'], '0.06': ['-8.454', '-0.270'], '0.04': ['-12.66', '-0.347'], '0.10': ['-4.955', '-0.097'], '0.12': ['-3.992', '0.000']}, '0.42': {'0.08': ['-6.388', '-0.374'], '0.02': ['-25.5', '-0.589'], '0.06': ['-8.532', '-0.452'], '0.04': ['-12.73', '-0.523'], '0.14': ['-3.349', '-0.104'], '0.16': ['-2.741', '0.000'], '0.10': ['-5.045', '-0.290'], '0.12': ['-4.089', '-0.200']}, '0.40': {'0.08': ['-6.466', '-0.564'], '0.02': ['-25.31', '-0.762'], '0.20': ['-1.874', '-0.000'], '0.06': ['-8.604', '-0.636'], '0.04': ['-12.80', '-0.702'], '0.14': ['-3.446', '-0.312'], '0.16': ['-2.845', '-0.215'], '0.10': ['-5.126', '-0.486'], '0.12': ['-4.179', '-0.402'], '0.18': ['-2.330', '-0.111']}, '0.08': {'0.02': ['-25.6', '-6.977'], '0.08': ['-7.035', '-7.035'], '0.06': ['-9.065', '-7.020'], '0.04': ['-13.16', '-6.999']}, '0.28': {'0.08': ['-6.827', '-1.828'], '0.28': ['-1.106', '-1.106'], '0.02': ['-25.57', '-1.939'], '0.20': ['-2.441', '-1.458'], '0.26': ['-1.406', '-1.203'], '0.24': ['-1.724', '-1.294'], '0.06': ['-8.928', '-1.871'], '0.04': ['-13.09', '-1.908'], '0.14': ['-3.906', '-1.665'], '0.16': ['-3.338', '-1.601'], '0.10': ['-5.523', '-1.779'], '0.12': ['-4.606', '-1.725'], '0.18': ['-2.859', '-1.532'], '0.22': ['-2.066', '-1.378']}, '0.02': {'0.02': ['-25.55', '-25.55']}, '0.26': {'0.08': ['-6.870', '-2.078'], '0.02': ['-25.59', '-2.175'], '0.20': ['-2.515', '-1.735'], '0.26': ['-1.497', '-1.497'], '0.24': ['-1.809', '-1.582'], '0.06': ['-8.96 6', '-2.117'], '0.04': ['-13.12', '-2.149'], '0.14': ['-3.965', '-1.929'], '0.16': ['-3.403', '-1.869'], '0.10': ['-5.573', '-2.034'], '0.12': ['-4.661', '-1.984'], '0.18': ['-2.928', '-1.805'], '0.22': ['-2.1 46', '-1.661']}, '0.20': {'0.08': ['-6.974', '-2.973'], '0.02': ['-25.64', '-3.025'], '0.20': ['-2.709', '-2.709'], '0.06': ['-9.05', '-2.997'], '0.04': ['-13.18', '-3.014'], '0.14': ['-4.118', '-2.863'], '0.1 6': ['-3.570', '-2.816'], '0.10': ['-5.694', '-2.942'], '0.12': ['-4.799', '-2.905'], '0.18': ['-3.109', '-2.765']}, '0.04': {'0.02': ['-25.56', '-13.09'], '0.04': ['-13.11', '-13.11']}, '0.14': {'0.08': ['-7.  033', '-4.294'], '0.02': ['-25.64', '-4.295'], '0.06': ['-9.087', '-4.301'], '0.04': ['-13.20', '-4.301'], '0.14': ['-4.231', '-4.231'], '0.10': ['-5.773', '-4.279'], '0.12': ['-4.896', '-4.258']}, '0.16': {'0 .08': ['-7.019', '-3.777'], '0.02': ['-25.65', '-3.796'], '0.06': ['-9.081', '-3.790'], '0.04': ['-13.20', '-3.796'], '0.14': ['-4.198', '-3.697'], '0.16': ['-3.659', '-3.659'], '0.10': ['-5.752', '-3.756'], ' 0.12': ['-4.868', '-3.729']}, '0.10': {'0.02': ['-25.62', '-5.760'],\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     '0.08': ['-7.042', '-5.798'], '0.06': ['-9.080', '-5.791'], '0.10': ['-5.797', '-5.797'], '0.04': ['-13.18', '-5.777']}, '0.12': {'0.08': [' -7.041', '-4.941'], '0.02': ['-25.63', '-4.923'], '0.06': ['-9.087', '-4.941'], '0.04': ['-13.19', '-4.934'], '0.10': ['-5.789', '-4.933'], '0.12': ['-4.917', '-4.917']}, '0.18': {'0.08': ['-6.999', '-3.345'], '0.02': ['-25.65', '-3.381'], '0.06': ['-9.068', '-3.363'], '0.04': ['-13.19', '-3.375'], '0.14': ['-4.160', '-3.249'], '0.16': ['-3.616', '-3.207'], '0.10': ['-5.726', '-3.319'], '0.12': ['-4.836', '-3.287'], '0.18': ['-3.160', '-3.160']}, '0.38': {'0.08': ['-6.539', '-0.757'], '0.02': ['-25.37', '-0.940'], '0.20': ['-1.986', '-0.231'], '0.24': ['-1.202', '0.000'], '0.06': ['-8.670', '-0.824'], '0.04': ['-12.86', '-0.885'], '0.14': ['-3.536', '-0.522'], '0.16': ['-2.941', '-0.432'], '0.10': ['-5.207', '-0.684'], '0.12': ['-4.263', '-0.606'], '0.18': ['-2.434', '-0.335'], '0.22': ['-1.579', '-0.120']}, '0.36': {'0.08': ['-6.606', '-9.555'], '0.28': ['-0.642', '0.000'], '0.02': ['-25.42', '-1.123'], '0.20': ['-2.089', '-0.464'], '0.26': ['-0.974', '-0.129'], '0.24': ['-1.322', '-0.249'], '0.06': ['-8.731', '-1.017'], '0.04': ['-12.91', '-1.073'], '0.14': ['-3.620', '-0.736'], '0.16': ['-3.032', '-0.651'], '0.10': ['-5.280', '-0.887'], '0.12': ['-4.342', '-0.814'], '0.18': ['-2.531', '-0.561'], '0.22': ['-1.690', '-0.360']}, '0.34 ': {'0.08': ['-6.668', '-1.159'], '0.28': ['-0.771', '-0.269'], '0.02': ['-25.46', '-1.312'], '0.20': ['-2.186', '-0.701'], '0.26': ['-1.094', '-0.389'], '0.24': ['-1.433', '-0.500'], '0.06': ['-8.788', '-1.21 6'], '0.32': ['-0.152', '0.000'], '0.04': ['-12.96', '-1.267'], '0.30': ['-0.459', '-0.140'], '0.14': ['-3.699', '-0.955'], '0.16': ['-3.116', '-0.876'], '0.10': ['-5.348', '-1.096'], '0.12': ['-4.415', '-1.02 8'], '0.18': ['-2.621', '-0.791'], '0.22': ['-1.794', '-0.604']}, '0.32': {'0.08': ['-6.725', '-1.371'], '0.28': ['-0.891', '-0.541'], '0.02': ['-25.50', '-1.510'], '0.20': ['-2.277', '-0.944'], '0.26': ['-1.2 06', '-0.653'], '0.24': ['-1.537', '-0.756'], '0.06': ['-8.839', '-1.423'], '0.32': ['-0.292', '-0.292'], '0.04': ['-13.01', '-1.470'], '0.30': ['-0.588', '-0.421'], '0.14': ['-3.773', '-1.181'], '0.16': ['-3.  195', '-1.108'], '0.10': ['-5.411', '-1.313'], '0.12': ['-4.484', '-1.250'], '0.18': ['-2.706', '-1.028'], '0.22': ['-1.891', '-0.853']}, '0.30': {'0.08': ['-6.778', '-1.596'], '0.28': ['-1.002', '-0.819'], '0 .02': ['-25.54', '-1.718'], '0.20': ['-2.361', '-1.195'], '0.26': ['-1.309', '-0.923'], '0.24': ['-1.634', '-1.020'], '0.06': ['-8.886', '-1.641'], '0.04': ['-13.05', '-1.682'], '0.30': ['-0.708', '-0.708'], ' 0.14': ['-3.842', '-1.417'], '0.16': ['-3.269', '-1.348'], '0.10': ['-5.469', '-1.540'], '0.12': ['-4.547', '-1.481'], '0.18': ['-2.785', '-1.274'], '0.22': ['-1.981', '-1.110']}, '0.24': {'0.08': ['-6.910', ' -2.349'], '0.02': ['-25.61', '-2.431'], '0.20': ['-2.584', '-2.032'], '0.24': ['-1.888', '-1.888'], '0.06': ['-8.999', '-2.382'], '0.04': ['-23.14', '-2.410'], '0.14': ['-4.021', '-2.212'], '0.16': ['-3.463', '-2.157'], '0.10': ['-5.618', '-2.309'], '0.12': ['-4.711', '-2.263'], '0.18': ['-2.993', '-2.097'], '0.22': ['-2.220', '-1.963']}}\n    w1, w2 = 0., 0.\n    wstart, incr = 0.01, 0.02\n    if w1i < wstart:\n        w1 = '%4.2f' % (wstart + old_div(incr, 2.))\n    if w2i < wstart:\n        w2 = '%4.2f' % (wstart + old_div(incr, 2.))\n    wnext = wstart + incr\n    while wstart < 0.5:\n        if w1i >= wstart and w1i < wnext:\n            w1 = '%4.2f' % (wstart + old_div(incr, 2.))\n        if w2i >= wstart and w2i < wnext:\n            w2 = '%4.2f' % (wstart + old_div(incr, 2.))\n        wstart += incr\n        wnext += incr\n    k1, k2 = float(K[w2][w1][0]), float(K[w2][w1][1])\n    return k1, k2", "response": "A binglookup function that returns a list of bingham statistics for the given window."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nspits out the cdf for data to file", "response": "def cdfout(data, file):\n    \"\"\"\n    spits out the cdf for data to file\n    \"\"\"\n    f = open(file, \"w\")\n    data.sort()\n    for j in range(len(data)):\n        y = old_div(float(j), float(len(data)))\n        out = str(data[j]) + ' ' + str(y) + '\\n'\n        f.write(out)\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the Bingham mean and associated statistical parameters from a nested list of dec inc intensity and intensity.", "response": "def dobingham(di_block):\n    \"\"\"\n    Calculates the Bingham mean and associated statistical parameters from\n    directions that are input as a di_block\n\n    Parameters\n    ----------\n    di_block : a nested list of [dec,inc] or [dec,inc,intensity]\n\n    Returns\n    -------\n    bpars : dictionary containing the Bingham mean and associated statistics\n    dictionary keys\n        dec : mean declination\n        inc : mean inclination\n        n : number of datapoints\n        Eta : major ellipse\n        Edec : declination of major ellipse axis\n        Einc : inclination of major ellipse axis\n        Zeta : minor ellipse\n        Zdec : declination of minor ellipse axis\n        Zinc : inclination of minor ellipse axis\n\n    \"\"\"\n    control, X, bpars = [], [], {}\n    N = len(di_block)\n    if N < 2:\n        return bpars\n#\n#  get cartesian coordinates\n#\n    for rec in di_block:\n        X.append(dir2cart([rec[0], rec[1], 1.]))\n#\n#   put in T matrix\n#\n    T = np.array(Tmatrix(X))\n    t, V = tauV(T)\n    w1, w2, w3 = t[2], t[1], t[0]\n    k1, k2 = binglookup(w1, w2)\n    PDir = cart2dir(V[0])\n    EDir = cart2dir(V[1])\n    ZDir = cart2dir(V[2])\n    if PDir[1] < 0:\n        PDir[0] += 180.\n        PDir[1] = -PDir[1]\n    PDir[0] = PDir[0] % 360.\n    bpars[\"dec\"] = PDir[0]\n    bpars[\"inc\"] = PDir[1]\n    bpars[\"Edec\"] = EDir[0]\n    bpars[\"Einc\"] = EDir[1]\n    bpars[\"Zdec\"] = ZDir[0]\n    bpars[\"Zinc\"] = ZDir[1]\n    bpars[\"n\"] = N\n#\n#  now for Bingham ellipses.\n#\n    fac1, fac2 = -2 * N * (k1) * (w3 - w1), -2 * N * (k2) * (w3 - w2)\n    sig31, sig32 = np.sqrt(old_div(1., fac1)), np.sqrt(old_div(1., fac2))\n    bpars[\"Zeta\"], bpars[\"Eta\"] = 2.45 * sig31 * \\\n        180. / np.pi, 2.45 * sig32 * 180. / np.pi\n    return bpars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflip lower hemisphere data to upper hemisphere data", "response": "def doflip(dec, inc):\n    \"\"\"\n    flips lower hemisphere data to upper hemisphere\n    \"\"\"\n    if inc < 0:\n        inc = -inc\n        dec = (dec + 180.) % 360.\n    return dec, inc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef doincfish(inc):\n    rad, SCOi, SSOi = old_div(np.pi, 180.), 0., 0.  # some definitions\n    abinc = []\n    for i in inc:\n        abinc.append(abs(i))\n    MI, std = gausspars(abinc)  # get mean inc and standard deviation\n    fpars = {}\n    N = len(inc)  # number of data\n    fpars['n'] = N\n    fpars['ginc'] = MI\n    if MI < 30:\n        fpars['inc'] = MI\n        fpars['k'] = 0\n        fpars['alpha95'] = 0\n        fpars['csd'] = 0\n        fpars['r'] = 0\n        print('WARNING: mean inc < 30, returning gaussian mean')\n        return fpars\n    for i in inc:  # sum over all incs (but take only positive inc)\n        coinc = (90. - abs(i)) * rad\n        SCOi += np.cos(coinc)\n        SSOi += np.sin(coinc)\n    Oo = (90.0 - MI) * rad  # first guess at mean\n    SCFlag = -1  # sign change flag\n    epsilon = float(N) * np.cos(Oo)  # RHS of zero equations\n    epsilon += (np.sin(Oo)**2 - np.cos(Oo)**2) * SCOi\n    epsilon -= 2. * np.sin(Oo) * np.cos(Oo) * SSOi\n    while SCFlag < 0:  # loop until cross zero\n        if MI > 0:\n            Oo -= (.01 * rad)  # get steeper\n        if MI < 0:\n            Oo += (.01 * rad)  # get shallower\n        prev = epsilon\n        epsilon = float(N) * np.cos(Oo)  # RHS of zero equations\n        epsilon += (np.sin(Oo)**2. - np.cos(Oo)**2.) * SCOi\n        epsilon -= 2. * np.sin(Oo) * np.cos(Oo) * SSOi\n        if abs(epsilon) > abs(prev):\n            MI = -1 * MI  # reverse direction\n        if epsilon * prev < 0:\n            SCFlag = 1  # changed sign\n    S, C = 0., 0.  # initialize for summation\n    for i in inc:\n        coinc = (90. - abs(i)) * rad\n        S += np.sin(Oo - coinc)\n        C += np.cos(Oo - coinc)\n    k = old_div((N - 1.), (2. * (N - C)))\n    Imle = 90. - (old_div(Oo, rad))\n    fpars[\"inc\"] = Imle\n    fpars[\"r\"], R = 2. * C - N, 2 * C - N\n    fpars[\"k\"] = k\n    f = fcalc(2, N - 1)\n    a95 = 1. - (0.5) * (old_div(S, C))**2 - (old_div(f, (2. * C * k)))\n#    b=20.**(1./(N-1.)) -1.\n#    a=1.-b*(N-R)/R\n#    a95=np.arccos(a)*180./np.pi\n    csd = old_div(81., np.sqrt(k))\n    fpars[\"alpha95\"] = a95\n    fpars[\"csd\"] = csd\n    return fpars", "response": "This function calculates the fisher mean inc from inc only data and returns the dictionary of the n - th data in the order they appear in the order they appear in the data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the Kent parameters for data", "response": "def dokent(data, NN):\n    \"\"\"\n    gets Kent  parameters for data\n    Parameters\n    ___________________\n    data :  nested pairs of [Dec,Inc]\n    NN  : normalization\n        NN is the number of data for Kent ellipse\n        NN is 1 for Kent ellipses of bootstrapped mean directions\n\n    Return\n    kpars dictionary keys\n        dec : mean declination\n        inc : mean inclination\n        n : number of datapoints\n        Eta : major ellipse\n        Edec : declination of major ellipse axis\n        Einc : inclination of major ellipse axis\n        Zeta : minor ellipse\n        Zdec : declination of minor ellipse axis\n        Zinc : inclination of minor ellipse axis\n    \"\"\"\n    X, kpars = [], {}\n    N = len(data)\n    if N < 2:\n        return kpars\n#\n#  get fisher mean and convert to co-inclination (theta)/dec (phi) in radians\n#\n    fpars = fisher_mean(data)\n    pbar = fpars[\"dec\"] * np.pi / 180.\n    tbar = (90. - fpars[\"inc\"]) * np.pi / 180.\n#\n#   initialize matrices\n#\n    H = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n    w = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n    b = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n    gam = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n    xg = []\n#\n#  set up rotation matrix H\n#\n    H = [[np.cos(tbar) * np.cos(pbar), -np.sin(pbar), np.sin(tbar) * np.cos(pbar)], [np.cos(tbar)\n                                                                                     * np.sin(pbar), np.cos(pbar), np.sin(pbar) * np.sin(tbar)], [-np.sin(tbar), 0., np.cos(tbar)]]\n#\n#  get cartesian coordinates of data\n#\n    for rec in data:\n        X.append(dir2cart([rec[0], rec[1], 1.]))\n#\n#   put in T matrix\n#\n    T = Tmatrix(X)\n    for i in range(3):\n        for j in range(3):\n            T[i][j] = old_div(T[i][j], float(NN))\n#\n# compute B=H'TH\n#\n    for i in range(3):\n        for j in range(3):\n            for k in range(3):\n                w[i][j] += T[i][k] * H[k][j]\n    for i in range(3):\n        for j in range(3):\n            for k in range(3):\n                b[i][j] += H[k][i] * w[k][j]\n#\n# choose a rotation w about North pole to diagonalize upper part of B\n#\n    psi = 0.5 * np.arctan(2. * b[0][1] / (b[0][0] - b[1][1]))\n    w = [[np.cos(psi), -np.sin(psi), 0],\n         [np.sin(psi), np.cos(psi), 0], [0., 0., 1.]]\n    for i in range(3):\n        for j in range(3):\n            gamtmp = 0.\n            for k in range(3):\n                gamtmp += H[i][k] * w[k][j]\n            gam[i][j] = gamtmp\n    for i in range(N):\n        xg.append([0., 0., 0.])\n        for k in range(3):\n            xgtmp = 0.\n            for j in range(3):\n                xgtmp += gam[j][k] * X[i][j]\n            xg[i][k] = xgtmp\n# compute asymptotic ellipse parameters\n#\n    xmu, sigma1, sigma2 = 0., 0., 0.\n    for i in range(N):\n        xmu += xg[i][2]\n        sigma1 = sigma1 + xg[i][0]**2\n        sigma2 = sigma2 + xg[i][1]**2\n    xmu = old_div(xmu, float(N))\n    sigma1 = old_div(sigma1, float(N))\n    sigma2 = old_div(sigma2, float(N))\n    g = -2.0 * np.log(0.05) / (float(NN) * xmu**2)\n    if np.sqrt(sigma1 * g) < 1:\n        eta = np.arcsin(np.sqrt(sigma1 * g))\n    if np.sqrt(sigma2 * g) < 1:\n        zeta = np.arcsin(np.sqrt(sigma2 * g))\n    if np.sqrt(sigma1 * g) >= 1.:\n        eta = old_div(np.pi, 2.)\n    if np.sqrt(sigma2 * g) >= 1.:\n        zeta = old_div(np.pi, 2.)\n#\n#  convert Kent parameters to directions,angles\n#\n    kpars[\"dec\"] = fpars[\"dec\"]\n    kpars[\"inc\"] = fpars[\"inc\"]\n    kpars[\"n\"] = NN\n    ZDir = cart2dir([gam[0][1], gam[1][1], gam[2][1]])\n    EDir = cart2dir([gam[0][0], gam[1][0], gam[2][0]])\n    kpars[\"Zdec\"] = ZDir[0]\n    kpars[\"Zinc\"] = ZDir[1]\n    kpars[\"Edec\"] = EDir[0]\n    kpars[\"Einc\"] = EDir[1]\n    if kpars[\"Zinc\"] < 0:\n        kpars[\"Zinc\"] = -kpars[\"Zinc\"]\n        kpars[\"Zdec\"] = (kpars[\"Zdec\"] + 180.) % 360.\n    if kpars[\"Einc\"] < 0:\n        kpars[\"Einc\"] = -kpars[\"Einc\"]\n        kpars[\"Edec\"] = (kpars[\"Edec\"] + 180.) % 360.\n    kpars[\"Zeta\"] = zeta * 180. / np.pi\n    kpars[\"Eta\"] = eta * 180. / np.pi\n    return kpars"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new nparray object with principal components from a list of dec inc directions.", "response": "def doprinc(data):\n    \"\"\"\n    Gets principal components from data in form of a list of [dec,inc] data.\n\n    Parameters\n    ----------\n    data : nested list of dec, inc directions\n\n    Returns\n    -------\n    ppars : dictionary with the principal components\n        dec : principal directiion declination\n        inc : principal direction inclination\n        V2dec : intermediate eigenvector declination\n        V2inc : intermediate eigenvector inclination\n        V3dec : minor eigenvector declination\n        V3inc : minor eigenvector inclination\n        tau1 : major eigenvalue\n        tau2 : intermediate eigenvalue\n        tau3 : minor eigenvalue\n        N  : number of points\n        Edir : elongation direction [dec, inc, length]\n    \"\"\"\n    ppars = {}\n    rad = old_div(np.pi, 180.)\n    X = dir2cart(data)\n    # for rec in data:\n    #    dir=[]\n    #    for c in rec: dir.append(c)\n    #    cart= (dir2cart(dir))\n    #    X.append(cart)\n#   put in T matrix\n#\n    T = np.array(Tmatrix(X))\n#\n#   get sorted evals/evects\n#\n    t, V = tauV(T)\n    Pdir = cart2dir(V[0])\n    ppars['Edir'] = cart2dir(V[1])  # elongation direction\n    dec, inc = doflip(Pdir[0], Pdir[1])\n    ppars['dec'] = dec\n    ppars['inc'] = inc\n    ppars['N'] = len(data)\n    ppars['tau1'] = t[0]\n    ppars['tau2'] = t[1]\n    ppars['tau3'] = t[2]\n    Pdir = cart2dir(V[1])\n    dec, inc = doflip(Pdir[0], Pdir[1])\n    ppars['V2dec'] = dec\n    ppars['V2inc'] = inc\n    Pdir = cart2dir(V[2])\n    dec, inc = doflip(Pdir[0], Pdir[1])\n    ppars['V3dec'] = dec\n    ppars['V3inc'] = inc\n    return ppars"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrotate points on a globe by an Euler pole rotation using method of Cox and Hart 1986, box 7-3. Parameters ---------- EP : Euler pole list [lat,lon,angle] Lats : list of latitudes of points to be rotated Lons : list of longitudes of points to be rotated Returns _________ RLats : rotated latitudes RLons : rotated longitudes", "response": "def pt_rot(EP, Lats, Lons):\n    \"\"\"\n    Rotates points on a globe by an Euler pole rotation using method of\n    Cox and Hart 1986, box 7-3.\n\n    Parameters\n    ----------\n    EP : Euler pole list [lat,lon,angle]\n    Lats : list of latitudes of points to be rotated\n    Lons : list of longitudes of points to be rotated\n\n    Returns\n    _________\n    RLats : rotated latitudes\n    RLons : rotated longitudes\n    \"\"\"\n# gets user input of Rotation pole lat,long, omega for plate and converts\n# to radians\n    E = dir2cart([EP[1], EP[0], 1.])  # EP is pole lat,lon omega\n    omega = EP[2] * np.pi / 180.  # convert to radians\n    RLats, RLons = [], []\n    for k in range(len(Lats)):\n        if Lats[k] <= 90.:  # peel off delimiters\n            # converts to rotation pole to cartesian coordinates\n            A = dir2cart([Lons[k], Lats[k], 1.])\n# defines cartesian coordinates of the pole A\n            R = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]\n            R[0][0] = E[0] * E[0] * (1 - np.cos(omega)) + np.cos(omega)\n            R[0][1] = E[0] * E[1] * (1 - np.cos(omega)) - E[2] * np.sin(omega)\n            R[0][2] = E[0] * E[2] * (1 - np.cos(omega)) + E[1] * np.sin(omega)\n            R[1][0] = E[1] * E[0] * (1 - np.cos(omega)) + E[2] * np.sin(omega)\n            R[1][1] = E[1] * E[1] * (1 - np.cos(omega)) + np.cos(omega)\n            R[1][2] = E[1] * E[2] * (1 - np.cos(omega)) - E[0] * np.sin(omega)\n            R[2][0] = E[2] * E[0] * (1 - np.cos(omega)) - E[1] * np.sin(omega)\n            R[2][1] = E[2] * E[1] * (1 - np.cos(omega)) + E[0] * np.sin(omega)\n            R[2][2] = E[2] * E[2] * (1 - np.cos(omega)) + np.cos(omega)\n# sets up rotation matrix\n            Ap = [0, 0, 0]\n            for i in range(3):\n                for j in range(3):\n                    Ap[i] += R[i][j] * A[j]\n# does the rotation\n            Prot = cart2dir(Ap)\n            RLats.append(Prot[1])\n            RLons.append(Prot[0])\n        else:  # preserve delimiters\n            RLats.append(Lats[k])\n            RLons.append(Lons[k])\n    return RLats, RLons"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dread(infile, cols):\n    data = []\n    f = open(infile, \"r\")\n    for line in f.readlines():\n        tmp = line.split()\n        rec = (tmp[0], float(tmp[cols[0]]), float(tmp[cols[1]]), float(tmp[cols[2]]),\n               float(tmp[cols[3]]))\n        data.append(rec)\n    f.close()\n    return data", "response": "reads in specimen tr dec inc int into data [."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a random draw from a Fisher distribution with mean declination and inclination of 90 with a specified kappa", "response": "def fshdev(k):\n    \"\"\"\n    Generate a random draw from a Fisher distribution with mean declination\n    of 0 and inclination of 90 with a specified kappa.\n\n    Parameters\n    ----------\n    k : kappa (precision parameter) of the distribution\n        k can be a single number or an array of values\n\n    Returns\n    ----------\n    dec, inc : declination and inclination of random Fisher distribution draw\n               if k is an array, dec, inc are returned as arrays, otherwise, single values\n    \"\"\"\n    k = np.array(k)\n    if len(k.shape) != 0:\n        n = k.shape[0]\n    else:\n        n = 1\n    R1 = random.random(size=n)\n    R2 = random.random(size=n)\n    L = np.exp(-2 * k)\n    a = R1 * (1 - L) + L\n    fac = np.sqrt(-np.log(a)/(2 * k))\n    inc = 90. - np.degrees(2 * np.arcsin(fac))\n    dec = np.degrees(2 * np.pi * R2)\n    if n == 1:\n        return dec[0], inc[0]  # preserve backward compatibility\n    else:\n        return dec, inc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget Lowe's power spectrum from gauss coefficients Parameters _________ data : nested list of [[l,m,g,h],...] as from pmag.unpack() Returns _______ Ls : list of degrees (l) Rs : power at degree l", "response": "def lowes(data):\n    \"\"\"\n    gets Lowe's power spectrum  from gauss coefficients\n\n    Parameters\n    _________\n    data : nested list of [[l,m,g,h],...] as from pmag.unpack()\n\n    Returns\n    _______\n    Ls : list of degrees (l)\n    Rs : power at  degree l\n\n    \"\"\"\n    lmax = data[-1][0]\n    Ls = list(range(1, lmax+1))\n    Rs = []\n    recno = 0\n    for l in Ls:\n        pow = 0\n        for m in range(0, l + 1):\n            pow += (l + 1) * ((1e-3 * data[recno][2])\n                              ** 2 + (1e-3 * data[recno][3])**2)\n            recno += 1\n        Rs.append(pow)\n    return Ls, Rs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef magnetic_lat(inc):\n    rad = old_div(np.pi, 180.)\n    paleo_lat = old_div(np.arctan(0.5 * np.tan(inc * rad)), rad)\n    return paleo_lat", "response": "returns magnetic latitude from inclination\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Dir_anis_corr(InDir, AniSpec):\n    Dir = np.zeros((3), 'f')\n    Dir[0] = InDir[0]\n    Dir[1] = InDir[1]\n    Dir[2] = 1.\n    chi, chi_inv = check_F(AniSpec)\n    if chi[0][0] == 1.:\n        return Dir  # isotropic\n    X = dir2cart(Dir)\n    M = np.array(X)\n    H = np.dot(M, chi_inv)\n    return cart2dir(H)", "response": "returns corrected Dec Inc\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake the 6 element 's' vector and the Dec,Inc, Int 'Dir' data, performs simple anisotropy correction. returns corrected Dec, Inc, Int", "response": "def doaniscorr(PmagSpecRec, AniSpec):\n    \"\"\"\n    takes the 6 element 's' vector and the Dec,Inc, Int 'Dir' data,\n    performs simple anisotropy correction. returns corrected Dec, Inc, Int\n    \"\"\"\n    AniSpecRec = {}\n    for key in list(PmagSpecRec.keys()):\n        AniSpecRec[key] = PmagSpecRec[key]\n    Dir = np.zeros((3), 'f')\n    Dir[0] = float(PmagSpecRec[\"specimen_dec\"])\n    Dir[1] = float(PmagSpecRec[\"specimen_inc\"])\n    Dir[2] = float(PmagSpecRec[\"specimen_int\"])\n# check if F test passes!  if anisotropy_sigma available\n    chi, chi_inv = check_F(AniSpec)\n    if chi[0][0] == 1.:  # isotropic\n        cDir = [Dir[0], Dir[1]]  # no change\n        newint = Dir[2]\n    else:\n        X = dir2cart(Dir)\n        M = np.array(X)\n        H = np.dot(M, chi_inv)\n        cDir = cart2dir(H)\n        Hunit = [old_div(H[0], cDir[2]), old_div(H[1], cDir[2]), old_div(\n            H[2], cDir[2])]  # unit vector parallel to Banc\n        Zunit = [0, 0, -1.]  # unit vector parallel to lab field\n        Hpar = np.dot(chi, Hunit)  # unit vector applied along ancient field\n        Zpar = np.dot(chi, Zunit)  # unit vector applied along lab field\n        # intensity of resultant vector from ancient field\n        HparInt = cart2dir(Hpar)[2]\n        # intensity of resultant vector from lab field\n        ZparInt = cart2dir(Zpar)[2]\n        newint = Dir[2] * ZparInt / HparInt\n        if cDir[0] - Dir[0] > 90:\n            cDir[1] = -cDir[1]\n            cDir[0] = (cDir[0] - 180.) % 360.\n    AniSpecRec[\"specimen_dec\"] = '%7.1f' % (cDir[0])\n    AniSpecRec[\"specimen_inc\"] = '%7.1f' % (cDir[1])\n    AniSpecRec[\"specimen_int\"] = '%9.4e' % (newint)\n    AniSpecRec[\"specimen_correction\"] = 'c'\n    if 'magic_method_codes' in list(AniSpecRec.keys()):\n        methcodes = AniSpecRec[\"magic_method_codes\"]\n    else:\n        methcodes = \"\"\n    if methcodes == \"\":\n        methcodes = \"DA-AC-\" + AniSpec['anisotropy_type']\n    if methcodes != \"\":\n        methcodes = methcodes + \":DA-AC-\" + AniSpec['anisotropy_type']\n    if chi[0][0] == 1.:  # isotropic\n        # indicates anisotropy was checked and no change necessary\n        methcodes = methcodes + ':DA-AC-ISO'\n    AniSpecRec[\"magic_method_codes\"] = methcodes.strip(\":\")\n    return AniSpecRec"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions to calculate the Watson Vw test statistic.", "response": "def vfunc(pars_1, pars_2):\n    \"\"\"\n    Calculate the Watson Vw test statistic. Calculated as 2*(Sw-Rw)\n\n    Parameters\n    ----------\n    pars_1 : dictionary of Fisher statistics from population 1\n    pars_2 : dictionary of Fisher statistics from population 2\n\n    Returns\n    -------\n    Vw : Watson's Vw statistic\n    \"\"\"\n    cart_1 = dir2cart([pars_1[\"dec\"], pars_1[\"inc\"], pars_1[\"r\"]])\n    cart_2 = dir2cart([pars_2['dec'], pars_2['inc'], pars_2[\"r\"]])\n    Sw = pars_1['k'] * pars_1['r'] + pars_2['k'] * pars_2['r']  # k1*r1+k2*r2\n    xhat_1 = pars_1['k'] * cart_1[0] + pars_2['k'] * cart_2[0]  # k1*x1+k2*x2\n    xhat_2 = pars_1['k'] * cart_1[1] + pars_2['k'] * cart_2[1]  # k1*y1+k2*y2\n    xhat_3 = pars_1['k'] * cart_1[2] + pars_2['k'] * cart_2[2]  # k1*z1+k2*z2\n    Rw = np.sqrt(xhat_1**2 + xhat_2**2 + xhat_3**2)\n    return 2 * (Sw - Rw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a pole position to a direction", "response": "def vgp_di(plat, plong, slat, slong):\n    \"\"\"\n    Converts a pole position (pole latitude, pole longitude) to a direction\n    (declination, inclination) at a given location (slat, slong) assuming a\n    dipolar field.\n\n    Parameters\n    ----------\n    plat : latitude of pole (vgp latitude)\n    plong : longitude of pole (vgp longitude)\n    slat : latitude of site\n    slong : longitude of site\n\n    Returns\n    ----------\n    dec,inc : tuple of declination and inclination\n    \"\"\"\n    plong = plong % 360\n    slong = slong % 360\n    signdec = 1.\n    delphi = abs(plong - slong)\n    if delphi != 0:\n        signdec = (plong - slong) / delphi\n    if slat == 90.:\n        slat = 89.99\n    thetaS = np.radians(90. - slat)\n    thetaP = np.radians(90. - plat)\n    delphi = np.radians(delphi)\n    cosp = np.cos(thetaS) * np.cos(thetaP) + np.sin(thetaS) * \\\n        np.sin(thetaP) * np.cos(delphi)\n    thetaM = np.arccos(cosp)\n    cosd = old_div((np.cos(thetaP) - np.cos(thetaM) *\n                    np.cos(thetaS)), (np.sin(thetaM) * np.sin(thetaS)))\n    C = abs(1. - cosd**2)\n    if C != 0:\n        dec = -np.arctan(cosd/np.sqrt(abs(C))) + (np.pi/2.)\n    else:\n        dec = np.arccos(cosd)\n    if -np.pi < signdec * delphi and signdec < 0:\n        dec = 2. * np.pi - dec  # checking quadrant\n    if signdec * delphi > np.pi:\n        dec = 2. * np.pi - dec\n    dec = np.degrees(dec) % 360.\n    inc = np.degrees(np.arctan2(2. * np.cos(thetaM), np.sin(thetaM)))\n    return dec, inc"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Watson s V statistic for two sets of directions Dir1 and Dir2.", "response": "def watsonsV(Dir1, Dir2):\n    \"\"\"\n    calculates Watson's V statistic for two sets of directions\n    \"\"\"\n    counter, NumSims = 0, 500\n#\n# first calculate the fisher means and cartesian coordinates of each set of Directions\n#\n    pars_1 = fisher_mean(Dir1)\n    pars_2 = fisher_mean(Dir2)\n#\n# get V statistic for these\n#\n    V = vfunc(pars_1, pars_2)\n#\n# do monte carlo simulation of datasets with same kappas, but common mean\n#\n    Vp = []  # set of Vs from simulations\n    print(\"Doing \", NumSims, \" simulations\")\n    for k in range(NumSims):\n        counter += 1\n        if counter == 50:\n            print(k + 1)\n            counter = 0\n        Dirp = []\n# get a set of N1 fisher distributed vectors with k1, calculate fisher stats\n        for i in range(pars_1[\"n\"]):\n            Dirp.append(fshdev(pars_1[\"k\"]))\n        pars_p1 = fisher_mean(Dirp)\n# get a set of N2 fisher distributed vectors with k2, calculate fisher stats\n        Dirp = []\n        for i in range(pars_2[\"n\"]):\n            Dirp.append(fshdev(pars_2[\"k\"]))\n        pars_p2 = fisher_mean(Dirp)\n# get the V for these\n        Vk = vfunc(pars_p1, pars_p2)\n        Vp.append(Vk)\n#\n# sort the Vs, get Vcrit (95th one)\n#\n    Vp.sort()\n    k = int(.95 * NumSims)\n    return V, Vp[k]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dimap(D, I):\n    try:\n        D = float(D)\n        I = float(I)\n    except TypeError:  # is an array\n        return dimap_V(D, I)\n# DEFINE FUNCTION VARIABLES\n    # initialize equal area projection x,y\n    XY = [0., 0.]\n\n# GET CARTESIAN COMPONENTS OF INPUT DIRECTION\n    X = dir2cart([D, I, 1.])\n\n# CHECK IF Z = 1 AND ABORT\n    if X[2] == 1.0:\n        return XY                       # return [0,0]\n\n# TAKE THE ABSOLUTE VALUE OF Z\n    if X[2] < 0:\n        # this only works on lower hemisphere projections\n        X[2] = -X[2]\n\n# CALCULATE THE X,Y COORDINATES FOR THE EQUAL AREA PROJECTION\n    # from Collinson 1983\n    R = old_div(np.sqrt(1. - X[2]), (np.sqrt(X[0]**2 + X[1]**2)))\n    XY[1], XY[0] = X[0] * R, X[1] * R\n\n# RETURN XY[X,Y]\n    return XY", "response": "Function to map directions to x y pairs in equal area projection"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction TO MAP DECLINATION, INCLINATIONS INTO EQUAL AREA PROJECTION, X,Y Usage: dimap_V(D, I) D and I are both numpy arrays", "response": "def dimap_V(D, I):\n    \"\"\"\n    FUNCTION TO MAP DECLINATION, INCLINATIONS INTO EQUAL AREA PROJECTION, X,Y\n\n    Usage:     dimap_V(D, I)\n        D and I are both numpy arrays\n\n    \"\"\"\n# GET CARTESIAN COMPONENTS OF INPUT DIRECTION\n    DI = np.array([D, I]).transpose()\n    X = dir2cart(DI).transpose()\n# CALCULATE THE X,Y COORDINATES FOR THE EQUAL AREA PROJECTION\n    # from Collinson 1983\n    R = np.sqrt(1. - abs(X[2]))/(np.sqrt(X[0]**2 + X[1]**2))\n    XY = np.array([X[1] * R, X[0] * R]).transpose()\n\n# RETURN XY[X,Y]\n    return XY"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getmeths(method_type):\n    meths = []\n    if method_type == 'GM':\n        meths.append('GM-PMAG-APWP')\n        meths.append('GM-ARAR')\n        meths.append('GM-ARAR-AP')\n        meths.append('GM-ARAR-II')\n        meths.append('GM-ARAR-NI')\n        meths.append('GM-ARAR-TF')\n        meths.append('GM-CC-ARCH')\n        meths.append('GM-CC-ARCHMAG')\n        meths.append('GM-C14')\n        meths.append('GM-FOSSIL')\n        meths.append('GM-FT')\n        meths.append('GM-INT-L')\n        meths.append('GM-INT-S')\n        meths.append('GM-ISO')\n        meths.append('GM-KAR')\n        meths.append('GM-PMAG-ANOM')\n        meths.append('GM-PMAG-POL')\n        meths.append('GM-PBPB')\n        meths.append('GM-RATH')\n        meths.append('GM-RBSR')\n        meths.append('GM-RBSR-I')\n        meths.append('GM-RBSR-MA')\n        meths.append('GM-SMND')\n        meths.append('GM-SMND-I')\n        meths.append('GM-SMND-MA')\n        meths.append('GM-CC-STRAT')\n        meths.append('GM-LUM-TH')\n        meths.append('GM-UPA')\n        meths.append('GM-UPB')\n        meths.append('GM-UTH')\n        meths.append('GM-UTHHE')\n    else:\n        pass\n    return meths", "response": "returns a list of all available MagIC method codes available for a given type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_up(ofile, Rec, file_type):\n    keylist = []\n    pmag_out = open(ofile, 'a')\n    outstring = \"tab \\t\" + file_type + \"\\n\"\n    pmag_out.write(outstring)\n    keystring = \"\"\n    for key in list(Rec.keys()):\n        keystring = keystring + '\\t' + key\n        keylist.append(key)\n    keystring = keystring + '\\n'\n    pmag_out.write(keystring[1:])\n    pmag_out.close()\n    return keylist", "response": "writes the first up of a MagIC template file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_age(Rec, sitekey, keybase, Ages, DefaultAge):\n    site = Rec[sitekey]\n    gotone = 0\n    if len(Ages) > 0:\n        for agerec in Ages:\n            if agerec[\"er_site_name\"] == site:\n                if \"age\" in list(agerec.keys()) and agerec[\"age\"] != \"\":\n                    Rec[keybase + \"age\"] = agerec[\"age\"]\n                    gotone = 1\n                if \"age_unit\" in list(agerec.keys()):\n                    Rec[keybase + \"age_unit\"] = agerec[\"age_unit\"]\n                if \"age_sigma\" in list(agerec.keys()):\n                    Rec[keybase + \"age_sigma\"] = agerec[\"age_sigma\"]\n    if gotone == 0 and len(DefaultAge) > 1:\n        sigma = 0.5 * (float(DefaultAge[1]) - float(DefaultAge[0]))\n        age = float(DefaultAge[0]) + sigma\n        Rec[keybase + \"age\"] = '%10.4e' % (age)\n        Rec[keybase + \"age_sigma\"] = '%10.4e' % (sigma)\n        Rec[keybase + \"age_unit\"] = DefaultAge[2]\n    return Rec", "response": "returns the age record for a given site"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adjust_ages(AgesIn):\n# get a list of age_units first\n    age_units, AgesOut, factors, factor, maxunit, age_unit = [], [], [], 1, 1, \"Ma\"\n    for agerec in AgesIn:\n        if agerec[1] not in age_units:\n            age_units.append(agerec[1])\n            if agerec[1] == \"Ga\":\n                factors.append(1e9)\n                maxunit, age_unit, factor = 1e9, \"Ga\", 1e9\n            if agerec[1] == \"Ma\":\n                if maxunit == 1:\n                    maxunit, age_unt, factor = 1e6, \"Ma\", 1e6\n                factors.append(1e6)\n            if agerec[1] == \"Ka\":\n                factors.append(1e3)\n                if maxunit == 1:\n                    maxunit, age_unit, factor = 1e3, \"Ka\", 1e3\n            if \"Years\" in agerec[1].split():\n                factors.append(1)\n    if len(age_units) == 1:  # all ages are of same type\n        for agerec in AgesIn:\n            AgesOut.append(agerec[0])\n    elif len(age_units) > 1:\n        for agerec in AgesIn:  # normalize all to largest age unit\n            if agerec[1] == \"Ga\":\n                AgesOut.append(agerec[0] * 1e9 / factor)\n            if agerec[1] == \"Ma\":\n                AgesOut.append(agerec[0] * 1e6 / factor)\n            if agerec[1] == \"Ka\":\n                AgesOut.append(agerec[0] * 1e3 / factor)\n            if \"Years\" in agerec[1].split():\n                if agerec[1] == \"Years BP\":\n                    AgesOut.append(old_div(agerec[0], factor))\n                if agerec[1] == \"Years Cal BP\":\n                    AgesOut.append(old_div(agerec[0], factor))\n                if agerec[1] == \"Years AD (+/-)\":\n                    # convert to years BP first\n                    AgesOut.append(old_div((1950 - agerec[0]), factor))\n                if agerec[1] == \"Years Cal AD (+/-)\":\n                    AgesOut.append(old_div((1950 - agerec[0]), factor))\n    return AgesOut, age_unit", "response": "Function to adjust ages to a common age_unit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning N uniformly distributed directions in order of dec and inc.", "response": "def get_unf(N=100):\n    \"\"\"\n    Generates N uniformly distributed directions\n    using the way described in Fisher et al. (1987).\n    Parameters\n    __________\n    N : number of directions, default is 100\n\n    Returns\n    ______\n    array of nested dec,inc pairs\n    \"\"\"\n#\n# get uniform directions  [dec,inc]\n    z = random.uniform(-1., 1., size=N)\n    t = random.uniform(0., 360., size=N)  # decs\n    i = np.arcsin(z) * 180. / np.pi  # incs\n    return np.array([t, i]).transpose()\n\n# def get_unf(N): #Jeff's way\n    \"\"\"\n     subroutine to retrieve N uniformly distributed directions\n    \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts 6 element s list to 3 a matrix", "response": "def s2a(s):\n    \"\"\"\n     convert 6 element \"s\" list to 3,3 a matrix (see Tauxe 1998)\n    \"\"\"\n    a = np.zeros((3, 3,), 'f')  # make the a matrix\n    for i in range(3):\n        a[i][i] = s[i]\n    a[0][1], a[1][0] = s[3], s[3]\n    a[1][2], a[2][1] = s[4], s[4]\n    a[0][2], a[2][0] = s[5], s[5]\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef a2s(a):\n    s = np.zeros((6,), 'f')  # make the a matrix\n    for i in range(3):\n        s[i] = a[i][i]\n    s[3] = a[0][1]\n    s[4] = a[1][2]\n    s[5] = a[0][2]\n    return s", "response": "convert 3 a matrix to 6 element list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string to a list of eigenvalues and eigenvectors and returns a tuple of the tau and the list of eigenvectors and directions", "response": "def doseigs(s):\n    \"\"\"\n    convert s format for eigenvalues and eigenvectors\n\n    Parameters\n    __________\n    s=[x11,x22,x33,x12,x23,x13] : the six tensor elements\n\n    Return\n    __________\n        tau : [t1,t2,t3]\n           tau is an list of eigenvalues in decreasing order:\n        V : [[V1_dec,V1_inc],[V2_dec,V2_inc],[V3_dec,V3_inc]]\n            is an list of the eigenvector directions\n    \"\"\"\n#\n    A = s2a(s)  # convert s to a (see Tauxe 1998)\n    tau, V = tauV(A)  # convert to eigenvalues (t), eigenvectors (V)\n    Vdirs = []\n    for v in V:  # convert from cartesian to direction\n        Vdir = cart2dir(v)\n        if Vdir[1] < 0:\n            Vdir[1] = -Vdir[1]\n            Vdir[0] = (Vdir[0] + 180.) % 360.\n        Vdirs.append([Vdir[0], Vdir[1]])\n    return tau, Vdirs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef doeigs_s(tau, Vdirs):\n    t = np.zeros((3, 3,), 'f')  # initialize the tau diagonal matrix\n    V = []\n    for j in range(3):\n        t[j][j] = tau[j]  # diagonalize tau\n    for k in range(3):\n        V.append(dir2cart([Vdirs[k][0], Vdirs[k][1], 1.0]))\n    V = np.transpose(V)\n    tmp = np.dot(V, t)\n    chi = np.dot(tmp, np.transpose(V))\n    return a2s(chi)", "response": "get elements of s from eigenvaulues"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fcalc(col, row):\n#\n    if row > 200:\n        row = 200\n    if col > 20:\n        col = 20\n    ftest = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n                      [1, 161.469, 199.493, 215.737, 224.5, 230.066, 234.001, 236.772, 238.949, 240.496, 241.838,\n                       242.968, 243.88, 244.798, 245.26, 245.956, 246.422, 246.89, 247.36, 247.596, 248.068],\n                      [2, 18.5128, 18.9995, 19.1642, 19.2467, 19.2969, 19.3299, 19.3536, 19.371, 19.3852, 19.3963,\n                       19.4043, 19.4122, 19.4186, 19.425, 19.4297, 19.4329, 19.4377, 19.4409, 19.4425, 19.4457],\n                      [3, 10.1278, 9.5522, 9.2767, 9.1173, 9.0133, 8.9408, 8.8868, 8.8452, 8.8124, 8.7857,\n                       8.7635, 8.7446, 8.7287, 8.715, 8.7028, 8.6923, 8.683, 8.6745, 8.667, 8.6602],\n                      [4, 7.7087, 6.9444, 6.5915, 6.3882, 6.2561, 6.1631, 6.0943, 6.0411, 5.9988, 5.9644,\n                       5.9359, 5.9117, 5.8912, 5.8733, 5.8578, 5.844, 5.8319, 5.8211, 5.8113, 5.8025],\n                      [5, 6.608, 5.7861, 5.4095, 5.1922, 5.0503, 4.9503, 4.8759, 4.8184, 4.7725, 4.735,\n                       4.7039, 4.6777, 4.6552, 4.6358, 4.6187, 4.6038, 4.5904, 4.5785, 4.5679, 4.5581],\n                      [6, 5.9874, 5.1433, 4.757, 4.5337, 4.3874, 4.2838, 4.2067, 4.1468, 4.099, 4.06,\n                       4.0275, 3.9999, 3.9764, 3.956, 3.9381, 3.9223, 3.9083, 3.8957, 3.8844, 3.8742],\n                      [7, 5.5914, 4.7374, 4.3469, 4.1204, 3.9715, 3.866, 3.787, 3.7257, 3.6767, 3.6366,\n                       3.603, 3.5747, 3.5504, 3.5292, 3.5107, 3.4944, 3.4799, 3.4669, 3.4552, 3.4445],\n                      [8, 5.3177, 4.459, 4.0662, 3.8378, 3.6875, 3.5806, 3.5004, 3.4381, 3.3881, 3.3472,\n                       3.313, 3.2839, 3.259, 3.2374, 3.2184, 3.2017, 3.1867, 3.1733, 3.1613, 3.1503],\n                      [9, 5.1174, 4.2565, 3.8626, 3.6331, 3.4817, 3.3738, 3.2928, 3.2296, 3.1789, 3.1373,\n                       3.1025, 3.0729, 3.0475, 3.0255, 3.0061, 2.989, 2.9737, 2.96, 2.9476, 2.9365],\n                      [10, 4.9647, 4.1028, 3.7083, 3.4781, 3.3258, 3.2171, 3.1355, 3.0717, 3.0204, 2.9782,\n                       2.9429, 2.913, 2.8872, 2.8648, 2.845, 2.8276, 2.812, 2.7981, 2.7855, 2.774],\n                      [11, 4.8443, 3.9823, 3.5875, 3.3567, 3.2039, 3.0946, 3.0123, 2.948, 2.8962, 2.8536,\n                       2.8179, 2.7876, 2.7614, 2.7386, 2.7186, 2.7009, 2.6851, 2.6709, 2.6581, 2.6464],\n                      [12, 4.7472, 3.8853, 3.4903, 3.2592, 3.1059, 2.9961, 2.9134, 2.8486, 2.7964, 2.7534,\n                       2.7173, 2.6866, 2.6602, 2.6371, 2.6169, 2.5989, 2.5828, 2.5684, 2.5554, 2.5436],\n                      [13, 4.6672, 3.8055, 3.4106, 3.1791, 3.0255, 2.9153, 2.8321, 2.7669, 2.7144, 2.6711,\n                       2.6347, 2.6037, 2.5769, 2.5536, 2.5331, 2.5149, 2.4987, 2.4841, 2.4709, 2.4589],\n                      [14, 4.6001, 3.7389, 3.3439, 3.1122, 2.9582, 2.8477, 2.7642, 2.6987, 2.6458, 2.6021,\n                       2.5655, 2.5343, 2.5073, 2.4837, 2.463, 2.4446, 2.4282, 2.4134, 2.4, 2.3879],\n                      [15, 4.543, 3.6824, 3.2874, 3.0555, 2.9013, 2.7905, 2.7066, 2.6408, 2.5877, 2.5437,\n                       2.5068, 2.4753, 2.4481, 2.4244, 2.4034, 2.3849, 2.3683, 2.3533, 2.3398, 2.3275],\n                      [16, 4.494, 3.6337, 3.2389, 3.0069, 2.8524, 2.7413, 2.6572, 2.5911, 2.5377, 2.4935,\n                       2.4564, 2.4247, 2.3973, 2.3733, 2.3522, 2.3335, 2.3167, 2.3016, 2.288, 2.2756],\n                      [17, 4.4513, 3.5916, 3.1968, 2.9647, 2.81, 2.6987, 2.6143, 2.548, 2.4943, 2.4499,\n                       2.4126, 2.3807, 2.3531, 2.329, 2.3077, 2.2888, 2.2719, 2.2567, 2.2429, 2.2303],\n                      [18, 4.4139, 3.5546, 3.1599, 2.9278, 2.7729, 2.6613, 2.5767, 2.5102, 2.4563, 2.4117,\n                       2.3742, 2.3421, 2.3143, 2.29, 2.2686, 2.2496, 2.2325, 2.2172, 2.2033, 2.1906],\n                      [19, 4.3808, 3.5219, 3.1274, 2.8951, 2.7401, 2.6283, 2.5435, 2.4768, 2.4227, 2.378,\n                       2.3402, 2.308, 2.28, 2.2556, 2.2341, 2.2149, 2.1977, 2.1823, 2.1683, 2.1555],\n                      [20, 4.3512, 3.4928, 3.0984, 2.8661, 2.7109, 2.599, 2.514, 2.4471, 2.3928, 2.3479,\n                       2.31, 2.2776, 2.2495, 2.2249, 2.2033, 2.184, 2.1667, 2.1511, 2.137, 2.1242],\n                      [21, 4.3248, 3.4668, 3.0725, 2.8401, 2.6848, 2.5727, 2.4876, 2.4205, 2.3661, 2.3209,\n                       2.2829, 2.2504, 2.2222, 2.1975, 2.1757, 2.1563, 2.1389, 2.1232, 2.109, 2.096],\n                      [22, 4.3009, 3.4434, 3.0492, 2.8167, 2.6613, 2.5491, 2.4638, 2.3965, 2.3419, 2.2967,\n                       2.2585, 2.2258, 2.1975, 2.1727, 2.1508, 2.1313, 2.1138, 2.098, 2.0837, 2.0707],\n                      [23, 4.2794, 3.4221, 3.028, 2.7955, 2.64, 2.5276, 2.4422, 2.3748, 2.3201, 2.2747,\n                       2.2364, 2.2036, 2.1752, 2.1503, 2.1282, 2.1086, 2.091, 2.0751, 2.0608, 2.0476],\n                      [24, 4.2597, 3.4029, 3.0088, 2.7763, 2.6206, 2.5082, 2.4226, 2.3551, 2.3003, 2.2547,\n                       2.2163, 2.1834, 2.1548, 2.1298, 2.1077, 2.088, 2.0703, 2.0543, 2.0399, 2.0267],\n                      [25, 4.2417, 3.3852, 2.9913, 2.7587, 2.603, 2.4904, 2.4047, 2.3371, 2.2821, 2.2365,\n                       2.1979, 2.1649, 2.1362, 2.1111, 2.0889, 2.0691, 2.0513, 2.0353, 2.0207, 2.0075],\n                      [26, 4.2252, 3.369, 2.9752, 2.7426, 2.5868, 2.4741, 2.3883, 2.3205, 2.2655, 2.2197,\n                       2.1811, 2.1479, 2.1192, 2.094, 2.0716, 2.0518, 2.0339, 2.0178, 2.0032, 1.9898],\n                      [27, 4.21, 3.3542, 2.9603, 2.7277, 2.5719, 2.4591, 2.3732, 2.3053, 2.2501, 2.2043,\n                       2.1656, 2.1323, 2.1035, 2.0782, 2.0558, 2.0358, 2.0179, 2.0017, 1.987, 1.9736],\n                      [28, 4.196, 3.3404, 2.9467, 2.7141, 2.5581, 2.4453, 2.3592, 2.2913, 2.236, 2.1901,\n                       2.1512, 2.1179, 2.0889, 2.0636, 2.0411, 2.021, 2.0031, 1.9868, 1.972, 1.9586],\n                      [29, 4.1829, 3.3276, 2.9341, 2.7014, 2.5454, 2.4324, 2.3463, 2.2783, 2.2229, 2.1768,\n                       2.1379, 2.1045, 2.0755, 2.05, 2.0275, 2.0074, 1.9893, 1.973, 1.9582, 1.9446],\n                      [30, 4.1709, 3.3158, 2.9223, 2.6896, 2.5335, 2.4205, 2.3343, 2.2662, 2.2107, 2.1646,\n                       2.1255, 2.0921, 2.0629, 2.0374, 2.0148, 1.9946, 1.9765, 1.9601, 1.9452, 1.9317],\n                      [31, 4.1597, 3.3048, 2.9113, 2.6787, 2.5225, 2.4094, 2.3232, 2.2549, 2.1994, 2.1531,\n                       2.1141, 2.0805, 2.0513, 2.0257, 2.003, 1.9828, 1.9646, 1.9481, 1.9332, 1.9196],\n                      [32, 4.1491, 3.2945, 2.9011, 2.6684, 2.5123, 2.3991, 2.3127, 2.2444, 2.1888, 2.1425,\n                       2.1033, 2.0697, 2.0404, 2.0147, 1.992, 1.9717, 1.9534, 1.9369, 1.9219, 1.9083],\n                      [33, 4.1392, 3.2849, 2.8915, 2.6589, 2.5027, 2.3894, 2.303, 2.2346, 2.1789, 2.1325,\n                       2.0933, 2.0596, 2.0302, 2.0045, 1.9817, 1.9613, 1.943, 1.9264, 1.9114, 1.8977],\n                      [34, 4.13, 3.2759, 2.8826, 2.6499, 2.4936, 2.3803, 2.2938, 2.2253, 2.1696, 2.1231,\n                       2.0838, 2.05, 2.0207, 1.9949, 1.972, 1.9516, 1.9332, 1.9166, 1.9015, 1.8877],\n                      [35, 4.1214, 3.2674, 2.8742, 2.6415, 2.4851, 2.3718, 2.2852, 2.2167, 2.1608, 2.1143,\n                       2.0749, 2.0411, 2.0117, 1.9858, 1.9629, 1.9424, 1.924, 1.9073, 1.8922, 1.8784],\n                      [36, 4.1132, 3.2594, 2.8663, 2.6335, 2.4771, 2.3637, 2.2771, 2.2085, 2.1526, 2.1061,\n                       2.0666, 2.0327, 2.0032, 1.9773, 1.9543, 1.9338, 1.9153, 1.8986, 1.8834, 1.8696],\n                      [37, 4.1055, 3.2519, 2.8588, 2.6261, 2.4696, 2.3562, 2.2695, 2.2008, 2.1449, 2.0982,\n                       2.0587, 2.0248, 1.9952, 1.9692, 1.9462, 1.9256, 1.9071, 1.8904, 1.8752, 1.8613],\n                      [38, 4.0981, 3.2448, 2.8517, 2.619, 2.4625, 2.349, 2.2623, 2.1935, 2.1375, 2.0909,\n                       2.0513, 2.0173, 1.9877, 1.9617, 1.9386, 1.9179, 1.8994, 1.8826, 1.8673, 1.8534],\n                      [39, 4.0913, 3.2381, 2.8451, 2.6123, 2.4558, 2.3422, 2.2555, 2.1867, 2.1306, 2.0839,\n                       2.0442, 2.0102, 1.9805, 1.9545, 1.9313, 1.9107, 1.8921, 1.8752, 1.8599, 1.8459],\n                      [40, 4.0848, 3.2317, 2.8388, 2.606, 2.4495, 2.3359, 2.249, 2.1802, 2.124, 2.0773,\n                       2.0376, 2.0035, 1.9738, 1.9476, 1.9245, 1.9038, 1.8851, 1.8682, 1.8529, 1.8389],\n                      [41, 4.0786, 3.2257, 2.8328, 2.6, 2.4434, 2.3298, 2.2429, 2.174, 2.1178, 2.071,\n                       2.0312, 1.9971, 1.9673, 1.9412, 1.9179, 1.8972, 1.8785, 1.8616, 1.8462, 1.8321],\n                      [42, 4.0727, 3.2199, 2.8271, 2.5943, 2.4377, 2.324, 2.2371, 2.1681, 2.1119, 2.065,\n                       2.0252, 1.991, 1.9612, 1.935, 1.9118, 1.8909, 1.8722, 1.8553, 1.8399, 1.8258],\n                      [43, 4.067, 3.2145, 2.8216, 2.5888, 2.4322, 2.3185, 2.2315, 2.1625, 2.1062, 2.0593,\n                       2.0195, 1.9852, 1.9554, 1.9292, 1.9059, 1.885, 1.8663, 1.8493, 1.8338, 1.8197],\n                      [44, 4.0617, 3.2093, 2.8165, 2.5837, 2.4271, 2.3133, 2.2262, 2.1572, 2.1009, 2.0539,\n                       2.014, 1.9797, 1.9499, 1.9236, 1.9002, 1.8794, 1.8606, 1.8436, 1.8281, 1.8139],\n                      [45, 4.0566, 3.2043, 2.8115, 2.5787, 2.4221, 2.3083, 2.2212, 2.1521, 2.0958, 2.0487,\n                       2.0088, 1.9745, 1.9446, 1.9182, 1.8949, 1.874, 1.8551, 1.8381, 1.8226, 1.8084],\n                      [46, 4.0518, 3.1996, 2.8068, 2.574, 2.4174, 2.3035, 2.2164, 2.1473, 2.0909, 2.0438,\n                       2.0039, 1.9695, 1.9395, 1.9132, 1.8898, 1.8688, 1.85, 1.8329, 1.8173, 1.8031],\n                      [47, 4.0471, 3.1951, 2.8024, 2.5695, 2.4128, 2.299, 2.2118, 2.1427, 2.0862, 2.0391,\n                       1.9991, 1.9647, 1.9347, 1.9083, 1.8849, 1.8639, 1.845, 1.8279, 1.8123, 1.798],\n                      [48, 4.0426, 3.1907, 2.7981, 2.5653, 2.4085, 2.2946, 2.2074, 2.1382, 2.0817, 2.0346,\n                       1.9946, 1.9601, 1.9301, 1.9037, 1.8802, 1.8592, 1.8402, 1.8231, 1.8075, 1.7932],\n                      [49, 4.0384, 3.1866, 2.7939, 2.5611, 2.4044, 2.2904, 2.2032, 2.134, 2.0774, 2.0303,\n                       1.9902, 1.9558, 1.9257, 1.8992, 1.8757, 1.8547, 1.8357, 1.8185, 1.8029, 1.7886],\n                      [50, 4.0343, 3.1826, 2.79, 2.5572, 2.4004, 2.2864, 2.1992, 2.1299, 2.0734, 2.0261,\n                       1.9861, 1.9515, 1.9214, 1.8949, 1.8714, 1.8503, 1.8313, 1.8141, 1.7985, 1.7841],\n                      [51, 4.0303, 3.1788, 2.7862, 2.5534, 2.3966, 2.2826, 2.1953, 2.126, 2.0694, 2.0222,\n                       1.982, 1.9475, 1.9174, 1.8908, 1.8673, 1.8462, 1.8272, 1.8099, 1.7942, 1.7798],\n                      [52, 4.0266, 3.1752, 2.7826, 2.5498, 2.3929, 2.2789, 2.1916, 2.1223, 2.0656, 2.0184,\n                       1.9782, 1.9436, 1.9134, 1.8869, 1.8633, 1.8422, 1.8231, 1.8059, 1.7901, 1.7758],\n                      [53, 4.023, 3.1716, 2.7791, 2.5463, 2.3894, 2.2754, 2.1881, 2.1187, 2.062, 2.0147,\n                       1.9745, 1.9399, 1.9097, 1.8831, 1.8595, 1.8383, 1.8193, 1.802, 1.7862, 1.7718],\n                      [54, 4.0196, 3.1683, 2.7757, 2.5429, 2.3861, 2.272, 2.1846, 2.1152, 2.0585, 2.0112,\n                       1.971, 1.9363, 1.9061, 1.8795, 1.8558, 1.8346, 1.8155, 1.7982, 1.7825, 1.768],\n                      [55, 4.0162, 3.165, 2.7725, 2.5397, 2.3828, 2.2687, 2.1813, 2.1119, 2.0552, 2.0078,\n                       1.9676, 1.9329, 1.9026, 1.876, 1.8523, 1.8311, 1.812, 1.7946, 1.7788, 1.7644],\n                      [56, 4.0129, 3.1618, 2.7694, 2.5366, 2.3797, 2.2656, 2.1781, 2.1087, 2.0519, 2.0045,\n                       1.9642, 1.9296, 1.8993, 1.8726, 1.8489, 1.8276, 1.8085, 1.7912, 1.7753, 1.7608],\n                      [57, 4.0099, 3.1589, 2.7665, 2.5336, 2.3767, 2.2625, 2.1751, 2.1056, 2.0488, 2.0014,\n                       1.9611, 1.9264, 1.896, 1.8693, 1.8456, 1.8244, 1.8052, 1.7878, 1.772, 1.7575],\n                      [58, 4.0069, 3.1559, 2.7635, 2.5307, 2.3738, 2.2596, 2.1721, 2.1026, 2.0458, 1.9983,\n                       1.958, 1.9233, 1.8929, 1.8662, 1.8424, 1.8212, 1.802, 1.7846, 1.7687, 1.7542],\n                      [59, 4.0039, 3.1531, 2.7608, 2.5279, 2.371, 2.2568, 2.1693, 2.0997, 2.0429, 1.9954,\n                       1.9551, 1.9203, 1.8899, 1.8632, 1.8394, 1.8181, 1.7989, 1.7815, 1.7656, 1.751],\n                      [60, 4.0012, 3.1504, 2.7581, 2.5252, 2.3683, 2.254, 2.1665, 2.097, 2.0401, 1.9926,\n                       1.9522, 1.9174, 1.887, 1.8603, 1.8364, 1.8151, 1.7959, 1.7784, 1.7625, 1.748],\n                      [61, 3.9985, 3.1478, 2.7555, 2.5226, 2.3657, 2.2514, 2.1639, 2.0943, 2.0374, 1.9899,\n                       1.9495, 1.9146, 1.8842, 1.8574, 1.8336, 1.8122, 1.793, 1.7755, 1.7596, 1.745],\n                      [62, 3.9959, 3.1453, 2.753, 2.5201, 2.3631, 2.2489, 2.1613, 2.0917, 2.0348, 1.9872,\n                       1.9468, 1.9119, 1.8815, 1.8547, 1.8308, 1.8095, 1.7902, 1.7727, 1.7568, 1.7422],\n                      [63, 3.9934, 3.1428, 2.7506, 2.5176, 2.3607, 2.2464, 2.1588, 2.0892, 2.0322, 1.9847,\n                       1.9442, 1.9093, 1.8789, 1.852, 1.8282, 1.8068, 1.7875, 1.77, 1.754, 1.7394],\n                      [64, 3.9909, 3.1404, 2.7482, 2.5153, 2.3583, 2.244, 2.1564, 2.0868, 2.0298, 1.9822,\n                       1.9417, 1.9068, 1.8763, 1.8495, 1.8256, 1.8042, 1.7849, 1.7673, 1.7514, 1.7368],\n                      [65, 3.9885, 3.1381, 2.7459, 2.513, 2.356, 2.2417, 2.1541, 2.0844, 2.0274, 1.9798,\n                       1.9393, 1.9044, 1.8739, 1.847, 1.8231, 1.8017, 1.7823, 1.7648, 1.7488, 1.7342],\n                      [66, 3.9862, 3.1359, 2.7437, 2.5108, 2.3538, 2.2395, 2.1518, 2.0821, 2.0251, 1.9775,\n                       1.937, 1.902, 1.8715, 1.8446, 1.8207, 1.7992, 1.7799, 1.7623, 1.7463, 1.7316],\n                      [67, 3.9841, 3.1338, 2.7416, 2.5087, 2.3516, 2.2373, 2.1497, 2.0799, 2.0229, 1.9752,\n                       1.9347, 1.8997, 1.8692, 1.8423, 1.8183, 1.7968, 1.7775, 1.7599, 1.7439, 1.7292],\n                      [68, 3.9819, 3.1317, 2.7395, 2.5066, 2.3496, 2.2352, 2.1475, 2.0778, 2.0207, 1.973,\n                       1.9325, 1.8975, 1.867, 1.84, 1.816, 1.7945, 1.7752, 1.7576, 1.7415, 1.7268],\n                      [69, 3.9798, 3.1297, 2.7375, 2.5046, 2.3475, 2.2332, 2.1455, 2.0757, 2.0186, 1.9709,\n                       1.9303, 1.8954, 1.8648, 1.8378, 1.8138, 1.7923, 1.7729, 1.7553, 1.7393, 1.7246],\n                      [70, 3.9778, 3.1277, 2.7355, 2.5027, 2.3456, 2.2312, 2.1435, 2.0737, 2.0166, 1.9689,\n                       1.9283, 1.8932, 1.8627, 1.8357, 1.8117, 1.7902, 1.7707, 1.7531, 1.7371, 1.7223],\n                      [71, 3.9758, 3.1258, 2.7336, 2.5007, 2.3437, 2.2293, 2.1415, 2.0717, 2.0146, 1.9669,\n                       1.9263, 1.8912, 1.8606, 1.8336, 1.8096, 1.7881, 1.7686, 1.751, 1.7349, 1.7202],\n                      [72, 3.9739, 3.1239, 2.7318, 2.4989, 2.3418, 2.2274, 2.1397, 2.0698, 2.0127, 1.9649,\n                       1.9243, 1.8892, 1.8586, 1.8316, 1.8076, 1.786, 1.7666, 1.7489, 1.7328, 1.7181],\n                      [73, 3.9721, 3.1221, 2.73, 2.4971, 2.34, 2.2256, 2.1378, 2.068, 2.0108, 1.9631,\n                       1.9224, 1.8873, 1.8567, 1.8297, 1.8056, 1.784, 1.7646, 1.7469, 1.7308, 1.716],\n                      [74, 3.9703, 3.1204, 2.7283, 2.4954, 2.3383, 2.2238, 2.1361, 2.0662, 2.009, 1.9612,\n                       1.9205, 1.8854, 1.8548, 1.8278, 1.8037, 1.7821, 1.7626, 1.7449, 1.7288, 1.714],\n                      [75, 3.9685, 3.1186, 2.7266, 2.4937, 2.3366, 2.2221, 2.1343, 2.0645, 2.0073, 1.9595,\n                       1.9188, 1.8836, 1.853, 1.8259, 1.8018, 1.7802, 1.7607, 1.7431, 1.7269, 1.7121],\n                      [76, 3.9668, 3.117, 2.7249, 2.4921, 2.3349, 2.2204, 2.1326, 2.0627, 2.0055, 1.9577,\n                       1.917, 1.8819, 1.8512, 1.8241, 1.8, 1.7784, 1.7589, 1.7412, 1.725, 1.7102],\n                      [77, 3.9651, 3.1154, 2.7233, 2.4904, 2.3333, 2.2188, 2.131, 2.0611, 2.0039, 1.956,\n                       1.9153, 1.8801, 1.8494, 1.8223, 1.7982, 1.7766, 1.7571, 1.7394, 1.7232, 1.7084],\n                      [78, 3.9635, 3.1138, 2.7218, 2.4889, 2.3318, 2.2172, 2.1294, 2.0595, 2.0022, 1.9544,\n                       1.9136, 1.8785, 1.8478, 1.8206, 1.7965, 1.7749, 1.7554, 1.7376, 1.7214, 1.7066],\n                      [79, 3.9619, 3.1123, 2.7203, 2.4874, 2.3302, 2.2157, 2.1279, 2.0579, 2.0006, 1.9528,\n                       1.912, 1.8769, 1.8461, 1.819, 1.7948, 1.7732, 1.7537, 1.7359, 1.7197, 1.7048],\n                      [80, 3.9604, 3.1107, 2.7188, 2.4859, 2.3287, 2.2142, 2.1263, 2.0564, 1.9991, 1.9512,\n                       1.9105, 1.8753, 1.8445, 1.8174, 1.7932, 1.7716, 1.752, 1.7342, 1.718, 1.7032],\n                      [81, 3.9589, 3.1093, 2.7173, 2.4845, 2.3273, 2.2127, 2.1248, 2.0549, 1.9976, 1.9497,\n                       1.9089, 1.8737, 1.8429, 1.8158, 1.7916, 1.77, 1.7504, 1.7326, 1.7164, 1.7015],\n                      [82, 3.9574, 3.1079, 2.716, 2.483, 2.3258, 2.2113, 2.1234, 2.0534, 1.9962, 1.9482,\n                       1.9074, 1.8722, 1.8414, 1.8143, 1.7901, 1.7684, 1.7488, 1.731, 1.7148, 1.6999],\n                      [83, 3.956, 3.1065, 2.7146, 2.4817, 2.3245, 2.2099, 2.122, 2.052, 1.9947, 1.9468,\n                       1.906, 1.8707, 1.8399, 1.8127, 1.7886, 1.7669, 1.7473, 1.7295, 1.7132, 1.6983],\n                      [84, 3.9546, 3.1051, 2.7132, 2.4803, 2.3231, 2.2086, 2.1206, 2.0506, 1.9933, 1.9454,\n                       1.9045, 1.8693, 1.8385, 1.8113, 1.7871, 1.7654, 1.7458, 1.728, 1.7117, 1.6968],\n                      [85, 3.9532, 3.1039, 2.7119, 2.479, 2.3218, 2.2072, 2.1193, 2.0493, 1.9919, 1.944,\n                       1.9031, 1.8679, 1.8371, 1.8099, 1.7856, 1.7639, 1.7443, 1.7265, 1.7102, 1.6953],\n                      [86, 3.9519, 3.1026, 2.7106, 2.4777, 2.3205, 2.2059, 2.118, 2.048, 1.9906, 1.9426,\n                       1.9018, 1.8665, 1.8357, 1.8085, 1.7842, 1.7625, 1.7429, 1.725, 1.7088, 1.6938],\n                      [87, 3.9506, 3.1013, 2.7094, 2.4765, 2.3193, 2.2047, 2.1167, 2.0467, 1.9893, 1.9413,\n                       1.9005, 1.8652, 1.8343, 1.8071, 1.7829, 1.7611, 1.7415, 1.7236, 1.7073, 1.6924],\n                      [88, 3.9493, 3.1001, 2.7082, 2.4753, 2.318, 2.2034, 2.1155, 2.0454, 1.9881, 1.94,\n                       1.8992, 1.8639, 1.833, 1.8058, 1.7815, 1.7598, 1.7401, 1.7223, 1.706, 1.691],\n                      [89, 3.9481, 3.0988, 2.707, 2.4741, 2.3169, 2.2022, 2.1143, 2.0442, 1.9868, 1.9388,\n                       1.8979, 1.8626, 1.8317, 1.8045, 1.7802, 1.7584, 1.7388, 1.7209, 1.7046, 1.6896],\n                      [90, 3.9469, 3.0977, 2.7058, 2.4729, 2.3157, 2.2011, 2.1131, 2.043, 1.9856, 1.9376,\n                       1.8967, 1.8613, 1.8305, 1.8032, 1.7789, 1.7571, 1.7375, 1.7196, 1.7033, 1.6883],\n                      [91, 3.9457, 3.0965, 2.7047, 2.4718, 2.3146, 2.1999, 2.1119, 2.0418, 1.9844, 1.9364,\n                       1.8955, 1.8601, 1.8292, 1.802, 1.7777, 1.7559, 1.7362, 1.7183, 1.702, 1.687],\n                      [92, 3.9446, 3.0955, 2.7036, 2.4707, 2.3134, 2.1988, 2.1108, 2.0407, 1.9833, 1.9352,\n                       1.8943, 1.8589, 1.828, 1.8008, 1.7764, 1.7546, 1.735, 1.717, 1.7007, 1.6857],\n                      [93, 3.9435, 3.0944, 2.7025, 2.4696, 2.3123, 2.1977, 2.1097, 2.0395, 1.9821, 1.934,\n                       1.8931, 1.8578, 1.8269, 1.7996, 1.7753, 1.7534, 1.7337, 1.7158, 1.6995, 1.6845],\n                      [94, 3.9423, 3.0933, 2.7014, 2.4685, 2.3113, 2.1966, 2.1086, 2.0385, 1.981, 1.9329,\n                       1.892, 1.8566, 1.8257, 1.7984, 1.7741, 1.7522, 1.7325, 1.7146, 1.6982, 1.6832],\n                      [95, 3.9412, 3.0922, 2.7004, 2.4675, 2.3102, 2.1955, 2.1075, 2.0374, 1.9799, 1.9318,\n                       1.8909, 1.8555, 1.8246, 1.7973, 1.7729, 1.7511, 1.7314, 1.7134, 1.6971, 1.682],\n                      [96, 3.9402, 3.0912, 2.6994, 2.4665, 2.3092, 2.1945, 2.1065, 2.0363, 1.9789, 1.9308,\n                       1.8898, 1.8544, 1.8235, 1.7961, 1.7718, 1.75, 1.7302, 1.7123, 1.6959, 1.6809],\n                      [97, 3.9392, 3.0902, 2.6984, 2.4655, 2.3082, 2.1935, 2.1054, 2.0353, 1.9778, 1.9297,\n                       1.8888, 1.8533, 1.8224, 1.7951, 1.7707, 1.7488, 1.7291, 1.7112, 1.6948, 1.6797],\n                      [98, 3.9381, 3.0892, 2.6974, 2.4645, 2.3072, 2.1925, 2.1044, 2.0343, 1.9768, 1.9287,\n                       1.8877, 1.8523, 1.8213, 1.794, 1.7696, 1.7478, 1.728, 1.71, 1.6936, 1.6786],\n                      [99, 3.9371, 3.0882, 2.6965, 2.4636, 2.3062, 2.1916, 2.1035, 2.0333, 1.9758, 1.9277,\n                       1.8867, 1.8513, 1.8203, 1.7929, 1.7686, 1.7467, 1.7269, 1.709, 1.6926, 1.6775],\n                      [100, 3.9361, 3.0873, 2.6955, 2.4626, 2.3053, 2.1906, 2.1025, 2.0323, 1.9748, 1.9267,\n                       1.8857, 1.8502, 1.8193, 1.7919, 1.7675, 1.7456, 1.7259, 1.7079, 1.6915, 1.6764],\n                      [101, 3.9352, 3.0864, 2.6946, 2.4617, 2.3044, 2.1897, 2.1016, 2.0314, 1.9739, 1.9257,\n                       1.8847, 1.8493, 1.8183, 1.7909, 1.7665, 1.7446, 1.7248, 1.7069, 1.6904, 1.6754],\n                      [102, 3.9342, 3.0854, 2.6937, 2.4608, 2.3035, 2.1888, 2.1007, 2.0304, 1.9729, 1.9248,\n                       1.8838, 1.8483, 1.8173, 1.7899, 1.7655, 1.7436, 1.7238, 1.7058, 1.6894, 1.6744],\n                      [103, 3.9333, 3.0846, 2.6928, 2.4599, 2.3026, 2.1879, 2.0997, 2.0295, 1.972, 1.9238,\n                       1.8828, 1.8474, 1.8163, 1.789, 1.7645, 1.7427, 1.7229, 1.7048, 1.6884, 1.6733],\n                      [104, 3.9325, 3.0837, 2.692, 2.4591, 2.3017, 2.187, 2.0989, 2.0287, 1.9711, 1.9229,\n                       1.8819, 1.8464, 1.8154, 1.788, 1.7636, 1.7417, 1.7219, 1.7039, 1.6874, 1.6723],\n                      [105, 3.9316, 3.0828, 2.6912, 2.4582, 2.3009, 2.1861, 2.098, 2.0278, 1.9702, 1.922,\n                       1.881, 1.8455, 1.8145, 1.7871, 1.7627, 1.7407, 1.7209, 1.7029, 1.6865, 1.6714],\n                      [106, 3.9307, 3.082, 2.6903, 2.4574, 2.3, 2.1853, 2.0971, 2.0269, 1.9694, 1.9212,\n                       1.8801, 1.8446, 1.8136, 1.7862, 1.7618, 1.7398, 1.72, 1.702, 1.6855, 1.6704],\n                      [107, 3.9299, 3.0812, 2.6895, 2.4566, 2.2992, 2.1845, 2.0963, 2.0261, 1.9685, 1.9203,\n                       1.8792, 1.8438, 1.8127, 1.7853, 1.7608, 1.7389, 1.7191, 1.7011, 1.6846, 1.6695],\n                      [108, 3.929, 3.0804, 2.6887, 2.4558, 2.2984, 2.1837, 2.0955, 2.0252, 1.9677, 1.9195,\n                       1.8784, 1.8429, 1.8118, 1.7844, 1.7599, 1.738, 1.7182, 1.7001, 1.6837, 1.6685],\n                      [109, 3.9282, 3.0796, 2.6879, 2.455, 2.2976, 2.1828, 2.0947, 2.0244, 1.9669, 1.9186,\n                       1.8776, 1.8421, 1.811, 1.7835, 1.7591, 1.7371, 1.7173, 1.6992, 1.6828, 1.6676],\n                      [110, 3.9274, 3.0788, 2.6872, 2.4542, 2.2968, 2.1821, 2.0939, 2.0236, 1.9661, 1.9178,\n                       1.8767, 1.8412, 1.8102, 1.7827, 1.7582, 1.7363, 1.7164, 1.6984, 1.6819, 1.6667],\n                      [111, 3.9266, 3.0781, 2.6864, 2.4535, 2.2961, 2.1813, 2.0931, 2.0229, 1.9653, 1.917,\n                       1.8759, 1.8404, 1.8093, 1.7819, 1.7574, 1.7354, 1.7156, 1.6975, 1.681, 1.6659],\n                      [112, 3.9258, 3.0773, 2.6857, 2.4527, 2.2954, 2.1806, 2.0924, 2.0221, 1.9645, 1.9163,\n                       1.8751, 1.8396, 1.8085, 1.7811, 1.7566, 1.7346, 1.7147, 1.6967, 1.6802, 1.665],\n                      [113, 3.9251, 3.0766, 2.6849, 2.452, 2.2946, 2.1798, 2.0916, 2.0213, 1.9637, 1.9155,\n                       1.8744, 1.8388, 1.8077, 1.7803, 1.7558, 1.7338, 1.7139, 1.6958, 1.6793, 1.6642],\n                      [114, 3.9243, 3.0758, 2.6842, 2.4513, 2.2939, 2.1791, 2.0909, 2.0206, 1.963, 1.9147,\n                       1.8736, 1.8381, 1.8069, 1.7795, 1.755, 1.733, 1.7131, 1.695, 1.6785, 1.6633],\n                      [115, 3.9236, 3.0751, 2.6835, 2.4506, 2.2932, 2.1784, 2.0902, 2.0199, 1.9623, 1.914,\n                       1.8729, 1.8373, 1.8062, 1.7787, 1.7542, 1.7322, 1.7123, 1.6942, 1.6777, 1.6625],\n                      [116, 3.9228, 3.0744, 2.6828, 2.4499, 2.2925, 2.1777, 2.0895, 2.0192, 1.9615, 1.9132,\n                       1.8721, 1.8365, 1.8054, 1.7779, 1.7534, 1.7314, 1.7115, 1.6934, 1.6769, 1.6617],\n                      [117, 3.9222, 3.0738, 2.6821, 2.4492, 2.2918, 2.177, 2.0888, 2.0185, 1.9608, 1.9125,\n                       1.8714, 1.8358, 1.8047, 1.7772, 1.7527, 1.7307, 1.7108, 1.6927, 1.6761, 1.6609],\n                      [118, 3.9215, 3.0731, 2.6815, 2.4485, 2.2912, 2.1763, 2.0881, 2.0178, 1.9601, 1.9118,\n                       1.8707, 1.8351, 1.804, 1.7765, 1.752, 1.7299, 1.71, 1.6919, 1.6754, 1.6602],\n                      [119, 3.9208, 3.0724, 2.6808, 2.4479, 2.2905, 2.1757, 2.0874, 2.0171, 1.9594, 1.9111,\n                       1.87, 1.8344, 1.8032, 1.7757, 1.7512, 1.7292, 1.7093, 1.6912, 1.6746, 1.6594],\n                      [120, 3.9202, 3.0718, 2.6802, 2.4472, 2.2899, 2.175, 2.0868, 2.0164, 1.9588, 1.9105,\n                       1.8693, 1.8337, 1.8026, 1.775, 1.7505, 1.7285, 1.7085, 1.6904, 1.6739, 1.6587],\n                      [121, 3.9194, 3.0712, 2.6795, 2.4466, 2.2892, 2.1744, 2.0861, 2.0158, 1.9581, 1.9098,\n                       1.8686, 1.833, 1.8019, 1.7743, 1.7498, 1.7278, 1.7078, 1.6897, 1.6732, 1.6579],\n                      [122, 3.9188, 3.0705, 2.6789, 2.446, 2.2886, 2.1737, 2.0855, 2.0151, 1.9575, 1.9091,\n                       1.868, 1.8324, 1.8012, 1.7736, 1.7491, 1.727, 1.7071, 1.689, 1.6724, 1.6572],\n                      [123, 3.9181, 3.0699, 2.6783, 2.4454, 2.288, 2.1731, 2.0849, 2.0145, 1.9568, 1.9085,\n                       1.8673, 1.8317, 1.8005, 1.773, 1.7484, 1.7264, 1.7064, 1.6883, 1.6717, 1.6565],\n                      [124, 3.9176, 3.0693, 2.6777, 2.4448, 2.2874, 2.1725, 2.0842, 2.0139, 1.9562, 1.9078,\n                       1.8667, 1.831, 1.7999, 1.7723, 1.7478, 1.7257, 1.7058, 1.6876, 1.6711, 1.6558],\n                      [125, 3.9169, 3.0687, 2.6771, 2.4442, 2.2868, 2.1719, 2.0836, 2.0133, 1.9556, 1.9072,\n                       1.866, 1.8304, 1.7992, 1.7717, 1.7471, 1.725, 1.7051, 1.6869, 1.6704, 1.6551],\n                      [126, 3.9163, 3.0681, 2.6765, 2.4436, 2.2862, 2.1713, 2.083, 2.0126, 1.955, 1.9066,\n                       1.8654, 1.8298, 1.7986, 1.771, 1.7464, 1.7244, 1.7044, 1.6863, 1.6697, 1.6544],\n                      [127, 3.9157, 3.0675, 2.6759, 2.443, 2.2856, 2.1707, 2.0824, 2.0121, 1.9544, 1.906,\n                       1.8648, 1.8291, 1.7979, 1.7704, 1.7458, 1.7237, 1.7038, 1.6856, 1.669, 1.6538],\n                      [128, 3.9151, 3.0669, 2.6754, 2.4424, 2.285, 2.1701, 2.0819, 2.0115, 1.9538, 1.9054,\n                       1.8642, 1.8285, 1.7974, 1.7698, 1.7452, 1.7231, 1.7031, 1.685, 1.6684, 1.6531],\n                      [129, 3.9145, 3.0664, 2.6749, 2.4419, 2.2845, 2.1696, 2.0813, 2.0109, 1.9532, 1.9048,\n                       1.8636, 1.828, 1.7967, 1.7692, 1.7446, 1.7225, 1.7025, 1.6843, 1.6677, 1.6525],\n                      [130, 3.914, 3.0659, 2.6743, 2.4414, 2.2839, 2.169, 2.0807, 2.0103, 1.9526, 1.9042,\n                       1.863, 1.8273, 1.7962, 1.7685, 1.744, 1.7219, 1.7019, 1.6837, 1.6671, 1.6519],\n                      [131, 3.9134, 3.0653, 2.6737, 2.4408, 2.2834, 2.1685, 2.0802, 2.0098, 1.9521, 1.9037,\n                       1.8624, 1.8268, 1.7956, 1.768, 1.7434, 1.7213, 1.7013, 1.6831, 1.6665, 1.6513],\n                      [132, 3.9129, 3.0648, 2.6732, 2.4403, 2.2829, 2.168, 2.0796, 2.0092, 1.9515, 1.9031,\n                       1.8619, 1.8262, 1.795, 1.7674, 1.7428, 1.7207, 1.7007, 1.6825, 1.6659, 1.6506],\n                      [133, 3.9123, 3.0642, 2.6727, 2.4398, 2.2823, 2.1674, 2.0791, 2.0087, 1.951, 1.9026,\n                       1.8613, 1.8256, 1.7944, 1.7668, 1.7422, 1.7201, 1.7001, 1.6819, 1.6653, 1.65],\n                      [134, 3.9118, 3.0637, 2.6722, 2.4392, 2.2818, 2.1669, 2.0786, 2.0082, 1.9504, 1.902,\n                       1.8608, 1.8251, 1.7939, 1.7662, 1.7416, 1.7195, 1.6995, 1.6813, 1.6647, 1.6494],\n                      [135, 3.9112, 3.0632, 2.6717, 2.4387, 2.2813, 2.1664, 2.0781, 2.0076, 1.9499, 1.9015,\n                       1.8602, 1.8245, 1.7933, 1.7657, 1.7411, 1.719, 1.6989, 1.6808, 1.6641, 1.6488],\n                      [136, 3.9108, 3.0627, 2.6712, 2.4382, 2.2808, 2.1659, 2.0775, 2.0071, 1.9494, 1.901,\n                       1.8597, 1.824, 1.7928, 1.7651, 1.7405, 1.7184, 1.6984, 1.6802, 1.6635, 1.6483],\n                      [137, 3.9102, 3.0622, 2.6707, 2.4378, 2.2803, 2.1654, 2.077, 2.0066, 1.9488, 1.9004,\n                       1.8592, 1.8235, 1.7922, 1.7646, 1.74, 1.7178, 1.6978, 1.6796, 1.663, 1.6477],\n                      [138, 3.9098, 3.0617, 2.6702, 2.4373, 2.2798, 2.1649, 2.0766, 2.0061, 1.9483, 1.8999,\n                       1.8586, 1.823, 1.7917, 1.7641, 1.7394, 1.7173, 1.6973, 1.6791, 1.6624, 1.6471],\n                      [139, 3.9092, 3.0613, 2.6697, 2.4368, 2.2794, 2.1644, 2.0761, 2.0056, 1.9478, 1.8994,\n                       1.8581, 1.8224, 1.7912, 1.7635, 1.7389, 1.7168, 1.6967, 1.6785, 1.6619, 1.6466],\n                      [140, 3.9087, 3.0608, 2.6692, 2.4363, 2.2789, 2.1639, 2.0756, 2.0051, 1.9473, 1.8989,\n                       1.8576, 1.8219, 1.7907, 1.763, 1.7384, 1.7162, 1.6962, 1.678, 1.6613, 1.646],\n                      [141, 3.9083, 3.0603, 2.6688, 2.4359, 2.2784, 2.1634, 2.0751, 2.0046, 1.9469, 1.8984,\n                       1.8571, 1.8214, 1.7901, 1.7625, 1.7379, 1.7157, 1.6957, 1.6775, 1.6608, 1.6455],\n                      [142, 3.9078, 3.0598, 2.6683, 2.4354, 2.2779, 2.163, 2.0747, 2.0042, 1.9464, 1.8979,\n                       1.8566, 1.8209, 1.7897, 1.762, 1.7374, 1.7152, 1.6952, 1.6769, 1.6603, 1.645],\n                      [143, 3.9073, 3.0594, 2.6679, 2.435, 2.2775, 2.1625, 2.0742, 2.0037, 1.9459, 1.8975,\n                       1.8562, 1.8204, 1.7892, 1.7615, 1.7368, 1.7147, 1.6946, 1.6764, 1.6598, 1.6444],\n                      [144, 3.9068, 3.0589, 2.6675, 2.4345, 2.277, 2.1621, 2.0737, 2.0033, 1.9455, 1.897,\n                       1.8557, 1.82, 1.7887, 1.761, 1.7364, 1.7142, 1.6941, 1.6759, 1.6592, 1.6439],\n                      [145, 3.9064, 3.0585, 2.667, 2.4341, 2.2766, 2.1617, 2.0733, 2.0028, 1.945, 1.8965,\n                       1.8552, 1.8195, 1.7882, 1.7605, 1.7359, 1.7137, 1.6936, 1.6754, 1.6587, 1.6434],\n                      [146, 3.906, 3.0581, 2.6666, 2.4337, 2.2762, 2.1612, 2.0728, 2.0024, 1.9445, 1.8961,\n                       1.8548, 1.819, 1.7877, 1.7601, 1.7354, 1.7132, 1.6932, 1.6749, 1.6582, 1.6429],\n                      [147, 3.9055, 3.0576, 2.6662, 2.4332, 2.2758, 2.1608, 2.0724, 2.0019, 1.9441, 1.8956,\n                       1.8543, 1.8186, 1.7873, 1.7596, 1.7349, 1.7127, 1.6927, 1.6744, 1.6578, 1.6424],\n                      [148, 3.9051, 3.0572, 2.6657, 2.4328, 2.2753, 2.1604, 2.072, 2.0015, 1.9437, 1.8952,\n                       1.8539, 1.8181, 1.7868, 1.7591, 1.7344, 1.7123, 1.6922, 1.6739, 1.6573, 1.6419],\n                      [149, 3.9046, 3.0568, 2.6653, 2.4324, 2.2749, 2.1599, 2.0716, 2.0011, 1.9432, 1.8947,\n                       1.8534, 1.8177, 1.7864, 1.7587, 1.734, 1.7118, 1.6917, 1.6735, 1.6568, 1.6414],\n                      [150, 3.9042, 3.0564, 2.6649, 2.4319, 2.2745, 2.1595, 2.0711, 2.0006, 1.9428, 1.8943,\n                       1.853, 1.8172, 1.7859, 1.7582, 1.7335, 1.7113, 1.6913, 1.673, 1.6563, 1.641],\n                      [151, 3.9038, 3.056, 2.6645, 2.4315, 2.2741, 2.1591, 2.0707, 2.0002, 1.9424, 1.8939,\n                       1.8526, 1.8168, 1.7855, 1.7578, 1.7331, 1.7109, 1.6908, 1.6726, 1.6558, 1.6405],\n                      [152, 3.9033, 3.0555, 2.6641, 2.4312, 2.2737, 2.1587, 2.0703, 1.9998, 1.942, 1.8935,\n                       1.8521, 1.8163, 1.785, 1.7573, 1.7326, 1.7104, 1.6904, 1.6721, 1.6554, 1.64],\n                      [153, 3.903, 3.0552, 2.6637, 2.4308, 2.2733, 2.1583, 2.0699, 1.9994, 1.9416, 1.8931,\n                       1.8517, 1.8159, 1.7846, 1.7569, 1.7322, 1.71, 1.6899, 1.6717, 1.6549, 1.6396],\n                      [154, 3.9026, 3.0548, 2.6634, 2.4304, 2.2729, 2.1579, 2.0695, 1.999, 1.9412, 1.8926,\n                       1.8513, 1.8155, 1.7842, 1.7565, 1.7318, 1.7096, 1.6895, 1.6712, 1.6545, 1.6391],\n                      [155, 3.9021, 3.0544, 2.6629, 2.43, 2.2725, 2.1575, 2.0691, 1.9986, 1.9407, 1.8923,\n                       1.8509, 1.8151, 1.7838, 1.7561, 1.7314, 1.7091, 1.6891, 1.6708, 1.654, 1.6387],\n                      [156, 3.9018, 3.054, 2.6626, 2.4296, 2.2722, 2.1571, 2.0687, 1.9982, 1.9403, 1.8918,\n                       1.8505, 1.8147, 1.7834, 1.7557, 1.7309, 1.7087, 1.6886, 1.6703, 1.6536, 1.6383],\n                      [157, 3.9014, 3.0537, 2.6622, 2.4293, 2.2717, 2.1568, 2.0684, 1.9978, 1.94, 1.8915,\n                       1.8501, 1.8143, 1.7829, 1.7552, 1.7305, 1.7083, 1.6882, 1.6699, 1.6532, 1.6378],\n                      [158, 3.901, 3.0533, 2.6618, 2.4289, 2.2714, 2.1564, 2.068, 1.9974, 1.9396, 1.8911,\n                       1.8497, 1.8139, 1.7826, 1.7548, 1.7301, 1.7079, 1.6878, 1.6695, 1.6528, 1.6374],\n                      [159, 3.9006, 3.0529, 2.6615, 2.4285, 2.271, 2.156, 2.0676, 1.997, 1.9392, 1.8907,\n                       1.8493, 1.8135, 1.7822, 1.7544, 1.7297, 1.7075, 1.6874, 1.6691, 1.6524, 1.637],\n                      [160, 3.9002, 3.0525, 2.6611, 2.4282, 2.2706, 2.1556, 2.0672, 1.9967, 1.9388, 1.8903,\n                       1.8489, 1.8131, 1.7818, 1.754, 1.7293, 1.7071, 1.687, 1.6687, 1.6519, 1.6366],\n                      [161, 3.8998, 3.0522, 2.6607, 2.4278, 2.2703, 2.1553, 2.0669, 1.9963, 1.9385, 1.8899,\n                       1.8485, 1.8127, 1.7814, 1.7537, 1.7289, 1.7067, 1.6866, 1.6683, 1.6515, 1.6361],\n                      [162, 3.8995, 3.0518, 2.6604, 2.4275, 2.27, 2.155, 2.0665, 1.9959, 1.9381, 1.8895,\n                       1.8482, 1.8124, 1.781, 1.7533, 1.7285, 1.7063, 1.6862, 1.6679, 1.6511, 1.6357],\n                      [163, 3.8991, 3.0515, 2.6601, 2.4271, 2.2696, 2.1546, 2.0662, 1.9956, 1.9377, 1.8892,\n                       1.8478, 1.812, 1.7806, 1.7529, 1.7282, 1.7059, 1.6858, 1.6675, 1.6507, 1.6353],\n                      [164, 3.8987, 3.0512, 2.6597, 2.4268, 2.2693, 2.1542, 2.0658, 1.9953, 1.9374, 1.8888,\n                       1.8474, 1.8116, 1.7803, 1.7525, 1.7278, 1.7055, 1.6854, 1.6671, 1.6503, 1.6349],\n                      [165, 3.8985, 3.0508, 2.6594, 2.4264, 2.2689, 2.1539, 2.0655, 1.9949, 1.937, 1.8885,\n                       1.8471, 1.8112, 1.7799, 1.7522, 1.7274, 1.7052, 1.685, 1.6667, 1.6499, 1.6345],\n                      [166, 3.8981, 3.0505, 2.6591, 2.4261, 2.2686, 2.1536, 2.0651, 1.9945, 1.9367, 1.8881,\n                       1.8467, 1.8109, 1.7795, 1.7518, 1.727, 1.7048, 1.6846, 1.6663, 1.6496, 1.6341],\n                      [167, 3.8977, 3.0502, 2.6587, 2.4258, 2.2683, 2.1533, 2.0648, 1.9942, 1.9363, 1.8878,\n                       1.8464, 1.8105, 1.7792, 1.7514, 1.7266, 1.7044, 1.6843, 1.6659, 1.6492, 1.6338],\n                      [168, 3.8974, 3.0498, 2.6584, 2.4254, 2.268, 2.1529, 2.0645, 1.9939, 1.936, 1.8874,\n                       1.846, 1.8102, 1.7788, 1.7511, 1.7263, 1.704, 1.6839, 1.6656, 1.6488, 1.6334],\n                      [169, 3.8971, 3.0495, 2.6581, 2.4251, 2.2676, 2.1526, 2.0641, 1.9936, 1.9357, 1.8871,\n                       1.8457, 1.8099, 1.7785, 1.7507, 1.7259, 1.7037, 1.6835, 1.6652, 1.6484, 1.633],\n                      [170, 3.8967, 3.0492, 2.6578, 2.4248, 2.2673, 2.1523, 2.0638, 1.9932, 1.9353, 1.8868,\n                       1.8454, 1.8095, 1.7781, 1.7504, 1.7256, 1.7033, 1.6832, 1.6648, 1.6481, 1.6326],\n                      [171, 3.8965, 3.0488, 2.6575, 2.4245, 2.267, 2.152, 2.0635, 1.9929, 1.935, 1.8864,\n                       1.845, 1.8092, 1.7778, 1.75, 1.7252, 1.703, 1.6828, 1.6645, 1.6477, 1.6323],\n                      [172, 3.8961, 3.0485, 2.6571, 2.4242, 2.2667, 2.1516, 2.0632, 1.9926, 1.9347, 1.8861,\n                       1.8447, 1.8088, 1.7774, 1.7497, 1.7249, 1.7026, 1.6825, 1.6641, 1.6473, 1.6319],\n                      [173, 3.8958, 3.0482, 2.6568, 2.4239, 2.2664, 2.1513, 2.0628, 1.9923, 1.9343, 1.8858,\n                       1.8443, 1.8085, 1.7771, 1.7493, 1.7246, 1.7023, 1.6821, 1.6638, 1.647, 1.6316],\n                      [174, 3.8954, 3.0479, 2.6566, 2.4236, 2.266, 2.151, 2.0626, 1.9919, 1.934, 1.8855,\n                       1.844, 1.8082, 1.7768, 1.749, 1.7242, 1.7019, 1.6818, 1.6634, 1.6466, 1.6312],\n                      [175, 3.8952, 3.0476, 2.6563, 2.4233, 2.2658, 2.1507, 2.0622, 1.9916, 1.9337, 1.8852,\n                       1.8437, 1.8078, 1.7764, 1.7487, 1.7239, 1.7016, 1.6814, 1.6631, 1.6463, 1.6309],\n                      [176, 3.8948, 3.0473, 2.6559, 2.423, 2.2655, 2.1504, 2.0619, 1.9913, 1.9334, 1.8848,\n                       1.8434, 1.8075, 1.7761, 1.7483, 1.7236, 1.7013, 1.6811, 1.6628, 1.646, 1.6305],\n                      [177, 3.8945, 3.047, 2.6556, 2.4227, 2.2652, 2.1501, 2.0616, 1.991, 1.9331, 1.8845,\n                       1.8431, 1.8072, 1.7758, 1.748, 1.7232, 1.7009, 1.6808, 1.6624, 1.6456, 1.6302],\n                      [178, 3.8943, 3.0467, 2.6554, 2.4224, 2.2649, 2.1498, 2.0613, 1.9907, 1.9328, 1.8842,\n                       1.8428, 1.8069, 1.7755, 1.7477, 1.7229, 1.7006, 1.6805, 1.6621, 1.6453, 1.6298],\n                      [179, 3.8939, 3.0465, 2.6551, 2.4221, 2.2646, 2.1495, 2.0611, 1.9904, 1.9325, 1.8839,\n                       1.8425, 1.8066, 1.7752, 1.7474, 1.7226, 1.7003, 1.6801, 1.6618, 1.645, 1.6295],\n                      [180, 3.8936, 3.0462, 2.6548, 2.4218, 2.2643, 2.1492, 2.0608, 1.9901, 1.9322, 1.8836,\n                       1.8422, 1.8063, 1.7749, 1.7471, 1.7223, 1.7, 1.6798, 1.6614, 1.6446, 1.6292],\n                      [181, 3.8933, 3.0458, 2.6545, 2.4216, 2.264, 2.149, 2.0605, 1.9899, 1.9319, 1.8833,\n                       1.8419, 1.806, 1.7746, 1.7468, 1.7219, 1.6997, 1.6795, 1.6611, 1.6443, 1.6289],\n                      [182, 3.8931, 3.0456, 2.6543, 2.4213, 2.2638, 2.1487, 2.0602, 1.9896, 1.9316, 1.883,\n                       1.8416, 1.8057, 1.7743, 1.7465, 1.7217, 1.6994, 1.6792, 1.6608, 1.644, 1.6286],\n                      [183, 3.8928, 3.0453, 2.654, 2.421, 2.2635, 2.1484, 2.0599, 1.9893, 1.9313, 1.8827,\n                       1.8413, 1.8054, 1.774, 1.7462, 1.7214, 1.6991, 1.6789, 1.6605, 1.6437, 1.6282],\n                      [184, 3.8925, 3.045, 2.6537, 2.4207, 2.2632, 2.1481, 2.0596, 1.989, 1.9311, 1.8825,\n                       1.841, 1.8051, 1.7737, 1.7459, 1.721, 1.6987, 1.6786, 1.6602, 1.6434, 1.6279],\n                      [185, 3.8923, 3.0448, 2.6534, 2.4205, 2.263, 2.1479, 2.0594, 1.9887, 1.9308, 1.8822,\n                       1.8407, 1.8048, 1.7734, 1.7456, 1.7208, 1.6984, 1.6783, 1.6599, 1.643, 1.6276],\n                      [186, 3.892, 3.0445, 2.6531, 2.4202, 2.2627, 2.1476, 2.0591, 1.9885, 1.9305, 1.8819,\n                       1.8404, 1.8045, 1.7731, 1.7453, 1.7205, 1.6981, 1.678, 1.6596, 1.6428, 1.6273],\n                      [187, 3.8917, 3.0442, 2.6529, 2.4199, 2.2624, 2.1473, 2.0588, 1.9882, 1.9302, 1.8816,\n                       1.8401, 1.8042, 1.7728, 1.745, 1.7202, 1.6979, 1.6777, 1.6593, 1.6424, 1.627],\n                      [188, 3.8914, 3.044, 2.6526, 2.4197, 2.2621, 2.1471, 2.0586, 1.9879, 1.9299, 1.8814,\n                       1.8399, 1.804, 1.7725, 1.7447, 1.7199, 1.6976, 1.6774, 1.659, 1.6421, 1.6267],\n                      [189, 3.8912, 3.0437, 2.6524, 2.4195, 2.2619, 2.1468, 2.0583, 1.9877, 1.9297, 1.8811,\n                       1.8396, 1.8037, 1.7722, 1.7444, 1.7196, 1.6973, 1.6771, 1.6587, 1.6418, 1.6264],\n                      [190, 3.8909, 3.0435, 2.6521, 2.4192, 2.2617, 2.1466, 2.0581, 1.9874, 1.9294, 1.8808,\n                       1.8393, 1.8034, 1.772, 1.7441, 1.7193, 1.697, 1.6768, 1.6584, 1.6416, 1.6261],\n                      [191, 3.8906, 3.0432, 2.6519, 2.4189, 2.2614, 2.1463, 2.0578, 1.9871, 1.9292, 1.8805,\n                       1.8391, 1.8032, 1.7717, 1.7439, 1.719, 1.6967, 1.6765, 1.6581, 1.6413, 1.6258],\n                      [192, 3.8903, 3.043, 2.6516, 2.4187, 2.2611, 2.1461, 2.0575, 1.9869, 1.9289, 1.8803,\n                       1.8388, 1.8029, 1.7714, 1.7436, 1.7188, 1.6964, 1.6762, 1.6578, 1.641, 1.6255],\n                      [193, 3.8901, 3.0427, 2.6514, 2.4184, 2.2609, 2.1458, 2.0573, 1.9866, 1.9286, 1.88,\n                       1.8385, 1.8026, 1.7712, 1.7433, 1.7185, 1.6961, 1.6759, 1.6575, 1.6407, 1.6252],\n                      [194, 3.8899, 3.0425, 2.6512, 2.4182, 2.2606, 2.1456, 2.057, 1.9864, 1.9284, 1.8798,\n                       1.8383, 1.8023, 1.7709, 1.7431, 1.7182, 1.6959, 1.6757, 1.6572, 1.6404, 1.6249],\n                      [195, 3.8896, 3.0422, 2.6509, 2.418, 2.2604, 2.1453, 2.0568, 1.9861, 1.9281, 1.8795,\n                       1.838, 1.8021, 1.7706, 1.7428, 1.7179, 1.6956, 1.6754, 1.657, 1.6401, 1.6247],\n                      [196, 3.8893, 3.042, 2.6507, 2.4177, 2.2602, 2.1451, 2.0566, 1.9859, 1.9279, 1.8793,\n                       1.8377, 1.8018, 1.7704, 1.7425, 1.7177, 1.6953, 1.6751, 1.6567, 1.6399, 1.6244],\n                      [197, 3.8891, 3.0418, 2.6504, 2.4175, 2.26, 2.1448, 2.0563, 1.9856, 1.9277, 1.879,\n                       1.8375, 1.8016, 1.7701, 1.7423, 1.7174, 1.6951, 1.6748, 1.6564, 1.6396, 1.6241],\n                      [198, 3.8889, 3.0415, 2.6502, 2.4173, 2.2597, 2.1446, 2.0561, 1.9854, 1.9274, 1.8788,\n                       1.8373, 1.8013, 1.7699, 1.742, 1.7172, 1.6948, 1.6746, 1.6562, 1.6393, 1.6238],\n                      [199, 3.8886, 3.0413, 2.65, 2.417, 2.2595, 2.1444, 2.0558, 1.9852, 1.9272, 1.8785,\n                       1.837, 1.8011, 1.7696, 1.7418, 1.7169, 1.6946, 1.6743, 1.6559, 1.6391, 1.6236],\n                      [200, 3.8883, 3.041, 2.6497, 2.4168, 2.2592, 2.1441, 2.0556, 1.9849, 1.9269, 1.8783, 1.8368, 1.8008, 1.7694, 1.7415, 1.7166, 1.6943, 1.6741, 1.6557, 1.6388, 1.62]])\n    return ftest[int(row)][int(col)]", "response": "Calculates the F - test for a given set of degrees of freedom."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tcalc(nf, p):\n#\n    if p == .05:\n        if nf > 2:\n            t = 4.3027\n        if nf > 3:\n            t = 3.1824\n        if nf > 4:\n            t = 2.7765\n        if nf > 5:\n            t = 2.5706\n        if nf > 6:\n            t = 2.4469\n        if nf > 7:\n            t = 2.3646\n        if nf > 8:\n            t = 2.3060\n        if nf > 9:\n            t = 2.2622\n        if nf > 10:\n            t = 2.2281\n        if nf > 11:\n            t = 2.2010\n        if nf > 12:\n            t = 2.1788\n        if nf > 13:\n            t = 2.1604\n        if nf > 14:\n            t = 2.1448\n        if nf > 15:\n            t = 2.1315\n        if nf > 16:\n            t = 2.1199\n        if nf > 17:\n            t = 2.1098\n        if nf > 18:\n            t = 2.1009\n        if nf > 19:\n            t = 2.0930\n        if nf > 20:\n            t = 2.0860\n        if nf > 21:\n            t = 2.0796\n        if nf > 22:\n            t = 2.0739\n        if nf > 23:\n            t = 2.0687\n        if nf > 24:\n            t = 2.0639\n        if nf > 25:\n            t = 2.0595\n        if nf > 26:\n            t = 2.0555\n        if nf > 27:\n            t = 2.0518\n        if nf > 28:\n            t = 2.0484\n        if nf > 29:\n            t = 2.0452\n        if nf > 30:\n            t = 2.0423\n        if nf > 31:\n            t = 2.0395\n        if nf > 32:\n            t = 2.0369\n        if nf > 33:\n            t = 2.0345\n        if nf > 34:\n            t = 2.0322\n        if nf > 35:\n            t = 2.0301\n        if nf > 36:\n            t = 2.0281\n        if nf > 37:\n            t = 2.0262\n        if nf > 38:\n            t = 2.0244\n        if nf > 39:\n            t = 2.0227\n        if nf > 40:\n            t = 2.0211\n        if nf > 41:\n            t = 2.0195\n        if nf > 42:\n            t = 2.0181\n        if nf > 43:\n            t = 2.0167\n        if nf > 44:\n            t = 2.0154\n        if nf > 45:\n            t = 2.0141\n        if nf > 46:\n            t = 2.0129\n        if nf > 47:\n            t = 2.0117\n        if nf > 48:\n            t = 2.0106\n        if nf > 49:\n            t = 2.0096\n        if nf > 50:\n            t = 2.0086\n        if nf > 51:\n            t = 2.0076\n        if nf > 52:\n            t = 2.0066\n        if nf > 53:\n            t = 2.0057\n        if nf > 54:\n            t = 2.0049\n        if nf > 55:\n            t = 2.0040\n        if nf > 56:\n            t = 2.0032\n        if nf > 57:\n            t = 2.0025\n        if nf > 58:\n            t = 2.0017\n        if nf > 59:\n            t = 2.0010\n        if nf > 60:\n            t = 2.0003\n        if nf > 61:\n            t = 1.9996\n        if nf > 62:\n            t = 1.9990\n        if nf > 63:\n            t = 1.9983\n        if nf > 64:\n            t = 1.9977\n        if nf > 65:\n            t = 1.9971\n        if nf > 66:\n            t = 1.9966\n        if nf > 67:\n            t = 1.9960\n        if nf > 68:\n            t = 1.9955\n        if nf > 69:\n            t = 1.9949\n        if nf > 70:\n            t = 1.9944\n        if nf > 71:\n            t = 1.9939\n        if nf > 72:\n            t = 1.9935\n        if nf > 73:\n            t = 1.9930\n        if nf > 74:\n            t = 1.9925\n        if nf > 75:\n            t = 1.9921\n        if nf > 76:\n            t = 1.9917\n        if nf > 77:\n            t = 1.9913\n        if nf > 78:\n            t = 1.9908\n        if nf > 79:\n            t = 1.9905\n        if nf > 80:\n            t = 1.9901\n        if nf > 81:\n            t = 1.9897\n        if nf > 82:\n            t = 1.9893\n        if nf > 83:\n            t = 1.9890\n        if nf > 84:\n            t = 1.9886\n        if nf > 85:\n            t = 1.9883\n        if nf > 86:\n            t = 1.9879\n        if nf > 87:\n            t = 1.9876\n        if nf > 88:\n            t = 1.9873\n        if nf > 89:\n            t = 1.9870\n        if nf > 90:\n            t = 1.9867\n        if nf > 91:\n            t = 1.9864\n        if nf > 92:\n            t = 1.9861\n        if nf > 93:\n            t = 1.9858\n        if nf > 94:\n            t = 1.9855\n        if nf > 95:\n            t = 1.9852\n        if nf > 96:\n            t = 1.9850\n        if nf > 97:\n            t = 1.9847\n        if nf > 98:\n            t = 1.9845\n        if nf > 99:\n            t = 1.9842\n        if nf > 100:\n            t = 1.9840\n        return t\n#\n    elif p == .01:\n        if nf > 2:\n            t = 9.9250\n        if nf > 3:\n            t = 5.8408\n        if nf > 4:\n            t = 4.6041\n        if nf > 5:\n            t = 4.0321\n        if nf > 6:\n            t = 3.7074\n        if nf > 7:\n            t = 3.4995\n        if nf > 8:\n            t = 3.3554\n        if nf > 9:\n            t = 3.2498\n        if nf > 10:\n            t = 3.1693\n        if nf > 11:\n            t = 3.1058\n        if nf > 12:\n            t = 3.0545\n        if nf > 13:\n            t = 3.0123\n        if nf > 14:\n            t = 2.9768\n        if nf > 15:\n            t = 2.9467\n        if nf > 16:\n            t = 2.9208\n        if nf > 17:\n            t = 2.8982\n        if nf > 18:\n            t = 2.8784\n        if nf > 19:\n            t = 2.8609\n        if nf > 20:\n            t = 2.8453\n        if nf > 21:\n            t = 2.8314\n        if nf > 22:\n            t = 2.8188\n        if nf > 23:\n            t = 2.8073\n        if nf > 24:\n            t = 2.7970\n        if nf > 25:\n            t = 2.7874\n        if nf > 26:\n            t = 2.7787\n        if nf > 27:\n            t = 2.7707\n        if nf > 28:\n            t = 2.7633\n        if nf > 29:\n            t = 2.7564\n        if nf > 30:\n            t = 2.7500\n        if nf > 31:\n            t = 2.7440\n        if nf > 32:\n            t = 2.7385\n        if nf > 33:\n            t = 2.7333\n        if nf > 34:\n            t = 2.7284\n        if nf > 35:\n            t = 2.7238\n        if nf > 36:\n            t = 2.7195\n        if nf > 37:\n            t = 2.7154\n        if nf > 38:\n            t = 2.7116\n        if nf > 39:\n            t = 2.7079\n        if nf > 40:\n            t = 2.7045\n        if nf > 41:\n            t = 2.7012\n        if nf > 42:\n            t = 2.6981\n        if nf > 43:\n            t = 2.6951\n        if nf > 44:\n            t = 2.6923\n        if nf > 45:\n            t = 2.6896\n        if nf > 46:\n            t = 2.6870\n        if nf > 47:\n            t = 2.6846\n        if nf > 48:\n            t = 2.6822\n        if nf > 49:\n            t = 2.6800\n        if nf > 50:\n            t = 2.6778\n        if nf > 51:\n            t = 2.6757\n        if nf > 52:\n            t = 2.6737\n        if nf > 53:\n            t = 2.6718\n        if nf > 54:\n            t = 2.6700\n        if nf > 55:\n            t = 2.6682\n        if nf > 56:\n            t = 2.6665\n        if nf > 57:\n            t = 2.6649\n        if nf > 58:\n            t = 2.6633\n        if nf > 59:\n            t = 2.6618\n        if nf > 60:\n            t = 2.6603\n        if nf > 61:\n            t = 2.6589\n        if nf > 62:\n            t = 2.6575\n        if nf > 63:\n            t = 2.6561\n        if nf > 64:\n            t = 2.6549\n        if nf > 65:\n            t = 2.6536\n        if nf > 66:\n            t = 2.6524\n        if nf > 67:\n            t = 2.6512\n        if nf > 68:\n            t = 2.6501\n        if nf > 69:\n            t = 2.6490\n        if nf > 70:\n            t = 2.6479\n        if nf > 71:\n            t = 2.6469\n        if nf > 72:\n            t = 2.6458\n        if nf > 73:\n            t = 2.6449\n        if nf > 74:\n            t = 2.6439\n        if nf > 75:\n            t = 2.6430\n        if nf > 76:\n            t = 2.6421\n        if nf > 77:\n            t = 2.6412\n        if nf > 78:\n            t = 2.6403\n        if nf > 79:\n            t = 2.6395\n        if nf > 80:\n            t = 2.6387\n        if nf > 81:\n            t = 2.6379\n        if nf > 82:\n            t = 2.6371\n        if nf > 83:\n            t = 2.6364\n        if nf > 84:\n            t = 2.6356\n        if nf > 85:\n            t = 2.6349\n        if nf > 86:\n            t = 2.6342\n        if nf > 87:\n            t = 2.6335\n        if nf > 88:\n            t = 2.6329\n        if nf > 89:\n            t = 2.6322\n        if nf > 90:\n            t = 2.6316\n        if nf > 91:\n            t = 2.6309\n        if nf > 92:\n            t = 2.6303\n        if nf > 93:\n            t = 2.6297\n        if nf > 94:\n            t = 2.6291\n        if nf > 95:\n            t = 2.6286\n        if nf > 96:\n            t = 2.6280\n        if nf > 97:\n            t = 2.6275\n        if nf > 98:\n            t = 2.6269\n        if nf > 99:\n            t = 2.6264\n        if nf > 100:\n            t = 2.6259\n        return t\n        return t\n    else:\n        return 0", "response": "calculate t - table for nf degrees of freedom"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating average s sigma from list of s.", "response": "def sbar(Ss):\n    \"\"\"\n    calculate average s,sigma from list of \"s\"s.\n    \"\"\"\n    if type(Ss) == list:\n        Ss = np.array(Ss)\n    npts = Ss.shape[0]\n    Ss = Ss.transpose()\n\n    avd, avs = [], []\n    # D=np.array([Ss[0],Ss[1],Ss[2],Ss[3]+0.5*(Ss[0]+Ss[1]),Ss[4]+0.5*(Ss[1]+Ss[2]),Ss[5]+0.5*(Ss[0]+Ss[2])]).transpose()\n    D = np.array([Ss[0], Ss[1], Ss[2], Ss[3] + 0.5 * (Ss[0] + Ss[1]),\n                  Ss[4] + 0.5 * (Ss[1] + Ss[2]), Ss[5] + 0.5 * (Ss[0] + Ss[2])])\n    for j in range(6):\n        avd.append(np.average(D[j]))\n        avs.append(np.average(Ss[j]))\n    D = D.transpose()\n    # for s in Ss:\n    #    print 'from sbar: ',s\n    #    D.append(s[:]) # append a copy of s\n    #    D[-1][3]=D[-1][3]+0.5*(s[0]+s[1])\n    #    D[-1][4]=D[-1][4]+0.5*(s[1]+s[2])\n    #    D[-1][5]=D[-1][5]+0.5*(s[0]+s[2])\n    #    for j in range(6):\n    #        avd[j]+=(D[-1][j])/float(npts)\n    #        avs[j]+=(s[j])/float(npts)\n#   calculate sigma\n    nf = (npts - 1) * 6  # number of degrees of freedom\n    s0 = 0\n    Dels = (D - avd)**2\n    s0 = np.sum(Dels)\n    sigma = np.sqrt(s0/float(nf))\n    return nf, sigma, avs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the hext parameters for nf sigma and s", "response": "def dohext(nf, sigma, s):\n    \"\"\"\n    calculates hext parameters for nf, sigma and s\n\n    Parameters\n    __________\n    nf :  number of degrees of freedom (measurements - 6)\n    sigma : the sigma of the measurements\n    s : [x11,x22,x33,x12,x23,x13] - the six tensor elements\n\n    Return\n    hpars : dictionary of Hext statistics with keys:\n        'F_crit' : critical value for anisotropy\n        'F12_crit' : critical value for tau1>tau2, tau2>3\n        'F' : value of F\n        'F12' : value of F12\n        'F23' : value of F23\n        'v1_dec': declination of principal eigenvector\n        'v1_inc': inclination of principal eigenvector\n        'v2_dec': declination of major eigenvector\n        'v2_inc': inclination of major eigenvector\n        'v3_dec': declination of minor eigenvector\n        'v3_inc': inclination of minor eigenvector\n        't1': principal eigenvalue\n        't2': major eigenvalue\n        't3': minor eigenvalue\n        'e12': angle of confidence ellipse of principal eigenvector in direction of major eigenvector\n        'e23': angle of confidence ellipse of major eigenvector in direction of minor eigenvector\n        'e13': angle of confidence ellipse of principal eigenvector in direction of minor eigenvector\n\n    If working with data set with no sigmas and the average is desired, use nf,sigma,avs=pmag.sbar(Ss) as input\n\n    \"\"\"\n\n#\n    hpars = {}\n    hpars['F_crit'] = '0'\n    hpars['F12_crit'] = '0'\n    hpars[\"F\"] = 0\n    hpars[\"F12\"] = 0\n    hpars[\"F23\"] = 0\n    hpars[\"v1_dec\"] = -1\n    hpars[\"v1_inc\"] = -1\n    hpars[\"v2_dec\"] = -1\n    hpars[\"v2_inc\"] = -1\n    hpars[\"v3_dec\"] = -1\n    hpars[\"v3_inc\"] = -1\n    hpars[\"t1\"] = -1\n    hpars[\"t2\"] = -1\n    hpars[\"t3\"] = -1\n    hpars[\"e12\"] = -1\n    hpars[\"e23\"] = -1\n    hpars[\"e13\"] = -1\n    if nf < 0 or sigma == 0:\n        return hpars\n    f = np.sqrt(2. * fcalc(2, nf))\n    t2sum = 0\n    tau, Vdir = doseigs(s)\n    for i in range(3):\n        t2sum += tau[i]**2\n    chibar = old_div((s[0] + s[1] + s[2]), 3.)\n    hpars['F_crit'] = '%s' % (fcalc(5, nf))\n    hpars['F12_crit'] = '%s' % (fcalc(2, nf))\n    hpars[\"F\"] = 0.4 * (t2sum - 3 * chibar**2) / (sigma**2)\n    hpars[\"F12\"] = 0.5 * (old_div((tau[0] - tau[1]), sigma))**2\n    hpars[\"F23\"] = 0.5 * (old_div((tau[1] - tau[2]), sigma))**2\n    hpars[\"v1_dec\"] = Vdir[0][0]\n    hpars[\"v1_inc\"] = Vdir[0][1]\n    hpars[\"v2_dec\"] = Vdir[1][0]\n    hpars[\"v2_inc\"] = Vdir[1][1]\n    hpars[\"v3_dec\"] = Vdir[2][0]\n    hpars[\"v3_inc\"] = Vdir[2][1]\n    hpars[\"t1\"] = tau[0]\n    hpars[\"t2\"] = tau[1]\n    hpars[\"t3\"] = tau[2]\n    hpars[\"e12\"] = np.arctan(\n        old_div((f * sigma), (2 * abs(tau[0] - tau[1])))) * 180. / np.pi\n    hpars[\"e23\"] = np.arctan(\n        old_div((f * sigma), (2 * abs(tau[1] - tau[2])))) * 180. / np.pi\n    hpars[\"e13\"] = np.arctan(\n        old_div((f * sigma), (2 * abs(tau[0] - tau[2])))) * 180. / np.pi\n    return hpars"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a design matrix for an anisotropy experiment", "response": "def design(npos):\n    \"\"\"\n     make a design matrix for an anisotropy experiment\n    \"\"\"\n    if npos == 15:\n        #\n        # rotatable design of Jelinek for kappabridge (see Tauxe, 1998)\n        #\n        A = np.array([[.5, .5, 0, -1., 0, 0], [.5, .5, 0, 1., 0, 0], [1, .0, 0, 0, 0, 0], [.5, .5, 0, -1., 0, 0], [.5, .5, 0, 1., 0, 0], [0, .5, .5, 0, -1., 0], [0, .5, .5, 0, 1., 0], [0, 1., 0, 0, 0, 0],\n                      [0, .5, .5, 0, -1., 0], [0, .5, .5, 0, 1., 0], [.5, 0, .5, 0, 0, -1.], [.5, 0, .5, 0, 0, 1.], [0, 0, 1., 0, 0, 0], [.5, 0, .5, 0, 0, -1.], [.5, 0, .5, 0, 0, 1.]])  # design matrix for 15 measurment positions\n    elif npos == 6:\n        A = np.array([[1., 0, 0, 0, 0, 0], [0, 1., 0, 0, 0, 0], [0, 0, 1., 0, 0, 0], [.5, .5, 0, 1., 0, 0], [\n                     0, .5, .5, 0, 1., 0], [.5, 0, .5, 0, 0, 1.]])  # design matrix for 6 measurment positions\n\n    else:\n        print(\"measurement protocol not supported yet \")\n        return\n    B = np.dot(np.transpose(A), A)\n    B = linalg.inv(B)\n    B = np.dot(B, np.transpose(A))\n    return A, B"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates least - squares matrix for 15 measurements from Jelinek [ 1976 ]", "response": "def dok15_s(k15):\n    \"\"\"\n    calculates least-squares matrix for 15 measurements from Jelinek [1976]\n    \"\"\"\n#\n    A, B = design(15)  # get design matrix for 15 measurements\n    sbar = np.dot(B, k15)  # get mean s\n    t = (sbar[0] + sbar[1] + sbar[2])  # trace\n    bulk = old_div(t, 3.)  # bulk susceptibility\n    Kbar = np.dot(A, sbar)  # get best fit values for K\n    dels = k15 - Kbar  # get deltas\n    dels, sbar = old_div(dels, t), old_div(sbar, t)  # normalize by trace\n    So = sum(dels**2)\n    sigma = np.sqrt(old_div(So, 9.))  # standard deviation\n    return sbar, sigma, bulk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the cross product of two vectors v and w", "response": "def cross(v, w):\n    \"\"\"\n     cross product of two vectors\n    \"\"\"\n    x = v[1] * w[2] - v[2] * w[1]\n    y = v[2] * w[0] - v[0] * w[2]\n    z = v[0] * w[1] - v[1] * w[0]\n    return [x, y, z]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrotate s to az pl", "response": "def dosgeo(s, az, pl):\n    \"\"\"\n    rotates  matrix a to az,pl returns  s\n    Parameters\n    __________\n    s : [x11,x22,x33,x12,x23,x13] - the six tensor elements\n    az : the azimuth of the specimen X direction\n    pl : the plunge (inclination) of the specimen X direction\n\n    Return\n    s_rot : [x11,x22,x33,x12,x23,x13] - after rotation\n    \"\"\"\n#\n    a = s2a(s)  # convert to 3,3 matrix\n#  first get three orthogonal axes\n    X1 = dir2cart((az, pl, 1.))\n    X2 = dir2cart((az + 90, 0., 1.))\n    X3 = cross(X1, X2)\n    A = np.transpose([X1, X2, X3])\n    b = np.zeros((3, 3,), 'f')  # initiale the b matrix\n    for i in range(3):\n        for j in range(3):\n            dum = 0\n            for k in range(3):\n                for l in range(3):\n                    dum += A[i][k] * A[j][l] * a[k][l]\n            b[i][j] = dum\n    s_rot = a2s(b)  # afer rotation\n    return s_rot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dostilt(s, bed_az, bed_dip):\n    tau, Vdirs = doseigs(s)\n    Vrot = []\n    for evec in Vdirs:\n        d, i = dotilt(evec[0], evec[1], bed_az, bed_dip)\n        Vrot.append([d, i])\n    s_rot = doeigs_s(tau, Vrot)\n    return s_rot", "response": "Rotates s tensor to stratigraphic coordinates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apseudo(Ss, ipar, sigma):\n#\n    Is = random.randint(0, len(Ss) - 1, size=len(Ss))  # draw N random integers\n    #Ss = np.array(Ss)\n    if not ipar: # ipar == 0:\n        BSs = Ss[Is]\n    else:  # need to recreate measurement - then do the parametric stuffr\n        A, B = design(6)  # get the design matrix for 6 measurementsa\n        K, BSs = [], []\n        for k in range(len(Ss)):\n            K.append(np.dot(A, Ss[k][0:6]))\n        Pars = np.random.normal(K, sigma)\n        for k in range(len(Ss)):\n            BSs.append(np.dot(B, Pars[k]))\n    return np.array(BSs)", "response": "draw a bootstrap sample of Ss"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget bootstrap parameters for s data", "response": "def sbootpars(Taus, Vs):\n    \"\"\"\n     get bootstrap parameters for s data\n    \"\"\"\n#\n    Tau1s, Tau2s, Tau3s = [], [], []\n    V1s, V2s, V3s = [], [], []\n    nb = len(Taus)\n    bpars = {}\n    for k in range(nb):\n        Tau1s.append(Taus[k][0])\n        Tau2s.append(Taus[k][1])\n        Tau3s.append(Taus[k][2])\n        V1s.append(Vs[k][0])\n        V2s.append(Vs[k][1])\n        V3s.append(Vs[k][2])\n    x, sig = gausspars(Tau1s)\n    bpars[\"t1_sigma\"] = sig\n    x, sig = gausspars(Tau2s)\n    bpars[\"t2_sigma\"] = sig\n    x, sig = gausspars(Tau3s)\n    bpars[\"t3_sigma\"] = sig\n    V1s=flip(V1s,combine=True)\n    kpars = dokent(V1s, len(V1s))\n    bpars[\"v1_dec\"] = kpars[\"dec\"]\n    bpars[\"v1_inc\"] = kpars[\"inc\"]\n    bpars[\"v1_zeta\"] = (kpars[\"Zeta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v1_eta\"] = (kpars[\"Eta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v1_zeta_dec\"] = kpars[\"Zdec\"]\n    bpars[\"v1_zeta_inc\"] = kpars[\"Zinc\"]\n    bpars[\"v1_eta_dec\"] = kpars[\"Edec\"]\n    bpars[\"v1_eta_inc\"] = kpars[\"Einc\"]\n    V2s=flip(V2s,combine=True)\n    kpars = dokent(V2s, len(V2s))\n    bpars[\"v2_dec\"] = kpars[\"dec\"]\n    bpars[\"v2_inc\"] = kpars[\"inc\"]\n    bpars[\"v2_zeta\"] = (kpars[\"Zeta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v2_eta\"] = (kpars[\"Eta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v2_zeta_dec\"] = kpars[\"Zdec\"]\n    bpars[\"v2_zeta_inc\"] = kpars[\"Zinc\"]\n    bpars[\"v2_eta_dec\"] = kpars[\"Edec\"]\n    bpars[\"v2_eta_inc\"] = kpars[\"Einc\"]\n    V3s=flip(V3s,combine=True)\n    kpars = dokent(V3s, len(V3s))\n    bpars[\"v3_dec\"] = kpars[\"dec\"]\n    bpars[\"v3_inc\"] = kpars[\"inc\"]\n    bpars[\"v3_zeta\"] = (kpars[\"Zeta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v3_eta\"] = (kpars[\"Eta\"] * np.sqrt(nb)) % 360.\n    bpars[\"v3_zeta_dec\"] = kpars[\"Zdec\"]\n    bpars[\"v3_zeta_inc\"] = kpars[\"Zinc\"]\n    bpars[\"v3_eta_dec\"] = kpars[\"Edec\"]\n    bpars[\"v3_eta_inc\"] = kpars[\"Einc\"]\n    return bpars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning bootstrap parameters for S data Parameters __________ Ss : nested array of [[x11 x22 x33 x12 x23 x13],....] data ipar : if True, do a parametric bootstrap nb : number of bootstraps Returns ________ Tmean : average eigenvalues Vmean : average eigvectors Taus : bootstrapped eigenvalues Vs : bootstrapped eigenvectors", "response": "def s_boot(Ss, ipar=0, nb=1000):\n    \"\"\"\n    Returns bootstrap parameters for S data\n\n    Parameters\n    __________\n    Ss : nested array of [[x11 x22 x33 x12 x23 x13],....] data\n    ipar : if True, do a parametric bootstrap\n    nb : number of bootstraps\n\n    Returns\n    ________\n    Tmean : average eigenvalues\n    Vmean : average eigvectors\n    Taus : bootstrapped eigenvalues\n    Vs :  bootstrapped eigenvectors\n\n    \"\"\"\n    #npts = len(Ss)\n    Ss = np.array(Ss)\n    npts = Ss.shape[0]\n# get average s for whole dataset\n    nf, Sigma, avs = sbar(Ss)\n    Tmean, Vmean = doseigs(avs)  # get eigenvectors of mean tensor\n#\n# now do bootstrap to collect Vs and taus of bootstrap means\n#\n    Taus, Vs = [], []  # number of bootstraps, list of bootstrap taus and eigenvectors\n#\n    for k in range(int(float(nb))):  # repeat nb times\n        #        if k%50==0:print k,' out of ',nb\n        # get a pseudosample - if ipar=1, do a parametric bootstrap\n        BSs = apseudo(Ss, ipar, Sigma)\n        nf, sigma, avbs = sbar(BSs)  # get bootstrap mean s\n        tau, Vdirs = doseigs(avbs)  # get bootstrap eigenparameters\n        Taus.append(tau)\n        Vs.append(Vdirs)\n    return Tmean, Vmean, Taus, Vs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef designAARM(npos):\n    #\n    \"\"\"\n    calculates B matrix for AARM calculations.\n    \"\"\"\n    if npos != 9:\n        print('Sorry - only 9 positions available')\n        return\n    Dec = [315., 225., 180., 135., 45., 90., 270.,\n           270., 270., 90., 0., 0., 0., 180., 180.]\n    Dip = [0., 0., 0., 0., 0., -45., -45., 0.,\n           45., 45., 45., -45., -90., -45., 45.]\n    index9 = [0, 1, 2, 5, 6, 7, 10, 11, 12]\n    H = []\n    for ind in range(15):\n        Dir = [Dec[ind], Dip[ind], 1.]\n        H.append(dir2cart(Dir))  # 15 field directionss\n#\n# make design matrix A\n#\n    A = np.zeros((npos * 3, 6), 'f')\n    tmpH = np.zeros((npos, 3), 'f')  # define tmpH\n    if npos == 9:\n        for i in range(9):\n            k = index9[i]\n            ind = i * 3\n            A[ind][0] = H[k][0]\n            A[ind][3] = H[k][1]\n            A[ind][5] = H[k][2]\n            ind = i * 3 + 1\n            A[ind][3] = H[k][0]\n            A[ind][1] = H[k][1]\n            A[ind][4] = H[k][2]\n            ind = i * 3 + 2\n            A[ind][5] = H[k][0]\n            A[ind][4] = H[k][1]\n            A[ind][2] = H[k][2]\n            for j in range(3):\n                tmpH[i][j] = H[k][j]\n        At = np.transpose(A)\n        ATA = np.dot(At, A)\n        ATAI = linalg.inv(ATA)\n        B = np.dot(ATAI, At)\n    else:\n        print(\"B matrix not yet supported\")\n        return\n    return B, H, tmpH", "response": "Calculates AARM matrix for AARM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a magic record into the domagic mag format", "response": "def domagicmag(file, Recs):\n    \"\"\"\n    converts a magic record back into the SIO mag format\n    \"\"\"\n    for rec in Recs:\n        type = \".0\"\n        meths = []\n        tmp = rec[\"magic_method_codes\"].split(':')\n        for meth in tmp:\n            meths.append(meth.strip())\n        if 'LT-T-I' in meths:\n            type = \".1\"\n        if 'LT-PTRM-I' in meths:\n            type = \".2\"\n        if 'LT-PTRM-MD' in meths:\n            type = \".3\"\n        treatment = float(rec[\"treatment_temp\"]) - 273\n        tr = '%i' % (treatment) + type\n        inten = '%8.7e ' % (float(rec[\"measurement_magn_moment\"]) * 1e3)\n        outstring = rec[\"er_specimen_name\"] + \" \" + tr + \" \" + rec[\"measurement_csd\"] + \\\n            \" \" + inten + \" \" + rec[\"measurement_dec\"] + \\\n            \" \" + rec[\"measurement_inc\"] + \"\\n\"\n        file.write(outstring)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncleans up unbalanced steps failure can be from missing steps", "response": "def cleanup(first_I, first_Z):\n    \"\"\"\n     cleans up unbalanced steps\n     failure can be from unbalanced final step, or from missing steps,\n     this takes care of  missing steps\n    \"\"\"\n    cont = 0\n    Nmin = len(first_I)\n    if len(first_Z) < Nmin:\n        Nmin = len(first_Z)\n    for kk in range(Nmin):\n        if first_I[kk][0] != first_Z[kk][0]:\n            print(\"\\n WARNING: \")\n            if first_I[kk] < first_Z[kk]:\n                del first_I[kk]\n            else:\n                del first_Z[kk]\n            print(\"Unmatched step number: \", kk + 1, '  ignored')\n            cont = 1\n        if cont == 1:\n            return first_I, first_Z, cont\n    return first_I, first_Z, cont"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sortarai(datablock, s, Zdiff, **kwargs):\n    if 'version' in list(kwargs.keys()) and kwargs['version'] == 3:\n        dec_key, inc_key = 'dir_dec', 'dir_inc'\n        Mkeys = ['magn_moment', 'magn_volume', 'magn_mass', 'magnitude']\n        meth_key = 'method_codes'\n        temp_key, dc_key = 'treat_temp', 'treat_dc_field'\n        dc_theta_key, dc_phi_key = 'treat_dc_field_theta', 'treat_dc_field_phi'\n        # convert dataframe to list of dictionaries\n        datablock = datablock.to_dict('records')\n    else:\n        dec_key, inc_key = 'measurement_dec', 'measurement_inc'\n        Mkeys = ['measurement_magn_moment', 'measurement_magn_volume',\n                 'measurement_magn_mass', 'measurement_magnitude']\n        meth_key = 'magic_method_codes'\n        temp_key, dc_key = 'treatment_temp', 'treatment_dc_field'\n        dc_theta_key, dc_phi_key = 'treatment_dc_field_theta', 'treatment_dc_field_phi'\n    first_Z, first_I, zptrm_check, ptrm_check, ptrm_tail = [], [], [], [], []\n    field, phi, theta = \"\", \"\", \"\"\n    starthere = 0\n    Treat_I, Treat_Z, Treat_PZ, Treat_PI, Treat_M = [], [], [], [], []\n    ISteps, ZSteps, PISteps, PZSteps, MSteps = [], [], [], [], []\n    GammaChecks = []  # comparison of pTRM direction acquired and lab field\n    rec = datablock[0]\n    for key in Mkeys:\n        if key in list(rec.keys()) and rec[key] != \"\":\n            momkey = key\n            break\n# first find all the steps\n    for k in range(len(datablock)):\n        rec = datablock[k]\n        temp = float(rec[temp_key])\n        methcodes = []\n        tmp = rec[meth_key].split(\":\")\n        for meth in tmp:\n            methcodes.append(meth.strip())\n        if 'LT-T-I' in methcodes and 'LP-TRM' not in methcodes and 'LP-PI-TRM' in methcodes:\n            Treat_I.append(temp)\n            ISteps.append(k)\n            if field == \"\":\n                field = float(rec[dc_key])\n            if phi == \"\":\n                phi = float(rec[dc_phi_key])\n                theta = float(rec[dc_theta_key])\n# stick  first zero field stuff into first_Z\n        if 'LT-NO' in methcodes:\n            Treat_Z.append(temp)\n            ZSteps.append(k)\n        if 'LT-T-Z' in methcodes:\n            Treat_Z.append(temp)\n            ZSteps.append(k)\n        if 'LT-PTRM-Z' in methcodes:\n            Treat_PZ.append(temp)\n            PZSteps.append(k)\n        if 'LT-PTRM-I' in methcodes:\n            Treat_PI.append(temp)\n            PISteps.append(k)\n        if 'LT-PTRM-MD' in methcodes:\n            Treat_M.append(temp)\n            MSteps.append(k)\n        if 'LT-NO' in methcodes:\n            dec = float(rec[dec_key])\n            inc = float(rec[inc_key])\n            st = float(rec[momkey])\n            first_I.append([273, 0., 0., 0., 1])\n            first_Z.append([273, dec, inc, st, 1])  # NRM step\n    for temp in Treat_I:  # look through infield steps and find matching Z step\n        if temp in Treat_Z:  # found a match\n            istep = ISteps[Treat_I.index(temp)]\n            irec = datablock[istep]\n            methcodes = []\n            tmp = irec[meth_key].split(\":\")\n            for meth in tmp:\n                methcodes.append(meth.strip())\n            # take last record as baseline to subtract\n            brec = datablock[istep - 1]\n            zstep = ZSteps[Treat_Z.index(temp)]\n            zrec = datablock[zstep]\n    # sort out first_Z records\n            if \"LP-PI-TRM-IZ\" in methcodes:\n                ZI = 0\n            else:\n                ZI = 1\n            dec = float(zrec[dec_key])\n            inc = float(zrec[inc_key])\n            st = float(zrec[momkey])\n            first_Z.append([temp, dec, inc, st, ZI])\n    # sort out first_I records\n            try:\n                idec = float(irec[dec_key])\n                iinc = float(irec[inc_key])\n                istr = float(irec[momkey])\n            except TypeError as ex:\n                raise Exception('Malformed data of some sort for dec/inc/moment in measurement: {}.  You must fix this before proceeding.\\n           Bad record: {}'.format(irec.get('measurement', ''), irec))\n            X = dir2cart([idec, iinc, istr])\n            BL = dir2cart([dec, inc, st])\n            I = []\n            for c in range(3):\n                I.append((X[c] - BL[c]))\n            if I[2] != 0:\n                iDir = cart2dir(I)\n                if Zdiff == 0:\n                    first_I.append([temp, iDir[0], iDir[1], iDir[2], ZI])\n                else:\n                    first_I.append([temp, 0., 0., I[2], ZI])\n                gamma = angle([iDir[0], iDir[1]], [phi, theta])\n            else:\n                first_I.append([temp, 0., 0., 0., ZI])\n                gamma = 0.0\n# put in Gamma check (infield trm versus lab field)\n            if 180. - gamma < gamma:\n                gamma = 180. - gamma\n            GammaChecks.append([temp - 273., gamma])\n    for temp in Treat_PI:  # look through infield steps and find matching Z step\n        step = PISteps[Treat_PI.index(temp)]\n        rec = datablock[step]\n        dec = float(rec[dec_key])\n        inc = float(rec[inc_key])\n        st = float(rec[momkey])\n        brec = datablock[step - 1]  # take last record as baseline to subtract\n        pdec = float(brec[dec_key])\n        pinc = float(brec[inc_key])\n        pint = float(brec[momkey])\n        X = dir2cart([dec, inc, st])\n        prevX = dir2cart([pdec, pinc, pint])\n        I = []\n        for c in range(3):\n            I.append(X[c] - prevX[c])\n        dir1 = cart2dir(I)\n        if Zdiff == 0:\n            ptrm_check.append([temp, dir1[0], dir1[1], dir1[2]])\n        else:\n            ptrm_check.append([temp, 0., 0., I[2]])\n# in case there are zero-field pTRM checks (not the SIO way)\n    for temp in Treat_PZ:\n        step = PZSteps[Treat_PZ.index(temp)]\n        rec = datablock[step]\n        dec = float(rec[dec_key])\n        inc = float(rec[inc_key])\n        st = float(rec[momkey])\n        brec = datablock[step - 1]\n        pdec = float(brec[dec_key])\n        pinc = float(brec[inc_key])\n        pint = float(brec[momkey])\n        X = dir2cart([dec, inc, st])\n        prevX = dir2cart([pdec, pinc, pint])\n        I = []\n        for c in range(3):\n            I.append(X[c] - prevX[c])\n        dir2 = cart2dir(I)\n        zptrm_check.append([temp, dir2[0], dir2[1], dir2[2]])\n    # get pTRM tail checks together -\n    for temp in Treat_M:\n        # tail check step - just do a difference in magnitude!\n        step = MSteps[Treat_M.index(temp)]\n        rec = datablock[step]\n        st = float(rec[momkey])\n        if temp in Treat_Z:\n            step = ZSteps[Treat_Z.index(temp)]\n            brec = datablock[step]\n            pint = float(brec[momkey])\n#        X=dir2cart([dec,inc,st])\n#        prevX=dir2cart([pdec,pinc,pint])\n#        I=[]\n#        for c in range(3):I.append(X[c]-prevX[c])\n#        d=cart2dir(I)\n#        ptrm_tail.append([temp,d[0],d[1],d[2]])\n            # difference - if negative, negative tail!\n            ptrm_tail.append([temp, 0, 0, st - pint])\n        else:\n            print(\n                s, '  has a tail check with no first zero field step - check input file! for step', temp - 273.)\n#\n# final check\n#\n    if len(first_Z) != len(first_I):\n        print(len(first_Z), len(first_I))\n        print(\" Something wrong with this specimen! Better fix it or delete it \")\n        input(\" press return to acknowledge message\")\n    araiblock = (first_Z, first_I, ptrm_check,\n                 ptrm_tail, zptrm_check, GammaChecks)\n    return araiblock, field", "response": "This function sorts data block in to first_Z first_I etc."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sortmwarai(datablock, exp_type):\n    first_Z, first_I, ptrm_check, ptrm_tail, zptrm_check = [], [], [], [], []\n    field, phi, theta = \"\", \"\", \"\"\n    POWT_I, POWT_Z, POWT_PZ, POWT_PI, POWT_M = [], [], [], [], []\n    ISteps, ZSteps, PZSteps, PISteps, MSteps = [], [], [], [], []\n    rad = old_div(np.pi, 180.)\n    ThetaChecks = []\n    DeltaChecks = []\n    GammaChecks = []\n# first find all the steps\n    for k in range(len(datablock)):\n        rec = datablock[k]\n        powt = int(float(rec[\"treatment_mw_energy\"]))\n        methcodes = []\n        tmp = rec[\"magic_method_codes\"].split(\":\")\n        for meth in tmp:\n            methcodes.append(meth.strip())\n        if 'LT-M-I' in methcodes and 'LP-MRM' not in methcodes:\n            POWT_I.append(powt)\n            ISteps.append(k)\n            if field == \"\":\n                field = float(rec['treatment_dc_field'])\n            if phi == \"\":\n                phi = float(rec['treatment_dc_field_phi'])\n                theta = float(rec['treatment_dc_field_theta'])\n        if 'LT-M-Z' in methcodes:\n            POWT_Z.append(powt)\n            ZSteps.append(k)\n        if 'LT-PMRM-Z' in methcodes:\n            POWT_PZ.append(powt)\n            PZSteps.append(k)\n        if 'LT-PMRM-I' in methcodes:\n            POWT_PI.append(powt)\n            PISteps.append(k)\n        if 'LT-PMRM-MD' in methcodes:\n            POWT_M.append(powt)\n            MSteps.append(k)\n        if 'LT-NO' in methcodes:\n            dec = float(rec[\"measurement_dec\"])\n            inc = float(rec[\"measurement_inc\"])\n            st = float(rec[\"measurement_magn_moment\"])\n            first_I.append([0, 0., 0., 0., 1])\n            first_Z.append([0, dec, inc, st, 1])  # NRM step\n    if exp_type == \"LP-PI-M-D\":\n        # now look trough infield steps and  find matching Z step\n        for powt in POWT_I:\n            if powt in POWT_Z:\n                istep = ISteps[POWT_I.index(powt)]\n                irec = datablock[istep]\n                methcodes = []\n                tmp = irec[\"magic_method_codes\"].split(\":\")\n                for meth in tmp:\n                    methcodes.append(meth.strip())\n                # take last record as baseline to subtract\n                brec = datablock[istep - 1]\n                zstep = ZSteps[POWT_Z.index(powt)]\n                zrec = datablock[zstep]\n    # sort out first_Z records\n                if \"LP-PI-M-IZ\" in methcodes:\n                    ZI = 0\n                else:\n                    ZI = 1\n                dec = float(zrec[\"measurement_dec\"])\n                inc = float(zrec[\"measurement_inc\"])\n                st = float(zrec[\"measurement_magn_moment\"])\n                first_Z.append([powt, dec, inc, st, ZI])\n    # sort out first_I records\n                idec = float(irec[\"measurement_dec\"])\n                iinc = float(irec[\"measurement_inc\"])\n                istr = float(irec[\"measurement_magn_moment\"])\n                X = dir2cart([idec, iinc, istr])\n                BL = dir2cart([dec, inc, st])\n                I = []\n                for c in range(3):\n                    I.append((X[c] - BL[c]))\n                iDir = cart2dir(I)\n                first_I.append([powt, iDir[0], iDir[1], iDir[2], ZI])\n# put in Gamma check (infield trm versus lab field)\n                gamma = angle([iDir[0], iDir[1]], [phi, theta])\n                GammaChecks.append([powt, gamma])\n    elif exp_type == \"LP-PI-M-S\":\n        # find last zero field step before first infield step\n        lzrec = datablock[ISteps[0] - 1]\n        irec = datablock[ISteps[0]]\n        ndec = float(lzrec[\"measurement_dec\"])\n        ninc = float(lzrec[\"measurement_inc\"])\n        nstr = float(lzrec[\"measurement_magn_moment\"])\n        NRM = dir2cart([ndec, ninc, nstr])\n        fdec = float(irec[\"treatment_dc_field_phi\"])\n        finc = float(irec[\"treatment_dc_field_theta\"])\n        Flab = dir2cart([fdec, finc, 1.])\n        for step in ISteps:\n            irec = datablock[step]\n            rdec = float(irec[\"measurement_dec\"])\n            rinc = float(irec[\"measurement_inc\"])\n            rstr = float(irec[\"measurement_magn_moment\"])\n            theta1 = angle([ndec, ninc], [rdec, rinc])\n            theta2 = angle([rdec, rinc], [fdec, finc])\n            powt = int(float(irec[\"treatment_mw_energy\"]))\n            ThetaChecks.append([powt, theta1 + theta2])\n            p = (180. - (theta1 + theta2))\n            nstr = rstr * (old_div(np.sin(theta2 * rad), np.sin(p * rad)))\n            tmstr = rstr * (old_div(np.sin(theta1 * rad), np.sin(p * rad)))\n            first_Z.append([powt, ndec, ninc, nstr, 1])\n            first_I.append([powt, dec, inc, tmstr, 1])\n# check if zero field steps are parallel to assumed NRM\n        for step in ZSteps:\n            zrec = datablock[step]\n            powt = int(float(zrec[\"treatment_mw_energy\"]))\n            zdec = float(zrec[\"measurement_dec\"])\n            zinc = float(zrec[\"measurement_inc\"])\n            delta = angle([ndec, ninc], [zdec, zinc])\n            DeltaChecks.append([powt, delta])\n    # get pTRMs together - take previous record and subtract\n    for powt in POWT_PI:\n        step = PISteps[POWT_PI.index(powt)]\n        rec = datablock[step]\n        dec = float(rec[\"measurement_dec\"])\n        inc = float(rec[\"measurement_inc\"])\n        st = float(rec[\"measurement_magn_moment\"])\n        brec = datablock[step - 1]  # take last record as baseline to subtract\n        pdec = float(brec[\"measurement_dec\"])\n        pinc = float(brec[\"measurement_inc\"])\n        pint = float(brec[\"measurement_magn_moment\"])\n        X = dir2cart([dec, inc, st])\n        prevX = dir2cart([pdec, pinc, pint])\n        I = []\n        for c in range(3):\n            I.append(X[c] - prevX[c])\n        dir1 = cart2dir(I)\n        ptrm_check.append([powt, dir1[0], dir1[1], dir1[2]])\n    # get zero field pTRM  checks together\n    for powt in POWT_PZ:\n        step = PZSteps[POWT_PZ.index(powt)]\n        rec = datablock[step]\n        dec = float(rec[\"measurement_dec\"])\n        inc = float(rec[\"measurement_inc\"])\n        st = float(rec[\"measurement_magn_moment\"])\n        brec = datablock[step - 1]\n        pdec = float(brec[\"measurement_dec\"])\n        pinc = float(brec[\"measurement_inc\"])\n        pint = float(brec[\"measurement_magn_moment\"])\n        X = dir2cart([dec, inc, st])\n        prevX = dir2cart([pdec, pinc, pint])\n        I = []\n        for c in range(3):\n            I.append(X[c] - prevX[c])\n        dir2 = cart2dir(I)\n        zptrm_check.append([powt, dir2[0], dir2[1], dir2[2]])\n    # get pTRM tail checks together -\n    for powt in POWT_M:\n        step = MSteps[POWT_M.index(powt)]  # tail check step\n        rec = datablock[step]\n#        dec=float(rec[\"measurement_dec\"])\n#        inc=float(rec[\"measurement_inc\"])\n        st = float(rec[\"measurement_magn_moment\"])\n        step = ZSteps[POWT_Z.index(powt)]\n        brec = datablock[step]\n#        pdec=float(brec[\"measurement_dec\"])\n#        pinc=float(brec[\"measurement_inc\"])\n        pint = float(brec[\"measurement_magn_moment\"])\n#        X=dir2cart([dec,inc,st])\n#        prevX=dir2cart([pdec,pinc,pint])\n#        I=[]\n#        for c in range(3):I.append(X[c]-prevX[c])\n#        d=cart2dir(I)\n #       ptrm_tail.append([powt,d[0],d[1],d[2]])\n        # just do absolute magnitude difference # not vector diff\n        ptrm_tail.append([powt, 0, 0, st - pint])\n    #  check\n    #\n        if len(first_Z) != len(first_I):\n            print(len(first_Z), len(first_I))\n            print(\" Something wrong with this specimen! Better fix it or delete it \")\n            input(\" press return to acknowledge message\")\n            print(MaxRec)\n    araiblock = (first_Z, first_I, ptrm_check, ptrm_tail,\n                 zptrm_check, GammaChecks, ThetaChecks, DeltaChecks)\n    return araiblock, field", "response": "sorts microwave double heating data block in to first_Z first_I etc."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef docustom(lon, lat, alt, gh):\n    model, date, itype = 0, 0, 1\n    sv = np.zeros(len(gh))\n    colat = 90. - lat\n    x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n    return x, y, z, f", "response": "Calculates the field from the coefficients in the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef doigrf(lon, lat, alt, date, **kwargs):\n    from . import coefficients as cf\n    gh, sv = [], []\n    colat = 90. - lat\n#! convert to colatitude for MB routine\n    if lon < 0:\n        lon = lon + 360.\n# ensure all positive east longitudes\n    itype = 1\n    models, igrf12coeffs = cf.get_igrf12()\n    if 'mod' in list(kwargs.keys()):\n        if kwargs['mod'] == 'arch3k':\n            psvmodels, psvcoeffs = cf.get_arch3k()  # use ARCH3k coefficients\n        elif kwargs['mod'] == 'cals3k':\n            # use CALS3K_4b coefficients between -1000,1940\n            psvmodels, psvcoeffs = cf.get_cals3k()\n        elif kwargs['mod'] == 'pfm9k':\n            # use PFM9k (Nilsson et al., 2014), coefficients from -7000 to 1900\n            psvmodels, psvcoeffs = cf.get_pfm9k()\n        elif kwargs['mod'] == 'hfm10k':\n            # use HFM.OL1.A1 (Constable et al., 2016), coefficients from -8000\n            # to 1900\n            psvmodels, psvcoeffs = cf.get_hfm10k()\n        elif kwargs['mod'] == 'cals10k.2':\n            # use CALS10k.2 (Constable et al., 2016), coefficients from -8000\n            # to 1900\n            psvmodels, psvcoeffs = cf.get_cals10k_2()\n        elif kwargs['mod'] == 'shadif14k':\n            # use CALS10k.2 (Constable et al., 2016), coefficients from -8000\n            # to 1900\n            psvmodels, psvcoeffs = cf.get_shadif14k()\n        else:\n            # Korte and Constable, 2011;  use prior to -1000, back to -8000\n            psvmodels, psvcoeffs = cf.get_cals10k()\n# use geodetic coordinates\n    if 'models' in kwargs:\n        if 'mod' in list(kwargs.keys()):\n            return psvmodels, psvcoeffs\n        else:\n            return models, igrf12coeffs\n    if date < -12000:\n        print('too old')\n        return\n    if 'mod' in list(kwargs.keys()) and kwargs['mod'] == 'shadif14k':\n        if date < -10000:\n            incr = 100\n        else:\n            incr = 50\n        model = date - date % incr\n        gh = psvcoeffs[psvmodels.index(int(model))]\n        sv = old_div(\n            (psvcoeffs[psvmodels.index(int(model + incr))] - gh), float(incr))\n        x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n    elif date < -1000:\n        incr = 10\n        model = date - date % incr\n        gh = psvcoeffs[psvmodels.index(int(model))]\n        sv = old_div(\n            (psvcoeffs[psvmodels.index(int(model + incr))] - gh), float(incr))\n        x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n    elif date < 1900:\n        if kwargs['mod'] == 'cals10k':\n            incr = 50\n        else:\n            incr = 10\n        model = date - date % incr\n        gh = psvcoeffs[psvmodels.index(model)]\n        if model + incr < 1900:\n            sv = old_div(\n                (psvcoeffs[psvmodels.index(model + incr)] - gh), float(incr))\n        else:\n            field2 = igrf12coeffs[models.index(1940)][0:120]\n            sv = old_div((field2 - gh), float(1940 - model))\n        x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n    else:\n        model = date - date % 5\n        if date < 2015:\n            gh = igrf12coeffs[models.index(model)]\n            sv = old_div((igrf12coeffs[models.index(model + 5)] - gh), 5.)\n            x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n        else:\n            gh = igrf12coeffs[models.index(2015)]\n            sv = igrf12coeffs[models.index(2015.20)]\n            x, y, z, f = magsyn(gh, sv, model, date, itype, alt, colat, lon)\n    if 'coeffs' in list(kwargs.keys()):\n        return gh\n    else:\n        return x, y, z, f", "response": "This function calculates the interpolated or extrapolated main field and returns the interpolated main field and the interpolated secular variation coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpack(gh):\n    data = []\n    k, l = 0, 1\n    while k + 1 < len(gh):\n        for m in range(l + 1):\n            if m == 0:\n                data.append([l, m, gh[k], 0])\n                k += 1\n            else:\n                data.append([l, m, gh[k], gh[k + 1]])\n                k += 2\n        l += 1\n    return data", "response": "Unpacks a list of gauss coefficients into l m g h type list\n  "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef measurements_methods(meas_data, noave):\n#\n    version_num = get_version()\n    sids = get_specs(meas_data)\n# list  of measurement records for this specimen\n#\n# step through spec by spec\n#\n    SpecTmps, SpecOuts = [], []\n    for spec in sids:\n        TRM, IRM3D, ATRM, CR = 0, 0, 0, 0\n        expcodes = \"\"\n# first collect all data for this specimen and do lab treatments\n        # list  of measurement records for this specimen\n        SpecRecs = get_dictitem(meas_data, 'er_specimen_name', spec, 'T')\n        for rec in SpecRecs:\n            if 'measurement_flag' not in list(rec.keys()):\n                rec['measurement_flag'] = 'g'\n            tmpmeths = rec['magic_method_codes'].split(\":\")\n            meths = []\n            if \"LP-TRM\" in tmpmeths:\n                TRM = 1  # catch these suckers here!\n            if \"LP-IRM-3D\" in tmpmeths:\n                IRM3D = 1  # catch these suckers here!\n            elif \"LP-AN-TRM\" in tmpmeths:\n                ATRM = 1  # catch these suckers here!\n            elif \"LP-CR-TRM\" in tmpmeths:\n                CR = 1  # catch these suckers here!\n#\n# otherwise write over existing method codes\n#\n# find NRM data (LT-NO)\n#\n            elif float(rec[\"measurement_temp\"]) >= 273. and float(rec[\"measurement_temp\"]) < 323.:\n                # between 0 and 50C is room T measurement\n                if (\"measurement_dc_field\" not in list(rec.keys()) or float(rec[\"measurement_dc_field\"]) == 0 or rec[\"measurement_dc_field\"] == \"\") and (\"measurement_ac_field\" not in list(rec.keys()) or float(rec[\"measurement_ac_field\"]) == 0 or rec[\"measurement_ac_field\"] == \"\"):\n                    # measurement done in zero field!\n                    if \"treatment_temp\" not in list(rec.keys()) or rec[\"treatment_temp\"].strip() == \"\" or (float(rec[\"treatment_temp\"]) >= 273. and float(rec[\"treatment_temp\"]) < 298.):\n                        # between 0 and 50C is room T treatment\n                        if \"treatment_ac_field\" not in list(rec.keys()) or rec[\"treatment_ac_field\"] == \"\" or float(rec[\"treatment_ac_field\"]) == 0:\n                            # no AF\n                            # no IRM!\n                            if \"treatment_dc_field\" not in list(rec.keys()) or rec[\"treatment_dc_field\"] == \"\" or float(rec[\"treatment_dc_field\"]) == 0:\n                                if \"LT-NO\" not in meths:\n                                    meths.append(\"LT-NO\")\n                            elif \"LT-IRM\" not in meths:\n                                meths.append(\"LT-IRM\")  # it's an IRM\n#\n# find AF/infield/zerofield\n#\n                        # no ARM\n                        elif \"treatment_dc_field\" not in list(rec.keys()) or rec[\"treatment_dc_field\"] == \"\" or float(rec[\"treatment_dc_field\"]) == 0:\n                            if \"LT-AF-Z\" not in meths:\n                                meths.append(\"LT-AF-Z\")\n                        else:  # yes ARM\n                            if \"LT-AF-I\" not in meths:\n                                meths.append(\"LT-AF-I\")\n#\n# find Thermal/infield/zerofield\n#\n                    elif float(rec[\"treatment_temp\"]) >= 323:  # treatment done at  high T\n                        if TRM == 1:\n                            if \"LT-T-I\" not in meths:\n                                # TRM - even if zero applied field!\n                                meths.append(\"LT-T-I\")\n                        # no TRM\n                        elif \"treatment_dc_field\" not in list(rec.keys()) or rec[\"treatment_dc_field\"] == \"\" or float(rec[\"treatment_dc_field\"]) == 0.:\n                            if \"LT-T-Z\" not in meths:\n                                # don't overwrite if part of a TRM experiment!\n                                meths.append(\"LT-T-Z\")\n                        else:  # yes TRM\n                            if \"LT-T-I\" not in meths:\n                                meths.append(\"LT-T-I\")\n#\n# find low-T infield,zero field\n#\n                    else:  # treatment done at low T\n                        # no field\n                        if \"treatment_dc_field\" not in list(rec.keys()) or rec[\"treatment_dc_field\"] == \"\" or float(rec[\"treatment_dc_field\"]) == 0:\n                            if \"LT-LT-Z\" not in meths:\n                                meths.append(\"LT-LT-Z\")\n                        else:  # yes field\n                            if \"LT-LT-I\" not in meths:\n                                meths.append(\"LT-LT-I\")\n                if \"measurement_chi_volume\" in list(rec.keys()) or \"measurement_chi_mass\" in list(rec.keys()):\n                    if \"LP-X\" not in meths:\n                        meths.append(\"LP-X\")\n                # measurement in presence of dc field and not susceptibility;\n                # hysteresis!\n                elif \"measurement_lab_dc_field\" in list(rec.keys()) and rec[\"measurement_lab_dc_field\"] != 0:\n                    if \"LP-HYS\" not in meths:\n                        hysq = input(\"Is this a hysteresis experiment? [1]/0\")\n                        if hysq == \"\" or hysq == \"1\":\n                            meths.append(\"LP-HYS\")\n                        else:\n                            metha = input(\n                                \"Enter the lab protocol code that best describes this experiment \")\n                            meths.append(metha)\n                methcode = \"\"\n                for meth in meths:\n                    methcode = methcode + meth.strip() + \":\"\n                rec[\"magic_method_codes\"] = methcode[:-1]  # assign them back\n#\n# done with first pass, collect and assign provisional method codes\n            if \"measurement_description\" not in list(rec.keys()):\n                rec[\"measurement_description\"] = \"\"\n            rec[\"er_citation_names\"] = \"This study\"\n            SpecTmps.append(rec)\n# ready for second pass through, step through specimens, check whether ptrm, ptrm tail checks, or AARM, etc.\n#\n    for spec in sids:\n        MD, pTRM, IZ, ZI = 0, 0, 0, 0  # these are flags for the lab protocol codes\n        expcodes = \"\"\n        NewSpecs, SpecMeths = [], []\n        experiment_name, measnum = \"\", 1\n        if IRM3D == 1:\n            experiment_name = \"LP-IRM-3D\"\n        if ATRM == 1:\n            experiment_name = \"LP-AN-TRM\"\n        if CR == 1:\n            experiment_name = \"LP-CR\"\n        NewSpecs = get_dictitem(SpecTmps, 'er_specimen_name', spec, 'T')\n#\n# first look for replicate measurements\n#\n        Ninit = len(NewSpecs)\n        if noave != 1:\n            # averages replicate measurements, returns treatment keys that are\n            # being used\n            vdata, treatkeys = vspec_magic(NewSpecs)\n            if len(vdata) != len(NewSpecs):\n                # print spec,'started with ',Ninit,' ending with ',len(vdata)\n                NewSpecs = vdata\n                # print \"Averaged replicate measurements\"\n#\n# now look through this specimen's records - try to figure out what experiment it is\n#\n        if len(NewSpecs) > 1:  # more than one meas for this spec - part of an unknown experiment\n            SpecMeths = get_list(NewSpecs, 'magic_method_codes').split(\":\")\n            # TRM steps, could be TRM acquisition, Shaw or a Thellier\n            # experiment or TDS experiment\n            if \"LT-T-I\" in SpecMeths and experiment_name == \"\":\n                #\n                # collect all the infield steps and look for changes in dc field vector\n                #\n                Steps, TI = [], 1\n                for rec in NewSpecs:\n                    methods = get_list(\n                        NewSpecs, 'magic_method_codes').split(\":\")\n                    if \"LT-T-I\" in methods:\n                        Steps.append(rec)  # get all infield steps together\n                rec_bak = Steps[0]\n                if \"treatment_dc_field_phi\" in list(rec_bak.keys()) and \"treatment_dc_field_theta\" in list(rec_bak.keys()):\n                    # at least there is field orientation info\n                    if rec_bak[\"treatment_dc_field_phi\"] != \"\" and rec_bak[\"treatment_dc_field_theta\"] != \"\":\n                        phi0, theta0 = rec_bak[\"treatment_dc_field_phi\"], rec_bak[\"treatment_dc_field_theta\"]\n                        for k in range(1, len(Steps)):\n                            rec = Steps[k]\n                            phi, theta = rec[\"treatment_dc_field_phi\"], rec[\"treatment_dc_field_theta\"]\n                            if phi != phi0 or theta != theta0:\n                                ANIS = 1   # if direction changes, is some sort of anisotropy experiment\n                if \"LT-AF-I\" in SpecMeths and \"LT-AF-Z\" in SpecMeths:  # must be Shaw :(\n                    experiment_name = \"LP-PI-TRM:LP-PI-ALT-AFARM\"\n                elif TRM == 1:\n                    experiment_name = \"LP-TRM\"\n            else:\n                TI = 0  # no infield steps at all\n            if \"LT-T-Z\" in SpecMeths and experiment_name == \"\":  # thermal demag steps\n                if TI == 0:\n                    experiment_name = \"LP-DIR-T\"  # just ordinary thermal demag\n                elif TRM != 1:  # heart pounding - could be some  kind of TRM normalized paleointensity or LP-TRM-TD experiment\n                    Temps = []\n                    for step in Steps:  # check through the infield steps - if all at same temperature, then must be a demag of a total TRM with checks\n                        if step['treatment_temp'] not in Temps:\n                            Temps.append(step['treatment_temp'])\n                    if len(Temps) > 1:\n                        experiment_name = \"LP-PI-TRM\"  # paleointensity normalized by TRM\n                    else:\n                        # thermal demag of a lab TRM (could be part of a\n                        # LP-PI-TDS experiment)\n                        experiment_name = \"LP-TRM-TD\"\n                TZ = 1\n            else:\n                TZ = 0  # no zero field steps at all\n            if \"LT-AF-I\" in SpecMeths:  # ARM steps\n                Steps = []\n                for rec in NewSpecs:\n                    tmp = rec[\"magic_method_codes\"].split(\":\")\n                    methods = []\n                    for meth in tmp:\n                        methods.append(meth.strip())\n                    if \"LT-AF-I\" in methods:\n                        Steps.append(rec)  # get all infield steps together\n                rec_bak = Steps[0]\n                if \"treatment_dc_field_phi\" in list(rec_bak.keys()) and \"treatment_dc_field_theta\" in list(rec_bak.keys()):\n                    # at least there is field orientation info\n                    if rec_bak[\"treatment_dc_field_phi\"] != \"\" and rec_bak[\"treatment_dc_field_theta\"] != \"\":\n                        phi0, theta0 = rec_bak[\"treatment_dc_field_phi\"], rec_bak[\"treatment_dc_field_theta\"]\n                        ANIS = 0\n                        for k in range(1, len(Steps)):\n                            rec = Steps[k]\n                            phi, theta = rec[\"treatment_dc_field_phi\"], rec[\"treatment_dc_field_theta\"]\n                            if phi != phi0 or theta != theta0:\n                                ANIS = 1   # if direction changes, is some sort of anisotropy experiment\n                        if ANIS == 1:\n                            experiment_name = \"LP-AN-ARM\"\n                if experiment_name == \"\":  # not anisotropy of ARM - acquisition?\n                    field0 = rec_bak[\"treatment_dc_field\"]\n                    ARM = 0\n                    for k in range(1, len(Steps)):\n                        rec = Steps[k]\n                        field = rec[\"treatment_dc_field\"]\n                        if field != field0:\n                            ARM = 1\n                    if ARM == 1:\n                        experiment_name = \"LP-ARM\"\n                AFI = 1\n            else:\n                AFI = 0  # no ARM steps at all\n            if \"LT-AF-Z\" in SpecMeths and experiment_name == \"\":  # AF demag steps\n                if AFI == 0:\n                    experiment_name = \"LP-DIR-AF\"  # just ordinary AF demag\n                else:  # heart pounding - a pseudothellier?\n                    experiment_name = \"LP-PI-ARM\"\n                AFZ = 1\n            else:\n                AFZ = 0  # no AF demag at all\n            if \"LT-IRM\" in SpecMeths:  # IRM\n                Steps = []\n                for rec in NewSpecs:\n                    tmp = rec[\"magic_method_codes\"].split(\":\")\n                    methods = []\n                    for meth in tmp:\n                        methods.append(meth.strip())\n                    if \"LT-IRM\" in methods:\n                        Steps.append(rec)  # get all infield steps together\n                rec_bak = Steps[0]\n                if \"treatment_dc_field_phi\" in list(rec_bak.keys()) and \"treatment_dc_field_theta\" in list(rec_bak.keys()):\n                    # at least there is field orientation info\n                    if rec_bak[\"treatment_dc_field_phi\"] != \"\" and rec_bak[\"treatment_dc_field_theta\"] != \"\":\n                        phi0, theta0 = rec_bak[\"treatment_dc_field_phi\"], rec_bak[\"treatment_dc_field_theta\"]\n                        ANIS = 0\n                        for k in range(1, len(Steps)):\n                            rec = Steps[k]\n                            phi, theta = rec[\"treatment_dc_field_phi\"], rec[\"treatment_dc_field_theta\"]\n                            if phi != phi0 or theta != theta0:\n                                ANIS = 1   # if direction changes, is some sort of anisotropy experiment\n                        if ANIS == 1:\n                            experiment_name = \"LP-AN-IRM\"\n                if experiment_name == \"\":  # not anisotropy of IRM - acquisition?\n                    field0 = rec_bak[\"treatment_dc_field\"]\n                    IRM = 0\n                    for k in range(1, len(Steps)):\n                        rec = Steps[k]\n                        field = rec[\"treatment_dc_field\"]\n                        if field != field0:\n                            IRM = 1\n                    if IRM == 1:\n                        experiment_name = \"LP-IRM\"\n                IRM = 1\n            else:\n                IRM = 0  # no IRM at all\n            if \"LP-X\" in SpecMeths:  # susceptibility run\n                Steps = get_dictitem(\n                    NewSpecs, 'magic_method_codes', 'LT-X', 'has')\n                if len(Steps) > 0:\n                    rec_bak = Steps[0]\n                    if \"treatment_dc_field_phi\" in list(rec_bak.keys()) and \"treatment_dc_field_theta\" in list(rec_bak.keys()):\n                        # at least there is field orientation info\n                        if rec_bak[\"treatment_dc_field_phi\"] != \"\" and rec_bak[\"treatment_dc_field_theta\"] != \"\":\n                            phi0, theta0 = rec_bak[\"treatment_dc_field_phi\"], rec_bak[\"treatment_dc_field_theta\"]\n                            ANIS = 0\n                            for k in range(1, len(Steps)):\n                                rec = Steps[k]\n                                phi, theta = rec[\"treatment_dc_field_phi\"], rec[\"treatment_dc_field_theta\"]\n                                if phi != phi0 or theta != theta0:\n                                    ANIS = 1   # if direction changes, is some sort of anisotropy experiment\n                            if ANIS == 1:\n                                experiment_name = \"LP-AN-MS\"\n            else:\n                CHI = 0  # no susceptibility at all\n    #\n    # now need to deal with special thellier experiment problems - first clear up pTRM checks and  tail checks\n    #\n            if experiment_name == \"LP-PI-TRM\":  # is some sort of thellier experiment\n                rec_bak = NewSpecs[0]\n                tmp = rec_bak[\"magic_method_codes\"].split(\":\")\n                methbak = []\n                for meth in tmp:\n                    methbak.append(meth.strip())  # previous steps method codes\n                for k in range(1, len(NewSpecs)):\n                    rec = NewSpecs[k]\n                    tmp = rec[\"magic_method_codes\"].split(\":\")\n                    meths = []\n                    for meth in tmp:\n                        # get this guys method codes\n                        meths.append(meth.strip())\n    #\n    # check if this is a pTRM check\n    #\n                    if float(rec[\"treatment_temp\"]) < float(rec_bak[\"treatment_temp\"]):  # went backward\n                        if \"LT-T-I\" in meths and \"LT-T-Z\" in methbak:  # must be a pTRM check after first z\n                            #\n                            # replace LT-T-I method code with LT-PTRM-I\n                            #\n                            methcodes = \"\"\n                            for meth in meths:\n                                if meth != \"LT-T-I\":\n                                    methcode = methcode + meth.strip() + \":\"\n                            methcodes = methcodes + \"LT-PTRM-I\"\n                            meths = methcodes.split(\":\")\n                            pTRM = 1\n                        elif \"LT-T-Z\" in meths and \"LT-T-I\" in methbak:  # must be pTRM check after first I\n                            #\n                            # replace LT-T-Z method code with LT-PTRM-Z\n                            #\n                            methcodes = \"\"\n                            for meth in meths:\n                                if meth != \"LT-T-Z\":\n                                    methcode = methcode + meth + \":\"\n                            methcodes = methcodes + \"LT-PTRM-Z\"\n                            meths = methcodes.split(\":\")\n                            pTRM = 1\n                    methcodes = \"\"\n                    for meth in meths:\n                        methcodes = methcodes + meth.strip() + \":\"\n                    # attach new method code\n                    rec[\"magic_method_codes\"] = methcodes[:-1]\n                    rec_bak = rec  # next previous record\n                    tmp = rec_bak[\"magic_method_codes\"].split(\":\")\n                    methbak = []\n                    for meth in tmp:\n                        # previous steps method codes\n                        methbak.append(meth.strip())\n    #\n    # done with assigning pTRM checks.  data should be \"fixed\" in NewSpecs\n    #\n    # now let's find out which steps are infield zerofield (IZ) and which are zerofield infield (ZI)\n    #\n                rec_bak = NewSpecs[0]\n                tmp = rec_bak[\"magic_method_codes\"].split(\":\")\n                methbak = []\n                for meth in tmp:\n                    methbak.append(meth.strip())  # previous steps method codes\n                if \"LT-NO\" not in methbak:  # first measurement is not NRM\n                    if \"LT-T-I\" in methbak:\n                        IZorZI = \"LP-PI-TRM-IZ\"  # first pair is IZ\n                    if \"LT-T-Z\" in methbak:\n                        IZorZI = \"LP-PI-TRM-ZI\"  # first pair is ZI\n                    if IZorZI not in methbak:\n                        methbak.append(IZorZI)\n                    methcode = \"\"\n                    for meth in methbak:\n                        methcode = methcode + meth + \":\"\n                    # fix first heating step when no NRM\n                    NewSpecs[0][\"magic_method_codes\"] = methcode[:-1]\n                else:\n                    IZorZI = \"\"  # first measurement is NRM and not one of a pair\n                for k in range(1, len(NewSpecs)):  # hunt through measurements again\n                    rec = NewSpecs[k]\n                    tmp = rec[\"magic_method_codes\"].split(\":\")\n                    meths = []\n                    for meth in tmp:\n                        # get this guys method codes\n                        meths.append(meth.strip())\n    #\n    # check if this start a new temperature step of a infield/zerofield pair\n    #\n                    if float(rec[\"treatment_temp\"]) > float(rec_bak[\"treatment_temp\"]) and \"LT-PTRM-I\" not in methbak:  # new pair?\n                        if \"LT-T-I\" in meths:  # infield of this pair\n                            IZorZI = \"LP-PI-TRM-IZ\"\n                            IZ = 1  # at least one IZ pair\n                        elif \"LT-T-Z\" in meths:  # zerofield\n                            IZorZI = \"LP-PI-TRM-ZI\"\n                            ZI = 1  # at least one ZI pair\n                    # new pair after out of sequence PTRM check?\n                    elif float(rec[\"treatment_temp\"]) > float(rec_bak[\"treatment_temp\"]) and \"LT-PTRM-I\" in methbak and IZorZI != \"LP-PI-TRM-ZI\":\n                        if \"LT-T-I\" in meths:  # infield of this pair\n                            IZorZI = \"LP-PI-TRM-IZ\"\n                            IZ = 1  # at least one IZ pair\n                        elif \"LT-T-Z\" in meths:  # zerofield\n                            IZorZI = \"LP-PI-TRM-ZI\"\n                            ZI = 1  # at least one ZI pair\n                    # stayed same temp\n                    if float(rec[\"treatment_temp\"]) == float(rec_bak[\"treatment_temp\"]):\n                        if \"LT-T-Z\" in meths and \"LT-T-I\" in methbak and IZorZI == \"LP-PI-TRM-ZI\":  # must be a tail check\n                            #\n                            # replace LT-T-Z method code with LT-PTRM-MD\n                            #\n                            methcodes = \"\"\n                            for meth in meths:\n                                if meth != \"LT-T-Z\":\n                                    methcode = methcode + meth + \":\"\n                            methcodes = methcodes + \"LT-PTRM-MD\"\n                            meths = methcodes.split(\":\")\n                            MD = 1\n    # fix method codes\n                    if \"LT-PTRM-I\" not in meths and \"LT-PTRM-MD\" not in meths and IZorZI not in meths:\n                        meths.append(IZorZI)\n                    newmeths = []\n                    for meth in meths:\n                        if meth not in newmeths:\n                            newmeths.append(meth)  # try to get uniq set\n                    methcode = \"\"\n                    for meth in newmeths:\n                        methcode = methcode + meth + \":\"\n                    rec[\"magic_method_codes\"] = methcode[:-1]\n                    rec_bak = rec  # moving on to next record, making current one the backup\n                    # get last specimen's method codes in a list\n                    methbak = rec_bak[\"magic_method_codes\"].split(\":\")\n\n    #\n    # done with this specimen's records, now  check if any pTRM checks or MD checks\n    #\n                if pTRM == 1:\n                    experiment_name = experiment_name + \":LP-PI-ALT-PTRM\"\n                if MD == 1:\n                    experiment_name = experiment_name + \":LP-PI-BT-MD\"\n                if IZ == 1 and ZI == 1:\n                    experiment_name = experiment_name + \":LP-PI-BT-IZZI\"\n                if IZ == 1 and ZI == 0:\n                    experiment_name = experiment_name + \":LP-PI-IZ\"  # Aitken method\n                if IZ == 0 and ZI == 1:\n                    experiment_name = experiment_name + \":LP-PI-ZI\"  # Coe method\n                IZ, ZI, pTRM, MD = 0, 0, 0, 0  # reset these for next specimen\n                for rec in NewSpecs:  # fix the experiment name for all recs for this specimen and save in SpecOuts\n                    # assign an experiment name to all specimen measurements\n                    # from this specimen\n                    if experiment_name != \"\":\n                        rec[\"magic_method_codes\"] = rec[\"magic_method_codes\"] + \\\n                            \":\" + experiment_name\n                    rec[\"magic_experiment_name\"] = spec + \":\" + experiment_name\n                    rec['measurement_number'] = '%i' % (\n                        measnum)  # assign measurement numbers\n                    measnum += 1\n                    #rec['sequence'] = '%i'%(seqnum)\n                    #seqnum += 1\n                    SpecOuts.append(rec)\n            elif experiment_name == \"LP-PI-TRM:LP-PI-ALT-AFARM\":  # is a Shaw experiment!\n                ARM, TRM = 0, 0\n                for rec in NewSpecs:  # fix the experiment name for all recs for this specimen and save in SpecOuts\n                    # assign an experiment name to all specimen measurements from this specimen\n                    # make the second ARM in Shaw experiments LT-AF-I-2, stick\n                    # in the AF of ARM and TRM codes\n                    meths = rec[\"magic_method_codes\"].split(\":\")\n                    if ARM == 1:\n                        if \"LT-AF-I\" in meths:\n                            del meths[meths.index(\"LT-AF-I\")]\n                            meths.append(\"LT-AF-I-2\")\n                            ARM = 2\n                        if \"LT-AF-Z\" in meths and TRM == 0:\n                            meths.append(\"LP-ARM-AFD\")\n                    if TRM == 1 and ARM == 1:\n                        if \"LT-AF-Z\" in meths:\n                            meths.append(\"LP-TRM-AFD\")\n                    if ARM == 2:\n                        if \"LT-AF-Z\" in meths:\n                            meths.append(\"LP-ARM2-AFD\")\n                    newcode = \"\"\n                    for meth in meths:\n                        newcode = newcode + meth + \":\"\n                    rec[\"magic_method_codes\"] = newcode[:-1]\n                    if \"LT-AF-I\" in meths:\n                        ARM = 1\n                    if \"LT-T-I\" in meths:\n                        TRM = 1\n                    rec[\"magic_method_codes\"] = rec[\"magic_method_codes\"] + \\\n                        \":\" + experiment_name\n                    rec[\"magic_experiment_name\"] = spec + \":\" + experiment_name\n                    rec['measurement_number'] = '%i' % (\n                        measnum)  # assign measurement numbers\n                    #rec['sequence'] = '%i'%(seqnum)\n                    #seqnum += 1\n                    measnum += 1\n                    SpecOuts.append(rec)\n            else:  # not a Thellier-Thellier  or a Shaw experiemnt\n                for rec in NewSpecs:\n                    if experiment_name == \"\":\n                        rec[\"magic_method_codes\"] = \"LT-NO\"\n                        rec[\"magic_experiment_name\"] = spec + \":LT-NO\"\n                        rec['measurement_number'] = '%i' % (\n                            measnum)  # assign measurement numbers\n                        #rec['sequence'] = '%i'%(seqnum)\n                        #seqnum += 1\n                        measnum += 1\n                    else:\n                        if experiment_name not in rec['magic_method_codes']:\n                            rec[\"magic_method_codes\"] = rec[\"magic_method_codes\"] + \\\n                                \":\" + experiment_name\n                            rec[\"magic_method_codes\"] = rec[\"magic_method_codes\"].strip(\n                                ':')\n                        rec['measurement_number'] = '%i' % (\n                            measnum)  # assign measurement numbers\n                        #rec['sequence'] = '%i'%(seqnum)\n                        #seqnum += 1\n                        measnum += 1\n                        rec[\"magic_experiment_name\"] = spec + \\\n                            \":\" + experiment_name\n                    rec[\"magic_software_packages\"] = version_num\n                    SpecOuts.append(rec)\n        else:\n            NewSpecs[0][\"magic_experiment_name\"] = spec + \":\" + \\\n                NewSpecs[0]['magic_method_codes'].split(':')[0]\n            NewSpecs[0][\"magic_software_packages\"] = version_num\n            # just copy over the single record as is\n            SpecOuts.append(NewSpecs[0])\n    return SpecOuts", "response": "get list of unique methods for this specimen"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_site(sample, convention, Z):\n    convention = str(convention)\n    site = sample  # default is that site = sample\n#\n#\n# Sample is final letter on site designation eg:  TG001a (used by SIO lab\n# in San Diego)\n    if convention == \"1\":\n        return sample[:-1]  # peel off terminal character\n#\n# Site-Sample format eg:  BG94-1  (used by PGL lab in Beijing)\n#\n    if convention == \"2\":\n        parts = sample.strip('-').split('-')\n        return parts[0]\n#\n# Sample is XXXX.YY where XXX is site and YY is sample\n#\n    if convention == \"3\":\n        parts = sample.split('.')\n        return parts[0]\n#\n# Sample is XXXXYYY where XXX is site desgnation and YYY is Z long integer\n#\n    if convention == \"4\":\n        k = int(Z) - 1\n        return sample[0:-k]  # peel off Z characters from site\n\n    if convention == \"5\":  # sample == site\n        return sample\n\n    if convention == \"6\":  # should be names in orient.txt\n        print(\"-W- Finding names in orient.txt is not currently supported\")\n\n    if convention == \"7\":  # peel off Z characters for site\n        k = int(Z)\n        return sample[0:k]\n\n    if convention == \"8\":  # peel off Z characters for site\n        return \"\"\n\n    if convention == \"9\":  # peel off Z characters for site\n        return sample\n\n    print(\"Error in site parsing routine\")\n    return", "response": "parse the site name from the sample name using the specified convention"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_samp_con():\n#\n    samp_con, Z = \"\", \"\"\n    while samp_con == \"\":\n        samp_con = input(\"\"\"\n        Sample naming convention:\n            [1] XXXXY: where XXXX is an arbitrary length site designation and Y\n                is the single character sample designation.  e.g., TG001a is the\n                first sample from site TG001.    [default]\n            [2] XXXX-YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [3] XXXX.YY: YY sample from site XXXX (XXX, YY of arbitary length)\n            [4-Z] XXXX[YYY]:  YYY is sample designation with Z characters from site XXX\n            [5] site name same as sample\n            [6] site is entered under a separate column\n            [7-Z] [XXXX]YYY:  XXXX is site designation with Z characters with sample name XXXXYYYY\n            NB: all others you will have to customize your self\n                 or e-mail ltauxe@ucsd.edu for help.\n            select one:\n\"\"\")\n    #\n        if samp_con == \"\" or samp_con == \"1\":\n            samp_con, Z = \"1\", 1\n        if \"4\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [4] must be in form 4-Z where Z is an integer\")\n                samp_con = \"\"\n            else:\n                Z = samp_con.split(\"-\")[1]\n                samp_con = \"4\"\n        if \"7\" in samp_con:\n            if \"-\" not in samp_con:\n                print(\"option [7] must be in form 7-Z where Z is an integer\")\n                samp_con = \"\"\n            else:\n                Z = samp_con.split(\"-\")[1]\n                samp_con = \"7\"\n        if samp_con.isdigit() == False or int(samp_con) > 7:\n            print(\"Try again\\n \")\n            samp_con = \"\"\n    return samp_con, Z", "response": "get sample naming convention"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to return the dip direction and dip that would yield the tilt corrected direction if applied to the uncorrected direction", "response": "def get_tilt(dec_geo, inc_geo, dec_tilt, inc_tilt):\n    \"\"\"\n    Function to return the dip direction and dip that would yield the tilt\n    corrected direction if applied to the uncorrected direction (geographic\n    coordinates)\n\n    Parameters\n    ----------\n    dec_geo : declination in geographic coordinates\n    inc_geo : inclination in geographic coordinates\n    dec_tilt : declination in tilt-corrected coordinates\n    inc_tilt : inclination in tilt-corrected coordinates\n\n    Returns\n    -------\n    DipDir, Dip : tuple of dip direction and dip\n    \"\"\"\n# strike is horizontal line equidistant from two input directions\n    SCart = [0, 0, 0]  # cartesian coordites of Strike\n    SCart[2] = 0.  # by definition\n    # cartesian coordites of Geographic D\n    GCart = dir2cart([dec_geo, inc_geo, 1.])\n    TCart = dir2cart([dec_tilt, inc_tilt, 1.])  # cartesian coordites of Tilt D\n    X = old_div((TCart[1] - GCart[1]), (GCart[0] - TCart[0]))\n    SCart[1] = np.sqrt(old_div(1, (X**2 + 1.)))\n    SCart[0] = SCart[1] * X\n    SDir = cart2dir(SCart)\n    DipDir = (SDir[0] - 90.) % 360.\n    DipDir = (SDir[0] + 90.) % 360.\n# D is creat circle distance between geo direction and strike\n# theta is GCD between geo and tilt (on unit sphere).  use law of cosines\n# to get small cirlce between geo and tilt (dip)\n    cosd = GCart[0] * SCart[0] + GCart[1] * \\\n        SCart[1]  # cosine of angle between two\n    d = np.arccos(cosd)\n    cosTheta = GCart[0] * TCart[0] + GCart[1] * TCart[1] + GCart[2] * TCart[2]\n    Dip = (old_div(180., np.pi)) * \\\n        np.arccos(-(old_div((cosd**2 - cosTheta), np.sin(d)**2)))\n    if Dip > 90:\n        Dip = -Dip\n    return DipDir, Dip"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting azimuth and pl from specimen dec inc gdec ginc", "response": "def get_azpl(cdec, cinc, gdec, ginc):\n    \"\"\"\n     gets azimuth and pl from specimen dec inc (cdec,cinc) and gdec,ginc (geographic)  coordinates\n    \"\"\"\n    TOL = 1e-4\n    rad = old_div(np.pi, 180.)\n    Xp = dir2cart([gdec, ginc, 1.])\n    X = dir2cart([cdec, cinc, 1.])\n    # find plunge first\n    az, pl, zdif, ang = 0., -90., 1., 360.\n    while zdif > TOL and pl < 180.:\n        znew = X[0] * np.sin(pl * rad) + X[2] * np.cos(pl * rad)\n        zdif = abs(Xp[2] - znew)\n        pl += .01\n\n    while ang > 0.1 and az < 360.:\n        d, i = dogeo(cdec, cinc, az, pl)\n        ang = angle([gdec, ginc], [d, i])\n        az += .01\n    return az - .01, pl - .01"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the priorities of the given methods", "response": "def set_priorities(SO_methods, ask):\n    \"\"\"\n     figure out which sample_azimuth to use, if multiple orientation methods\n    \"\"\"\n    # if ask set to 1, then can change priorities\n    SO_methods = [meth.strip() for meth in SO_methods]\n    SO_defaults = ['SO-SUN', 'SO-GPS-DIFF', 'SO-SUN-SIGHT', 'SO-SIGHT', 'SO-SIGHT-BS',\n                   'SO-CMD-NORTH', 'SO-MAG', 'SO-SM', 'SO-REC', 'SO-V', 'SO-CORE', 'SO-NO']\n    SO_priorities, prior_list = [], []\n    if len(SO_methods) >= 1:\n        for l in range(len(SO_defaults)):\n            if SO_defaults[l] in SO_methods:\n                SO_priorities.append(SO_defaults[l])\n    pri, change = 0, \"1\"\n    if ask == 1:\n        print(\"\"\"These methods of sample orientation were found:\n      They have been assigned a provisional priority (top = zero, last = highest number) \"\"\")\n        for m in range(len(SO_defaults)):\n            if SO_defaults[m] in SO_methods:\n                SO_priorities[SO_methods.index(SO_defaults[m])] = pri\n                pri += 1\n        while change == \"1\":\n            prior_list = SO_priorities\n            for m in range(len(SO_methods)):\n                print(SO_methods[m], SO_priorities[m])\n            change = input(\"Change these?  1/[0] \")\n            if change != \"1\":\n                break\n        SO_priorities = []\n        for l in range(len(SO_methods)):\n            print(SO_methods[l])\n            print(\" Priority?   \", prior_list)\n            pri = int(input())\n            SO_priorities.append(pri)\n            del prior_list[prior_list.index(pri)]\n    return SO_priorities"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind EOL of input file", "response": "def get_EOL(file):\n    \"\"\"\n     find EOL of input file (whether mac,PC or unix format)\n    \"\"\"\n    f = open(file, 'r')\n    firstline = f.read(350)\n    EOL = \"\"\n    for k in range(350):\n        if firstline[k:k + 2] == \"\\r\\n\":\n            print(file, ' appears to be a dos file')\n            EOL = '\\r\\n'\n            break\n    if EOL == \"\":\n        for k in range(350):\n            if firstline[k] == \"\\r\":\n                print(file, ' appears to be a mac file')\n                EOL = '\\r'\n    if EOL == \"\":\n        print(file, \" appears to be a  unix file\")\n        EOL = '\\n'\n    f.close()\n    return EOL"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sortshaw(s, datablock):\n    for rec in datablock:\n        methcodes = rec[\"magic_method_codes\"].split(\":\")\n        step = float(rec[\"treatment_ac_field\"])\n        str = float(rec[\"measurement_magn_moment\"])\n        if \"LT-NO\" in methcodes:\n            NRM.append([0, str])\n        if \"LT-T-I\" in methcodes:\n            TRM.append([0, str])\n            field = float(rec[\"treatment_dc_field\"])\n        if \"LT-AF-I\" in methcodes:\n            ARM1.append([0, str])\n        if \"LT-AF-I-2\" in methcodes:\n            ARM2.append([0, str])\n        if \"LT-AF-Z\" in methcodes:\n            if \"LP-ARM-AFD\" in methcodes:\n                ARM1.append([step, str])\n            elif \"LP-TRM-AFD\" in methcodes:\n                TRM.append([step, str])\n            elif \"LP-ARM2-AFD\" in methcodes:\n                ARM2.append([step, str])\n            else:\n                NRM.append([step, str])\n    cont = 1\n    while cont == 1:\n        if len(NRM) != len(TRM):\n            print(\"Uneven NRM/TRM steps: \")\n            NRM, TRM, cont = cleanup(TRM, NRM)\n        else:\n            cont = 0\n    cont = 1\n    while cont == 1:\n        if len(ARM1) != len(ARM2):\n            print(\"Uneven ARM1/ARM2 steps: \")\n            ARM1, ARM2, cont = cleanup(ARM2, ARM1)\n        else:\n            cont = 0\n#\n# final check\n#\n    if len(NRM) != len(TRM) or len(ARM1) != len(ARM2):\n        print(len(NRM), len(TRM), len(ARM1), len(ARM2))\n        print(\" Something wrong with this specimen! Better fix it or delete it \")\n        input(\" press return to acknowledge message\")\n# now do the ratio to \"fix\" NRM/TRM data\n# a\n    TRM_ADJ = []\n    for kk in range(len(TRM)):\n        step = TRM[kk][0]\n        for k in range(len(ARM1)):\n            if ARM1[k][0] == step:\n                TRM_ADJ.append([step, TRM[kk][1] * ARM1[k][1] / ARM2[k][1]])\n                break\n    shawblock = (NRM, TRM, ARM1, ARM2, TRM_ADJ)\n    return shawblock, field", "response": "Sort data block in to ARM1 ARM2 NRM TRM and ARM1 and ARM2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the vector at a given latitude and longitude for a specified .", "response": "def getvec(gh, lat, lon):\n    \"\"\"\n    Evaluates the vector at a given latitude and longitude for a specified\n    set of coefficients\n\n    Parameters\n    ----------\n    gh : a list of gauss coefficients\n    lat : latitude of location\n    long : longitude of location\n\n    Returns\n    -------\n    vec : direction in [dec, inc, intensity]\n    \"\"\"\n    sv = []\n    pad = 120 - len(gh)\n    for x in range(pad):\n        gh.append(0.)\n    for x in range(len(gh)):\n        sv.append(0.)\n#! convert to colatitude for MB routine\n    itype = 1\n    colat = 90. - lat\n    date, alt = 2000., 0.  # use a dummy date and altitude\n    x, y, z, f = magsyn(gh, sv, date, date, itype, alt, colat, lon)\n    vec = cart2dir([x, y, z])\n    vec[2] = f\n    return vec"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets sigma as a function of degree l from Constable and Parker 1988", "response": "def s_l(l, alpha):\n    \"\"\"\n    get sigma as a function of degree l from Constable and Parker (1988)\n    \"\"\"\n    a2 = alpha**2\n    c_a = 0.547\n    s_l = np.sqrt(old_div(((c_a**(2. * l)) * a2), ((l + 1.) * (2. * l + 1.))))\n    return s_l"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mktk03(terms, seed, G2, G3):\n# random.seed(n)\n    p = 0\n    n = seed\n    gh = []\n    g10, sfact, afact = -18e3, 3.8, 2.4\n    g20 = G2 * g10\n    g30 = G3 * g10\n    alpha = g10/afact\n    s1 = s_l(1, alpha)\n    s10 = sfact * s1\n    gnew = random.normal(g10, s10)\n    if p == 1:\n        print(1, 0, gnew, 0)\n    gh.append(gnew)\n    gh.append(random.normal(0, s1))\n    gnew = gh[-1]\n    gh.append(random.normal(0, s1))\n    hnew = gh[-1]\n    if p == 1:\n        print(1, 1, gnew, hnew)\n    for l in range(2, terms + 1):\n        for m in range(l + 1):\n            OFF = 0.0\n            if l == 2 and m == 0:\n                OFF = g20\n            if l == 3 and m == 0:\n                OFF = g30\n            s = s_l(l, alpha)\n            j = (l - m) % 2\n            if j == 1:\n                s = s * sfact\n            gh.append(random.normal(OFF, s))\n            gnew = gh[-1]\n            if m == 0:\n                hnew = 0\n            else:\n                gh.append(random.normal(0, s))\n                hnew = gh[-1]\n            if p == 1:\n                print(l, m, gnew, hnew)\n    return gh", "response": "Generates a list of gauss coefficients drawn from the TK03 distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating paleoinclination from latitude using dipole formula", "response": "def pinc(lat):\n    \"\"\"\n    calculate paleoinclination from latitude using dipole formula: tan(I) = 2tan(lat)\n    Parameters\n    ________________\n\n    lat : either a single value or an array of latitudes\n\n    Returns\n    -------\n\n    array of inclinations\n    \"\"\"\n    tanl = np.tan(np.radians(lat))\n    inc = np.arctan(2. * tanl)\n    return np.degrees(inc)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate paleolatitude from inclination using dipole formula", "response": "def plat(inc):\n    \"\"\"\n    calculate paleolatitude from inclination using dipole formula: tan(I) = 2tan(lat)\n    Parameters\n    ________________\n\n    inc : either a single value or an array of inclinations\n\n    Returns\n    -------\n\n    array of latitudes\n    \"\"\"\n    tani = np.tan(np.radians(inc))\n    lat = np.arctan(tani/2.)\n    return np.degrees(lat)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pseudo(DIs, random_seed=None):\n    if random_seed != None:\n        np.random.seed(random_seed)\n    Inds = np.random.randint(len(DIs), size=len(DIs))\n    D = np.array(DIs)\n    return D[Inds]", "response": "Draw a bootstrap sample of directions returning as many bootstrapped samples as in the input directions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn bootstrap means for Directional data Parameters _________________ DIs : nested list of Dec,Inc pairs nb : number of bootstrap pseudosamples Returns ------- BDIs: nested list of bootstrapped mean Dec,Inc pairs", "response": "def di_boot(DIs, nb=5000):\n    \"\"\"\n     returns bootstrap means  for Directional data\n     Parameters\n     _________________\n     DIs : nested list of Dec,Inc pairs\n     nb : number of bootstrap pseudosamples\n\n     Returns\n    -------\n     BDIs:   nested list of bootstrapped mean Dec,Inc pairs\n    \"\"\"\n#\n# now do bootstrap to collect BDIs  bootstrap means\n#\n    BDIs = []  # number of bootstraps, list of bootstrap directions\n#\n\n    for k in range(nb):  # repeat nb times\n        #        if k%50==0:print k,' out of ',nb\n        pDIs = pseudo(DIs)  # get a pseudosample\n        bfpars = fisher_mean(pDIs)  # get bootstrap mean bootstrap sample\n        BDIs.append([bfpars['dec'], bfpars['inc']])\n    return BDIs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dir_df_boot(dir_df, nb=5000, par=False):\n    N = dir_df.dir_dec.values.shape[0]  # number of data points\n    BDIs = []\n    for k in range(nb):\n        pdir_df = dir_df.sample(n=N, replace=True)  # bootstrap pseudosample\n        pdir_df.reset_index(inplace=True)  # reset the index\n        if par:  # do a parametric bootstrap\n            for i in pdir_df.index:  # set through the pseudosample\n                n = pdir_df.loc[i, 'dir_n']  # get number of samples/site\n                # get ks for each sample\n                ks = np.ones(shape=n)*pdir_df.loc[i, 'dir_k']\n                # draw a fisher distributed set of directions\n                decs, incs = fshdev(ks)\n                di_block = np.column_stack((decs, incs))\n                #  rotate them to the mean\n                di_block = dodirot_V(\n                    di_block, pdir_df.loc[i, 'dir_dec'], pdir_df.loc[i, 'dir_inc'])\n                # get the new mean direction for the pseudosample\n                fpars = fisher_mean(di_block)\n                # replace the pseudo sample mean direction\n                pdir_df.loc[i, 'dir_dec'] = fpars['dec']\n                pdir_df.loc[i, 'dir_inc'] = fpars['inc']\n        # get bootstrap mean bootstrap sample\n        bfpars = dir_df_fisher_mean(pdir_df)\n        BDIs.append([bfpars['dec'], bfpars['inc']])\n    return BDIs", "response": "This function is used to bootstrap the direction DataFrame with optional parametric bootstraps."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dir_df_fisher_mean(dir_df):\n    N = dir_df.dir_dec.values.shape[0]  # number of data points\n    fpars = {}\n    if N < 2:\n        return fpars\n    dirs = dir_df[['dir_dec', 'dir_inc']].values\n    X = dir2cart(dirs).transpose()\n    Xbar = np.array([X[0].sum(), X[1].sum(), X[2].sum()])\n    R = np.sqrt(Xbar[0]**2+Xbar[1]**2+Xbar[2]**2)\n    Xbar = Xbar/R\n    dir = cart2dir(Xbar)\n    fpars[\"dec\"] = dir[0]\n    fpars[\"inc\"] = dir[1]\n    fpars[\"n\"] = N\n    fpars[\"r\"] = R\n    if N != R:\n        k = (N - 1.) / (N - R)\n        fpars[\"k\"] = k\n        csd = 81./np.sqrt(k)\n    else:\n        fpars['k'] = 'inf'\n        csd = 0.\n    b = 20.**(1./(N - 1.)) - 1\n    a = 1 - b * (N - R) / R\n    if a < -1:\n        a = -1\n    a95 = np.degrees(np.arccos(a))\n    fpars[\"alpha95\"] = a95\n    fpars[\"csd\"] = csd\n    if a < 0:\n        fpars[\"alpha95\"] = 180.0\n    return fpars", "response": "Calculates the fisher mean for Pandas data frame dir_df"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pseudosample(x):\n#\n    BXs = []\n    for k in range(len(x)):\n        ind = random.randint(0, len(x) - 1)\n        BXs.append(x[ind])\n    return BXs", "response": "draw a bootstrap sample of x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bc02(data):\n\n    plate, site_lat, site_lon, age = data[0], data[1], data[2], data[3]\n    apwp = get_plate_data(plate)\n    recs = apwp.split()\n    #\n    # put it into  usable form in plate_data\n    #\n    k, plate_data = 0, []\n    while k < len(recs) - 3:\n        rec = [float(recs[k]), float(recs[k + 1]), float(recs[k + 2])]\n        plate_data.append(rec)\n        k = k + 3\n\n    #\n    # find the right pole for the age\n    #\n    for i in range(len(plate_data)):\n        if age >= plate_data[i][0] and age <= plate_data[i + 1][0]:\n            if (age - plate_data[i][0]) < (plate_data[i][0] - age):\n                rec = i\n            else:\n                rec = i + 1\n            break\n    pole_lat = plate_data[rec][1]\n    pole_lon = plate_data[rec][2]\n    return pole_lat, pole_lon", "response": "This function is used to get the APWP from Besse and Courtillot 2002 paper from Besse and Courtillot 2002 paper from 2002."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo a linear regression", "response": "def linreg(x, y):\n    \"\"\"\n    does a linear regression\n    \"\"\"\n    if len(x) != len(y):\n        print('x and y must be same length')\n        return\n    xx, yy, xsum, ysum, xy, n, sum = 0, 0, 0, 0, 0, len(x), 0\n    linpars = {}\n    for i in range(n):\n        xx += x[i] * x[i]\n        yy += y[i] * y[i]\n        xy += x[i] * y[i]\n        xsum += x[i]\n        ysum += y[i]\n        xsig = np.sqrt(old_div((xx - old_div(xsum**2, n)), (n - 1.)))\n        ysig = np.sqrt(old_div((yy - old_div(ysum**2, n)), (n - 1.)))\n    linpars['slope'] = old_div(\n        (xy - (xsum * ysum / n)), (xx - old_div((xsum**2), n)))\n    linpars['b'] = old_div((ysum - linpars['slope'] * xsum), n)\n    linpars['r'] = old_div((linpars['slope'] * xsig), ysig)\n    for i in range(n):\n        a = y[i] - linpars['b'] - linpars['slope'] * x[i]\n        sum += a\n    linpars['sigma'] = old_div(sum, (n - 2.))\n    linpars['n'] = n\n    return linpars"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns 'flattened' inclination, assuming factor, f and King (1955) formula: tan (I_o) = f tan (I_f) Parameters __________ incs : array of inclination (I_f) data to flatten f : flattening factor Returns _______ I_o : inclinations after flattening", "response": "def squish(incs, f):\n    \"\"\"\n    returns 'flattened' inclination, assuming factor, f and King (1955) formula:\n    tan (I_o) = f tan (I_f)\n\n    Parameters\n    __________\n    incs : array of inclination (I_f)  data to flatten\n    f : flattening factor\n\n    Returns\n    _______\n    I_o :  inclinations after flattening\n    \"\"\"\n    incs = np.radians(incs)\n    I_o = f * np.tan(incs)  # multiply tangent by flattening factor\n    return np.degrees(np.arctan(I_o))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of GPTS timescales and Chron labels for the given time period.", "response": "def get_ts(ts):\n    \"\"\"\n    returns GPTS timescales.\n    options are:  ck95, gts04, and gts12\n    returns timescales and Chron labels\n    \"\"\"\n    if ts == 'ck95':\n        TS = [0, 0.780, 0.990, 1.070, 1.770, 1.950, 2.140, 2.150, 2.581, 3.040, 3.110, 3.220, 3.330, 3.580, 4.180, 4.290, 4.480, 4.620, 4.800, 4.890, 4.980, 5.230, 5.894, 6.137, 6.269, 6.567, 6.935, 7.091, 7.135, 7.170, 7.341, 7.375, 7.432, 7.562, 7.650, 8.072, 8.225, 8.257, 8.699, 9.025, 9.230, 9.308, 9.580, 9.642, 9.740, 9.880, 9.920, 10.949, 11.052, 11.099, 11.476, 11.531, 11.935, 12.078, 12.184, 12.401, 12.678, 12.708, 12.775, 12.819, 12.991, 13.139, 13.302, 13.510, 13.703, 14.076, 14.178, 14.612, 14.800, 14.888, 15.034, 15.155, 16.014, 16.293, 16.327, 16.488, 16.556, 16.726, 17.277, 17.615, 18.281, 18.781, 19.048, 20.131, 20.518, 20.725, 20.996, 21.320, 21.768, 21.859, 22.151, 22.248, 22.459, 22.493, 22.588,\n              22.750, 22.804, 23.069, 23.353, 23.535, 23.677, 23.800, 23.999, 24.118, 24.730, 24.781, 24.835, 25.183, 25.496, 25.648, 25.823, 25.951, 25.992, 26.554, 27.027, 27.972, 28.283, 28.512, 28.578, 28.745, 29.401, 29.662, 29.765, 30.098, 30.479, 30.939, 33.058, 33.545, 34.655, 34.940, 35.343, 35.526, 35.685, 36.341, 36.618, 37.473, 37.604, 37.848, 37.920, 38.113, 38.426, 39.552, 39.631, 40.130, 41.257, 41.521, 42.536, 43.789, 46.264, 47.906, 49.037, 49.714, 50.778, 50.946, 51.047, 51.743, 52.364, 52.663, 52.757, 52.801, 52.903, 53.347, 55.904, 56.391, 57.554, 57.911, 60.920, 61.276, 62.499, 63.634, 63.976, 64.745, 65.578, 67.610, 67.735, 68.737, 71.071, 71.338, 71.587, 73.004, 73.291, 73.374, 73.619, 79.075, 83.000]\n        Labels = [['C1n', 0], ['C1r', 0.78], ['C2', 1.77], ['C2An', 2.581], ['C2Ar', 3.58], ['C3n', 4.18], ['C3r', 5.23], ['C3An', 5.894], ['C3Ar', 6.567], ['C3Bn', 6.935], ['C3Br', 7.091], ['C4n', 7.432], ['C4r', 8.072], ['C4An', 8.699], ['C4Ar', 9.025], ['C5n', 9.74], ['C5r', 10.949], ['C5An', 11.935], ['C5Ar', 12.401], ['C5AAn', 12.991], ['C5AAr', 13.139], ['C5ABn', 13.302], ['C5ABr', 13.51], ['C5ACn', 13.703], ['C5ACr', 14.076], ['C5ADn', 14.178], ['C5ADr', 14.612], ['C5Bn', 14.8], ['C5Br', 15.155], ['C5Cn', 16.014], ['C5Cr', 16.726], ['C5Dn', 17.277], ['C5Dr', 17.615], ['C5En', 18.281], ['C5Er', 18.781], ['C6n', 19.048], ['C6r', 20.131], ['C6An', 20.518], ['C6Ar', 21.32], ['C6AAn', 21.768], ['C6AAr', 21.859], ['C6Bn', 22.588], ['C6Br', 23.069], ['C6Cn', 23.353], ['C6Cr', 24.118], ['C7n', 24.73], ['C7r', 25.183], ['C7A', 25.496], ['C8n', 25.823], ['C8r', 26.554], [\n            'C9n', 27.027], ['C9r', 27.972], ['C10n', 28.283], ['C10r', 28.745], ['C11n', 29.401], ['C11r', 30.098], ['C12n', 30.479], ['C12r', 30.939], ['C13n', 33.058], ['C13r', 33.545], ['C15n', 34.655], ['C15r', 34.94], ['C16n', 35.343], ['C16r', 36.341], ['C17n', 36.618], ['C17r', 38.113], ['C18n', 38.426], ['C18r', 40.13], ['C19n', 41.257], ['C19r', 41.521], ['C20n', 42.536], ['C20r', 43.789], ['C21n', 46.264], ['C21r', 47.906], ['C22n', 49.037], ['C22r', 49.714], ['C23n', 50.778], ['C23r', 51.743], ['C24n', 52.364], ['C24r', 53.347], ['C25n', 55.904], ['C25r', 56.391], ['C26n', 57.554], ['C26r', 57.911], ['C27n', 60.92], ['C27r', 61.276], ['C28n', 62.499], ['C28r', 63.634], ['C29n', 63.976], ['C29r', 64.745], ['C30n', 65.578], ['C30r', 67.61], ['C31n', 67.735], ['C31r', 68.737], ['C32n', 71.071], ['C32r', 73.004], ['C33n', 73.619], ['C33r', 79.075], ['C34n', 83]]\n        return TS, Labels\n    if ts == 'gts04':\n        TS = [0, 0.781, 0.988, 1.072, 1.778, 1.945, 2.128, 2.148, 2.581, 3.032, 3.116, 3.207, 3.33, 3.596, 4.187, 4.3, 4.493, 4.631, 4.799, 4.896, 4.997, 5.235, 6.033, 6.252, 6.436, 6.733, 7.14, 7.212, 7.251, 7.285, 7.454, 7.489, 7.528, 7.642, 7.695, 8.108, 8.254, 8.3, 8.769, 9.098, 9.312, 9.409, 9.656, 9.717, 9.779, 9.934, 9.987, 11.04, 11.118, 11.154, 11.554, 11.614, 12.014, 12.116, 12.207, 12.415, 12.73, 12.765, 12.82, 12.878, 13.015, 13.183, 13.369, 13.605, 13.734, 14.095, 14.194, 14.581, 14.784, 14.877, 15.032, 15.16, 15.974, 16.268, 16.303, 16.472, 16.543, 16.721, 17.235, 17.533, 17.717, 17.74, 18.056, 18.524, 18.748, 20, 20.04, 20.213, 20.439, 20.709, 21.083, 21.159, 21.403, 21.483, 21.659, 21.688, 21.767,\n              21.936, 21.992, 22.268, 22.564, 22.754, 22.902, 23.03, 23.249, 23.375, 24.044, 24.102, 24.163, 24.556, 24.915, 25.091, 25.295, 25.444, 25.492, 26.154, 26.714, 27.826, 28.186, 28.45, 28.525, 28.715, 29.451, 29.74, 29.853, 30.217, 30.627, 31.116, 33.266, 33.738, 34.782, 35.043, 35.404, 35.567, 35.707, 36.276, 36.512, 37.235, 37.345, 37.549, 37.61, 37.771, 38.032, 38.975, 39.041, 39.464, 40.439, 40.671, 41.59, 42.774, 45.346, 47.235, 48.599, 49.427, 50.73, 50.932, 51.057, 51.901, 52.648, 53.004, 53.116, 53.167, 53.286, 53.808, 56.665, 57.18, 58.379, 58.737, 61.65, 61.983, 63.104, 64.128, 64.432, 65.118, 65.861, 67.696, 67.809, 68.732, 70.961, 71.225, 71.474, 72.929, 73.231, 73.318, 73.577, 79.543, 84]\n        Labels = [['C1n', 0.000], ['C1r', 0.781], ['C2', 1.778], ['C2An', 2.581], ['C2Ar', 3.596], ['C3n', 4.187], ['C3r', 5.235], ['C3An', 6.033], ['C3Ar', 6.733], ['C3Bn', 7.140], ['C3Br', 7.212], ['C4n', 7.528], ['C4r', 8.108], ['C4An', 8.769], ['C4Ar', 9.098], ['C5n', 9.779], ['C5r', 11.040], ['C5An', 12.014], ['C5Ar', 12.415], ['C5AAn', 13.015], ['C5AAr', 13.183], ['C5ABn', 13.369], ['C5ABr', 13.605], ['C5ACn', 13.734], ['C5ACr', 14.095], ['C5ADn', 14.194], ['C5ADr', 14.581], ['C5Bn', 14.784], ['C5Br', 15.160], ['C5Cn', 15.974], ['C5Cr', 16.721], ['C5Dn', 17.235], ['C5Dr', 17.533], ['C5En', 18.056], ['C5Er', 18.524], ['C6n', 18.748], ['C6r', 19.772], ['C6An', 20.040], ['C6Ar', 20.709], ['C6AAn', 21.083], ['C6AAr', 21.159], ['C6Bn', 21.767], ['C6Br', 22.268], ['C6Cn', 22.564], ['C6Cr', 23.375], ['C7n', 24.044], ['C7r', 24.556], ['C7A', 24.919], ['C8n', 25.295], [\n            'C8r', 26.154], ['C9n', 26.714], ['C9r', 27.826], ['C10n', 28.186], ['C11n', 29.451], ['C11r', 30.217], ['C12n', 30.627], ['C12r', 31.116], ['C13n', 33.266], ['C13r', 33.738], ['C15n', 34.782], ['C15r', 35.043], ['C16n', 35.404], ['C16r', 36.276], ['C17n', 36.512], ['C17r', 37.771], ['C18n', 38.032], ['C18r', 39.464], ['C19n', 40.439], ['C19r', 40.671], ['C20n', 41.590], ['C20r', 42.774], ['C21n', 45.346], ['C21r', 47.235], ['C22n', 48.599], ['C22r', 49.427], ['C23n', 50.730], ['C23r', 51.901], ['C24n', 52.648], ['C24r', 53.808], ['C25n', 56.665], ['C25r', 57.180], ['C26n', 58.379], ['C26r', 58.737], ['C27n', 61.650], ['C27r', 61.938], ['C28n', 63.104], ['C28r', 64.128], ['C29n', 64.432], ['C29r', 65.118], ['C30n', 65.861], ['C30r', 67.696], ['C31n', 67.809], ['C31r', 68.732], ['C32n', 70.961], ['C32r', 72.929], ['C33n', 73.577], ['C33r', 79.543], ['C34n', 84.000]]\n        return TS, Labels\n    if ts == 'gts12':\n        TS = [0, 0.781, 0.988, 1.072, 1.173, 1.185, 1.778, 1.945, 2.128, 2.148, 2.581, 3.032, 3.116, 3.207, 3.330, 3.596, 4.187, 4.300, 4.493, 4.631, 4.799, 4.896, 4.997, 5.235, 6.033, 6.252, 6.436, 6.733, 7.140, 7.212, 7.251, 7.285, 7.454, 7.489, 7.528, 7.642, 7.695, 8.108, 8.254, 8.300, 8.771, 9.105, 9.311, 9.426, 9.647, 9.721, 9.786, 9.937, 9.984, 11.056, 11.146, 11.188, 11.592, 11.657, 12.049, 12.174, 12.272, 12.474, 12.735, 12.770, 12.829, 12.887, 13.032, 13.183, 13.363, 13.608, 13.739, 14.070, 14.163, 14.609, 14.775, 14.870, 15.032, 15.160, 15.974, 16.268, 16.303, 16.472, 16.543, 16.721, 17.235, 17.533, 17.717, 17.740, 18.056, 18.524, 18.748, 19.722, 20.040, 20.213, 20.439, 20.709, 21.083, 21.159, 21.403, 21.483, 21.659,\n              21.688, 21.767, 21.936, 21.992, 22.268, 22.564, 22.754, 22.902, 23.030, 23.233, 23.295, 23.962, 24.000, 24.109, 24.474, 24.761, 24.984, 25.099, 25.264, 25.304, 25.987, 26.420, 27.439, 27.859, 28.087, 28.141, 28.278, 29.183, 29.477, 29.527, 29.970, 30.591, 31.034, 33.157, 33.705, 34.999, 35.294, 35.706, 35.892, 36.051, 36.700, 36.969, 37.753, 37.872, 38.093, 38.159, 38.333, 38.615, 39.627, 39.698, 40.145, 41.154, 41.390, 42.301, 43.432, 45.724, 47.349, 48.566, 49.344, 50.628, 50.835, 50.961, 51.833, 52.620, 53.074, 53.199, 53.274, 53.416, 53.983, 57.101, 57.656, 58.959, 59.237, 62.221, 62.517, 63.494, 64.667, 64.958, 65.688, 66.398, 68.196, 68.369, 69.269, 71.449, 71.689, 71.939, 73.649, 73.949, 74.049, 74.309, 79.900, 83.64]\n        Labels = [['C1n', 0.000], ['C1r', 0.781], ['C2n', 1.778], ['C2r', 1.945], ['C2An', 2.581], ['C2Ar', 3.596], ['C3n', 4.187], ['C3r', 5.235], ['C3An', 6.033], ['C3Ar', 6.733], ['C3Bn', 7.140], ['C3Br', 7.212], ['C4n', 7.528], ['C4r', 8.108], ['C4An', 8.771], ['C4Ar', 9.105], ['C5n', 9.786], ['C5r', 11.056], ['C5An', 12.049], ['C5Ar', 12.474], ['C5AAn', 13.032], ['C5AAr', 13.183], ['C5ABn', 13.363], ['C5ABr', 13.608], ['C5ACn', 13.739], ['C5ACr', 14.070], ['C5ADn', 14.163], ['C5ADr', 14.609], ['C5Bn', 14.775], ['C5Br', 15.160], ['C5Cn', 15.974], ['C5Cr', 16.721], ['C5Dn', 17.235], ['C5Dr', 17.533], ['C5En', 18.056], ['C5Er', 18.524], ['C6n', 18.748], ['C6r', 19.722], ['C6An', 20.040], ['C6Ar', 20.709], ['C6AAn', 21.083], ['C6AAr', 21.159], ['C6Bn', 21.767], ['C6Br', 22.268], ['C6Cn', 22.564], ['C6Cr', 23.295], ['C7n', 23.962], ['C7r', 24.474], ['C7An', 24.761], ['C7Ar', 24.984], ['C8n', 25.099], [\n            'C8r', 25.987], ['C9n', 26.420], ['C9r', 27.439], ['C10n', 27.859], ['C10r', 28.278], ['C11n', 29.183], ['C11r', 29.970], ['C12n', 30.591], ['C12r', 31.034], ['C13n', 33.157], ['C13r', 33.705], ['C15n', 34.999], ['C15r', 35.294], ['C16n', 35.706], ['C16r', 36.700], ['C17n', 36.969], ['C17r', 38.333], ['C18n', 38.615], ['C18r', 40.145], ['C19n', 41.154], ['C19r', 41.390], ['C20n', 42.301], ['C20r', 43.432], ['C21n', 45.724], ['C21r', 47.349], ['C22n', 48.566], ['C22r', 49.344], ['C23n', 50.628], ['C23r', 51.833], ['C24n', 52.620], ['C24r', 53.983], ['C25n', 57.101], ['C25r', 57.656], ['C26n', 58.959], ['C26r', 59.237], ['C27n', 62.221], ['C27r', 62.517], ['C28n', 63.494], ['C28r', 64.667], ['C29n', 64.958], ['C29r', 65.688], ['C30n', 66.398], ['C30r', 68.196], ['C31n', 68.369], ['C31r', 69.269], ['C32n', 71.449], ['C32r', 73.649], ['C33n', 74.309], ['C33r', 79.900], ['C34n', 83.64]]\n        return TS, Labels\n    print(\"Time Scale Option Not Available\")\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a statement in a new namespace and return the output of the statement.", "response": "def execute(st, **kwargs):\n    \"\"\"\n    Work around for Python3 exec function which doesn't allow changes to the local namespace because of scope.\n    This breaks a lot of the old functionality in the code which was origionally in Python2. So this function\n    runs just like exec except that it returns the output of the input statement to the local namespace. It may\n    break if you start feeding it multiline monoliths of statements (haven't tested) but you shouldn't do that\n    anyway (bad programming).\n\n    Parameters\n    -----------\n    st : the statement you want executed and for which you want the return\n    kwargs : anything that may need to be in this namespace to execute st\n\n    Returns\n    -------\n    The return value of executing the input statement\n    \"\"\"\n    namespace = kwargs\n    exec(\"b = {}\".format(st), namespace)\n    return namespace['b']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the acceptance criteria for thellier_gui and demag_gui", "response": "def initialize_acceptance_criteria(**kwargs):\n    '''\n    initialize acceptance criteria with NULL values for thellier_gui and demag_gui\n\n    acceptance criteria format is doctionaries:\n\n    acceptance_criteria={}\n        acceptance_criteria[crit]={}\n            acceptance_criteria[crit]['category']=\n            acceptance_criteria[crit]['criterion_name']=\n            acceptance_criteria[crit]['value']=\n            acceptance_criteria[crit]['threshold_type']\n            acceptance_criteria[crit]['decimal_points']\n\n   'category':\n       'DE-SPEC','DE-SAMP'..etc\n   'criterion_name':\n       MagIC name\n   'value':\n        a number (for 'regular criteria')\n        a string (for 'flag')\n        1 for True (if criteria is bullean)\n        0 for False (if criteria is bullean)\n        -999 means N/A\n   'threshold_type':\n       'low'for low threshold value\n       'high'for high threshold value\n        [flag1.flag2]: for flags\n        'bool' for boolean flags (can be 'g','b' or True/Flase or 1/0)\n   'decimal_points':\n       number of decimal points in rounding\n       (this is used in displaying criteria in the dialog box)\n       -999 means Exponent with 3 descimal points for floats and string for string\n    '''\n\n    acceptance_criteria = {}\n    # --------------------------------\n    # 'DE-SPEC'\n    # --------------------------------\n    # low cutoff value\n    category = 'DE-SPEC'\n    for crit in ['specimen_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    category = 'DE-SPEC'\n    for crit in ['specimen_mad', 'specimen_dang', 'specimen_alpha95']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        acceptance_criteria[crit]['decimal_points'] = 1\n\n    # flag\n    for crit in ['specimen_direction_type']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        if crit == 'specimen_direction_type':\n            acceptance_criteria[crit]['threshold_type'] = ['l', 'p']\n        if crit == 'specimen_polarity':\n            acceptance_criteria[crit]['threshold_type'] = [\n                'n', 'r', 't', 'e', 'i']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'DE-SAMP'\n    # --------------------------------\n\n    # low cutoff value\n    category = 'DE-SAMP'\n    for crit in ['sample_n', 'sample_n_lines', 'sample_n_planes']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    category = 'DE-SAMP'\n    for crit in ['sample_r', 'sample_alpha95', 'sample_sigma', 'sample_k', 'sample_tilt_correction']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['sample_tilt_correction']:\n            acceptance_criteria[crit]['decimal_points'] = 0\n        elif crit in ['sample_alpha95']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # flag\n    for crit in ['sample_direction_type', 'sample_polarity']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        if crit == 'sample_direction_type':\n            acceptance_criteria[crit]['threshold_type'] = ['l', 'p']\n        if crit == 'sample_polarity':\n            acceptance_criteria[crit]['threshold_type'] = [\n                'n', 'r', 't', 'e', 'i']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'DE-SITE'\n    # --------------------------------\n\n    # low cutoff value\n    category = 'DE-SITE'\n    for crit in ['site_n', 'site_n_lines', 'site_n_planes']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['site_k', 'site_r', 'site_alpha95', 'site_sigma', 'site_tilt_correction']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['site_tilt_correction']:\n            acceptance_criteria[crit]['decimal_points'] = 0\n        else:\n            acceptance_criteria[crit]['decimal_points'] = 1\n\n    # flag\n    for crit in ['site_direction_type', 'site_polarity']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        if crit == 'site_direction_type':\n            acceptance_criteria[crit]['threshold_type'] = ['l', 'p']\n        if crit == 'site_polarity':\n            acceptance_criteria[crit]['threshold_type'] = [\n                'n', 'r', 't', 'e', 'i']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'DE-STUDY'\n    # --------------------------------\n    category = 'DE-STUDY'\n    # low cutoff value\n    for crit in ['average_k', 'average_n', 'average_nn', 'average_nnn', 'average_r']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        if crit in ['average_n', 'average_nn', 'average_nnn']:\n            acceptance_criteria[crit]['decimal_points'] = 0\n        elif crit in ['average_alpha95']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # high cutoff value\n    for crit in ['average_alpha95', 'average_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['average_alpha95']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'IE-SPEC' (a long list from SPD.v.1.0)\n    # --------------------------------\n    category = 'IE-SPEC'\n\n    # low cutoff value\n    for crit in ['specimen_int_n', 'specimen_f', 'specimen_fvds', 'specimen_frac', 'specimen_q', 'specimen_w', 'specimen_r_sq', 'specimen_int_ptrm_n',\n                 'specimen_int_ptrm_tail_n', 'specimen_ac_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n        if crit in ['specimen_int_n', 'specimen_int_ptrm_n', 'specimen_int_ptrm_tail_n', 'specimen_ac_n']:\n            acceptance_criteria[crit]['decimal_points'] = 0\n        elif crit in ['specimen_f', 'specimen_fvds', 'specimen_frac', 'specimen_q']:\n            acceptance_criteria[crit]['decimal_points'] = 2\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # high cutoff value\n    for crit in ['specimen_b_sigma', 'specimen_b_beta', 'specimen_g', 'specimen_gmax', 'specimen_k', 'specimen_k_sse', 'specimen_k_prime', 'specimen_k_prime_sse',\n                 'specimen_coeff_det_sq', 'specimen_z', 'specimen_z_md', 'specimen_int_mad', 'specimen_int_mad_anc', 'specimen_int_alpha', 'specimen_alpha', 'specimen_alpha_prime',\n                 'specimen_theta', 'specimen_int_dang', 'specimen_int_crm', 'specimen_ptrm', 'specimen_dck', 'specimen_drat', 'specimen_maxdev', 'specimen_cdrat',\n                 'specimen_drats', 'specimen_mdrat', 'specimen_mdev', 'specimen_dpal', 'specimen_tail_drat', 'specimen_dtr', 'specimen_md', 'specimen_dt', 'specimen_dac', 'specimen_gamma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['specimen_int_mad', 'specimen_int_mad_anc', 'specimen_int_dang', 'specimen_drat', 'specimen_cdrat', 'specimen_drats', 'specimen_tail_drat', 'specimen_dtr', 'specimen_md', 'specimen_dac', 'specimen_gamma']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        elif crit in ['specimen_gmax']:\n            acceptance_criteria[crit]['decimal_points'] = 2\n        elif crit in ['specimen_b_sigma', 'specimen_b_beta', 'specimen_g', 'specimen_k', 'specimen_k_prime']:\n            acceptance_criteria[crit]['decimal_points'] = 3\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # flags\n    for crit in ['specimen_scat']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = 'bool'\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'IE-SAMP'\n    # --------------------------------\n    category = 'IE-SAMP'\n\n    # low cutoff value\n    for crit in ['sample_int_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['sample_int_rel_sigma', 'sample_int_rel_sigma_perc', 'sample_int_sigma', 'sample_int_sigma_perc']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['sample_int_rel_sigma_perc', 'sample_int_sigma_perc']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'IE-SITE'\n    # --------------------------------\n    category = 'IE-SITE'\n\n    # low cutoff value\n    for crit in ['site_int_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['site_int_rel_sigma', 'site_int_rel_sigma_perc', 'site_int_sigma', 'site_int_sigma_perc']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['site_int_rel_sigma_perc', 'site_int_sigma_perc']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'IE-STUDY'\n    # --------------------------------\n    category = 'IE-STUDY'\n    # low cutoff value\n    for crit in ['average_int_n', 'average_int_n', 'average_int_nn', 'average_int_nnn', ]:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['average_int_rel_sigma', 'average_int_rel_sigma_perc', 'average_int_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        if crit in ['average_int_rel_sigma_perc']:\n            acceptance_criteria[crit]['decimal_points'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'NPOLE'\n    # --------------------------------\n    category = 'NPOLE'\n    # flags\n    for crit in ['site_polarity']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = ['n', 'r']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'NPOLE'\n    # --------------------------------\n    category = 'RPOLE'\n    # flags\n    for crit in ['site_polarity']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = ['n', 'r']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'VADM'\n    # --------------------------------\n    category = 'VADM'\n    # low cutoff value\n    for crit in ['vadm_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        if crit in ['vadm_n']:\n            acceptance_criteria[crit]['decimal_points'] = 0\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'VADM'\n    # --------------------------------\n    category = 'VADM'\n    # low cutoff value\n    for crit in ['vadm_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['vadm_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'VADM'\n    # --------------------------------\n    category = 'VDM'\n    # low cutoff value\n    for crit in ['vdm_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['vdm_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'VGP'\n    # --------------------------------\n    category = 'VDM'\n    # low cutoff value\n    for crit in ['vgp_n']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = 0\n\n    # high cutoff value\n    for crit in ['vgp_alpha95', 'vgp_dm', 'vgp_dp', 'vgp_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        if crit in ['vgp_alpha95']:\n            acceptance_criteria[crit]['decimal_points', 'vgp_dm', 'vgp_dp'] = 1\n        else:\n            acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'AGE'\n    # --------------------------------\n    category = 'AGE'\n    # low cutoff value\n    for crit in ['average_age_min']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"low\"\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # high cutoff value\n    for crit in ['average_age_max', 'average_age_sigma']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # flags\n    for crit in ['average_age_unit']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = [\n            'Ga', 'Ka', 'Ma', 'Years AD (+/-)', 'Years BP', 'Years Cal AD (+/-)', 'Years Cal BP']\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    # --------------------------------\n    # 'ANI'\n    # --------------------------------\n    category = 'ANI'\n    # high cutoff value\n    for crit in ['anisotropy_alt', 'sample_aniso_mean', 'site_aniso_mean']:  # value is in precent\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = \"high\"\n        acceptance_criteria[crit]['decimal_points'] = 3\n\n    # flags\n\n    for crit in ['specimen_aniso_ftest_flag']:\n        acceptance_criteria[crit] = {}\n        acceptance_criteria[crit]['category'] = category\n        acceptance_criteria[crit]['criterion_name'] = crit\n        acceptance_criteria[crit]['value'] = -999\n        acceptance_criteria[crit]['threshold_type'] = 'bool'\n        acceptance_criteria[crit]['decimal_points'] = -999\n\n    return(acceptance_criteria)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_criteria_from_file(path, acceptance_criteria, **kwargs):\n    '''\n    Read accceptance criteria from magic criteria file\n    # old format:\n    multiple lines.  pmag_criteria_code defines the type of criteria\n\n    to deal with old format this function reads all the lines and ignore empty cells.\n    i.e., the program assumes that in each column there is only one value (in one of the lines)\n\n    special case in the old format:\n        specimen_dang has a value and pmag_criteria_code is IE-specimen.\n        The program assumes that the user means specimen_int_dang\n    # New format for thellier_gui and demag_gui:\n    one long line. pmag_criteria_code=ACCEPT\n\n    path is the full path to the criteria file\n\n    the function takes exiting acceptance_criteria\n    and updtate it with criteria from file\n\n    output:\n    acceptance_criteria={}\n    acceptance_criteria[MagIC Variable Names]={}\n    acceptance_criteria[MagIC Variable Names]['value']:\n        a number for acceptance criteria value\n        -999 for N/A\n        1/0 for True/False or Good/Bad\n    acceptance_criteria[MagIC Variable Names]['threshold_type']:\n        \"low\":  lower cutoff value i.e. crit>=value pass criteria\n        \"high\": high cutoff value i.e. crit<=value pass criteria\n        [string1,string2,....]: for flags\n    acceptance_criteria[MagIC Variable Names]['decimal_points']:number of decimal points in rounding\n            (this is used in displaying criteria in the dialog box)\n\n    '''\n    warnings = []\n    acceptance_criteria_list = list(acceptance_criteria.keys())\n    if 'data_model' in list(kwargs.keys()) and kwargs['data_model'] == 3:\n        crit_data = acceptance_criteria  # data already read in\n    else:\n        crit_data, file_type = magic_read(path)\n        if 'criteria' not in file_type:\n            if 'empty' in file_type:\n                print('-W- No criteria found: {} '.format(path))\n            else:\n                print(\n                    '-W- {} could not be read and may be improperly formatted...'.format(path))\n    for rec in crit_data:\n        # gather metadata\n        metadata_dict = {'pmag_criteria_code': '',\n                         'criteria_definition': '', 'er_citation_names': ''}\n        for metadata in metadata_dict:\n            if metadata in rec:\n                metadata_dict[metadata] = rec[metadata]\n        # check each record for correct name and compatibility\n        for crit in list(rec.keys()):\n            if crit == 'anisotropy_ftest_flag' and crit not in list(rec.keys()):\n                crit = 'specimen_aniso_ftest_flag'  # convert legacy criterion to 2.5\n            rec[crit] = rec[crit].strip('\\n')\n            if crit in ['pmag_criteria_code', 'criteria_definition', 'magic_experiment_names', 'er_citation_names']:\n                continue\n            elif rec[crit] == \"\":\n                continue\n\n            # this catches all the ones that are being overwritten\n            if crit in acceptance_criteria:\n                if acceptance_criteria[crit]['value'] not in [-999, '-999', -999]:\n\n                    print(\n                        \"-W- You have multiple different criteria that both use column: {}.\\nThe last will be used:\\n{}.\".format(crit, rec))\n                    warn_string = 'multiple criteria for column: {} (only last will be used)'.format(\n                        crit)\n                    if warn_string not in warnings:\n                        warnings.append(warn_string)\n            if crit == \"specimen_dang\" and \"pmag_criteria_code\" in list(rec.keys()) and \"IE-SPEC\" in rec[\"pmag_criteria_code\"]:\n                crit = \"specimen_int_dang\"\n                print(\"-W- Found backward compatibility problem with selection criteria specimen_dang. Cannot be associated with IE-SPEC. Program assumes that the statistic is specimen_int_dang\")\n                if 'specimen_int_dang' not in acceptance_criteria:\n                    acceptance_criteria[\"specimen_int_dang\"] = {}\n                acceptance_criteria[\"specimen_int_dang\"]['value'] = float(\n                    rec[\"specimen_dang\"])\n\n            elif crit not in acceptance_criteria_list:\n                print(\n                    \"-W- WARNING: criteria code %s is not supported by PmagPy GUI. please check\" % crit)\n                acceptance_criteria[crit] = {}\n                acceptance_criteria[crit]['value'] = rec[crit]\n                acceptance_criteria[crit]['threshold_type'] = \"inherited\"\n                acceptance_criteria[crit]['decimal_points'] = -999\n                acceptance_criteria[crit]['category'] = None\n            # boolean flag\n            elif acceptance_criteria[crit]['threshold_type'] == 'bool':\n                if str(rec[crit]) in ['1', 'g', 'True', 'TRUE']:\n                    acceptance_criteria[crit]['value'] = True\n                else:\n                    acceptance_criteria[crit]['value'] = False\n\n            # criteria as flags\n            elif type(acceptance_criteria[crit]['threshold_type']) == list:\n                if str(rec[crit]) in acceptance_criteria[crit]['threshold_type']:\n                    acceptance_criteria[crit]['value'] = str(rec[crit])\n                else:\n                    print(\n                        \"-W- WARNING: data %s from criteria code  %s and is not supported by PmagPy GUI. please check\" % (crit, rec[crit]))\n            elif float(rec[crit]) == -999:\n                pass\n            else:\n                acceptance_criteria[crit]['value'] = float(rec[crit])\n            # add in metadata to each record\n            acceptance_criteria[crit].update(metadata_dict)\n    if \"return_warnings\" in kwargs:\n        return (acceptance_criteria, warnings)\n    else:\n        return(acceptance_criteria)", "response": "Read accceptance criteria from a magic criteria file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a flag to a variable if it is not present.", "response": "def add_flag(var, flag):\n    \"\"\"\n    for use when calling command-line scripts from withing a program.\n    if a variable is present, add its proper command_line flag.\n    return a string.\n    \"\"\"\n    if var:\n        var = flag + \" \" + str(var)\n    else:\n        var = \"\"\n    return var"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value after a named command - line flag such as - f or sys. argv if it is present in sys. argv otherwise default_val.", "response": "def get_named_arg(name, default_val=None, reqd=False):\n    \"\"\"\n    Extract the value after a command-line flag such as '-f' and return it.\n    If the command-line flag is missing, return default_val.\n    If reqd == True and the command-line flag is missing, throw an error.\n\n    Parameters\n    ----------\n    name : str\n        command line flag, e.g. \"-f\"\n    default_val\n        value to use if command line flag is missing, e.g. \"measurements.txt\"\n        default is None\n    reqd : bool\n        throw error if reqd==True and command line flag is missing.\n        if reqd == True, default_val will be ignored.\n        default is False.\n\n    Returns\n    ---------\n    Desired value from sys.argv if available, otherwise default_val.\n    \"\"\"\n    if name in sys.argv:  # if the command line flag is found in sys.argv\n        ind = sys.argv.index(name)\n        return sys.argv[ind + 1]\n    if reqd:  # if arg is required but not present\n        raise MissingCommandLineArgException(name)\n    return default_val"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a list of recs [ rec1 rec2 rec3... ] each rec is a dictionary.", "response": "def merge_recs_headers(recs):\n    '''\n    take a list of recs [rec1,rec2,rec3....], each rec is a dictionary.\n    make sure that all recs have the same headers.\n    '''\n    headers = []\n    for rec in recs:\n        keys = list(rec.keys())\n        for key in keys:\n            if key not in headers:\n                headers.append(key)\n    for rec in recs:\n        for header in headers:\n            if header not in list(rec.keys()):\n                rec[header] = \"\"\n    return recs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_file_name(fname, dir_path='.'):\n    if not fname:\n        return ''\n    file_dir_path, file_name = os.path.split(fname)\n    if (not file_dir_path) or (file_dir_path == '.'):\n        full_file = os.path.join(dir_path, fname)\n    else:\n        full_file = fname\n    return os.path.realpath(full_file)", "response": "Parse file name information and output full path."}
