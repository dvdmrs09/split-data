{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npulling a repository :param remote: git-remote instance :param branch: name of the branch to pull", "response": "def pull(self, remote, branch=None):\n        '''Pull a repository\n        :param remote: git-remote instance\n        :param branch: name of the branch to pull\n        '''\n        pb = ProgressBar()\n        pb.setup(self.name)\n        if branch:\n            remote.pull(branch, progress=pb)\n        else: # pragma: no cover\n            remote.pull(progress=pb)\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes a repository :param remote: git-remote instance :param branch: name of the branch to push :return: PushInfo, git push output lines", "response": "def push(self, remote, branch=None):\n        '''Push a repository\n        :param remote: git-remote instance\n        :param branch: name of the branch to push\n        :return: PushInfo, git push output lines\n        '''\n        pb = ProgressBar()\n        pb.setup(self.name, ProgressBar.Action.PUSH)\n        if branch:\n            result = remote.push(branch, progress=pb)\n        else: #pragma: no cover\n            result = remote.push(progress=pb)\n        print()\n        return result, pb.other_lines"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npull a repository :param remote: git-remote instance :param branch: name of the branch to pull", "response": "def fetch(self, remote, branch, local_branch = None, force=False):\n        '''Pull a repository\n        :param remote: git-remote instance\n        :param branch: name of the branch to pull\n        '''\n        pb = ProgressBar()\n        pb.setup(self.name)\n\n        if local_branch:\n            branch = ':'.join([branch, local_branch])\n\n        remote.fetch(branch, update_head_ok=True, force=force, progress=pb)\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(self, user, repo, branch=None, rw=True):\n        '''Clones a new repository\n\n        :param user: namespace of the repository\n        :param repo: name slug of the repository\n        :Param branch: branch to pull as tracking\n\n        This command is fairly simple, and pretty close to the real `git clone`\n        command, except it does not take a full path, but just a namespace/slug\n        path for a given service.\n        '''\n        log.info('Cloning {}\u2026'.format(repo))\n\n        project = self.get_repository(user, repo)\n        if not branch:\n            branch = self.get_project_default_branch(project)\n\n        is_empty = self.is_repository_empty(project)\n        if is_empty:\n            self.repository.init()\n        else:\n            url = self.get_parent_project_url(user, repo, rw=rw)\n            if url:\n                parent_user, parent_project = self.convert_url_into_slug(url).split('/')\n                self.add(user=parent_user, repo=parent_project, name='upstream', alone=True)\n\n        remote, *_ = self.add(user=user, repo=repo, tracking=True, rw=rw)\n        if not is_empty:\n            self.pull(remote, branch)", "response": "Clones a new repository and pulls it as tracking\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, repo, user=None, name=None, tracking=False, alone=False, rw=True, auto_slug=False):\n        '''Adding repository as remote\n\n        :param repo: Name slug of the repository to add\n        :param name: Name of the remote when stored\n        :param tracking: When set, makes this remote the tracking for master operations\n        :param alone: When set, exclude this remote from the \"all\" remote\n\n        This method creates a new remote within the current repository *repo*.\n        It chooses the name of the repository based on the service's name as\n        held by current instance of RepositoryService, or uses the *name*\n        parameter.\n\n        It also creates an *all* remote that contains all the remotes added by\n        this tool, to make it possible to push to all remotes at once.\n        '''\n        # git vcs add upstream\n        if repo == 'upstream':\n            try:\n                slug = self.convert_url_into_slug(self.repository.remote(self.name).url)\n                if not slug:\n                    raise ValueError()\n            except ValueError:\n                raise ResourceNotFoundError('No existing remote for {} to find an upstream of'.format(self.name))\n\n            url = self.get_parent_project_url(*slug.split('/'))\n            if not url:\n                raise ResourceNotFoundError('No upstream on {} found for this project.'.format(self.name))\n            slug = self.convert_url_into_slug(url)\n            if not slug:\n                raise ResourceNotFoundError('No upstream on {} found for this project.'.format(self.name))\n            user, repo = slug.split('/')\n            name = 'upstream'\n            alone = True\n            auto_slug = False\n\n        # git vcs add\n        if auto_slug and not name:\n            remote_urls = set()\n            for remote in self.repository.remotes:\n                for url in remote.urls:\n                    if self.fqdn in url:\n                        remote_urls.add(self.convert_url_into_slug(url))\n            remote_urls = list(remote_urls)\n            if len(remote_urls) > 0:\n                raise ResourceNotFoundError('Couldn\\'t find a remote to '\n                        '{service}. Please run git add {service} user/project'.format(service=self.name))\n            for idx, url in enumerate(remote_urls, 1):\n                print(\"[{:d}] {}\".format(idx, url))\n            try:\n                remote_url = int(input('Please choose an slug to use as \\'{}\\' remote: '.format(self.name)))\n            except ValueError:\n                raise ArgumentError('Please enter a valid integer value.')\n            if remote_url < 1 or remote_url > idx:\n                raise ArgumentError('Please enter one of the suggested choices.')\n            user, repo = remote_urls[remote_url-1].split('/')\n            name = self.name\n\n        if not name:\n            name = self.name\n\n        if not user:\n            if '/' in repo:\n                user, repo = repo.split('/')\n            else:\n                raise ArgumentError('Unable to parse repository {}, missing path separator.'.format(repo))\n\n        # removing remote if it already exists\n        # and extract all repository\n        all_remote = None\n        for r in self.repository.remotes:\n            if r.name == name:\n                self.repository.delete_remote(r)\n            # find and stash the remote 'all'\n            elif r.name == 'all':\n                all_remote = r\n        # update remote 'all'\n        if not alone:\n            # if remote all does not exists\n            if not all_remote:\n                self.repository.create_remote('all', self.format_path(repo, user, rw=rw))\n            else:\n                url = self.format_path(repo, user, rw=True)\n                # check if url not already in remote all\n                if url not in all_remote.urls:\n                    all_remote.set_url(new_url=self.format_path(repo, user, rw=rw), add=True)\n\n        # adding \"self\" as the tracking remote\n        if tracking:\n            remote = self.repository.create_remote(name, self.format_path(repo, user, rw=rw))\n            # lookup tracking branch (usually master)\n            for branch in self.repository.branches:\n                if tracking == branch.name:\n                    # set that branch as tracking\n                    branch.set_tracking_branch(remote.refs[0])\n                    break\n        else:\n            remote = self.repository.create_remote(name, self.format_path(repo, user, rw=rw))\n\n        return remote, user, repo", "response": "Add a new remote to the current repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens the URL of a repository in the user s browser", "response": "def open(self, user=None, repo=None):\n        '''Open the URL of a repository in the user's browser'''\n        webbrowser.open(self.format_path(repo, namespace=user, rw=False))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches given request as a branch and switch if pull is true", "response": "def request_fetch(self, user, repo, request, pull=False, force=False): #pragma: no cover\n        '''Fetches given request as a branch, and switch if pull is true\n\n        :param repo: name of the repository to create\n\n        Meant to be implemented by subclasses\n        '''\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, object, callback):\n\t\tdef signal_fired(sender, object, iface, signal, params):\n\t\t\tcallback(*params)\n\t\treturn object._bus.subscribe(sender=object._bus_name, object=object._path, iface=self._iface_name, signal=self.__name__, signal_fired=signal_fired)", "response": "Subscribe to the signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a remote object.", "response": "def get(self, bus_name, object_path=None, **kwargs):\n\t\t\"\"\"Get a remote object.\n\n\t\tParameters\n\t\t----------\n\t\tbus_name : string\n\t\t\tName of the service that exposes this object.\n\t\t\tYou may start with \".\" - then org.freedesktop will be automatically prepended.\n\t\tobject_path : string, optional\n\t\t\tPath of the object. If not provided, bus_name translated to path format is used.\n\n\t\tReturns\n\t\t-------\n\t\tProxyObject implementing all the Interfaces exposed by the remote object.\n\t\tNote that it inherits from multiple Interfaces, so the method you want to use\n\t\tmay be shadowed by another one, eg. from a newer version of the interface.\n\t\tTherefore, to interact with only a single interface, use:\n\t\t>>> bus.get(\"org.freedesktop.systemd1\")[\"org.freedesktop.systemd1.Manager\"]\n\t\tor simply\n\t\t>>> bus.get(\".systemd1\")[\".Manager\"]\n\t\twhich will give you access to the one specific interface.\n\t\t\"\"\"\n\t\t# Python 2 sux\n\t\tfor kwarg in kwargs:\n\t\t\tif kwarg not in (\"timeout\",):\n\t\t\t\traise TypeError(self.__qualname__ + \" got an unexpected keyword argument '{}'\".format(kwarg))\n\t\ttimeout = kwargs.get(\"timeout\", None)\n\n\t\tbus_name = auto_bus_name(bus_name)\n\t\tobject_path = auto_object_path(bus_name, object_path)\n\n\t\tret = self.con.call_sync(\n\t\t\tbus_name, object_path,\n\t\t\t'org.freedesktop.DBus.Introspectable', \"Introspect\", None, GLib.VariantType.new(\"(s)\"),\n\t\t\t0, timeout_to_glib(timeout), None)\n\n\t\tif not ret:\n\t\t\traise KeyError(\"no such object; you might need to pass object path as the 2nd argument for get()\")\n\n\t\txml, = ret.unpack()\n\n\t\ttry:\n\t\t\tintrospection = ET.fromstring(xml)\n\t\texcept:\n\t\t\traise KeyError(\"object provides invalid introspection XML\")\n\n\t\treturn CompositeInterface(introspection)(self, bus_name, object_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_name(self, name, allow_replacement=True, replace=False):\n\t\treturn NameOwner(self, name, allow_replacement, replace)", "response": "Requests a bus name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubscribe to matching signals on connection and invokes signal_fired callback if the signal is received.", "response": "def subscribe(self, sender=None, iface=None, signal=None, object=None, arg0=None, flags=0, signal_fired=None):\n\t\t\"\"\"Subscribes to matching signals.\n\n\t\tSubscribes to signals on connection and invokes signal_fired callback\n\t\twhenever the signal is received.\n\n\t\tTo receive signal_fired callback, you need an event loop.\n\t\thttps://github.com/LEW21/pydbus/blob/master/doc/tutorial.rst#setting-up-an-event-loop\n\n\t\tParameters\n\t\t----------\n\t\tsender : string, optional\n\t\t\tSender name to match on (unique or well-known name) or None to listen from all senders.\n\t\tiface : string, optional\n\t\t\tInterface name to match on or None to match on all interfaces.\n\t\tsignal : string, optional\n\t\t\tSignal name to match on or None to match on all signals.\n\t\tobject : string, optional\n\t\t\tObject path to match on or None to match on all object paths.\n\t\targ0 : string, optional\n\t\t\tContents of first string argument to match on or None to match on all kinds of arguments.\n\t\tflags : SubscriptionFlags, optional\n\t\tsignal_fired : callable, optional\n\t\t\tInvoked when there is a signal matching the requested data.\n\t\t\tParameters: sender, object, iface, signal, params\n\n\t\tReturns\n\t\t-------\n\t\tSubscription\n\t\t\tAn object you can use as a context manager to unsubscribe from the signal later.\n\n\t\tSee Also\n\t\t--------\n\t\tSee https://developer.gnome.org/gio/2.44/GDBusConnection.html#g-dbus-connection-signal-subscribe\n\t\tfor more information.\n\t\t\"\"\"\n\t\tcallback = (lambda con, sender, object, iface, signal, params: signal_fired(sender, object, iface, signal, params.unpack())) if signal_fired is not None else lambda *args: None\n\t\treturn Subscription(self.con, sender, iface, signal, object, arg0, flags, callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef own_name(self, name, flags=0, name_aquired=None, name_lost=None):\n\t\twarnings.warn(\"own_name() is deprecated, use request_name() instead.\", DeprecationWarning)\n\n\t\tname_aquired_handler = (lambda con, name: name_aquired()) if name_aquired is not None else None\n\t\tname_lost_handler    = (lambda con, name: name_lost())    if name_lost    is not None else None\n\t\treturn NameOwner(self.con, name, flags, name_aquired_handler, name_lost_handler)", "response": "Asynchronously acquires a bus name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef watch_name(self, name, flags=0, name_appeared=None, name_vanished=None):\n\t\tname_appeared_handler = (lambda con, name, name_owner: name_appeared(name_owner)) if name_appeared is not None else None\n\t\tname_vanished_handler = (lambda con, name:             name_vanished())           if name_vanished is not None else None\n\t\treturn NameWatcher(self.con, name, flags, name_appeared_handler, name_vanished_handler)", "response": "Asynchronously watches a name on the bus specified by bus_type and calls NameWatcher. name_appeared and name_vanished callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsubscribe to the signal.", "response": "def connect(self, object, callback):\n\t\t\"\"\"Subscribe to the signal.\"\"\"\n\t\treturn subscription(self.map.setdefault(object, []), callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads Yahoo Finance data from Yahoo Finance.", "response": "def download(tickers, start=None, end=None, actions=False, threads=False,\n             group_by='column', auto_adjust=False, progress=True,\n             period=\"max\", interval=\"1d\", prepost=False, **kwargs):\n    \"\"\"Download yahoo tickers\n    :Parameters:\n        tickers : str, list\n            List of tickers to download\n        period : str\n            Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n            Either Use period parameter or use start and end\n        interval : str\n            Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n            Intraday data cannot extend last 60 days\n        start: str\n            Download start date string (YYYY-MM-DD) or _datetime.\n            Default is 1900-01-01\n        end: str\n            Download end date string (YYYY-MM-DD) or _datetime.\n            Default is now\n        group_by : str\n            Group by 'ticker' or 'column' (default)\n        prepost : bool\n            Include Pre and Post market data in results?\n            Default is False\n        auto_adjust: bool\n            Adjust all OHLC automatically? Default is False\n        actions: bool\n            Download dividend + stock splits data. Default is False\n        threads: bool / int\n            How many threads to use for mass downloading. Default is False\n    \"\"\"\n    global _PROGRESS_BAR, _DFS\n\n    # create ticker list\n    tickers = tickers if isinstance(tickers, list) else tickers.split()\n\n    if progress:\n        _PROGRESS_BAR = _ProgressBar(len(tickers), 'downloaded')\n\n    # reset _DFS\n    _DFS = {}\n\n    # set thread count if True\n    if threads is True:\n        threads = min([len(tickers), _multitasking.cpu_count()])\n\n    # download using threads\n    if isinstance(threads, int):\n        _multitasking.set_max_threads(threads)\n        for i, ticker in enumerate(tickers):\n            _download_one_threaded(ticker, period=period, interval=interval,\n                                   start=start, end=end, prepost=prepost,\n                                   actions=actions, auto_adjust=auto_adjust,\n                                   progress=(progress and i > 0))\n        while len(_DFS) < len(tickers):\n            _time.sleep(0.01)\n\n    # download synchronously\n    else:\n        for i, ticker in enumerate(tickers):\n            data = _download_one(ticker, period=period, interval=interval,\n                                 start=start, end=end, prepost=prepost,\n                                 actions=actions, auto_adjust=auto_adjust)\n            _DFS[ticker.upper()] = data\n            if progress:\n                _PROGRESS_BAR.animate()\n\n    if progress:\n        _PROGRESS_BAR.completed()\n\n    data = _pd.concat(_DFS.values(), axis=1, keys=_DFS.keys())\n    if group_by == 'column':\n        data.columns = data.columns.swaplevel(0, 1)\n        data.sort_index(level=0, axis=1, inplace=True)\n\n    if len(tickers) == 1:\n        data = _DFS[tickers[0]]\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_with_asked_args(callback, args):\n    # FIXME: support default args\n    # FIXME: support kwargs\n    # co_varnames contains both args and local variables, in order. \n    # We only pick the local variables\n    asked_arg_names = callback.__code__.co_varnames[:callback.__code__.co_argcount]\n    asked_arg_values = []\n    missing_args = []\n    for asked_arg_name in asked_arg_names:\n        if asked_arg_name in args:\n            asked_arg_values.append(args[asked_arg_name])\n        else:\n            missing_args.append(asked_arg_name)\n    if missing_args:\n        raise TypeError(\n            '{}() missing required positional argument: {}'.format(\n                callback.__code__.co_name,\n                ', '.join(missing_args)\n            )\n        )\n    return callback(*asked_arg_values)", "response": "Call callback with only the args it wants from args\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_serverproxy_handler(name, command, environment, timeout, absolute_url, port):\n    # FIXME: Set 'name' properly\n    class _Proxy(SuperviseAndProxyHandler):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.name = name\n            self.proxy_base = name\n            self.absolute_url = absolute_url\n            self.requested_port = port\n\n        @property\n        def process_args(self):\n            return {\n                'port': self.port,\n                'base_url': self.base_url,\n            }\n\n        def _render_template(self, value):\n            args = self.process_args\n            if type(value) is str:\n                return value.format(**args)\n            elif type(value) is list:\n                return [self._render_template(v) for v in value]\n            elif type(value) is dict:\n                return {\n                    self._render_template(k): self._render_template(v)\n                    for k, v in value.items()\n                }\n            else:\n                raise ValueError('Value of unrecognized type {}'.format(type(value)))\n\n        def get_cmd(self):\n            if callable(command):\n                return self._render_template(call_with_asked_args(command, self.process_args))\n            else:\n                return self._render_template(command)\n\n        def get_env(self):\n            if callable(environment):\n                return self._render_template(call_with_asked_args(environment, self.process_args))\n            else:\n                return self._render_template(environment)\n\n        def get_timeout(self):\n            return timeout\n\n    return _Proxy", "response": "Create a new handler class that can be used to serve a server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate tornado handlers for the server_processes.", "response": "def make_handlers(base_url, server_processes):\n    \"\"\"\n    Get tornado handlers for registered server_processes\n    \"\"\"\n    handlers = []\n    for sp in server_processes:\n        handler = _make_serverproxy_handler(\n            sp.name,\n            sp.command,\n            sp.environment,\n            sp.timeout,\n            sp.absolute_url,\n            sp.port,\n        )\n        handlers.append((\n            ujoin(base_url, sp.name, r'(.*)'), handler, dict(state={}),\n        ))\n        handlers.append((\n            ujoin(base_url, sp.name), AddSlashHandler\n        ))\n    return handlers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen a websocket connection to the proxied backend.", "response": "async def open(self, port, proxied_path=''):\n        \"\"\"\n        Called when a client opens a websocket connection.\n\n        We establish a websocket connection to the proxied backend &\n        set up a callback to relay messages through.\n        \"\"\"\n        if not proxied_path.startswith('/'):\n            proxied_path = '/' + proxied_path\n\n        client_uri = self.get_client_uri('ws', port, proxied_path)\n        headers = self.request.headers\n\n        def message_cb(message):\n            \"\"\"\n            Callback when the backend sends messages to us\n\n            We just pass it back to the frontend\n            \"\"\"\n            # Websockets support both string (utf-8) and binary data, so let's\n            # make sure we signal that appropriately when proxying\n            self._record_activity()\n            if message is None:\n                self.close()\n            else:\n                self.write_message(message, binary=isinstance(message, bytes))\n\n        def ping_cb(data):\n            \"\"\"\n            Callback when the backend sends pings to us.\n\n            We just pass it back to the frontend.\n            \"\"\"\n            self._record_activity()\n            self.ping(data)\n\n        async def start_websocket_connection():\n            self.log.info('Trying to establish websocket connection to {}'.format(client_uri))\n            self._record_activity()\n            request = httpclient.HTTPRequest(url=client_uri, headers=headers)\n            self.ws = await pingable_ws_connect(request=request,\n                on_message_callback=message_cb, on_ping_callback=ping_cb)\n            self._record_activity()\n            self.log.info('Websocket connection established to {}'.format(client_uri))\n\n        ioloop.IOLoop.current().add_callback(start_websocket_connection)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when we receive a message from our client.", "response": "def on_message(self, message):\n        \"\"\"\n        Called when we receive a message from our client.\n\n        We proxy it to the backend.\n        \"\"\"\n        self._record_activity()\n        if hasattr(self, 'ws'):\n            self.ws.write_message(message, binary=isinstance(message, bytes))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when the client sends a ping to the backend.", "response": "def on_ping(self, data):\n        \"\"\"\n        Called when the client pings our websocket connection.\n\n        We proxy it to the backend.\n        \"\"\"\n        self.log.debug('jupyter_server_proxy: on_ping: {}'.format(data))\n        self._record_activity()\n        if hasattr(self, 'ws'):\n            self.ws.protocol.write_ping(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the path to the context file that is used to access the proxy.", "response": "def _get_context_path(self, port):\n        \"\"\"\n        Some applications need to know where they are being proxied from.\n        This is either:\n        - {base_url}/proxy/{port}\n        - {base_url}/proxy/absolute/{port}\n        - {base_url}/{proxy_base}\n        \"\"\"\n        if self.proxy_base:\n            return url_path_join(self.base_url, self.proxy_base)\n        if self.absolute_url:\n            return url_path_join(self.base_url, 'proxy', 'absolute', str(port))\n        else:\n            return url_path_join(self.base_url, 'proxy', str(port))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def proxy(self, port, proxied_path):\n        '''\n        This serverextension handles:\n            {base_url}/proxy/{port([0-9]+)}/{proxied_path}\n            {base_url}/proxy/absolute/{port([0-9]+)}/{proxied_path}\n            {base_url}/{proxy_base}/{proxied_path}\n        '''\n\n        if 'Proxy-Connection' in self.request.headers:\n            del self.request.headers['Proxy-Connection']\n\n        self._record_activity()\n\n        if self.request.headers.get(\"Upgrade\", \"\").lower() == 'websocket':\n            # We wanna websocket!\n            # jupyterhub/jupyter-server-proxy@36b3214\n            self.log.info(\"we wanna websocket, but we don't define WebSocketProxyHandler\")\n            self.set_status(500)\n\n        body = self.request.body\n        if not body:\n            if self.request.method == 'POST':\n                body = b''\n            else:\n                body = None\n\n        client = httpclient.AsyncHTTPClient()\n\n        req = self._build_proxy_request(port, proxied_path, body)\n        response = await client.fetch(req, raise_error=False)\n        # record activity at start and end of requests\n        self._record_activity()\n\n        # For all non http errors...\n        if response.error and type(response.error) is not httpclient.HTTPError:\n            self.set_status(500)\n            self.write(str(response.error))\n        else:\n            self.set_status(response.code, response.reason)\n\n            # clear tornado default header\n            self._headers = httputil.HTTPHeaders()\n\n            for header, v in response.headers.get_all():\n                if header not in ('Content-Length', 'Transfer-Encoding',\n                                  'Content-Encoding', 'Connection'):\n                    # some header appear multiple times, eg 'Set-Cookie'\n                    self.add_header(header, v)\n\n            if response.body:\n                self.write(response.body)", "response": "This method handles the proxy request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_subprotocol(self, subprotocols):\n        '''Select a single Sec-WebSocket-Protocol during handshake.'''\n        if isinstance(subprotocols, list) and subprotocols:\n            self.log.info('Client sent subprotocols: {}'.format(subprotocols))\n            return subprotocols[0]\n        return super().select_subprotocol(subprotocols)", "response": "Select a single Sec - WebSocket - Protocol during handshake."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nallocate a port for use by the application.", "response": "def port(self):\n        \"\"\"\n        Allocate either the requested port or a random empty port for use by\n        application\n        \"\"\"\n        if 'port' not in self.state:\n            sock = socket.socket()\n            sock.bind(('', self.requested_port))\n            self.state['port'] = sock.getsockname()[1]\n            sock.close()\n        return self.state['port']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def ensure_process(self):\n        # We don't want multiple requests trying to start the process at the same time\n        # FIXME: Make sure this times out properly?\n        # Invariant here should be: when lock isn't being held, either 'proc' is in state &\n        # running, or not.\n        with (await self.state['proc_lock']):\n            if 'proc' not in self.state:\n                # FIXME: Prevent races here\n                # FIXME: Handle graceful exits of spawned processes here\n                cmd = self.get_cmd()\n                server_env = os.environ.copy()\n\n                # Set up extra environment variables for process\n                server_env.update(self.get_env())\n\n                timeout = self.get_timeout()\n\n                proc = SupervisedProcess(self.name, *cmd, env=server_env, ready_func=self._http_ready_func, ready_timeout=timeout, log=self.log)\n                self.state['proc'] = proc\n\n                try:\n                    await proc.start()\n\n                    is_ready = await proc.ready()\n\n                    if not is_ready:\n                        await proc.kill()\n                        raise web.HTTPError(500, 'could not start {} in time'.format(self.name))\n                except:\n                    # Make sure we remove proc from state in any error condition\n                    del self.state['proc']\n                    raise", "response": "Start the process if it is not running."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_shiny():\n    '''Manage a Shiny instance.'''\n\n    name = 'shiny'\n    def _get_shiny_cmd(port):\n        conf = dedent(\"\"\"\n            run_as {user};\n            server {{\n                listen {port};\n                location / {{\n                    site_dir {site_dir};\n                    log_dir {site_dir}/logs;\n                    directory_index on;\n                }}\n            }}\n        \"\"\").format(\n            user=getpass.getuser(),\n            port=str(port),\n            site_dir=os.getcwd()\n        )\n\n        f = tempfile.NamedTemporaryFile(mode='w', delete=False)\n        f.write(conf)\n        f.close()\n        return ['shiny-server', f.name]\n\n    return {\n        'command': _get_shiny_cmd,\n        'launcher_entry': {\n            'title': 'Shiny',\n            'icon_path': os.path.join(os.path.dirname(os.path.abspath(__file__)), 'icons', 'shiny.svg')\n        }\n    }", "response": "Manage a Shiny instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a request line into a tuple of the class name and the value of the request line.", "response": "def _parse_requestline(line):\n        \"\"\"\n        http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5\n\n        >>> Entry._parse_requestline('GET / HTTP/1.0') == ('GET', '/', '1.0')\n        True\n        >>> Entry._parse_requestline('post /testurl htTP/1.1') == ('POST', '/testurl', '1.1')\n        True\n        >>> Entry._parse_requestline('Im not a RequestLine')\n        Traceback (most recent call last):\n            ...\n        ValueError: Not a Request-Line\n        \"\"\"\n        m = re.match(r'({})\\s+(.*)\\s+HTTP/(1.[0|1])'.format('|'.join(Entry.METHODS)), line, re.I)\n        if m:\n            return m.group(1).upper(), m.group(2), m.group(3)\n        else:\n            raise ValueError('Not a Request-Line')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a prefix. @param node: An XML node on which the prefix will be used. @type node: L{sax.element.Element} @param ns: A namespace needing an unique prefix. @type ns: (prefix, uri) @return: The I{ns} with a new prefix.", "response": "def genprefix(cls, node, ns):\n        \"\"\"\n        Generate a prefix.\n        @param node: An XML node on which the prefix will be used.\n        @type node: L{sax.element.Element}\n        @param ns: A namespace needing an unique prefix.\n        @type ns: (prefix, uri)\n        @return: The I{ns} with a new prefix.\n        \"\"\"\n        for n in range(1, 1024):\n            p = 'ns%d' % n\n            u = node.resolvePrefix(p, default=None)\n            if u is None or u == ns[1]:\n                return (p, ns[1])\n        raise Exception('auto prefix, exhausted')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply the import rule to the specified schema.", "response": "def apply(self, root):\n        \"\"\"\n        Apply the import (rule) to the specified schema.\n        If the schema does not already contain an import for the\n        I{namespace} specified here, it is added.\n        @param root: A schema root.\n        @type root: L{Element}\n        \"\"\"\n        if not self.filter.match(root, self.ns):\n            return\n        if self.exists(root):\n            return\n        node = Element('import', ns=self.xsdns)\n        node.set('namespace', self.ns)\n        if self.location is not None:\n            node.set('schemaLocation', self.location)\n        log.debug('inserting: %s', node)\n        root.insert(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, root):\n        node = Element('import', ns=self.xsdns)\n        node.set('namespace', self.ns)\n        if self.location is not None:\n            node.set('schemaLocation', self.location)\n        log.debug('%s inserted', node)\n        root.insert(node)", "response": "Adds an <xs : import/> to the specified schema root."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking to see if the import element already exists in the specified schema root by matching the namespace of the current element.", "response": "def exists(self, root):\n        \"\"\"\n        Check to see if the <xs:import/> already exists\n        in the specified schema root by matching I{namesapce}.\n        @param root: A schema root.\n        @type root: L{Element}\n        \"\"\"\n        for node in root.children:\n            if node.name != 'import':\n                continue\n            ns = node.get('namespace')\n            if self.ns == ns:\n                return 1\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(self, result):\n        if result is None:\n            return True\n        reject = result in self.history\n        if reject:\n            log.debug('result %s, rejected by\\n%s', Repr(result), self)\n        return reject", "response": "Filter the specified result based on query criteria."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery result post processing. @param result: A query result. @type result: L{sxbase.SchemaObject}", "response": "def result(self, result):\n        \"\"\"\n        Query result post processing.\n        @param result: A query result.\n        @type result: L{sxbase.SchemaObject}\n        \"\"\"\n        if result is None:\n            log.debug('%s, not-found', self.ref)\n            return\n        if self.resolved:\n            result = result.resolve()\n        log.debug('%s, found as: %s', self.ref, Repr(result))\n        self.history.append(result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, *items):\n        for item in items:\n            self.unsorted.append(item)\n            key = item[0]\n            self.index[key] = item\n        return self", "response": "Add items to be sorted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the list based on dependancies.", "response": "def sort(self):\n        \"\"\"\n        Sort the list based on dependancies.\n        @return: The sorted items.\n        @rtype: list\n        \"\"\"\n        self.sorted = list()\n        self.pushed = set()\n        for item in self.unsorted:\n            popped = []\n            self.push(item)\n            while len(self.stack):\n                try:\n                    top = self.top()\n                    ref = next(top[1])\n                    refd = self.index.get(ref)\n                    if refd is None:\n                        log.debug('\"%s\" not found, skipped', Repr(ref))\n                        continue\n                    self.push(refd)\n                except StopIteration:\n                    popped.append(self.pop())\n                    continue\n            for p in popped:\n                self.sorted.append(p)\n        self.unsorted = self.sorted\n        return self.sorted"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes and item onto the sorting stack.", "response": "def push(self, item):\n        \"\"\"\n        Push and item onto the sorting stack.\n        @param item: An item to push.\n        @type item: I{item}\n        @return: The number of items pushed.\n        @rtype: int\n        \"\"\"\n        if item in self.pushed:\n            return\n        frame = (item, iter(item[1]))\n        self.stack.append(frame)\n        self.pushed.add(item)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmangling the name by hashing the name and appending the I { x }.", "response": "def mangle(self, name, x):\n        \"\"\"\n        Mangle the name by hashing the I{name} and appending I{x}.\n        @return: the mangled name.\n        \"\"\"\n        h = hashlib.md5(name.encode('utf8')).hexdigest()\n        return '%s-%s' % (h, x)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, url):\n        cache = self.cache()\n        id = self.mangle(url, 'document')\n        d = cache.get(id)\n        if d is None:\n            d = self.download(url)\n            cache.put(id, d)\n        self.plugins.document.parsed(url=url, document=d.root())\n        return d", "response": "Open an XML document at the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(self, url):\n        store = DocumentStore()\n        fp = store.open(url)\n        if fp is None:\n            fp = self.options.transport.open(Request(url))\n        content = fp.read()\n        fp.close()\n        ctx = self.plugins.document.loaded(url=url, document=content)\n        content = ctx.document\n        sax = Parser()\n        return sax.parse(string=content)", "response": "Download the docuemnt.\n        @param url: A document url.\n        @type url: str.\n        @return: A file pointer to the docuemnt.\n        @rtype: file-like"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, url):\n        cache = self.cache()\n        id = self.mangle(url, 'wsdl')\n        d = cache.get(id)\n        if d is None:\n            d = self.fn(url, self.options)\n            cache.put(id, d)\n        else:\n            d.options = self.options\n            for imp in d.imports:\n                imp.imported.options = self.options\n        return d", "response": "Open a WSDL at the specified I { url."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, url):\n        protocol, location = self.split(url)\n        if protocol == self.protocol:\n            return self.find(location)\n        else:\n            return None", "response": "Open a document at the specified url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting the url into protocol and location.", "response": "def split(self, url):\n        \"\"\"\n        Split the url into I{protocol} and I{location}\n        @param url: A URL.\n        @param url: str\n        @return: (I{url}, I{location})\n        @rtype: tuple\n        \"\"\"\n        parts = url.split('://', 1)\n        if len(parts) == 2:\n            return parts\n        else:\n            return (None, url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds all child objects from the given XML node.", "response": "def add_children(self, root):\n        \"\"\" Add child objects using the factory \"\"\"\n        for c in root.getChildren(ns=wsdlns):\n            child = Factory.create(c, self)\n            if child is None:\n                continue\n            self.children.append(child)\n            if isinstance(child, Import):\n                self.imports.append(child)\n                continue\n            if isinstance(child, Types):\n                self.types.append(child)\n                continue\n            if isinstance(child, Message):\n                self.messages[child.qname] = child\n                continue\n            if isinstance(child, PortType):\n                self.port_types[child.qname] = child\n                continue\n            if isinstance(child, Binding):\n                self.bindings[child.qname] = child\n                continue\n            if isinstance(child, Service):\n                self.services.append(child)\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_schema(self):\n        container = SchemaCollection(self)\n        for t in [t for t in self.types if t.local()]:\n            for root in t.contents():\n                schema = Schema(root, self.url, self.options, container)\n                container.add(schema)\n        if not len(container):  # empty\n            root = Element.buildPath(self.root, 'types/schema')\n            schema = Schema(root, self.url, self.options, container)\n            container.add(schema)\n        self.schema = container.load(self.options)\n        for s in [t.schema() for t in self.types if t.imported()]:\n            self.schema.merge(s)\n        return self.schema", "response": "Processes the types and creates the schema collection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_wrapped(self):\n        for b in self.bindings.values():\n            for op in b.operations.values():\n                for body in (op.soap.input.body, op.soap.output.body):\n                    body.wrapped = False\n                    if len(body.parts) != 1:\n                        continue\n                    for p in body.parts:\n                        if p.element is None:\n                            continue\n                        query = ElementQuery(p.element)\n                        pt = query.execute(self.schema)\n                        if pt is None:\n                            raise TypeNotFound(query.ref)\n                        resolved = pt.resolve()\n                        if resolved.builtin():\n                            continue\n                        body.wrapped = True", "response": "set wrapped flag on messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, definitions):\n        url = self.location\n        log.debug('importing (%s)', url)\n        if '://' not in url:\n            url = urljoin(definitions.url, url)\n        options = definitions.options\n        d = Definitions(url, options)\n        if d.root.match(Definitions.Tag, wsdlns):\n            self.import_definitions(definitions, d)\n            return\n        if d.root.match(Schema.Tag, Namespace.xsdns):\n            self.import_schema(definitions, d)\n            return\n        raise Exception('document at \"%s\" is unknown' % url)", "response": "Load the object by opening the URL and importing the schema and definitions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_schema(self, definitions, d):\n        if not len(definitions.types):\n            types = Types.create(definitions)\n            definitions.types.append(types)\n        else:\n            types = definitions.types[-1]\n        types.root.append(d.root)\n        log.debug('imported (XSD):\\n%s', d.root)", "response": "import schema as content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the qualified value of attribute named a.", "response": "def __getref(self, a, tns):\n        \"\"\" Get the qualified value of attribute named 'a'.\"\"\"\n        s = self.root.get(a)\n        if s is None:\n            return s\n        else:\n            return qualify(s, self.root, tns)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving named references to other WSDL objects.", "response": "def resolve(self, definitions):\n        \"\"\"\n        Resolve named references to other WSDL objects.\n        @param definitions: A definitions object.\n        @type definitions: L{Definitions}\n        \"\"\"\n        for op in self.operations.values():\n            if op.input is None:\n                op.input = Message(Element('no-input'), definitions)\n            else:\n                qref = qualify(op.input, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % op.input)\n                else:\n                    op.input = msg\n            if op.output is None:\n                op.output = Message(Element('no-output'), definitions)\n            else:\n                qref = qualify(op.output, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % op.output)\n                else:\n                    op.output = msg\n            for f in op.faults:\n                qref = qualify(f.message, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % f.message)\n                f.message = msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_operations(self, root, definitions):\n        dsop = Element('operation', ns=soapns)\n        for c in root.getChildren('operation'):\n            op = Facade('Operation')\n            op.name = c.get('name')\n            sop = c.getChild('operation', default=dsop)\n            soap = Facade('soap')\n            soap.action = '\"%s\"' % sop.get('soapAction', default='')\n            soap.style = sop.get('style', default=self.soap.style)\n            soap.input = Facade('Input')\n            soap.input.body = Facade('Body')\n            soap.input.headers = []\n            soap.output = Facade('Output')\n            soap.output.body = Facade('Body')\n            soap.output.headers = []\n            op.soap = soap\n            input = c.getChild('input')\n            if input is None:\n                input = Element('input', ns=wsdlns)\n            body = input.getChild('body')\n            self.body(definitions, soap.input.body, body)\n            for header in input.getChildren('header'):\n                self.header(definitions, soap.input, header)\n            output = c.getChild('output')\n            if output is None:\n                output = Element('output', ns=wsdlns)\n            body = output.getChild('body')\n            self.body(definitions, soap.output.body, body)\n            for header in output.getChildren('header'):\n                self.header(definitions, soap.output, header)\n            faults = []\n            for fault in c.getChildren('fault'):\n                sf = fault.getChild('fault')\n                if sf is None:\n                    continue\n                fn = fault.get('name')\n                f = Facade('Fault')\n                f.name = sf.get('name', default=fn)\n                f.use = sf.get('use', default='literal')\n                faults.append(f)\n            soap.faults = faults\n            self.operations[op.name] = op", "response": "Adds the operations to the XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef body(self, definitions, body, root):\n        if root is None:\n            body.use = 'literal'\n            body.namespace = definitions.tns\n            body.parts = ()\n            return\n        parts = root.get('parts')\n        if parts is None:\n            body.parts = ()\n        else:\n            body.parts = re.split('[\\s,]', parts)\n        body.use = root.get('use', default='literal')\n        ns = root.get('namespace')\n        if ns is None:\n            body.namespace = definitions.tns\n        else:\n            prefix = root.findPrefix(ns, 'b0')\n            body.namespace = (prefix, ns)", "response": "add the input and output body properties"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the input / output header properties", "response": "def header(self, definitions, parent, root):\n        \"\"\" add the input/output header properties \"\"\"\n        if root is None:\n            return\n        header = Facade('Header')\n        parent.headers.append(header)\n        header.use = root.get('use', default='literal')\n        ns = root.get('namespace')\n        if ns is None:\n            header.namespace = definitions.tns\n        else:\n            prefix = root.findPrefix(ns, 'h0')\n            header.namespace = (prefix, ns)\n        msg = root.get('message')\n        if msg is not None:\n            header.message = msg\n        part = root.get('part')\n        if part is not None:\n            header.part = part"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresolves named references to other WSDL objects.", "response": "def resolve(self, definitions):\n        \"\"\"\n        Resolve named references to other WSDL objects.  This includes\n        cross-linking information (from) the portType (to) the I{soap}\n        protocol information on the binding for each operation.\n        @param definitions: A definitions object.\n        @type definitions: L{Definitions}\n        \"\"\"\n        self.resolveport(definitions)\n        for op in self.operations.values():\n            self.resolvesoapbody(definitions, op)\n            self.resolveheaders(definitions, op)\n            self.resolvefaults(definitions, op)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolveport(self, definitions):\n        ref = qualify(self.type, self.root, definitions.tns)\n        port_type = definitions.port_types.get(ref)\n        if port_type is None:\n            raise Exception(\"portType '%s', not-found\" % self.type)\n        else:\n            self.type = port_type", "response": "Resolve the type reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding the invocation location for the service method.", "response": "def setlocation(self, url, names=None):\n        \"\"\"\n        Override the invocation location (url) for service method.\n        @param url: A url location.\n        @type url: A url.\n        @param names:  A list of method names.  None=ALL\n        @type names: [str,..]\n        \"\"\"\n        for p in self.ports:\n            for m in p.methods.values():\n                if names is None or m.name in names:\n                    m.location = url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding ( escape ) special XML characters.", "response": "def escape(self):\n        \"\"\"\n        Encode (escape) special XML characters.\n        @return: The text with XML special characters escaped.\n        @rtype: L{Text}\n        \"\"\"\n        if not self.escaped:\n            post = sax.encoder.encode(self)\n            escaped = post != self\n            return Text(post, lang=self.lang, escaped=escaped)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding ( unescape ) special XML characters.", "response": "def unescape(self):\n        \"\"\"\n        Decode (unescape) special XML characters.\n        @return: The text with escaped XML special characters decoded.\n        @rtype: L{Text}\n        \"\"\"\n        if self.escaped:\n            post = sax.encoder.decode(self)\n            return Text(post, lang=self.lang)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a python I{file - like object and a string - like object.", "response": "def parse(self, file=None, string=None):\n        \"\"\"\n        SAX parse XML text.\n        @param file: Parse a python I{file-like} object.\n        @type file: I{file-like} object.\n        @param string: Parse string XML.\n        @type string: str\n        \"\"\"\n        timer = metrics.Timer()\n        timer.start()\n        sax, handler = self.saxparser()\n        if file is not None:\n            sax.parse(file)\n            timer.stop()\n            metrics.log.debug('sax (%s) duration: %s', file, timer)\n            return handler.nodes[0]\n        if string is not None:\n            if isinstance(string, str):\n                string = string.encode()\n            parseString(string, handler)\n            timer.stop()\n            metrics.log.debug('%s\\nsax duration: %s', string, timer)\n            return handler.nodes[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self, node, type):\n        content = Content(node)\n        content.type = type\n        return Core.process(self, content)", "response": "Process an object graph representation of the xml node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append_attribute(self, name, value, content):\n        type = self.resolver.findattr(name)\n        if type is None:\n            log.warn('attribute (%s) type, not-found', name)\n        else:\n            value = self.translated(value, type)\n        Core.append_attribute(self, name, value, content)", "response": "Append an attribute name value into the current content."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend text nodes into the current content.", "response": "def append_text(self, content):\n        \"\"\"\n        Append text nodes into L{Content.data}\n        Here is where the I{true} type is used to translate the value\n        into the proper python type.\n        @param content: The current content being unmarshalled.\n        @type content: L{Content}\n        \"\"\"\n        Core.append_text(self, content)\n        known = self.resolver.top().resolved\n        content.text = self.translated(content.text, known)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntranslates the value using the schema type", "response": "def translated(self, value, type):\n        \"\"\" translate using the schema type \"\"\"\n        if value is not None:\n            resolved = type.resolve()\n            return resolved.translate(value)\n        else:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the content using the AttributeNames optional type information.", "response": "def process(self, content):\n        \"\"\"\n        Process (marshal) the tag with the specified value using the\n        optional type information.\n        @param content: The content to process.\n        @type content: L{Object}\n        \"\"\"\n        log.debug('processing:\\n%s', content)\n        self.reset()\n        if content.tag is None:\n            content.tag = content.value.__class__.__name__\n        document = Document()\n        if isinstance(content.value, Property):\n            root = self.node(content)  # root is never used?\n            self.append(document, content)\n        else:\n            self.append(document, content)\n        return document.root()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending the specified content to the specified parent node.", "response": "def append(self, parent, content):\n        \"\"\"\n        Append the specified L{content} to the I{parent}.\n        @param parent: The parent node to append to.\n        @type parent: L{Element}\n        @param content: The content to append.\n        @type content: L{Object}\n        \"\"\"\n        log.debug('appending parent:\\n%s\\ncontent:\\n%s', parent, content)\n        if self.start(content):\n            self.appender.append(parent, content)\n            self.end(parent, content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef qref(self):\n        qref = self.type\n        if qref is None and len(self) == 0:\n            ls = []\n            m = RestrictionMatcher()\n            finder = NodeFinder(m, 1)\n            finder.find(self, ls)\n            if len(ls):\n                return ls[0].ref\n        return qref", "response": "Get the I { type qualified reference to the referenced xsd type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an xsd : anyType reference", "response": "def anytype(self):\n        \"\"\" create an xsd:anyType reference \"\"\"\n        p, u = Namespace.xsdns\n        mp = self.root.findPrefix(u)\n        if mp is None:\n            mp = p\n            self.root.addPrefix(p, u)\n        return ':'.join((mp, 'anyType'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind(cls, ns, location=None):\n        if location is None:\n            location = ns\n        cls.locations[ns] = location", "response": "Bind a namespace to a schema location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, options):\n        if self.opened:\n            return\n        self.opened = True\n        log.debug('%s, importing ns=\"%s\", location=\"%s\"',\n                  self.id,\n                  self.ns[1],\n                  self.location\n                  )\n        result = self.locate()\n        if result is None:\n            if self.location is None:\n                log.debug('imported schema (%s) not-found', self.ns[1])\n            else:\n                result = self.download(options)\n        log.debug('imported:\\n%s', result)\n        return result", "response": "Open and import the refrenced schema."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(self, options):\n        if self.opened:\n            return\n        self.opened = True\n        log.debug('%s, including location=\"%s\"', self.id, self.location)\n        result = self.download(options)\n        log.debug('included:\\n%s', result)\n        return result", "response": "Open and include the refrenced schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(cls, root, schema):\n        fn = cls.tags.get(root.name)\n        if fn is not None:\n            return fn(schema, root)\n        else:\n            return None", "response": "Create an object based on the root tag name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(cls, root, schema, filter=('*',)):\n        children = []\n        for node in root.getChildren(ns=Namespace.xsdns):\n            if '*' in filter or node.name in filter:\n                child = cls.create(node, schema)\n                if child is None:\n                    continue\n                children.append(child)\n                c = cls.build(node, schema, child.childtags())\n                child.rawchildren = c\n        return children", "response": "Build an xsobject representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclone this object. @param parent: The parent for the clone. @type parent: L{element.Element} @return: A copy of this object assigned to the new parent. @rtype: L{Attribute}", "response": "def clone(self, parent=None):\n        \"\"\"\n        Clone this object.\n        @param parent: The parent for the clone.\n        @type parent: L{element.Element}\n        @return: A copy of this object assigned to the new parent.\n        @rtype: L{Attribute}\n        \"\"\"\n        a = Attribute(self.qname(), self.value)\n        a.parent = parent\n        return a"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setValue(self, value):\n        if isinstance(value, Text):\n            self.value = value\n        else:\n            self.value = Text(value)\n        return self", "response": "Set the attributes value\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the attributes namespace.", "response": "def namespace(self):\n        \"\"\"\n        Get the attributes namespace.  This may either be the namespace\n        defined by an optional prefix, or its parent's namespace.\n        @return: The attribute's namespace\n        @rtype: (I{prefix}, I{name})\n        \"\"\"\n        if self.prefix is None:\n            return Namespace.default\n        else:\n            return self.resolvePrefix(self.prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolvePrefix(self, prefix):\n        ns = Namespace.default\n        if self.parent is not None:\n            ns = self.parent.resolvePrefix(prefix)\n        return ns", "response": "Resolves the specified prefix to a known namespace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch by name and namespace.", "response": "def match(self, name=None, ns=None):\n        \"\"\"\n        Match by (optional) name and/or (optional) namespace.\n        @param name: The optional attribute tag name.\n        @type name: str\n        @param ns: An optional namespace.\n        @type ns: (I{prefix}, I{name})\n        @return: True if matched.\n        @rtype: boolean\n        \"\"\"\n        if name is None:\n            byname = True\n        else:\n            byname = self.name == name\n        if ns is None:\n            byns = True\n        else:\n            byns = self.namespace()[1] == ns[1]\n        return byname and byns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nappend the content to the parent node.", "response": "def append(self, parent, content):\n        \"\"\"\n        Select an appender and append the content to parent.\n        @param parent: A parent node.\n        @type parent: L{Element}\n        @param content: The content to append.\n        @type content: L{Content}\n        \"\"\"\n        appender = self.default\n        for a in self.appenders:\n            if a[0] == content.value:\n                appender = a[1]\n                break\n        appender.append(parent, content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cast(self, content):\n        aty = content.aty[1]\n        resolved = content.type.resolve()\n        array = Factory.object(resolved.name)\n        array.item = []\n        query = TypeQuery(aty)\n        ref = query.execute(self.schema)\n        if ref is None:\n            raise TypeNotFound(qref)\n        for x in content.value:\n            if isinstance(x, (list, tuple)):\n                array.item.append(x)\n                continue\n            if isinstance(x, Object):\n                md = x.__metadata__\n                md.sxtype = ref\n                array.item.append(x)\n                continue\n            if isinstance(x, dict):\n                x = Factory.object(ref.name, x)\n                md = x.__metadata__\n                md.sxtype = ref\n                array.item.append(x)\n                continue\n            x = Factory.property(ref.name, x)\n            md = x.__metadata__\n            md.sxtype = ref\n            array.item.append(x)\n        content.value = array\n        return self", "response": "Cast the items found in content I { value } to the type I { metadata }."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the root element of a service method.", "response": "def method(self, method):\n        \"\"\"\n        Get the document root.  For I{rpc/(literal|encoded)}, this is the\n        name of the method qualifed by the schema tns.\n        @param method: A service method.\n        @type method: I{service.Method}\n        @return: A root element.\n        @rtype: L{Element}\n        \"\"\"\n        ns = method.soap.input.body.namespace\n        if ns[0] is None:\n            ns = ('ns0', ns[1])\n        method = Element(method.name, ns=ns)\n        return method"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the appropriate XML decoder.", "response": "def unmarshaller(self, typed=True):\n        \"\"\"\n        Get the appropriate XML decoder.\n        @return: Either the (basic|typed) unmarshaller.\n        @rtype: L{UmxTyped}\n        \"\"\"\n        if typed:\n            return UmxEncoded(self.schema())\n        else:\n            return RPC.unmarshaller(self, typed)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset options. @param kwargs: keyword arguments. @see: L{Options}", "response": "def set_options(self, **kwargs):\n        \"\"\"\n        Set options.\n        @param kwargs: keyword arguments.\n        @see: L{Options}\n        \"\"\"\n        p = Unskin(self.options)\n        p.update(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a prefix to a namespace.", "response": "def add_prefix(self, prefix, uri):\n        \"\"\"\n        Add I{static} mapping of an XML namespace prefix to a namespace.\n        This is useful for cases when a wsdl and referenced schemas make heavy\n        use of namespaces and those namespaces are subject to changed.\n        @param prefix: An XML namespace prefix.\n        @type prefix: str\n        @param uri: An XML namespace URI.\n        @type uri: str\n        @raise Exception: when prefix is already mapped.\n        \"\"\"\n        root = self.wsdl.root\n        mapped = root.resolvePrefix(prefix, None)\n        if mapped is None:\n            root.addPrefix(prefix, uri)\n            return\n        if mapped[1] != uri:\n            raise Exception('\"%s\" already mapped as \"%s\"' % (prefix, mapped))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone(self):\n        class Uninitialized(Client):\n            def __init__(self):\n                pass\n        clone = Uninitialized()\n        clone.options = Options()\n        cp = Unskin(clone.options)\n        mp = Unskin(self.options)\n        cp.update(deepcopy(mp))\n        clone.wsdl = self.wsdl\n        clone.factory = self.factory\n        clone.service = ServiceSelector(clone, self.wsdl.services)\n        clone.sd = self.sd\n        clone.messages = dict(tx=None, rx=None)\n        return clone", "response": "Get a shallow clone of this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __find(self, name):\n        port = None\n        if not len(self.__ports):\n            raise Exception('No ports defined: %s' % self.__qn)\n        if isinstance(name, int):\n            qn = '%s[%d]' % (self.__qn, name)\n            try:\n                port = self.__ports[name]\n            except IndexError:\n                raise PortNotFound(qn)\n        else:\n            qn = '.'.join((self.__qn, name))\n            for p in self.__ports:\n                if name == p.name:\n                    port = p\n                    break\n        if port is None:\n            raise PortNotFound(qn)\n        qn = '.'.join((self.__qn, port.name))\n        return MethodSelector(self.__client, port.methods, qn)", "response": "Find a I { port } by name or index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __dp(self):\n        dp = self.__client.options.port\n        if dp is None:\n            return None\n        else:\n            return self.__find(dp)", "response": "Get the I { default } port if defined in the I { options.\n        option."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef headers(self):\n        action = self.method.soap.action\n        result = {\n            'Content-Type': 'text/xml; charset=utf-8',\n            'SOAPAction': action\n        }\n        result.update(self.options.headers)\n        log.debug('headers = %s', result)\n        return result", "response": "Get http headers or the http https request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef invoke(self, args, kwargs):\n        simulation = kwargs[self.injkey]\n        msg = simulation.get('msg')\n        reply = simulation.get('reply')\n        fault = simulation.get('fault')\n        if msg is None:\n            if reply is not None:\n                return self.__reply(reply, args, kwargs)\n            if fault is not None:\n                return self.__fault(fault)\n            raise Exception('(reply|fault) expected when msg=None')\n        sax = Parser()\n        msg = sax.parse(string=msg)\n        return self.send(msg)", "response": "Send the required soap message to invoke the specified method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate the fault reply", "response": "def __fault(self, reply):\n        \"\"\" simulate the (fault) reply \"\"\"\n        binding = self.method.binding.output\n        if self.options.faults:\n            r, p = binding.get_fault(reply)\n            self.last_received(r)\n            return (500, p)\n        else:\n            return (500, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, content):\n        self.start(content)\n        self.append_attributes(content)\n        self.append_children(content)\n        self.append_text(content)\n        self.end(content)\n        return self.postprocess(content)", "response": "Process the specified node and convert the XML document into a I { suds } L { object.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_attributes(self, content):\n        attributes = AttrList(content.node.attributes)\n        for attr in attributes.real():\n            name = attr.name\n            value = attr.value\n            self.append_attribute(name, value, content)", "response": "Append attributes nodes into the current content."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append_attribute(self, name, value, content):\n        key = name\n        key = '_%s' % reserved.get(key, key)\n        setattr(content.data, key, value)", "response": "Append an attribute name value into content. data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append_text(self, content):\n        if content.node.hasText():\n            content.text = content.node.getText()", "response": "Append text nodes into the content."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds and return the proper object.", "response": "def start(self, content):\n        \"\"\"\n        Processing on I{node} has started.  Build and return\n        the proper object.\n        @param content: The current content being unmarshalled.\n        @type content: L{Content}\n        @return: A subclass of Object.\n        @rtype: L{Object}\n        \"\"\"\n        content.data = Factory.object(content.node.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a unicode safe string representation of an object", "response": "def tostr(object, encoding=None):\n    \"\"\" get a unicode safe string representation of an object \"\"\"\n    if isinstance(object, basestring):\n        if encoding is None:\n            return object\n        else:\n            return object.encode(encoding)\n    if isinstance(object, tuple):\n        s = ['(']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(')')\n        return ''.join(s)\n    if isinstance(object, list):\n        s = ['[']\n        for item in object:\n            if isinstance(item, basestring):\n                s.append(item)\n            else:\n                s.append(tostr(item))\n            s.append(', ')\n        s.append(']')\n        return ''.join(s)\n    if isinstance(object, dict):\n        s = ['{']\n        for item in object.items():\n            if isinstance(item[0], basestring):\n                s.append(item[0])\n            else:\n                s.append(tostr(item[0]))\n            s.append(' = ')\n            if isinstance(item[1], basestring):\n                s.append(item[1])\n            else:\n                s.append(tostr(item[1]))\n            s.append(', ')\n        s.append('}')\n        return ''.join(s)\n    try:\n        return unicode(object)\n    except:\n        return str(object)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a qualified reference that is I { qualified by namespace.", "response": "def qualify(ref, resolvers, defns=Namespace.default):\n    \"\"\"\n    Get a reference that is I{qualified} by namespace.\n    @param ref: A referenced schema type name.\n    @type ref: str\n    @param resolvers: A list of objects to be used to resolve types.\n    @type resolvers: [L{sax.element.Element},]\n    @param defns: An optional target namespace used to qualify references\n        when no prefix is specified.\n    @type defns: A default namespace I{tuple: (prefix,uri)} used when ref not\n        prefixed.\n    @return: A qualified reference.\n    @rtype: (name, namespace-uri)\n    \"\"\"\n    ns = None\n    p, n = splitPrefix(ref)\n    if p is not None:\n        if not isinstance(resolvers, (list, tuple)):\n            resolvers = (resolvers,)\n        for r in resolvers:\n            resolved = r.resolvePrefix(p)\n            if resolved[1] is not None:\n                ns = resolved\n                break\n        if ns is None:\n            raise Exception('prefix (%s) not resolved' % p)\n    else:\n        ns = defns\n    return (n, ns[1])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the object is a I { qualified reference.", "response": "def isqref(object):\n    \"\"\"\n    Get whether the object is a I{qualified reference}.\n    @param object: An object to be tested.\n    @type object: I{any}\n    @rtype: boolean\n    @see: L{qualify}\n    \"\"\"\n    return (\n        isinstance(object, tuple) and\n        len(object) == 2 and\n        isinstance(object[0], basestring) and\n        isinstance(object[1], basestring))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a schema node to the collection.", "response": "def add(self, schema):\n        \"\"\"\n        Add a schema node to the collection.  Schema(s) within the same target\n        namespace are consolidated.\n        @param schema: A schema object.\n        @type schema: (L{Schema})\n        \"\"\"\n        key = schema.tns[1]\n        existing = self.namespaces.get(key)\n        if existing is None:\n            self.children.append(schema)\n            self.namespaces[key] = schema\n        else:\n            existing.root.children += schema.root.children\n            existing.root.nsprefixes.update(schema.root.nsprefixes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the schema objects for the root nodes.", "response": "def load(self, options):\n        \"\"\"\n        Load the schema objects for the root nodes.\n            - de-references schemas\n            - merge schemas\n        @param options: An options dictionary.\n        @type options: L{options.Options}\n        @return: The merged schema.\n        @rtype: L{Schema}\n        \"\"\"\n        if options.autoblend:\n            self.autoblend()\n        for child in self.children:\n            child.build()\n        for child in self.children:\n            child.open_imports(options)\n        for child in self.children:\n            child.dereference()\n        log.debug('loaded:\\n%s', self)\n        merged = self.merge()\n        log.debug('MERGED:\\n%s', merged)\n        return merged"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autoblend(self):\n        namespaces = self.namespaces.keys()\n        for s in self.children:\n            for ns in namespaces:\n                tns = s.root.get('targetNamespace')\n                if tns == ns:\n                    continue\n                for imp in s.root.getChildren('import'):\n                    if imp.get('namespace') == ns:\n                        continue\n                imp = Element('import', ns=Namespace.xsdns)\n                imp.set('namespace', ns)\n                s.root.append(imp)\n        return self", "response": "Ensure that all schemas within the collection have a blending effect."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mktns(self):\n        tns = [None, self.root.get('targetNamespace')]\n        if tns[1] is not None:\n            tns[0] = self.root.findPrefix(tns[1])\n        return tuple(tns)", "response": "Make the schema s target namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the schema using the root node", "response": "def build(self):\n        \"\"\"\n        Build the schema (object graph) using the root node\n        using the factory.\n            - Build the graph.\n            - Collate the children.\n        \"\"\"\n        self.children = BasicFactory.build(self.root, self)\n        collated = BasicFactory.collate(self.children)\n        self.children = collated[0]\n        self.attributes = collated[2]\n        self.imports = collated[1]\n        self.elements = collated[3]\n        self.types = collated[4]\n        self.groups = collated[5]\n        self.agrps = collated[6]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge(self, schema):\n        for item in schema.attributes.items():\n            if item[0] in self.attributes:\n                continue\n            self.all.append(item[1])\n            self.attributes[item[0]] = item[1]\n        for item in schema.elements.items():\n            if item[0] in self.elements:\n                continue\n            self.all.append(item[1])\n            self.elements[item[0]] = item[1]\n        for item in schema.types.items():\n            if item[0] in self.types:\n                continue\n            self.all.append(item[1])\n            self.types[item[0]] = item[1]\n        for item in schema.groups.items():\n            if item[0] in self.groups:\n                continue\n            self.all.append(item[1])\n            self.groups[item[0]] = item[1]\n        for item in schema.agrps.items():\n            if item[0] in self.agrps:\n                continue\n            self.all.append(item[1])\n            self.agrps[item[0]] = item[1]\n        schema.merged = True\n        return self", "response": "Merge the contents of the schema into this schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dereference(self):\n        all = []\n        indexes = {}\n        for child in self.children:\n            child.content(all)\n        deplist = DepList()\n        for x in all:\n            x.qualify()\n            midx, deps = x.dependencies()\n            item = (x, tuple(deps))\n            deplist.add(item)\n            indexes[x] = midx\n        for x, deps in deplist.sort():\n            midx = indexes.get(x)\n            if midx is None:\n                continue\n            d = deps[midx]\n            log.debug('(%s) merging %s <== %s', self.tns[1], Repr(x), Repr(d))\n            x.merge(d)", "response": "Instruct all children to perform dereferencing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the specified reference is a builtin False otherwise.", "response": "def custom(self, ref, context=None):\n        \"\"\"\n        Get whether the specified reference is B{not} an (xs) builtin.\n        @param ref: A str or qref.\n        @type ref: (str|qref)\n        @return: True if B{not} a builtin, else False.\n        @rtype: bool\n        \"\"\"\n        if ref is None:\n            return True\n        else:\n            return not self.builtin(ref, context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the specified reference is a builtin.", "response": "def builtin(self, ref, context=None):\n        \"\"\"\n        Get whether the specified reference is an (xs) builtin.\n        @param ref: A str or qref.\n        @type ref: (str|qref)\n        @return: True if builtin, else False.\n        @rtype: bool\n        \"\"\"\n        w3 = 'http://www.w3.org'\n        try:\n            if isqref(ref):\n                ns = ref[1]\n                return ref[0] in Factory.tags and ns.startswith(w3)\n            if context is None:\n                context = self.root\n            prefix = splitPrefix(ref)[0]\n            prefixes = context.findPrefixes(w3, 'startswith')\n            return prefix in prefixes and ref[0] in Factory.tags\n        except:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the specified soap envelope body and replace the contents of the referenced node with the contents of the referenced node.", "response": "def process(self, body):\n        \"\"\"\n        Process the specified soap envelope body and replace I{multiref} node\n        references with the contents of the referenced node.\n        @param body: A soap envelope body node.\n        @type body: L{Element}\n        @return: The processed I{body}\n        @rtype: L{Element}\n        \"\"\"\n        self.nodes = []\n        self.catalog = {}\n        self.build_catalog(body)\n        self.update(body)\n        body.children = self.nodes\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the specified node by replacing the references with the contents of the referenced nodes and remove the I { href } attribute.", "response": "def update(self, node):\n        \"\"\"\n        Update the specified I{node} by replacing the I{multiref} references\n        with the contents of the referenced nodes and remove the I{href}\n        attribute.\n        @param node: A node to update.\n        @type node: L{Element}\n        @return: The updated node\n        @rtype: L{Element}\n        \"\"\"\n        self.replace_references(node)\n        for c in node.children:\n            self.update(c)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_references(self, node):\n        href = node.getAttribute('href')\n        if href is None:\n            return\n        id = href.getValue()\n        ref = self.catalog.get(id)\n        if ref is None:\n            log.error('soap multiref: %s, not-resolved', id)\n            return\n        node.append(ref.children)\n        node.setText(ref.getText())\n        for a in ref.attributes:\n            if a.name != 'id':\n                node.append(a)\n        node.remove(href)", "response": "Replaces the contents of the I { ref } attribute with the contents of the I { href } attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the I { catalog } of multiref nodes by id and the list of non - multiref nodes by id.", "response": "def build_catalog(self, body):\n        \"\"\"\n        Create the I{catalog} of multiref nodes by id and the list of\n        non-multiref nodes.\n        @param body: A soap envelope body node.\n        @type body: L{Element}\n        \"\"\"\n        for child in body.children:\n            if self.soaproot(child):\n                self.nodes.append(child)\n            id = child.get('id')\n            if id is None:\n                continue\n            key = '#%s' % id\n            self.catalog[key] = child"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates if the specified I { node is a soap encoded root.", "response": "def soaproot(self, node):\n        \"\"\"\n        Get whether the specified I{node} is a soap encoded root.\n        This is determined by examining @soapenc:root='1'.\n        The node is considered to be a root when the attribute\n        is not specified.\n        @param node: A node to evaluate.\n        @type node: L{Element}\n        @return: True if a soap encoded root.\n        @rtype: bool\n        \"\"\"\n        root = node.getAttribute('root', ns=soapenc)\n        if root is None:\n            return True\n        else:\n            return root.value == '1'"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the schema type at the specified path.", "response": "def find(self, path, resolved=True):\n        \"\"\"\n        Get the definition object for the schema type located at the specified\n        path.\n        The path may contain (.) dot notation to specify nested types.\n        Actually, the path separator is usually a (.) but can be redefined\n        during contruction.\n        @param path: A (.) separated path to a schema type.\n        @type path: basestring\n        @param resolved: A flag indicating that the fully resolved type\n            should be returned.\n        @type resolved: boolean\n        @return: The found schema I{type}\n        @rtype: L{xsd.sxbase.SchemaObject}\n        \"\"\"\n        result = None\n        parts = self.split(path)\n        try:\n            result = self.root(parts)\n            if len(parts) > 1:\n                result = result.resolve(nobuiltin=True)\n                result = self.branch(result, parts)\n                result = self.leaf(result, parts)\n            if resolved:\n                result = result.resolve(nobuiltin=True)\n        except PathResolver.BadPath:\n            log.error('path: \"%s\", not-found' % path)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef leaf(self, parent, parts):\n        name = splitPrefix(parts[-1])[1]\n        if name.startswith('@'):\n            result, path = parent.get_attribute(name[1:])\n        else:\n            result, ancestry = parent.get_child(name)\n        if result is None:\n            raise PathResolver.BadPath(name)\n        return result", "response": "Find the leaf.\n        @param parts: A list of path parts.\n        @type parts: [str,..]\n        @param parent: The leaf's parent.\n        @type parent: L{xsd.sxbase.SchemaObject}\n        @return: The leaf.\n        @rtype: L{xsd.sxbase.SchemaObject}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nqualify the name as either plain or fully qualified name.", "response": "def qualify(self, name):\n        \"\"\"\n        Qualify the name as either:\n          - plain name\n          - ns prefixed name (eg: ns0:Person)\n          - fully ns qualified name (eg: {http://myns-uri}Person)\n        @param name: The name of an object in the schema.\n        @type name: str\n        @return: A qualifed name.\n        @rtype: qname\n        \"\"\"\n        m = self.altp.match(name)\n        if m is None:\n            return qualify(name, self.wsdl.root, self.wsdl.tns)\n        else:\n            return (m.group(4), m.group(2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the string s on (. while preserving any (. inside the syntax for full ns qualification.", "response": "def split(self, s):\n        \"\"\"\n        Split the string on (.) while preserving any (.) inside the\n        '{}' alternalte syntax for full ns qualification.\n        @param s: A plain or qualifed name.\n        @type s: str\n        @return: A list of the name's parts.\n        @rtype: [str,..]\n        \"\"\"\n        parts = []\n        b = 0\n        while 1:\n            m = self.splitp.match(s, b)\n            if m is None:\n                break\n            b, e = m.span()\n            parts.append(s[b:e])\n            b = e + 1\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush an I{object onto the stack.", "response": "def push(self, x):\n        \"\"\"\n        Push an I{object} onto the stack.\n        @param x: An object to push.\n        @type x: L{Frame}\n        @return: The pushed frame.\n        @rtype: L{Frame}\n        \"\"\"\n        if isinstance(x, Frame):\n            frame = x\n        else:\n            frame = Frame(x)\n        self.stack.append(frame)\n        log.debug('push: (%s)\\n%s', Repr(frame), Repr(self.stack))\n        return frame"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npops the frame at the top of the stack.", "response": "def pop(self):\n        \"\"\"\n        Pop the frame at the top of the stack.\n        @return: The popped frame, else None.\n        @rtype: L{Frame}\n        \"\"\"\n        if len(self.stack):\n            popped = self.stack.pop()\n            log.debug('pop: (%s)\\n%s', Repr(popped), Repr(self.stack))\n            return popped\n        else:\n            log.debug('stack empty, not-popped')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding an attribute type definition.", "response": "def findattr(self, name, resolved=True):\n        \"\"\"\n        Find an attribute type definition.\n        @param name: An attribute name.\n        @type name: basestring\n        @param resolved: A flag indicating that the fully resolved type should\n            be returned.\n        @type resolved: boolean\n        @return: The found schema I{type}\n        @rtype: L{xsd.sxbase.SchemaObject}\n        \"\"\"\n        name = '@%s' % name\n        parent = self.top().resolved\n        if parent is None:\n            result, ancestry = self.query(name, node)\n        else:\n            result, ancestry = self.getchild(name, parent)\n        if result is None:\n            return result\n        if resolved:\n            result = result.resolve()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving type referenced by type", "response": "def known(self, node):\n        \"\"\" resolve type referenced by @xsi:type \"\"\"\n        ref = node.get('type', Namespace.xsins)\n        if ref is None:\n            return None\n        qref = qualify(ref, node, node.namespace())\n        query = BlindQuery(qref)\n        return query.execute(self.schema)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the type specified in the object s metadata", "response": "def known(self, object):\n        \"\"\" get the type specified in the object's metadata \"\"\"\n        try:\n            md = object.__metadata__\n            known = md.sxtype\n            return known\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if string I { s contains special characters.", "response": "def needsEncoding(self, s):\n        \"\"\"\n        Get whether string I{s} contains special characters.\n        @param s: A string to check.\n        @type s: str\n        @return: True if needs encoding.\n        @rtype: boolean\n        \"\"\"\n        if isinstance(s, str):\n            for c in self.special:\n                if c in s:\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode special characters found in string I { s.", "response": "def encode(self, s):\n        \"\"\"\n        Encode special characters found in string I{s}.\n        @param s: A string to encode.\n        @type s: str\n        @return: The encoded string.\n        @rtype: str\n        \"\"\"\n        if isinstance(s, str) and self.needsEncoding(s):\n            for x in self.encodings:\n                s = re.sub(x[0], x[1], s)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding special characters encodings found in string I { s.", "response": "def decode(self, s):\n        \"\"\"\n        Decode special characters encodings found in string I{s}.\n        @param s: A string to decode.\n        @type s: str\n        @return: The decoded string.\n        @rtype: str\n        \"\"\"\n        if isinstance(s, str) and '&' in s:\n            for x in self.decodings:\n                s = s.replace(x[0], x[1])\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npushes our prefixes to the wsdl so that the wsdl s root contains the correct namespace.", "response": "def pushprefixes(self):\n        \"\"\"\n        Add our prefixes to the wsdl so that when users invoke methods\n        and reference the prefixes, the will resolve properly.\n        \"\"\"\n        for ns in self.prefixes:\n            self.wsdl.root.addPrefix(ns[0], ns[1])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addports(self):\n        timer = metrics.Timer()\n        timer.start()\n        for port in self.service.ports:\n            p = self.findport(port)\n            for op in port.binding.operations.values():\n                m = p[0].method(op.name)\n                binding = m.binding.input\n                method = (m.name, binding.param_defs(m))\n                p[1].append(method)\n                metrics.log.debug(\"method '%s' created: %s\", m.name, timer)\n            p[1].sort()\n        timer.stop()", "response": "Add all ports to the internal list of services."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef findport(self, port):\n        for p in self.ports:\n            if p[0] == p:\n                return p\n        p = (port, [])\n        self.ports.append(p)\n        return p", "response": "Find and return a port tuple for the specified port. Created and added when not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef paramtypes(self):\n        for m in [p[1] for p in self.ports]:\n            for p in [p[1] for p in m]:\n                for pd in p:\n                    if pd[1] in self.params:\n                        continue\n                    item = (pd[1], pd[1].resolve())\n                    self.params.append(item)", "response": "get all parameter types"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all public types", "response": "def publictypes(self):\n        \"\"\" get all public types \"\"\"\n        for t in self.wsdl.schema.types.values():\n            if t in self.params:\n                continue\n            if t in self.types:\n                continue\n            item = (t, t)\n            self.types.append(item)\n        self.types.sort(key=lambda x: x[0].name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nextprefix(self):\n        used = [ns[0] for ns in self.prefixes]\n        used += [ns[0] for ns in self.wsdl.root.nsprefixes.items()]\n        for n in range(0, 1024):\n            p = 'ns%d' % n\n            if p not in used:\n                return p\n        raise Exception('prefixes exhausted')", "response": "Get the next available prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the prefix for the specified namespace.", "response": "def getprefix(self, u):\n        \"\"\"\n        Get the prefix for the specified namespace (uri)\n        @param u: A namespace uri.\n        @type u: str\n        @return: The namspace.\n        @rtype: (prefix, uri).\n        \"\"\"\n        for ns in Namespace.all:\n            if u == ns[1]:\n                return ns[0]\n        for ns in self.prefixes:\n            if u == ns[1]:\n                return ns[0]\n        raise Exception('ns (%s) not mapped' % u)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a connection. @param u2request: A urllib2 request. @type u2request: urllib2.Requet. @return: The opened file-like urllib2 object. @rtype: fp", "response": "def u2open(self, u2request):\n        \"\"\"\n        Open a connection.\n        @param u2request: A urllib2 request.\n        @type u2request: urllib2.Requet.\n        @return: The opened file-like urllib2 object.\n        @rtype: fp\n        \"\"\"\n        tm = self.options.timeout\n        url = self.u2opener()\n        if self.u2ver() < 2.6:\n            socket.setdefaulttimeout(tm)\n            return url.open(u2request)\n        else:\n            return url.open(u2request, timeout=tm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a urllib opener.", "response": "def u2opener(self):\n        \"\"\"\n        Create a urllib opener.\n        @return: An opener.\n        @rtype: I{OpenerDirector}\n        \"\"\"\n        if self.urlopener is None:\n            return u2.build_opener(*self.u2handlers())\n        else:\n            return self.urlopener"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a collection of urllib handlers.", "response": "def u2handlers(self):\n        \"\"\"\n        Get a collection of urllib handlers.\n        @return: A list of handlers to be installed in the opener.\n        @rtype: [Handler,...]\n        \"\"\"\n        handlers = []\n        handlers.append(u2.ProxyHandler(self.proxy))\n        return handlers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the major or minor version of the urllib2 lib.", "response": "def u2ver(self):\n        \"\"\"\n        Get the major/minor version of the urllib2 lib.\n        @return: The urllib2 version.\n        @rtype: float\n        \"\"\"\n        try:\n            part = u2.__version__.split('.', 1)\n            n = float('.'.join(part))\n            return n\n        except Exception as e:\n            log.exception(e)\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef buildPath(self, parent, path):\n        for tag in path.split('/'):\n            child = parent.getChild(tag)\n            if child is None:\n                child = Element(tag, parent)\n            parent = child\n        return child", "response": "Build the specifed pat as a/b / c where missing intermediate nodes are are\n        built automatically."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setPrefix(self, p, u=None):\n        self.prefix = p\n        if p is not None and u is not None:\n            self.addPrefix(p, u)\n        return self", "response": "Sets the prefix for the element namespace prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the fully qualified name of this element.", "response": "def qname(self):\n        \"\"\"\n        Get the B{fully} qualified name of this element\n        @return: The fully qualified name.\n        @rtype: basestring\n        \"\"\"\n        if self.prefix is None:\n            return self.name\n        else:\n            return '%s:%s' % (self.prefix, self.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove this element from its parent s child list and I { parent } = I { None }.", "response": "def detach(self):\n        \"\"\"\n        Detach from parent.\n        @return: This element removed from its parent's\n            child list and I{parent}=I{None}\n        @rtype: L{Element}\n        \"\"\"\n        if self.parent is not None:\n            if self in self.parent.children:\n                self.parent.children.remove(self)\n            self.parent = None\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting an attribute s value.", "response": "def set(self, name, value):\n        \"\"\"\n        Set an attribute's value.\n        @param name: The name of the attribute.\n        @type name: basestring\n        @param value: The attribute value.\n        @type value: basestring\n        @see: __setitem__()\n        \"\"\"\n        attr = self.getAttribute(name)\n        if attr is None:\n            attr = Attribute(name, value)\n            self.append(attr)\n        else:\n            attr.setValue(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the element s text value.", "response": "def setText(self, value):\n        \"\"\"\n        Set the element's L{Text} content.\n        @param value: The element's text value.\n        @type value: basestring\n        @return: self\n        @rtype: I{Element}\n        \"\"\"\n        if isinstance(value, Text):\n            self.text = value\n        else:\n            self.text = Text(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim(self):\n        if self.hasText():\n            self.text = self.text.trim()\n        return self", "response": "Trim leading and trailing whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the specified child element or attribute from the current object.", "response": "def remove(self, child):\n        \"\"\"\n        Remove the specified child element or attribute.\n        @param child: A child to remove.\n        @type child: L{Element}|L{Attribute}\n        @return: The detached I{child} when I{child} is an element, else None.\n        @rtype: L{Element}|None\n        \"\"\"\n        if isinstance(child, Element):\n            return child.detach()\n        if isinstance(child, Attribute):\n            self.attributes.remove(child)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing child with the specified content.", "response": "def replaceChild(self, child, content):\n        \"\"\"\n        Replace I{child} with the specified I{content}.\n        @param child: A child element.\n        @type child: L{Element}\n        @param content: An element or collection of elements.\n        @type content: L{Element} or [L{Element},]\n        \"\"\"\n        if child not in self.children:\n            raise Exception('child not-found')\n        index = self.children.index(child)\n        self.remove(child)\n        if not isinstance(content, (list, tuple)):\n            content = (content,)\n        for node in content:\n            self.children.insert(index, node.detach())\n            node.parent = self\n            index += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an attribute by name and namespace.", "response": "def getAttribute(self, name, ns=None, default=None):\n        \"\"\"\n        Get an attribute by name and (optional) namespace\n        @param name: The name of a contained attribute (may contain prefix).\n        @type name: basestring\n        @param ns: An optional namespace\n        @type ns: (I{prefix}, I{name})\n        @param default: Returned when attribute not-found.\n        @type default: L{Attribute}\n        @return: The requested attribute object.\n        @rtype: L{Attribute}\n        \"\"\"\n        if ns is None:\n            prefix, name = splitPrefix(name)\n            if prefix is None:\n                ns = None\n            else:\n                ns = self.resolvePrefix(prefix)\n        for a in self.attributes:\n            if a.match(name, ns):\n                return a\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetach and return this element s children.", "response": "def detachChildren(self):\n        \"\"\"\n        Detach and return this element's children.\n        @return: The element's children (detached).\n        @rtype: [L{Element},...]\n        \"\"\"\n        detached = self.children\n        self.children = []\n        for child in detached:\n            child.parent = None\n        return detached"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolvePrefix(self, prefix, default=Namespace.default):\n        n = self\n        while n is not None:\n            if prefix in n.nsprefixes:\n                return (prefix, n.nsprefixes[prefix])\n            if prefix in self.specialprefixes:\n                return (prefix, self.specialprefixes[prefix])\n            n = n.parent\n        return default", "response": "Resolves the specified prefix to a namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updatePrefix(self, p, u):\n        if p in self.nsprefixes:\n            self.nsprefixes[p] = u\n        for c in self.children:\n            c.updatePrefix(p, u)\n        return self", "response": "Update a prefix mapping for the branch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clearPrefix(self, prefix):\n        if prefix in self.nsprefixes:\n            del self.nsprefixes[prefix]\n        return self", "response": "Removes the specified prefix from the prefix mappings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the first prefix that has been mapped to a namespace URI.", "response": "def findPrefix(self, uri, default=None):\n        \"\"\"\n        Find the first prefix that has been mapped to a namespace URI.\n        The local mapping is searched, then it walks up the tree until\n        it reaches the top or finds a match.\n        @param uri: A namespace URI.\n        @type uri: basestring\n        @param default: A default prefix when not found.\n        @type default: basestring\n        @return: A mapped prefix.\n        @rtype: basestring\n        \"\"\"\n        for item in self.nsprefixes.items():\n            if item[1] == uri:\n                prefix = item[0]\n                return prefix\n        for item in self.specialprefixes.items():\n            if item[1] == uri:\n                prefix = item[0]\n                return prefix\n        if self.parent is not None:\n            return self.parent.findPrefix(uri, default)\n        else:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findPrefixes(self, uri, match='eq'):\n        result = []\n        for item in self.nsprefixes.items():\n            if self.matcher[match](item[1], uri):\n                prefix = item[0]\n                result.append(prefix)\n        for item in self.specialprefixes.items():\n            if self.matcher[match](item[1], uri):\n                prefix = item[0]\n                result.append(prefix)\n        if self.parent is not None:\n            result += self.parent.findPrefixes(uri, match)\n        return result", "response": "Find all prefixes that have been mapped to a namespace URI."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npromote prefix declarations up the tree as far as possible.", "response": "def promotePrefixes(self):\n        \"\"\"\n        Push prefix declarations up the tree as far as possible.  Prefix\n        mapping are pushed to its parent unless the parent has the\n        prefix mapped to another URI or the parent has the prefix.\n        This is propagated up the tree until the top is reached.\n        @return: self\n        @rtype: L{Element}\n        \"\"\"\n        for c in self.children:\n            c.promotePrefixes()\n        if self.parent is None:\n            return\n        _pref = []\n        for p, u in self.nsprefixes.items():\n            if p in self.parent.nsprefixes:\n                pu = self.parent.nsprefixes[p]\n                if pu == u:\n                    _pref.append(p)\n                continue\n            if p != self.parent.prefix:\n                self.parent.nsprefixes[p] = u\n                _pref.append(p)\n\n        for x in _pref:\n            del self.nsprefixes[x]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef refitPrefixes(self):\n        for c in self.children:\n            c.refitPrefixes()\n        if self.prefix is not None:\n            ns = self.resolvePrefix(self.prefix)\n            if ns[1] is not None:\n                self.expns = ns[1]\n        self.prefix = None\n        self.nsprefixes = {}\n        return self", "response": "Refit namespace qualification by replacing prefixes\n        with explicit namespaces. Also purges prefix mapping table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a flattened representation of the branch.", "response": "def branch(self):\n        \"\"\"\n        Get a flattened representation of the branch.\n        @return: A flat list of nodes.\n        @rtype: [L{Element},..]\n        \"\"\"\n        branch = [self]\n        for c in self.children:\n            branch += c.branch()\n        return branch"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ancestors(self):\n        ancestors = []\n        p = self.parent\n        while p is not None:\n            ancestors.append(p)\n            p = p.parent\n        return ancestors", "response": "Get a list of ancestors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef walk(self, visitor):\n        visitor(self)\n        for c in self.children:\n            c.walk(visitor)\n        return self", "response": "Walk the branch and call the visitor function\n        on each node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prune(self):\n        pruned = []\n        for c in self.children:\n            c.prune()\n            if c.isempty(False):\n                pruned.append(c)\n        for p in pruned:\n            self.children.remove(p)", "response": "Prune the branch of empty nodes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self):\n        try:\n            child = self.children[self.pos]\n            self.pos += 1\n            return child\n        except:\n            raise StopIteration()", "response": "Get the next child."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the I { unique } set of namespaces referenced in the branch.", "response": "def getNamespaces(self):\n        \"\"\"\n        Get the I{unique} set of namespaces referenced in the branch.\n        @return: A set of namespaces.\n        @rtype: set\n        \"\"\"\n        s = set()\n        for n in self.branch + self.node.ancestors():\n            if self.permit(n.expns):\n                s.add(n.expns)\n            s = s.union(self.pset(n))\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pset(self, n):\n        s = set()\n        for ns in n.nsprefixes.items():\n            if self.permit(ns):\n                s.add(ns[1])\n        return s", "response": "Convert the nodes nsprefixes into a set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef genPrefixes(self):\n        prefixes = {}\n        n = 0\n        for u in self.namespaces:\n            p = 'ns%d' % n\n            prefixes[u] = p\n            n += 1\n        return prefixes", "response": "Generate a I { reverse mapping of unique prefixes for all namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrefitting all of the nodes in the branch.", "response": "def refitNodes(self):\n        \"\"\"\n        Refit (normalize) all of the nodes in the branch.\n        \"\"\"\n        for n in self.branch:\n            if n.prefix is not None:\n                ns = n.namespace()\n                if self.permit(ns):\n                    n.prefix = self.prefixes[ns[1]]\n            self.refitAttrs(n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refitAddr(self, a):\n        if a.prefix is not None:\n            ns = a.namespace()\n            if self.permit(ns):\n                a.prefix = self.prefixes[ns[1]]\n        self.refitValue(a)", "response": "Refit (normalize) the attribute.\n        @param a: An attribute.\n        @type a: L{Attribute}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefits ( normalize ) the attribute s value.", "response": "def refitValue(self, a):\n        \"\"\"\n        Refit (normalize) the attribute's value.\n        @param a: An attribute.\n        @type a: L{Attribute}\n        \"\"\"\n        p, name = splitPrefix(a.getValue())\n        if p is None:\n            return\n        ns = a.resolvePrefix(p)\n        if self.permit(ns):\n            u = ns[1]\n            p = self.prefixes[u]\n            a.setValue(':'.join((p, name)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refitMappings(self):\n        for n in self.branch:\n            n.nsprefixes = {}\n        n = self.node\n        for u, p in self.prefixes.items():\n            n.addPrefix(p, u)", "response": "Refit all of the nsprefix mappings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef skip(self, ns):\n        return ns in (None, Namespace.default, Namespace.xsdns, Namespace.xsins, Namespace.xmlns)", "response": "Returns True if the I { ns } is to B { not } be normalized."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef splitPrefix(name):\n    if isinstance(name, str) and ':' in name:\n        return tuple(name.split(':', 1))\n    else:\n        return (None, name)", "response": "Split the name into a tuple containing the first element in\n    and the second element in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef time_from_match(match_object):\n    hour = int(match_object.group('hour'))\n    minute = int(match_object.group('minute'))\n    second = int(match_object.group('second'))\n    subsecond = match_object.group('subsecond')\n\n    microsecond = 0\n    if subsecond is not None:\n        subsecond_denominator = 10.0 ** len(subsecond)\n        subsecond = int(subsecond)\n        microsecond = subsecond * (1000000 / subsecond_denominator)\n        microsecond = int(round(microsecond))\n\n    return datetime.time(hour, minute, second, microsecond)", "response": "Create a time object from a regular expression match."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a timezone information object from a regular expression match.", "response": "def tzinfo_from_match(match_object):\n    \"\"\"Create a timezone information object from a regular expression match.\n\n    The regular expression match is expected to be from RE_DATE, RE_TIME or\n    RE_DATETIME.\n\n    @param match_object: The regular expression match.\n    @type value: B{re}.I{MatchObject}\n    @return: A timezone information object.\n    @rtype: B{datetime}.I{tzinfo}\n\n    \"\"\"\n    tz_utc = match_object.group('tz_utc')\n    tz_sign = match_object.group('tz_sign')\n    tz_hour = int(match_object.group('tz_hour') or 0)\n    tz_minute = int(match_object.group('tz_minute') or 0)\n    tz_second = int(match_object.group('tz_second') or 0)\n\n    tzinfo = None\n    if tz_utc is not None:\n        tzinfo = UtcTimezone()\n    elif tz_sign is not None:\n        tz_delta = datetime.timedelta(hours=tz_hour,\n                                      minutes=tz_minute,\n                                      seconds=tz_second)\n        if tz_delta == datetime.timedelta():\n            tzinfo = UtcTimezone()\n        else:\n            tz_multiplier = int('%s1' % (tz_sign, ))\n            tz_delta = tz_multiplier * tz_delta\n            tzinfo = FixedOffsetTimezone(tz_delta)\n\n    return tzinfo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the string date.", "response": "def parse(value):\n        \"\"\"Parse the string date.\n\n        This supports the subset of ISO8601 used by xsd:date, but is lenient\n        with what is accepted, handling most reasonable syntax.\n\n        Any timezone is parsed but ignored  because a) it's meaningless without\n        a time and b) B{datetime}.I{date} does not support a tzinfo property.\n\n        @param value: A date string.\n        @type value: str\n        @return: A date object.\n        @rtype: B{datetime}.I{date}\n\n        \"\"\"\n        match_result = RE_DATE.match(value)\n        if match_result is None:\n            raise ValueError('date data has invalid format \"%s\"' % value)\n\n        value = date_from_match(match_result)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the string date.", "response": "def parse(value):\n        \"\"\"Parse the string date.\n\n        This supports the subset of ISO8601 used by xsd:time, but is lenient\n        with what is accepted, handling most reasonable syntax.\n\n        @param value: A time string.\n        @type value: str\n        @return: A time object.\n        @rtype: B{datetime}.I{time}\n\n        \"\"\"\n        match_result = RE_TIME.match(value)\n        if match_result is None:\n            raise ValueError('date data has invalid format \"%s\"' % (value, ))\n\n        date = time_from_match(match_result)\n        tzinfo = tzinfo_from_match(match_result)\n\n        value = date.replace(tzinfo=tzinfo)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(value):\n        match_result = RE_DATETIME.match(value)\n        if match_result is None:\n            raise ValueError('date data has invalid format \"%s\"' % (value, ))\n\n        date = date_from_match(match_result)\n        time = time_from_match(match_result)\n        tzinfo = tzinfo_from_match(match_result)\n\n        value = datetime.datetime.combine(date, time)\n        value = value.replace(tzinfo=tzinfo)\n\n        return value", "response": "Parse the string datetime. is\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string representation of the time offset.", "response": "def tzname(self, dt):\n        \"\"\"\n        http://docs.python.org/library/datetime.html#datetime.tzinfo.tzname\n\n        \"\"\"\n        sign = '+'\n        if self.__offset < datetime.timedelta():\n            sign = '-'\n\n        # total_seconds was introduced in Python 2.7\n        if hasattr(self.__offset, 'total_seconds'):\n            total_seconds = self.__offset.total_seconds()\n        else:\n            total_seconds = (self.__offset.days * 24 * 60 * 60) + \\\n                            (self.__offset.seconds) + \\\n                            (self.__offset.microseconds / 1000000.0)\n\n        hours = total_seconds // (60 * 60)\n        total_seconds -= hours * 60 * 60\n\n        minutes = total_seconds // 60\n        total_seconds -= minutes * 60\n\n        seconds = total_seconds // 1\n        total_seconds -= seconds\n\n        if seconds:\n            return '%s%02d:%02d:%02d' % (sign, hours, minutes, seconds)\n        else:\n            return '%s%02d:%02d' % (sign, hours, minutes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the UTC offset of the object.", "response": "def utcoffset(self, dt):\n        \"\"\"\n        http://docs.python.org/library/datetime.html#datetime.tzinfo.utcoffset\n\n        \"\"\"\n        if self.__is_daylight_time(dt):\n            return self.__dst_offset\n        else:\n            return self.__offset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the dst of the object.", "response": "def dst(self, dt):\n        \"\"\"\n        http://docs.python.org/library/datetime.html#datetime.tzinfo.dst\n\n        \"\"\"\n        if self.__is_daylight_time(dt):\n            return self.__dst_offset - self.__offset\n        else:\n            return datetime.timedelta(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tzname(self, dt):\n        if self.__is_daylight_time(dt):\n            return time.tzname[1]\n        else:\n            return time.tzname[0]", "response": "Returns the timezone name of the given datetime."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge(a, b):\n    for item in a:\n        setattr(b, item[0], item[1])\n        b.__metadata__ = b.__metadata__\n    return b", "response": "Merge all attributes and metadata from I { a to I { b.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef footprint(sobject):\n    n = 0\n    for a in sobject.__keylist__:\n        v = getattr(sobject, a)\n        if v is None:\n            continue\n        if isinstance(v, Object):\n            n += footprint(v)\n            continue\n        if hasattr(v, '__len__'):\n            if len(v):\n                n += 1\n            continue\n        n += 1\n    return n", "response": "Get the I { virtual footprint } of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting s string representation of object", "response": "def tostr(self, object, indent=-2):\n        \"\"\" get s string representation of object \"\"\"\n        history = []\n        return self.process(object, history, indent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the object using the specified indent and newline", "response": "def process(self, object, h, n=0, nl=False):\n        \"\"\" print object using the specified indent (n) and newline (nl). \"\"\"\n        if object is None:\n            return 'None'\n        if isinstance(object, Object):\n            if len(object) == 0:\n                return '<empty>'\n            else:\n                return self.print_object(object, h, n+2, nl)\n        if isinstance(object, dict):\n            if len(object) == 0:\n                return '<empty>'\n            else:\n                return self.print_dictionary(object, h, n+2, nl)\n        if isinstance(object, (list, tuple)):\n            if len(object) == 0:\n                return '<empty>'\n            else:\n                return self.print_collection(object, h, n+2)\n        if isinstance(object, basestring):\n            return '\"%s\"' % tostr(object)\n        return '%s' % tostr(object)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_object(self, d, h, n, nl=False):\n        s = []\n        cls = d.__class__\n        md = d.__metadata__\n        if d in h:\n            s.append('(')\n            s.append(cls.__name__)\n            s.append(')')\n            s.append('...')\n            return ''.join(s)\n        h.append(d)\n        if nl:\n            s.append('\\n')\n            s.append(self.indent(n))\n        if cls != Object:\n            s.append('(')\n            if isinstance(d, Facade):\n                s.append(md.facade)\n            else:\n                s.append(cls.__name__)\n            s.append(')')\n        s.append('{')\n        for item in d:\n            if self.exclude(d, item):\n                continue\n            item = self.unwrap(d, item)\n            s.append('\\n')\n            s.append(self.indent(n+1))\n            if isinstance(item[1], (list, tuple)):\n                s.append(item[0])\n                s.append('[]')\n            else:\n                s.append(item[0])\n            s.append(' = ')\n            s.append(self.process(item[1], h, n, True))\n        s.append('\\n')\n        s.append(self.indent(n))\n        s.append('}')\n        h.pop()\n        return ''.join(s)", "response": "print complex object using the specified indent and newline"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepend(cls, d, s, filter=Filter()):\n        i = 0\n        for x in s:\n            if x in filter:\n                d.insert(i, x)\n                i += 1", "response": "Prepend schema object s from source list to destination list while applying the filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends schema object s from source list to destination list while applying the filter.", "response": "def append(cls, d, s, filter=Filter()):\n        \"\"\"\n        Append schema object's from B{s}ource list to\n        the B{d}estination list while applying the filter.\n        @param d: The destination list.\n        @type d: list\n        @param s: The source list.\n        @type s: list\n        @param filter: A filter that allows items to be appended.\n        @type filter: L{Filter}\n        \"\"\"\n        for item in s:\n            if item in filter:\n                d.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attributes(self, filter=Filter()):\n        result = []\n        for child, ancestry in self:\n            if child.isattr() and child in filter:\n                result.append((child, ancestry))\n        return result", "response": "Get only the attribute content."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_attribute(self, name):\n        for child, ancestry in self.attributes():\n            if child.name == name:\n                return (child, ancestry)\n        return (None, [])", "response": "Get a I { non - attribute by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a child by name.", "response": "def get_child(self, name):\n        \"\"\"\n        Get (find) a I{non-attribute} child by name.\n        @param name: A child name.\n        @type name: str\n        @return: A tuple: the requested (child, ancestry).\n        @rtype: (L{SchemaObject}, [L{SchemaObject},..])\n        \"\"\"\n        for child, ancestry in self.children():\n            if child.any() or child.name == name:\n                return (child, ancestry)\n        return (None, [])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting this properties namespace", "response": "def namespace(self, prefix=None):\n        \"\"\"\n        Get this properties namespace\n        @param prefix: The default prefix.\n        @type prefix: str\n        @return: The schema's target namespace\n        @rtype: (I{prefix},I{URI})\n        \"\"\"\n        ns = self.schema.tns\n        if ns[0] is None:\n            ns = (prefix, ns[1])\n        return ns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unbounded(self):\n        max = self.max\n        if max is None:\n            max = '1'\n        if max.isdigit():\n            return (int(max) > 1)\n        else:\n            return max == 'unbounded'", "response": "Returns True if this node is unbounded False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a referenced type in self or children.", "response": "def find(self, qref, classes=()):\n        \"\"\"\n        Find a referenced type in self or children.\n        @param qref: A qualified reference.\n        @type qref: qref\n        @param classes: A list of classes used to qualify the match.\n        @type classes: [I{class},...]\n        @return: The referenced type.\n        @rtype: L{SchemaObject}\n        @see: L{qualify()}\n        \"\"\"\n        if not len(classes):\n            classes = (self.__class__,)\n        if self.qname == qref and self.__class__ in classes:\n            return self\n        for c in self.rawchildren:\n            p = c.find(qref, classes)\n            if p is not None:\n                return p\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts attribute values that are references to other objects into I { qref.", "response": "def qualify(self):\n        \"\"\"\n        Convert attribute values, that are references to other\n        objects, into I{qref}.  Qualfied using default document namespace.\n        Since many wsdls are written improperly: when the document does\n        not define a default namespace, the schema target namespace is used\n        to qualify references.\n        \"\"\"\n        defns = self.root.defaultNamespace()\n        if Namespace.none(defns):\n            defns = self.schema.tns\n        for a in self.autoqualified():\n            ref = getattr(self, a)\n            if ref is None:\n                continue\n            if isqref(ref):\n                continue\n            qref = qualify(ref, self.root, defns)\n            log.debug('%s, convert %s=\"%s\" to %s', self.id, a, ref, qref)\n            setattr(self, a, qref)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge another object as needed.", "response": "def merge(self, other):\n        \"\"\"\n        Merge another object as needed.\n        \"\"\"\n        other.qualify()\n        for n in ('name',\n                  'qname',\n                  'min',\n                  'max',\n                  'default',\n                  'type',\n                  'nillable',\n                  'form_qualified',):\n            if getattr(self, n) is not None:\n                continue\n            v = getattr(other, n)\n            if v is None:\n                continue\n            setattr(self, n, v)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef content(self, collection=None, filter=Filter(), history=None):\n        if collection is None:\n            collection = []\n        if history is None:\n            history = []\n        if self in history:\n            return collection\n        history.append(self)\n        if self in filter:\n            collection.append(self)\n        for c in self.rawchildren:\n            c.content(collection, filter, history[:])\n        return collection", "response": "Get a I { flattened list of this node s contents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str(self, indent=0, history=None):\n        if history is None:\n            history = []\n        if self in history:\n            return '%s ...' % Repr(self)\n        history.append(self)\n        tab = '%*s' % (indent * 3, '')\n        result = []\n        result.append('%s<%s' % (tab, self.id))\n        for n in self.description():\n            if not hasattr(self, n):\n                continue\n            v = getattr(self, n)\n            if v is None:\n                continue\n            result.append(' %s=\"%s\"' % (n, v))\n        if len(self):\n            result.append('>')\n            for c in self.rawchildren:\n                result.append('\\n')\n                result.append(c.str(indent+1, history[:]))\n                if c.isattr():\n                    result.append('@')\n            result.append('\\n%s' % tab)\n            result.append('</%s>' % self.__class__.__name__)\n        else:\n            result.append(' />')\n        return ''.join(result)", "response": "Return a string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntraverse the tree looking for matches.", "response": "def find(self, node, list):\n        \"\"\"\n        Traverse the tree looking for matches.\n        @param node: A node to match on.\n        @type node: L{SchemaObject}\n        @param list: A list to fill.\n        @type list: list\n        \"\"\"\n        if self.matcher.match(node):\n            list.append(node)\n            self.limit -= 1\n            if self.limit == 0:\n                return\n        for c in node.rawchildren:\n            self.find(c, list)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setduration(self, **duration):\n        if len(duration) == 1:\n            arg = [x[0] for x in duration.items()]\n            if not arg[0] in self.units:\n                raise Exception('must be: %s' % str(self.units))\n            self.duration = arg\n        return self", "response": "Set the caching duration which defines how long the file will be cached."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mktmp(self):\n        try:\n            if not os.path.isdir(self.location):\n                os.makedirs(self.location)\n        except:\n            log.debug(self.location, exc_info=1)\n        return self", "response": "Make the I { location } directory if it doesn t already exits."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that the file has not expired based on the I { duration }.", "response": "def validate(self, fn):\n        \"\"\"\n        Validate that the file has not expired based on the I{duration}.\n        @param fn: The file name.\n        @type fn: str\n        \"\"\"\n        if self.duration[1] < 1:\n            return\n        created = dt.fromtimestamp(os.path.getctime(fn))\n        d = {self.duration[0]: self.duration[1]}\n        expired = created+timedelta(**d)\n        if expired < dt.now():\n            log.debug('%s expired, deleted', fn)\n            os.remove(fn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the tag with the specified value using the optional type information.", "response": "def process(self, value, tag=None):\n        \"\"\"\n        Process (marshal) the tag with the specified value using the\n        optional type information.\n        @param value: The value (content) of the XML node.\n        @type value: (L{Object}|any)\n        @param tag: The (optional) tag name for the value.  The default is\n            value.__class__.__name__\n        @type tag: str\n        @return: An xml node.\n        @rtype: L{Element}\n        \"\"\"\n        content = Content(tag=tag, value=value)\n        result = Core.process(self, content)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the reply XML for the specified method and return the result.", "response": "def get_reply(self, method, reply):\n        \"\"\"\n        Process the I{reply} for the specified I{method} by sax parsing the\n        I{reply} and then unmarshalling into python object(s).\n        @param method: The name of the invoked method.\n        @type method: str\n        @param reply: The reply XML received after invoking the specified\n            method.\n        @type reply: str\n        @return: The unmarshalled reply.  The returned value is an L{Object}\n            for a I{list} depending on whether the service returns a single\n            object or a collection.\n        @rtype: tuple ( L{Element}, L{Object} )\n        \"\"\"\n        reply = self.replyfilter(reply)\n        sax = Parser()\n        replyroot = sax.parse(string=reply)\n        plugins = PluginContainer(self.options().plugins)\n        plugins.message.parsed(reply=replyroot)\n        soapenv = replyroot.getChild('Envelope')\n        soapenv.promotePrefixes()\n        soapbody = soapenv.getChild('Body')\n        self.detect_fault(soapbody)\n        soapbody = self.multiref.process(soapbody)\n        nodes = self.replycontent(method, soapbody)\n        rtypes = self.returned_types(method)\n        if len(rtypes) > 1:\n            result = self.replycomposite(rtypes, nodes)\n            return (replyroot, result)\n        if len(rtypes) == 1:\n            if rtypes[0].unbounded():\n                result = self.replylist(rtypes[0], nodes)\n                return (replyroot, result)\n            if len(nodes):\n                unmarshaller = self.unmarshaller()\n                resolved = rtypes[0].resolve(nobuiltin=True)\n                result = unmarshaller.process(nodes[0], resolved)\n                return (replyroot, result)\n        return (replyroot, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetecting the fault element in the soap body.", "response": "def detect_fault(self, body):\n        \"\"\"\n        Detect I{hidden} soapenv:Fault element in the soap body.\n        @param body: The soap envelope body.\n        @type body: L{Element}\n        @raise WebFault: When found.\n        \"\"\"\n        fault = body.getChild('Fault', envns)\n        if fault is None:\n            return\n        unmarshaller = self.unmarshaller(False)\n        p = unmarshaller.process(fault)\n        if self.options().faults:\n            raise WebFault(p, fault)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a I{list} reply. This method is called when it has been detected that the reply is a list. @param rt: The return I{type}. @type rt: L{suds.xsd.sxbase.SchemaObject} @param nodes: A collection of XML nodes. @type nodes: [L{Element},...] @return: A list of I{unmarshalled} objects. @rtype: [L{Object},...]", "response": "def replylist(self, rt, nodes):\n        \"\"\"\n        Construct a I{list} reply.  This method is called when it has been\n            detected\n        that the reply is a list.\n        @param rt: The return I{type}.\n        @type rt: L{suds.xsd.sxbase.SchemaObject}\n        @param nodes: A collection of XML nodes.\n        @type nodes: [L{Element},...]\n        @return: A list of I{unmarshalled} objects.\n        @rtype: [L{Object},...]\n        \"\"\"\n        result = []\n        resolved = rt.resolve(nobuiltin=True)\n        unmarshaller = self.unmarshaller()\n        for node in nodes:\n            sobject = unmarshaller.process(node, resolved)\n            result.append(sobject)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replycomposite(self, rtypes, nodes):\n        dictionary = {}\n        for rt in rtypes:\n            dictionary[rt.name] = rt\n        unmarshaller = self.unmarshaller()\n        composite = Factory.object('reply')\n        for node in nodes:\n            tag = node.name\n            rt = dictionary.get(tag, None)\n            if rt is None:\n                if node.get('id') is None:\n                    raise Exception('<%s/> not mapped to message part' % tag)\n                else:\n                    continue\n            resolved = rt.resolve(nobuiltin=True)\n            sobject = unmarshaller.process(node, resolved)\n            value = getattr(composite, tag, None)\n            if value is None:\n                if rt.unbounded():\n                    value = []\n                    setattr(composite, tag, value)\n                    value.append(sobject)\n                else:\n                    setattr(composite, tag, sobject)\n            else:\n                if not isinstance(value, list):\n                    value = [value, ]\n                    setattr(composite, tag, value)\n                value.append(sobject)\n        return composite", "response": "This method is called when the reply has multiple root nodes. This method is called when it has been\n        detected that the reply has multiple root nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fault(self, reply):\n        reply = self.replyfilter(reply)\n        sax = Parser()\n        faultroot = sax.parse(string=reply)\n        soapenv = faultroot.getChild('Envelope')\n        soapbody = soapenv.getChild('Body')\n        fault = soapbody.getChild('Fault')\n        unmarshaller = self.unmarshaller(False)\n        p = unmarshaller.process(fault)\n        if self.options().faults:\n            raise WebFault(p, faultroot)\n        return (faultroot, p.detail)", "response": "Extracts the fault from the specified soap reply."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a parameter fragment for the specified I { method } using the specified parameter definition and the specified value.", "response": "def mkparam(self, method, pdef, object):\n        \"\"\"\n        Builds a parameter for the specified I{method} using the parameter\n        definition (pdef) and the specified value (object).\n        @param method: A method name.\n        @type method: str\n        @param pdef: A parameter definition.\n        @type pdef: tuple: (I{name}, L{xsd.sxbase.SchemaObject})\n        @param object: The parameter value.\n        @type object: any\n        @return: The parameter fragment.\n        @rtype: L{Element}\n        \"\"\"\n        marshaller = self.marshaller()\n        content = \\\n            Content(tag=pdef[0],\n                    value=object,\n                    type=pdef[1],\n                    real=pdef[1].resolve())\n        return marshaller.process(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef body(self, content):\n        body = Element('Body', ns=envns)\n        body.append(content)\n        return body", "response": "Build the B{<Body/> element for an outbound message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of parameter definitions defined for the specified method.", "response": "def bodypart_types(self, method, input=True):\n        \"\"\"\n        Get a list of I{parameter definitions} (pdef) defined for the specified\n        method.\n        Each I{pdef} is a tuple (I{name}, L{xsd.sxbase.SchemaObject})\n        @param method: A service method.\n        @type method: I{service.Method}\n        @param input: Defines input/output message.\n        @type input: boolean\n        @return:  A list of parameter definitions\n        @rtype: [I{pdef},]\n        \"\"\"\n        result = []\n        if input:\n            parts = method.soap.input.body.parts\n        else:\n            parts = method.soap.output.body.parts\n        for p in parts:\n            if p.element is not None:\n                query = ElementQuery(p.element)\n            else:\n                query = TypeQuery(p.type)\n            pt = query.execute(self.schema())\n            if pt is None:\n                raise TypeNotFound(query.ref)\n            if p.type is not None:\n                pt = PartElement(p.name, pt)\n            if input:\n                if pt.name is None:\n                    result.append((p.name, pt))\n                else:\n                    result.append((pt.name, pt))\n            else:\n                result.append(pt)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of parameter definitions defined for the specified method.", "response": "def headpart_types(self, method, input=True):\n        \"\"\"\n        Get a list of I{parameter definitions} (pdef) defined for the specified\n        method.\n        Each I{pdef} is a tuple (I{name}, L{xsd.sxbase.SchemaObject})\n        @param method: A service method.\n        @type method: I{service.Method}\n        @param input: Defines input/output message.\n        @type input: boolean\n        @return:  A list of parameter definitions\n        @rtype: [I{pdef},]\n        \"\"\"\n        result = []\n        if input:\n            headers = method.soap.input.headers\n        else:\n            headers = method.soap.output.headers\n        for header in headers:\n            part = header.part\n            if part.element is not None:\n                query = ElementQuery(part.element)\n            else:\n                query = TypeQuery(part.type)\n            pt = query.execute(self.schema())\n            if pt is None:\n                raise TypeNotFound(query.ref)\n            if part.type is not None:\n                pt = PartElement(part.name, pt)\n            if input:\n                if pt.name is None:\n                    result.append((part.name, pt))\n                else:\n                    result.append((pt.name, pt))\n            else:\n                result.append(pt)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef returned_types(self, method):\n        result = []\n        for rt in self.bodypart_types(method, input=False):\n            result.append(rt)\n        return result", "response": "Get the L{xsd. sxbase. SchemaObject returned by the method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntranslating the content object to a suds object.", "response": "def translate(self, content):\n        \"\"\"\n        Translate using the XSD type information.\n        Python I{dict} is translated to a suds object.  Most\n        importantly, primative values are translated from python\n        types to XML types using the XSD type.\n        @param content: The content to translate.\n        @type content: L{Object}\n        @return: self\n        @rtype: L{Typed}\n        \"\"\"\n        v = content.value\n        if v is None:\n            return\n        if isinstance(v, dict):\n            cls = content.real.name\n            content.value = Factory.object(cls, v)\n            md = content.value.__metadata__\n            md.sxtype = content.type\n            return\n        v = content.real.translate(v, False)\n        content.value = v\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort(self, content):\n        v = content.value\n        if isinstance(v, Object):\n            md = v.__metadata__\n            md.ordering = self.ordering(content.real)\n        return self", "response": "Sorts suds object attributes based on ordering defined\n        in the XSD type information."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the attribute ordering defined in the specified xsd type information.", "response": "def ordering(self, type):\n        \"\"\"\n        Get the attribute ordering defined in the specified\n        XSD type information.\n        @param type: An XSD type object.\n        @type type: SchemaObject\n        @return: An ordered list of attribute names.\n        @rtype: list\n        \"\"\"\n        result = []\n        for child, ancestry in type.resolve():\n            name = child.name\n            if child.name is None:\n                continue\n            if child.isattr():\n                name = '_%s' % child.name\n            result.append(name)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets xml representation of the object.", "response": "def xml(self):\n        \"\"\"\n        Get xml representation of the object.\n        @return: The root node.\n        @rtype: L{Element}\n        \"\"\"\n        root = Element('Security', ns=wssens)\n        root.set('mustUnderstand', str(self.mustUnderstand).lower())\n        for t in self.tokens:\n            root.append(t.xml())\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the nonce which is arbitraty set of bytes to prevent reply attacks.", "response": "def setnonce(self, text=None):\n        \"\"\"\n        Set I{nonce} which is arbitraty set of bytes to prevent\n        reply attacks.\n        @param text: The nonce text value.\n            Generated when I{None}.\n        @type text: str\n        \"\"\"\n        if text is None:\n            s = []\n            s.append(self.username)\n            s.append(self.password)\n            s.append(Token.sysdate())\n            m = md5()\n            m.update(':'.join(s).encode(\"utf-8\"))\n            self.nonce = m.hexdigest()\n        else:\n            self.nonce = text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setcreated(self, dt=None):\n        if dt is None:\n            self.created = Token.utc()\n        else:\n            self.created = dt", "response": "Set the created date & time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xml(self):\n        root = Element('UsernameToken', ns=wssens)\n        u = Element('Username', ns=wssens)\n        u.setText(self.username)\n        root.append(u)\n        p = Element('Password', ns=wssens)\n        p.setText(self.password)\n        root.append(p)\n        if self.nonce is not None:\n            n = Element('Nonce', ns=wssens)\n            n.setText(self.nonce)\n            root.append(n)\n        if self.created is not None:\n            n = Element('Created', ns=wsuns)\n            n.setText(str(DateTime(self.created)))\n            root.append(n)\n        return root", "response": "Get xml representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setaty(self, content):\n        name = 'arrayType'\n        ns = (None, 'http://schemas.xmlsoap.org/soap/encoding/')\n        aty = content.node.get(name, ns)\n        if aty is not None:\n            content.aty = aty\n            parts = aty.split('[')\n            ref = parts[0]\n            if len(parts) == 2:\n                self.applyaty(content, ref)\n            else:\n                pass  # (2) dimensional array\n        return self", "response": "Set the arrayType attribute of the current object to the given content."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying the type referenced in the XSI type reference to the content object.", "response": "def applyaty(self, content, xty):\n        \"\"\"\n        Apply the type referenced in the I{arrayType} to the content\n        (child nodes) of the array.  Each element (node) in the array\n        that does not have an explicit xsi:type attribute is given one\n        based on the I{arrayType}.\n        @param content: An array content.\n        @type content: L{Content}\n        @param xty: The XSI type reference.\n        @type xty: str\n        @return: self\n        @rtype: L{Encoded}\n        \"\"\"\n        name = 'type'\n        ns = Namespace.xsins\n        parent = content.node\n        for child in parent.getChildren():\n            ref = child.get(name, ns)\n            if ref is None:\n                parent.addPrefix(ns[0], ns[1])\n                attr = ':'.join((ns[0], name))\n                child.set(attr, xty)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef promote(self, content):\n        for n, v in content.data:\n            if isinstance(v, list):\n                content.data = v\n                return\n        content.data = []", "response": "Promote the content. data with the first attribute\n            of the current content. data that is a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding an object for the specified typename as defined in the schema", "response": "def build(self, name):\n        \"build an object for the specified typename as defined in the schema\"\n        if isinstance(name, basestring):\n            type = self.resolver.find(name)\n            if type is None:\n                raise TypeNotFound(name)\n        else:\n            type = name\n        cls = type.name\n        if type.mixed():\n            data = Factory.property(cls)\n        else:\n            data = Factory.object(cls)\n        resolved = type.resolve()\n        md = data.__metadata__\n        md.sxtype = resolved\n        md.ordering = self.ordering(resolved)\n        history = []\n        self.add_attributes(data, resolved)\n        for child, ancestry in type.children():\n            if self.skip_child(child, ancestry):\n                continue\n            self.process(data, child, history[:])\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd required attributes to data", "response": "def add_attributes(self, data, type):\n        \"\"\" add required attributes \"\"\"\n        for attr, ancestry in type.attributes():\n            name = '_%s' % attr.name\n            value = attr.get_default()\n            setattr(data, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, pA, pB):\n        if pA in pB.links or \\\n           pB in pA.links:\n            raise Exception('Already linked')\n        dA = pA.domains()\n        dB = pB.domains()\n        for d in dA:\n            if d in dB:\n                raise Exception('Duplicate domain \"%s\" found' % d)\n        for d in dB:\n            if d in dA:\n                raise Exception('Duplicate domain \"%s\" found' % d)\n        kA = pA.keys()\n        kB = pB.keys()\n        for k in kA:\n            if k in kB:\n                raise Exception('Duplicate key %s found' % k)\n        for k in kB:\n            if k in kA:\n                raise Exception('Duplicate key %s found' % k)\n        return self", "response": "Validate that the two properties of a link are linked."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, value):\n        if value is None:\n            return\n        if len(self.classes) and not isinstance(value, self.classes):\n            msg = '\"%s\" must be: %s' % (self.name, self.classes)\n            raise AttributeError(msg)", "response": "Validate the value is of the correct class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef definition(self, name):\n        d = self.definitions.get(name)\n        if d is None:\n            raise AttributeError(name)\n        return d", "response": "Get the definition for the property I { name.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the properties of this object with the values from the specified object.", "response": "def update(self, other):\n        \"\"\"\n        Update the property values as specified by keyword/value.\n        @param other: An object to update from.\n        @type other: (dict|L{Properties})\n        @return: self\n        @rtype: L{Properties}\n        \"\"\"\n        if isinstance(other, Properties):\n            other = other.defined\n        for n, v in other.items():\n            self.set(n, v)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, name, value):\n        self.provider(name).__set(name, value)\n        return self", "response": "Set the value of a property by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, name, *df):\n        return self.provider(name).__get(name, *df)", "response": "Get the value of a property by I { name.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlink(self, *others):\n        if not len(others):\n            others = self.links[:]\n        for p in self.links[:]:\n            if p in others:\n                p.teardown()\n        return self", "response": "Unlink the specified properties object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef provider(self, name, history=None):\n        if history is None:\n            history = []\n        history.append(self)\n        if name in self.definitions:\n            return self\n        for x in self.links:\n            if x in history:\n                continue\n            provider = x.provider(name, history)\n            if provider is not None:\n                return provider\n        history.remove(self)\n        if len(history):\n            return None\n        return self", "response": "Find the provider of the property by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the set of property names.", "response": "def keys(self, history=None):\n        \"\"\"\n        Get the set of I{all} property names.\n        @param history: A history of nodes checked to prevent\n            circular hunting.\n        @type history: [L{Properties},..]\n        @return: A set of property names.\n        @rtype: list\n        \"\"\"\n        if history is None:\n            history = []\n        history.append(self)\n        keys = set()\n        keys.update(self.definitions.keys())\n        for x in self.links:\n            if x in history:\n                continue\n            keys.update(x.keys(history))\n        history.remove(self)\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef domains(self, history=None):\n        if history is None:\n            history = []\n        history.append(self)\n        domains = set()\n        domains.add(self.domain)\n        for x in self.links:\n            if x in history:\n                continue\n            domains.update(x.domains(history))\n        history.remove(self)\n        return domains", "response": "Get the set of domain names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prime(self):\n        for d in self.definitions.values():\n            self.defined[d.name] = d.default\n        return self", "response": "Prime the stored values based on default values\n            found in property definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link(self, other):\n        p = other.__pts__\n        return self.properties.link(p)", "response": "Link this object with an I { other properties object to create a network of properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting scanners and whether or not they are enabled.", "response": "def list_scanners(zap_helper, scanners):\n    \"\"\"Get a list of scanners and whether or not they are enabled.\"\"\"\n    scanner_list = zap_helper.zap.ascan.scanners()\n\n    if scanners is not None and 'all' not in scanners:\n        scanner_list = filter_by_ids(scanner_list, scanners)\n\n    click.echo(tabulate([[s['id'], s['name'], s['policyId'], s['enabled'], s['attackStrength'], s['alertThreshold']]\n                         for s in scanner_list],\n                        headers=['ID', 'Name', 'Policy ID', 'Enabled', 'Strength', 'Threshold'],\n                        tablefmt='grid'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_scanner_strength(zap_helper, scanners, strength):\n    if not scanners or 'all' in scanners:\n        scanners = _get_all_scanner_ids(zap_helper)\n\n    with zap_error_handler():\n        zap_helper.set_scanner_attack_strength(scanners, strength)\n\n    console.info('Set attack strength to {0}.'.format(strength))", "response": "Set the attack strength for scanners."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_scanner_threshold(zap_helper, scanners, threshold):\n    if not scanners or 'all' in scanners:\n        scanners = _get_all_scanner_ids(zap_helper)\n\n    with zap_error_handler():\n        zap_helper.set_scanner_alert_threshold(scanners, threshold)\n\n    console.info('Set alert threshold to {0}.'.format(threshold))", "response": "Set the alert threshold for scanners."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all scanner IDs.", "response": "def _get_all_scanner_ids(zap_helper):\n    \"\"\"Get all scanner IDs.\"\"\"\n    scanners = zap_helper.zap.ascan.scanners()\n    return [s['id'] for s in scanners]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_policies(zap_helper, policy_ids):\n    policies = filter_by_ids(zap_helper.zap.ascan.policies(), policy_ids)\n\n    click.echo(tabulate([[p['id'], p['name'], p['enabled'], p['attackStrength'], p['alertThreshold']]\n                         for p in policies],\n                        headers=['ID', 'Name', 'Enabled', 'Strength', 'Threshold'],\n                        tablefmt='grid'))", "response": "Get a list of policies and whether or not they are enabled."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_policies(zap_helper, policy_ids):\n    if not policy_ids:\n        policy_ids = _get_all_policy_ids(zap_helper)\n\n    with zap_error_handler():\n        zap_helper.enable_policies_by_ids(policy_ids)", "response": "Enable policies for the current application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the attack strength for policies.", "response": "def set_policy_strength(zap_helper, policy_ids, strength):\n    \"\"\"Set the attack strength for policies.\"\"\"\n    if not policy_ids:\n        policy_ids = _get_all_policy_ids(zap_helper)\n\n    with zap_error_handler():\n        zap_helper.set_policy_attack_strength(policy_ids, strength)\n\n    console.info('Set attack strength to {0}.'.format(strength))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the alert threshold for policies.", "response": "def set_policy_threshold(zap_helper, policy_ids, threshold):\n    \"\"\"Set the alert threshold for policies.\"\"\"\n    if not policy_ids:\n        policy_ids = _get_all_policy_ids(zap_helper)\n\n    with zap_error_handler():\n        zap_helper.set_policy_alert_threshold(policy_ids, threshold)\n\n    console.info('Set alert threshold to {0}.'.format(threshold))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_all_policy_ids(zap_helper):\n    policies = zap_helper.zap.ascan.policies()\n    return [p['id'] for p in policies]", "response": "Get all policy IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the ZAP daemon.", "response": "def start(self, options=None):\n        \"\"\"Start the ZAP Daemon.\"\"\"\n        if self.is_running():\n            self.logger.warn('ZAP is already running on port {0}'.format(self.port))\n            return\n\n        if platform.system() == 'Windows' or platform.system().startswith('CYGWIN'):\n            executable = 'zap.bat'\n        else:\n            executable = 'zap.sh'\n\n        executable_path = os.path.join(self.zap_path, executable)\n        if not os.path.isfile(executable_path):\n            raise ZAPError(('ZAP was not found in the path \"{0}\". You can set the path to where ZAP is ' +\n                            'installed on your system using the --zap-path command line parameter or by ' +\n                            'default using the ZAP_PATH environment variable.').format(self.zap_path))\n\n        zap_command = [executable_path, '-daemon', '-port', str(self.port)]\n        if options:\n            extra_options = shlex.split(options)\n            zap_command += extra_options\n\n        if self.log_path is None:\n            log_path = os.path.join(self.zap_path, 'zap.log')\n        else:\n            log_path = os.path.join(self.log_path, 'zap.log')\n\n        self.logger.debug('Starting ZAP process with command: {0}.'.format(' '.join(zap_command)))\n        self.logger.debug('Logging to {0}'.format(log_path))\n        with open(log_path, 'w+') as log_file:\n            subprocess.Popen(\n                zap_command, cwd=self.zap_path, stdout=log_file,\n                stderr=subprocess.STDOUT)\n\n        self.wait_for_zap(self.timeout)\n\n        self.logger.debug('ZAP started successfully.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits for ZAP to be ready to receive API calls.", "response": "def wait_for_zap(self, timeout):\n        \"\"\"Wait for ZAP to be ready to receive API calls.\"\"\"\n        timeout_time = time.time() + timeout\n        while not self.is_running():\n            if time.time() > timeout_time:\n                raise ZAPError('Timed out waiting for ZAP to start.')\n            time.sleep(2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_running(self):\n        try:\n            result = requests.get(self.proxy_url)\n        except RequestException:\n            return False\n\n        if 'ZAP-Header' in result.headers.get('Access-Control-Allow-Headers', []):\n            return True\n\n        raise ZAPError('Another process is listening on {0}'.format(self.proxy_url))", "response": "Check if ZAP is running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_url(self, url, sleep_after_open=2):\n        self.zap.urlopen(url)\n        # Give the sites tree a chance to get updated\n        time.sleep(sleep_after_open)", "response": "Open a URL through ZAP and return the dict of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_spider(self, target_url, context_name=None, user_name=None):\n        self.logger.debug('Spidering target {0}...'.format(target_url))\n\n        context_id, user_id = self._get_context_and_user_ids(context_name, user_name)\n\n        if user_id:\n            self.logger.debug('Running spider in context {0} as user {1}'.format(context_id, user_id))\n            scan_id = self.zap.spider.scan_as_user(context_id, user_id, target_url)\n        else:\n            scan_id = self.zap.spider.scan(target_url)\n\n        if not scan_id:\n            raise ZAPError('Error running spider.')\n        elif not scan_id.isdigit():\n            raise ZAPError('Error running spider: \"{0}\"'.format(scan_id))\n\n        self.logger.debug('Started spider with ID {0}...'.format(scan_id))\n\n        while int(self.zap.spider.status()) < 100:\n            self.logger.debug('Spider progress %: {0}'.format(self.zap.spider.status()))\n            time.sleep(self._status_check_sleep)\n\n        self.logger.debug('Spider #{0} completed'.format(scan_id))", "response": "Run a spider against a URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning an active scan against a URL.", "response": "def run_active_scan(self, target_url, recursive=False, context_name=None, user_name=None):\n        \"\"\"Run an active scan against a URL.\"\"\"\n        self.logger.debug('Scanning target {0}...'.format(target_url))\n\n        context_id, user_id = self._get_context_and_user_ids(context_name, user_name)\n\n        if user_id:\n            self.logger.debug('Scanning in context {0} as user {1}'.format(context_id, user_id))\n            scan_id = self.zap.ascan.scan_as_user(target_url, context_id, user_id, recursive)\n        else:\n            scan_id = self.zap.ascan.scan(target_url, recurse=recursive)\n\n        if not scan_id:\n            raise ZAPError('Error running active scan.')\n        elif not scan_id.isdigit():\n            raise ZAPError(('Error running active scan: \"{0}\". Make sure the URL is in the site ' +\n                            'tree by using the open-url or scanner commands before running an active ' +\n                            'scan.').format(scan_id))\n\n        self.logger.debug('Started scan with ID {0}...'.format(scan_id))\n\n        while int(self.zap.ascan.status()) < 100:\n            self.logger.debug('Scan progress %: {0}'.format(self.zap.ascan.status()))\n            time.sleep(self._status_check_sleep)\n\n        self.logger.debug('Scan #{0} completed'.format(scan_id))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning AJAX Spider against a URL.", "response": "def run_ajax_spider(self, target_url):\n        \"\"\"Run AJAX Spider against a URL.\"\"\"\n        self.logger.debug('AJAX Spidering target {0}...'.format(target_url))\n\n        self.zap.ajaxSpider.scan(target_url)\n\n        while self.zap.ajaxSpider.status == 'running':\n            self.logger.debug('AJAX Spider: {0}'.format(self.zap.ajaxSpider.status))\n            time.sleep(self._status_check_sleep)\n\n        self.logger.debug('AJAX Spider completed')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alerts(self, alert_level='High'):\n        alerts = self.zap.core.alerts()\n        alert_level_value = self.alert_levels[alert_level]\n\n        alerts = sorted((a for a in alerts if self.alert_levels[a['risk']] >= alert_level_value),\n                        key=lambda k: self.alert_levels[k['risk']], reverse=True)\n\n        return alerts", "response": "Get a filtered list of alerts at the given alert level and sorted by alert level."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a list of currently enabled scanners.", "response": "def enabled_scanner_ids(self):\n        \"\"\"Retrieves a list of currently enabled scanners.\"\"\"\n        enabled_scanners = []\n        scanners = self.zap.ascan.scanners()\n\n        for scanner in scanners:\n            if scanner['enabled'] == 'true':\n                enabled_scanners.append(scanner['id'])\n\n        return enabled_scanners"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_scanners_by_ids(self, scanner_ids):\n        scanner_ids = ','.join(scanner_ids)\n        self.logger.debug('Enabling scanners with IDs {0}'.format(scanner_ids))\n        return self.zap.ascan.enable_scanners(scanner_ids)", "response": "Enable a list of scanner IDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisable a list of scanner IDs.", "response": "def disable_scanners_by_ids(self, scanner_ids):\n        \"\"\"Disable a list of scanner IDs.\"\"\"\n        scanner_ids = ','.join(scanner_ids)\n        self.logger.debug('Disabling scanners with IDs {0}'.format(scanner_ids))\n        return self.zap.ascan.disable_scanners(scanner_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_scanners_by_group(self, group):\n        if group == 'all':\n            self.logger.debug('Enabling all scanners')\n            return self.zap.ascan.enable_all_scanners()\n\n        try:\n            scanner_list = self.scanner_group_map[group]\n        except KeyError:\n            raise ZAPError(\n                'Invalid group \"{0}\" provided. Valid groups are: {1}'.format(\n                    group, ', '.join(self.scanner_groups)\n                )\n            )\n\n        self.logger.debug('Enabling scanner group {0}'.format(group))\n        return self.enable_scanners_by_ids(scanner_list)", "response": "Enables the scanners in the group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisable the scanners in the group.", "response": "def disable_scanners_by_group(self, group):\n        \"\"\"\n        Disables the scanners in the group if it matches one in the scanner_group_map.\n        \"\"\"\n        if group == 'all':\n            self.logger.debug('Disabling all scanners')\n            return self.zap.ascan.disable_all_scanners()\n\n        try:\n            scanner_list = self.scanner_group_map[group]\n        except KeyError:\n            raise ZAPError(\n                'Invalid group \"{0}\" provided. Valid groups are: {1}'.format(\n                    group, ', '.join(self.scanner_groups)\n                )\n            )\n\n        self.logger.debug('Disabling scanner group {0}'.format(group))\n        return self.disable_scanners_by_ids(scanner_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables the provided scanners by group and or numeric ID.", "response": "def enable_scanners(self, scanners):\n        \"\"\"\n        Enable the provided scanners by group and/or IDs.\n        \"\"\"\n        scanner_ids = []\n        for scanner in scanners:\n            if scanner in self.scanner_groups:\n                self.enable_scanners_by_group(scanner)\n            elif scanner.isdigit():\n                scanner_ids.append(scanner)\n            else:\n                raise ZAPError('Invalid scanner \"{0}\" provided. Must be a valid group or numeric ID.'.format(scanner))\n\n        if scanner_ids:\n            self.enable_scanners_by_ids(scanner_ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disable_scanners(self, scanners):\n        scanner_ids = []\n        for scanner in scanners:\n            if scanner in self.scanner_groups:\n                self.disable_scanners_by_group(scanner)\n            elif scanner.isdigit():\n                scanner_ids.append(scanner)\n            else:\n                raise ZAPError('Invalid scanner \"{0}\" provided. Must be a valid group or numeric ID.'.format(scanner))\n\n        if scanner_ids:\n            self.disable_scanners_by_ids(scanner_ids)", "response": "Disable the provided scanners by group and or numeric ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_enabled_scanners(self, scanners):\n        self.logger.debug('Disabling all current scanners')\n        self.zap.ascan.disable_all_scanners()\n        self.enable_scanners(scanners)", "response": "Set all the scanners that are enabled for this locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the attack strength for the given scanners.", "response": "def set_scanner_attack_strength(self, scanner_ids, attack_strength):\n        \"\"\"Set the attack strength for the given scanners.\"\"\"\n        for scanner_id in scanner_ids:\n            self.logger.debug('Setting strength for scanner {0} to {1}'.format(scanner_id, attack_strength))\n            result = self.zap.ascan.set_scanner_attack_strength(scanner_id, attack_strength)\n            if result != 'OK':\n                raise ZAPError('Error setting strength for scanner with ID {0}: {1}'.format(scanner_id, result))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_scanner_alert_threshold(self, scanner_ids, alert_threshold):\n        for scanner_id in scanner_ids:\n            self.logger.debug('Setting alert threshold for scanner {0} to {1}'.format(scanner_id, alert_threshold))\n            result = self.zap.ascan.set_scanner_alert_threshold(scanner_id, alert_threshold)\n            if result != 'OK':\n                raise ZAPError('Error setting alert threshold for scanner with ID {0}: {1}'.format(scanner_id, result))", "response": "Set the alert theshold for the given policies."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset enabled policies from a list of IDs.", "response": "def enable_policies_by_ids(self, policy_ids):\n        \"\"\"Set enabled policy from a list of IDs.\"\"\"\n        policy_ids = ','.join(policy_ids)\n        self.logger.debug('Setting enabled policies to IDs {0}'.format(policy_ids))\n        self.zap.ascan.set_enabled_policies(policy_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_policy_attack_strength(self, policy_ids, attack_strength):\n        for policy_id in policy_ids:\n            self.logger.debug('Setting strength for policy {0} to {1}'.format(policy_id, attack_strength))\n            result = self.zap.ascan.set_policy_attack_strength(policy_id, attack_strength)\n            if result != 'OK':\n                raise ZAPError('Error setting strength for policy with ID {0}: {1}'.format(policy_id, result))", "response": "Set the attack strength for the given policies."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_policy_alert_threshold(self, policy_ids, alert_threshold):\n        for policy_id in policy_ids:\n            self.logger.debug('Setting alert threshold for policy {0} to {1}'.format(policy_id, alert_threshold))\n            result = self.zap.ascan.set_policy_alert_threshold(policy_id, alert_threshold)\n            if result != 'OK':\n                raise ZAPError('Error setting alert threshold for policy with ID {0}: {1}'.format(policy_id, result))", "response": "Set the alert theshold for the given policies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exclude_from_all(self, exclude_regex):\n        try:\n            re.compile(exclude_regex)\n        except re.error:\n            raise ZAPError('Invalid regex \"{0}\" provided'.format(exclude_regex))\n\n        self.logger.debug('Excluding {0} from proxy, spider and active scanner.'.format(exclude_regex))\n\n        self.zap.core.exclude_from_proxy(exclude_regex)\n        self.zap.spider.exclude_from_scan(exclude_regex)\n        self.zap.ascan.exclude_from_scan(exclude_regex)", "response": "Exclude a pattern from proxy spider and active scanner."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating and save XML report", "response": "def xml_report(self, file_path):\n        \"\"\"Generate and save XML report\"\"\"\n        self.logger.debug('Generating XML report')\n        report = self.zap.core.xmlreport()\n        self._write_report(report, file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef md_report(self, file_path):\n        self.logger.debug('Generating MD report')\n        report = self.zap.core.mdreport()\n        self._write_report(report, file_path)", "response": "Generate and save MD report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate and save HTML report.", "response": "def html_report(self, file_path):\n        \"\"\"Generate and save HTML report.\"\"\"\n        self.logger.debug('Generating HTML report')\n        report = self.zap.core.htmlreport()\n        self._write_report(report, file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the given report to the given file path.", "response": "def _write_report(report, file_path):\n        \"\"\"Write report to the given file path.\"\"\"\n        with open(file_path, mode='wb') as f:\n            if not isinstance(report, binary_type):\n                report = report.encode('utf-8')\n            f.write(report)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the context ID for a given context name.", "response": "def get_context_info(self, context_name):\n        \"\"\"Get the context ID for a given context name.\"\"\"\n        context_info = self.zap.context.context(context_name)\n        if not isinstance(context_info, dict):\n            raise ZAPError('Context with name \"{0}\" wasn\\'t found'.format(context_name))\n\n        return context_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_user_id_from_name(self, context_id, user_name):\n        users = self.zap.users.users_list(context_id)\n        for user in users:\n            if user['name'] == user_name:\n                return user['id']\n\n        raise ZAPError('No user with the name \"{0}\"\" was found for context {1}'.format(user_name, context_id))", "response": "Get a user ID from the user name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist scripts currently loaded into ZAP.", "response": "def list_scripts(zap_helper):\n    \"\"\"List scripts currently loaded into ZAP.\"\"\"\n    scripts = zap_helper.zap.script.list_scripts\n    output = []\n    for s in scripts:\n        if 'enabled' not in s:\n            s['enabled'] = 'N/A'\n\n        output.append([s['name'], s['type'], s['engine'], s['enabled']])\n\n    click.echo(tabulate(output, headers=['Name', 'Type', 'Engine', 'Enabled'], tablefmt='grid'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_engines(zap_helper):\n    engines = zap_helper.zap.script.list_engines\n    console.info('Available engines: {}'.format(', '.join(engines)))", "response": "List available engines that can be used to run scripts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_script(zap_helper, **options):\n    with zap_error_handler():\n        if not os.path.isfile(options['file_path']):\n            raise ZAPError('No file found at \"{0}\", cannot load script.'.format(options['file_path']))\n\n        if not _is_valid_script_engine(zap_helper.zap, options['engine']):\n            engines = zap_helper.zap.script.list_engines\n            raise ZAPError('Invalid script engine provided. Valid engines are: {0}'.format(', '.join(engines)))\n\n        console.debug('Loading script \"{0}\" from \"{1}\"'.format(options['name'], options['file_path']))\n        result = zap_helper.zap.script.load(options['name'], options['script_type'], options['engine'],\n                                            options['file_path'], scriptdescription=options['description'])\n\n        if result != 'OK':\n            raise ZAPError('Error loading script: {0}'.format(result))\n\n    console.info('Script \"{0}\" loaded'.format(options['name']))", "response": "Load a script from a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_valid_script_engine(zap, engine):\n    engine_names = zap.script.list_engines\n    short_names = [e.split(' : ')[1] for e in engine_names]\n\n    return engine in engine_names or engine in short_names", "response": "Check if given script engine is valid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if ZAP is running and able to receive API calls.", "response": "def check_status(zap_helper, timeout):\n    \"\"\"\n    Check if ZAP is running and able to receive API calls.\n\n    You can provide a timeout option which is the amount of time in seconds\n    the command should wait for ZAP to start if it is not currently running.\n    This is useful to run before calling other commands if ZAP was started\n    outside of zap-cli. For example:\n\n        zap-cli status -t 60 && zap-cli open-url \"http://127.0.0.1/\"\n\n    Exits with code 1 if ZAP is either not running or the command timed out\n    waiting for ZAP to start.\n    \"\"\"\n    with helpers.zap_error_handler():\n        if zap_helper.is_running():\n            console.info('ZAP is running')\n        elif timeout is not None:\n            zap_helper.wait_for_zap(timeout)\n            console.info('ZAP is running')\n        else:\n            console.error('ZAP is not running')\n            sys.exit(2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen a URL using the ZAP proxy.", "response": "def open_url(zap_helper, url):\n    \"\"\"Open a URL using the ZAP proxy.\"\"\"\n    console.info('Accessing URL {0}'.format(url))\n    zap_helper.open_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spider_url(zap_helper, url, context_name, user_name):\n    console.info('Running spider...')\n    with helpers.zap_error_handler():\n        zap_helper.run_spider(url, context_name, user_name)", "response": "Run the spider against a URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun an Active Scan against a URL.", "response": "def active_scan(zap_helper, url, scanners, recursive, context_name, user_name):\n    \"\"\"\n    Run an Active Scan against a URL.\n\n    The URL to be scanned must be in ZAP's site tree, i.e. it should have already\n    been opened using the open-url command or found by running the spider command.\n    \"\"\"\n    console.info('Running an active scan...')\n\n    with helpers.zap_error_handler():\n        if scanners:\n            zap_helper.set_enabled_scanners(scanners)\n\n        zap_helper.run_active_scan(url, recursive, context_name, user_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow alerts at the given alert level.", "response": "def show_alerts(zap_helper, alert_level, output_format, exit_code):\n    \"\"\"Show alerts at the given alert level.\"\"\"\n    alerts = zap_helper.alerts(alert_level)\n\n    helpers.report_alerts(alerts, output_format)\n\n    if exit_code:\n        code = 1 if len(alerts) > 0 else 0\n        sys.exit(code)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quick_scan(zap_helper, url, **options):\n    if options['self_contained']:\n        console.info('Starting ZAP daemon')\n        with helpers.zap_error_handler():\n            zap_helper.start(options['start_options'])\n\n    console.info('Running a quick scan for {0}'.format(url))\n\n    with helpers.zap_error_handler():\n        if options['scanners']:\n            zap_helper.set_enabled_scanners(options['scanners'])\n\n        if options['exclude']:\n            zap_helper.exclude_from_all(options['exclude'])\n\n        zap_helper.open_url(url)\n\n        if options['spider']:\n            zap_helper.run_spider(url, options['context_name'], options['user_name'])\n\n        if options['ajax_spider']:\n            zap_helper.run_ajax_spider(url)\n\n        zap_helper.run_active_scan(url, options['recursive'], options['context_name'], options['user_name'])\n\n    alerts = zap_helper.alerts(options['alert_level'])\n\n    helpers.report_alerts(alerts, options['output_format'])\n\n    if options['self_contained']:\n        console.info('Shutting down ZAP daemon')\n        with helpers.zap_error_handler():\n            zap_helper.shutdown()\n\n    exit_code = 1 if len(alerts) > 0 else 0\n    sys.exit(exit_code)", "response": "Run a quick scan of a site."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report(zap_helper, output, output_format):\n    if output_format == 'html':\n        zap_helper.html_report(output)\n    elif output_format == 'md':\n        zap_helper.md_report(output)\n    else:\n        zap_helper.xml_report(output)\n\n    console.info('Report saved to \"{0}\"'.format(output))", "response": "Generate XML MD or HTML report."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_ids(ctx, param, value):\n    if not value:\n        return None\n\n    ids = [x.strip() for x in value.split(',')]\n    for id_item in ids:\n        if not id_item.isdigit():\n            raise click.BadParameter('Non-numeric value \"{0}\" provided for an ID.'.format(id_item))\n\n    return ids", "response": "Validate a list of IDs and convert them to a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_scanner_list(ctx, param, value):\n    if not value:\n        return None\n\n    valid_groups = ctx.obj.scanner_groups\n    scanners = [x.strip() for x in value.split(',')]\n\n    if 'all' in scanners:\n        return ['all']\n\n    scanner_ids = []\n    for scanner in scanners:\n        if scanner.isdigit():\n            scanner_ids.append(scanner)\n        elif scanner in valid_groups:\n            scanner_ids += ctx.obj.scanner_group_map[scanner]\n        else:\n            raise click.BadParameter('Invalid scanner \"{0}\" provided. Must be a valid group or numeric ID.'\n                                     .format(scanner))\n\n    return scanner_ids", "response": "Validate a comma - separated list of scanners and extract it into a list of groups and IDs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_regex(ctx, param, value):\n    if not value:\n        return None\n\n    try:\n        re.compile(value)\n    except re.error:\n        raise click.BadParameter('Invalid regex \"{0}\" provided'.format(value))\n\n    return value", "response": "Validate that a provided regex compiles."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints our alerts in the given format.", "response": "def report_alerts(alerts, output_format='table'):\n    \"\"\"\n    Print our alerts in the given format.\n    \"\"\"\n    num_alerts = len(alerts)\n\n    if output_format == 'json':\n        click.echo(json.dumps(alerts, indent=4))\n    else:\n        console.info('Issues found: {0}'.format(num_alerts))\n        if num_alerts > 0:\n            click.echo(tabulate([[a['alert'], a['risk'], a['cweid'], a['url']] for a in alerts],\n                                headers=['Alert', 'Risk', 'CWE ID', 'URL'], tablefmt='grid'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_by_ids(original_list, ids_to_filter):\n    if not ids_to_filter:\n        return original_list\n\n    return [i for i in original_list if i['id'] in ids_to_filter]", "response": "Filter a list of dicts by IDs using an id key on each dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_session(zap_helper, file_path):\n    console.debug('Saving the session to \"{0}\"'.format(file_path))\n    zap_helper.zap.core.save_session(file_path, overwrite='true')", "response": "Save the session to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a given session.", "response": "def load_session(zap_helper, file_path):\n    \"\"\"Load a given session.\"\"\"\n    with zap_error_handler():\n        if not os.path.isfile(file_path):\n            raise ZAPError('No file found at \"{0}\", cannot load session.'.format(file_path))\n        console.debug('Loading session from \"{0}\"'.format(file_path))\n        zap_helper.zap.core.load_session(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef context_list(zap_helper):\n    contexts = zap_helper.zap.context.context_list\n    if len(contexts):\n        console.info('Available contexts: {0}'.format(contexts[1:-1]))\n    else:\n        console.info('No contexts available in the current session')", "response": "List the available contexts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef context_new(zap_helper, name):\n    console.info('Creating context with name: {0}'.format(name))\n    res = zap_helper.zap.context.new_context(contextname=name)\n    console.info('Context \"{0}\" created with ID: {1}'.format(name, res))", "response": "Create a new context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincludes a pattern in a given context.", "response": "def context_include(zap_helper, name, pattern):\n    \"\"\"Include a pattern in a given context.\"\"\"\n    console.info('Including regex {0} in context with name: {1}'.format(pattern, name))\n    with zap_error_handler():\n        result = zap_helper.zap.context.include_in_context(contextname=name, regex=pattern)\n\n        if result != 'OK':\n            raise ZAPError('Including regex from context failed: {}'.format(result))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexcluding a pattern from a given context.", "response": "def context_exclude(zap_helper, name, pattern):\n    \"\"\"Exclude a pattern from a given context.\"\"\"\n    console.info('Excluding regex {0} from context with name: {1}'.format(pattern, name))\n    with zap_error_handler():\n        result = zap_helper.zap.context.exclude_from_context(contextname=name, regex=pattern)\n\n        if result != 'OK':\n            raise ZAPError('Excluding regex from context failed: {}'.format(result))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay information about a context.", "response": "def context_info(zap_helper, context_name):\n    \"\"\"Get info about the given context.\"\"\"\n    with zap_error_handler():\n        info = zap_helper.get_context_info(context_name)\n\n    console.info('ID: {}'.format(info['id']))\n    console.info('Name: {}'.format(info['name']))\n    console.info('Authentication type: {}'.format(info['authType']))\n    console.info('Included regexes: {}'.format(info['includeRegexs']))\n    console.info('Excluded regexes: {}'.format(info['excludeRegexs']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef context_list_users(zap_helper, context_name):\n    with zap_error_handler():\n        info = zap_helper.get_context_info(context_name)\n\n    users = zap_helper.zap.users.users_list(info['id'])\n    if len(users):\n        user_list = ', '.join([user['name'] for user in users])\n        console.info('Available users for the context {0}: {1}'.format(context_name, user_list))\n    else:\n        console.info('No users configured for the context {}'.format(context_name))", "response": "List the users available for a given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef context_import(zap_helper, file_path):\n    with zap_error_handler():\n        result = zap_helper.zap.context.import_context(file_path)\n\n        if not result.isdigit():\n            raise ZAPError('Importing context from file failed: {}'.format(result))\n\n    console.info('Imported context from {}'.format(file_path))", "response": "Import a saved context file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context_export(zap_helper, name, file_path):\n    with zap_error_handler():\n        result = zap_helper.zap.context.export_context(name, file_path)\n\n        if result != 'OK':\n            raise ZAPError('Exporting context to file failed: {}'.format(result))\n\n    console.info('Exported context {0} to {1}'.format(name, file_path))", "response": "Export a given context to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_value(obj, key, default=missing):\n    if \".\" in key:\n        return _get_value_for_keys(obj, key.split(\".\"), default)\n    else:\n        return _get_value_for_key(obj, key, default)", "response": "Slightly - modified version of marshmallow. utils. get_value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rapply(d, func, *args, **kwargs):\n    if isinstance(d, (tuple, list)):\n        return [_rapply(each, func, *args, **kwargs) for each in d]\n    if isinstance(d, dict):\n        return {\n            key: _rapply(value, func, *args, **kwargs) for key, value in iteritems(d)\n        }\n    else:\n        return func(d, *args, **kwargs)", "response": "Apply a function to all values in a dictionary or list of dictionaries recursively."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction applied by HyperlinksField to get the correct value in the the schema.", "response": "def _url_val(val, key, obj, **kwargs):\n    \"\"\"Function applied by `HyperlinksField` to get the correct value in the\n    schema.\n    \"\"\"\n    if isinstance(val, URLFor):\n        return val.serialize(key, obj, **kwargs)\n    else:\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize the object to a URL.", "response": "def _serialize(self, value, key, obj):\n        \"\"\"Output the URL for the endpoint, given the kwargs passed to\n        ``__init__``.\n        \"\"\"\n        param_values = {}\n        for name, attr_tpl in iteritems(self.params):\n            attr_name = _tpl(str(attr_tpl))\n            if attr_name:\n                attribute_value = _get_value(obj, attr_name, default=missing)\n                if attribute_value is None:\n                    return None\n                if attribute_value is not missing:\n                    param_values[name] = attribute_value\n                else:\n                    raise AttributeError(\n                        \"{attr_name!r} is not a valid \"\n                        \"attribute of {obj!r}\".format(attr_name=attr_name, obj=obj)\n                    )\n            else:\n                param_values[name] = attr_tpl\n        return url_for(self.endpoint, **param_values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _attach_fields(obj):\n    for attr in base_fields.__all__:\n        if not hasattr(obj, attr):\n            setattr(obj, attr, getattr(base_fields, attr))\n    for attr in fields.__all__:\n        setattr(obj, attr, getattr(fields, attr))", "response": "Attach all the marshmallow fields classes to obj including all the marshmallow s custom fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_app(self, app):\n        app.extensions = getattr(app, \"extensions\", {})\n\n        # If using Flask-SQLAlchemy, attach db.session to ModelSchema\n        if has_sqla and \"sqlalchemy\" in app.extensions:\n            db = app.extensions[\"sqlalchemy\"].db\n            self.ModelSchema.OPTIONS_CLASS.session = db.session\n        app.extensions[EXTENSION_NAME] = self", "response": "Initializes the application with the extension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a JSON response containing the serialized data.", "response": "def jsonify(self, obj, many=sentinel, *args, **kwargs):\n        \"\"\"Return a JSON response containing the serialized data.\n\n\n        :param obj: Object to serialize.\n        :param bool many: Whether `obj` should be serialized as an instance\n            or as a collection. If unset, defaults to the value of the\n            `many` attribute on this Schema.\n        :param kwargs: Additional keyword arguments passed to `flask.jsonify`.\n\n        .. versionchanged:: 0.6.0\n            Takes the same arguments as `marshmallow.Schema.dump`. Additional\n            keyword arguments are passed to `flask.jsonify`.\n\n        .. versionchanged:: 0.6.3\n            The `many` argument for this method defaults to the value of\n            the `many` attribute on the Schema. Previously, the `many`\n            argument of this method defaulted to False, regardless of the\n            value of `Schema.many`.\n        \"\"\"\n        if many is sentinel:\n            many = self.many\n        if _MARSHMALLOW_VERSION_INFO[0] >= 3:\n            data = self.dump(obj, many=many)\n        else:\n            data = self.dump(obj, many=many).data\n        return flask.jsonify(data, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _hjoin_multiline(join_char, strings):\n    cstrings = [string.split(\"\\n\") for string in strings]\n    max_num_lines = max(len(item) for item in cstrings)\n    pp = []\n    for k in range(max_num_lines):\n        p = [cstring[k] for cstring in cstrings]\n        pp.append(join_char + join_char.join(p) + join_char)\n\n    return \"\\n\".join([p.rstrip() for p in pp])", "response": "Horizontal join of multiline strings\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chunks(raw):\n    for i in range(0, len(raw), EVENT_SIZE):\n        yield struct.unpack(EVENT_FORMAT, raw[i:i+EVENT_SIZE])", "response": "Yield successive EVENT_SIZE sized chunks from raw."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts time into C style timeval.", "response": "def convert_timeval(seconds_since_epoch):\n    \"\"\"Convert time into C style timeval.\"\"\"\n    frac, whole = math.modf(seconds_since_epoch)\n    microseconds = math.floor(frac * 1000000)\n    seconds = math.floor(whole)\n    return seconds, microseconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsingles subprocess for reading mouse events on Mac using newer Quartz.", "response": "def quartz_mouse_process(pipe):\n    \"\"\"Single subprocess for reading mouse events on Mac using newer Quartz.\"\"\"\n    # Quartz only on the mac, so don't warn about Quartz\n    # pylint: disable=import-error\n    import Quartz\n    # pylint: disable=no-member\n\n    class QuartzMouseListener(QuartzMouseBaseListener):\n        \"\"\"Loosely emulate Evdev mouse behaviour on the Macs.\n        Listen for key events then buffer them in a pipe.\n        \"\"\"\n        def install_handle_input(self):\n            \"\"\"Constants below listed at:\n            https://developer.apple.com/documentation/coregraphics/\n            cgeventtype?language=objc#topics\n            \"\"\"\n            # Keep Mac Names to make it easy to find the documentation\n            # pylint: disable=invalid-name\n\n            NSMachPort = Quartz.CGEventTapCreate(\n                Quartz.kCGSessionEventTap,\n                Quartz.kCGHeadInsertEventTap,\n                Quartz.kCGEventTapOptionDefault,\n                Quartz.CGEventMaskBit(Quartz.kCGEventLeftMouseDown) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventLeftMouseUp) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventRightMouseDown) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventRightMouseUp) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventMouseMoved) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventLeftMouseDragged) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventRightMouseDragged) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventScrollWheel) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventTabletPointer) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventTabletProximity) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventOtherMouseDown) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventOtherMouseUp) |\n                Quartz.CGEventMaskBit(Quartz.kCGEventOtherMouseDragged),\n                self.handle_input,\n                None)\n\n            CFRunLoopSourceRef = Quartz.CFMachPortCreateRunLoopSource(\n                None,\n                NSMachPort,\n                0)\n            CFRunLoopRef = Quartz.CFRunLoopGetCurrent()\n            Quartz.CFRunLoopAddSource(\n                CFRunLoopRef,\n                CFRunLoopSourceRef,\n                Quartz.kCFRunLoopDefaultMode)\n            Quartz.CGEventTapEnable(\n                NSMachPort,\n                True)\n\n        def listen(self):\n            \"\"\"Listen for quartz events.\"\"\"\n            while self.active:\n                Quartz.CFRunLoopRunInMode(\n                    Quartz.kCFRunLoopDefaultMode, 5, False)\n\n        def uninstall_handle_input(self):\n            self.active = False\n\n        def _get_mouse_button_number(self, event):\n            \"\"\"Get the mouse button number from an event.\"\"\"\n            return Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGMouseEventButtonNumber)\n\n        def _get_click_state(self, event):\n            \"\"\"The click state from an event.\"\"\"\n            return Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGMouseEventClickState)\n\n        def _get_scroll(self, event):\n            \"\"\"The scroll values from an event.\"\"\"\n            scroll_y = Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGScrollWheelEventDeltaAxis1)\n            scroll_x = Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGScrollWheelEventDeltaAxis2)\n            return scroll_x, scroll_y\n\n        def _get_absolute(self, event):\n            \"\"\"Get abolute cursor location.\"\"\"\n            return Quartz.CGEventGetLocation(event)\n\n        def _get_relative(self, event):\n            \"\"\"Get the relative mouse movement.\"\"\"\n            delta_x = Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGMouseEventDeltaX)\n            delta_y = Quartz.CGEventGetIntegerValueField(\n                event, Quartz.kCGMouseEventDeltaY)\n            return delta_x, delta_y\n\n    mouse = QuartzMouseListener(pipe)\n    mouse.listen()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsingle subprocess for reading mouse events on Mac using older AppKit.", "response": "def appkit_mouse_process(pipe):\n    \"\"\"Single subprocess for reading mouse events on Mac using older AppKit.\"\"\"\n    # pylint: disable=import-error,too-many-locals\n\n    # Note Objective C does not support a Unix style fork.\n    # So these imports have to be inside the child subprocess since\n    # otherwise the child process cannot use them.\n\n    # pylint: disable=no-member, no-name-in-module\n    from Foundation import NSObject\n    from AppKit import NSApplication, NSApp\n    from Cocoa import (NSEvent, NSLeftMouseDownMask,\n                       NSLeftMouseUpMask, NSRightMouseDownMask,\n                       NSRightMouseUpMask, NSMouseMovedMask,\n                       NSLeftMouseDraggedMask,\n                       NSRightMouseDraggedMask, NSMouseEnteredMask,\n                       NSMouseExitedMask, NSScrollWheelMask,\n                       NSOtherMouseDownMask, NSOtherMouseUpMask)\n    from PyObjCTools import AppHelper\n    import objc\n\n    class MacMouseSetup(NSObject):\n        \"\"\"Setup the handler.\"\"\"\n        @objc.python_method\n        def init_with_handler(self, handler):\n            \"\"\"\n            Init method that receives the write end of the pipe.\n            \"\"\"\n            # ALWAYS call the super's designated initializer.\n            # Also, make sure to re-bind \"self\" just in case it\n            # returns something else!\n            # pylint: disable=self-cls-assignment\n            self = super(MacMouseSetup, self).init()\n            self.handler = handler\n            # Unlike Python's __init__, initializers MUST return self,\n            # because they are allowed to return any object!\n            return self\n\n        # pylint: disable=invalid-name, unused-argument\n        def applicationDidFinishLaunching_(self, notification):\n            \"\"\"Bind the listen method as the handler for mouse events.\"\"\"\n\n            mask = (NSLeftMouseDownMask | NSLeftMouseUpMask |\n                    NSRightMouseDownMask | NSRightMouseUpMask |\n                    NSMouseMovedMask | NSLeftMouseDraggedMask |\n                    NSRightMouseDraggedMask | NSScrollWheelMask |\n                    NSMouseEnteredMask | NSMouseExitedMask |\n                    NSOtherMouseDownMask | NSOtherMouseUpMask)\n            NSEvent.addGlobalMonitorForEventsMatchingMask_handler_(\n                mask, self.handler)\n\n    class MacMouseListener(AppKitMouseBaseListener):\n        \"\"\"Loosely emulate Evdev mouse behaviour on the Macs.\n        Listen for key events then buffer them in a pipe.\n        \"\"\"\n        def install_handle_input(self):\n            \"\"\"Install the hook.\"\"\"\n            self.app = NSApplication.sharedApplication()\n            # pylint: disable=no-member\n            delegate = MacMouseSetup.alloc().init_with_handler(\n                self.handle_input)\n            NSApp().setDelegate_(delegate)\n            AppHelper.runEventLoop()\n\n        def __del__(self):\n            \"\"\"Stop the listener on deletion.\"\"\"\n            AppHelper.stopEventLoop()\n\n    # pylint: disable=unused-variable\n    mouse = MacMouseListener(pipe, events=[])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsingles subprocesses for reading keyboard on Mac.", "response": "def mac_keyboard_process(pipe):\n    \"\"\"Single subprocesses for reading keyboard on Mac.\"\"\"\n    # pylint: disable=import-error,too-many-locals\n    # Note Objective C does not support a Unix style fork.\n    # So these imports have to be inside the child subprocess since\n    # otherwise the child process cannot use them.\n\n    # pylint: disable=no-member, no-name-in-module\n    from AppKit import NSApplication, NSApp\n    from Foundation import NSObject\n    from Cocoa import (NSEvent, NSKeyDownMask, NSKeyUpMask,\n                       NSFlagsChangedMask)\n    from PyObjCTools import AppHelper\n    import objc\n\n    class MacKeyboardSetup(NSObject):\n        \"\"\"Setup the handler.\"\"\"\n\n        @objc.python_method\n        def init_with_handler(self, handler):\n            \"\"\"\n            Init method that receives the write end of the pipe.\n            \"\"\"\n            # ALWAYS call the super's designated initializer.\n            # Also, make sure to re-bind \"self\" just in case it\n            # returns something else!\n\n            # pylint: disable=self-cls-assignment\n            self = super(MacKeyboardSetup, self).init()\n\n            self.handler = handler\n\n            # Unlike Python's __init__, initializers MUST return self,\n            # because they are allowed to return any object!\n            return self\n\n        # pylint: disable=invalid-name, unused-argument\n        def applicationDidFinishLaunching_(self, notification):\n            \"\"\"Bind the handler to listen to keyboard events.\"\"\"\n            mask = NSKeyDownMask | NSKeyUpMask | NSFlagsChangedMask\n            NSEvent.addGlobalMonitorForEventsMatchingMask_handler_(\n                mask, self.handler)\n\n    class MacKeyboardListener(AppKitKeyboardListener):\n        \"\"\"Loosely emulate Evdev keyboard behaviour on the Mac.\n        Listen for key events then buffer them in a pipe.\n        \"\"\"\n        def install_handle_input(self):\n            \"\"\"Install the hook.\"\"\"\n            self.app = NSApplication.sharedApplication()\n            # pylint: disable=no-member\n            delegate = MacKeyboardSetup.alloc().init_with_handler(\n                self.handle_input)\n            NSApp().setDelegate_(delegate)\n            AppHelper.runEventLoop()\n\n        def __del__(self):\n            \"\"\"Stop the listener on deletion.\"\"\"\n            AppHelper.stopEventLoop()\n\n    # pylint: disable=unused-variable\n    keyboard = MacKeyboardListener(pipe)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops vibration aka force feedback aka rumble on Windows after duration miliseconds.", "response": "def delay_and_stop(duration, dll, device_number):\n    \"\"\"Stop vibration aka force feedback aka rumble on\n    Windows after duration miliseconds.\"\"\"\n    xinput = getattr(ctypes.windll, dll)\n    time.sleep(duration/1000)\n    xinput_set_state = xinput.XInputSetState\n    xinput_set_state.argtypes = [\n        ctypes.c_uint, ctypes.POINTER(XinputVibration)]\n    xinput_set_state.restype = ctypes.c_uint\n    vibration = XinputVibration(0, 0)\n    xinput_set_state(device_number, ctypes.byref(vibration))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an evdev style structure.", "response": "def create_event_object(self,\n                            event_type,\n                            code,\n                            value,\n                            timeval=None):\n        \"\"\"Create an evdev style structure.\"\"\"\n        if not timeval:\n            self.update_timeval()\n            timeval = self.timeval\n        try:\n            event_code = self.type_codes[event_type]\n        except KeyError:\n            raise UnknownEventType(\n                \"We don't know what kind of event a %s is.\" % event_type)\n\n        event = struct.pack(EVENT_FORMAT,\n                            timeval[0],\n                            timeval[1],\n                            event_code,\n                            code,\n                            value)\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emulate_wheel(self, data, direction, timeval):\n        if direction == 'x':\n            code = 0x06\n        elif direction == 'z':\n            # Not enitely sure if this exists\n            code = 0x07\n        else:\n            code = 0x08\n\n        if WIN:\n            data = data // 120\n\n        return self.create_event_object(\n            \"Relative\",\n            code,\n            data,\n            timeval)", "response": "Emulate relative values for the mouse wheel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef emulate_rel(self, key_code, value, timeval):\n        return self.create_event_object(\n            \"Relative\",\n            key_code,\n            value,\n            timeval)", "response": "Emulate the relative changes of the mouse cursor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef emulate_press(self, key_code, scan_code, value, timeval):\n        scan_event = self.create_event_object(\n            \"Misc\",\n            0x04,\n            scan_code,\n            timeval)\n        key_event = self.create_event_object(\n            \"Key\",\n            key_code,\n            value,\n            timeval)\n        return scan_event, key_event", "response": "Emulate a button press."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemulate a repeat event.", "response": "def emulate_repeat(self, value, timeval):\n        \"\"\"The repeat press of a key/mouse button, e.g. double click.\"\"\"\n        repeat_event = self.create_event_object(\n            \"Repeat\",\n            2,\n            value,\n            timeval)\n        return repeat_event"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emulate_abs(self, x_val, y_val, timeval):\n        x_event = self.create_event_object(\n            \"Absolute\",\n            0x00,\n            x_val,\n            timeval)\n        y_event = self.create_event_object(\n            \"Absolute\",\n            0x01,\n            y_val,\n            timeval)\n        return x_event, y_event", "response": "Emulate the absolute co -ordinates of the mouse cursor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlisten for keyboard input.", "response": "def listen():\n        \"\"\"Listen for keyboard input.\"\"\"\n        msg = MSG()\n        ctypes.windll.user32.GetMessageA(ctypes.byref(msg), 0, 0, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fptr(self):\n        cmpfunc = ctypes.CFUNCTYPE(ctypes.c_int,\n                                   WPARAM,\n                                   LPARAM,\n                                   ctypes.POINTER(KBDLLHookStruct))\n        return cmpfunc(self.handle_input)", "response": "Get the function pointer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the key input.", "response": "def handle_input(self, ncode, wparam, lparam):\n        \"\"\"Process the key input.\"\"\"\n        value = WIN_KEYBOARD_CODES[wparam]\n        scan_code = lparam.contents.scan_code\n        vk_code = lparam.contents.vk_code\n        self.update_timeval()\n\n        events = []\n        # Add key event\n        scan_key, key_event = self.emulate_press(\n            vk_code, scan_code, value, self.timeval)\n        events.append(scan_key)\n        events.append(key_event)\n\n        # End with a sync marker\n        events.append(self.sync_marker(self.timeval))\n\n        # We are done\n        self.write_to_pipe(events)\n\n        return ctypes.windll.user32.CallNextHookEx(\n            self.hooked, ncode, wparam, lparam)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_input(self, ncode, wparam, lparam):\n        x_pos = lparam.contents.x_pos\n        y_pos = lparam.contents.y_pos\n        data = lparam.contents.mousedata\n\n        # This is how we can distinguish mouse 1 from mouse 2\n        # extrainfo = lparam.contents.extrainfo\n        # The way windows seems to do it is there is primary mouse\n        # and all other mouses report as mouse 2\n\n        # Also useful later will be to support the flags field\n        # flags = lparam.contents.flags\n        # This shows if the event was from a real device or whether it\n        # was injected somehow via software\n\n        self.emulate_mouse(wparam, x_pos, y_pos, data)\n\n        # Give back control to Windows to wait for and process the\n        # next event\n        return ctypes.windll.user32.CallNextHookEx(\n            self.hooked, ncode, wparam, lparam)", "response": "Process the key input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemulate the mouse events using the data Windows has given us.", "response": "def emulate_mouse(self, key_code, x_val, y_val, data):\n        \"\"\"Emulate the ev codes using the data Windows has given us.\n\n        Note that by default in Windows, to recognise a double click,\n        you just notice two clicks in a row within a reasonablely\n        short time period.\n\n        However, if the application developer sets the application\n        window's class style to CS_DBLCLKS, the operating system will\n        notice the four button events (down, up, down, up), intercept\n        them and then send a single key code instead.\n\n        There are no such special double click codes on other\n        platforms, so not obvious what to do with them. It might be\n        best to just convert them back to four events.\n\n        Currently we do nothing.\n\n        ((0x0203, 'WM_LBUTTONDBLCLK'),\n         (0x0206, 'WM_RBUTTONDBLCLK'),\n         (0x0209, 'WM_MBUTTONDBLCLK'),\n         (0x020D, 'WM_XBUTTONDBLCLK'))\n\n        \"\"\"\n        # Once again ignore Windows' relative time (since system\n        # startup) and use the absolute time (since epoch i.e. 1st Jan\n        # 1970).\n        self.update_timeval()\n\n        events = []\n\n        if key_code == 0x0200:\n            # We have a mouse move alone.\n            # So just pass through to below\n            pass\n        elif key_code == 0x020A:\n            # We have a vertical mouse wheel turn\n            events.append(self.emulate_wheel(data, 'y', self.timeval))\n        elif key_code == 0x020E:\n            # We have a horizontal mouse wheel turn\n            # https://msdn.microsoft.com/en-us/library/windows/desktop/\n            # ms645614%28v=vs.85%29.aspx\n            events.append(self.emulate_wheel(data, 'x', self.timeval))\n        else:\n            # We have a button press.\n\n            # Distinguish the second extra button\n            if key_code == 0x020B and data == 2:\n                key_code = 0x020B2\n            elif key_code == 0x020C and data == 2:\n                key_code = 0x020C2\n\n            # Get the mouse codes\n            code, value, scan_code = self.mouse_codes[key_code]\n            # Add in the press events\n            scan_event, key_event = self.emulate_press(\n                code, scan_code, value, self.timeval)\n            events.append(scan_event)\n            events.append(key_event)\n\n        # Add in the absolute position of the mouse cursor\n        x_event, y_event = self.emulate_abs(x_val, y_val, self.timeval)\n        events.append(x_event)\n        events.append(y_event)\n\n        # End with a sync marker\n        events.append(self.sync_marker(self.timeval))\n\n        # We are done\n        self.write_to_pipe(events)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_button(self, event, event_type):\n        # 0 for left\n        # 1 for right\n        # 2 for middle/center\n        # 3 for side\n        mouse_button_number = self._get_mouse_button_number(event)\n\n        # Identify buttons 3,4,5\n        if event_type in (25, 26):\n            event_type = event_type + (mouse_button_number * 0.1)\n\n        # Add buttons to events\n        event_type_string, event_code, value, scan = self.codes[event_type]\n        if event_type_string == \"Key\":\n            scan_event, key_event = self.emulate_press(\n                event_code, scan, value, self.timeval)\n            self.events.append(scan_event)\n            self.events.append(key_event)\n\n        # doubleclick/n-click of button\n        click_state = self._get_click_state(event)\n\n        repeat = self.emulate_repeat(click_state, self.timeval)\n        self.events.append(repeat)", "response": "Convert the button information from quartz into evdev format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_scrollwheel(self, event):\n        # relative Scrollwheel\n        scroll_x, scroll_y = self._get_scroll(event)\n\n        if scroll_x:\n            self.events.append(\n                self.emulate_wheel(scroll_x, 'x', self.timeval))\n\n        if scroll_y:\n            self.events.append(\n                self.emulate_wheel(scroll_y, 'y', self.timeval))", "response": "Handle the scrollwheel event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_absolute(self, event):\n        (x_val, y_val) = self._get_absolute(event)\n        x_event, y_event = self.emulate_abs(\n            int(x_val),\n            int(y_val),\n            self.timeval)\n        self.events.append(x_event)\n        self.events.append(y_event)", "response": "Absolute mouse position on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_input(self, proxy, event_type, event, refcon):\n        self.update_timeval()\n        self.events = []\n\n        if event_type in (1, 2, 3, 4, 25, 26, 27):\n            self.handle_button(event, event_type)\n\n        if event_type == 22:\n            self.handle_scrollwheel(event)\n\n        # Add in the absolute position of the mouse cursor\n        self.handle_absolute(event)\n\n        # Add in the relative position of the mouse cursor\n        self.handle_relative(event)\n\n        # End with a sync marker\n        self.events.append(self.sync_marker(self.timeval))\n\n        # We are done\n        self.write_to_pipe(self.events)", "response": "Handle an input event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the changes from the appkit event.", "response": "def _get_deltas(event):\n        \"\"\"Get the changes from the appkit event.\"\"\"\n        delta_x = round(event.deltaX())\n        delta_y = round(event.deltaY())\n        delta_z = round(event.deltaZ())\n        return delta_x, delta_y, delta_z"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles mouse click events.", "response": "def handle_button(self, event, event_type):\n        \"\"\"Handle mouse click.\"\"\"\n        mouse_button_number = self._get_mouse_button_number(event)\n        # Identify buttons 3,4,5\n        if event_type in (25, 26):\n            event_type = event_type + (mouse_button_number * 0.1)\n        # Add buttons to events\n        event_type_name, event_code, value, scan = self.codes[event_type]\n        if event_type_name == \"Key\":\n            scan_event, key_event = self.emulate_press(\n                event_code, scan, value, self.timeval)\n            self.events.append(scan_event)\n            self.events.append(key_event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_absolute(self, event):\n        point = self._get_absolute(event)\n        x_pos = round(point.x)\n        y_pos = round(point.y)\n        x_event, y_event = self.emulate_abs(x_pos, y_pos, self.timeval)\n        self.events.append(x_event)\n        self.events.append(y_event)", "response": "Emulate absolute mouse position on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake endev from appkit scroll wheel event.", "response": "def handle_scrollwheel(self, event):\n        \"\"\"Make endev from appkit scroll wheel event.\"\"\"\n        delta_x, delta_y, delta_z = self._get_deltas(event)\n        if delta_x:\n            self.events.append(\n                self.emulate_wheel(delta_x, 'x', self.timeval))\n        if delta_y:\n            self.events.append(\n                self.emulate_wheel(delta_y, 'y', self.timeval))\n        if delta_z:\n            self.events.append(\n                self.emulate_wheel(delta_z, 'z', self.timeval))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_relative(self, event):\n        delta_x, delta_y, delta_z = self._get_deltas(event)\n        if delta_x:\n            self.events.append(\n                self.emulate_rel(0x00,\n                                 delta_x,\n                                 self.timeval))\n        if delta_y:\n            self.events.append(\n                self.emulate_rel(0x01,\n                                 delta_y,\n                                 self.timeval))\n        if delta_z:\n            self.events.append(\n                self.emulate_rel(0x02,\n                                 delta_z,\n                                 self.timeval))", "response": "Handle relative mouse events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the mouse event.", "response": "def handle_input(self, event):\n        \"\"\"Process the mouse event.\"\"\"\n        self.update_timeval()\n        self.events = []\n        code = self._get_event_type(event)\n\n        # Deal with buttons\n        self.handle_button(event, code)\n\n        # Mouse wheel\n        if code == 22:\n            self.handle_scrollwheel(event)\n        # Other relative mouse movements\n        else:\n            self.handle_relative(event)\n\n        # Add in the absolute position of the mouse cursor\n        self.handle_absolute(event)\n\n        # End with a sync marker\n        self.events.append(self.sync_marker(self.timeval))\n\n        # We are done\n        self.write_to_pipe(self.events)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the key value.", "response": "def _get_key_value(self, event, event_type):\n        \"\"\"Get the key value.\"\"\"\n        if event_type == 10:\n            value = 1\n        elif event_type == 11:\n            value = 0\n        elif event_type == 12:\n            value = self._get_flag_value(event)\n        else:\n            value = -1\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess they keyboard input.", "response": "def handle_input(self, event):\n        \"\"\"Process they keyboard input.\"\"\"\n        self.update_timeval()\n        self.events = []\n        code = self._get_event_key_code(event)\n\n        if code in self.codes:\n            new_code = self.codes[code]\n        else:\n            new_code = 0\n        event_type = self._get_event_type(event)\n        value = self._get_key_value(event, event_type)\n        scan_event, key_event = self.emulate_press(\n            new_code, code, value, self.timeval)\n\n        self.events.append(scan_event)\n        self.events.append(key_event)\n        # End with a sync marker\n        self.events.append(self.sync_marker(self.timeval))\n        # We are done\n        self.write_to_pipe(self.events)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets useful infomation from the device path.", "response": "def _get_path_infomation(self):\n        \"\"\"Get useful infomation from the device path.\"\"\"\n        long_identifier = self._device_path.split('/')[4]\n        protocol, remainder = long_identifier.split('-', 1)\n        identifier, _, device_type = remainder.rsplit('-', 2)\n        return (protocol, identifier, device_type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_total_read_size(self):\n        if self.read_size:\n            read_size = EVENT_SIZE * self.read_size\n        else:\n            read_size = EVENT_SIZE\n        return read_size", "response": "How much event data to process at once."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a friendly Python object from an evdev style event.", "response": "def _make_event(self, tv_sec, tv_usec, ev_type, code, value):\n        \"\"\"Create a friendly Python object from an evdev style event.\"\"\"\n        event_type = self.manager.get_event_type(ev_type)\n        eventinfo = {\n            \"ev_type\": event_type,\n            \"state\": value,\n            \"timestamp\": tv_sec + (tv_usec / 1000000),\n            \"code\": self.manager.get_event_string(event_type, code)\n        }\n\n        return InputEvent(self, eventinfo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pipe(self):\n        if self._evdev:\n            return None\n\n        if not self.__pipe:\n            target_function = self._get_target_function()\n            if not target_function:\n                return None\n\n            self.__pipe, child_conn = Pipe(duplex=False)\n            self._listener = Process(target=target_function,\n                                     args=(child_conn,), daemon=True)\n            self._listener.start()\n        return self.__pipe", "response": "On Windows we use a pipe to emulate a Linux style character\n            buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget data from the character device.", "response": "def _get_data(self, read_size):\n        \"\"\"Get data from the character device.\"\"\"\n        if NIX:\n            return super(Keyboard, self)._get_data(read_size)\n        return self._pipe.recv_bytes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets data from the character device.", "response": "def _get_data(self, read_size):\n        \"\"\"Get data from the character device.\"\"\"\n        if NIX:\n            return super(Mouse, self)._get_data(read_size)\n        return self._pipe.recv_bytes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the number of the joystick.", "response": "def _number_xpad(self):\n        \"\"\"Get the number of the joystick.\"\"\"\n        js_path = self._device_path.replace('-event', '')\n        js_chardev = os.path.realpath(js_path)\n        try:\n            number_text = js_chardev.split('js')[1]\n        except IndexError:\n            return\n        try:\n            number = int(number_text)\n        except ValueError:\n            return\n        self.__device_number = number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an evdev style object.", "response": "def create_event_object(self,\n                            event_type,\n                            code,\n                            value,\n                            timeval=None):\n        \"\"\"Create an evdev style object.\"\"\"\n        if not timeval:\n            timeval = self.__get_timeval()\n        try:\n            event_code = self.manager.codes['type_codes'][event_type]\n        except KeyError:\n            raise UnknownEventType(\n                \"We don't know what kind of event a %s is.\" % event_type)\n        event = struct.pack(EVENT_FORMAT,\n                            timeval[0],\n                            timeval[1],\n                            event_code,\n                            code,\n                            value)\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __write_to_character_device(self, event_list, timeval=None):\n        # Remember the position of the stream\n        pos = self._character_device.tell()\n        # Go to the end of the stream\n        self._character_device.seek(0, 2)\n        # Write the new data to the end\n        for event in event_list:\n            self._character_device.write(event)\n        # Add a sync marker\n        sync = self.create_event_object(\"Sync\", 0, 0, timeval)\n        self._character_device.write(sync)\n        # Put the stream back to its original position\n        self._character_device.seek(pos)", "response": "Emulate the Linux character device on other platforms such as USB and Windows."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __handle_changed_state(self, state):\n        timeval = self.__get_timeval()\n        events = self.__get_button_events(state, timeval)\n        events.extend(self.__get_axis_events(state, timeval))\n        if events:\n            self.__write_to_character_device(events, timeval)", "response": "This method is called when the state of the state machine has changed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __map_button(self, button):\n        _, start_code, start_value = button\n        value = start_value\n        ev_type = \"Key\"\n        code = self.manager.codes['xpad'][start_code]\n        if 1 <= start_code <= 4:\n            ev_type = \"Absolute\"\n        if start_code == 1 and start_value == 1:\n            value = -1\n        elif start_code == 3 and start_value == 1:\n            value = -1\n        return code, value, ev_type", "response": "Get the linux xpad code from the Windows xinput code."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __map_axis(self, axis):\n        start_code, start_value = axis\n        value = start_value\n        code = self.manager.codes['xpad'][start_code]\n        return code, value", "response": "Get the linux xpad code from the Windows xinput code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the button events from xinput.", "response": "def __get_button_events(self, state, timeval=None):\n        \"\"\"Get the button events from xinput.\"\"\"\n        changed_buttons = self.__detect_button_events(state)\n        events = self.__emulate_buttons(changed_buttons, timeval)\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the stick events from xinput.", "response": "def __get_axis_events(self, state, timeval=None):\n        \"\"\"Get the stick events from xinput.\"\"\"\n        axis_changes = self.__detect_axis_events(state)\n        events = self.__emulate_axis(axis_changes, timeval)\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes the events use the Linux style format.", "response": "def __emulate_axis(self, axis_changes, timeval=None):\n        \"\"\"Make the axis events use the Linux style format.\"\"\"\n        events = []\n        for axis in axis_changes:\n            code, value = self.__map_axis(axis)\n            event = self.create_event_object(\n                \"Absolute\",\n                code,\n                value,\n                timeval=timeval)\n            events.append(event)\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __emulate_buttons(self, changed_buttons, timeval=None):\n        events = []\n        for button in changed_buttons:\n            code, value, ev_type = self.__map_button(button)\n            event = self.create_event_object(\n                ev_type,\n                code,\n                value,\n                timeval=timeval)\n            events.append(event)\n        return events", "response": "Make the button events use the Linux style format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets bit values as a list for a given number.", "response": "def __get_bit_values(self, number, size=32):\n        \"\"\"Get bit values as a list for a given number\n\n        >>> get_bit_values(1) == [0]*31 + [1]\n        True\n\n        >>> get_bit_values(0xDEADBEEF)\n        [1L, 1L, 0L, 1L, 1L, 1L, 1L,\n        0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L,\n        1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L]\n\n        You may override the default word size of 32-bits to match your actual\n        application.\n        >>> get_bit_values(0x3, 2)\n        [1L, 1L]\n\n        >>> get_bit_values(0x3, 4)\n        [0L, 0L, 1L, 1L]\n\n        \"\"\"\n        res = list(self.__gen_bit_values(number))\n        res.reverse()\n        # 0-pad the most significant bit\n        res = [0] * (size - len(res)) + res\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the state of the gamepad.", "response": "def __read_device(self):\n        \"\"\"Read the state of the gamepad.\"\"\"\n        state = XinputState()\n        res = self.manager.xinput.XInputGetState(\n            self.__device_number, ctypes.byref(state))\n        if res == XINPUT_ERROR_SUCCESS:\n            return state\n        if res != XINPUT_ERROR_DEVICE_NOT_CONNECTED:\n            raise RuntimeError(\n                \"Unknown error %d attempting to get state of device %d\" % (\n                    res, self.__device_number))\n        # else (device is not connected)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the vibration on the Windows.", "response": "def _start_vibration_win(self, left_motor, right_motor):\n        \"\"\"Start the vibration, which will run until stopped.\"\"\"\n        xinput_set_state = self.manager.xinput.XInputSetState\n        xinput_set_state.argtypes = [\n            ctypes.c_uint, ctypes.POINTER(XinputVibration)]\n        xinput_set_state.restype = ctypes.c_uint\n        vibration = XinputVibration(\n            int(left_motor * 65535), int(right_motor * 65535))\n        xinput_set_state(self.__device_number, ctypes.byref(vibration))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop the vibration on the Windows.", "response": "def _stop_vibration_win(self):\n        \"\"\"Stop the vibration.\"\"\"\n        xinput_set_state = self.manager.xinput.XInputSetState\n        xinput_set_state.argtypes = [\n            ctypes.c_uint, ctypes.POINTER(XinputVibration)]\n        xinput_set_state.restype = ctypes.c_uint\n        stop_vibration = ctypes.byref(XinputVibration(0, 0))\n        xinput_set_state(self.__device_number, stop_vibration)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_vibration_win(self, left_motor, right_motor, duration):\n        self._start_vibration_win(left_motor, right_motor)\n        stop_process = Process(target=delay_and_stop,\n                               args=(duration,\n                                     self.manager.xinput_dll,\n                                     self.__device_number))\n        stop_process.start()", "response": "Control the motors on Windows."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_vibration_code(self, left_motor, right_motor, duration):\n        inner_event = struct.pack(\n            '2h6x2h2x2H28x',\n            0x50,\n            -1,\n            duration,\n            0,\n            int(left_motor * 65535),\n            int(right_motor * 65535))\n        buf_conts = ioctl(self._write_device, 1076905344, inner_event)\n        return int(codecs.encode(buf_conts[1:3], 'hex'), 16)", "response": "This method returns the vibration code from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncontrols the motors on Linux.", "response": "def _set_vibration_nix(self, left_motor, right_motor, duration):\n        \"\"\"Control the motors on Linux.\n        Duration is in miliseconds.\"\"\"\n        code = self.__get_vibration_code(left_motor, right_motor, duration)\n        secs, msecs = convert_timeval(time.time())\n        outer_event = struct.pack(EVENT_FORMAT, secs, msecs, 0x15, code, 1)\n        self._write_device.write(outer_event)\n        self._write_device.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncontrols the speed of both motors seperately or together.", "response": "def set_vibration(self, left_motor, right_motor, duration):\n        \"\"\"Control the speed of both motors seperately or together.\n        left_motor and right_motor arguments require a number between\n        0 (off) and 1 (full).\n        duration is miliseconds, e.g. 1000 for a second.\"\"\"\n        if WIN:\n            self._set_vibration_win(left_motor, right_motor, duration)\n        elif NIX:\n            self._set_vibration_nix(left_motor, right_motor, duration)\n        else:\n            raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef max_brightness(self):\n        status_filename = os.path.join(self.path, 'max_brightness')\n        with open(status_filename) as status_fp:\n            result = status_fp.read()\n        status_text = result.strip()\n        try:\n            status = int(status_text)\n        except ValueError:\n            return status_text\n        return status", "response": "Get the device s maximum brightness level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_event(self, event_type, code, value):\n        secs, msecs = convert_timeval(time.time())\n        data = struct.pack(EVENT_FORMAT,\n                           secs,\n                           msecs,\n                           event_type,\n                           code,\n                           value)\n        self._write_device.write(data)\n        self._write_device.flush()", "response": "Make a new event and send it to the character device."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets up the device path and type code.", "response": "def _post_init(self):\n        \"\"\"Set up the device path and type code.\"\"\"\n        self._led_type_code = self.manager.get_typecode('LED')\n        self.device_path = os.path.realpath(os.path.join(self.path, 'device'))\n        if '::' in self.name:\n            chardev, code_name = self.name.split('::')\n            if code_name in self.manager.codes['LED_type_codes']:\n                self.code = self.manager.codes['LED_type_codes'][code_name]\n            try:\n                event_number = chardev.split('input')[1]\n            except IndexError:\n                print(\"Failed with\", self.name)\n                raise\n            else:\n                self._character_device_path = '/dev/input/event' + event_number\n                self._match_device()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a new event and send it to the character device.", "response": "def _make_event(self, value):  # pylint: disable=arguments-differ\n        \"\"\"Make a new event and send it to the character device.\"\"\"\n        super(SystemLED, self)._make_event(\n            self._led_type_code,\n            self.code,\n            value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _match_device(self):\n        for device in self.manager.all_devices:\n            if (device.get_char_device_path() ==\n                    self._character_device_path):\n                self.device = device\n                device.leds.append(self)\n                break", "response": "If the LED is connected to an input device associate the objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _post_init(self):\n        if WIN:\n            self._find_devices_win()\n        elif MAC:\n            self._find_devices_mac()\n        else:\n            self._find_devices()\n        self._update_all_devices()\n        if NIX:\n            self._find_leds()", "response": "Call the find devices method for the relevant platform."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_all_devices(self):\n        self.all_devices = []\n        self.all_devices.extend(self.keyboards)\n        self.all_devices.extend(self.mice)\n        self.all_devices.extend(self.gamepads)\n        self.all_devices.extend(self.other_devices)", "response": "Update the all_devices list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse each device and add it to the relevant list.", "response": "def _parse_device_path(self, device_path, char_path_override=None):\n        \"\"\"Parse each device and add to the approriate list.\"\"\"\n\n        # 1. Make sure that we can parse the device path.\n        try:\n            device_type = device_path.rsplit('-', 1)[1]\n        except IndexError:\n            warn(\"The following device path was skipped as it could \"\n                 \"not be parsed: %s\" % device_path, RuntimeWarning)\n            return\n\n        # 2. Make sure each device is only added once.\n        realpath = os.path.realpath(device_path)\n        if realpath in self._raw:\n            return\n        self._raw.append(realpath)\n\n        # 3. All seems good, append the device to the relevant list.\n        if device_type == 'kbd':\n            self.keyboards.append(Keyboard(self, device_path,\n                                           char_path_override))\n        elif device_type == 'mouse':\n            self.mice.append(Mouse(self, device_path,\n                                   char_path_override))\n        elif device_type == 'joystick':\n            self.gamepads.append(GamePad(self,\n                                         device_path,\n                                         char_path_override))\n        else:\n            self.other_devices.append(OtherDevice(self,\n                                                  device_path,\n                                                  char_path_override))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_xinput(self):\n        for dll in XINPUT_DLL_NAMES:\n            try:\n                self.xinput = getattr(ctypes.windll, dll)\n            except OSError:\n                pass\n            else:\n                # We found an xinput driver\n                self.xinput_dll = dll\n                break\n        else:\n            # We didn't find an xinput library\n            warn(\n                \"No xinput driver dll found, gamepads not supported.\",\n                RuntimeWarning)", "response": "Find most recent xinput library."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_devices_win(self):\n        self._find_xinput()\n        self._detect_gamepads()\n        self._count_devices()\n        if self._raw_device_counts['keyboards'] > 0:\n            self.keyboards.append(Keyboard(\n                self,\n                \"/dev/input/by-id/usb-A_Nice_Keyboard-event-kbd\"))\n\n        if self._raw_device_counts['mice'] > 0:\n            self.mice.append(Mouse(\n                self,\n                \"/dev/input/by-id/usb-A_Nice_Mouse_called_Arthur-event-mouse\"))", "response": "Find devices on Windows."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind devices on Mac.", "response": "def _find_devices_mac(self):\n        \"\"\"Find devices on Mac.\"\"\"\n        self.keyboards.append(Keyboard(self))\n        self.mice.append(MightyMouse(self))\n        self.mice.append(Mouse(self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsees what Windows s GetRawInputDeviceList wants to tell us.", "response": "def _count_devices(self):\n        \"\"\"See what Windows' GetRawInputDeviceList wants to tell us.\n\n        For now, we are just seeing if there is at least one keyboard\n        and/or mouse attached.\n\n        GetRawInputDeviceList could be used to help distinguish between\n        different keyboards and mice on the system in the way Linux\n        can. However, Roma uno die non est condita.\n\n        \"\"\"\n        number_of_devices = ctypes.c_uint()\n\n        if ctypes.windll.user32.GetRawInputDeviceList(\n                ctypes.POINTER(ctypes.c_int)(),\n                ctypes.byref(number_of_devices),\n                ctypes.sizeof(RawInputDeviceList)) == -1:\n            warn(\"Call to GetRawInputDeviceList was unsuccessful.\"\n                 \"We have no idea if a mouse or keyboard is attached.\",\n                 RuntimeWarning)\n            return\n\n        devices_found = (RawInputDeviceList * number_of_devices.value)()\n\n        if ctypes.windll.user32.GetRawInputDeviceList(\n                devices_found,\n                ctypes.byref(number_of_devices),\n                ctypes.sizeof(RawInputDeviceList)) == -1:\n            warn(\"Call to GetRawInputDeviceList was unsuccessful.\"\n                 \"We have no idea if a mouse or keyboard is attached.\",\n                 RuntimeWarning)\n            return\n\n        for device in devices_found:\n            if device.dwType == 0:\n                self._raw_device_counts['mice'] += 1\n            elif device.dwType == 1:\n                self._raw_device_counts['keyboards'] += 1\n            elif device.dwType == 2:\n                self._raw_device_counts['otherhid'] += 1\n            else:\n                self._raw_device_counts['unknown'] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_by(self, key):\n        by_path = glob.glob('/dev/input/by-{key}/*-event-*'.format(key=key))\n        for device_path in by_path:\n            self._parse_device_path(device_path)", "response": "Find devices by key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_special(self):\n        charnames = self._get_char_names()\n        for eventdir in glob.glob('/sys/class/input/event*'):\n            char_name = os.path.split(eventdir)[1]\n            if char_name in charnames:\n                continue\n            name_file = os.path.join(eventdir, 'device', 'name')\n            with open(name_file) as name_file:\n                device_name = name_file.read().strip()\n                if device_name in self.codes['specials']:\n                    self._parse_device_path(\n                        self.codes['specials'][device_name],\n                        os.path.join('/dev/input', char_name))", "response": "Look for special devices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_event_string(self, evtype, code):\n        if WIN and evtype == 'Key':\n            # If we can map the code to a common one then do it\n            try:\n                code = self.codes['wincodes'][code]\n            except KeyError:\n                pass\n        try:\n            return self.codes[evtype][code]\n        except KeyError:\n            raise UnknownEventCode(\"We don't know this event.\", evtype, code)", "response": "Get the string name of the event."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows an image on the display.", "response": "def set_display(self, index=None):\n        \"\"\"Show an image on the display.\"\"\"\n        # pylint: disable=no-member\n        if index:\n            image = self.microbit.Image.STD_IMAGES[index]\n        else:\n            image = self.default_image\n        self.microbit.display.show(image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_rumble(self):\n        self.left_rumble = self._get_ready_to('99500')\n        self.right_rumble = self._get_ready_to('00599')\n        self.double_rumble = self._get_ready_to('99599')", "response": "Setup the three animations which simulate a rumble."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_ready_to(self, rumble):\n        # pylint: disable=no-member\n        return [self.microbit.Image(':'.join(\n            [rumble if char == '1' else '00500'\n             for char in code])) for code in SPIN_UP_MOTOR]", "response": "Get the list of images ready to be used in the image queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsimulates the motors running at full.", "response": "def _full_speed_rumble(self, images, duration):\n        \"\"\"Simulate the motors running at full.\"\"\"\n        while duration > 0:\n            self.microbit.display.show(images[0])  # pylint: disable=no-member\n            time.sleep(0.04)\n            self.microbit.display.show(images[1])  # pylint: disable=no-member\n            time.sleep(0.04)\n            duration -= 0.08"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _spin_up(self, images, duration):\n        total = 0\n        # pylint: disable=no-member\n\n        for image in images:\n            self.microbit.display.show(image)\n            time.sleep(0.05)\n            total += 0.05\n            if total >= duration:\n                return\n        remaining = duration - total\n        self._full_speed_rumble(images[-2:], remaining)\n        self.set_display()", "response": "Simulate the motors getting warmed up."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_vibration(self, left_motor, right_motor, duration):\n        if left_motor and right_motor:\n            return self._spin_up(self.double_rumble, duration/1000)\n        if left_motor:\n            return self._spin_up(self.left_rumble, duration/1000)\n        if right_motor:\n            return self._spin_up(self.right_rumble, duration/1000)\n        return -1", "response": "Control the speed of both motors seperately or together."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding each new event to the event queue.", "response": "def handle_new_events(self, events):\n        \"\"\"Add each new events to the event queue.\"\"\"\n        for event in events:\n            self.events.append(\n                self.create_event_object(\n                    event[0],\n                    event[1],\n                    int(event[2])))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the state as the raw abolute numbers.", "response": "def handle_abs(self):\n        \"\"\"Gets the state as the raw abolute numbers.\"\"\"\n        # pylint: disable=no-member\n        x_raw = self.microbit.accelerometer.get_x()\n        y_raw = self.microbit.accelerometer.get_y()\n        x_abs = ('Absolute', 0x00, x_raw)\n        y_abs = ('Absolute', 0x01, y_raw)\n        return x_abs, y_abs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the state of the virtual dpad.", "response": "def handle_dpad(self):\n        \"\"\"Gets the state of the virtual dpad.\"\"\"\n        # pylint: disable=no-member\n        x_raw = self.microbit.accelerometer.get_x()\n        y_raw = self.microbit.accelerometer.get_y()\n        minus_sens = self.sensitivity * -1\n        if x_raw < minus_sens:\n            x_state = ('Absolute', 0x10, -1)\n        elif x_raw > self.sensitivity:\n            x_state = ('Absolute', 0x10, 1)\n        else:\n            x_state = ('Absolute', 0x10, 0)\n\n        if y_raw < minus_sens:\n            y_state = ('Absolute', 0x11, -1)\n        elif y_raw > self.sensitivity:\n            y_state = ('Absolute', 0x11, 1)\n        else:\n            y_state = ('Absolute', 0x11, 1)\n\n        return x_state, y_state"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntracks differences in the device state.", "response": "def check_state(self):\n        \"\"\"Tracks differences in the device state.\"\"\"\n        if self.dpad:\n            x_state, y_state = self.handle_dpad()\n        else:\n            x_state, y_state = self.handle_abs()\n\n        # pylint: disable=no-member\n        new_state = set((\n            x_state,\n            y_state,\n            ('Key', 0x130, int(self.microbit.button_a.is_pressed())),\n            ('Key', 0x131, int(self.microbit.button_b.is_pressed())),\n            ('Key', 0x13a, int(self.microbit.pin0.is_touched())),\n            ('Key', 0x133, int(self.microbit.pin1.is_touched())),\n            ('Key', 0x134, int(self.microbit.pin2.is_touched())),\n        ))\n        events = new_state - self.state\n        self.state = new_state\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend differences in the device state to the MicroBitPad as events.", "response": "def handle_input(self):\n        \"\"\"Sends differences in the device state to the MicroBitPad\n        as events.\"\"\"\n        difference = self.check_state()\n        if not difference:\n            return\n        self.events = []\n        self.handle_new_events(difference)\n        self.update_timeval()\n        self.events.append(self.sync_marker(self.timeval))\n        self.write_to_pipe(self.events)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints out some event infomation when the mouse is used.", "response": "def main():\n    \"\"\"Just print out some event infomation when the mouse is used.\"\"\"\n    while 1:\n        events = get_mouse()\n        for event in events:\n            print(event.ev_type, event.code, event.state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints out some event infomation when keys are pressed.", "response": "def main():\n    \"\"\"Just print out some event infomation when keys are pressed.\"\"\"\n    while 1:\n        events = get_key()\n        if events:\n            for event in events:\n                print(event.ev_type, event.code, event.state)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    while 1:\n        events = get_gamepad()\n        for event in events:\n            print(event.ev_type, event.code, event.state)", "response": "Print out some event infomation when the gamepad is used."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(self, value):\n        errors = []\n\n        # Make sure the type validates first.\n        valid = self._is_valid(value)\n        if not valid:\n            errors.append(self.fail(value))\n            return errors\n\n        # Then validate all the constraints second.\n        for constraint in self._constraints_inst:\n            error = constraint.is_valid(value)\n            if error:\n                errors.append(error)\n\n        return errors", "response": "Check if value is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_schema(self, schema_dict, validators):\n        schema_flat = util.flatten(schema_dict)\n\n        for key, expression in schema_flat.items():\n            try:\n                schema_flat[key] = syntax.parse(expression, validators)\n            except SyntaxError as e:\n                # Tack on some more context and rethrow.\n                error = str(e) + ' at node \\'%s\\'' % key\n                raise SyntaxError(error)\n        return schema_flat", "response": "Go through a schema and construct validators."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate(self, validator, data, key, position=None, includes=None):\n        errors = []\n\n        if position:\n            position = '%s.%s' % (position, key)\n        else:\n            position = key\n\n        try:  # Pull value out of data. Data can be a map or a list/sequence\n            data_item = util.get_value(data, key)\n        except KeyError:  # Oops, that field didn't exist.\n            if validator.is_optional:  # Optional? Who cares.\n                return errors\n            # SHUT DOWN EVERTYHING\n            errors.append('%s: Required field missing' % position)\n            return errors\n\n        return self._validate_item(validator, data_item, position, includes)", "response": "Run through a schema and a data structure and validate the data structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a single data item against validator.", "response": "def _validate_item(self, validator, data_item, position, includes):\n        \"\"\"\n        Validates a single data item against validator.\n\n        Returns an array of errors.\n        \"\"\"\n        errors = []\n\n        # Optional field with optional value? Who cares.\n        if data_item is None and validator.is_optional and validator.can_be_none:\n            return errors\n\n        errors += self._validate_primitive(validator, data_item, position)\n\n        if errors:\n            return errors\n\n        if isinstance(validator, val.Include):\n            errors += self._validate_include(validator, data_item,\n                                             includes, position)\n\n        elif isinstance(validator, (val.Map, val.List)):\n            errors += self._validate_map_list(validator, data_item,\n                                              includes, position)\n\n        elif isinstance(validator, val.Any):\n            errors += self._validate_any(validator, data_item,\n                                         includes, position)\n\n        return errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the schema_name in the data file folder and returns the first element in the path.", "response": "def _find_data_path_schema(data_path, schema_name):\n    \"\"\" Starts in the data file folder and recursively looks\n    in parents for `schema_name` \"\"\"\n    if not data_path or data_path == '/' or data_path == '.':\n        return None\n    directory = os.path.dirname(data_path)\n    path = glob.glob(os.path.join(directory, schema_name))\n    if not path:\n        return _find_schema(directory, schema_name)\n    return path[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if schema_name is a valid file and returns the path to the file.", "response": "def _find_schema(data_path, schema_name):\n    \"\"\" Checks if `schema_name` is a valid file, if not\n    searches in `data_path` for it. \"\"\"\n\n    path = glob.glob(schema_name)\n    for p in path:\n        if os.path.isfile(p):\n            return p\n\n    return _find_data_path_schema(data_path, schema_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a flattened dictionary from a dictionary of nested dictionaries and lists. keep_iter will treat iterables as valid values while also flattening them.", "response": "def flatten(dic, keep_iter=False, position=None):\n    \"\"\"\n    Returns a flattened dictionary from a dictionary of nested dictionaries and lists.\n    `keep_iter` will treat iterables as valid values, while also flattening them.\n    \"\"\"\n    child = {}\n    if not dic:\n        return {}\n\n    for k, v in get_iter(dic):\n        if isstr(k):\n            k = k.replace('.', '_')\n        if position:\n            item_position = '%s.%s' % (position, k)\n        else:\n            item_position = '%s' % k\n\n        if is_iter(v):\n            child.update(flatten(dic[k], keep_iter, item_position))\n            if keep_iter:\n                child[item_position] = v\n        else:\n            child[item_position] = v\n\n    return child"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_representation(self, obj):\n        value = self.model_field.__get__(obj, None)\n        return smart_text(value, strings_only=True)", "response": "convert value to representation"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating value. Uses document field's ``validate()``", "response": "def run_validators(self, value):\n        \"\"\" validate value.\n\n        Uses document field's ``validate()``\n        \"\"\"\n        try:\n            self.model_field.validate(value)\n        except MongoValidationError as e:\n            raise ValidationError(e.message)\n        super(DocumentField, self).run_validators(value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert data to internal value.", "response": "def to_internal_value(self, data):\n        \"\"\"\n        Dicts of native values <- Dicts of primitive datatypes.\n        \"\"\"\n\n        if html.is_html_input(data):\n            data = html.parse_html_dict(data)\n        if not isinstance(data, dict):\n            self.fail('not_a_dict', input_type=type(data).__name__)\n        if not self.allow_empty and len(data.keys()) == 0:\n            message = self.error_messages['empty']\n            raise ValidationError({\n                api_settings.NON_FIELD_ERRORS_KEY: [message]\n            })\n        return {\n            six.text_type(key): self.child.run_validation(value)\n            for key, value in data.items()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_internal_value(self, data):\n        # for EmbeddedDocumentSerializers create initial data\n        # so that _get_dynamic_data could use them\n        for field in self._writable_fields:\n            if isinstance(field, EmbeddedDocumentSerializer) and field.field_name in data:\n                field.initial_data = data[field.field_name]\n\n        ret = super(DocumentSerializer, self).to_internal_value(data)\n\n        # for EmbeddedDocumentSerializers create _validated_data\n        # so that create()/update() could use them\n        for field in self._writable_fields:\n            if isinstance(field, EmbeddedDocumentSerializer) and field.field_name in ret:\n                field._validated_data = ret[field.field_name]\n\n        return ret", "response": "Returns the value of this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_customization_for_nested_field(self, field_name):\n\n        # This method is supposed to be called after self.get_fields(),\n        # thus it assumes that fields and exclude are mutually exclusive\n        # and at least one of them is set.\n        #\n        # Also, all the sanity checks are left up to nested field's\n        # get_fields() method, so if something is wrong with customization\n        # nested get_fields() will report this.\n\n        fields = getattr(self.Meta, 'fields', None)\n        exclude = getattr(self.Meta, 'exclude', None)\n\n        if fields and fields != ALL_FIELDS and not isinstance(fields, (list, tuple)):\n            raise TypeError(\n                'The `fields` option must be a list or tuple or \"__all__\". '\n                'Got %s.' % type(fields).__name__\n            )\n\n        if exclude and not isinstance(exclude, (list, tuple)):\n            raise TypeError(\n                'The `exclude` option must be a list or tuple. Got %s.' %\n                type(exclude).__name__\n            )\n\n        assert not (fields and exclude), (\n            \"Cannot set both 'fields' and 'exclude' options on \"\n            \"serializer {serializer_class}.\".format(\n                serializer_class=self.__class__.__name__\n            )\n        )\n\n        if fields is None and exclude is None:\n            warnings.warn(\n                \"Creating a ModelSerializer without either the 'fields' \"\n                \"attribute or the 'exclude' attribute is deprecated \"\n                \"since 3.3.0. Add an explicit fields = '__all__' to the \"\n                \"{serializer_class} serializer.\".format(\n                    serializer_class=self.__class__.__name__\n                ),\n                DeprecationWarning\n            )\n            fields = ALL_FIELDS  # assume that fields are ALL_FIELDS\n\n        # TODO: validators\n\n        # get nested_fields or nested_exclude (supposed to be mutually exclusive, assign the other one to None)\n        if fields:\n            if fields == ALL_FIELDS:\n                nested_fields = ALL_FIELDS\n            else:\n                nested_fields = [field[len(field_name + '.'):] for field in fields if field.startswith(field_name + '.')]\n            nested_exclude = None\n        else:\n            # leave all the sanity checks up to get_fields() method of nested field's serializer\n            nested_fields = None\n            nested_exclude = [field[len(field_name + '.'):] for field in exclude if field.startswith(field_name + '.')]\n\n        # get nested_extra_kwargs (including read-only fields)\n        # TODO: uniqueness extra kwargs\n        extra_kwargs = self.get_extra_kwargs()\n        nested_extra_kwargs = {key[len(field_name + '.'):]: value for key, value in extra_kwargs.items() if key.startswith(field_name + '.')}\n\n        # get nested_validate_methods dict {name: function}, rename e.g. 'validate_author__age()' -> 'validate_age()'\n        # so that we can add them to nested serializer's definition under this new name\n        # validate_methods are normally checked in rest_framework.Serializer.to_internal_value()\n        nested_validate_methods = {}\n        for attr in dir(self.__class__):\n            if attr.startswith('validate_%s__' % field_name.replace('.', '__')):\n                method = get_unbound_function(getattr(self.__class__, attr))\n                method_name = 'validate_' + attr[len('validate_%s__' % field_name.replace('.', '__')):]\n                nested_validate_methods[method_name] = method\n\n        return Customization(nested_fields, nested_exclude, nested_extra_kwargs, nested_validate_methods)", "response": "Returns a customization for the nested field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_customization(self, serializer, customization):\n        # apply fields or exclude\n        if customization.fields is not None:\n            if len(customization.fields) == 0:\n                # customization fields are empty, set Meta.fields to '__all__'\n                serializer.Meta.fields = ALL_FIELDS\n            else:\n                serializer.Meta.fields = customization.fields\n        if customization.exclude is not None:\n            serializer.Meta.exclude = customization.exclude\n\n        # apply extra_kwargs\n        if customization.extra_kwargs is not None:\n            serializer.Meta.extra_kwargs = customization.extra_kwargs\n\n        # apply validate_methods\n        for method_name, method in customization.validate_methods.items():\n            setattr(serializer, method_name, method)", "response": "Applies customization to a nested or embedded DocumentSerializer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate _validated_data with dynamic data", "response": "def to_internal_value(self, data):\n        \"\"\"\n        Updates _validated_data with dynamic data, i.e. data,\n        not listed in fields.\n        \"\"\"\n        ret = super(DynamicDocumentSerializer, self).to_internal_value(data)\n        dynamic_data = self._get_dynamic_data(ret)\n        ret.update(dynamic_data)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_dynamic_data(self, validated_data):\n        result = {}\n\n        for key in self.initial_data:\n            if key not in validated_data:\n                try:\n                    field = self.fields[key]\n                    # no exception? this is either SkipField or error\n                    # in particular, this might be a read-only field\n                    # that was mistakingly given a value\n                    if not isinstance(field, drf_fields.SkipField):\n                        msg = (\n                            'Field %s is missing from validated data,'\n                            'but is not a SkipField!'\n                        ) % key\n                        raise AssertionError(msg)\n                except KeyError:  # ok, this is dynamic data\n                    result[key] = self.initial_data[key]\n        return result", "response": "Returns a dict of data not declared in serializer fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_field_info(model):\n    # Deal with the primary key.\n    if issubclass(model, mongoengine.EmbeddedDocument):\n        pk = None\n    else:\n        pk = model._fields[model._meta['id_field']]\n\n    # Deal with regular fields.\n    fields = OrderedDict()\n\n    # Deal with forward relationships.\n    # Pass forward relations since there is no relations on mongodb\n    references = OrderedDict()\n\n    embedded = OrderedDict()\n\n    def add_field(name, field):\n        if isinstance(field, REFERENCING_FIELD_TYPES):\n            references[name] = get_relation_info(field)\n        elif isinstance(field, EMBEDDING_FIELD_TYPES):\n            embedded[name] = get_relation_info(field)\n        elif isinstance(field, COMPOUND_FIELD_TYPES):\n            fields[name] = field\n            if field.field:\n                add_field(name + '.child', field.field)\n        elif field is pk:\n            return\n        else:\n            fields[name] = field\n\n    for field_name in model._fields_ordered:\n        add_field(field_name, model._fields[field_name])\n\n    # Shortcut that merges both regular fields and the pk,\n    # for simplifying regular field lookup.\n    fields_and_pk = OrderedDict()\n    fields_and_pk['pk'] = pk\n    fields_and_pk[getattr(pk, 'name', 'pk')] = pk\n    fields_and_pk.update(fields)\n\n    return FieldInfo(pk,\n                     fields,\n                     references,\n                     fields_and_pk,\n                     embedded)", "response": "Given a model class returns a FieldInfo instance which is a tuple containing metadata about the various field types on the model and the fields of the model that are referenced by the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of keyword arguments for a basic non - relational field.", "response": "def get_field_kwargs(field_name, model_field):\n    \"\"\"\n    Creating a default instance of a basic non-relational field.\n    \"\"\"\n    kwargs = {}\n\n    # The following will only be used by ModelField classes.\n    # Gets removed for everything else.\n    kwargs['model_field'] = model_field\n\n    if hasattr(model_field, 'verbose_name') and needs_label(model_field, field_name):\n        kwargs['label'] = capfirst(model_field.verbose_name)\n\n    if hasattr(model_field, 'help_text'):\n        kwargs['help_text'] = model_field.help_text\n\n    if isinstance(model_field, me_fields.DecimalField):\n        precision = model_field.precision\n        max_value = getattr(model_field, 'max_value', None)\n        if max_value is not None:\n            max_length = len(str(max_value)) + precision\n        else:\n            max_length = 65536\n        kwargs['decimal_places'] = precision\n        kwargs['max_digits'] = max_length\n\n    if isinstance(model_field, me_fields.GeoJsonBaseField):\n        kwargs['geo_type'] = model_field._type\n\n    if isinstance(model_field, me_fields.SequenceField) or model_field.primary_key or model_field.db_field == '_id':\n        # If this field is read-only, then return early.\n        # Further keyword arguments are not valid.\n        kwargs['read_only'] = True\n        return kwargs\n\n    if model_field.default and not isinstance(model_field, me_fields.ComplexBaseField):\n        kwargs['default'] = model_field.default\n\n    if model_field.null:\n        kwargs['allow_null'] = True\n\n    if model_field.null and isinstance(model_field, me_fields.StringField):\n        kwargs['allow_blank'] = True\n\n    if 'default' not in kwargs:\n        kwargs['required'] = model_field.required\n\n        # handle special cases - compound fields: mongoengine.ListField/DictField\n        if kwargs['required'] is True:\n            if isinstance(model_field, me_fields.ListField) or isinstance(model_field, me_fields.DictField):\n                kwargs['allow_empty'] = False\n\n    if model_field.choices:\n        # If this model field contains choices, then return early.\n        # Further keyword arguments are not valid.\n        kwargs['choices'] = model_field.choices\n        return kwargs\n\n    if isinstance(model_field, me_fields.StringField):\n        if model_field.regex:\n            kwargs['regex'] = model_field.regex\n\n    max_length = getattr(model_field, 'max_length', None)\n    if max_length is not None and isinstance(model_field, me_fields.StringField):\n        kwargs['max_length'] = max_length\n\n    min_length = getattr(model_field, 'min_length', None)\n    if min_length is not None and isinstance(model_field, me_fields.StringField):\n        kwargs['min_length'] = min_length\n\n    max_value = getattr(model_field, 'max_value', None)\n    if max_value is not None and isinstance(model_field, NUMERIC_FIELD_TYPES):\n        kwargs['max_value'] = max_value\n\n    min_value = getattr(model_field, 'min_value', None)\n    if min_value is not None and isinstance(model_field, NUMERIC_FIELD_TYPES):\n        kwargs['min_value'] = min_value\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a default instance of a flat relational field.", "response": "def get_relation_kwargs(field_name, relation_info):\n    \"\"\"\n    Creating a default instance of a flat relational field.\n    \"\"\"\n    model_field, related_model = relation_info\n    kwargs = {}\n    if related_model and not issubclass(related_model, EmbeddedDocument):\n        kwargs['queryset'] = related_model.objects\n\n    if model_field:\n        if hasattr(model_field, 'verbose_name') and needs_label(model_field, field_name):\n            kwargs['label'] = capfirst(model_field.verbose_name)\n        if hasattr(model_field, 'help_text'):\n            kwargs['help_text'] = model_field.help_text\n\n        kwargs['required'] = model_field.required\n\n        if model_field.null:\n            kwargs['allow_null'] = True\n        if getattr(model_field, 'unique', False):\n            validator = UniqueValidator(queryset=related_model.objects)\n            kwargs['validators'] = [validator]\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_nested_relation_kwargs(field_name, relation_info):\n    kwargs = get_relation_kwargs(field_name, relation_info)\n    kwargs.pop('queryset')\n    kwargs.pop('required')\n    kwargs['read_only'] = True\n    return kwargs", "response": "Creates a default instance of a nested serializer\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef density_dir(CIJ):\n    '''\n    Density is the fraction of present connections to possible connections.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        directed weighted/binary connection matrix\n\n    Returns\n    -------\n    kden : float\n        density\n    N : int\n        number of vertices\n    k : int\n        number of edges\n\n    Notes\n    -----\n    Assumes CIJ is directed and has no self-connections.\n    Weight information is discarded.\n    '''\n    n = len(CIJ)\n    k = np.size(np.where(CIJ.flatten()))\n    kden = k / (n * n - n)\n    return kden, n, k", "response": "Returns the density of the nodes in the node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the density of the undirected connection matrix.", "response": "def density_und(CIJ):\n    '''\n    Density is the fraction of present connections to possible connections.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        undirected (weighted/binary) connection matrix\n\n    Returns\n    -------\n    kden : float\n        density\n    N : int\n        number of vertices\n    k : int\n        number of edges\n\n    Notes\n    -----\n    Assumes CIJ is undirected and has no self-connections.\n            Weight information is discarded.\n    '''\n    n = len(CIJ)\n    k = np.size(np.where(np.triu(CIJ).flatten()))\n    kden = k / ((n * n - n) / 2)\n    return kden, n, k"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rentian_scaling(A, xyz, n, seed=None):\n    '''\n    Physical Rentian scaling (or more simply Rentian scaling) is a property\n    of systems that are cost-efficiently embedded into physical space. It is\n    what is called a \"topo-physical\" property because it combines information\n    regarding the topological organization of the graph with information\n    about the physical placement of connections. Rentian scaling is present\n    in very large scale integrated circuits, the C. elegans neuronal network,\n    and morphometric and diffusion-based graphs of human anatomical networks.\n    Rentian scaling is determined by partitioning the system into cubes,\n    counting the number of nodes inside of each cube (N), and the number of\n    edges traversing the boundary of each cube (E). If the system displays\n    Rentian scaling, these two variables N and E will scale with one another\n    in loglog space. The Rent's exponent is given by the slope of log10(E)\n    vs. log10(N), and can be reported alone or can be compared to the\n    theoretical minimum Rent's exponent to determine how cost efficiently the\n    network has been embedded into physical space. Note: if a system displays\n    Rentian scaling, it does not automatically mean that the system is\n    cost-efficiently embedded (although it does suggest that). Validation\n    occurs when comparing to the theoretical minimum Rent's exponent for that\n    system.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        unweighted, binary, symmetric adjacency matrix\n    xyz : Nx3 np.ndarray\n        vector of node placement coordinates\n    n : int\n        Number of partitions to compute. Each partition is a data point; you\n        want a large enough number to adequately compute Rent's exponent.\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    N : Mx1 np.ndarray\n        Number of nodes in each of the M partitions\n    E : Mx1 np.ndarray\n\n    Notes\n    -----\n    Subsequent Analysis:\n    Rentian scaling plots are then created by: figure; loglog(E,N,'*');\n    To determine the Rent's exponent, p, it is important not to use\n    partitions which may\n    be affected by boundary conditions. In Bassett et al. 2010 PLoS CB, only\n    partitions with N<M/2 were used in the estimation of the Rent's exponent.\n    Thus, we can define N_prime = N(find(N<M/2)) and\n    E_prime = E(find(N<M/2)).\n    Next we need to determine the slope of Eprime vs. Nprime in loglog space,\n    which is the Rent's\n    exponent. There are many ways of doing this with more or less statistical\n    rigor. Robustfit in MATLAB is one such option:\n       [b,stats] = robustfit(log10(N_prime),log10(E_prime))\n    Then the Rent's exponent is b(1,2) and the standard error of the\n    estimation is given by stats.se(1,2).\n\n    Note: n=5000 was used in Bassett et al. 2010 in PLoS CB.\n    '''\n    rng = get_rng(seed)\n    m = np.size(xyz, axis=0)  # find number of nodes in system\n\n    # rescale coordinates so they are all greater than unity\n    xyzn = xyz - np.tile(np.min(xyz, axis=0) - 1, (m, 1))\n\n    # find the absolute minimum and maximum over all directions\n    nmax = np.max(xyzn)\n    nmin = np.min(xyzn)\n\n    count = 0\n    N = np.zeros((n,))\n    E = np.zeros((n,))\n\n    # create partitions and count the number of nodes inside the partition (n)\n    # and the number of edges traversing the boundary of the partition (e)\n    while count < n:\n        # define cube endpoints\n        randx = np.sort((1 + nmax - nmin) * rng.random_sample((2,)))\n\n        # find nodes in cube\n        l1 = xyzn[:, 0] > randx[0]\n        l2 = xyzn[:, 0] < randx[1]\n        l3 = xyzn[:, 1] > randx[0]\n        l4 = xyzn[:, 1] < randx[1]\n        l5 = xyzn[:, 2] > randx[0]\n        l6 = xyzn[:, 2] < randx[1]\n\n        L, = np.where((l1 & l2 & l3 & l4 & l5 & l6).flatten())\n        if np.size(L):\n            # count edges crossing at the boundary of the cube\n            E[count] = np.sum(A[np.ix_(L, np.setdiff1d(range(m), L))])\n            # count nodes inside of the cube\n            N[count] = np.size(L)\n            count += 1\n\n    return N, E", "response": "Returns a property that is called a physical rentian scaling system that is used to compute the cost efficiently embedded system of the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _str_member_list(self, name):\n        out = []\n        if self[name]:\n            out += ['.. rubric:: %s' % name, '']\n            prefix = getattr(self, '_name', '')\n\n            if prefix:\n                prefix = '~%s.' % prefix\n\n            autosum = []\n            others = []\n            for param, param_type, desc in self[name]:\n                param = param.strip()\n                if not self._obj or hasattr(self._obj, param):\n                    autosum += [\"   %s%s\" % (prefix, param)]\n                else:\n                    others.append((param, param_type, desc))\n\n            if autosum:\n                # GAEL: Toctree commented out below because it creates\n                # hundreds of sphinx warnings\n                # out += ['.. autosummary::', '   :toctree:', '']\n                out += ['.. autosummary::', '']\n                out += autosum\n\n            if others:\n                maxlen_0 = max([len(x[0]) for x in others])\n                maxlen_1 = max([len(x[1]) for x in others])\n                hdr = \"=\" * maxlen_0 + \"  \" + \"=\" * maxlen_1 + \"  \" + \"=\" * 10\n                fmt = '%%%ds  %%%ds  ' % (maxlen_0, maxlen_1)\n                n_indent = maxlen_0 + maxlen_1 + 4\n                out += [hdr]\n                for param, param_type, desc in others:\n                    out += [fmt % (param.strip(), param_type)]\n                    out += self._str_indent(desc, n_indent)\n                out += [hdr]\n            out += ['']\n        return out", "response": "Generate a member listing of the class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef degrees_dir(CIJ):\n    '''\n    Node degree is the number of links connected to the node. The indegree\n    is the number of inward links and the outdegree is the number of\n    outward links.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        directed binary/weighted connection matrix\n\n    Returns\n    -------\n    id : Nx1 np.ndarray\n        node in-degree\n    od : Nx1 np.ndarray\n        node out-degree\n    deg : Nx1 np.ndarray\n        node degree (in-degree + out-degree)\n\n    Notes\n    -----\n    Inputs are assumed to be on the columns of the CIJ matrix.\n           Weight information is discarded.\n    '''\n    CIJ = binarize(CIJ, copy=True)  # ensure CIJ is binary\n    id = np.sum(CIJ, axis=0)  # indegree = column sum of CIJ\n    od = np.sum(CIJ, axis=1)  # outdegree = row sum of CIJ\n    deg = id + od  # degree = indegree+outdegree\n    return id, od, deg", "response": "Computes the degree of the node in - degree matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef degrees_und(CIJ):\n    '''\n    Node degree is the number of links connected to the node.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        undirected binary/weighted connection matrix\n\n    Returns\n    -------\n    deg : Nx1 np.ndarray\n        node degree\n\n    Notes\n    -----\n    Weight information is discarded.\n    '''\n    CIJ = binarize(CIJ, copy=True)  # ensure CIJ is binary\n    return np.sum(CIJ, axis=0)", "response": "Returns the number of undirected links connected to the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strengths_dir(CIJ):\n    '''\n    Node strength is the sum of weights of links connected to the node. The\n    instrength is the sum of inward link weights and the outstrength is the\n    sum of outward link weights.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        directed weighted connection matrix\n\n    Returns\n    -------\n    is : Nx1 np.ndarray\n        node in-strength\n    os : Nx1 np.ndarray\n        node out-strength\n    str : Nx1 np.ndarray\n        node strength (in-strength + out-strength)\n\n    Notes\n    -----\n    Inputs are assumed to be on the columns of the CIJ matrix.\n    '''\n    istr = np.sum(CIJ, axis=0)\n    ostr = np.sum(CIJ, axis=1)\n    return istr + ostr", "response": "Returns the node strength of the node in - strength and out - strength of the given directed weighted connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strengths_und_sign(W):\n    '''\n    Node strength is the sum of weights of links connected to the node.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected connection matrix with positive and negative weights\n\n    Returns\n    -------\n    Spos : Nx1 np.ndarray\n        nodal strength of positive weights\n    Sneg : Nx1 np.ndarray\n        nodal strength of positive weights\n    vpos : float\n        total positive weight\n    vneg : float\n        total negative weight\n    '''\n    W = W.copy()\n    n = len(W)\n    np.fill_diagonal(W, 0)  # clear diagonal\n    Spos = np.sum(W * (W > 0), axis=0)  # positive strengths\n    Sneg = np.sum(W * (W < 0), axis=0) # negative strengths\n\n    vpos = np.sum(W[W > 0])  # positive weight\n    vneg = np.sum(W[W < 0])  # negative weight\n    return Spos, Sneg, vpos, vneg", "response": "Returns the sum of positive and negative weights of links connected to the node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the MATCHING_IND_UND method for undirected adjacency matrix CIJ.", "response": "def matching_ind_und(CIJ0):\n    '''\n    M0 = MATCHING_IND_UND(CIJ) computes matching index for undirected\n    graph specified by adjacency matrix CIJ. Matching index is a measure of\n    similarity between two nodes' connectivity profiles (excluding their\n    mutual connection, should it exist).\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        undirected adjacency matrix\n\n    Returns\n    -------\n    M0 : NxN np.ndarray\n        matching index matrix\n    '''\n    K = np.sum(CIJ0, axis=0)\n    n = len(CIJ0)\n    R = (K != 0)\n    N = np.sum(R)\n    xR, = np.where(R == 0)\n    CIJ = np.delete(np.delete(CIJ0, xR, axis=0), xR, axis=1)\n    I = np.logical_not(np.eye(N))\n    M = np.zeros((N, N))\n\n    for i in range(N):\n        c1 = CIJ[i, :]\n        use = np.logical_or(c1, CIJ)\n        use[:, i] = 0\n        use *= I\n\n        ncon1 = c1 * use\n        ncon2 = c1 * CIJ\n        ncon = np.sum(ncon1 + ncon2, axis=1)\n        print(ncon)\n\n        M[:, i] = 2 * np.sum(np.logical_and(ncon1, ncon2), axis=1) / ncon\n\n    M *= I\n    M[np.isnan(M)] = 0\n    M0 = np.zeros((n, n))\n    yR, = np.where(R)\n    M0[np.ix_(yR, yR)] = M\n    return M0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate pairwise dice similarity for each vertex between two matrices. Treats the matrices as binary and undirected.", "response": "def dice_pairwise_und(a1, a2):\n    '''\n    Calculates pairwise dice similarity for each vertex between two\n    matrices. Treats the matrices as binary and undirected.\n\n    Paramaters\n    ----------\n    A1 : NxN np.ndarray\n        Matrix 1\n    A2 : NxN np.ndarray\n        Matrix 2\n\n    Returns\n    -------\n    D : Nx1 np.ndarray\n        dice similarity vector\n    '''\n    a1 = binarize(a1, copy=True)\n    a2 = binarize(a2, copy=True)  # ensure matrices are binary\n\n    n = len(a1)\n    np.fill_diagonal(a1, 0)\n    np.fill_diagonal(a2, 0)  # set diagonals to 0\n\n    d = np.zeros((n,))  # dice similarity\n\n    # calculate the common neighbors for each vertex\n    for i in range(n):\n        d[i] = 2 * (np.sum(np.logical_and(a1[:, i], a2[:, i])) /\n                    (np.sum(a1[:, i]) + np.sum(a2[:, i])))\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef corr_flat_und(a1, a2):\n    '''\n    Returns the correlation coefficient between two flattened adjacency\n    matrices.  Only the upper triangular part is used to avoid double counting\n    undirected matrices.  Similarity metric for weighted matrices.\n\n    Parameters\n    ----------\n    A1 : NxN np.ndarray\n        undirected matrix 1\n    A2 : NxN np.ndarray\n        undirected matrix 2\n\n    Returns\n    -------\n    r : float\n        Correlation coefficient describing edgewise similarity of a1 and a2\n    '''\n    n = len(a1)\n    if len(a2) != n:\n        raise BCTParamError(\"Cannot calculate flattened correlation on \"\n                            \"matrices of different size\")\n    triu_ix = np.where(np.triu(np.ones((n, n)), 1))\n    return np.corrcoef(a1[triu_ix].flat, a2[triu_ix].flat)[0][1]", "response": "Returns the correlation coefficient between two undirected adjacency matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef corr_flat_dir(a1, a2):\n    '''\n    Returns the correlation coefficient between two flattened adjacency\n    matrices.  Similarity metric for weighted matrices.\n\n    Parameters\n    ----------\n    A1 : NxN np.ndarray\n        directed matrix 1\n    A2 : NxN np.ndarray\n        directed matrix 2\n\n    Returns\n    -------\n    r : float\n        Correlation coefficient describing edgewise similarity of a1 and a2\n    '''\n    n = len(a1)\n    if len(a2) != n:\n        raise BCTParamError(\"Cannot calculate flattened correlation on \"\n                            \"matrices of different size\")\n    ix = np.logical_not(np.eye(n))\n    return np.corrcoef(a1[ix].flat, a2[ix].flat)[0][1]", "response": "Returns the correlation coefficient between two flattened adjacency matrices."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backbone_wu(CIJ, avgdeg):\n    '''\n    The network backbone contains the dominant connections in the network\n    and may be used to aid network visualization. This function computes\n    the backbone of a given weighted and undirected connection matrix CIJ,\n    using a minimum-spanning-tree based algorithm.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        weighted undirected connection matrix\n    avgdeg : int\n        desired average degree of backbone\n\n    Returns\n    -------\n    CIJtree : NxN np.ndarray\n        connection matrix of the minimum spanning tree of CIJ\n    CIJclus : NxN np.ndarray\n        connection matrix of the minimum spanning tree plus strongest\n        connections up to some average degree 'avgdeg'. Identical to CIJtree\n        if the degree requirement is already met.\n\n    Notes\n    -----\n    NOTE: nodes with zero strength are discarded.\n    NOTE: CIJclus will have a total average degree exactly equal to\n         (or very close to) 'avgdeg'.\n    NOTE: 'avgdeg' backfill is handled slightly differently than in Hagmann\n         et al 2008.\n    '''\n    n = len(CIJ)\n    if not np.all(CIJ == CIJ.T):\n        raise BCTParamError('backbone_wu can only be computed for undirected '\n                            'matrices.  If your matrix is has noise, correct it with np.around')\n    CIJtree = np.zeros((n, n))\n\n    # find strongest edge (if multiple edges are tied, use only first one)\n    i, j = np.where(np.max(CIJ) == CIJ)\n    im = [i[0], i[1]]  # what?  why take two values?  doesnt that mess up multiples?\n    jm = [j[0], j[1]]\n\n    # copy into tree graph\n    CIJtree[im, jm] = CIJ[im, jm]\n    in_ = im\n    out = np.setdiff1d(range(n), in_)\n\n    # repeat n-2 times\n    for ix in range(n - 2):\n        CIJ_io = CIJ[np.ix_(in_, out)]\n        i, j = np.where(np.max(CIJ_io) == CIJ_io)\n        # i,j=np.where(np.max(CIJ[in_,out])==CIJ[in_,out])\n        print(i, j)\n        im = in_[i[0]]\n        jm = out[j[0]]\n\n        # copy into tree graph\n        CIJtree[im, jm] = CIJ[im, jm]\n        CIJtree[jm, im] = CIJ[jm, im]\n        in_ = np.append(in_, jm)\n        out = np.setdiff1d(range(n), in_)\n\n    # now add connections back with the total number of added connections\n    # determined by the desired avgdeg\n\n    CIJnotintree = CIJ * np.logical_not(CIJtree)\n    ix, = np.where(CIJnotintree.flat)\n    a = np.sort(CIJnotintree.flat[ix])[::-1]\n    cutoff = avgdeg * n - 2 * (n - 1) - 1\n    # if the avgdeg req is already satisfied, skip this\n    if cutoff >= np.size(a):\n        CIJclus = CIJtree.copy()\n    else:\n        thr = a[cutoff]\n        CIJclus = CIJtree + CIJnotintree * (CIJnotintree >= thr)\n\n    return CIJtree, CIJclus", "response": "This function computes the network backbone of a given weighted undirected connection matrix and an average degree of the undirected connection matrix. This function computes the network backbone of a given weighted undirected connection matrix CIJ and returns the network backbone of the network with dominant connections in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reorderMAT(m, H=5000, cost='line'):\n    '''\n    This function reorders the connectivity matrix in order to place more\n    edges closer to the diagonal. This often helps in displaying community\n    structure, clusters, etc.\n\n    Parameters\n    ----------\n    MAT : NxN np.ndarray\n        connection matrix\n    H : int\n        number of reordering attempts\n    cost : str\n        'line' or 'circ' for shape of lattice (linear or ring lattice).\n        Default is linear lattice.\n\n    Returns\n    -------\n    MATreordered : NxN np.ndarray\n        reordered connection matrix\n    MATindices : Nx1 np.ndarray\n        reordered indices\n    MATcost : float\n        objective function cost of reordered matrix\n\n    Notes\n    -----\n    I'm not 100% sure how the algorithms between this and reorder_matrix\n    differ, but this code looks a ton sketchier and might have had some minor\n    bugs in it.  Considering reorder_matrix() does the same thing using a well\n    vetted simulated annealing algorithm, just use that. ~rlaplant\n    '''\n    from scipy import linalg, stats\n    m = m.copy()\n    n = len(m)\n    np.fill_diagonal(m, 0)\n\n    # generate cost function\n    if cost == 'line':\n        profile = stats.norm.pdf(range(1, n + 1), 0, n / 2)[::-1]\n    elif cost == 'circ':\n        profile = stats.norm.pdf(range(1, n + 1), n / 2, n / 4)[::-1]\n    else:\n        raise BCTParamError('dfun must be line or circ')\n    costf = linalg.toeplitz(profile, r=profile)\n\n    lowcost = np.sum(costf * m)\n\n    # keep track of starting configuration\n    m_start = m.copy()\n    starta = np.arange(n)\n    # reorder\n    for h in range(H):\n        a = np.arange(n)\n        # choose two positions and flip them\n        r1, r2 = rng.randint(n, size=(2,))\n        a[r1] = r2\n        a[r2] = r1\n        costnew = np.sum((m[np.ix_(a, a)]) * costf)\n        # if this reduced the overall cost\n        if costnew < lowcost:\n            m = m[np.ix_(a, a)]\n            r2_swap = starta[r2]\n            r1_swap = starta[r1]\n            starta[r1] = r2_swap\n            starta[r2] = r1_swap\n            lowcost = costnew\n\n    M_reordered = m_start[np.ix_(starta, starta)]\n    m_indices = starta\n    cost = lowcost\n    return M_reordered, m_indices, cost", "response": "This function reorders the connectivity matrix in order to place more\n    edges closer to the diagonal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makerandCIJdegreesfixed(inv, outv, seed=None):\n    '''\n    This function generates a directed random network with a specified\n    in-degree and out-degree sequence.\n\n    Parameters\n    ----------\n    inv : Nx1 np.ndarray\n        in-degree vector\n    outv : Nx1 np.ndarray\n        out-degree vector\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    CIJ : NxN np.ndarray\n\n    Notes\n    -----\n    Necessary conditions include:\n            length(in) = length(out) = n\n            sum(in) = sum(out) = k\n            in(i), out(i) < n-1\n            in(i) + out(j) < n+2\n            in(i) + out(i) < n\n\n        No connections are placed on the main diagonal\n\n        The algorithm used in this function is not, technically, guaranteed to\n        terminate. If a valid distribution of in and out degrees is provided,\n        this function will find it in bounded time with probability\n        1-(1/(2*(k^2))). This turns out to be a serious problem when\n        computing infinite degree matrices, but offers good performance\n        otherwise.\n    '''\n    rng = get_rng(seed)\n\n    n = len(inv)\n    k = np.sum(inv)\n    in_inv = np.zeros((k,))\n    out_inv = np.zeros((k,))\n    i_in = 0\n    i_out = 0\n\n    for i in range(n):\n        in_inv[i_in:i_in + inv[i]] = i\n        out_inv[i_out:i_out + outv[i]] = i\n        i_in += inv[i]\n        i_out += outv[i]\n\n    CIJ = np.eye(n)\n    edges = np.array((out_inv, in_inv[rng.permutation(k)]))\n\n    # create CIJ and check for double edges and self connections\n    for i in range(k):\n        if CIJ[edges[0, i], edges[1, i]]:\n            tried = set()\n            while True:\n                if len(tried) == k:\n                    raise BCTParamError('Could not resolve the given '\n                                        'in and out vectors')\n                switch = rng.randint(k)\n                while switch in tried:\n                    switch = rng.randint(k)\n                if not (CIJ[edges[0, i], edges[1, switch]] or\n                        CIJ[edges[0, switch], edges[1, i]]):\n                    CIJ[edges[0, switch], edges[1, switch]] = 0\n                    CIJ[edges[0, switch], edges[1, i]] = 1\n                    if switch < i:\n                        CIJ[edges[0, switch], edges[1, switch]] = 0\n                        CIJ[edges[0, switch], edges[1, i]] = 1\n                    t = edges[1, i]\n                    edges[1, i] = edges[1, switch]\n                    edges[1, switch] = t\n                    break\n                tried.add(switch)\n        else:\n            CIJ[edges[0, i], edges[1, i]] = 1\n\n    CIJ -= np.eye(n)\n    return CIJ", "response": "This function generates a directed random network with a specified in - degree vector and a specified out - degree sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef maketoeplitzCIJ(n, k, s, seed=None):\n    '''\n    This function generates a directed network with a Gaussian drop-off in\n    edge density with increasing distance from the main diagonal. There are\n    toroidal boundary counditions (i.e. no ring-like \"wrapping around\").\n\n    Parameters\n    ----------\n    N : int\n        number of vertices\n    K : int\n        number of edges\n    s : float\n        standard deviation of toeplitz\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    CIJ : NxN np.ndarray\n        connection matrix\n\n    Notes\n    -----\n    no connections are placed on the main diagonal.\n    '''\n    rng = get_rng(seed)\n    from scipy import linalg, stats\n    pf = stats.norm.pdf(range(1, n), .5, s)\n    template = linalg.toeplitz(np.append((0,), pf), r=np.append((0,), pf))\n    template *= (k / np.sum(template))\n\n    CIJ = np.zeros((n, n))\n    itr = 0\n    while np.sum(CIJ) != k:\n        CIJ = (rng.random_sample((n, n)) < template)\n        itr += 1\n        if itr > 10000:\n            raise BCTParamError('Infinite loop was caught generating toeplitz '\n                                'matrix.  This means the matrix could not be resolved with the '\n                                'specified parameters.')\n\n    return CIJ", "response": "This function generates a directed network with a Gaussian drop - off inversion of toeplitz."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef randmio_dir(R, itr, seed=None):\n    '''\n    This function randomizes a directed network, while preserving the in-\n    and out-degree distributions. In weighted networks, the function\n    preserves the out-strength but not the in-strength distributions.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        directed binary/weighted connection matrix\n    itr : int\n        rewiring parameter. Each edge is rewired approximately itr times.\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    R : NxN np.ndarray\n        randomized network\n    eff : int\n        number of actual rewirings carried out\n    '''\n    rng = get_rng(seed)\n    R = R.copy()\n    n = len(R)\n    i, j = np.where(R)\n    k = len(i)\n    itr *= k\n\n    max_attempts = np.round(n * k / (n * (n - 1)))\n    eff = 0\n\n    for it in range(int(itr)):\n        att = 0\n        while att <= max_attempts:  # while not rewired\n            while True:\n                e1 = rng.randint(k)\n                e2 = rng.randint(k)\n                while e1 == e2:\n                    e2 = rng.randint(k)\n                a = i[e1]\n                b = j[e1]\n                c = i[e2]\n                d = j[e2]\n\n                if a != c and a != d and b != c and b != d:\n                    break  # all 4 vertices must be different\n\n            # rewiring condition\n            if not (R[a, d] or R[c, b]):\n                R[a, d] = R[a, b]\n                R[a, b] = 0\n                R[c, b] = R[c, d]\n                R[c, d] = 0\n\n                i.setflags(write=True)\n                j.setflags(write=True)\n                i[e1] = d\n                j[e2] = b  # reassign edge indices\n                eff += 1\n                break\n            att += 1\n\n    return R, eff", "response": "This function randomizes a directed network while preserving the in - degree distributions and out - degree distributions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef randmio_und_signed(R, itr, seed=None):\n    '''\n    This function randomizes an undirected weighted network with positive\n    and negative weights, while simultaneously preserving the degree\n    distribution of positive and negative weights. The function does not\n    preserve the strength distribution in weighted networks.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected binary/weighted connection matrix\n    itr : int\n        rewiring parameter. Each edge is rewired approximately itr times.\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    R : NxN np.ndarray\n        randomized network\n    '''\n    rng = get_rng(seed)\n    R = R.copy()\n    n = len(R)\n\n    itr *= int(n * (n -1) / 2)\n\n    max_attempts = int(np.round(n / 2))\n    eff = 0\n\n    for it in range(int(itr)):\n        att = 0\n        while att <= max_attempts:\n\n            a, b, c, d = pick_four_unique_nodes_quickly(n, rng)\n\n            r0_ab = R[a, b]\n            r0_cd = R[c, d]\n            r0_ad = R[a, d]\n            r0_cb = R[c, b]\n\n            #rewiring condition\n            if (    np.sign(r0_ab) == np.sign(r0_cd) and\n                    np.sign(r0_ad) == np.sign(r0_cb) and\n                    np.sign(r0_ab) != np.sign(r0_ad)):\n        \n                R[a, d] = R[d, a] = r0_ab\n                R[a, b] = R[b, a] = r0_ad\n\n                R[c, b] = R[b, c] = r0_cd\n                R[c, d] = R[d, c] = r0_cb\n\n                eff += 1\n                break\n\n            att += 1\n\n    return R, eff", "response": "This function randomizes an undirected weighted network with positive and negative weights while simultaneously preserving the degree\n    distribution of positive and negative weights."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomize_graph_partial_und(A, B, maxswap, seed=None):\n    '''\n    A = RANDOMIZE_GRAPH_PARTIAL_UND(A,B,MAXSWAP) takes adjacency matrices A\n    and B and attempts to randomize matrix A by performing MAXSWAP\n    rewirings. The rewirings will avoid any spots where matrix B is\n    nonzero.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        undirected adjacency matrix to randomize\n    B : NxN np.ndarray\n        mask; edges to avoid\n    maxswap : int\n        number of rewirings\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    A : NxN np.ndarray\n        randomized matrix\n\n    Notes\n    -----\n    1. Graph may become disconnected as a result of rewiring. Always\n      important to check.\n    2. A can be weighted, though the weighted degree sequence will not be\n      preserved.\n    3. A must be undirected.\n    '''\n    rng = get_rng(seed)\n    A = A.copy()\n    i, j = np.where(np.triu(A, 1))\n    i.setflags(write=True)\n    j.setflags(write=True)\n    m = len(i)\n\n    nswap = 0\n    while nswap < maxswap:\n        while True:\n            e1, e2 = rng.randint(m, size=(2,))\n            while e1 == e2:\n                e2 = rng.randint(m)\n            a = i[e1]\n            b = j[e1]\n            c = i[e2]\n            d = j[e2]\n\n            if a != c and a != d and b != c and b != d:\n                break  # all 4 vertices must be different\n\n        if rng.random_sample() > .5:\n            i[e2] = d\n            j[e2] = c  # flip edge c-d with 50% probability\n            c = i[e2]\n            d = j[e2]  # to explore all potential rewirings\n\n        # rewiring condition\n        if not (A[a, d] or A[c, b] or B[a, d] or B[c, b]):  # avoid specified ixes\n            A[a, d] = A[a, b]\n            A[a, b] = 0\n            A[d, a] = A[b, a]\n            A[b, a] = 0\n            A[c, b] = A[c, d]\n            A[c, d] = 0\n            A[b, c] = A[d, c]\n            A[d, c] = 0\n\n            j[e1] = d\n            j[e2] = b  # reassign edge indices\n            nswap += 1\n    return A", "response": "Randomize the undirected graph A and B."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomizer_bin_und(R, alpha, seed=None):\n    '''\n    This function randomizes a binary undirected network, while preserving\n    the degree distribution. The function directly searches for rewirable\n    edge pairs (rather than trying to rewire edge pairs at random), and\n    hence avoids long loops and works especially well in dense matrices.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary undirected connection matrix\n    alpha : float\n        fraction of edges to rewire\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    R : NxN np.ndarray\n        randomized network\n    '''\n    rng = get_rng(seed)\n    R = binarize(R, copy=True)  # binarize\n    if not np.all(R == R.T):\n        raise BCTParamError(\n            'randomizer_bin_und only takes undirected matrices')\n\n    ax = len(R)\n    nr_poss_edges = (np.dot(ax, ax) - ax) / 2  # find maximum possible edges\n\n    savediag = np.diag(R)\n    np.fill_diagonal(R, np.inf)  # replace diagonal with high value\n\n    # if there are more edges than non-edges, invert the matrix to reduce\n    # computation time.  \"invert\" means swap meaning of 0 and 1, not matrix\n    # inversion\n\n    i, j = np.where(np.triu(R, 1))\n    k = len(i)\n    if k > nr_poss_edges / 2:\n        swap = True\n        R = np.logical_not(R)\n        np.fill_diagonal(R, np.inf)\n        i, j = np.where(np.triu(R, 1))\n        k = len(i)\n    else:\n        swap = False\n\n    # exclude fully connected nodes\n    fullnodes = np.where((np.sum(np.triu(R, 1), axis=0) +\n                          np.sum(np.triu(R, 1), axis=1).T) == (ax - 1))\n    if np.size(fullnodes):\n        R[fullnodes, :] = 0\n        R[:, fullnodes] = 0\n        np.fill_diagonal(R, np.inf)\n        i, j = np.where(np.triu(R, 1))\n        k = len(i)\n\n    if k == 0 or k >= (nr_poss_edges - 1):\n        raise BCTParamError(\"No possible randomization\")\n\n    for it in range(k):\n        if rng.random_sample() > alpha:\n            continue  # rewire alpha% of edges\n\n        a = i[it]\n        b = j[it]  # it is the chosen edge from a<->b\n\n        alliholes, = np.where(R[:, a] == 0)  # find where each end can connect\n        alljholes, = np.where(R[:, b] == 0)\n\n        # we can only use edges with connection to neither node\n        i_intersect = np.intersect1d(alliholes, alljholes)\n        # find which of these nodes are connected\n        ii, jj = np.where(R[np.ix_(i_intersect, i_intersect)])\n\n        # if there is an edge to switch\n        if np.size(ii):\n            # choose one randomly\n            nummates = np.size(ii)\n            mate = rng.randint(nummates)\n\n            # randomly orient the second edge\n            if rng.random_sample() > .5:\n                c = i_intersect[ii[mate]]\n                d = i_intersect[jj[mate]]\n            else:\n                d = i_intersect[ii[mate]]\n                c = i_intersect[jj[mate]]\n\n            # swap the edges\n            R[a, b] = 0\n            R[c, d] = 0\n            R[b, a] = 0\n            R[d, c] = 0\n            R[a, c] = 1\n            R[b, d] = 1\n            R[c, a] = 1\n            R[d, b] = 1\n\n            # update the edge index (this is inefficient)\n            for m in range(k):\n                if i[m] == d and j[m] == c:\n                    i.setflags(write=True)\n                    j.setflags(write=True)\n                    i[it] = c\n                    j[m] = b\n                elif i[m] == c and j[m] == d:\n                    i.setflags(write=True)\n                    j.setflags(write=True)\n                    j[it] = c\n                    i[m] = b\n\n    # restore fullnodes\n    if np.size(fullnodes):\n        R[fullnodes, :] = 1\n        R[:, fullnodes] = 1\n\n    # restore inversion\n    if swap:\n        R = np.logical_not(R)\n\n    # restore diagonal\n    np.fill_diagonal(R, 0)\n    R += savediag\n\n    return np.array(R, dtype=int)", "response": "This function randomizes a binary undirected network while preserving\n    the degree distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generative_model(A, D, m, eta, gamma=None, model_type='matching', \n    model_var='powerlaw', epsilon=1e-6, copy=True, seed=None):\n    '''\n    Generates synthetic networks using the models described in\n    Betzel et al. (2016) Neuroimage. See this paper for more details.\n\n    Succinctly, the probability of forming a connection between nodes u and v is\n    P(u,v) = E(u,v)**eta * K(u,v)**gamma\n    where eta and gamma are hyperparameters, E(u,v) is the euclidean or similar\n    distance measure, and K(u,v) is the algorithm that defines the model.\n\n    This describes the power law formulation, an alternative formulation uses\n    the exponential function\n    P(u,v) = exp(E(u,v)*eta) * exp(K(u,v)*gamma)\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Binary network of seed connections\n    D : np.ndarray\n        Matrix of euclidean distances or other distances between nodes\n    m : int\n        Number of connections that should be present in the final synthetic \n        network\n    eta : np.ndarray\n        A vector describing a range of values to estimate for eta, the \n        hyperparameter describing exponential weighting of the euclidean\n        distance.\n    gamma : np.ndarray\n        A vector describing a range of values to estimate for theta, the\n        hyperparameter describing exponential weighting of the basis\n        algorithm. If model_type='euclidean' or another distance metric,\n        this can be None.\n    model_type : Enum(str)\n        euclidean : Uses only euclidean distances to generate connection \n            probabilities\n        neighbors : count of common neighbors\n        matching : matching index, the normalized overlap in neighborhoods\n        clu-avg : Average clustering coefficient\n        clu-min : Minimum clustering coefficient\n        clu-max : Maximum clustering coefficient\n        clu-diff : Difference in clustering coefficient\n        clu-prod : Product of clustering coefficient\n        deg-avg : Average degree\n        deg-min : Minimum degree\n        deg-max : Maximum degree\n        deg-diff : Difference in degree\n        deg-prod : Product of degrees\n    model_var : Enum(str)\n        Default value is powerlaw. If so, uses formulation of P(u,v) as\n        described above. Alternate value is exponential. If so, uses\n        P(u,v) = exp(E(u,v)*eta) * exp(K(u,v)*gamma)\n    epsilon : float\n        A small positive value added to all P(u,v). The default value is 1e-6\n    copy : bool\n        Some algorithms add edges directly to the input matrix. Set this flag\n        to make a copy of the input matrix instead. Defaults to True.\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n    '''\n    rng = get_rng(seed)\n    if copy:\n        A = A.copy()\n\n    n = len(D)\n    \n    #These parameters don't do any of the voronoi narrowing.\n    #Its a list of eta values paired with gamma values.\n    #To try 3 eta and 3 gamma pairs, should use 9 list values.\n    if len(eta) != len(gamma):\n        raise BCTParamError('Eta and gamma hyperparameters must be lists of '\n            'the same size')\n\n    nparams = len(eta)\n\n    B = np.zeros((n, n, nparams))\n\n    def k_avg(K):\n        return ((np.tile(K, (n, 1)) + np.transpose(np.tile(K, (n, 1))))/2 +\n            epsilon)\n\n    def k_diff(K):\n        return np.abs(np.tile(K, (n, 1)) - \n                      np.transpose(np.tile(K, (n, 1)))) + epsilon\n\n    def k_max(K):\n        return np.max(np.dstack((np.tile(K, (n, 1)),\n                                 np.transpose(np.tile(K, (n, 1))))),\n                      axis=2) + epsilon\n\n    def k_min(K):\n        return np.min(np.dstack((np.tile(K, (n, 1)),\n                                 np.transpose(np.tile(K, (n, 1))))),\n                      axis=2) + epsilon\n\n    def k_prod(K):\n        return np.outer(K, np.transpose(K)) + epsilon\n\n    def s_avg(K, sc):\n        return (K+sc) / 2 + epsilon\n\n    def s_diff(K, sc):\n        return np.abs(K-sc) + epsilon\n\n    def s_min(K, sc):\n        return np.where(K < sc, K + epsilon, sc + epsilon)\n    \n    def s_max(K, sc):\n        #return np.max((K, sc.T), axis=0)\n        return np.where(K > sc, K + epsilon, sc + epsilon)\n\n    def s_prod(K, sc):\n        return K * sc + epsilon\n\n    def x_avg(K, ixes):\n        nr_ixes = np.size(np.where(ixes))\n        Ksc = np.tile(K, (nr_ixes, 1))\n        Kix = np.transpose(np.tile(K[ixes], (n, 1)))\n        return s_avg(Ksc, Kix)\n\n    def x_diff(K, ixes):\n        nr_ixes = np.size(np.where(ixes))\n        Ksc = np.tile(K, (nr_ixes, 1))\n        Kix = np.transpose(np.tile(K[ixes], (n, 1)))\n        return s_diff(Ksc, Kix)\n\n    def x_max(K, ixes):\n        nr_ixes = np.size(np.where(ixes))\n        Ksc = np.tile(K, (nr_ixes, 1))\n        Kix = np.transpose(np.tile(K[ixes], (n, 1)))\n        return s_max(Ksc, Kix)\n\n    def x_min(K, ixes):\n        nr_ixes = np.size(np.where(ixes))\n        Ksc = np.tile(K, (nr_ixes, 1))\n        Kix = np.transpose(np.tile(K[ixes], (n, 1)))\n        return s_min(Ksc, Kix)\n\n    def x_prod(K, ixes):\n        nr_ixes = np.size(np.where(ixes))\n        Ka = np.reshape(K[ixes], (nr_ixes, 1))\n        Kb = np.reshape(np.transpose(K), (1, n))\n        return np.outer(Ka, Kb) + epsilon\n\n\n    def clu_gen(A, K, D, m, eta, gamma, model_var, x_fun):\n        mseed = np.size(np.where(A.flat))//2\n\n        A = A>0\n\n        if type(model_var) == tuple:\n            mv1, mv2 = model_var\n        else:\n            mv1, mv2 = model_var, model_var\n\n        if mv1 in ('powerlaw', 'power_law'):\n            Fd = D**eta\n        elif mv1 in ('exponential',):\n            Fd = np.exp(eta*D) \n\n        if mv2 in ('powerlaw', 'power_law'):\n            Fk = K**gamma\n        elif mv2 in ('exponential',):\n            Fk = np.exp(gamma*K) \n\n        c = clustering_coef_bu(A)\n        k = np.sum(A, axis=1)\n\n        Ff = Fd * Fk * np.logical_not(A)\n        u,v = np.where(np.triu(np.ones((n,n)), 1))\n\n        #print(mseed, m)\n        for i in range(mseed+1, m):\n            C = np.append(0, np.cumsum(Ff[u,v]))\n            r = np.sum(rng.random_sample()*C[-1] >= C)\n            uu = u[r]\n            vv = v[r]\n            A[uu,vv] = A[vv,uu] = 1\n            k[uu] += 1\n            k[vv] += 1\n\n            bu = A[uu,:].astype(bool)\n            bv = A[vv,:].astype(bool)\n            su = A[np.ix_(bu, bu)]\n            sv = A[np.ix_(bu, bu)]\n\n            bth = np.logical_and(bu, bv)\n            c[bth] += 2/(k[bth]**2 - k[bth])\n            c[uu] = np.size(np.where(su.flat))/(k[uu]*(k[uu]-1))\n            c[vv] = np.size(np.where(sv.flat))/(k[vv]*(k[vv]-1))\n            c[k<=1] = 0\n            bth[uu] = 1\n            bth[vv] = 1\n    \n            k_result = x_fun(c, bth)\n\n            #print(np.shape(k_result))\n            #print(np.shape(K))\n            #print(K)\n            #print(np.shape(K[bth,:]))\n\n            K[bth,:] = k_result\n            K[:,bth] = k_result.T\n\n            if mv2 in ('powerlaw', 'power_law'):\n                Ff[bth,:] = Fd[bth,:] * K[bth,:]**gamma\n                Ff[:,bth] = Fd[:,bth] * K[:,bth]**gamma\n            elif mv2 in ('exponential',):\n                Ff[bth,:] = Fd[bth,:] * np.exp(K[bth,:])*gamma\n                Ff[:,bth] = Fd[:,bth] * np.exp(K[:,bth])*gamma\n\n            Ff = Ff * np.logical_not(A)\n\n        return A\n\n    def deg_gen(A, K, D, m, eta, gamma, model_var, s_fun):\n        mseed = np.size(np.where(A.flat))//2\n\n        k = np.sum(A, axis=1)\n\n        if type(model_var) == tuple:\n            mv1, mv2 = model_var\n        else:\n            mv1, mv2 = model_var, model_var\n\n        if mv1 in ('powerlaw', 'power_law'):\n            Fd = D**eta\n        elif mv1 in ('exponential',):\n            Fd = np.exp(eta*D) \n\n        if mv2 in ('powerlaw', 'power_law'):\n            Fk = K**gamma\n        elif mv2 in ('exponential',):\n            Fk = np.exp(gamma*K) \n\n        P = Fd * Fk * np.logical_not(A)\n        u,v = np.where(np.triu(np.ones((n,n)), 1))\n\n        b = np.zeros((m,), dtype=int)\n\n#        print(mseed)\n#        print(np.shape(u),np.shape(v))\n#        print(np.shape(b))\n#        print(np.shape(A[u,v]))\n#        print(np.shape(np.where(A[u,v])), 'sqishy')\n#        print(np.shape(P), 'squnnaq')\n\n        #b[:mseed] = np.where(A[np.ix_(u,v)]) \n        b[:mseed] = np.squeeze(np.where(A[u,v]))\n        #print(mseed, m)\n        for i in range(mseed, m):\n            C = np.append(0, np.cumsum(P[u,v]))\n            r = np.sum(rng.random_sample()*C[-1] >= C)\n            uu = u[r]\n            vv = v[r]\n            k[uu] += 1\n            k[vv] += 1\n\n            if mv2 in ('powerlaw', 'power_law'):\n                Fk[:,uu] = Fk[uu,:] = s_fun(k, k[uu]) ** gamma\n                Fk[:,vv] = Fk[vv,:] = s_fun(k, k[vv]) ** gamma\n            elif mv2 in ('exponential',):\n                Fk[:,uu] = Fk[uu,:] = np.exp(s_fun(k, k[uu]) * gamma)\n                Fk[:,vv] = Fk[vv,:] = np.exp(s_fun(k, k[vv]) * gamma)\n\n            P = Fd * Fk\n\n            b[i] = r\n\n            P[u[b[:i]], v[b[:i]]] = P[v[b[:i]], u[b[:i]]] = 0\n\n            A[u[r], v[r]] = A[v[r], u[r]] = 1\n            #P[b[u[:i]], b[v[:i]]] = P[b[v[:i]], b[u[:i]]] = 0\n\n            #A[uu,vv] = A[vv,uu] = 1\n\n\n#        indx = v*n + u\n#        indx[b]\n#\n#        nH = np.zeros((n,n))\n#        nH.ravel()[indx[b]]=1\n#\n#        nG = np.zeros((n,n))\n#        nG[ u[b], v[b] ]=1\n#        nG = nG + nG.T\n#\n#        print(np.shape(np.where(A != nG)))\n#\n#        import pdb\n#        pdb.set_trace()\n\n        return A\n\n    def matching_gen(A, K, D, m, eta, gamma, model_var):\n        K += epsilon\n\n        mseed = np.size(np.where(A.flat))//2\n\n        if type(model_var) == tuple:\n            mv1, mv2 = model_var\n        else:\n            mv1, mv2 = model_var, model_var\n\n        if mv1 in ('powerlaw', 'power_law'):\n            Fd = D**eta\n        elif mv1 in ('exponential',):\n            Fd = np.exp(eta*D) \n\n        if mv2 in ('powerlaw', 'power_law'):\n            Fk = K**gamma\n        elif mv2 in ('exponential',):\n            Fk = np.exp(gamma*K) \n\n        Ff = Fd * Fk * np.logical_not(A)\n        u,v = np.where(np.triu(np.ones((n,n)), 1))\n    \n        for ii in range(mseed, m):\n            C = np.append(0, np.cumsum(Ff[u,v]))\n            r = np.sum(rng.random_sample()*C[-1] >= C)\n            uu = u[r]\n            vv = v[r]\n            A[uu,vv] = A[vv,uu] = 1\n\n            updateuu, = np.where(np.inner(A, A[:,uu]))\n            np.delete(updateuu, np.where(updateuu == uu))\n            np.delete(updateuu, np.where(updateuu == vv))\n\n            c1 = np.append(A[:,uu], A[uu,:])\n            for i in range(len(updateuu)):\n                j = updateuu[i]\n                c2 = np.append(A[:,j], A[j,:])\n    \n                use = np.logical_or(c1, c2)\n                use[uu] = use[uu+n] = use[j] = use[j+n] = 0\n                ncon = np.sum(c1[use]) + np.sum(c2[use])\n                if ncon == 0:\n                    K[uu, j] = K[j, uu] = epsilon\n                else:\n                    K[uu, j] = K[j, uu] = (2 / ncon *\n                        np.sum(np.logical_and(c1[use], c2[use])) + epsilon)\n\n            updatevv, = np.where(np.inner(A, A[:,vv]))\n            np.delete(updatevv, np.where(updatevv == uu))\n            np.delete(updatevv, np.where(updatevv == vv))\n        \n            c1 = np.append(A[:,vv], A[vv,:])\n            for i in range(len(updatevv)):\n                j = updatevv[i]\n                c2 = np.append(A[:,j], A[j,:])\n    \n                use = np.logical_or(c1, c2)\n                use[vv] = use[vv+n] = use[j] = use[j+n] = 0\n                ncon = np.sum(c1[use]) + np.sum(c2[use])\n                if ncon == 0:\n                    K[vv, j] = K[j, vv] = epsilon\n                else:\n                    K[vv, j] = K[j, vv] = (2 / ncon *\n                        np.sum(np.logical_and(c1[use], c2[use])) + epsilon)\n\n            Ff = Fd * Fk * np.logical_not(A)\n\n        return A\n    \n    def neighbors_gen(A, K, D, m, eta, gamma, model_var):\n        K += epsilon\n\n        mseed = np.size(np.where(A.flat))//2\n\n        if type(model_var) == tuple:\n            mv1, mv2 = model_var\n        else:\n            mv1, mv2 = model_var, model_var\n\n        if mv1 in ('powerlaw', 'power_law'):\n            Fd = D**eta\n        elif mv1 in ('exponential',):\n            Fd = np.exp(eta*D) \n\n        if mv2 in ('powerlaw', 'power_law'):\n            Fk = K**gamma\n        elif mv2 in ('exponential',):\n            Fk = np.exp(gamma*K) \n\n        Ff = Fd * Fk * np.logical_not(A)\n        u,v = np.where(np.triu(np.ones((n,n)), 1))\n    \n        for ii in range(mseed, m):\n            C = np.append(0, np.cumsum(Ff[u,v]))\n            r = np.sum(rng.random_sample()*C[-1] >= C)\n            uu = u[r]\n            vv = v[r]\n            A[uu, vv] = A[vv, uu] = 1\n\n            x = A[uu, :].astype(int)\n            y = A[:, vv].astype(int)\n    \n            K[uu, y] += 1\n            K[y, uu] += 1\n            K[vv, x] += 1\n            K[x, vv] += 1\n\n            if mv2 in ('powerlaw', 'power_law'):\n                Fk = K**gamma\n            elif mv2 in ('exponential',):\n                Fk = np.exp(gamma*K) \n\n            if mv2 in ('powerlaw', 'power_law'):\n                Ff[uu, y] = Ff[y, uu] = Fd[uu, y] * (K[uu, y] ** gamma)\n                Ff[vv, x] = Ff[x, vv] = Fd[vv, x] * (K[vv, x] ** gamma)\n            elif mv2 in ('exponential',):\n                Ff[uu, y] = Ff[y, uu] = Fd[uu, y] * np.exp(gamma * K[uu, y])\n                Ff[vv, x] = Ff[x, vv] = Fd[vv, x] * np.exp(gamma * K[vv, x])\n\n            Ff[np.where(A)] = 0\n\n        return A\n\n    def euclidean_gen(A, D, m, eta, model_var):\n        mseed = np.size(np.where(A.flat))//2\n\n        if type(model_var) == tuple:\n            mv1, mv2 = model_var\n        else:\n            mv1, mv2 = model_var, model_var\n\n        if mv1 != mv2:\n            raise BCTParamError('Too many hyperparameters specified')\n\n        if mv1 in ('powerlaw', 'power_law'):\n            Fd = D ** eta\n        elif mv1 in ('exponential',):\n            Fd = np.exp(eta ** D)\n\n        u,v = np.where(np.triu(np.ones((n,n)), 1))\n        P = Fd * np.logical_not(A)\n\n        b = np.zeros((m,), dtype=int)\n        b[:mseed] = np.squeeze(np.where(A[u, v]))\n        for i in range(mseed, m):\n            C = np.append(0, np.cumsum(P[u, v]))\n            r = np.sum(rng.random_sample()*C[-1] >= C)\n            b[i] = r\n            P = Fd\n            P[u[b[:i]], v[b[:i]]] = P[v[b[:i]], u[b[:i]]] = 0\n\n            A[u[r], v[r]] = A[v[r], u[r]] = 1\n\n        return A\n\n    if model_type in ('clu-avg', 'clu_avg'):\n        Kseed = k_avg(clustering_coef_bu(A))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = clu_gen(A, Kseed, D, m, ep, gp, model_var, x_avg)\n\n    elif model_type in ('clu-diff', 'clu_diff'):\n        Kseed = k_diff(clustering_coef_bu(A))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = clu_gen(A, Kseed, D, m, ep, gp, model_var, x_diff)\n\n    elif model_type in ('clu-max', 'clu_max'):\n        Kseed = k_max(clustering_coef_bu(A))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = clu_gen(A, Kseed, D, m, ep, gp, model_var, x_max) \n\n    elif model_type in ('clu-min', 'clu_min'):\n        Kseed = k_min(clustering_coef_bu(A))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = clu_gen(A, Kseed, D, m, ep, gp, model_var, x_min) \n\n    elif model_type in ('clu-prod', 'clu_prod'):\n        Kseed = k_prod(clustering_coef_bu(A))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = clu_gen(A, Kseed, D, m, ep, gp, model_var, x_prod)\n\n    elif model_type in ('deg-avg', 'deg_avg'):\n        Kseed = k_avg(np.sum(A, axis=1))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = deg_gen(A, Kseed, D, m, ep, gp, model_var, s_avg)\n\n    elif model_type in ('deg-diff', 'deg_diff'):\n        Kseed = k_diff(np.sum(A, axis=1))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = deg_gen(A, Kseed, D, m, ep, gp, model_var, s_diff)\n    \n    elif model_type in ('deg-max', 'deg_max'):\n        Kseed = k_max(np.sum(A, axis=1))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = deg_gen(A, Kseed, D, m, ep, gp, model_var, s_max)\n\n    elif model_type in ('deg-min', 'deg_min'):\n        Kseed = k_min(np.sum(A, axis=1))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = deg_gen(A, Kseed, D, m, ep, gp, model_var, s_min)\n\n    elif model_type in ('deg-prod', 'deg_prod'):\n        Kseed = k_prod(np.sum(A, axis=1))\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = deg_gen(A, Kseed, D, m, ep, gp, model_var, s_prod)\n\n    elif model_type in ('neighbors',):\n        Kseed = np.inner(A, A)\n        np.fill_diagonal(Kseed, 0)\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = neighbors_gen(A, Kseed, D, m, ep, gp, model_var)\n\n    elif model_type in ('matching', 'matching-ind', 'matching_ind'):\n        mi, _, _ = matching_ind(A)\n        Kseed = mi + mi.T\n        for j, (ep, gp) in enumerate(zip(eta, gamma)):\n            B[:,:,j] = matching_gen(A, Kseed, D, m, ep, gp, model_var)\n\n    elif model_type in ('spatial', 'geometric', 'euclidean'):\n        for j, ep in enumerate(eta):\n            B[:,:,j] = euclidean_gen(A, D, m, ep, model_var) \n\n    return np.squeeze(B)", "response": "Generates a synthetic network of the same type as the given model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate_generative_model(A, Atgt, D, eta, gamma=None, \n    model_type='matching', model_var='powerlaw', epsilon=1e-6, seed=None):\n    '''\n    Generates synthetic networks with parameters provided and evaluates their\n    energy function. The energy function is defined as in Betzel et al. 2016.\n    Basically it takes the Kolmogorov-Smirnov statistics of 4 network\n    measures; comparing the degree distributions, clustering coefficients,\n    betweenness centrality, and Euclidean distances between connected regions.\n    \n    The energy is globally low if the synthetic network matches the target.\n    Energy is defined as the maximum difference across the four statistics.\n    '''\n    m = np.size(np.where(Atgt.flat))//2\n    n = len(Atgt)\n    xk = np.sum(Atgt, axis=1)\n    xc = clustering_coef_bu(Atgt)\n    xb = betweenness_bin(Atgt)\n    xe = D[np.triu(Atgt, 1) > 0]\n\n    B = generative_model(A, D, m, eta, gamma, model_type=model_type, \n                         model_var=model_var, epsilon=epsilon, copy=True, seed=seed)\n\n    #if eta != gamma then an error is thrown within generative model\n    \n    nB = len(eta)\n\n    if nB == 1:\n        B = np.reshape(B, np.append(np.shape(B), 1))\n\n    K = np.zeros((nB, 4))\n\n    def kstats(x, y):\n        bin_edges = np.concatenate([[-np.inf],\n                                    np.sort(np.concatenate((x, y))), \n                                    [np.inf]])\n\n        bin_x,_ = np.histogram(x, bin_edges)\n        bin_y,_ = np.histogram(y, bin_edges)\n\n        #print(np.shape(bin_x))\n\n        sum_x = np.cumsum(bin_x) / np.sum(bin_x)\n        sum_y = np.cumsum(bin_y) / np.sum(bin_y)\n\n        cdfsamp_x = sum_x[:-1]\n        cdfsamp_y = sum_y[:-1]\n\n        delta_cdf = np.abs(cdfsamp_x - cdfsamp_y)\n\n        print(np.shape(delta_cdf))\n        #print(delta_cdf)\n        print(np.argmax(delta_cdf), np.max(delta_cdf))\n\n        return np.max(delta_cdf)\n\n    for ib in range(nB):\n        Bc = B[:,:,ib]\n        yk = np.sum(Bc, axis=1)\n        yc = clustering_coef_bu(Bc)\n        yb = betweenness_bin(Bc)\n        ye = D[np.triu(Bc, 1) > 0]\n\n        K[ib, 0] = kstats(xk, yk)\n        K[ib, 1] = kstats(xc, yc)\n        K[ib, 2] = kstats(xb, yb)\n        K[ib, 3] = kstats(xe, ye)\n\n    return np.max(K, axis=1)", "response": "Evaluate the Kolmogorov - Smirnov model for a synthetic network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef betweenness_bin(G):\n    '''\n    Node betweenness centrality is the fraction of all shortest paths in\n    the network that contain a given node. Nodes with high values of\n    betweenness centrality participate in a large number of shortest paths.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed/undirected connection matrix\n\n    BC : Nx1 np.ndarray\n        node betweenness centrality vector\n\n    Notes\n    -----\n    Betweenness centrality may be normalised to the range [0,1] as\n    BC/[(N-1)(N-2)], where N is the number of nodes in the network.\n    '''\n    G = np.array(G, dtype=float)  # force G to have float type so it can be\n    # compared to float np.inf\n\n    n = len(G)  # number of nodes\n    I = np.eye(n)  # identity matrix\n    d = 1  # path length\n    NPd = G.copy()  # number of paths of length |d|\n    NSPd = G.copy()  # number of shortest paths of length |d|\n    NSP = G.copy()  # number of shortest paths of any length\n    L = G.copy()  # length of shortest paths\n\n    NSP[np.where(I)] = 1\n    L[np.where(I)] = 1\n\n    # calculate NSP and L\n    while np.any(NSPd):\n        d += 1\n        NPd = np.dot(NPd, G)\n        NSPd = NPd * (L == 0)\n        NSP += NSPd\n        L = L + d * (NSPd != 0)\n\n    L[L == 0] = np.inf  # L for disconnected vertices is inf\n    L[np.where(I)] = 0\n    NSP[NSP == 0] = 1  # NSP for disconnected vertices is 1\n\n    DP = np.zeros((n, n))  # vertex on vertex dependency\n    diam = d - 1\n\n    # calculate DP\n    for d in range(diam, 1, -1):\n        DPd1 = np.dot(((L == d) * (1 + DP) / NSP), G.T) * \\\n            ((L == (d - 1)) * NSP)\n        DP += DPd1\n\n    return np.sum(DP, axis=0)", "response": "Returns the fraction of all shortest paths in a given node in the network that contain a given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef betweenness_wei(G):\n    '''\n    Node betweenness centrality is the fraction of all shortest paths in\n    the network that contain a given node. Nodes with high values of\n    betweenness centrality participate in a large number of shortest paths.\n\n    Parameters\n    ----------\n    L : NxN np.ndarray\n        directed/undirected weighted connection matrix\n\n    Returns\n    -------\n    BC : Nx1 np.ndarray\n        node betweenness centrality vector\n\n    Notes\n    -----\n       The input matrix must be a connection-length matrix, typically\n        obtained via a mapping from weight to length. For instance, in a\n        weighted correlation network higher correlations are more naturally\n        interpreted as shorter distances and the input matrix should\n        consequently be some inverse of the connectivity matrix.\n       Betweenness centrality may be normalised to the range [0,1] as\n        BC/[(N-1)(N-2)], where N is the number of nodes in the network.\n    '''\n    n = len(G)\n    BC = np.zeros((n,))  # vertex betweenness\n\n    for u in range(n):\n        D = np.tile(np.inf, (n,))\n        D[u] = 0  # distance from u\n        NP = np.zeros((n,))\n        NP[u] = 1  # number of paths from u\n        S = np.ones((n,), dtype=bool)  # distance permanence\n        P = np.zeros((n, n))  # predecessors\n        Q = np.zeros((n,), dtype=int)  # indices\n        q = n - 1  # order of non-increasing distance\n\n        G1 = G.copy()\n        V = [u]\n        while True:\n            S[V] = 0  # distance u->V is now permanent\n            G1[:, V] = 0  # no in-edges as already shortest\n            for v in V:\n                Q[q] = v\n                q -= 1\n                W, = np.where(G1[v, :])  # neighbors of v\n                for w in W:\n                    Duw = D[v] + G1[v, w]  # path length to be tested\n                    if Duw < D[w]:  # if new u->w shorter than old\n                        D[w] = Duw\n                        NP[w] = NP[v]  # NP(u->w) = NP of new path\n                        P[w, :] = 0\n                        P[w, v] = 1  # v is the only predecessor\n                    elif Duw == D[w]:  # if new u->w equal to old\n                        NP[w] += NP[v]  # NP(u->w) sum of old and new\n                        P[w, v] = 1  # v is also predecessor\n\n            if D[S].size == 0:\n                break  # all nodes were reached\n            if np.isinf(np.min(D[S])):  # some nodes cannot be reached\n                Q[:q + 1], = np.where(np.isinf(D))  # these are first in line\n                break\n            V, = np.where(D == np.min(D[S]))\n\n        DP = np.zeros((n,))\n        for w in Q[:n - 1]:\n            BC[w] += DP[w]\n            for v in np.where(P[w, :])[0]:\n                DP[v] += (1 + DP[w]) * NP[v] / NP[w]\n\n    return BC", "response": "Returns the fraction of all shortest paths in a given node in a network that contain a given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diversity_coef_sign(W, ci):\n    '''\n    The Shannon-entropy based diversity coefficient measures the diversity\n    of intermodular connections of individual nodes and ranges from 0 to 1.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected connection matrix with positive and negative weights\n    ci : Nx1 np.ndarray\n        community affiliation vector\n\n    Returns\n    -------\n    Hpos : Nx1 np.ndarray\n        diversity coefficient based on positive connections\n    Hneg : Nx1 np.ndarray\n        diversity coefficient based on negative connections\n    '''\n    n = len(W)  # number of nodes\n\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n\n    m = np.max(ci)  # number of modules\n\n    def entropy(w_):\n        S = np.sum(w_, axis=1)  # strength\n        Snm = np.zeros((n, m))  # node-to-module degree\n        for i in range(m):\n            Snm[:, i] = np.sum(w_[:, ci == i + 1], axis=1)\n        pnm = Snm / (np.tile(S, (m, 1)).T)\n        pnm[np.isnan(pnm)] = 0\n        pnm[np.logical_not(pnm)] = 1\n        return -np.sum(pnm * np.log(pnm), axis=1) / np.log(m)\n\n    #explicitly ignore compiler warning for division by zero\n    with np.errstate(invalid='ignore'):\n        Hpos = entropy(W * (W > 0))\n        Hneg = entropy(-W * (W < 0))\n\n    return Hpos, Hneg", "response": "The Shannon - entropy based diversity coefficient measures the diversity of intermodular connections of individual nodes and ranges from 0 to 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef edge_betweenness_bin(G):\n    '''\n    Edge betweenness centrality is the fraction of all shortest paths in\n    the network that contain a given edge. Edges with high values of\n    betweenness centrality participate in a large number of shortest paths.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed/undirected connection matrix\n\n    Returns\n    -------\n    EBC : NxN np.ndarray\n        edge betweenness centrality matrix\n    BC : Nx1 np.ndarray\n        node betweenness centrality vector\n\n    Notes\n    -----\n    Betweenness centrality may be normalised to the range [0,1] as\n    BC/[(N-1)(N-2)], where N is the number of nodes in the network.\n    '''\n    n = len(G)\n    BC = np.zeros((n,))  # vertex betweenness\n    EBC = np.zeros((n, n))  # edge betweenness\n\n    for u in range(n):\n        D = np.zeros((n,))\n        D[u] = 1  # distance from u\n        NP = np.zeros((n,))\n        NP[u] = 1  # number of paths from u\n        P = np.zeros((n, n))  # predecessors\n        Q = np.zeros((n,), dtype=int)  # indices\n        q = n - 1  # order of non-increasing distance\n\n        Gu = G.copy()\n        V = np.array([u])\n        while V.size:\n            Gu[:, V] = 0  # remove remaining in-edges\n            for v in V:\n                Q[q] = v\n                q -= 1\n                W, = np.where(Gu[v, :])  # neighbors of V\n                for w in W:\n                    if D[w]:\n                        NP[w] += NP[v]  # NP(u->w) sum of old and new\n                        P[w, v] = 1  # v is a predecessor\n                    else:\n                        D[w] = 1\n                        NP[w] = NP[v]  # NP(u->v) = NP of new path\n                        P[w, v] = 1  # v is a predecessor\n            V, = np.where(np.any(Gu[V, :], axis=0))\n\n        if np.any(np.logical_not(D)):  # if some vertices unreachable\n            Q[:q], = np.where(np.logical_not(D))  # ...these are first in line\n\n        DP = np.zeros((n,))\t\t\t\t# dependency\n        for w in Q[:n - 1]:\n            BC[w] += DP[w]\n            for v in np.where(P[w, :])[0]:\n                DPvw = (1 + DP[w]) * NP[v] / NP[w]\n                DP[v] += DPvw\n                EBC[v, w] += DPvw\n\n    return EBC, BC", "response": "Computes the fraction of all shortest paths in a given edge betweenness centrality in a large number of shortest paths in a given binary directed or undirected connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the eigenvector centrality of the undirected adjacency matrix.", "response": "def eigenvector_centrality_und(CIJ):\n    '''\n    Eigenector centrality is a self-referential measure of centrality:\n    nodes have high eigenvector centrality if they connect to other nodes\n    that have high eigenvector centrality. The eigenvector centrality of\n    node i is equivalent to the ith element in the eigenvector\n    corresponding to the largest eigenvalue of the adjacency matrix.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary/weighted undirected adjacency matrix\n\n    v : Nx1 np.ndarray\n        eigenvector associated with the largest eigenvalue of the matrix\n    '''\n    from scipy import linalg\n\n    n = len(CIJ)\n    vals, vecs = linalg.eig(CIJ)\n    i = np.argmax(vals)\n    return np.abs(vecs[:, i])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef erange(CIJ):\n    '''\n    Shortcuts are central edges which significantly reduce the\n    characteristic path length in the network.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    Erange : NxN np.ndarray\n        range for each edge, i.e. the length of the shortest path from i to j\n        for edge c(i,j) after the edge has been removed from the graph\n    eta : float\n        average range for the entire graph\n    Eshort : NxN np.ndarray\n        entries are ones for shortcut edges\n    fs : float\n        fractions of shortcuts in the graph\n\n    Follows the treatment of 'shortcuts' by Duncan Watts\n    '''\n    N = len(CIJ)\n    K = np.size(np.where(CIJ)[1])\n    Erange = np.zeros((N, N))\n    i, j = np.where(CIJ)\n\n    for c in range(len(i)):\n        CIJcut = CIJ.copy()\n        CIJcut[i[c], j[c]] = 0\n        R, D = reachdist(CIJcut)\n        Erange[i[c], j[c]] = D[i[c], j[c]]\n\n    # average range (ignore Inf)\n    eta = (np.sum(Erange[np.logical_and(Erange > 0, Erange < np.inf)]) /\n           len(Erange[np.logical_and(Erange > 0, Erange < np.inf)]))\n\n    # Original entries of D are ones, thus entries of Erange\n    # must be two or greater.\n    # If Erange(i,j) > 2, then the edge is a shortcut.\n    # 'fshort' is the fraction of shortcuts over the entire graph.\n\n    Eshort = Erange > 2\n    fs = len(np.where(Eshort)) / K\n\n    return Erange, eta, Eshort, fs", "response": "Returns the range of the shortest path of each edge in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the flow coefficient for each node and averaged over the network.", "response": "def flow_coef_bd(CIJ):\n    '''\n    Computes the flow coefficient for each node and averaged over the\n    network, as described in Honey et al. (2007) PNAS. The flow coefficient\n    is similar to betweenness centrality, but works on a local\n    neighborhood. It is mathematically related to the clustering\n    coefficient  (cc) at each node as, fc+cc <= 1.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    fc : Nx1 np.ndarray\n        flow coefficient for each node\n    FC : float\n        average flow coefficient over the network\n    total_flo : int\n        number of paths that \"flow\" across the central node\n    '''\n    N = len(CIJ)\n\n    fc = np.zeros((N,))\n    total_flo = np.zeros((N,))\n    max_flo = np.zeros((N,))\n\n    # loop over nodes\n    for v in range(N):\n        # find neighbors - note: both incoming and outgoing connections\n        nb, = np.where(CIJ[v, :] + CIJ[:, v].T)\n        fc[v] = 0\n        if np.where(nb)[0].size:\n            CIJflo = -CIJ[np.ix_(nb, nb)]\n            for i in range(len(nb)):\n                for j in range(len(nb)):\n                    if CIJ[nb[i], v] and CIJ[v, nb[j]]:\n                        CIJflo[i, j] += 1\n            total_flo[v] = np.sum(\n                (CIJflo == 1) * np.logical_not(np.eye(len(nb))))\n            max_flo[v] = len(nb) * len(nb) - len(nb)\n            fc[v] = total_flo[v] / max_flo[v]\n\n    fc[np.isnan(fc)] = 0\n    FC = np.mean(fc)\n\n    return fc, FC, total_flo"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gateway_coef_sign(W, ci, centrality_type='degree'):\n    '''\n    The gateway coefficient is a variant of participation coefficient.\n    It is weighted by how critical the connections are to intermodular\n    connectivity (e.g. if a node is the only connection between its\n    module and another module, it will have a higher gateway coefficient,\n    unlike participation coefficient).\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected signed connection matrix\n    ci : Nx1 np.ndarray\n        community affiliation vector\n    centrality_type : enum\n        'degree' - uses the weighted degree (i.e, node strength)\n        'betweenness' - uses the betweenness centrality\n\n    Returns\n    -------\n    Gpos : Nx1 np.ndarray\n        gateway coefficient for positive weights\n    Gneg : Nx1 np.ndarray\n        gateway coefficient for negative weights\n\n    Reference:\n        Vargas ER, Wahl LM, Eur Phys J B (2014) 87:1-10\n    '''\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n    n = len(W)\n    np.fill_diagonal(W, 0)\n\n    def gcoef(W):\n        #strength\n        s = np.sum(W, axis=1)\n        #neighbor community affiliation\n        Gc = np.inner((W != 0), np.diag(ci))\n        #community specific neighbors\n        Sc2 = np.zeros((n,))\n        #extra modular weighting\n        ksm = np.zeros((n,))\n        #intra modular wieghting\n        centm = np.zeros((n,))\n\n        if centrality_type == 'degree':\n            cent = s.copy()\n        elif centrality_type == 'betweenness':\n            cent = betweenness_wei(invert(W))\n\n        nr_modules = int(np.max(ci))\n        for i in range(1, nr_modules+1):\n            ks = np.sum(W * (Gc == i), axis=1)\n            print(np.sum(ks))\n            Sc2 += ks ** 2\n            for j in range(1, nr_modules+1):\n                #calculate extramodular weights\n                ksm[ci == j] += ks[ci == j] / np.sum(ks[ci == j])\n\n            #calculate intramodular weights\n            centm[ci == i] = np.sum(cent[ci == i])\n\n        #print(Gc)\n        #print(centm)\n        #print(ksm)\n        #print(ks)\n\n        centm = centm / max(centm)\n        #calculate total weights\n        gs = (1 - ksm * centm) ** 2\n\n        Gw = 1 - Sc2 * gs / s ** 2\n        Gw[np.where(np.isnan(Gw))] = 0\n        Gw[np.where(np.logical_not(Gw))] = 0\n\n        return Gw\n\n    G_pos = gcoef(W * (W > 0))\n    G_neg = gcoef(-W * (W < 0))\n    return G_pos, G_neg", "response": "Returns a new undirected signed connection matrix with the given participation coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the within - module degree z - score of a given community affiliation vector.", "response": "def module_degree_zscore(W, ci, flag=0):\n    '''\n    The within-module degree z-score is a within-module version of degree\n    centrality.\n\n    Parameters\n    ----------\n    W : NxN np.narray\n        binary/weighted directed/undirected connection matrix\n    ci : Nx1 np.array_like\n        community affiliation vector\n    flag : int\n        Graph type. 0: undirected graph (default)\n                    1: directed graph in degree\n                    2: directed graph out degree\n                    3: directed graph in and out degree\n\n    Returns\n    -------\n    Z : Nx1 np.ndarray\n        within-module degree Z-score\n    '''\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n\n    if flag == 2:\n        W = W.copy()\n        W = W.T\n    elif flag == 3:\n        W = W.copy()\n        W = W + W.T\n\n    n = len(W)\n    Z = np.zeros((n,))  # number of vertices\n    for i in range(1, int(np.max(ci) + 1)):\n        Koi = np.sum(W[np.ix_(ci == i, ci == i)], axis=1)\n        Z[np.where(ci == i)] = (Koi - np.mean(Koi)) / np.std(Koi)\n\n    Z[np.where(np.isnan(Z))] = 0\n    return Z"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pagerank_centrality(A, d, falff=None):\n    '''\n    The PageRank centrality is a variant of eigenvector centrality. This\n    function computes the PageRank centrality of each vertex in a graph.\n\n    Formally, PageRank is defined as the stationary distribution achieved\n    by instantiating a Markov chain on a graph. The PageRank centrality of\n    a given vertex, then, is proportional to the number of steps (or amount\n    of time) spent at that vertex as a result of such a process.\n\n    The PageRank index gets modified by the addition of a damping factor,\n    d. In terms of a Markov chain, the damping factor specifies the\n    fraction of the time that a random walker will transition to one of its\n    current state's neighbors. The remaining fraction of the time the\n    walker is restarted at a random vertex. A common value for the damping\n    factor is d = 0.85.\n\n    Parameters\n    ----------\n    A : NxN np.narray\n        adjacency matrix\n    d : float\n        damping factor (see description)\n    falff : Nx1 np.ndarray | None\n        Initial page rank probability, non-negative values. Default value is\n        None. If not specified, a naive bayesian prior is used.\n\n    Returns\n    -------\n    r : Nx1 np.ndarray\n        vectors of page rankings\n\n    Notes\n    -----\n    Note: The algorithm will work well for smaller matrices (number of\n    nodes around 1000 or less)\n    '''\n    from scipy import linalg\n\n    N = len(A)\n    if falff is None:\n        norm_falff = np.ones((N,)) / N\n    else:\n        norm_falff = falff / np.sum(falff)\n\n    deg = np.sum(A, axis=0)\n    deg[deg == 0] = 1\n    D1 = np.diag(1 / deg)\n    B = np.eye(N) - d * np.dot(A, D1)\n    b = (1 - d) * norm_falff\n    r = linalg.solve(B, b)\n    r /= np.sum(r)\n    return r", "response": "This function computes the PageRank centrality of each vertex in a graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef participation_coef(W, ci, degree='undirected'):\n    '''\n    Participation coefficient is a measure of diversity of intermodular\n    connections of individual nodes.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        binary/weighted directed/undirected connection matrix\n    ci : Nx1 np.ndarray\n        community affiliation vector\n    degree : str\n        Flag to describe nature of graph 'undirected': For undirected graphs\n                                         'in': Uses the in-degree\n                                         'out': Uses the out-degree\n\n    Returns\n    -------\n    P : Nx1 np.ndarray\n        participation coefficient\n    '''\n    if degree == 'in':\n        W = W.T\n\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n\n    n = len(W)  # number of vertices\n    Ko = np.sum(W, axis=1)  # (out) degree\n    Gc = np.dot((W != 0), np.diag(ci))  # neighbor community affiliation\n    Kc2 = np.zeros((n,))  # community-specific neighbors\n\n    for i in range(1, int(np.max(ci)) + 1):\n        Kc2 += np.square(np.sum(W * (Gc == i), axis=1))\n\n    P = np.ones((n,)) - Kc2 / np.square(Ko)\n    # P=0 if for nodes with no (out) neighbors\n    P[np.where(np.logical_not(Ko))] = 0\n\n    return P", "response": "Returns the participation coefficient of a given node in the undirected graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef participation_coef_sparse(W, ci, degree='undirected'):\n\t'''\n\tParticipation coefficient is a measure of diversity of intermodular\n\tconnections of individual nodes.\n\tParameters\n\t----------\n\tW : NxN np.ndarray\n\t\tbinary/weighted directed/undirected connection\n\t\tmust be as scipy.sparse.csr matrix\n\tci : Nx1 np.ndarray\n\t\tcommunity affiliation vector\n\tdegree : str\n\t\tFlag to describe nature of graph 'undirected': For undirected graphs\n\t\t\t\t\t\t\t\t\t\t 'in': Uses the in-degree\n\t\t\t\t\t\t\t\t\t\t 'out': Uses the out-degree\n\tReturns\n\t-------\n\tP : Nx1 np.ndarray\n\t\tparticipation coefficient\n\t'''\n\tif degree == 'in':\n\t\tW = W.T\n\n\t_, ci = np.unique(ci, return_inverse=True)\n\tci += 1\n\n\tn = W.shape[0]  # number of vertices\n\tKo = np.array(W.sum(axis=1)).flatten().astype(float)  # (out) degree\n\tGc = W.copy().astype('int16')\n\tGc[Gc!=0] = 1 \n\tGc = Gc * np.diag(ci)# neighbor community affiliation\n\t\n\tP = np.zeros((n))\n\tfor i in range(1, int(np.max(ci)) + 1):\n\t\tP = P + (np.array((W.multiply(Gc == i).astype(int)).sum(axis=1)).flatten() / Ko)**2\n\tP = 1 - P\n\t# P=0 if for nodes with no (out) neighbors\n\tP[np.where(np.logical_not(Ko))] = 0\n\n\treturn P", "response": "This function computes the participation coefficient of a given node in a directed graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef participation_coef_sign(W, ci):\n    '''\n    Participation coefficient is a measure of diversity of intermodular\n    connections of individual nodes.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected connection matrix with positive and negative weights\n    ci : Nx1 np.ndarray\n        community affiliation vector\n\n    Returns\n    -------\n    Ppos : Nx1 np.ndarray\n        participation coefficient from positive weights\n    Pneg : Nx1 np.ndarray\n        participation coefficient from negative weights\n    '''\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n\n    n = len(W)  # number of vertices\n\n    def pcoef(W_):\n        S = np.sum(W_, axis=1)  # strength\n        # neighbor community affil.\n        Gc = np.dot(np.logical_not(W_ == 0), np.diag(ci))\n        Sc2 = np.zeros((n,))\n\n        for i in range(1, int(np.max(ci) + 1)):\n            Sc2 += np.square(np.sum(W_ * (Gc == i), axis=1))\n\n        P = np.ones((n,)) - Sc2 / np.square(S)\n        P[np.where(np.isnan(P))] = 0\n        P[np.where(np.logical_not(P))] = 0  # p_ind=0 if no (out)neighbors\n        return P\n\n    #explicitly ignore compiler warning for division by zero\n    with np.errstate(invalid='ignore'):\n        Ppos = pcoef(W * (W > 0))\n        Pneg = pcoef(-W * (W < 0))\n\n    return Ppos, Pneg", "response": "Returns the participation coefficient of a single node in the community."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_motif34(m, n=None):\n    '''\n    This function returns all motif isomorphs for a given motif id and\n    class (3 or 4). The function also returns the motif id for a given\n    motif matrix\n\n    1. Input:       Motif_id,           e.g. 1 to 13, if class is 3\n                 Motif_class,        number of nodes, 3 or 4.\n    Output:      Motif_matrices,     all isomorphs for the given motif\n\n    2. Input:       Motif_matrix        e.g. [0 1 0; 0 0 1; 1 0 0]\n    Output       Motif_id            e.g. 1 to 13, if class is 3\n\n    Parameters\n    ----------\n    m : int | matrix\n        In use case 1, a motif_id which is an integer.\n        In use case 2, the entire matrix of the motif\n        (e.g. [0 1 0; 0 0 1; 1 0 0])\n    n : int | None\n        In use case 1, the motif class, which is the number of nodes. This is\n        either 3 or 4.\n        In use case 2, None.\n\n    Returns\n    -------\n    M : np.ndarray | int\n        In use case 1, returns all isomorphs for the given motif\n        In use case 2, returns the motif_id for the specified motif matrix\n    '''\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    z = (0,)\n    if n == 3:\n        mot = io.loadmat(fname)\n        m3 = mot['m3']\n        id3 = mot['id3'].squeeze()\n        ix, = np.where(id3 == m)\n        M = np.zeros((3, 3, len(ix)))\n        for i, ind in enumerate(ix):\n            M[:, :, i] = np.reshape(np.concatenate(\n                (z, m3[ind, 0:3], z, m3[ind, 3:6], z)), (3, 3))\n    elif n == 4:\n        mot = io.loadmat(fname)\n        m4 = mot['m4']\n        id4 = mot['id4'].squeeze()\n        ix, = np.where(id4 == m)\n        M = np.zeros((4, 4, len(ix)))\n        for i, ind in enumerate(ix):\n            M[:, :, i] = np.reshape(np.concatenate(\n                (z, m4[ind, 0:4], z, m4[ind, 4:8], z, m4[ind, 8:12], z)), (4, 4))\n    elif n is None:\n        try:\n            m = np.array(m)\n        except TypeError:\n            raise BCTParamError('motif matrix must be an array-like')\n        if m.shape[0] == 3:\n            M, = np.where(motif3struct_bin(m))\n        elif m.shape[0] == 4:\n            M, = np.where(motif4struct_bin(m))\n        else:\n            raise BCTParamError('motif matrix must be 3x3 or 4x4')\n    else:\n        raise BCTParamError('Invalid motif class, must be 3, 4, or None')\n\n    return M", "response": "This function returns all motif isomorphs for a given motif id and motif class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef motif3funct_wei(W):\n    '''\n    Functional motifs are subsets of connection patterns embedded within\n    anatomical motifs. Motif frequency is the frequency of occurrence of\n    motifs around a node. Motif intensity and coherence are weighted\n    generalizations of motif frequency.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted directed connection matrix (all weights between 0 and 1)\n\n    Returns\n    -------\n    I : 13xN np.ndarray\n        motif intensity matrix\n    Q : 13xN np.ndarray\n        motif coherence matrix\n    F : 13xN np.ndarray\n        motif frequency matrix\n\n    Notes\n    -----\n    Average intensity and coherence are given by I./F and Q./F.\n    '''\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    mot = io.loadmat(fname)\n    m3 = mot['m3']\n    id3 = mot['id3'].squeeze()\n    n3 = mot['n3'].squeeze()\n\n    n = len(W)\n    I = np.zeros((13, n))  # intensity\n    Q = np.zeros((13, n))  # coherence\n    F = np.zeros((13, n))  # frequency\n\n    A = binarize(W, copy=True)  # create binary adjmat\n    As = np.logical_or(A, A.T)  # symmetrized adjmat\n\n    for u in range(n - 2):\n        # v1: neighbors of u (>u)\n        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])\n        for v1 in np.where(V1)[0]:\n            # v2: neighbors of v1 (>u)\n            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])\n            V2[V1] = 0  # not already in V1\n            # and all neighbors of u (>v1)\n            V2 = np.logical_or(\n                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)\n            for v2 in np.where(V2)[0]:\n                a = np.array((A[v1, u], A[v2, u], A[u, v1],\n                              A[v2, v1], A[u, v2], A[v1, v2]))\n                ix = (np.dot(m3, a) == n3)\n                m = np.sum(ix)\n\n                w = np.array((W[v1, u], W[v2, u], W[u, v1],\n                              W[v2, v1], W[u, v2], W[v1, v2]))\n\n                M = m3[ix, :] * np.tile(w, (m, 1))\n                id = id3[ix] - 1\n                l = n3[ix]\n                x = np.sum(M, axis=1) / l  # arithmetic mean\n                M[M == 0] = 1  # enable geometric mean\n                i = np.prod(M, axis=1)**(1 / l)  # intensity\n                q = i / x  # coherence\n\n                # unique motif occurrences\n                idu, jx = np.unique(id, return_index=True)\n                jx = np.append((0,), jx + 1)\n\n                mu = len(idu)  # number of unique motifs\n                i2, q2, f2 = np.zeros((3, mu))\n\n                for h in range(mu):\n                    i2[h] = np.sum(i[jx[h] + 1:jx[h + 1] + 1])\n                    q2[h] = np.sum(q[jx[h] + 1:jx[h + 1] + 1])\n                    f2[h] = jx[h + 1] - jx[h]\n\n                # then add to cumulative count\n                I[idu, u] += i2\n                I[idu, v1] += i2\n                I[idu, v2] += i2\n                Q[idu, u] += q2\n                Q[idu, v1] += q2\n                Q[idu, v2] += q2\n                F[idu, u] += f2\n                F[idu, v1] += f2\n                F[idu, v2] += f2\n\n    return I, Q, F", "response": "Functional motifs are subsets of connection patterns embedded within a motif."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef motif3struct_bin(A):\n    '''\n    Structural motifs are patterns of local connectivity. Motif frequency\n    is the frequency of occurrence of motifs around a node.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    F : 13xN np.ndarray\n        motif frequency matrix\n    f : 13x1 np.ndarray\n        motif frequency vector (averaged over all nodes)\n    '''\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    mot = io.loadmat(fname)\n    m3n = mot['m3n']\n    id3 = mot['id3'].squeeze()\n\n    n = len(A)  # number of vertices in A\n    f = np.zeros((13,))  # motif count for whole graph\n    F = np.zeros((13, n))  # motif frequency\n\n    A = binarize(A, copy=True)  # ensure A is binary\n    As = np.logical_or(A, A.T)  # symmetrized adjmat\n\n    for u in range(n - 2):\n        # v1: neighbors of u (>u)\n        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])\n        for v1 in np.where(V1)[0]:\n            # v2: neighbors of v1 (>u)\n            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])\n            V2[V1] = 0  # not already in V1\n            # and all neighbors of u (>v1)\n            V2 = np.logical_or(\n                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)\n            for v2 in np.where(V2)[0]:\n                a = np.array((A[v1, u], A[v2, u], A[u, v1],\n                              A[v2, v1], A[u, v2], A[v1, v2]))\n                s = np.uint32(np.sum(np.power(10, np.arange(5, -1, -1)) * a))\n                ix = id3[np.squeeze(s == m3n)] - 1\n                F[ix, u] += 1\n                F[ix, v1] += 1\n                F[ix, v2] += 1\n                f[ix] += 1\n\n    return f, F", "response": "This function is used to generate a structure of motifs around a node in the motif graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef motif3struct_wei(W):\n    '''\n    Structural motifs are patterns of local connectivity. Motif frequency\n    is the frequency of occurrence of motifs around a node. Motif intensity\n    and coherence are weighted generalizations of motif frequency.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted directed connection matrix (all weights between 0 and 1)\n\n    Returns\n    -------\n    I : 13xN np.ndarray\n        motif intensity matrix\n    Q : 13xN np.ndarray\n        motif coherence matrix\n    F : 13xN np.ndarray\n        motif frequency matrix\n\n    Notes\n    -----\n    Average intensity and coherence are given by I./F and Q./F.\n    '''\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    mot = io.loadmat(fname)\n    m3 = mot['m3']\n    m3n = mot['m3n']\n    id3 = mot['id3'].squeeze()\n    n3 = mot['n3'].squeeze()\n\n    n = len(W)  # number of vertices in W\n    I = np.zeros((13, n))  # intensity\n    Q = np.zeros((13, n))  # coherence\n    F = np.zeros((13, n))  # frequency\n\n    A = binarize(W, copy=True)  # create binary adjmat\n    As = np.logical_or(A, A.T)  # symmetrized adjmat\n\n    for u in range(n - 2):\n        # v1: neighbors of u (>u)\n        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])\n        for v1 in np.where(V1)[0]:\n            # v2: neighbors of v1 (>u)\n            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])\n            V2[V1] = 0  # not already in V1\n            # and all neighbors of u (>v1)\n            V2 = np.logical_or(\n                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)\n            for v2 in np.where(V2)[0]:\n                a = np.array((A[v1, u], A[v2, u], A[u, v1],\n                              A[v2, v1], A[u, v2], A[v1, 2]))\n                s = np.uint32(np.sum(np.power(10, np.arange(5, -1, -1)) * a))\n                ix = np.squeeze(s == m3n)\n\n                w = np.array((W[v1, u], W[v2, u], W[u, v1],\n                              W[v2, v1], W[u, v2], W[v1, v2]))\n\n                M = w * m3[ix, :]\n                id = id3[ix] - 1\n                l = n3[ix]\n                x = np.sum(M, axis=1) / l  # arithmetic mean\n                M[M == 0] = 1  # enable geometric mean\n                i = np.prod(M, axis=1)**(1 / l)  # intensity\n                q = i / x  # coherence\n\n                # add to cumulative counts\n                I[id, u] += i\n                I[id, v1] += i\n                I[id, v2] += i\n                Q[id, u] += q\n                Q[id, v1] += q\n                Q[id, v2] += q\n                F[id, u] += 1\n                F[id, v1] += 1\n                F[id, v1] += 1\n\n    return I, Q, F", "response": "This function returns a structure of motifs that are part of motif 3."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef motif4struct_wei(W):\n    '''\n    Structural motifs are patterns of local connectivity. Motif frequency\n    is the frequency of occurrence of motifs around a node. Motif intensity\n    and coherence are weighted generalizations of motif frequency.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted directed connection matrix (all weights between 0 and 1)\n\n    Returns\n    -------\n    I : 199xN np.ndarray\n        motif intensity matrix\n    Q : 199xN np.ndarray\n        motif coherence matrix\n    F : 199xN np.ndarray\n        motif frequency matrix\n\n    Notes\n    -----\n    Average intensity and coherence are given by I./F and Q./F.\n    '''\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    mot = io.loadmat(fname)\n    m4 = mot['m4']\n    m4n = mot['m4n']\n    id4 = mot['id4'].squeeze()\n    n4 = mot['n4'].squeeze()\n\n    n = len(W)\n    I = np.zeros((199, n))  # intensity\n    Q = np.zeros((199, n))  # coherence\n    F = np.zeros((199, n))  # frequency\n\n    A = binarize(W, copy=True)  # ensure A is binary\n    As = np.logical_or(A, A.T)  # symmetrized adjmat\n\n    for u in range(n - 3):\n        # v1: neighbors of u (>u)\n        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])\n        for v1 in np.where(V1)[0]:\n            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])\n            V2[V1] = 0  # not already in V1\n            # and all neighbors of u (>v1)\n            V2 = np.logical_or(\n                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)\n            for v2 in np.where(V2)[0]:\n                vz = np.max((v1, v2))  # vz: largest rank node\n                # v3: all neighbors of v2 (>u)\n                V3 = np.append(np.zeros((u,), dtype=int), As[v2, u + 1:n + 1])\n                V3[V2] = 0  # not already in V1 and V2\n                # and all neighbors of v1 (>v2)\n                V3 = np.logical_or(\n                    np.append(np.zeros((v2,)), As[v1, v2 + 1:n + 1]), V3)\n                V3[V1] = 0  # not already in V1\n                # and all neighbors of u (>vz)\n                V3 = np.logical_or(\n                    np.append(np.zeros((vz,)), As[u, vz + 1:n + 1]), V3)\n                for v3 in np.where(V3)[0]:\n                    a = np.array((A[v1, u], A[v2, u], A[v3, u], A[u, v1], A[v2, v1],\n                                  A[v3, v1], A[u, v2], A[v1, v2], A[\n                                      v3, v2], A[u, v3], A[v1, v3],\n                                  A[v2, v3]))\n                    s = np.uint64(\n                        np.sum(np.power(10, np.arange(11, -1, -1)) * a))\n                    # print np.shape(s),np.shape(m4n)\n                    ix = np.squeeze(s == m4n)\n\n                    w = np.array((W[v1, u], W[v2, u], W[v3, u], W[u, v1], W[v2, v1],\n                                  W[v3, v1], W[u, v2], W[v1, v2], W[\n                                      v3, v2], W[u, v3], W[v1, v3],\n                                  W[v2, v3]))\n\n                    M = w * m4[ix, :]\n                    id = id4[ix] - 1\n                    l = n4[ix]\n                    x = np.sum(M, axis=1) / l  # arithmetic mean\n                    M[M == 0] = 1  # enable geometric mean\n                    i = np.prod(M, axis=1)**(1 / l)  # intensity\n                    q = i / x  # coherence\n\n                    # then add to cumulative count\n                    I[id, u] += i\n                    I[id, v1] += i\n                    I[id, v2] += i\n                    I[id, v3] += i\n                    Q[id, u] += q\n                    Q[id, v1] += q\n                    Q[id, v2] += q\n                    Q[id, v3] += q\n                    F[id, u] += 1\n                    F[id, v1] += 1\n                    F[id, v2] += 1\n                    F[id, v3] += 1\n\n    return I, Q, F", "response": "This function returns a structure of motifs that are used in motif4struct_wei."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef threshold_absolute(W, thr, copy=True):\n    '''\n    This function thresholds the connectivity matrix by absolute weight\n    magnitude. All weights below the given threshold, and all weights\n    on the main diagonal (self-self connections) are set to 0.\n\n    If copy is not set, this function will *modify W in place.*\n\n    Parameters\n    ----------\n    W : np.ndarray\n        weighted connectivity matrix\n    thr : float\n        absolute weight threshold\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : np.ndarray\n        thresholded connectivity matrix\n    '''\n    if copy:\n        W = W.copy()\n    np.fill_diagonal(W, 0)  # clear diagonal\n    W[W < thr] = 0  # apply threshold\n    return W", "response": "This function thresholds the connectivity matrix by absolute weight thr."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef threshold_proportional(W, p, copy=True):\n    '''\n    This function \"thresholds\" the connectivity matrix by preserving a\n    proportion p (0<p<1) of the strongest weights. All other weights, and\n    all weights on the main diagonal (self-self connections) are set to 0.\n\n    If copy is not set, this function will *modify W in place.*\n\n    Parameters\n    ----------\n    W : np.ndarray\n        weighted connectivity matrix\n    p : float\n        proportional weight threshold (0<p<1)\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : np.ndarray\n        thresholded connectivity matrix\n\n    Notes\n    -----\n    The proportion of elements set to 0 is a fraction of all elements\n    in the matrix, whether or not they are already 0. That is, this function\n    has the following behavior:\n\n    >> x = np.random.random_sample((10,10))\n    >> x_25 = threshold_proportional(x, .25)\n    >> np.size(np.where(x_25)) #note this double counts each nonzero element\n    46\n    >> x_125 = threshold_proportional(x, .125)\n    >> np.size(np.where(x_125))\n    22\n    >> x_test = threshold_proportional(x_25, .5)\n    >> np.size(np.where(x_test))\n    46\n\n    That is, the 50% thresholding of x_25 does nothing because >=50% of the\n    elements in x_25 are aleady <=0. This behavior is the same as in BCT. Be\n    careful with matrices that are both signed and sparse.\n    '''\n    from .miscellaneous_utilities import teachers_round as round\n\n    if p > 1 or p < 0:\n        raise BCTParamError('Threshold must be in range [0,1]')\n    if copy:\n        W = W.copy()\n    n = len(W)\t\t\t\t\t\t# number of nodes\n    np.fill_diagonal(W, 0)\t\t\t# clear diagonal\n\n    if np.allclose(W, W.T):\t\t\t\t# if symmetric matrix\n        W[np.tril_indices(n)] = 0\t\t# ensure symmetry is preserved\n        ud = 2\t\t\t\t\t\t# halve number of removed links\n    else:\n        ud = 1\n\n    ind = np.where(W)\t\t\t\t\t# find all links\n\n    I = np.argsort(W[ind])[::-1]\t\t# sort indices by magnitude\n\n    en = int(round((n * n - n) * p / ud))\t\t# number of links to be preserved\n\n    W[(ind[0][I][en:], ind[1][I][en:])] = 0  # apply threshold\n    #W[np.ix_(ind[0][I][en:], ind[1][I][en:])]=0\n\n    if ud == 2:\t\t\t\t\t\t# if symmetric matrix\n        W[:, :] = W + W.T\t\t\t\t\t\t# reconstruct symmetry\n\n    return W", "response": "This function thresholds the connectivity matrix by preserving strongest weights and the strongest weights."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef binarize(W, copy=True):\n    '''\n    Binarizes an input weighted connection matrix.  If copy is not set, this\n    function will *modify W in place.*\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted connectivity matrix\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : NxN np.ndarray\n        binary connectivity matrix\n    '''\n    if copy:\n        W = W.copy()\n    W[W != 0] = 1\n    return W", "response": "Binarizes an input weighted connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize(W, copy=True):\n    '''\n    Normalizes an input weighted connection matrix.  If copy is not set, this\n    function will *modify W in place.*\n\n    Parameters\n    ----------\n    W : np.ndarray\n        weighted connectivity matrix\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : np.ndarray\n        normalized connectivity matrix\n    '''\n    if copy:\n        W = W.copy()\n    W /= np.max(np.abs(W))\n    return W", "response": "Normalizes an input weighted connectivity matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invert(W, copy=True):\n    '''\n    Inverts elementwise the weights in an input connection matrix.\n    In other words, change the from the matrix of internode strengths to the\n    matrix of internode distances.\n\n    If copy is not set, this function will *modify W in place.*\n\n    Parameters\n    ----------\n    W : np.ndarray\n        weighted connectivity matrix\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : np.ndarray\n        inverted connectivity matrix\n    '''\n    if copy:\n        W = W.copy()\n    E = np.where(W)\n    W[E] = 1. / W[E]\n    return W", "response": "Inverts elementwise the weights in an input connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfixing a bunch of common problems. More specifically remove Inf and NaN and zero diagonal.", "response": "def autofix(W, copy=True):\n    '''\n    Fix a bunch of common problems. More specifically, remove Inf and NaN,\n    ensure exact binariness and symmetry (i.e. remove floating point\n    instability), and zero diagonal.\n\n\n    Parameters\n    ----------\n    W : np.ndarray\n        weighted connectivity matrix\n    copy : bool\n        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n        in place. Default value=True.\n\n    Returns\n    -------\n    W : np.ndarray\n        connectivity matrix with fixes applied\n    '''\n    if copy:\n        W = W.copy()\n\n    # zero diagonal\n    np.fill_diagonal(W, 0)\n\n    # remove np.inf and np.nan\n    W[np.logical_or(np.where(np.isinf(W)), np.where(np.isnan(W)))] = 0\n\n    # ensure exact binarity\n    u = np.unique(W)\n    if np.all(np.logical_or(np.abs(u) < 1e-8, np.abs(u - 1) < 1e-8)):\n        W = np.around(W, decimal=5)\n\n    # ensure exact symmetry\n    if np.allclose(W, W.T):\n        W = np.around(W, decimals=5)\n\n    return W"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes as input a set of vertex partitions CI of dimensions [vertex x partition]. Each column in CI contains the assignments of each vertex to a class/community/module. This function aggregates the partitions in CI into a square [vertex x vertex] agreement matrix D, whose elements indicate the number of times any two vertices were assigned to the same class. In the case that the number of nodes and partitions in CI is large (greater than ~1000 nodes or greater than ~1000 partitions), the script can be made faster by computing D in pieces. The optional input BUFFSZ determines the size of each piece. Trial and error has found that BUFFSZ ~ 150 works well. Parameters ---------- ci : NxM np.ndarray set of M (possibly degenerate) partitions of N nodes buffsz : int | None sets buffer size. If not specified, defaults to 1000 Returns ------- D : NxN np.ndarray agreement matrix", "response": "def agreement(ci, buffsz=1000):\n    '''\n    Takes as input a set of vertex partitions CI of\n    dimensions [vertex x partition]. Each column in CI contains the\n    assignments of each vertex to a class/community/module. This function\n    aggregates the partitions in CI into a square [vertex x vertex]\n    agreement matrix D, whose elements indicate the number of times any two\n    vertices were assigned to the same class.\n\n    In the case that the number of nodes and partitions in CI is large\n    (greater than ~1000 nodes or greater than ~1000 partitions), the script\n    can be made faster by computing D in pieces. The optional input BUFFSZ\n    determines the size of each piece. Trial and error has found that\n    BUFFSZ ~ 150 works well.\n\n    Parameters\n    ----------\n    ci : NxM np.ndarray\n        set of M (possibly degenerate) partitions of N nodes\n    buffsz : int | None\n        sets buffer size. If not specified, defaults to 1000\n\n    Returns\n    -------\n    D : NxN np.ndarray\n        agreement matrix\n    '''\n    ci = np.array(ci)\n    n_nodes, n_partitions = ci.shape\n\n    if n_partitions <= buffsz: # Case 1: Use all partitions at once\n        ind = dummyvar(ci)\n        D = np.dot(ind, ind.T)\n    else: # Case 2: Add together results from subsets of partitions\n        a = np.arange(0, n_partitions, buffsz)\n        b = np.arange(buffsz, n_partitions, buffsz)\n        if len(a) != len(b):\n            b = np.append(b, n_partitions)\n        D = np.zeros((n_nodes, n_nodes))\n        for i, j in zip(a, b):\n            y = ci[:, i:j]\n            ind = dummyvar(y)\n            D += np.dot(ind, ind.T)\n\n    np.fill_diagonal(D, 0)\n    return D"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the clustering coefficient of a binary directed connection matrix A.", "response": "def clustering_coef_bd(A):\n    '''\n    The clustering coefficient is the fraction of triangles around a node\n    (equiv. the fraction of nodes neighbors that are neighbors of each other).\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    C : Nx1 np.ndarray\n        clustering coefficient vector\n\n    Notes\n    -----\n    Methodological note: In directed graphs, 3 nodes generate up to 8\n    triangles (2*2*2 edges). The number of existing triangles is the main\n    diagonal of S^3/2. The number of all (in or out) neighbour pairs is\n    K(K-1)/2. Each neighbour pair may generate two triangles. \"False pairs\"\n    are i<->j edge pairs (these do not generate triangles). The number of\n    false pairs is the main diagonal of A^2.\n    Thus the maximum possible number of triangles =\n           = (2 edges)*([ALL PAIRS] - [FALSE PAIRS])\n           = 2 * (K(K-1)/2 - diag(A^2))\n           = K(K-1) - 2(diag(A^2))\n    '''\n    S = A + A.T  # symmetrized input graph\n    K = np.sum(S, axis=1)  # total degree (in+out)\n    cyc3 = np.diag(np.dot(S, np.dot(S, S))) / 2  # number of 3-cycles\n    K[np.where(cyc3 == 0)] = np.inf  # if no 3-cycles exist, make C=0\n    # number of all possible 3 cycles\n    CYC3 = K * (K - 1) - 2 * np.diag(np.dot(A, A))\n    C = cyc3 / CYC3\n    return C"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clustering_coef_bu(G):\n    '''\n    The clustering coefficient is the fraction of triangles around a node\n    (equiv. the fraction of nodes neighbors that are neighbors of each other).\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary undirected connection matrix\n\n    Returns\n    -------\n    C : Nx1 np.ndarray\n        clustering coefficient vector\n    '''\n    n = len(G)\n    C = np.zeros((n,))\n\n    for u in range(n):\n        V, = np.where(G[u, :])\n        k = len(V)\n        if k >= 2:  # degree must be at least 2\n            S = G[np.ix_(V, V)]\n            C[u] = np.sum(S) / (k * k - k)\n\n    return C", "response": "Computes the clustering coefficient of a node in a binary undirected connection matrix G."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the weighted clustering coefficient of a node.", "response": "def clustering_coef_wu(W):\n    '''\n    The weighted clustering coefficient is the average \"intensity\" of\n    triangles around a node.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted undirected connection matrix\n\n    Returns\n    -------\n    C : Nx1 np.ndarray\n        clustering coefficient vector\n    '''\n    K = np.array(np.sum(np.logical_not(W == 0), axis=1), dtype=float)\n    ws = cuberoot(W)\n    cyc3 = np.diag(np.dot(ws, np.dot(ws, ws)))\n    K[np.where(cyc3 == 0)] = np.inf  # if no 3-cycles exist, set C=0\n    C = cyc3 / (K * (K - 1))\n    return C"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the weighted clustering coefficient generalized or separated by default zhang Horvath and Constantini.", "response": "def clustering_coef_wu_sign(W, coef_type='default'):\n    '''\n    Returns the weighted clustering coefficient generalized or separated\n    for positive and negative weights.\n  \n    Three Algorithms are supported; herefore referred to as default, zhang,\n    and constantini.\n\n    1. Default (Onnela et al.), as in the traditional clustering coefficient\n       computation. Computed separately for positive and negative weights.\n    2. Zhang & Horvath. Similar to Onnela formula except weight information\n       incorporated in denominator. Reduces sensitivity of the measure to\n       weights directly connected to the node of interest. Computed\n       separately for positive and negative weights.\n    3. Constantini & Perugini generalization of Zhang & Horvath formula.\n       Takes both positive and negative weights into account simultaneously.\n       Particularly sensitive to non-redundancy in path information based on\n       sign. Returns only one value.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted undirected connection matrix\n    corr_type : enum\n        Allowed values are 'default', 'zhang', 'constantini'\n\n    Returns\n    -------\n    Cpos : Nx1 np.ndarray\n        Clustering coefficient vector for positive weights\n    Cneg : Nx1 np.ndarray\n        Clustering coefficient vector for negative weights, unless\n        coef_type == 'constantini'.\n\n    References:\n        Onnela et al. (2005) Phys Rev E 71:065103\n        Zhang & Horvath (2005) Stat Appl Genet Mol Biol 41:1544-6115\n        Costantini & Perugini (2014) PLOS ONE 9:e88669\n    '''\n    n = len(W)\n    np.fill_diagonal(W, 0)\n\n    if coef_type == 'default':\n        W_pos = W * (W > 0)\n        K_pos = np.array(np.sum(np.logical_not(W_pos == 0), axis=1),\n                         dtype=float)\n        ws_pos = cuberoot(W_pos)\n        cyc3_pos = np.diag(np.dot(ws_pos, np.dot(ws_pos, ws_pos)))\n        K_pos[np.where(cyc3_pos == 0)] = np.inf\n        C_pos = cyc3_pos / (K_pos * (K_pos - 1))\n\n        W_neg = -W * (W < 0)\n        K_neg = np.array(np.sum(np.logical_not(W_neg == 0), axis=1),\n                         dtype=float)\n        ws_neg = cuberoot(W_neg)\n        cyc3_neg = np.diag(np.dot(ws_neg, np.dot(ws_neg, ws_neg)))\n        K_neg[np.where(cyc3_neg == 0)] = np.inf\n        C_neg = cyc3_neg / (K_neg * (K_neg - 1))\n\n        return C_pos, C_neg\n\n    elif coef_type in ('zhang', 'Zhang'):\n        W_pos = W * (W > 0)\n        cyc3_pos = np.zeros((n,))\n        cyc2_pos = np.zeros((n,))\n\n        W_neg = -W * (W < 0)\n        cyc3_neg = np.zeros((n,))\n        cyc2_neg = np.zeros((n,))\n\n        for i in range(n):\n            for j in range(n):\n                for q in range(n):\n                    cyc3_pos[i] += W_pos[j, i] * W_pos[i, q] * W_pos[j, q]\n                    cyc3_neg[i] += W_neg[j, i] * W_neg[i, q] * W_neg[j, q]\n                    if j != q:\n                        cyc2_pos[i] += W_pos[j, i] * W_pos[i, q]\n                        cyc2_neg[i] += W_neg[j, i] * W_neg[i, q]\n\n        cyc2_pos[np.where(cyc3_pos == 0)] = np.inf\n        C_pos = cyc3_pos / cyc2_pos\n\n        cyc2_neg[np.where(cyc3_neg == 0)] = np.inf\n        C_neg = cyc3_neg / cyc2_neg\n\n        return C_pos, C_neg\n\n    elif coef_type in ('constantini', 'Constantini'):\n        cyc3 = np.zeros((n,))\n        cyc2 = np.zeros((n,))\n\n        for i in range(n):\n            for j in range(n):\n                for q in range(n):\n                    cyc3[i] += W[j, i] * W[i, q] * W[j, q]\n                    if j != q:\n                        cyc2[i] += W[j, i] * W[i, q]\n\n        cyc2[np.where(cyc3 == 0)] = np.inf\n        C = cyc3 / cyc2\n        return C"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the components of an undirected graph specified by the binary and undirected adjacency matrix adj. Components and their constitutent nodes are assigned the same index and stored in the vector, comps. The vector, comp_sizes, contains the number of nodes beloning to each component. Parameters ---------- A : NxN np.ndarray binary undirected adjacency matrix no_depend : Any Does nothing, included for backwards compatibility Returns ------- comps : Nx1 np.ndarray vector of component assignments for each node comp_sizes : Mx1 np.ndarray vector of component sizes Notes ----- Note: disconnected nodes will appear as components with a component size of 1 Note: The identity of each component (i.e. its numerical value in the result) is not guaranteed to be identical the value returned in BCT, matlab code, although the component topology is. Many thanks to Nick Cullen for providing this implementation", "response": "def get_components(A, no_depend=False):\n    '''\n    Returns the components of an undirected graph specified by the binary and\n    undirected adjacency matrix adj. Components and their constitutent nodes\n    are assigned the same index and stored in the vector, comps. The vector,\n    comp_sizes, contains the number of nodes beloning to each component.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary undirected adjacency matrix\n    no_depend : Any\n        Does nothing, included for backwards compatibility\n\n    Returns\n    -------\n    comps : Nx1 np.ndarray\n        vector of component assignments for each node\n    comp_sizes : Mx1 np.ndarray\n        vector of component sizes\n\n    Notes\n    -----\n    Note: disconnected nodes will appear as components with a component\n    size of 1\n\n    Note: The identity of each component (i.e. its numerical value in the\n    result) is not guaranteed to be identical the value returned in BCT,\n    matlab code, although the component topology is.\n\n    Many thanks to Nick Cullen for providing this implementation\n    '''\n\n    if not np.all(A == A.T):  # ensure matrix is undirected\n        raise BCTParamError('get_components can only be computed for undirected'\n                            ' matrices.  If your matrix is noisy, correct it with np.around')\n    \n    A = binarize(A, copy=True)\n    n = len(A)\n    np.fill_diagonal(A, 1)\n\n    edge_map = [{u,v} for u in range(n) for v in range(n) if A[u,v] == 1]\n    union_sets = []\n    for item in edge_map:\n        temp = []\n        for s in union_sets:\n\n            if not s.isdisjoint(item):\n                item = s.union(item)\n            else:\n                temp.append(s)\n        temp.append(item)\n        union_sets = temp\n\n    comps = np.array([i+1 for v in range(n) for i in \n        range(len(union_sets)) if v in union_sets[i]])\n    comp_sizes = np.array([len(s) for s in union_sets])\n\n    return comps, comp_sizes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the components of an undirected graph specified by the binary and undirected adjacency matrix adj. Components and their constitutent nodes are assigned the same index and stored in the vector, comps. The vector, comp_sizes, contains the number of nodes beloning to each component. Parameters ---------- adj : NxN np.ndarray binary undirected adjacency matrix no_depend : bool If true, doesn't import networkx to do the calculation. Default value is false. Returns ------- comps : Nx1 np.ndarray vector of component assignments for each node comp_sizes : Mx1 np.ndarray vector of component sizes Notes ----- Note: disconnected nodes will appear as components with a component size of 1 Note: The identity of each component (i.e. its numerical value in the result) is not guaranteed to be identical the value returned in BCT, although the component topology is. Note: networkx is used to do the computation efficiently. If networkx is not available a breadth-first search that does not depend on networkx is used instead, but this is less efficient. The corresponding BCT function does the computation by computing the Dulmage-Mendelsohn decomposition. I don't know what a Dulmage-Mendelsohn decomposition is and there doesn't appear to be a python equivalent. If you think of a way to implement this better, let me know.", "response": "def get_components_old(A, no_depend=False):\n    '''\n    Returns the components of an undirected graph specified by the binary and\n    undirected adjacency matrix adj. Components and their constitutent nodes\n    are assigned the same index and stored in the vector, comps. The vector,\n    comp_sizes, contains the number of nodes beloning to each component.\n\n    Parameters\n    ----------\n    adj : NxN np.ndarray\n        binary undirected adjacency matrix\n    no_depend : bool\n        If true, doesn't import networkx to do the calculation. Default value\n        is false.\n\n    Returns\n    -------\n    comps : Nx1 np.ndarray\n        vector of component assignments for each node\n    comp_sizes : Mx1 np.ndarray\n        vector of component sizes\n\n    Notes\n    -----\n    Note: disconnected nodes will appear as components with a component\n    size of 1\n\n    Note: The identity of each component (i.e. its numerical value in the\n    result) is not guaranteed to be identical the value returned in BCT,\n    although the component topology is.\n\n    Note: networkx is used to do the computation efficiently. If networkx is\n    not available a breadth-first search that does not depend on networkx is\n    used instead, but this is less efficient. The corresponding BCT function\n    does the computation by computing the Dulmage-Mendelsohn decomposition. I\n    don't know what a Dulmage-Mendelsohn decomposition is and there doesn't\n    appear to be a python equivalent. If you think of a way to implement this\n    better, let me know.\n        '''\n    # nonsquare matrices cannot be symmetric; no need to check\n\n    if not np.all(A == A.T):  # ensure matrix is undirected\n        raise BCTParamError('get_components can only be computed for undirected'\n                            ' matrices.  If your matrix is noisy, correct it with np.around')\n\n    A = binarize(A, copy=True)\n    n = len(A)\n    np.fill_diagonal(A, 1)\n\n    try:\n        if no_depend:\n            raise ImportError()\n        else:\n            import networkx as nx\n        net = nx.from_numpy_matrix(A)\n        cpts = list(nx.connected_components(net))\n\n        cptvec = np.zeros((n,))\n        cptsizes = np.zeros(len(cpts))\n        for i, cpt in enumerate(cpts):\n            cptsizes[i] = len(cpt)\n            for node in cpt:\n                cptvec[node] = i + 1\n\n    except ImportError:\n        # if networkx is not available use less efficient breadth first search\n        cptvec = np.zeros((n,))\n        r, _ = breadthdist(A)\n        for node, reach in enumerate(r):\n            if cptvec[node] > 0:\n                continue\n            else:\n                cptvec[np.where(reach)] = np.max(cptvec) + 1\n\n        cptsizes = np.zeros(np.max(cptvec))\n        for i in np.arange(np.max(cptvec)):\n            cptsizes[i] = np.size(np.where(cptvec == i + 1))\n\n    return cptvec, cptsizes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transitivity_bd(A):\n    '''\n    Transitivity is the ratio of 'triangles to triplets' in the network.\n    (A classical version of the clustering coefficient).\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    T : float\n        transitivity scalar\n\n    Notes\n    -----\n    Methodological note: In directed graphs, 3 nodes generate up to 8\n    triangles (2*2*2 edges). The number of existing triangles is the main\n\n    diagonal of S^3/2. The number of all (in or out) neighbour pairs is\n    K(K-1)/2. Each neighbour pair may generate two triangles. \"False pairs\"\n    are i<->j edge pairs (these do not generate triangles). The number of\n    false pairs is the main diagonal of A^2. Thus the maximum possible\n    number of triangles = (2 edges)*([ALL PAIRS] - [FALSE PAIRS])\n                        = 2 * (K(K-1)/2 - diag(A^2))\n                        = K(K-1) - 2(diag(A^2))\n    '''\n    S = A + A.T  # symmetrized input graph\n    K = np.sum(S, axis=1)  # total degree (in+out)\n    cyc3 = np.diag(np.dot(S, np.dot(S, S))) / 2  # number of 3-cycles\n    CYC3 = K * (K - 1) - 2 * np.diag(np.dot(A, A))  # number of all possible 3-cycles\n    return np.sum(cyc3) / np.sum(CYC3)", "response": "This function calculates the transitivity of a binary directed connection matrix A."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transitivity_bu(A):\n    '''\n    Transitivity is the ratio of 'triangles to triplets' in the network.\n    (A classical version of the clustering coefficient).\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary undirected connection matrix\n\n    Returns\n    -------\n    T : float\n        transitivity scalar\n    '''\n    tri3 = np.trace(np.dot(A, np.dot(A, A)))\n    tri2 = np.sum(np.dot(A, A)) - np.trace(np.dot(A, A))\n    return tri3 / tri2", "response": "This function calculates the transitivity of a binary undirected connection matrix A."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transitivity_wu(W):\n    '''\n    Transitivity is the ratio of 'triangles to triplets' in the network.\n    (A classical version of the clustering coefficient).\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        weighted undirected connection matrix\n\n    Returns\n    -------\n    T : int\n        transitivity scalar\n    '''\n    K = np.sum(np.logical_not(W == 0), axis=1)\n    ws = cuberoot(W)\n    cyc3 = np.diag(np.dot(ws, np.dot(ws, ws)))\n    return np.sum(cyc3, axis=0) / np.sum(K * (K - 1), axis=0)", "response": "Returns the transitivity of a weighted undirected connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert from a community index vector to a 2D python list of modules", "response": "def ci2ls(ci):\n    '''\n    Convert from a community index vector to a 2D python list of modules\n    The list is a pure python list, not requiring numpy.\n\n    Parameters\n    ----------\n    ci : Nx1 np.ndarray\n        the community index vector\n    zeroindexed : bool\n        If True, ci uses zero-indexing (lowest value is 0). Defaults to False.\n\n    Returns\n    -------\n    ls : listof(list)\n        pure python list with lowest value zero-indexed\n        (regardless of zero-indexing parameter)\n    '''\n    if not np.size(ci):\n        return ci  # list is empty\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n    nr_indices = int(max(ci))\n    ls = []\n    for c in range(nr_indices):\n        ls.append([])\n    for i, x in enumerate(ci):\n        ls[ci[i] - 1].append(i)\n    return ls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ls2ci(ls, zeroindexed=False):\n    '''\n    Convert from a 2D python list of modules to a community index vector.\n    The list is a pure python list, not requiring numpy.\n\n    Parameters\n    ----------\n    ls : listof(list)\n        pure python list with lowest value zero-indexed\n        (regardless of value of zeroindexed parameter)\n    zeroindexed : bool\n        If True, ci uses zero-indexing (lowest value is 0). Defaults to False.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        community index vector\n    '''\n    if ls is None or np.size(ls) == 0:\n        return ()  # list is empty\n    nr_indices = sum(map(len, ls))\n    ci = np.zeros((nr_indices,), dtype=int)\n    z = int(not zeroindexed)\n    for i, x in enumerate(ls):\n        for j, y in enumerate(ls[i]):\n            ci[ls[i][j]] = i + z\n    return ci", "response": "Convert from a 2D python list of modules to a community index vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef community_louvain(W, gamma=1, ci=None, B='modularity', seed=None):\n    '''\n    The optimal community structure is a subdivision of the network into\n    nonoverlapping groups of nodes which maximizes the number of within-group\n    edges and minimizes the number of between-group edges.\n\n    This function is a fast an accurate multi-iterative generalization of the\n    louvain community detection algorithm. This function subsumes and improves\n    upon modularity_[louvain,finetune]_[und,dir]() and additionally allows to\n    optimize other objective functions (includes built-in Potts Model i\n    Hamiltonian, allows for custom objective-function matrices).\n\n    Parameters\n    ----------\n    W : NxN np.array\n        directed/undirected weighted/binary adjacency matrix\n    gamma : float\n        resolution parameter. default value=1. Values 0 <= gamma < 1 detect\n        larger modules while gamma > 1 detects smaller modules.\n        ignored if an objective function matrix is specified.\n    ci : Nx1 np.arraylike\n        initial community affiliation vector. default value=None\n    B : str | NxN np.arraylike\n        string describing objective function type, or provides a custom\n        NxN objective-function matrix. builtin values \n            'modularity' uses Q-metric as objective function\n            'potts' uses Potts model Hamiltonian.\n            'negative_sym' symmetric treatment of negative weights\n            'negative_asym' asymmetric treatment of negative weights\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    ci : Nx1 np.array\n        final community structure\n    q : float\n        optimized q-statistic (modularity only)\n    '''\n    rng = get_rng(seed)\n    n = len(W)\n    s = np.sum(W)\n\n    #if np.min(W) < -1e-10:\n    #    raise BCTParamError('adjmat must not contain negative weights')\n\n    if ci is None:\n        ci = np.arange(n) + 1\n    else:\n        if len(ci) != n:\n            raise BCTParamError('initial ci vector size must equal N')\n        _, ci = np.unique(ci, return_inverse=True)\n        ci += 1\n    Mb = ci.copy()\n    renormalize = False\n    if B in ('negative_sym', 'negative_asym'):\n        renormalize = True\n        W0 = W * (W > 0)\n        s0 = np.sum(W0)\n        B0 = W0 - gamma * np.outer(np.sum(W0, axis=1), np.sum(W0, axis=0)) / s0\n\n        W1 = -W * (W < 0)\n        s1 = np.sum(W1)\n        if s1:\n            B1 = W1 - gamma * np.outer(np.sum(W1, axis=1), np.sum(W1, axis=0)) / s1\n        else:\n            B1 = 0\n\n    elif np.min(W) < -1e-10:\n        raise BCTParamError(\"Input connection matrix contains negative \"\n            'weights but objective function dealing with negative weights '\n            'was not selected')\n\n    if B == 'potts' and np.any(np.logical_not(np.logical_or(W == 0, W == 1))):\n        raise BCTParamError('Potts hamiltonian requires binary input matrix')\n\n    if B == 'modularity':\n        B = W - gamma * np.outer(np.sum(W, axis=1), np.sum(W, axis=0)) / s\n    elif B == 'potts':\n        B = W - gamma * np.logical_not(W)\n    elif B == 'negative_sym':\n        B = (B0 / (s0 + s1)) - (B1 / (s0 + s1))\n    elif B == 'negative_asym':\n        B = (B0 / s0) - (B1 / (s0 + s1))\n    else:\n        try:\n            B = np.array(B)\n        except:\n            raise BCTParamError('unknown objective function type')\n\n        if B.shape != W.shape:\n            raise BCTParamError('objective function matrix does not match '\n                                'size of adjacency matrix')\n        if not np.allclose(B, B.T):\n            print ('Warning: objective function matrix not symmetric, '\n                   'symmetrizing')\n            B = (B + B.T) / 2\n    \n    Hnm = np.zeros((n, n))\n    for m in range(1, n + 1):\n        Hnm[:, m - 1] = np.sum(B[:, ci == m], axis=1)  # node to module degree\n    H = np.sum(Hnm, axis=1)  # node degree\n    Hm = np.sum(Hnm, axis=0)  # module degree\n\n    q0 = -np.inf\n    # compute modularity\n    q = np.sum(B[np.tile(ci, (n, 1)) == np.tile(ci, (n, 1)).T]) / s\n\n    first_iteration = True\n\n    while q - q0 > 1e-10:\n        it = 0\n        flag = True\n        while flag:\n            it += 1\n            if it > 1000:\n                raise BCTParamError('Modularity infinite loop style G. '\n                                    'Please contact the developer.')\n            flag = False\n            for u in rng.permutation(n):\n                ma = Mb[u] - 1\n                dQ = Hnm[u, :] - Hnm[u, ma] + B[u, u]  # algorithm condition\n                dQ[ma] = 0\n\n                max_dq = np.max(dQ)\n                if max_dq > 1e-10:\n                    flag = True\n                    mb = np.argmax(dQ)\n\n                    Hnm[:, mb] += B[:, u]\n                    Hnm[:, ma] -= B[:, u]  # change node-to-module strengths\n\n                    Hm[mb] += H[u]\n                    Hm[ma] -= H[u]  # change module strengths\n\n                    Mb[u] = mb + 1\n\n        _, Mb = np.unique(Mb, return_inverse=True)\n        Mb += 1\n\n        M0 = ci.copy()\n        if first_iteration:\n            ci = Mb.copy()\n            first_iteration = False\n        else:\n            for u in range(1, n + 1):\n                ci[M0 == u] = Mb[u - 1]  # assign new modules\n\n        n = np.max(Mb)\n        b1 = np.zeros((n, n))\n        for i in range(1, n + 1):\n            for j in range(i, n + 1):\n                # pool weights of nodes in same module\n                bm = np.sum(B[np.ix_(Mb == i, Mb == j)])\n                b1[i - 1, j - 1] = bm\n                b1[j - 1, i - 1] = bm\n        B = b1.copy()\n\n        Mb = np.arange(1, n + 1)\n        Hnm = B.copy()\n        H = np.sum(B, axis=0)\n        Hm = H.copy()\n\n        q0 = q\n\n        q = np.trace(B)  # compute modularity\n    \n    # Workaround to normalize\n    if not renormalize:\n        return ci, q/s\n    else:\n        return ci, q", "response": "This function calculates the optimal community structure for a given set of modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _safe_squeeze(arr, *args, **kwargs):\n    out = np.squeeze(arr, *args, **kwargs)\n    if np.ndim(out) == 0:\n        out = out.reshape((1,))\n    return out", "response": "A function that does not reduce a 1 - dimensional array down to a zero - dimensional array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef modularity_finetune_und_sign(W, qtype='sta', gamma=1, ci=None, seed=None):\n    '''\n    The optimal community structure is a subdivision of the network into\n    nonoverlapping groups of nodes in a way that maximizes the number of\n    within-group edges, and minimizes the number of between-group edges.\n    The modularity is a statistic that quantifies the degree to which the\n    network may be subdivided into such clearly delineated groups.\n\n    This algorithm is inspired by the Kernighan-Lin fine-tuning algorithm\n    and is designed to refine a previously detected community structure.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected weighted/binary connection matrix with positive and\n        negative weights.\n    qtype : str\n        modularity type. Can be 'sta' (default), 'pos', 'smp', 'gja', 'neg'.\n        See Rubinov and Sporns (2011) for a description.\n    gamma : float\n        resolution parameter. default value=1. Values 0 <= gamma < 1 detect\n        larger modules while gamma > 1 detects smaller modules.\n    ci : Nx1 np.ndarray | None\n        initial community affiliation vector\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        refined community affiliation vector\n    Q : float\n        optimized modularity metric\n\n    Notes\n    -----\n    Ci and Q may vary from run to run, due to heuristics in the\n    algorithm. Consequently, it may be worth to compare multiple runs.\n    '''\n    rng = get_rng(seed)\n\n    n = len(W)  # number of nodes/modules\n    if ci is None:\n        ci = np.arange(n) + 1\n    else:\n        _, ci = np.unique(ci, return_inverse=True)\n        ci += 1\n\n    W0 = W * (W > 0)  # positive weights matrix\n    W1 = -W * (W < 0)  # negative weights matrix\n    s0 = np.sum(W0)  # positive sum of weights\n    s1 = np.sum(W1)  # negative sum of weights\n    Knm0 = np.zeros((n, n))  # positive node-to-module-degree\n    Knm1 = np.zeros((n, n))  # negative node-to-module degree\n\n    for m in range(int(np.max(ci))):  # loop over modules\n        Knm0[:, m] = np.sum(W0[:, ci == m + 1], axis=1)\n        Knm1[:, m] = np.sum(W1[:, ci == m + 1], axis=1)\n\n    Kn0 = np.sum(Knm0, axis=1)  # positive node degree\n    Kn1 = np.sum(Knm1, axis=1)  # negative node degree\n    Km0 = np.sum(Knm0, axis=0)  # positive module degree\n    Km1 = np.sum(Knm1, axis=0)  # negative module degree\n\n    if qtype == 'smp':\n        d0 = 1 / s0\n        d1 = 1 / s1  # dQ=dQ0/s0-dQ1/s1\n    elif qtype == 'gja':\n        d0 = 1 / (s0 + s1)\n        d1 = 1 / (s0 + s1)  # dQ=(dQ0-dQ1)/(s0+s1)\n    elif qtype == 'sta':\n        d0 = 1 / s0\n        d1 = 1 / (s0 + s1)  # dQ=dQ0/s0-dQ1/(s0+s1)\n    elif qtype == 'pos':\n        d0 = 1 / s0\n        d1 = 0  # dQ=dQ0/s0\n    elif qtype == 'neg':\n        d0 = 0\n        d1 = 1 / s1  # dQ=-dQ1/s1\n    else:\n        raise KeyError('modularity type unknown')\n\n    if not s0:  # adjust for absent positive weights\n        s0 = 1\n        d0 = 0\n    if not s1:  # adjust for absent negative weights\n        s1 = 1\n        d1 = 0\n\n    flag = True  # flag for within hierarchy search\n    h = 0\n    while flag:\n        h += 1\n        if h > 1000:\n            raise BCTParamError('Modularity infinite loop style D')\n        flag = False\n        for u in rng.permutation(n):  # loop over nodes in random order\n            ma = ci[u] - 1  # current module of u\n            dq0 = ((Knm0[u, :] + W0[u, u] - Knm0[u, ma]) -\n                   gamma * Kn0[u] * (Km0 + Kn0[u] - Km0[ma]) / s0)\n            dq1 = ((Knm1[u, :] + W1[u, u] - Knm1[u, ma]) -\n                   gamma * Kn1[u] * (Km1 + Kn1[u] - Km1[ma]) / s1)\n            dq = d0 * dq0 - d1 * dq1  # rescaled changes in modularity\n            dq[ma] = 0  # no changes for same module\n\n            # print dq,ma,u\n\n            max_dq = np.max(dq)  # maximal increase in modularity\n            mb = np.argmax(dq)  # corresponding module\n            if max_dq > 1e-10:  # if maximal increase is positive\n                # print h,max_dq,mb,u\n                flag = True\n                ci[u] = mb + 1  # reassign module\n\n                Knm0[:, mb] += W0[:, u]\n                Knm0[:, ma] -= W0[:, u]\n                Knm1[:, mb] += W1[:, u]\n                Knm1[:, ma] -= W1[:, u]\n                Km0[mb] += Kn0[u]\n                Km0[ma] -= Kn0[u]\n                Km1[mb] += Kn1[u]\n                Km1[ma] -= Kn1[u]\n\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n    m = np.tile(ci, (n, 1))\n    q0 = (W0 - np.outer(Kn0, Kn0) / s0) * (m == m.T)\n    q1 = (W1 - np.outer(Kn1, Kn1) / s1) * (m == m.T)\n    q = d0 * np.sum(q0) - d1 * np.sum(q1)\n\n    return ci, q", "response": "This function calculates the modularity of a undirected network into a community structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modularity_louvain_dir(W, gamma=1, hierarchy=False, seed=None):\n    '''\n    The optimal community structure is a subdivision of the network into\n    nonoverlapping groups of nodes in a way that maximizes the number of\n    within-group edges, and minimizes the number of between-group edges.\n    The modularity is a statistic that quantifies the degree to which the\n    network may be subdivided into such clearly delineated groups.\n\n    The Louvain algorithm is a fast and accurate community detection\n    algorithm (as of writing). The algorithm may also be used to detect\n    hierarchical community structure.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        directed weighted/binary connection matrix\n    gamma : float\n        resolution parameter. default value=1. Values 0 <= gamma < 1 detect\n        larger modules while gamma > 1 detects smaller modules.\n    hierarchy : bool\n        Enables hierarchical output. Defalut value=False\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        refined community affiliation vector. If hierarchical output enabled,\n        it is an NxH np.ndarray instead with multiple iterations\n    Q : float\n        optimized modularity metric. If hierarchical output enabled, becomes\n        an Hx1 array of floats instead.\n\n    Notes\n    -----\n    Ci and Q may vary from run to run, due to heuristics in the\n    algorithm. Consequently, it may be worth to compare multiple runs.\n    '''\n    rng = get_rng(seed)\n\n    n = len(W)  # number of nodes\n    s = np.sum(W)  # total weight of edges\n    h = 0  # hierarchy index\n    ci = []\n    ci.append(np.arange(n) + 1)  # hierarchical module assignments\n    q = []\n    q.append(-1)  # hierarchical modularity index\n    n0 = n\n\n    while True:\n        if h > 300:\n            raise BCTParamError('Modularity Infinite Loop Style E.  Please '\n                                'contact the developer with this error.')\n        k_o = np.sum(W, axis=1)  # node in/out degrees\n        k_i = np.sum(W, axis=0)\n        km_o = k_o.copy()  # module in/out degrees\n        km_i = k_i.copy()\n        knm_o = W.copy()  # node-to-module in/out degrees\n        knm_i = W.copy()\n\n        m = np.arange(n) + 1  # initial module assignments\n\n        flag = True  # flag for within hierarchy search\n        it = 0\n        while flag:\n            it += 1\n            if it > 1000:\n                raise BCTParamError('Modularity Infinite Loop Style F.  Please '\n                                    'contact the developer with this error.')\n            flag = False\n\n            # loop over nodes in random order\n            for u in rng.permutation(n):\n                ma = m[u] - 1\n                # algorithm condition\n                dq_o = ((knm_o[u, :] - knm_o[u, ma] + W[u, u]) -\n                        gamma * k_o[u] * (km_i - km_i[ma] + k_i[u]) / s)\n                dq_i = ((knm_i[u, :] - knm_i[u, ma] + W[u, u]) -\n                        gamma * k_i[u] * (km_o - km_o[ma] + k_o[u]) / s)\n                dq = (dq_o + dq_i) / 2\n                dq[ma] = 0\n\n                max_dq = np.max(dq)  # find maximal modularity increase\n                if max_dq > 1e-10:  # if maximal increase positive\n                    mb = np.argmax(dq)  # take only one value\n\n                    knm_o[:, mb] += W[u, :].T  # change node-to-module degrees\n                    knm_o[:, ma] -= W[u, :].T\n                    knm_i[:, mb] += W[:, u]\n                    knm_i[:, ma] -= W[:, u]\n                    km_o[mb] += k_o[u]  # change module out-degrees\n                    km_o[ma] -= k_o[u]\n                    km_i[mb] += k_i[u]\n                    km_i[ma] -= k_i[u]\n\n                    m[u] = mb + 1  # reassign module\n                    flag = True\n\n        _, m = np.unique(m, return_inverse=True)\n        m += 1\n        h += 1\n        ci.append(np.zeros((n0,)))\n        # for i,mi in enumerate(m):\t\t#loop through module assignments\n        for i in range(n):\n            # ci[h][np.where(ci[h-1]==i)]=mi\t#assign new modules\n            ci[h][np.where(ci[h - 1] == i + 1)] = m[i]\n\n        n = np.max(m)  # new number of modules\n        W1 = np.zeros((n, n))  # new weighted matrix\n        for i in range(n):\n            for j in range(n):\n                # pool weights of nodes in same module\n                W1[i, j] = np.sum(W[np.ix_(m == i + 1, m == j + 1)])\n\n        q.append(0)\n        # compute modularity\n        q[h] = np.trace(W1) / s - gamma * np.sum(np.dot(W1 / s, W1 / s))\n        if q[h] - q[h - 1] < 1e-10:  # if modularity does not increase\n            break\n\n    ci = np.array(ci, dtype=int)\n    if hierarchy:\n        ci = ci[1:-1]\n        q = q[1:-1]\n        return ci, q\n    else:\n        return ci[h - 1], q[h - 1]", "response": "This function calculates the modularity of a network in a Louvain - style manner."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef modularity_louvain_und(W, gamma=1, hierarchy=False, seed=None):\n    '''\n    The optimal community structure is a subdivision of the network into\n    nonoverlapping groups of nodes in a way that maximizes the number of\n    within-group edges, and minimizes the number of between-group edges.\n    The modularity is a statistic that quantifies the degree to which the\n    network may be subdivided into such clearly delineated groups.\n\n    The Louvain algorithm is a fast and accurate community detection\n    algorithm (as of writing). The algorithm may also be used to detect\n    hierarchical community structure.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected weighted/binary connection matrix\n    gamma : float\n        resolution parameter. default value=1. Values 0 <= gamma < 1 detect\n        larger modules while gamma > 1 detects smaller modules.\n    hierarchy : bool\n        Enables hierarchical output. Defalut value=False\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        refined community affiliation vector. If hierarchical output enabled,\n        it is an NxH np.ndarray instead with multiple iterations\n    Q : float\n        optimized modularity metric. If hierarchical output enabled, becomes\n        an Hx1 array of floats instead.\n\n    Notes\n    -----\n    Ci and Q may vary from run to run, due to heuristics in the\n    algorithm. Consequently, it may be worth to compare multiple runs.\n    '''\n    rng = get_rng(seed)\n\n    n = len(W)  # number of nodes\n    s = np.sum(W)  # weight of edges\n    h = 0  # hierarchy index\n    ci = []\n    ci.append(np.arange(n) + 1)  # hierarchical module assignments\n    q = []\n    q.append(-1)  # hierarchical modularity values\n    n0 = n\n\n    #knm = np.zeros((n,n))\n    # for j in np.xrange(n0+1):\n    #    knm[:,j] = np.sum(w[;,\n\n    while True:\n        if h > 300:\n            raise BCTParamError('Modularity Infinite Loop Style B.  Please '\n                                'contact the developer with this error.')\n        k = np.sum(W, axis=0)  # node degree\n        Km = k.copy()  # module degree\n        Knm = W.copy()  # node-to-module degree\n\n        m = np.arange(n) + 1  # initial module assignments\n\n        flag = True  # flag for within-hierarchy search\n        it = 0\n        while flag:\n            it += 1\n            if it > 1000:\n                raise BCTParamError('Modularity Infinite Loop Style C.  Please '\n                                    'contact the developer with this error.')\n            flag = False\n\n            # loop over nodes in random order\n            for i in rng.permutation(n):\n                ma = m[i] - 1\n                # algorithm condition\n                dQ = ((Knm[i, :] - Knm[i, ma] + W[i, i]) -\n                      gamma * k[i] * (Km - Km[ma] + k[i]) / s)\n                dQ[ma] = 0\n\n                max_dq = np.max(dQ)  # find maximal modularity increase\n                if max_dq > 1e-10:  # if maximal increase positive\n                    j = np.argmax(dQ)  # take only one value\n                    # print max_dq,j,dQ[j]\n\n                    Knm[:, j] += W[:, i]  # change node-to-module degrees\n                    Knm[:, ma] -= W[:, i]\n\n                    Km[j] += k[i]  # change module degrees\n                    Km[ma] -= k[i]\n\n                    m[i] = j + 1  # reassign module\n                    flag = True\n\n        _, m = np.unique(m, return_inverse=True)  # new module assignments\n        # print m,h\n        m += 1\n        h += 1\n        ci.append(np.zeros((n0,)))\n        # for i,mi in enumerate(m):\t#loop through initial module assignments\n        for i in range(n):\n            # print i, m[i], n0, h, len(m), n\n            # ci[h][np.where(ci[h-1]==i+1)]=mi\t#assign new modules\n            ci[h][np.where(ci[h - 1] == i + 1)] = m[i]\n\n        n = np.max(m)  # new number of modules\n        W1 = np.zeros((n, n))  # new weighted matrix\n        for i in range(n):\n            for j in range(i, n):\n                # pool weights of nodes in same module\n                wp = np.sum(W[np.ix_(m == i + 1, m == j + 1)])\n                W1[i, j] = wp\n                W1[j, i] = wp\n        W = W1\n\n        q.append(0)\n        # compute modularity\n        q[h] = np.trace(W) / s - gamma * np.sum(np.dot(W / s, W / s))\n        if q[h] - q[h - 1] < 1e-10:  # if modularity does not increase\n            break\n\n    ci = np.array(ci, dtype=int)\n    if hierarchy:\n        ci = ci[1:-1]\n        q = q[1:-1]\n        return ci, q\n    else:\n        return ci[h - 1], q[h - 1]", "response": "This function calculates the modularity of a undirected network into a community structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modularity_louvain_und_sign(W, gamma=1, qtype='sta', seed=None):\n    '''\n    The optimal community structure is a subdivision of the network into\n    nonoverlapping groups of nodes in a way that maximizes the number of\n    within-group edges, and minimizes the number of between-group edges.\n    The modularity is a statistic that quantifies the degree to which the\n    network may be subdivided into such clearly delineated groups.\n\n    The Louvain algorithm is a fast and accurate community detection\n    algorithm (at the time of writing).\n\n    Use this function as opposed to modularity_louvain_und() only if the\n    network contains a mix of positive and negative weights.  If the network\n    contains all positive weights, the output will be equivalent to that of\n    modularity_louvain_und().\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected weighted/binary connection matrix with positive and\n        negative weights\n    qtype : str\n        modularity type. Can be 'sta' (default), 'pos', 'smp', 'gja', 'neg'.\n        See Rubinov and Sporns (2011) for a description.\n    gamma : float\n        resolution parameter. default value=1. Values 0 <= gamma < 1 detect\n        larger modules while gamma > 1 detects smaller modules.\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        refined community affiliation vector\n    Q : float\n        optimized modularity metric\n\n    Notes\n    -----\n    Ci and Q may vary from run to run, due to heuristics in the\n    algorithm. Consequently, it may be worth to compare multiple runs.\n    '''\n    rng = get_rng(seed)\n\n    n = len(W)  # number of nodes\n\n    W0 = W * (W > 0)  # positive weights matrix\n    W1 = -W * (W < 0)  # negative weights matrix\n    s0 = np.sum(W0)  # weight of positive links\n    s1 = np.sum(W1)  # weight of negative links\n\n    if qtype == 'smp':\n        d0 = 1 / s0\n        d1 = 1 / s1  # dQ=dQ0/s0-sQ1/s1\n    elif qtype == 'gja':\n        d0 = 1 / (s0 + s1)\n        d1 = d0  # dQ=(dQ0-dQ1)/(s0+s1)\n    elif qtype == 'sta':\n        d0 = 1 / s0\n        d1 = 1 / (s0 + s1)  # dQ=dQ0/s0-dQ1/(s0+s1)\n    elif qtype == 'pos':\n        d0 = 1 / s0\n        d1 = 0  # dQ=dQ0/s0\n    elif qtype == 'neg':\n        d0 = 0\n        d1 = 1 / s1  # dQ=-dQ1/s1\n    else:\n        raise KeyError('modularity type unknown')\n\n    if not s0:  # adjust for absent positive weights\n        s0 = 1\n        d0 = 0\n    if not s1:  # adjust for absent negative weights\n        s1 = 1\n        d1 = 0\n\n    h = 1  # hierarchy index\n    nh = n  # number of nodes in hierarchy\n    ci = [None, np.arange(n) + 1]  # hierarchical module assignments\n    q = [-1, 0]  # hierarchical modularity values\n    while q[h] - q[h - 1] > 1e-10:\n        if h > 300:\n            raise BCTParamError('Modularity Infinite Loop Style A.  Please '\n                                'contact the developer with this error.')\n        kn0 = np.sum(W0, axis=0)  # positive node degree\n        kn1 = np.sum(W1, axis=0)  # negative node degree\n        km0 = kn0.copy()  # positive module degree\n        km1 = kn1.copy()  # negative module degree\n        knm0 = W0.copy()  # positive node-to-module degree\n        knm1 = W1.copy()  # negative node-to-module degree\n\n        m = np.arange(nh) + 1  # initial module assignments\n        flag = True  # flag for within hierarchy search\n        it = 0\n        while flag:\n            it += 1\n            if it > 1000:\n                raise BCTParamError('Infinite Loop was detected and stopped. '\n                                    'This was probably caused by passing in a directed matrix.')\n            flag = False\n            # loop over nodes in random order\n            for u in rng.permutation(nh):\n                ma = m[u] - 1\n                dQ0 = ((knm0[u, :] + W0[u, u] - knm0[u, ma]) -\n                       gamma * kn0[u] * (km0 + kn0[u] - km0[ma]) / s0)  # positive dQ\n                dQ1 = ((knm1[u, :] + W1[u, u] - knm1[u, ma]) -\n                       gamma * kn1[u] * (km1 + kn1[u] - km1[ma]) / s1)  # negative dQ\n\n                dQ = d0 * dQ0 - d1 * dQ1  # rescaled changes in modularity\n                dQ[ma] = 0  # no changes for same module\n\n                max_dQ = np.max(dQ)  # maximal increase in modularity\n                if max_dQ > 1e-10:  # if maximal increase is positive\n                    flag = True\n                    mb = np.argmax(dQ)\n\n                    # change positive node-to-module degrees\n                    knm0[:, mb] += W0[:, u]\n                    knm0[:, ma] -= W0[:, u]\n                    # change negative node-to-module degrees\n                    knm1[:, mb] += W1[:, u]\n                    knm1[:, ma] -= W1[:, u]\n                    km0[mb] += kn0[u]  # change positive module degrees\n                    km0[ma] -= kn0[u]\n                    km1[mb] += kn1[u]  # change negative module degrees\n                    km1[ma] -= kn1[u]\n\n                    m[u] = mb + 1  # reassign module\n\n        h += 1\n        ci.append(np.zeros((n,)))\n        _, m = np.unique(m, return_inverse=True)\n        m += 1\n\n        for u in range(nh):  # loop through initial module assignments\n            ci[h][np.where(ci[h - 1] == u + 1)] = m[u]  # assign new modules\n\n        nh = np.max(m)  # number of new nodes\n        wn0 = np.zeros((nh, nh))  # new positive weights matrix\n        wn1 = np.zeros((nh, nh))\n\n        for u in range(nh):\n            for v in range(u, nh):\n                wn0[u, v] = np.sum(W0[np.ix_(m == u + 1, m == v + 1)])\n                wn1[u, v] = np.sum(W1[np.ix_(m == u + 1, m == v + 1)])\n                wn0[v, u] = wn0[u, v]\n                wn1[v, u] = wn1[u, v]\n\n        W0 = wn0\n        W1 = wn1\n\n        q.append(0)\n        # compute modularity\n        q0 = np.trace(W0) - np.sum(np.dot(W0, W0)) / s0\n        q1 = np.trace(W1) - np.sum(np.dot(W1, W1)) / s1\n        q[h] = d0 * q0 - d1 * q1\n\n    _, ci_ret = np.unique(ci[-1], return_inverse=True)\n    ci_ret += 1\n\n    return ci_ret, q[-1]", "response": "This function calculates the modularity of a undirected network and returns the optimal community structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef modularity_und_sign(W, ci, qtype='sta'):\n    '''\n    This function simply calculates the signed modularity for a given\n    partition. It does not do automatic partition generation right now.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected weighted/binary connection matrix with positive and\n        negative weights\n    ci : Nx1 np.ndarray\n        community partition\n    qtype : str\n        modularity type. Can be 'sta' (default), 'pos', 'smp', 'gja', 'neg'.\n        See Rubinov and Sporns (2011) for a description.\n\n    Returns\n    -------\n    ci : Nx1 np.ndarray\n        the partition which was input (for consistency of the API)\n    Q : float\n        maximized modularity metric\n\n    Notes\n    -----\n    uses a deterministic algorithm\n    '''\n    n = len(W)\n    _, ci = np.unique(ci, return_inverse=True)\n    ci += 1\n\n    W0 = W * (W > 0)  # positive weights matrix\n    W1 = -W * (W < 0)  # negative weights matrix\n    s0 = np.sum(W0)  # positive sum of weights\n    s1 = np.sum(W1)  # negative sum of weights\n    Knm0 = np.zeros((n, n))  # positive node-to-module degree\n    Knm1 = np.zeros((n, n))  # negative node-to-module degree\n\n    for m in range(int(np.max(ci))):  # loop over initial modules\n        Knm0[:, m] = np.sum(W0[:, ci == m + 1], axis=1)\n        Knm1[:, m] = np.sum(W1[:, ci == m + 1], axis=1)\n\n    Kn0 = np.sum(Knm0, axis=1)  # positive node degree\n    Kn1 = np.sum(Knm1, axis=1)  # negative node degree\n    Km0 = np.sum(Knm0, axis=0)  # positive module degree\n    Km1 = np.sum(Knm1, axis=0)  # negaitve module degree\n\n    if qtype == 'smp':\n        d0 = 1 / s0\n        d1 = 1 / s1  # dQ=dQ0/s0-dQ1/s1\n    elif qtype == 'gja':\n        d0 = 1 / (s0 + s1)\n        d1 = 1 / (s0 + s1)  # dQ=(dQ0-dQ1)/(s0+s1)\n    elif qtype == 'sta':\n        d0 = 1 / s0\n        d1 = 1 / (s0 + s1)  # dQ=dQ0/s0-dQ1/(s0+s1)\n    elif qtype == 'pos':\n        d0 = 1 / s0\n        d1 = 0  # dQ=dQ0/s0\n    elif qtype == 'neg':\n        d0 = 0\n        d1 = 1 / s1  # dQ=-dQ1/s1\n    else:\n        raise KeyError('modularity type unknown')\n\n    if not s0:  # adjust for absent positive weights\n        s0 = 1\n        d0 = 0\n    if not s1:  # adjust for absent negative weights\n        s1 = 1\n        d1 = 0\n\n    m = np.tile(ci, (n, 1))\n\n    q0 = (W0 - np.outer(Kn0, Kn0) / s0) * (m == m.T)\n    q1 = (W1 - np.outer(Kn1, Kn1) / s1) * (m == m.T)\n    q = d0 * np.sum(q0) - d1 * np.sum(q1)\n\n    return ci, q", "response": "This function calculates the signed modularity for a given undirected weighted binary connection matrix with positive and negative weights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the NBS for populations X and Y for a t-statistic threshold of alpha. Parameters ---------- x : NxNxP np.ndarray matrix representing the first population with P subjects. must be symmetric. y : NxNxQ np.ndarray matrix representing the second population with Q subjects. Q need not equal P. must be symmetric. thresh : float minimum t-value used as threshold k : int number of permutations used to estimate the empirical null distribution tail : {'left', 'right', 'both'} enables specification of particular alternative hypothesis 'left' : mean population of X < mean population of Y 'right' : mean population of Y < mean population of X 'both' : means are unequal (default) paired : bool use paired sample t-test instead of population t-test. requires both subject populations to have equal N. default value = False verbose : bool print some extra information each iteration. defaults value = False seed : hashable, optional If None (default), use the np.random's global random state to generate random numbers. Otherwise, use a new np.random.RandomState instance seeded with the given value. Returns ------- pval : Cx1 np.ndarray A vector of corrected p-values for each component of the networks identified. If at least one p-value is less than alpha, the omnibus null hypothesis can be rejected at alpha significance. The null hypothesis is that the value of the connectivity from each edge has equal mean across the two populations. adj : IxIxC np.ndarray an adjacency matrix identifying the edges comprising each component. edges are assigned indexed values. null : Kx1 np.ndarray A vector of K sampled from the null distribution of maximal component size. Notes ----- ALGORITHM DESCRIPTION The NBS is a nonparametric statistical test used to isolate the components of an N x N undirected connectivity matrix that differ significantly between two distinct populations. Each element of the connectivity matrix stores a connectivity value and each member of the two populations possesses a distinct connectivity matrix. A component of a connectivity matrix is defined as a set of interconnected edges. The NBS is essentially a procedure to control the family-wise error rate, in the weak sense, when the null hypothesis is tested independently at each of the N(N-1)/2 edges comprising the undirected connectivity matrix. The NBS can provide greater statistical power than conventional procedures for controlling the family-wise error rate, such as the false discovery rate, if the set of edges at which the null hypothesis is rejected constitues a large component or components. The NBS comprises fours steps: 1. Perform a two-sample T-test at each edge indepedently to test the hypothesis that the value of connectivity between the two populations come from distributions with equal means. 2. Threshold the T-statistic available at each edge to form a set of suprathreshold edges. 3. Identify any components in the adjacency matrix defined by the set of suprathreshold edges. These are referred to as observed components. Compute the size of each observed component identified; that is, the number of edges it comprises. 4. Repeat K times steps 1-3, each time randomly permuting members of the two populations and storing the size of the largest component identified for each permuation. This yields an empirical estimate of the null distribution of maximal component size. A corrected p-value for each observed component is then calculated using this null distribution. [1] Zalesky A, Fornito A, Bullmore ET (2010) Network-based statistic: Identifying differences in brain networks. NeuroImage. 10.1016/j.neuroimage.2010.06.041", "response": "def nbs_bct(x, y, thresh, k=1000, tail='both', paired=False, verbose=False, seed=None):\n    '''\n    Performs the NBS for populations X and Y for a t-statistic threshold of\n    alpha.\n\n    Parameters\n    ----------\n    x : NxNxP np.ndarray\n        matrix representing the first population with P subjects. must be\n        symmetric.\n    y : NxNxQ np.ndarray\n        matrix representing the second population with Q subjects. Q need not\n        equal P. must be symmetric.\n    thresh : float\n        minimum t-value used as threshold\n    k : int\n        number of permutations used to estimate the empirical null \n        distribution\n    tail : {'left', 'right', 'both'}\n        enables specification of particular alternative hypothesis\n        'left' : mean population of X < mean population of Y\n        'right' : mean population of Y < mean population of X\n        'both' : means are unequal (default)\n    paired : bool\n        use paired sample t-test instead of population t-test. requires both\n        subject populations to have equal N. default value = False\n    verbose : bool\n        print some extra information each iteration. defaults value = False\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n\n    Returns\n    -------\n    pval : Cx1 np.ndarray\n        A vector of corrected p-values for each component of the networks\n        identified. If at least one p-value is less than alpha, the omnibus\n        null hypothesis can be rejected at alpha significance. The null\n        hypothesis is that the value of the connectivity from each edge has\n        equal mean across the two populations.\n    adj : IxIxC np.ndarray\n        an adjacency matrix identifying the edges comprising each component.\n        edges are assigned indexed values.\n    null : Kx1 np.ndarray\n        A vector of K sampled from the null distribution of maximal component \n        size.\n\n    Notes\n    -----\n    ALGORITHM DESCRIPTION \n    The NBS is a nonparametric statistical test used to isolate the \n    components of an N x N undirected connectivity matrix that differ \n    significantly between two distinct populations. Each element of the \n    connectivity matrix stores a connectivity value and each member of \n    the two populations possesses a distinct connectivity matrix. A \n    component of a connectivity matrix is defined as a set of \n    interconnected edges. \n\n    The NBS is essentially a procedure to control the family-wise error \n    rate, in the weak sense, when the null hypothesis is tested \n    independently at each of the N(N-1)/2 edges comprising the undirected\n    connectivity matrix. The NBS can provide greater statistical power \n    than conventional procedures for controlling the family-wise error \n    rate, such as the false discovery rate, if the set of edges at which\n    the null hypothesis is rejected constitues a large component or\n    components.\n    The NBS comprises fours steps:\n    1. Perform a two-sample T-test at each edge indepedently to test the\n       hypothesis that the value of connectivity between the two\n       populations come from distributions with equal means. \n    2. Threshold the T-statistic available at each edge to form a set of\n       suprathreshold edges. \n    3. Identify any components in the adjacency matrix defined by the set\n       of suprathreshold edges. These are referred to as observed \n       components. Compute the size of each observed component \n       identified; that is, the number of edges it comprises. \n    4. Repeat K times steps 1-3, each time randomly permuting members of\n       the two populations and storing the size of the largest component \n       identified for each permuation. This yields an empirical estimate\n       of the null distribution of maximal component size. A corrected \n       p-value for each observed component is then calculated using this\n       null distribution.\n\n    [1] Zalesky A, Fornito A, Bullmore ET (2010) Network-based statistic:\n        Identifying differences in brain networks. NeuroImage.\n        10.1016/j.neuroimage.2010.06.041\n    '''\n    rng = get_rng(seed)\n\n    def ttest2_stat_only(x, y, tail):\n        t = np.mean(x) - np.mean(y)\n        n1, n2 = len(x), len(y)\n        s = np.sqrt(((n1 - 1) * np.var(x, ddof=1) + (n2 - 1)\n                     * np.var(y, ddof=1)) / (n1 + n2 - 2))\n        denom = s * np.sqrt(1 / n1 + 1 / n2)\n        if denom == 0:\n            return 0\n        if tail == 'both':\n            return np.abs(t / denom)\n        if tail == 'left':\n            return -t / denom\n        else:\n            return t / denom\n\n    def ttest_paired_stat_only(A, B, tail):\n        n = len(A - B)\n        df = n - 1\n        sample_ss = np.sum((A - B)**2) - np.sum(A - B)**2 / n\n        unbiased_std = np.sqrt(sample_ss / (n - 1))\n        z = np.mean(A - B) / unbiased_std\n        t = z * np.sqrt(n)\n        if tail == 'both':\n            return np.abs(t)\n        if tail == 'left':\n            return -t\n        else:\n            return t\n\n    if tail not in ('both', 'left', 'right'):\n        raise BCTParamError('Tail must be both, left, right')\n\n    ix, jx, nx = x.shape\n    iy, jy, ny = y.shape\n\n    if not ix == jx == iy == jy:\n        raise BCTParamError('Population matrices are of inconsistent size')\n    else:\n        n = ix\n\n    if paired and nx != ny:\n        raise BCTParamError('Population matrices must be an equal size')\n\n    # only consider upper triangular edges\n    ixes = np.where(np.triu(np.ones((n, n)), 1))\n\n    # number of edges\n    m = np.size(ixes, axis=1)\n\n    # vectorize connectivity matrices for speed\n    xmat, ymat = np.zeros((m, nx)), np.zeros((m, ny))\n\n    for i in range(nx):\n        xmat[:, i] = x[:, :, i][ixes].squeeze()\n    for i in range(ny):\n        ymat[:, i] = y[:, :, i][ixes].squeeze()\n    del x, y\n\n    # perform t-test at each edge\n    t_stat = np.zeros((m,))\n    for i in range(m):\n        if paired:\n            t_stat[i] = ttest_paired_stat_only(xmat[i, :], ymat[i, :], tail)\n        else:\n            t_stat[i] = ttest2_stat_only(xmat[i, :], ymat[i, :], tail)\n\n    # threshold\n    ind_t, = np.where(t_stat > thresh)\n\n    if len(ind_t) == 0:\n        raise BCTParamError(\"Unsuitable threshold\")\n\n    # suprathreshold adjacency matrix\n    adj = np.zeros((n, n))\n    adj[(ixes[0][ind_t], ixes[1][ind_t])] = 1\n    # adj[ixes][ind_t]=1\n    adj = adj + adj.T\n\n    a, sz = get_components(adj)\n\n    # convert size from nodes to number of edges\n    # only consider components comprising more than one node (e.g. a/l 1 edge)\n    ind_sz, = np.where(sz > 1)\n    ind_sz += 1\n    nr_components = np.size(ind_sz)\n    sz_links = np.zeros((nr_components,))\n    for i in range(nr_components):\n        nodes, = np.where(ind_sz[i] == a)\n        sz_links[i] = np.sum(adj[np.ix_(nodes, nodes)]) / 2\n        adj[np.ix_(nodes, nodes)] *= (i + 2)\n\n    # subtract 1 to delete any edges not comprising a component\n    adj[np.where(adj)] -= 1\n\n    if np.size(sz_links):\n        max_sz = np.max(sz_links)\n    else:\n        # max_sz=0\n        raise BCTParamError('True matrix is degenerate')\n    print('max component size is %i' % max_sz)\n\n    # estimate empirical null distribution of maximum component size by\n    # generating k independent permutations\n    print('estimating null distribution with %i permutations' % k)\n\n    null = np.zeros((k,))\n    hit = 0\n    for u in range(k):\n        # randomize\n        if paired:\n            indperm = np.sign(0.5 - rng.rand(1, nx))\n            d = np.hstack((xmat, ymat)) * np.hstack((indperm, indperm))\n        else:\n            d = np.hstack((xmat, ymat))[:, rng.permutation(nx + ny)]\n\n        t_stat_perm = np.zeros((m,))\n        for i in range(m):\n            if paired:\n                t_stat_perm[i] = ttest_paired_stat_only(\n                    d[i, :nx], d[i, -nx:], tail)\n            else:\n                t_stat_perm[i] = ttest2_stat_only(d[i, :nx], d[i, -ny:], tail)\n\n        ind_t, = np.where(t_stat_perm > thresh)\n\n        adj_perm = np.zeros((n, n))\n        adj_perm[(ixes[0][ind_t], ixes[1][ind_t])] = 1\n        adj_perm = adj_perm + adj_perm.T\n\n        a, sz = get_components(adj_perm)\n\n        ind_sz, = np.where(sz > 1)\n        ind_sz += 1\n        nr_components_perm = np.size(ind_sz)\n        sz_links_perm = np.zeros((nr_components_perm))\n        for i in range(nr_components_perm):\n            nodes, = np.where(ind_sz[i] == a)\n            sz_links_perm[i] = np.sum(adj_perm[np.ix_(nodes, nodes)]) / 2\n\n        if np.size(sz_links_perm):\n            null[u] = np.max(sz_links_perm)\n        else:\n            null[u] = 0\n\n        # compare to the true dataset\n        if null[u] >= max_sz:\n            hit += 1\n\n        if verbose:\n            print(('permutation %i of %i.  Permutation max is %s.  Observed max'\n                   ' is %s.  P-val estimate is %.3f') % (\n                u, k, null[u], max_sz, hit / (u + 1)))\n        elif (u % (k / 10) == 0 or u == k - 1):\n            print('permutation %i of %i.  p-value so far is %.3f' % (u, k,\n                                                                     hit / (u + 1)))\n\n    pvals = np.zeros((nr_components,))\n    # calculate p-vals\n    for i in range(nr_components):\n        pvals[i] = np.size(np.where(null >= sz_links[i])) / k\n\n    return pvals, adj, null"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef breadth(CIJ, source):\n    '''\n    Implementation of breadth-first search.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed/undirected connection matrix\n    source : int\n        source vertex\n\n    Returns\n    -------\n    distance : Nx1 np.ndarray\n        vector of distances between source and ith vertex (0 for source)\n    branch : Nx1 np.ndarray\n        vertex that precedes i in the breadth-first search (-1 for source)\n\n    Notes\n    -----\n    Breadth-first search tree does not contain all paths (or all\n    shortest paths), but allows the determination of at least one path with\n    minimum distance. The entire graph is explored, starting from source\n    vertex 'source'.\n    '''\n    n = len(CIJ)\n\n    # colors: white,gray,black\n    white = 0\n    gray = 1\n    black = 2\n\n    color = np.zeros((n,))\n    distance = np.inf * np.ones((n,))\n    branch = np.zeros((n,))\n\n    # start on vertex source\n    color[source] = gray\n    distance[source] = 0\n    branch[source] = -1\n    Q = [source]\n\n    # keep going until the entire graph is explored\n    while Q:\n        u = Q[0]\n        ns, = np.where(CIJ[u, :])\n        for v in ns:\n            # this allows the source distance itself to be recorded\n            if distance[v] == 0:\n                distance[v] = distance[u] + 1\n            if color[v] == white:\n                color[v] = gray\n                distance[v] = distance[u] + 1\n                branch[v] = u\n                Q.append(v)\n        Q = Q[1:]\n        color[u] = black\n\n    return distance, branch", "response": "Implementation of breadth - first search."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the characteristic path length the global efficiency ecc radius and diameter of the next vertex in the network.", "response": "def charpath(D, include_diagonal=False, include_infinite=True):\n    '''\n    The characteristic path length is the average shortest path length in\n    the network. The global efficiency is the average inverse shortest path\n    length in the network.\n\n    Parameters\n    ----------\n    D : NxN np.ndarray\n        distance matrix\n    include_diagonal : bool\n        If True, include the weights on the diagonal. Default value is False.\n    include_infinite : bool\n        If True, include infinite distances in calculation\n\n    Returns\n    -------\n    lambda : float\n        characteristic path length\n    efficiency : float\n        global efficiency\n    ecc : Nx1 np.ndarray\n        eccentricity at each vertex\n    radius : float\n        radius of graph\n    diameter : float\n        diameter of graph\n\n    Notes\n    -----\n    The input distance matrix may be obtained with any of the distance\n    functions, e.g. distance_bin, distance_wei.\n    Characteristic path length is calculated as the global mean of\n    the distance matrix D, excludings any 'Infs' but including distances on\n    the main diagonal.\n    '''\n    D = D.copy()\n\n    if not include_diagonal:\n        np.fill_diagonal(D, np.nan)\n\n    if not include_infinite:\n        D[np.isinf(D)] = np.nan\n\n    Dv = D[np.logical_not(np.isnan(D))].ravel()\n\n    # mean of finite entries of D[G]\n    lambda_ = np.mean(Dv)\n\n    # efficiency: mean of inverse entries of D[G]\n    efficiency = np.mean(1 / Dv)\n\n    # eccentricity for each vertex (ignore inf)\n    ecc = np.array(np.ma.masked_where(np.isnan(D), D).max(axis=1))\n\n    # radius of graph\n    radius = np.min(ecc)  # but what about zeros?\n\n    # diameter of graph\n    diameter = np.max(ecc)\n\n    return lambda_, efficiency, ecc, radius, diameter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the probability of a cycle of path length d.", "response": "def cycprob(Pq):\n    '''\n    Cycles are paths which begin and end at the same node. Cycle\n    probability for path length d, is the fraction of all paths of length\n    d-1 that may be extended to form cycles of length d.\n\n    Parameters\n    ----------\n    Pq : NxNxQ np.ndarray\n        Path matrix with Pq[i,j,q] = number of paths from i to j of length q.\n        Produced by findpaths()\n\n    Returns\n    -------\n    fcyc : Qx1 np.ndarray\n        fraction of all paths that are cycles for each path length q\n    pcyc : Qx1 np.ndarray\n        probability that a non-cyclic path of length q-1 can be extended to\n        form a cycle of length q for each path length q\n    '''\n\n    # note: fcyc[1] must be zero, as there cannot be cycles of length 1\n    fcyc = np.zeros(np.size(Pq, axis=2))\n    for q in range(np.size(Pq, axis=2)):\n        if np.sum(Pq[:, :, q]) > 0:\n            fcyc[q] = np.sum(np.diag(Pq[:, :, q])) / np.sum(Pq[:, :, q])\n        else:\n            fcyc[q] = 0\n\n    # note: pcyc[1] is not defined (set to zero)\n    # note: pcyc[2] is equal to the fraction of reciprocal connections\n    # note: there are no non-cyclic paths of length N and no cycles of len N+1\n    pcyc = np.zeros(np.size(Pq, axis=2))\n    for q in range(np.size(Pq, axis=2)):\n        if np.sum(Pq[:, :, q - 1]) - np.sum(np.diag(Pq[:, :, q - 1])) > 0:\n            pcyc[q] = (np.sum(np.diag(Pq[:, :, q - 1])) /\n                       np.sum(Pq[:, :, q - 1]) - np.sum(np.diag(Pq[:, :, q - 1])))\n        else:\n            pcyc[q] = 0\n\n    return fcyc, pcyc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the distance matrix of a binary directed or undirected connection matrix.", "response": "def distance_bin(G):\n    '''\n    The distance matrix contains lengths of shortest paths between all\n    pairs of nodes. An entry (u,v) represents the length of shortest path\n    from node u to node v. The average shortest path length is the\n    characteristic path length of the network.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed/undirected connection matrix\n\n    Returns\n    -------\n    D : NxN\n        distance matrix\n\n    Notes\n    -----\n    Lengths between disconnected nodes are set to Inf.\n    Lengths on the main diagonal are set to 0.\n    Algorithm: Algebraic shortest paths.\n    '''\n    G = binarize(G, copy=True)\n    D = np.eye(len(G))\n    n = 1\n    nPATH = G.copy()  # n path matrix\n    L = (nPATH != 0)  # shortest n-path matrix\n\n    while np.any(L):\n        D += n * L\n        n += 1\n        nPATH = np.dot(nPATH, G)\n        L = (nPATH != 0) * (D == 0)\n\n    D[D == 0] = np.inf  # disconnected nodes are assigned d=inf\n    np.fill_diagonal(D, 0)\n    return D"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the distance between all the nodes in the given network and the shortest path between the nodes u and v.", "response": "def distance_wei(G):\n    '''\n    The distance matrix contains lengths of shortest paths between all\n    pairs of nodes. An entry (u,v) represents the length of shortest path\n    from node u to node v. The average shortest path length is the\n    characteristic path length of the network.\n\n    Parameters\n    ----------\n    L : NxN np.ndarray\n        Directed/undirected connection-length matrix.\n        NB L is not the adjacency matrix. See below.\n\n    Returns\n    -------\n    D : NxN np.ndarray\n        distance (shortest weighted path) matrix\n    B : NxN np.ndarray\n        matrix of number of edges in shortest weighted path\n\n    Notes\n    -----\n       The input matrix must be a connection-length matrix, typically\n    obtained via a mapping from weight to length. For instance, in a\n    weighted correlation network higher correlations are more naturally\n    interpreted as shorter distances and the input matrix should\n    consequently be some inverse of the connectivity matrix.\n       The number of edges in shortest weighted paths may in general\n    exceed the number of edges in shortest binary paths (i.e. shortest\n    paths computed on the binarized connectivity matrix), because shortest\n    weighted paths have the minimal weighted distance, but not necessarily\n    the minimal number of edges.\n       Lengths between disconnected nodes are set to Inf.\n       Lengths on the main diagonal are set to 0.\n\n    Algorithm: Dijkstra's algorithm.\n    '''\n    n = len(G)\n    D = np.zeros((n, n))  # distance matrix\n    D[np.logical_not(np.eye(n))] = np.inf\n    B = np.zeros((n, n))  # number of edges matrix\n\n    for u in range(n):\n        # distance permanence (true is temporary)\n        S = np.ones((n,), dtype=bool)\n        G1 = G.copy()\n        V = [u]\n        while True:\n            S[V] = 0  # distance u->V is now permanent\n            G1[:, V] = 0  # no in-edges as already shortest\n            for v in V:\n                W, = np.where(G1[v, :])  # neighbors of shortest nodes\n\n                td = np.array(\n                    [D[u, W].flatten(), (D[u, v] + G1[v, W]).flatten()])\n                d = np.min(td, axis=0)\n                wi = np.argmin(td, axis=0)\n\n                D[u, W] = d  # smallest of old/new path lengths\n                ind = W[np.where(wi == 1)]  # indices of lengthened paths\n                # increment nr_edges for lengthened paths\n                B[u, ind] = B[u, v] + 1\n\n            if D[u, S].size == 0:  # all nodes reached\n                break\n            minD = np.min(D[u, S])\n            if np.isinf(minD):  # some nodes cannot be reached\n                break\n\n            V, = np.where(D[u, :] == minD)\n\n    return D, B"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef distance_wei_floyd(adjacency, transform=None):\n\n    if transform is not None:\n        if transform == 'log':\n            if np.logical_or(adjacency > 1, adjacency < 0).any():\n                raise ValueError(\"Connection strengths must be in the \" +\n                                 \"interval [0,1) to use the transform \" +\n                                 \"-log(w_ij).\")\n            SPL = -np.log(adjacency)\n        elif transform == 'inv':\n            SPL = 1. / adjacency\n        else:\n            raise ValueError(\"Unexpected transform type. Only 'log' and \" +\n                             \"'inv' are accepted\")\n    else:\n        SPL = adjacency.copy().astype('float')\n        SPL[SPL == 0] = np.inf\n\n    n = adjacency.shape[1]\n\n    flag_find_paths = True\n    hops = np.array(adjacency != 0).astype('float')\n    Pmat = np.repeat(np.atleast_2d(np.arange(0, n)), n, 0)\n\n    for k in range(n):\n        i2k_k2j = np.repeat(SPL[:, [k]], n, 1) + np.repeat(SPL[[k], :], n, 0)\n\n        if flag_find_paths:\n            path = SPL > i2k_k2j\n            i, j = np.where(path)\n            hops[path] = hops[i, k] + hops[k, j]\n            Pmat[path] = Pmat[i, k]\n\n        SPL = np.min(np.stack([SPL, i2k_k2j], 2), 2)\n\n    I = np.eye(n) > 0\n    SPL[I] = 0\n\n    if flag_find_paths:\n        hops[I], Pmat[I] = 0, 0\n\n    return SPL, hops, Pmat", "response": "Computes the topological length of the shortest path connecting every pair of nodes in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes global and local efficiency of a binary undirected connection matrix.", "response": "def efficiency_bin(G, local=False):\n    '''\n    The global efficiency is the average of inverse shortest path length,\n    and is inversely related to the characteristic path length.\n\n    The local efficiency is the global efficiency computed on the\n    neighborhood of the node, and is related to the clustering coefficient.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary undirected connection matrix\n    local : bool\n        If True, computes local efficiency instead of global efficiency.\n        Default value = False.\n\n    Returns\n    -------\n    Eglob : float\n        global efficiency, only if local=False\n    Eloc : Nx1 np.ndarray\n        local efficiency, only if local=True\n    '''\n    def distance_inv(g):\n        D = np.eye(len(g))\n        n = 1\n        nPATH = g.copy()\n        L = (nPATH != 0)\n\n        while np.any(L):\n            D += n * L\n            n += 1\n            nPATH = np.dot(nPATH, g)\n            L = (nPATH != 0) * (D == 0)\n        D[np.logical_not(D)] = np.inf\n        D = 1 / D\n        np.fill_diagonal(D, 0)\n        return D\n\n    G = binarize(G)\n    n = len(G)  # number of nodes\n    if local:\n        E = np.zeros((n,))  # local efficiency\n\n        for u in range(n):\n            # V,=np.where(G[u,:])\t\t\t#neighbors\n            # k=len(V)\t\t\t\t\t#degree\n            # if k>=2:\t\t\t\t\t#degree must be at least 2\n            #\te=distance_inv(G[V].T[V])\n            #\tE[u]=np.sum(e)/(k*k-k)\t#local efficiency computation\n\n            # find pairs of neighbors\n            V, = np.where(np.logical_or(G[u, :], G[u, :].T))\n            # inverse distance matrix\n            e = distance_inv(G[np.ix_(V, V)])\n            # symmetrized inverse distance matrix\n            se = e + e.T\n\n            # symmetrized adjacency vector\n            sa = G[u, V] + G[V, u].T\n            numer = np.sum(np.outer(sa.T, sa) * se) / 2\n            if numer != 0:\n                denom = np.sum(sa)**2 - np.sum(sa * sa)\n                E[u] = numer / denom  # local efficiency\n\n    else:\n        e = distance_inv(G)\n        E = np.sum(e) / (n * n - n)  # global efficiency\n    return E"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef efficiency_wei(Gw, local=False):\n    '''\n    The global efficiency is the average of inverse shortest path length,\n    and is inversely related to the characteristic path length.\n\n    The local efficiency is the global efficiency computed on the\n    neighborhood of the node, and is related to the clustering coefficient.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected weighted connection matrix\n        (all weights in W must be between 0 and 1)\n    local : bool\n        If True, computes local efficiency instead of global efficiency.\n        Default value = False.\n\n    Returns\n    -------\n    Eglob : float\n        global efficiency, only if local=False\n    Eloc : Nx1 np.ndarray\n        local efficiency, only if local=True\n\n    Notes\n    -----\n       The  efficiency is computed using an auxiliary connection-length\n    matrix L, defined as L_ij = 1/W_ij for all nonzero L_ij; This has an\n    intuitive interpretation, as higher connection weights intuitively\n    correspond to shorter lengths.\n       The weighted local efficiency broadly parallels the weighted\n    clustering coefficient of Onnela et al. (2005) and distinguishes the\n    influence of different paths based on connection weights of the\n    corresponding neighbors to the node in question. In other words, a path\n    between two neighbors with strong connections to the node in question\n    contributes more to the local efficiency than a path between two weakly\n    connected neighbors. Note that this weighted variant of the local\n    efficiency is hence not a strict generalization of the binary variant.\n\n    Algorithm:  Dijkstra's algorithm\n    '''\n    def distance_inv_wei(G):\n        n = len(G)\n        D = np.zeros((n, n))  # distance matrix\n        D[np.logical_not(np.eye(n))] = np.inf\n\n        for u in range(n):\n            # distance permanence (true is temporary)\n            S = np.ones((n,), dtype=bool)\n            G1 = G.copy()\n            V = [u]\n            while True:\n                S[V] = 0  # distance u->V is now permanent\n                G1[:, V] = 0  # no in-edges as already shortest\n                for v in V:\n                    W, = np.where(G1[v, :])  # neighbors of smallest nodes\n                    td = np.array(\n                        [D[u, W].flatten(), (D[u, v] + G1[v, W]).flatten()])\n                    D[u, W] = np.min(td, axis=0)\n\n                if D[u, S].size == 0:  # all nodes reached\n                    break\n                minD = np.min(D[u, S])\n                if np.isinf(minD):  # some nodes cannot be reached\n                    break\n                V, = np.where(D[u, :] == minD)\n\n        np.fill_diagonal(D, 1)\n        D = 1 / D\n        np.fill_diagonal(D, 0)\n        return D\n\n    n = len(Gw)\n    Gl = invert(Gw, copy=True)  # connection length matrix\n    A = np.array((Gw != 0), dtype=int)\n    if local:\n        E = np.zeros((n,))  # local efficiency\n        for u in range(n):\n            # V,=np.where(Gw[u,:])\t\t#neighbors\n            # k=len(V)\t\t\t\t\t#degree\n            # if k>=2:\t\t\t\t\t#degree must be at least 2\n            #\te=(distance_inv_wei(Gl[V].T[V])*np.outer(Gw[V,u],Gw[u,V]))**1/3\n            #\tE[u]=np.sum(e)/(k*k-k)\n\n            # find pairs of neighbors\n            V, = np.where(np.logical_or(Gw[u, :], Gw[:, u].T))\n            # symmetrized vector of weights\n            sw = cuberoot(Gw[u, V]) + cuberoot(Gw[V, u].T)\n            # inverse distance matrix\n            e = distance_inv_wei(Gl[np.ix_(V, V)])\n            # symmetrized inverse distance matrix\n            se = cuberoot(e) + cuberoot(e.T)\n\n            numer = np.sum(np.outer(sw.T, sw) * se) / 2\n            if numer != 0:\n                # symmetrized adjacency vector\n                sa = A[u, V] + A[V, u].T\n                denom = np.sum(sa)**2 - np.sum(sa * sa)\n                # print numer,denom\n                E[u] = numer / denom  # local efficiency\n\n    else:\n        e = distance_inv_wei(Gl)\n        E = np.sum(e) / (n * n - n)\n    return E", "response": "This function computes the global and local efficiency of a single node in question."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwalking are sequences of linked nodes, that may visit a single node more than once. This function finds the number of walks of a given length, between any two nodes. Parameters ---------- CIJ : NxN np.ndarray binary directed/undirected connection matrix Returns ------- Wq : NxNxQ np.ndarray Wq[i,j,q] is the number of walks from i to j of length q twalk : int total number of walks found wlq : Qx1 np.ndarray walk length distribution as a function of q Notes ----- Wq grows very quickly for larger N,K,q. Weights are discarded.", "response": "def findwalks(CIJ):\n    '''\n    Walks are sequences of linked nodes, that may visit a single node more\n    than once. This function finds the number of walks of a given length,\n    between any two nodes.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed/undirected connection matrix\n\n    Returns\n    -------\n    Wq : NxNxQ np.ndarray\n        Wq[i,j,q] is the number of walks from i to j of length q\n    twalk : int\n        total number of walks found\n    wlq : Qx1 np.ndarray\n        walk length distribution as a function of q\n\n    Notes\n    -----\n    Wq grows very quickly for larger N,K,q. Weights are discarded.\n    '''\n    CIJ = binarize(CIJ, copy=True)\n    n = len(CIJ)\n    Wq = np.zeros((n, n, n))\n    CIJpwr = CIJ.copy()\n    Wq[:, :, 1] = CIJ\n    for q in range(n):\n        CIJpwr = np.dot(CIJpwr, CIJ)\n        Wq[:, :, q] = CIJpwr\n\n    twalk = np.sum(Wq)  # total number of walks\n    wlq = np.sum(np.sum(Wq, axis=0), axis=0)\n    return Wq, twalk, wlq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the binary reachability matrix describing the length of shortest paths between all pairs of nodes u and v.", "response": "def reachdist(CIJ, ensure_binary=True):\n    '''\n    The binary reachability matrix describes reachability between all pairs\n    of nodes. An entry (u,v)=1 means that there exists a path from node u\n    to node v; alternatively (u,v)=0.\n\n    The distance matrix contains lengths of shortest paths between all\n    pairs of nodes. An entry (u,v) represents the length of shortest path\n    from node u to  node v. The average shortest path length is the\n    characteristic path length of the network.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed/undirected connection matrix\n    ensure_binary : bool\n        Binarizes input. Defaults to true. No user who is not testing\n        something will ever want to not use this, use distance_wei instead for\n        unweighted matrices.\n\n    Returns\n    -------\n    R : NxN np.ndarray\n        binary reachability matrix\n    D : NxN np.ndarray\n        distance matrix\n\n    Notes\n    -----\n    faster but more memory intensive than \"breadthdist.m\".\n    '''\n    def reachdist2(CIJ, CIJpwr, R, D, n, powr, col, row):\n        CIJpwr = np.dot(CIJpwr, CIJ)\n        R = np.logical_or(R, CIJpwr != 0)\n        D += R\n\n        if powr <= n and np.any(R[np.ix_(row, col)] == 0):\n            powr += 1\n            R, D, powr = reachdist2(CIJ, CIJpwr, R, D, n, powr, col, row)\n        return R, D, powr\n\n    if ensure_binary:\n        CIJ = binarize(CIJ)\n\n    R = CIJ.copy()\n    D = CIJ.copy()\n    powr = 2\n    n = len(CIJ)\n    CIJpwr = CIJ.copy()\n\n    # check for vertices that have no incoming or outgoing connections\n    # these are ignored by reachdist\n    id = np.sum(CIJ, axis=0)\n    od = np.sum(CIJ, axis=1)\n    id0, = np.where(id == 0)  # nothing goes in, so column(R) will be 0\n    od0, = np.where(od == 0)  # nothing comes out, so row(R) will be 0\n    # use these colums and rows to check for reachability\n    col = list(range(n))\n    col = np.delete(col, id0)\n    row = list(range(n))\n    row = np.delete(row, od0)\n\n    R, D, powr = reachdist2(CIJ, CIJpwr, R, D, n, powr, col, row)\n\n    #'invert' CIJdist to get distances\n    D = powr - D + 1\n\n    # put inf if no path found\n    D[D == n + 2] = np.inf\n    D[:, id0] = np.inf\n    D[od0, :] = np.inf\n\n    return R, D"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the amount of information needed to find a random walker from an adjacency array.", "response": "def search_information(adjacency, transform=None, has_memory=False):\n    \"\"\"\n    Calculates search information of `adjacency`\n\n    Computes the amount of information (measured in bits) that a random walker\n    needs to follow the shortest path between a given pair of nodes.\n\n    Parameters\n    ----------\n    adjacency : (N x N) array_like\n        Weighted/unweighted, direct/undirected connection weight/length array\n    transform : str, optional\n        If `adjacency` is a connection weight array, specify a transform to map\n        input connection weights to connection lengths. Options include ['log',\n        'inv'], where 'log' is `-np.log(adjacency)` and 'inv' is `1/adjacency`.\n        Default: None\n    has_memory : bool, optional\n        This flag defines whether or not the random walker \"remembers\" its\n        previous step, which has the effect of reducing the amount of\n        information needed to find the next state. Default: False\n\n    Returns\n    -------\n    SI : (N x N) ndarray\n        Pair-wise search information array. Note that `SI[i,j]` may be\n        different from `SI[j,i]``; hence, `SI` is not a symmetric matrix even\n        when `adjacency` is symmetric.\n\n    References\n    ----------\n    .. [1] Goni, J., van den Heuvel, M. P., Avena-Koenigsberger, A., de\n       Mendizabal, N. V., Betzel, R. F., Griffa, A., Hagmann, P.,\n       Corominas-Murtra, B., Thiran, J-P., & Sporns, O. (2014). Resting-brain\n       functional connectivity predicted by analytic measures of network\n       communication. Proceedings of the National Academy of Sciences, 111(2),\n       833-838.\n    .. [2] Rosvall, M., Trusina, A., Minnhagen, P., & Sneppen, K. (2005).\n       Networks and cities: An information perspective. Physical Review\n       Letters, 94(2), 028701.\n    \"\"\"\n\n    N = len(adjacency)\n\n    if np.allclose(adjacency, adjacency.T):\n        flag_triu = True\n    else:\n        flag_triu = False\n\n    T = np.linalg.solve(np.diag(np.sum(adjacency, axis=1)), adjacency)\n    _, hops, Pmat = distance_wei_floyd(adjacency, transform)\n\n    SI = np.zeros((N, N))\n    SI[np.eye(N) > 0] = np.nan\n\n    for i in range(N):\n        for j in range(N):\n            if (j > i and flag_triu) or (not flag_triu and i != j):\n                path = retrieve_shortest_path(i, j, hops, Pmat)\n                lp = len(path) - 1\n                if flag_triu:\n                    if np.any(path):\n                        pr_step_ff = np.zeros(lp)\n                        pr_step_bk = np.zeros(lp)\n                        if has_memory:\n                            pr_step_ff[0] = T[path[0], path[1]]\n                            pr_step_bk[lp-1] = T[path[lp], path[lp-1]]\n                            for z in range(1, lp):\n                                pr_step_ff[z] = T[path[z], path[z+1]] / (1 - T[path[z-1], path[z]])\n                                pr_step_bk[lp-z-1] = T[path[lp-z], path[lp-z-1]] / (1 - T[path[lp-z+1], path[lp-z]])\n                        else:\n                            for z in range(lp):\n                                pr_step_ff[z] = T[path[z], path[z+1]]\n                                pr_step_bk[z] = T[path[z+1], path[z]]\n\n                        prob_sp_ff = np.prod(pr_step_ff)\n                        prob_sp_bk = np.prod(pr_step_bk)\n                        SI[i, j] = -np.log2(prob_sp_ff)\n                        SI[j, i] = -np.log2(prob_sp_bk)\n                else:\n                    if np.any(path):\n                        pr_step_ff = np.zeros(lp)\n                        if has_memory:\n                            pr_step_ff[0] = T[path[0], path[1]]\n                            for z in range(1, lp):\n                                pr_step_ff[z] = T[path[z], path[z+1]] / (1 - T[path[z-1], path[z]])\n                        else:\n                            for z in range(lp):\n                                pr_step_ff[z] = T[path[z], path[z+1]]\n\n                        prob_sp_ff = np.prod(pr_step_ff)\n                        SI[i, j] = -np.log2(prob_sp_ff)\n                    else:\n                        SI[i, j] = np.inf\n\n    return SI"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mean_first_passage_time(adjacency):\n\n    P = np.linalg.solve(np.diag(np.sum(adjacency, axis=1)), adjacency)\n\n    n = len(P)\n    D, V = np.linalg.eig(P.T)\n\n    aux = np.abs(D - 1)\n    index = np.where(aux == aux.min())[0]\n\n    if aux[index] > 10e-3:\n        raise ValueError(\"Cannot find eigenvalue of 1. Minimum eigenvalue \" +\n                         \"value is {0}. Tolerance was \".format(aux[index]+1) +\n                         \"set at 10e-3.\")\n\n    w = V[:, index].T\n    w = w / np.sum(w)\n\n    W = np.real(np.repeat(w, n, 0))\n    I = np.eye(n)\n\n    Z = np.linalg.inv(I - P + W)\n\n    mfpt = (np.repeat(np.atleast_2d(np.diag(Z)), n, 0) - Z) / W\n\n    return mfpt", "response": "Calculates the mean first passage time of an adjacency matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nround a number to teachers.", "response": "def teachers_round(x):\n    '''\n    Do rounding such that .5 always rounds to 1, and not bankers rounding.\n    This is for compatibility with matlab functions, and ease of testing.\n    '''\n    if ((x > 0) and (x % 1 >= 0.5)) or ((x < 0) and (x % 1 > 0.5)):\n        return int(np.ceil(x))\n    else:\n        return int(np.floor(x))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pick_four_unique_nodes_quickly(n, seed=None):\n    '''\n    This is equivalent to np.random.choice(n, 4, replace=False)\n\n    Another fellow suggested np.random.random_sample(n).argpartition(4) which is\n    clever but still substantially slower.\n    '''\n    rng = get_rng(seed)\n    k = rng.randint(n**4)\n    a = k % n\n    b = k // n % n\n    c = k // n ** 2 % n\n    d = k // n ** 3 % n\n    if (a != b and a != c and a != d and b != c and b != d and c != d):\n        return (a, b, c, d)\n    else:\n        # the probability of finding a wrong configuration is extremely low\n        # unless for extremely small n. if n is extremely small the\n        # computational demand is not a problem.\n\n        # In my profiling it only took 0.4 seconds to include the uniqueness\n        # check in 1 million runs of this function so I think it is OK.\n        return pick_four_unique_nodes_quickly(n, rng)", "response": "This function picks four unique nodes from the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dummyvar(cis, return_sparse=False):\n    '''\n    This is an efficient implementation of matlab's \"dummyvar\" command\n    using sparse matrices.\n\n    input: partitions, NxM array-like containing M partitions of N nodes\n        into <=N distinct communities\n\n    output: dummyvar, an NxR matrix containing R column variables (indicator\n        variables) with N entries, where R is the total number of communities\n        summed across each of the M partitions.\n\n        i.e.\n        r = sum((max(len(unique(partitions[i]))) for i in range(m)))\n    '''\n    # num_rows is not affected by partition indexes\n    n = np.size(cis, axis=0)\n    m = np.size(cis, axis=1)\n    r = np.sum((np.max(len(np.unique(cis[:, i])))) for i in range(m))\n    nnz = np.prod(cis.shape)\n\n    ix = np.argsort(cis, axis=0)\n    # s_cis=np.sort(cis,axis=0)\n    # FIXME use the sorted indices to sort by row efficiently\n    s_cis = cis[ix][:, range(m), range(m)]\n\n    mask = np.hstack((((True,),) * m, (s_cis[:-1, :] != s_cis[1:, :]).T))\n    indptr, = np.where(mask.flat)\n    indptr = np.append(indptr, nnz)\n\n    import scipy.sparse as sp\n    dv = sp.csc_matrix((np.repeat((1,), nnz), ix.T.flat, indptr), shape=(n, r))\n    return dv.toarray()", "response": "This is an efficient implementation of matlab s dummyvar command that returns a matrix containing the number of distinct communities of the node with the same number of nodes and the same number of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a RandomState instance for the current object.", "response": "def get_rng(seed=None):\n    \"\"\"\n    By default, or if `seed` is np.random, return the global RandomState\n    instance used by np.random.\n    If `seed` is a RandomState instance, return it unchanged.\n    Otherwise, use the passed (hashable) argument to seed a new instance\n    of RandomState and return it.\n\n    Parameters\n    ----------\n    seed : hashable or np.random.RandomState or np.random, optional\n\n    Returns\n    -------\n    np.random.RandomState\n    \"\"\"\n    if seed is None or seed == np.random:\n        return np.random.mtrand._rand\n    elif isinstance(seed, np.random.RandomState):\n        return seed\n    try:\n        rstate =  np.random.RandomState(seed)\n    except ValueError:\n        rstate = np.random.RandomState(random.Random(seed).randint(0, 2**32-1))\n    return rstate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the assortativity coefficient between the undirected and directed connection matrices of the given degree and edge.", "response": "def assortativity_bin(CIJ, flag=0):\n    '''\n    The assortativity coefficient is a correlation coefficient between the\n    degrees of all nodes on two opposite ends of a link. A positive\n    assortativity coefficient indicates that nodes tend to link to other\n    nodes with the same or similar degree.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed/undirected connection matrix\n    flag : int\n        0 : undirected graph; degree/degree correlation\n        1 : directed graph; out-degree/in-degree correlation\n        2 : directed graph; in-degree/out-degree correlation\n        3 : directed graph; out-degree/out-degree correlation\n        4 : directed graph; in-degree/in-degreen correlation\n\n    Returns\n    -------\n    r : float\n        assortativity coefficient\n\n    Notes\n    -----\n    The function accepts weighted networks, but all connection\n    weights are ignored. The main diagonal should be empty. For flag 1\n    the function computes the directed assortativity described in Rubinov\n    and Sporns (2010) NeuroImage.\n    '''\n    if flag == 0:  # undirected version\n        deg = degrees_und(CIJ)\n        i, j = np.where(np.triu(CIJ, 1) > 0)\n        K = len(i)\n        degi = deg[i]\n        degj = deg[j]\n    else:  # directed version\n        id, od, deg = degrees_dir(CIJ)\n        i, j = np.where(CIJ > 0)\n        K = len(i)\n\n        if flag == 1:\n            degi = od[i]\n            degj = id[j]\n        elif flag == 2:\n            degi = id[i]\n            degj = od[j]\n        elif flag == 3:\n            degi = od[i]\n            degj = od[j]\n        elif flag == 4:\n            degi = id[i]\n            degj = id[j]\n        else:\n            raise ValueError('Flag must be 0-4')\n\n    # compute assortativity\n    term1 = np.sum(degi * degj) / K\n    term2 = np.square(np.sum(.5 * (degi + degj)) / K)\n    term3 = np.sum(.5 * (degi * degi + degj * degj)) / K\n    r = (term1 - term2) / (term3 - term2)\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assortativity_wei(CIJ, flag=0):\n    '''\n    The assortativity coefficient is a correlation coefficient between the\n    strengths (weighted degrees) of all nodes on two opposite ends of a link.\n    A positive assortativity coefficient indicates that nodes tend to link to\n    other nodes with the same or similar strength.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        weighted directed/undirected connection matrix\n    flag : int\n        0 : undirected graph; strength/strength correlation\n        1 : directed graph; out-strength/in-strength correlation\n        2 : directed graph; in-strength/out-strength correlation\n        3 : directed graph; out-strength/out-strength correlation\n        4 : directed graph; in-strength/in-strengthn correlation\n\n    Returns\n    -------\n    r : float\n        assortativity coefficient\n\n    Notes\n    -----\n    The main diagonal should be empty. For flag 1\n       the function computes the directed assortativity described in Rubinov\n       and Sporns (2010) NeuroImage.\n    '''\n    if flag == 0:  # undirected version\n        str = strengths_und(CIJ)\n        i, j = np.where(np.triu(CIJ, 1) > 0)\n        K = len(i)\n        stri = str[i]\n        strj = str[j]\n    else:\n        ist, ost = strengths_dir(CIJ)  # directed version\n        i, j = np.where(CIJ > 0)\n        K = len(i)\n\n        if flag == 1:\n            stri = ost[i]\n            strj = ist[j]\n        elif flag == 2:\n            stri = ist[i]\n            strj = ost[j]\n        elif flag == 3:\n            stri = ost[i]\n            strj = ost[j]\n        elif flag == 4:\n            stri = ist[i]\n            strj = ost[j]\n        else:\n            raise ValueError('Flag must be 0-4')\n\n    # compute assortativity\n    term1 = np.sum(stri * strj) / K\n    term2 = np.square(np.sum(.5 * (stri + strj)) / K)\n    term3 = np.sum(.5 * (stri * stri + strj * strj)) / K\n    r = (term1 - term2) / (term3 - term2)\n    return r", "response": "This function computes the assortativity coefficient between the undirected graph and directed graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef core_periphery_dir(W, gamma=1, C0=None, seed=None):\n    ''' \n    The optimal core/periphery subdivision is a partition of the network \n    into two nonoverlapping groups of nodes, a core group and a periphery\n    group. The number of core-group edges is maximized, and the number of\n    within periphery edges is minimized.\n\n    The core-ness is a statistic which quantifies the goodness of the\n    optimal core/periphery subdivision (with arbitrary relative value).\n\n    The algorithm uses a variation of the Kernighan-Lin graph partitioning\n    algorithm to optimize a core-structure objective described in\n    Borgatti & Everett (2000) Soc Networks 21:375-395\n\n    See Rubinov, Ypma et al. (2015) PNAS 112:10032-7\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        directed connection matrix\n    gamma : core-ness resolution parameter\n        Default value = 1\n        gamma > 1 detects small core, large periphery\n        0 < gamma < 1 detects large core, small periphery\n    C0 : NxN np.ndarray\n        Initial core structure\n    seed : hashable, optional\n        If None (default), use the np.random's global random state to generate random numbers.\n        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n    '''\n    rng = get_rng(seed)\n    n = len(W)\n    np.fill_diagonal(W, 0)\n\n    if C0 == None:\n        C = rng.randint(2, size=(n,))\n    else:\n        C = C0.copy()\n\n    #methodological note, the core-detection null model is not corrected\n    #for degree cf community detection (to enable detection of hubs)\n\n    s = np.sum(W)\n    p = np.mean(W)\n    b = W - gamma * p\n    B = (b + b.T) / (2 * s)\n    cix, = np.where(C)\n    ncix, = np.where(np.logical_not(C))\n    q = np.sum(B[np.ix_(cix, cix)]) - np.sum(B[np.ix_(ncix, ncix)])\n\n    #sqish\n\n    flag = True\n    it = 0\n    while flag:\n        it += 1  \n        if it > 100:\n            raise BCTParamError('Infinite Loop aborted')\n\n        flag = False\n        #initial node indices\n        ixes = np.arange(n)    \n\n        Ct = C.copy()\n        while len(ixes) > 0:\n            Qt = np.zeros((n,))\n            ctix, = np.where(Ct)\n            nctix, = np.where(np.logical_not(Ct))\n            q0 = (np.sum(B[np.ix_(ctix, ctix)]) - \n                  np.sum(B[np.ix_(nctix, nctix)]))\n            Qt[ctix] = q0 - 2 * np.sum(B[ctix, :], axis=1)\n            Qt[nctix] = q0 + 2 * np.sum(B[nctix, :], axis=1)\n\n            max_Qt = np.max(Qt[ixes])\n            u, = np.where(np.abs(Qt[ixes]-max_Qt) < 1e-10)\n            #tunourn\n            u = u[rng.randint(len(u))]\n            Ct[ixes[u]] = np.logical_not(Ct[ixes[u]])\n            #casga\n\n            ixes = np.delete(ixes, u)\n            \n            if max_Qt - q > 1e-10:\n                flag = True\n                C = Ct.copy()\n                cix, = np.where(C)\n                ncix, = np.where(np.logical_not(C))\n                q = (np.sum(B[np.ix_(cix, cix)]) - \n                     np.sum(B[np.ix_(ncix, ncix)]))\n\n    cix, = np.where(C)\n    ncix, = np.where(np.logical_not(C))\n    q = np.sum(B[np.ix_(cix, cix)]) - np.sum(B[np.ix_(ncix, ncix)])\n    return C, q", "response": "This function computes the optimal core - periphery subdivision of a network by using a methodological note that the core - detection is performed on the network and the periphery - detection is performed on the periphery - group edges of the network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kcore_bd(CIJ, k, peel=False):\n    '''\n    The k-core is the largest subnetwork comprising nodes of degree at\n    least k. This function computes the k-core for a given binary directed\n    connection matrix by recursively peeling off nodes with degree lower\n    than k, until no such nodes remain.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed adjacency matrix\n    k : int\n        level of k-core\n    peel : bool\n        If True, additionally calculates peelorder and peellevel. Defaults to\n        False.\n\n    Returns\n    -------\n    CIJkcore : NxN np.ndarray\n        connection matrix of the k-core. This matrix only contains nodes of\n        degree at least k.\n    kn : int\n        size of k-core\n    peelorder : Nx1 np.ndarray\n        indices in the order in which they were peeled away during k-core\n        decomposition. only returned if peel is specified.\n    peellevel : Nx1 np.ndarray\n        corresponding level - nodes in at the same level have been peeled\n        away at the same time. only return if peel is specified\n\n    Notes\n    -----\n    'peelorder' and 'peellevel' are similar the the k-core sub-shells\n    described in Modha and Singh (2010).\n    '''\n    if peel:\n        peelorder, peellevel = ([], [])\n    iter = 0\n    CIJkcore = CIJ.copy()\n\n    while True:\n        id, od, deg = degrees_dir(CIJkcore)  # get degrees of matrix\n\n        # find nodes with degree <k\n        ff, = np.where(np.logical_and(deg < k, deg > 0))\n\n        if ff.size == 0:\n            break  # if none found -> stop\n\n        # else peel away found nodes\n        iter += 1\n        CIJkcore[ff, :] = 0\n        CIJkcore[:, ff] = 0\n\n        if peel:\n            peelorder.append(ff)\n        if peel:\n            peellevel.append(iter * np.ones((len(ff),)))\n\n    kn = np.sum(deg > 0)\n\n    if peel:\n        return CIJkcore, kn, peelorder, peellevel\n    else:\n        return CIJkcore, kn", "response": "This function computes the k - core for a given binary directed adjacency matrix. The k - core is the largest subnetwork comprising nodes of degree lower\n        than k."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kcore_bu(CIJ, k, peel=False):\n    '''\n    The k-core is the largest subnetwork comprising nodes of degree at\n    least k. This function computes the k-core for a given binary\n    undirected connection matrix by recursively peeling off nodes with\n    degree lower than k, until no such nodes remain.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary undirected connection matrix\n    k : int\n        level of k-core\n    peel : bool\n        If True, additionally calculates peelorder and peellevel. Defaults to\n        False.\n\n    Returns\n    -------\n    CIJkcore : NxN np.ndarray\n        connection matrix of the k-core. This matrix only contains nodes of\n        degree at least k.\n    kn : int\n        size of k-core\n    peelorder : Nx1 np.ndarray\n        indices in the order in which they were peeled away during k-core\n        decomposition. only returned if peel is specified.\n    peellevel : Nx1 np.ndarray\n        corresponding level - nodes in at the same level have been peeled\n        away at the same time. only return if peel is specified\n\n    Notes\n    -----\n    'peelorder' and 'peellevel' are similar the the k-core sub-shells\n    described in Modha and Singh (2010).\n    '''\n    if peel:\n        peelorder, peellevel = ([], [])\n    iter = 0\n    CIJkcore = CIJ.copy()\n\n    while True:\n        deg = degrees_und(CIJkcore)  # get degrees of matrix\n\n        # find nodes with degree <k\n        ff, = np.where(np.logical_and(deg < k, deg > 0))\n\n        if ff.size == 0:\n            break  # if none found -> stop\n\n        # else peel away found nodes\n        iter += 1\n        CIJkcore[ff, :] = 0\n        CIJkcore[:, ff] = 0\n\n        if peel:\n            peelorder.append(ff)\n        if peel:\n            peellevel.append(iter * np.ones((len(ff),)))\n\n    kn = np.sum(deg > 0)\n\n    if peel:\n        return CIJkcore, kn, peelorder, peellevel\n    else:\n        return CIJkcore, kn", "response": "This function computes the k - core for a given binary undirected connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef local_assortativity_wu_sign(W):\n    '''\n    Local assortativity measures the extent to which nodes are connected to\n    nodes of similar strength. Adapted from Thedchanamoorthy et al. 2014\n    formula to allowed weighted/signed networks.\n\n    Parameters\n    ----------\n    W : NxN np.ndarray\n        undirected connection matrix with positive and negative weights\n    \n    Returns\n    -------\n    loc_assort_pos : Nx1 np.ndarray\n        local assortativity from positive weights\n    loc_assort_neg : Nx1 np.ndarray\n        local assortativity from negative weights\n    '''\n    n = len(W)\n\n    np.fill_diagonal(W, 0)\n    r_pos = assortativity_wei(W * (W > 0))\n    r_neg = assortativity_wei(W * (W < 0))\n\n    str_pos, str_neg, _, _ = strengths_und_sign(W)\n\n    loc_assort_pos = np.zeros((n,))\n    loc_assort_neg = np.zeros((n,))\n\n    for curr_node in range(n):\n        jp = np.where(W[curr_node, :] > 0)\n        loc_assort_pos[curr_node] = np.sum(np.abs(str_pos[jp] - \n            str_pos[curr_node])) / str_pos[curr_node]\n        jn = np.where(W[curr_node, :] < 0)\n        loc_assort_neg[curr_node] = np.sum(np.abs(str_neg[jn] -\n            str_neg[curr_node])) / str_neg[curr_node]\n\n    loc_assort_pos = ((r_pos + 1) / n - \n        loc_assort_pos / np.sum(loc_assort_pos))\n    loc_assort_neg = ((r_neg + 1) / n -\n        loc_assort_neg / np.sum(loc_assort_neg))\n\n    return loc_assort_pos, loc_assort_neg", "response": "Returns the extent to which nodes are connected to the next local assortativity."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rich_club_bd(CIJ, klevel=None):\n    '''\n    The rich club coefficient, R, at level k is the fraction of edges that\n    connect nodes of degree k or higher out of the maximum number of edges\n    that such nodes might share.\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        binary directed connection matrix\n    klevel : int | None\n        sets the maximum level at which the rich club coefficient will be\n        calculated. If None (default), the maximum level is set to the\n        maximum degree of the adjacency matrix\n\n    Returns\n    -------\n    R : Kx1 np.ndarray\n        vector of rich-club coefficients for levels 1 to klevel\n    Nk : int\n        number of nodes with degree > k\n    Ek : int\n        number of edges remaining in subgraph with degree > k\n    '''\n    # definition of degree as used for RC coefficients\n    # degree is taken to be the sum of incoming and outgoing connections\n    id, od, deg = degrees_dir(CIJ)\n\n    if klevel is None:\n        klevel = int(np.max(deg))\n\n    R = np.zeros((klevel,))\n    Nk = np.zeros((klevel,))\n    Ek = np.zeros((klevel,))\n    for k in range(klevel):\n        SmallNodes, = np.where(deg <= k + 1)  # get small nodes with degree <=k\n        subCIJ = np.delete(CIJ, SmallNodes, axis=0)\n        subCIJ = np.delete(subCIJ, SmallNodes, axis=1)\n        Nk[k] = np.size(subCIJ, axis=1)  # number of nodes with degree >k\n        Ek[k] = np.sum(subCIJ)  # number of connections in subgraph\n        # unweighted rich club coefficient\n        R[k] = Ek[k] / (Nk[k] * (Nk[k] - 1))\n\n    return R, Nk, Ek", "response": "Calculates the rich - club coefficient R Nk and Ek for a given binary directed connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rich_club_wd(CIJ, klevel=None):\n    '''\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        weighted directed connection matrix\n    klevel : int | None\n        sets the maximum level at which the rich club coefficient will be\n        calculated. If None (default), the maximum level is set to the\n        maximum degree of the adjacency matrix\n\n    Returns\n    -------\n    Rw : Kx1 np.ndarray\n        vector of rich-club coefficients for levels 1 to klevel\n    '''\n    nr_nodes = len(CIJ)\n    # degree of each node is defined here as in+out\n    deg = np.sum((CIJ != 0), axis=0) + np.sum((CIJ.T != 0), axis=0)\n\n    if klevel is None:\n        klevel = np.max(deg)\n    Rw = np.zeros((klevel,))\n\n    # sort the weights of the network, with the strongest connection first\n    wrank = np.sort(CIJ.flat)[::-1]\n\n    for k in range(klevel):\n        SmallNodes, = np.where(deg < k + 1)\n        if np.size(SmallNodes) == 0:\n            Rw[k] = np.nan\n            continue\n\n        # remove small nodes with node degree < k\n        cutCIJ = np.delete(\n            np.delete(CIJ, SmallNodes, axis=0), SmallNodes, axis=1)\n        # total weight of connections in subset E>r\n        Wr = np.sum(cutCIJ)\n        # total number of connections in subset E>r\n        Er = np.size(np.where(cutCIJ.flat != 0), axis=1)\n        # E>r number of connections with max weight in network\n        wrank_r = wrank[:Er]\n        # weighted rich-club coefficient\n        Rw[k] = Wr / np.sum(wrank_r)\n    return Rw", "response": "Calculates the rich - club coefficient for a set of nodes in the given directed connection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_pad_index(self, array):\n        try:\n            return list(array).index(self.pad_value)\n        except ValueError:\n            return len(array)", "response": "Find the index of the padding value in the array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting true length of y.", "response": "def get_length(self, y):\n        \"\"\"Get true length of y.\n\n        Args:\n            y (list): padded list.\n\n        Returns:\n            lens: true length of y.\n\n        Examples:\n            >>> y = [[1, 0, 0], [1, 1, 0], [1, 1, 1]]\n            >>> self.get_length(y)\n            [1, 2, 3]\n        \"\"\"\n        lens = [self.find_pad_index(row) for row in y]\n        return lens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert label index to name.", "response": "def convert_idx_to_name(self, y, lens):\n        \"\"\"Convert label index to name.\n\n        Args:\n            y (list): label index list.\n            lens (list): true length of y.\n\n        Returns:\n            y: label name list.\n\n        Examples:\n            >>> # assumes that id2label = {1: 'B-LOC', 2: 'I-LOC'}\n            >>> y = [[1, 0, 0], [1, 2, 0], [1, 1, 1]]\n            >>> lens = [1, 2, 3]\n            >>> self.convert_idx_to_name(y, lens)\n            [['B-LOC'], ['B-LOC', 'I-LOC'], ['B-LOC', 'B-LOC', 'B-LOC']]\n        \"\"\"\n        y = [[self.id2label[idx] for idx in row[:l]]\n             for row, l in zip(y, lens)]\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X, y):\n        y_pred = self.model.predict_on_batch(X)\n\n        # reduce dimension.\n        y_true = np.argmax(y, -1)\n        y_pred = np.argmax(y_pred, -1)\n\n        lens = self.get_length(y_true)\n\n        y_true = self.convert_idx_to_name(y_true, lens)\n        y_pred = self.convert_idx_to_name(y_pred, lens)\n\n        return y_true, y_pred", "response": "Predict the cluster class for each tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef score(self, y_true, y_pred):\n        score = f1_score(y_true, y_pred)\n        print(' - f1: {:04.2f}'.format(score * 100))\n        print(classification_report(y_true, y_pred, digits=4))\n        return score", "response": "Calculate f1 score.\n\n        Args:\n            y_true (list): true sequences.\n            y_pred (list): predicted sequences.\n\n        Returns:\n            score: f1 score."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_entities(seq, suffix=False):\n    # for nested list\n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + ['O']]\n\n    prev_tag = 'O'\n    prev_type = ''\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + ['O']):\n        if suffix:\n            tag = chunk[-1]\n            type_ = chunk.split('-')[0]\n        else:\n            tag = chunk[0]\n            type_ = chunk.split('-')[-1]\n\n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i-1))\n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n        prev_tag = tag\n        prev_type = type_\n\n    return chunks", "response": "Gets the entities from a sequence of labels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a chunk ended between the previous and current word.", "response": "def end_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk ended between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_end: boolean.\n    \"\"\"\n    chunk_end = False\n\n    if prev_tag == 'E': chunk_end = True\n    if prev_tag == 'S': chunk_end = True\n\n    if prev_tag == 'B' and tag == 'B': chunk_end = True\n    if prev_tag == 'B' and tag == 'S': chunk_end = True\n    if prev_tag == 'B' and tag == 'O': chunk_end = True\n    if prev_tag == 'I' and tag == 'B': chunk_end = True\n    if prev_tag == 'I' and tag == 'S': chunk_end = True\n    if prev_tag == 'I' and tag == 'O': chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    chunk_start = False\n\n    if tag == 'B': chunk_start = True\n    if tag == 'S': chunk_start = True\n\n    if prev_tag == 'E' and tag == 'E': chunk_start = True\n    if prev_tag == 'E' and tag == 'I': chunk_start = True\n    if prev_tag == 'S' and tag == 'E': chunk_start = True\n    if prev_tag == 'S' and tag == 'I': chunk_start = True\n    if prev_tag == 'O' and tag == 'E': chunk_start = True\n    if prev_tag == 'O' and tag == 'I': chunk_start = True\n\n    if tag != 'O' and tag != '.' and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start", "response": "Checks if a chunk started between the previous and current word."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f1_score(y_true, y_pred, average='micro', suffix=False):\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n    nb_true = len(true_entities)\n\n    p = nb_correct / nb_pred if nb_pred > 0 else 0\n    r = nb_correct / nb_true if nb_true > 0 else 0\n    score = 2 * p * r / (p + r) if p + r > 0 else 0\n\n    return score", "response": "Compute the F1 score."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the precision of the true positives and false positives.", "response": "def precision_score(y_true, y_pred, average='micro', suffix=False):\n    \"\"\"Compute the precision.\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample.\n\n    The best value is 1 and the worst value is 0.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import precision_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> precision_score(y_true, y_pred)\n        0.50\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n\n    score = nb_correct / nb_pred if nb_pred > 0 else 0\n\n    return score"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recall_score(y_true, y_pred, average='micro', suffix=False):\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_true = len(true_entities)\n\n    score = nb_correct / nb_true if nb_true > 0 else 0\n\n    return score", "response": "Compute the recall.\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The best value is 1 and the worst value is 0.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import recall_score\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> recall_score(y_true, y_pred)\n        0.50"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the performance metrics for the current set of target values.", "response": "def performance_measure(y_true, y_pred):\n    \"\"\"\n    Compute the performance metrics: TP, FP, FN, TN\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        performance_dict : dict\n\n    Example:\n        >>> from seqeval.metrics import performance_measure\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> performance_measure(y_true, y_pred)\n        (3, 3, 1, 4)\n    \"\"\"\n    performace_dict = dict()\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n    performace_dict['TP'] = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred)\n                                if ((y_t != 'O') or (y_p != 'O')))\n    performace_dict['FP'] = sum(y_t != y_p for y_t, y_p in zip(y_true, y_pred))\n    performace_dict['FN'] = sum(((y_t != 'O') and (y_p == 'O'))\n                                for y_t, y_p in zip(y_true, y_pred))\n    performace_dict['TN'] = sum((y_t == y_p == 'O')\n                                for y_t, y_p in zip(y_true, y_pred))\n\n    return performace_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a text report showing the main classification metrics.", "response": "def classification_report(y_true, y_pred, digits=2, suffix=False):\n    \"\"\"Build a text report showing the main classification metrics.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a classifier.\n        digits : int. Number of digits for formatting output floating point values.\n\n    Returns:\n        report : string. Text summary of the precision, recall, F1 score for each class.\n\n    Examples:\n        >>> from seqeval.metrics import classification_report\n        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n        >>> print(classification_report(y_true, y_pred))\n                     precision    recall  f1-score   support\n        <BLANKLINE>\n               MISC       0.00      0.00      0.00         1\n                PER       1.00      1.00      1.00         1\n        <BLANKLINE>\n          micro avg       0.50      0.50      0.50         2\n          macro avg       0.50      0.50      0.50         2\n        <BLANKLINE>\n    \"\"\"\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    name_width = 0\n    d1 = defaultdict(set)\n    d2 = defaultdict(set)\n    for e in true_entities:\n        d1[e[0]].add((e[1], e[2]))\n        name_width = max(name_width, len(e[0]))\n    for e in pred_entities:\n        d2[e[0]].add((e[1], e[2]))\n\n    last_line_heading = 'macro avg'\n    width = max(name_width, len(last_line_heading), digits)\n\n    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n    report = head_fmt.format(u'', *headers, width=width)\n    report += u'\\n\\n'\n\n    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n\n    ps, rs, f1s, s = [], [], [], []\n    for type_name, true_entities in d1.items():\n        pred_entities = d2[type_name]\n        nb_correct = len(true_entities & pred_entities)\n        nb_pred = len(pred_entities)\n        nb_true = len(true_entities)\n\n        p = nb_correct / nb_pred if nb_pred > 0 else 0\n        r = nb_correct / nb_true if nb_true > 0 else 0\n        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n\n        ps.append(p)\n        rs.append(r)\n        f1s.append(f1)\n        s.append(nb_true)\n\n    report += u'\\n'\n\n    # compute averages\n    report += row_fmt.format('micro avg',\n                             precision_score(y_true, y_pred, suffix=suffix),\n                             recall_score(y_true, y_pred, suffix=suffix),\n                             f1_score(y_true, y_pred, suffix=suffix),\n                             np.sum(s),\n                             width=width, digits=digits)\n    report += row_fmt.format(last_line_heading,\n                             np.average(ps, weights=s),\n                             np.average(rs, weights=s),\n                             np.average(f1s, weights=s),\n                             np.sum(s),\n                             width=width, digits=digits)\n\n    return report"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a datetime. timedelta object into a seconds interval for rotating file ouput.", "response": "def _timedelta_to_seconds(td):\n        \"\"\"Convert a datetime.timedelta object into a seconds interval for\n        rotating file ouput.\n\n        :param td: datetime.timedelta\n        :return: time in seconds\n        :rtype: int\n        \"\"\"\n        if isinstance(td, numbers.Real):\n            td = datetime.timedelta(seconds=td)\n        return td.total_seconds()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getLogger(name=None, **kwargs):\n    adapter = _LOGGERS.get(name)\n    if not adapter:\n        # NOTE(jd) Keep using the `adapter' variable here because so it's not\n        # collected by Python since _LOGGERS contains only a weakref\n        adapter = KeywordArgumentAdapter(logging.getLogger(name), kwargs)\n        _LOGGERS[name] = adapter\n    return adapter", "response": "Build a logger with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_default_log_levels(loggers_and_log_levels):\n    for logger, level in loggers_and_log_levels:\n        if isinstance(level, str):\n            level = level.upper()\n        logging.getLogger(logger).setLevel(level)", "response": "Set default log levels for some loggers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate SWAG client from the current context.", "response": "def create_swag_from_ctx(ctx):\n    \"\"\"Creates SWAG client from the current context.\"\"\"\n    swag_opts = {}\n    if ctx.type == 'file':\n        swag_opts = {\n            'swag.type': 'file',\n            'swag.data_dir': ctx.data_dir,\n            'swag.data_file': ctx.data_file\n        }\n    elif ctx.type == 's3':\n        swag_opts = {\n            'swag.type': 's3',\n            'swag.bucket_name': ctx.bucket_name,\n            'swag.data_file': ctx.data_file,\n            'swag.region': ctx.region\n        }\n    elif ctx.type == 'dynamodb':\n        swag_opts = {\n            'swag.type': 'dynamodb',\n            'swag.region': ctx.region\n        }\n    return SWAGManager(**parse_swag_config_options(swag_opts))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file(ctx, data_dir, data_file):\n    if not ctx.file:\n        ctx.data_file = data_file\n\n    if not ctx.data_dir:\n        ctx.data_dir = data_dir\n\n    ctx.type = 'file'", "response": "Use the File SWAG Backend"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse the S3 SWAG backend.", "response": "def s3(ctx, bucket_name, data_file, region):\n    \"\"\"Use the S3 SWAG backend.\"\"\"\n    if not ctx.data_file:\n        ctx.data_file = data_file\n\n    if not ctx.bucket_name:\n        ctx.bucket_name = bucket_name\n\n    if not ctx.region:\n        ctx.region = region\n\n    ctx.type = 's3'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists SWAG account info.", "response": "def list(ctx):\n    \"\"\"List SWAG account info.\"\"\"\n    if ctx.namespace != 'accounts':\n        click.echo(\n            click.style('Only account data is available for listing.', fg='red')\n        )\n        return\n\n    swag = create_swag_from_ctx(ctx)\n    accounts = swag.get_all()\n    _table = [[result['name'], result.get('id')] for result in accounts]\n    click.echo(\n        tabulate(_table, headers=[\"Account Name\", \"Account Number\"])\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist accounts pertaining to named service.", "response": "def list_service(ctx, name):\n    \"\"\"Retrieve accounts pertaining to named service.\"\"\"\n    swag = create_swag_from_ctx(ctx)\n    accounts = swag.get_service_enabled(name)\n\n    _table = [[result['name'], result.get('id')] for result in accounts]\n    click.echo(\n        tabulate(_table, headers=[\"Account Name\", \"Account Number\"])\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef migrate(ctx, start_version, end_version):\n    if ctx.type == 'file':\n        if ctx.data_file:\n            file_path = ctx.data_file\n        else:\n            file_path = os.path.join(ctx.data_file, ctx.namespace + '.json')\n\n        # todo make this more like alemebic and determine/load versions automatically\n        with open(file_path, 'r') as f:\n            data = json.loads(f.read())\n\n        data = run_migration(data, start_version, end_version)\n        with open(file_path, 'w') as f:\n            f.write(json.dumps(data))", "response": "Transition from one SWAG schema to another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransfer SWAG data from one backend to another", "response": "def propagate(ctx):\n    \"\"\"Transfers SWAG data from one backend to another\"\"\"\n    data = []\n    if ctx.type == 'file':\n        if ctx.data_file:\n            file_path = ctx.data_file\n        else:\n            file_path = os.path.join(ctx.data_dir, ctx.namespace + '.json')\n\n        with open(file_path, 'r') as f:\n            data = json.loads(f.read())\n\n    swag_opts = {\n        'swag.type': 'dynamodb'\n    }\n\n    swag = SWAGManager(**parse_swag_config_options(swag_opts))\n\n    for item in data:\n        time.sleep(2)\n        swag.create(item, dry_run=ctx.dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(ctx, data):\n    swag = create_swag_from_ctx(ctx)\n    data = json.loads(data.read())\n\n    for account in data:\n        swag.create(account, dry_run=ctx.dry_run)", "response": "Create a new SWAG item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deploy_service(ctx, path, name, regions, disabled):\n    enabled = False if disabled else True\n\n    swag = create_swag_from_ctx(ctx)\n    accounts = swag.get_all(search_filter=path)\n    log.debug('Searching for accounts. Found: {} JMESPath: `{}`'.format(len(accounts), path))\n    for a in accounts:\n        try:\n            if not swag.get_service(name, search_filter=\"[?id=='{id}']\".format(id=a['id'])):\n                log.info('Found an account to update. AccountName: {name} AccountNumber: {number}'.format(name=a['name'], number=a['id']))\n                status = []\n                for region in regions:\n                    status.append(\n                        {\n                            'enabled': enabled,\n                            'region': region\n\n                         }\n                    )\n                a['services'].append(\n                    {\n                        'name': name,\n                        'status': status\n                     }\n                )\n\n                swag.update(a, dry_run=ctx.dry_run)\n        except InvalidSWAGDataException as e:\n            log.warning('Found a data quality issue. AccountName: {name} AccountNumber: {number}'.format(name=a['name'], number=a['id']))\n\n    log.info('Service has been deployed to all matching accounts.')", "response": "Deploys a new service JSON to multiple accounts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seed_aws_data(ctx, data):\n    swag = create_swag_from_ctx(ctx)\n    for k, v in json.loads(data.read()).items():\n        for account in v['accounts']:\n            data = {\n                    'description': 'This is an AWS owned account used for {}'.format(k),\n                    'id': account['account_id'],\n                    'contacts': [],\n                    'owner': 'aws',\n                    'provider': 'aws',\n                    'sensitive': False,\n                    'email': 'support@amazon.com',\n                    'name': k + '-' + account['region']\n                }\n\n            click.echo(click.style(\n                'Seeded Account. AccountName: {}'.format(data['name']), fg='green')\n            )\n\n            swag.create(data, dry_run=ctx.dry_run)", "response": "Seeds SWAG from a list of known AWS accounts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seed_aws_organization(ctx, owner):\n    swag = create_swag_from_ctx(ctx)\n    accounts = swag.get_all()\n    _ids = [result.get('id') for result in accounts]\n\n    client = boto3.client('organizations')\n    paginator = client.get_paginator('list_accounts')\n    response_iterator = paginator.paginate()\n\n    count = 0\n    for response in response_iterator:\n        for account in response['Accounts']:\n            if account['Id'] in _ids:\n                click.echo(click.style(\n                    'Ignoring Duplicate Account.  AccountId: {} already exists in SWAG'.format(account['Id']), fg='yellow')\n                )\n                continue\n\n            if account['Status'] == 'SUSPENDED':\n                status = 'deprecated'\n            else:\n                status = 'created'\n\n            data = {\n                'id': account['Id'],\n                'name': account['Name'],\n                'description': 'Account imported from AWS organization.',\n                'email': account['Email'],\n                'owner': owner,\n                'provider': 'aws',\n                'contacts': [],\n                'sensitive': False,\n                'status': [{'region': 'all', 'status': status}]\n            }\n\n            click.echo(click.style(\n                'Seeded Account. AccountName: {}'.format(data['name']), fg='green')\n            )\n\n            count += 1\n            swag.create(data, dry_run=ctx.dry_run)\n\n    click.echo('Seeded {} accounts to SWAG.'.format(count))", "response": "Seeds SWAG from an AWS organziation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_file(client, bucket, data_file):\n    logger.debug('Loading item from s3. Bucket: {bucket} Key: {key}'.format(\n        bucket=bucket,\n        key=data_file\n    ))\n\n    # If the file doesn't exist, then return an empty dict:\n    try:\n        data = _get_from_s3(client, bucket, data_file)\n\n    except ClientError as ce:\n        if ce.response['Error']['Code'] == 'NoSuchKey':\n            return {}\n\n        else:\n            raise ce\n\n    if sys.version_info > (3,):\n        data = data.decode('utf-8')\n\n    return json.loads(data)", "response": "Tries to load JSON data from S3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the given items to the given bucket in S3.", "response": "def save_file(client, bucket, data_file, items, dry_run=None):\n    \"\"\"Tries to write JSON data to data file in S3.\"\"\"\n    logger.debug('Writing {number_items} items to s3. Bucket: {bucket} Key: {key}'.format(\n        number_items=len(items),\n        bucket=bucket,\n        key=data_file\n    ))\n\n    if not dry_run:\n        return _put_to_s3(client, bucket, data_file, json.dumps(items))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new item in file.", "response": "def create(self, item, dry_run=None):\n        \"\"\"Creates a new item in file.\"\"\"\n        logger.debug('Creating new item. Item: {item} Path: {data_file}'.format(\n            item=item,\n            data_file=self.data_file\n        ))\n\n        items = load_file(self.client, self.bucket_name, self.data_file)\n        items = append_item(self.namespace, self.version, item, items)\n        save_file(self.client, self.bucket_name, self.data_file, items, dry_run=dry_run)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the item info in file.", "response": "def update(self, item, dry_run=None):\n        \"\"\"Updates item info in file.\"\"\"\n        logger.debug('Updating item. Item: {item} Path: {data_file}'.format(\n            item=item,\n            data_file=self.data_file\n        ))\n        self.delete(item, dry_run=dry_run)\n        return self.create(item, dry_run=dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all items in file.", "response": "def get_all(self):\n        \"\"\"Gets all items in file.\"\"\"\n        logger.debug('Fetching items. Path: {data_file}'.format(\n            data_file=self.data_file\n        ))\n\n        return load_file(self.client, self.bucket_name, self.data_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef health_check(self):\n        logger.debug('Health Check on S3 file for: {namespace}'.format(\n            namespace=self.namespace\n        ))\n\n        try:\n            self.client.head_object(Bucket=self.bucket_name, Key=self.data_file)\n            return True\n        except ClientError as e:\n            logger.debug('Error encountered with S3.  Assume unhealthy')", "response": "Checks if the file exists in S3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting item in file.", "response": "def delete(self, item, dry_run=None):\n        \"\"\"Deletes item in file.\"\"\"\n        logger.debug('Deleting item. Item: {item} Table: {namespace}'.format(\n            item=item,\n            namespace=self.namespace\n        ))\n\n        if not dry_run:\n            self.table.delete_item(Key={'id': item['id']})\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, item, dry_run=None):\n        logger.debug('Updating item. Item: {item} Table: {namespace}'.format(\n            item=item,\n            namespace=self.namespace\n        ))\n\n        if not dry_run:\n            self.table.put_item(Item=item)\n\n        return item", "response": "Updates the item info in file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all items in file.", "response": "def get_all(self):\n        \"\"\"Gets all items in file.\"\"\"\n        logger.debug('Fetching items. Table: {namespace}'.format(\n            namespace=self.namespace\n        ))\n\n        rows = []\n\n        result = self.table.scan()\n\n        while True:\n            next_token = result.get('LastEvaluatedKey', None)\n            rows += result['Items']\n\n            if next_token:\n                result = self.table.scan(ExclusiveStartKey=next_token)\n            else:\n                break\n\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef health_check(self):\n        logger.debug('Health Check on Table: {namespace}'.format(\n            namespace=self.namespace\n        ))\n\n        try:\n            self.get_all()\n            return True\n\n        except ClientError as e:\n            logger.exception(e)\n            logger.error('Error encountered with Database. Assume unhealthy')\n            return False", "response": "Checks if Dynamo is functioning. Returns True if Dynamo is functioning False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_swag_config_options(config):\n    options = {}\n    for key, val in config.items():\n        if key.startswith('swag.backend.'):\n            options[key[12:]] = val\n        if key.startswith('swag.'):\n            options[key[5:]] = val\n\n    if options.get('type') == 's3':\n        return S3OptionsSchema(strict=True).load(options).data\n    elif options.get('type') == 'dynamodb':\n        return DynamoDBOptionsSchema(strict=True).load(options).data\n    else:\n        return FileOptionsSchema(strict=True).load(options).data", "response": "Ensures that options passed to the backend are valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an account name attempts to retrieve associated account info.", "response": "def get_by_name(account_name, bucket, region='us-west-2', json_path='accounts.json', alias=None):\n    \"\"\"Given an account name, attempts to retrieve associated account info.\"\"\"\n    for account in get_all_accounts(bucket, region, json_path)['accounts']:\n        if 'aws' in account['type']:\n            if account['name'] == account_name:\n                return account\n            elif alias:\n                for a in account['alias']:\n                    if a == account_name:\n                        return account"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_by_aws_account_number(account_number, bucket, region='us-west-2', json_path='accounts.json'):\n    for account in get_all_accounts(bucket, region, json_path)['accounts']:\n        if 'aws' in account['type']:\n            if account['metadata']['account_number'] == account_number:\n                return account", "response": "Given an account number attempts to retrieve associated account info."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_accounts(bucket, region='us-west-2', json_path='accounts.json', **filters):\n    swag_opts = {\n        'swag.type': 's3',\n        'swag.bucket_name': bucket,\n        'swag.bucket_region': region,\n        'swag.data_file': json_path,\n        'swag.schema_version': 1\n    }\n\n    swag = SWAGManager(**parse_swag_config_options(swag_opts))\n    accounts = swag.get_all()\n    accounts = [account for account in accounts['accounts'] if is_sub_dict(filters, account)]\n    return {'accounts': accounts}", "response": "Fetches all the accounts from SWAG."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_file(data_file):\n    try:\n        with open(data_file, 'r', encoding='utf-8') as f:\n            return json.loads(f.read())\n\n    except JSONDecodeError as e:\n        return []", "response": "Tries to load JSON from data file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_file(data_file, data, dry_run=None):\n    if dry_run:\n        return\n\n    with open(data_file, 'w', encoding='utf-8') as f:\n        if sys.version_info > (3, 0):\n            f.write(json.dumps(data))\n        else:\n            f.write(json.dumps(data).decode('utf-8'))", "response": "Writes JSON data to data file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the item in file.", "response": "def delete(self, item, dry_run=None):\n        \"\"\"Deletes item in file.\"\"\"\n        logger.debug('Deleting item. Item: {item} Path: {data_file}'.format(\n            item=item,\n            data_file=self.data_file\n        ))\n\n        items = load_file(self.data_file)\n        items = remove_item(self.namespace, self.version, item, items)\n        save_file(self.data_file, items, dry_run=dry_run)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all items in file.", "response": "def get_all(self):\n        \"\"\"Gets all items in file.\"\"\"\n        logger.debug('Fetching items. Path: {data_file}'.format(\n            data_file=self.data_file\n        ))\n\n        return load_file(self.data_file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking to make sure the file is there.", "response": "def health_check(self):\n        \"\"\"Checks to make sure the file is there.\"\"\"\n        logger.debug('Health Check on file for: {namespace}'.format(\n            namespace=self.namespace\n        ))\n\n        return os.path.isfile(self.data_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates item against version schema.", "response": "def validate(item, namespace='accounts', version=2, context=None):\n    \"\"\"Validate item against version schema.\n    \n    Args:\n        item: data object\n        namespace: backend namespace\n        version: schema version\n        context: schema context object\n    \"\"\"\n    if namespace == 'accounts':\n        if version == 2:\n            schema = v2.AccountSchema(strict=True, context=context)\n            return schema.load(item).data\n        elif version == 1:\n            return v1.AccountSchema(strict=True).load(item).data\n        raise InvalidSWAGDataException('Schema version is not supported. Version: {}'.format(version))\n    raise InvalidSWAGDataException('Namespace not supported. Namespace: {}'.format(namespace))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring a SWAG manager. Overrides existing configuration.", "response": "def configure(self, *args, **kwargs):\n        \"\"\"Configures a SWAG manager. Overrides existing configuration.\"\"\"\n\n        self.version = kwargs['schema_version']\n        self.namespace = kwargs['namespace']\n        self.backend = get(kwargs['type'])(*args, **kwargs)\n        self.context = kwargs.pop('schema_context', {})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new item in backend.", "response": "def create(self, item, dry_run=None):\n        \"\"\"Create a new item in backend.\"\"\"\n        return self.backend.create(validate(item, version=self.version, context=self.context), dry_run=dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, item, dry_run=None):\n        return self.backend.delete(item, dry_run=dry_run)", "response": "Delete an item in backend."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate an item in backend.", "response": "def update(self, item, dry_run=None):\n        \"\"\"Update an item in backend.\"\"\"\n        return self.backend.update(validate(item, version=self.version, context=self.context), dry_run=dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all(self, search_filter=None):\n        items = self.backend.get_all()\n\n        if not items:\n            if self.version == 1:\n                return {self.namespace: []}\n            return []\n\n        if search_filter:\n            items = jmespath.search(search_filter, items)\n\n        return items", "response": "Fetch all data from backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_service_enabled(self, name, accounts_list=None, search_filter=None, region=None):\n        if not accounts_list:\n            accounts = self.get_all(search_filter=search_filter)\n        else:\n            accounts = accounts_list\n\n        if self.version == 1:\n            accounts = accounts['accounts']\n\n        enabled = []\n        for account in accounts:\n            if self.version == 1:\n                account_filter = \"accounts[?id=='{id}']\".format(id=account['id'])\n            else:\n                account_filter = \"[?id=='{id}']\".format(id=account['id'])\n\n            service = self.get_service(name, search_filter=account_filter)\n\n            if self.version == 1:\n                if service:\n                    service = service['enabled']  # no region information available in v1\n            else:\n                if not region:\n                    service_filter = \"status[?enabled]\"\n                else:\n                    service_filter = \"status[?(region=='{region}' || region=='all') && enabled]\".format(region=region)\n\n                service = jmespath.search(service_filter, service)\n\n            if service:\n                enabled.append(account)\n\n        return enabled", "response": "Get a list of accounts where a service has been enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches account name as referenced by a particular service.", "response": "def get_service_name(self, name, search_filter):\n        \"\"\"Fetch account name as referenced by a particular service. \"\"\"\n        service_filter = \"services[?name=='{}'].metadata.name\".format(name)\n        return one(jmespath.search(service_filter, self.get(search_filter)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all accounts with name specified optionally include aliases.", "response": "def get_by_name(self, name, alias=None):\n        \"\"\"Fetch all accounts with name specified, optionally include aliases.\"\"\"\n        search_filter = \"[?name=='{}']\".format(name)\n\n        if alias:\n            if self.version == 1:\n                search_filter = \"accounts[?name=='{name}' || contains(alias, '{name}')]\".format(name=name)\n\n            elif self.version == 2:\n                search_filter = \"[?name=='{name}' || contains(aliases, '{name}')]\".format(name=name)\n\n        return self.get_all(search_filter)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns migration against a data set.", "response": "def run_migration(data, version_start, version_end):\n    \"\"\"Runs migration against a data set.\"\"\"\n    items = []\n    if version_start == 1 and version_end == 2:\n        for item in data['accounts']:\n            items.append(v2.upgrade(item))\n\n    if version_start == 2 and version_end == 1:\n        for item in data:\n            items.append(v2.downgrade(item))\n        items = {'accounts': items}\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef upgrade(account):\n    environ = 'test'\n    if 'prod' in account['tags']:\n        environ = 'prod'\n\n    owner = 'netflix'\n    if not account['ours']:\n        owner = 'third-party'\n\n    services = []\n    if account['metadata'].get('s3_name'):\n        services.append(\n            dict(\n                name='s3',\n                metadata=dict(\n                    name=account['metadata']['s3_name']\n                ),\n                status=[\n                    dict(\n                        region='all',\n                        enabled=True\n                    )\n                ]\n            )\n        )\n\n    if account['metadata'].get('cloudtrail_index'):\n        services.append(\n            dict(\n                name='cloudtrail',\n                metadata=dict(\n                    esIndex=account['metadata']['cloudtrail_index'],\n                    kibanaUrl=account['metadata']['cloudtrail_kibana_url']\n                ),\n                status=[\n                    dict(\n                        region='all',\n                        enabled=True\n                    )\n                ]\n            )\n        )\n\n    if account.get('bastion'):\n        services.append(\n            dict(\n                name='bastion',\n                metadata=dict(\n                    hostname=account['bastion']\n                ),\n                status=[\n                    dict(\n                        region='all',\n                        enabled=True\n                    )\n                ]\n            )\n        )\n\n    for service in account['services'].keys():\n        s = dict(\n            name=service,\n            status=[\n                dict(\n                    region='all',\n                    enabled=account['services'][service].get('enabled', True)\n                )\n            ]\n        )\n\n        if service == 'spinnaker':\n            s['metadata'] = {'name': account['services'][service]['name']}\n\n        if service == 'lazyfalcon':\n            if account['services'][service].get('owner'):\n                s['metadata'] = {'owner': account['services'][service]['owner']}\n\n        if service == 'titus':\n            s['metadata'] = {'stacks': account['services'][service]['stacks']}\n\n        services.append(s)\n\n    if account['metadata'].get('project_id'):\n        item_id = account['metadata']['project_id']\n    elif account['metadata'].get('account_number'):\n        item_id = account['metadata']['account_number']\n    else:\n        raise Exception('No id found, are you sure this is in v1 swag format.')\n\n    status = []\n    if account['type'] == 'aws':\n        status = [\n            {\n                'region': 'us-east-1',\n                'status': 'ready'\n            },\n            {\n                'region': 'us-west-2',\n                'status': 'ready'\n            },\n            {\n                'region': 'eu-west-1',\n                'status': 'ready'\n            },\n            {\n                'region': 'us-east-2',\n                'status': 'in-active'\n            },\n            {\n                'region': 'us-west-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ca-central-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ap-south-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ap-northeast-2',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ap-northeast-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ap-southeast-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'ap-southeast-2',\n                'status': 'in-active'\n            },\n            {\n                'region': 'eu-west-2',\n                'status': 'in-active'\n            },\n            {\n                'region': 'eu-central-1',\n                'status': 'in-active'\n            },\n            {\n                'region': 'sa-east-1',\n                'status': 'in-active'\n            },\n        ]\n\n    return dict(\n        id=item_id,\n        email=account['metadata'].get('email'),\n        name=account['name'],\n        contacts=account['owners'],\n        provider=account['type'],\n        status=status,\n        tags=list(set(account['tags'])),\n        environment=environ,\n        description=account['description'],\n        sensitive=account['cmc_required'],\n        owner=owner,\n        aliases=account['alias'],\n        services=services,\n        account_status=account['account_status']\n    )", "response": "Transforms data from a v1 format to a v2 format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef downgrade(account):\n    d_account = dict(schema_version=1, metadata={'email': account['email']},\n                     tags=list(set([account['environment']] + account.get('tags', []))))\n\n    v1_services = {}\n    for service in account.get('services', []):\n        if service['name'] == 's3':\n            if service['metadata'].get('name'):\n                d_account['metadata']['s3_name'] = service['metadata']['name']\n\n        elif service['name'] == 'cloudtrail':\n            d_account['metadata']['cloudtrail_index'] = service['metadata']['esIndex']\n            d_account['metadata']['cloudtrail_kibana_url'] = service['metadata']['kibanaUrl']\n\n        elif service['name'] == 'bastion':\n            d_account['bastion'] = service['metadata']['hostname']\n\n        elif service['name'] == 'titus':\n            v1_services['titus'] = {\n                'stacks': service['metadata']['stacks'],\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'spinnaker':\n            v1_services['spinnaker'] = {\n                'name': service['metadata'].get('name', account[\"name\"]),\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'awwwdit':\n            v1_services['awwwdit'] = {\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'security_monkey':\n            v1_services['security_monkey'] = {\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'poseidon':\n            v1_services['poseidon'] = {\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'rolliepollie':\n            v1_services['rolliepollie'] = {\n                'enabled': service['status'][0]['enabled']\n            }\n\n        elif service['name'] == 'lazyfalcon':\n            owner = None\n\n            if service.get('metadata'):\n                if service['metadata'].get('owner'):\n                    owner = service['metadata']['owner']\n\n            v1_services['lazyfalcon'] = {\n                'enabled': service['status'][0]['enabled'],\n                'owner': owner\n            }\n\n    if account['provider'] == 'aws':\n        d_account['metadata']['account_number'] = account['id']\n\n    elif account['provider'] == 'gcp':\n        d_account['metadata']['project_id'] = account['id']\n\n    d_account['id'] = account['provider'] + '-' + account['id']\n    d_account['cmc_required'] = account['sensitive']\n    d_account['name'] = account['name']\n    d_account['alias'] = account['aliases']\n    d_account['description'] = account['description']\n    d_account['owners'] = account['contacts']\n    d_account['type'] = account['provider']\n    d_account['ours'] = True if account['owner'] == 'netflix' else False\n    d_account['netflix'] = True if account['owner'] == 'netflix' else False\n    d_account['services'] = v1_services\n    d_account['account_status'] = account['account_status']\n\n    return d_account", "response": "Transforms data from v2 format to v1 format"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the type of the resource is one of the allowed values.", "response": "def validate_type(self, data):\n        \"\"\"Performs field validation against the schema context\n        if values have been provided to SWAGManager via the\n        swag.schema_context config object.\n\n        If the schema context for a given field is empty, then\n        we assume any value is valid for the given schema field.\n        \"\"\"\n        fields_to_validate = ['type', 'environment', 'owner']\n        for field in fields_to_validate:\n            value = data.get(field)\n            allowed_values = self.context.get(field)\n            if allowed_values and value not in allowed_values:\n                raise ValidationError('Must be one of {}'.format(allowed_values), field_names=field)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_account_status(self, data):\n        deleted_status = 'deleted'\n        region_status = data.get('status')\n        account_status = data.get('account_status')\n        for region in region_status:\n            if region['status'] != deleted_status and account_status == deleted_status:\n                raise ValidationError('Account Status cannot be \"deleted\" if a region is not \"deleted\"')", "response": "Validates that the account_status field is valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that the regions field is valid.", "response": "def validate_regions_schema(self, data):\n        \"\"\"Performs field validation for regions.  This should be\n        a dict with region names as the key and RegionSchema as the value\n        \"\"\"\n        region_schema = RegionSchema()\n        supplied_regions = data.get('regions', {})\n        for region in supplied_regions.keys():\n            result = region_schema.validate(supplied_regions[region])\n            if len(result.keys()) > 0:\n                raise ValidationError(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an iris coordinate to a HoloViews dimension.", "response": "def coord_to_dimension(coord):\n    \"\"\"\n    Converts an iris coordinate to a HoloViews dimension.\n    \"\"\"\n    kwargs = {}\n    if coord.units.is_time_reference():\n        kwargs['value_format'] = get_date_format(coord)\n    else:\n        kwargs['unit'] = str(coord.units)\n    return Dimension(coord.name(), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort a list of DimCoords using the order of the coordinates.", "response": "def sort_coords(coord):\n    \"\"\"\n    Sorts a list of DimCoords trying to ensure that\n    dates and pressure levels appear first and the\n    longitude and latitude appear last in the correct\n    order.\n    \"\"\"\n    import iris\n    order = {'T': -2, 'Z': -1, 'X': 1, 'Y': 2}\n    axis = iris.util.guess_coord_axis(coord)\n    return (order.get(axis, 0), coord and coord.name())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an array of the values along the supplied dimension.", "response": "def values(cls, dataset, dim, expanded=True, flat=True, compute=True):\n        \"\"\"\n        Returns an array of the values along the supplied dimension.\n        \"\"\"\n        dim = dataset.get_dimension(dim, strict=True)\n        if dim in dataset.vdims:\n            coord_names = [c.name() for c in dataset.data.dim_coords]\n            data = dataset.data.copy().data\n            data = cls.canonicalize(dataset, data, coord_names)\n            return data.T.flatten() if flat else data\n        elif expanded:\n            data = cls.coords(dataset, dim.name, expanded=True)\n            return data.T.flatten() if flat else data\n        else:\n            return cls.coords(dataset, dim.name, ordered=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groupby(cls, dataset, dims, container_type=HoloMap, group_type=None, **kwargs):\n        import iris\n\n        if not isinstance(dims, list): dims = [dims]\n        dims = [dataset.get_dimension(d, strict=True) for d in dims]\n        constraints = [d.name for d in dims]\n        slice_dims = [d for d in dataset.kdims if d not in dims]\n\n        # Update the kwargs appropriately for Element group types\n        group_kwargs = {}\n        group_type = dict if group_type == 'raw' else group_type\n        if issubclass(group_type, Element):\n            group_kwargs.update(util.get_param_values(dataset))\n            group_kwargs['kdims'] = slice_dims\n        group_kwargs.update(kwargs)\n\n        drop_dim = any(d not in group_kwargs['kdims'] for d in slice_dims)\n\n        unique_coords = product(*[cls.values(dataset, d, expanded=False)\n                                  for d in dims])\n        data = []\n        for key in unique_coords:\n            constraint = iris.Constraint(**dict(zip(constraints, key)))\n            extracted = dataset.data.extract(constraint)\n            if drop_dim:\n                extracted = group_type(extracted, kdims=slice_dims,\n                                       vdims=dataset.vdims).columns()\n            cube = group_type(extracted, **group_kwargs)\n            data.append((key, cube))\n        if issubclass(container_type, NdMapping):\n            with item_check(False), sorted_context(False):\n                return container_type(data, kdims=dims)\n        else:\n            return container_type(data)", "response": "Groups the data by one or more dimensions returning a container_type indexed by the grouped dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef concat_dim(cls, datasets, dim, vdims):\n        import iris\n        from iris.experimental.equalise_cubes import equalise_attributes\n\n        cubes = []\n        for c, cube in datasets.items():\n            cube = cube.copy()\n            cube.add_aux_coord(iris.coords.DimCoord([c], var_name=dim.name))\n            cubes.append(cube)\n        cubes = iris.cube.CubeList(cubes)\n        equalise_attributes(cubes)\n        return cubes.merge_cube()", "response": "Concatenates datasets along one dimension and returns a new list of cubes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the range along a particular dimension.", "response": "def range(cls, dataset, dimension):\n        \"\"\"\n        Computes the range along a particular dimension.\n        \"\"\"\n        dim = dataset.get_dimension(dimension, strict=True)\n        values = dataset.dimension_values(dim.name, False)\n        return (np.nanmin(values), np.nanmax(values))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming coords on the Cube.", "response": "def redim(cls, dataset, dimensions):\n        \"\"\"\n        Rename coords on the Cube.\n        \"\"\"\n        new_dataset = dataset.data.copy()\n        for name, new_dim in dimensions.items():\n            if name == new_dataset.name():\n                new_dataset.rename(new_dim.name)\n            for coord in new_dataset.dim_coords:\n                if name == coord.name():\n                    coord.rename(new_dim.name)\n        return new_dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the total number of samples in the dataset.", "response": "def length(cls, dataset):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n        \"\"\"\n        return np.product([len(d.points) for d in dataset.data.coords(dim_coords=True)], dtype=np.intp)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a key dimension to a dense representation.", "response": "def add_dimension(cls, columns, dimension, dim_pos, values, vdim):\n        \"\"\"\n        Adding value dimensions not currently supported by iris interface.\n        Adding key dimensions not possible on dense interfaces.\n        \"\"\"\n        if not vdim:\n            raise Exception(\"Cannot add key dimension to a dense representation.\")\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming a selection dictionary to an iris Constraint.", "response": "def select_to_constraint(cls, dataset, selection):\n        \"\"\"\n        Transform a selection dictionary to an iris Constraint.\n        \"\"\"\n        import iris\n\n        def get_slicer(start, end):\n            def slicer(cell):\n                return start <= cell.point < end\n            return slicer\n        constraint_kwargs = {}\n        for dim, constraint in selection.items():\n            if isinstance(constraint, slice):\n                constraint = (constraint.start, constraint.stop)\n            if isinstance(constraint, tuple):\n                if constraint == (None, None):\n                    continue\n                constraint = get_slicer(*constraint)\n            dim = dataset.get_dimension(dim, strict=True)\n            constraint_kwargs[dim.name] = constraint\n        return iris.Constraint(**constraint_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies a selection to the data.", "response": "def select(cls, dataset, selection_mask=None, **selection):\n        \"\"\"\n        Apply a selection to the data.\n        \"\"\"\n        import iris\n        constraint = cls.select_to_constraint(dataset, selection)\n        pre_dim_coords = [c.name() for c in dataset.data.dim_coords]\n        indexed = cls.indexed(dataset, selection)\n        extracted = dataset.data.extract(constraint)\n        if indexed and not extracted.dim_coords:\n            return extracted.data.item()\n        post_dim_coords = [c.name() for c in extracted.dim_coords]\n        dropped = [c for c in pre_dim_coords if c not in post_dim_coords]\n        for d in dropped:\n            extracted = iris.util.new_axis(extracted, d)\n\n        return extracted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a HoloViews element type to the equivalent GeoViews element.", "response": "def convert_to_geotype(element, crs=None):\n    \"\"\"\n    Converts a HoloViews element type to the equivalent GeoViews\n    element if given a coordinate reference system.\n    \"\"\"\n    geotype = getattr(gv_element, type(element).__name__, None)\n    if crs is None or geotype is None or isinstance(element, _Element):\n        return element\n    return geotype(element, crs=crs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the coordinate reference system of the supplied object.", "response": "def find_crs(op, element):\n    \"\"\"\n    Traverses the supplied object looking for coordinate reference\n    systems (crs). If multiple clashing reference systems are found\n    it will throw an error.\n    \"\"\"\n    crss = [crs for crs in element.traverse(lambda x: x.crs, [_Element])\n            if crs is not None]\n    if not crss:\n        return {}\n    crs = crss[0]\n    if any(crs != ocrs for ocrs in crss[1:]):\n        raise ValueError('Cannot %s Elements in different '\n                         'coordinate reference systems.'\n                         % type(op).__name__)\n    return {'crs': crs}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting any elements in the input to their equivalent geotypes", "response": "def add_crs(op, element, **kwargs):\n    \"\"\"\n    Converts any elements in the input to their equivalent geotypes\n    if given a coordinate reference system.\n    \"\"\"\n    return element.map(lambda x: convert_to_geotype(x, kwargs.get('crs')), Element)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the geometries of the current feature.", "response": "def geoms(self, scale=None, bounds=None, as_element=True):\n        \"\"\"\n        Returns the geometries held by the Feature.\n\n        Parameters\n        ----------\n        scale: str\n          Scale of the geometry to return expressed as string.\n          Available scales depends on the Feature type.\n\n          NaturalEarthFeature:\n           '10m', '50m', '110m'\n\n          GSHHSFeature:\n           'auto', 'coarse', 'low', 'intermediate', 'high', 'full'\n\n        bounds: tuple\n          Tuple of a bounding region to query for geometries in\n        as_element: boolean\n          Whether to wrap the geometries in an element\n\n        Returns\n        -------\n        geometries: Polygons/Path\n          Polygons or Path object wrapping around returned geometries\n        \"\"\"\n        feature = self.data\n        if scale is not None:\n            feature = feature.with_scale(scale)\n\n        if bounds:\n            extent = (bounds[0], bounds[2], bounds[1], bounds[3])\n        else:\n            extent = None\n        geoms = [g for g in feature.intersecting_geometries(extent) if g is not None]\n        if not as_element:\n            return geoms\n        elif not geoms or 'Polygon' in geoms[0].geom_type:\n            return Polygons(geoms, crs=feature.crs)\n        elif 'Point' in geoms[0].geom_type:\n            return Points(geoms, crs=feature.crs)\n        else:\n            return Path(geoms, crs=feature.crs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the edgepaths of the nodes in the current node.", "response": "def edgepaths(self):\n        \"\"\"\n        Returns the fixed EdgePaths or computes direct connections\n        between supplied nodes.\n        \"\"\"\n        edgepaths = super(Graph, self).edgepaths\n        edgepaths.crs = self.crs\n        return edgepaths"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the edgepaths of the current node", "response": "def edgepaths(self):\n        \"\"\"\n        Returns the fixed EdgePaths or computes direct connections\n        between supplied nodes.\n        \"\"\"\n        edgepaths = super(TriMesh, self).edgepaths\n        edgepaths.crs = self.crs\n        return edgepaths"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a shapefile from disk and optionally merges it with a dataset.", "response": "def from_shapefile(cls, shapefile, *args, **kwargs):\n        \"\"\"\n        Loads a shapefile from disk and optionally merges\n        it with a dataset. See ``from_records`` for full\n        signature.\n\n        Parameters\n        ----------\n        records: list of cartopy.io.shapereader.Record\n           Iterator containing Records.\n        dataset: holoviews.Dataset\n           Any HoloViews Dataset type.\n        on: str or list or dict\n          A mapping between the attribute names in the records and the\n          dimensions in the dataset.\n        value: str\n          The value dimension in the dataset the values will be drawn\n          from.\n        index: str or list\n          One or more dimensions in the dataset the Shapes will be\n          indexed by.\n        drop_missing: boolean\n          Whether to drop shapes which are missing from the provides\n          dataset.\n\n        Returns\n        -------\n        shapes: Polygons or Path object\n          A Polygons or Path object containing the geometries\n        \"\"\"\n        reader = Reader(shapefile)\n        return cls.from_records(reader.records(), *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading data from a collection of records and optionally merge them with a dataset.", "response": "def from_records(cls, records, dataset=None, on=None, value=None,\n                     index=[], drop_missing=False, element=None, **kwargs):\n        \"\"\"\n        Load data from a collection of `cartopy.io.shapereader.Record`\n        objects and optionally merge it with a dataset to assign\n        values to each polygon and form a chloropleth. Supplying just\n        records will return an NdOverlayof Shape Elements with a\n        numeric index. If a dataset is supplied, a mapping between the\n        attribute names in the records and the dimension names in the\n        dataset must be supplied. The values assigned to each shape\n        file can then be drawn from the dataset by supplying a\n        ``value`` and keys the Shapes are indexed by specifying one or\n        index dimensions.\n\n        Parameters\n        ----------\n        records: list of cartopy.io.shapereader.Record\n           Iterator containing Records.\n        dataset: holoviews.Dataset\n           Any HoloViews Dataset type.\n        on: str or list or dict\n          A mapping between the attribute names in the records and the\n          dimensions in the dataset.\n        value: str\n          The value dimension in the dataset the values will be drawn\n          from.\n        index: str or list\n          One or more dimensions in the dataset the Shapes will be\n          indexed by.\n        drop_missing: boolean\n          Whether to drop shapes which are missing from the provides\n          dataset.\n\n        Returns\n        -------\n        shapes: Polygons or Path object\n          A Polygons or Path object containing the geometries\n        \"\"\"\n        if dataset is not None and not on:\n            raise ValueError('To merge dataset with shapes mapping '\n                             'must define attribute(s) to merge on.')\n\n        if util.pd and isinstance(dataset, util.pd.DataFrame):\n            dataset = Dataset(dataset)\n\n        if not isinstance(on, (dict, list)):\n            on = [on]\n        if on and not isinstance(on, dict):\n            on = {o: o for o in on}\n        if not isinstance(index, list):\n            index = [index]\n\n        kdims = []\n        for ind in index:\n            if dataset and dataset.get_dimension(ind):\n                dim = dataset.get_dimension(ind)\n            else:\n                dim = Dimension(ind)\n            kdims.append(dim)\n\n\n        ddims = []\n        if dataset:\n            if value:\n                vdims = [dataset.get_dimension(value)]\n            else:\n                vdims = dataset.vdims\n            ddims = dataset.dimensions()\n            if None in vdims:\n                raise ValueError('Value dimension %s not found '\n                                 'in dataset dimensions %s' % (value, ddims) )\n        else:\n            vdims = []\n\n        data = []\n        for i, rec in enumerate(records):\n            geom = {}\n            if dataset:\n                selection = {dim: rec.attributes.get(attr, None)\n                             for attr, dim in on.items()}\n                row = dataset.select(**selection)\n                if len(row):\n                    values = {k: v[0] for k, v in row.iloc[0].columns().items()}\n                elif drop_missing:\n                    continue\n                else:\n                    values = {vd.name: np.nan for vd in vdims}\n                geom.update(values)\n\n            if index:\n                for kdim in kdims:\n                    if kdim in ddims and len(row):\n                        k = row[kdim.name][0]\n                    elif kdim.name in rec.attributes:\n                        k = rec.attributes[kdim.name]\n                    else:\n                        k = None\n                    geom[kdim.name] = k\n            geom['geometry'] = rec.geometry\n            data.append(geom)\n\n        if element is not None:\n            pass\n        elif data and data[0]:\n            if isinstance(data[0]['geometry'], poly_types):\n                element = Polygons\n            else:\n                element = Path\n        else:\n            element = Polygons\n\n        return element(data, vdims=kdims+vdims, **kwargs).opts(color=value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the first subplot that is triggered by the corresponding stream.", "response": "def get_cb_plot(cb, plot=None):\n    \"\"\"\n    Finds the subplot with the corresponding stream.\n    \"\"\"\n    plot = plot or cb.plot\n    if isinstance(plot, GeoOverlayPlot):\n        plots = [get_cb_plot(cb, p) for p in plot.subplots.values()]\n        plots = [p for p in plots if any(s in cb.streams and getattr(s, '_triggering', False)\n                                         for s in p.streams)]\n        if plots:\n            plot = plots[0]\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef skip(cb, msg, attributes):\n    if not all(a in msg for a in attributes):\n        return True\n    plot = get_cb_plot(cb)\n    return (not getattr(plot, 'geographic', False) or\n            not hasattr(plot.current_frame, 'crs'))", "response": "Skips applying transforms if data is not geographic."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef project_ranges(cb, msg, attributes):\n    if skip(cb, msg, attributes):\n        return msg\n\n    plot = get_cb_plot(cb)\n    x0, x1 = msg.get('x_range', (0, 1000))\n    y0, y1 = msg.get('y_range', (0, 1000))\n    extents = x0, y0, x1, y1\n    x0, y0, x1, y1 = project_extents(extents, plot.projection,\n                                     plot.current_frame.crs)\n    coords = {'x_range': (x0, x1), 'y_range': (y0, y1)}\n    return {k: v for k, v in coords.items() if k in attributes}", "response": "Projects ranges supplied by a callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef project_point(cb, msg, attributes=('x', 'y')):\n    if skip(cb, msg, attributes): return msg\n    plot = get_cb_plot(cb)\n    x, y = msg.get('x', 0), msg.get('y', 0)\n    crs = plot.current_frame.crs\n    coordinates = crs.transform_points(plot.projection, np.array([x]), np.array([y]))\n    msg['x'], msg['y'] = coordinates[0, :2]\n    return {k: v for k, v in msg.items() if k in attributes}", "response": "Projects a single point supplied by a callback"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef project_drawn(cb, msg):\n    stream = cb.streams[0]\n    old_data = stream.data\n    stream.update(data=msg['data'])\n    element = stream.element\n    stream.update(data=old_data)\n    proj = cb.plot.projection\n    if not isinstance(element, _Element) or element.crs == proj:\n        return None\n    crs = element.crs\n    element.crs = proj\n    return project(element, projection=crs)", "response": "Projects a drawn element to the declared coordinate system"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_weight_files(cls):\n        deleted = []\n        for f in cls._files:\n            try:\n                os.remove(f)\n                deleted.append(f)\n            except FileNotFoundError:\n                pass\n        print('Deleted %d weight files' % len(deleted))\n        cls._files = []", "response": "Cleans existing weight files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_projection(el):\n    result = None\n    if hasattr(el, 'crs'):\n        result = (int(el._auxiliary_component), el.crs)\n    return result", "response": "Get coordinate reference system from non - auxiliary elements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the extents of the specified element using the GeoAxes algorithm.", "response": "def get_extents(self, element, ranges, range_type='combined'):\n        \"\"\"\n        Subclasses the get_extents method using the GeoAxes\n        set_extent method to project the extents to the\n        Elements coordinate reference system.\n        \"\"\"\n        proj = self.projection\n        if self.global_extent and range_type in ('combined', 'data'):\n            (x0, x1), (y0, y1) = proj.x_limits, proj.y_limits\n            return (x0, y0, x1, y1)\n        extents = super(ProjectionPlot, self).get_extents(element, ranges, range_type)\n        if not getattr(element, 'crs', None) or not self.geographic:\n            return extents\n        elif any(e is None or not np.isfinite(e) for e in extents):\n            extents = None\n        else:\n            extents = project_extents(extents, element.crs, proj)\n        return (np.NaN,)*4 if not extents else extents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrap_lons(lons, base, period):\n    lons = lons.astype(np.float64)\n    return ((lons - base + period * 2) % period) + base", "response": "Wrap longitude values into the range between base and base + period."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geom_dict_to_array_dict(geom_dict, coord_names=['Longitude', 'Latitude']):\n    x, y = coord_names\n    geom = geom_dict['geometry']\n    new_dict = {k: v for k, v in geom_dict.items() if k != 'geometry'}\n    array = geom_to_array(geom)\n    new_dict[x] = array[:, 0]\n    new_dict[y] = array[:, 1]\n    if geom.geom_type == 'Polygon':\n        holes = []\n        for interior in geom.interiors:\n            holes.append(geom_to_array(interior))\n        if holes:\n            new_dict['holes'] = [holes]\n    elif geom.geom_type == 'MultiPolygon':\n        outer_holes = []\n        for g in geom:\n            holes = []\n            for interior in g.interiors:\n                holes.append(geom_to_array(interior))\n            outer_holes.append(holes)\n        if any(hs for hs in outer_holes):\n            new_dict['holes'] = outer_holes\n    return new_dict", "response": "Converts a dictionary containing an geometry key to a dictionary containing x - and y - coordinate arrays and if present a list - of - lists of holes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a Polygons element into a list of geometry dictionaries.", "response": "def polygons_to_geom_dicts(polygons, skip_invalid=True):\n    \"\"\"\n    Converts a Polygons element into a list of geometry dictionaries,\n    preserving all value dimensions.\n\n    For array conversion the following conventions are applied:\n\n    * Any nan separated array are converted into a MultiPolygon\n    * Any array without nans is converted to a Polygon\n    * If there are holes associated with a nan separated array\n      the holes are assigned to the polygons by testing for an\n      intersection\n    * If any single array does not have at least three coordinates\n      it is skipped by default\n    * If skip_invalid=False and an array has less than three\n      coordinates it will be converted to a LineString\n    \"\"\"\n    interface = polygons.interface.datatype\n    if interface == 'geodataframe':\n        return [row.to_dict() for _, row in polygons.data.iterrows()]\n    elif interface == 'geom_dictionary':\n        return polygons.data\n\n    polys = []\n    xdim, ydim = polygons.kdims\n    has_holes = polygons.has_holes\n    holes = polygons.holes() if has_holes else None\n    for i, polygon in enumerate(polygons.split(datatype='columns')):\n        array = np.column_stack([polygon.pop(xdim.name), polygon.pop(ydim.name)])\n        splits = np.where(np.isnan(array[:, :2].astype('float')).sum(axis=1))[0]\n        arrays = np.split(array, splits+1) if len(splits) else [array]\n\n        invalid = False\n        subpolys = []\n        subholes = None\n        if has_holes:\n            subholes = [[LinearRing(h) for h in hs] for hs in holes[i]]\n        for j, arr in enumerate(arrays):\n            if j != (len(arrays)-1):\n                arr = arr[:-1] # Drop nan\n\n            if len(arr) == 0:\n                continue\n            elif len(arr) == 1:\n                if skip_invalid:\n                    continue\n                poly = Point(arr[0])\n                invalid = True\n            elif len(arr) == 2:\n                if skip_invalid:\n                    continue\n                poly = LineString(arr)\n                invalid = True\n            elif not len(splits):\n                poly = Polygon(arr, (subholes[j] if has_holes else []))\n            else:\n                poly = Polygon(arr)\n                hs = [h for h in subholes[j]] if has_holes else []\n                poly = Polygon(poly.exterior, holes=hs)\n            subpolys.append(poly)\n\n        if invalid:\n            polys += [dict(polygon, geometry=sp) for sp in subpolys]\n            continue\n        elif len(subpolys) == 1:\n            geom = subpolys[0]\n        elif subpolys:\n            geom = MultiPolygon(subpolys)\n        else:\n            continue\n        polygon['geometry'] = geom\n        polys.append(polygon)\n    return polys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a Path element into a list of geometry dictionaries preserving all value dimensions.", "response": "def path_to_geom_dicts(path, skip_invalid=True):\n    \"\"\"\n    Converts a Path element into a list of geometry dictionaries,\n    preserving all value dimensions.\n    \"\"\"\n    interface = path.interface.datatype\n    if interface == 'geodataframe':\n        return [row.to_dict() for _, row in path.data.iterrows()]\n    elif interface == 'geom_dictionary':\n        return path.data\n\n    geoms = []\n    invalid = False\n    xdim, ydim = path.kdims\n    for i, path in enumerate(path.split(datatype='columns')):\n        array = np.column_stack([path.pop(xdim.name), path.pop(ydim.name)])\n        splits = np.where(np.isnan(array[:, :2].astype('float')).sum(axis=1))[0]\n        arrays = np.split(array, splits+1) if len(splits) else [array]\n        subpaths = []\n        for j, arr in enumerate(arrays):\n            if j != (len(arrays)-1):\n                arr = arr[:-1] # Drop nan\n\n            if len(arr) == 0:\n                continue\n            elif len(arr) == 1:\n                if skip_invalid:\n                    continue\n                g = Point(arr[0])\n                invalid = True\n            else:\n                g = LineString(arr)\n            subpaths.append(g)\n\n        if invalid:\n            geoms += [dict(path, geometry=sp) for sp in subpaths]\n            continue\n        elif len(subpaths) == 1:\n            geom = subpaths[0]\n        elif subpaths:\n            geom = MultiLineString(subpaths)\n        path['geometry'] = geom\n        geoms.append(path)\n    return geoms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a polygon to a CCW polygon.", "response": "def to_ccw(geom):\n    \"\"\"\n    Reorients polygon to be wound counter-clockwise.\n    \"\"\"\n    if isinstance(geom, sgeom.Polygon) and not geom.exterior.is_ccw:\n        geom = sgeom.polygon.orient(geom)\n    return geom"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geom_length(geom):\n    if geom.geom_type == 'Point':\n        return 1\n    if hasattr(geom, 'exterior'):\n        geom = geom.exterior\n    if not geom.geom_type.startswith('Multi') and hasattr(geom, 'array_interface_base'):\n        return len(geom.array_interface_base['data'])//2\n    else:\n        length = 0\n        for g in geom:\n            length += geom_length(g)\n        return length", "response": "Calculates the length of a shapely geometry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the mesh data from a 2D Element ensuring that the data isn t on a cylindrical coordinate system and wraps around.", "response": "def geo_mesh(element):\n    \"\"\"\n    Get mesh data from a 2D Element ensuring that if the data is\n    on a cylindrical coordinate system and wraps globally that data\n    actually wraps around.\n    \"\"\"\n    if len(element.vdims) > 1:\n        xs, ys = (element.dimension_values(i, False, False)\n                  for i in range(2))\n        zs = np.dstack([element.dimension_values(i, False, False)\n                        for i in range(2, 2+len(element.vdims))])\n    else:\n        xs, ys, zs = (element.dimension_values(i, False, False)\n                      for i in range(3))\n    lon0, lon1 = element.range(0)\n    if isinstance(element.crs, ccrs._CylindricalProjection) and (lon1 - lon0) == 360:\n        xs = np.append(xs, xs[0:1] + 360, axis=0)\n        zs = np.ma.concatenate([zs, zs[:, 0:1]], axis=1)\n    return xs, ys, zs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the crs represents a valid grid projection or ESPG string.", "response": "def check_crs(crs):\n    \"\"\"\n    Checks if the crs represents a valid grid, projection or ESPG string.\n\n    (Code copied from https://github.com/fmaussion/salem)\n\n    Examples\n    --------\n    >>> p = check_crs('+units=m +init=epsg:26915')\n    >>> p.srs\n    '+units=m +init=epsg:26915 '\n    >>> p = check_crs('wrong')\n    >>> p is None\n    True\n    Returns\n    -------\n    A valid crs if possible, otherwise None\n    \"\"\"\n    import pyproj\n    if isinstance(crs, pyproj.Proj):\n        out = crs\n    elif isinstance(crs, dict) or isinstance(crs, basestring):\n        try:\n            out = pyproj.Proj(crs)\n        except RuntimeError:\n            try:\n                out = pyproj.Proj(init=crs)\n            except RuntimeError:\n                out = None\n    else:\n        out = None\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a pyproj. Proj to a cartopy. ccrs. Projection object", "response": "def proj_to_cartopy(proj):\n    \"\"\"\n    Converts a pyproj.Proj to a cartopy.crs.Projection\n\n    (Code copied from https://github.com/fmaussion/salem)\n\n    Parameters\n    ----------\n    proj: pyproj.Proj\n        the projection to convert\n    Returns\n    -------\n    a cartopy.crs.Projection object\n    \"\"\"\n\n    import cartopy.crs as ccrs\n    try:\n        from osgeo import osr\n        has_gdal = True\n    except ImportError:\n        has_gdal = False\n\n    proj = check_crs(proj)\n\n    if proj.is_latlong():\n        return ccrs.PlateCarree()\n\n    srs = proj.srs\n    if has_gdal:\n        # this is more robust, as srs could be anything (espg, etc.)\n        s1 = osr.SpatialReference()\n        s1.ImportFromProj4(proj.srs)\n        srs = s1.ExportToProj4()\n\n    km_proj = {'lon_0': 'central_longitude',\n               'lat_0': 'central_latitude',\n               'x_0': 'false_easting',\n               'y_0': 'false_northing',\n               'k': 'scale_factor',\n               'zone': 'zone',\n               }\n    km_globe = {'a': 'semimajor_axis',\n                'b': 'semiminor_axis',\n                }\n    km_std = {'lat_1': 'lat_1',\n              'lat_2': 'lat_2',\n              }\n    kw_proj = dict()\n    kw_globe = dict()\n    kw_std = dict()\n    for s in srs.split('+'):\n        s = s.split('=')\n        if len(s) != 2:\n            continue\n        k = s[0].strip()\n        v = s[1].strip()\n        try:\n            v = float(v)\n        except:\n            pass\n        if k == 'proj':\n            if v == 'tmerc':\n                cl = ccrs.TransverseMercator\n            if v == 'lcc':\n                cl = ccrs.LambertConformal\n            if v == 'merc':\n                cl = ccrs.Mercator\n            if v == 'utm':\n                cl = ccrs.UTM\n        if k in km_proj:\n            kw_proj[km_proj[k]] = v\n        if k in km_globe:\n            kw_globe[km_globe[k]] = v\n        if k in km_std:\n            kw_std[km_std[k]] = v\n\n    globe = None\n    if kw_globe:\n        globe = ccrs.Globe(**kw_globe)\n    if kw_std:\n        kw_proj['standard_parallels'] = (kw_std['lat_1'], kw_std['lat_2'])\n\n    # mercatoooor\n    if cl.__name__ == 'Mercator':\n        kw_proj.pop('false_easting', None)\n        kw_proj.pop('false_northing', None)\n\n    return cl(globe=globe, **kw_proj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of objects that are part of the given CRS.", "response": "def process_crs(crs):\n    \"\"\"\n    Parses cartopy CRS definitions defined in one of a few formats:\n\n      1. EPSG codes:   Defined as string of the form \"EPSG: {code}\" or an integer\n      2. proj.4 string: Defined as string of the form \"{proj.4 string}\"\n      3. cartopy.crs.CRS instance\n      4. None defaults to crs.PlateCaree\n    \"\"\"\n    try:\n        import cartopy.crs as ccrs\n        import geoviews as gv # noqa\n        import pyproj\n    except:\n        raise ImportError('Geographic projection support requires GeoViews and cartopy.')\n\n    if crs is None:\n        return ccrs.PlateCarree()\n\n    if isinstance(crs, basestring) and crs.lower().startswith('epsg'):\n        try:\n            crs = ccrs.epsg(crs[5:].lstrip().rstrip())\n        except:\n            raise ValueError(\"Could not parse EPSG code as CRS, must be of the format 'EPSG: {code}.'\")\n    elif isinstance(crs, int):\n        crs = ccrs.epsg(crs)\n    elif isinstance(crs, (basestring, pyproj.Proj)):\n        try:\n            crs = proj_to_cartopy(crs)\n        except:\n            raise ValueError(\"Could not parse EPSG code as CRS, must be of the format 'proj4: {proj4 string}.'\")\n    elif not isinstance(crs, ccrs.CRS):\n        raise ValueError(\"Projection must be defined as a EPSG code, proj4 string, cartopy CRS or pyproj.Proj.\")\n    return crs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_tiff(filename, crs=None, apply_transform=False, nan_nodata=False, **kwargs):\n    try:\n        import xarray as xr\n    except:\n        raise ImportError('Loading tiffs requires xarray to be installed')\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        da = xr.open_rasterio(filename)\n    return from_xarray(da, crs, apply_transform, nan_nodata, **kwargs)", "response": "Loads an RGB or Image element from a geotiff file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a DataArray to an HoloViews element.", "response": "def from_xarray(da, crs=None, apply_transform=False, nan_nodata=False, **kwargs):\n    \"\"\"\n    Returns an RGB or Image element given an xarray DataArray\n    loaded using xr.open_rasterio.\n\n    If a crs attribute is present on the loaded data it will\n    attempt to decode it into a cartopy projection otherwise it\n    will default to a non-geographic HoloViews element.\n\n    Parameters\n    ----------\n    da: xarray.DataArray\n      DataArray to convert to element\n    crs: Cartopy CRS or EPSG string (optional)\n      Overrides CRS inferred from the data\n    apply_transform: boolean\n      Whether to apply affine transform if defined on the data\n    nan_nodata: boolean\n      If data contains nodata values convert them to NaNs\n    **kwargs:\n      Keyword arguments passed to the HoloViews/GeoViews element\n\n    Returns\n    -------\n    element: Image/RGB/QuadMesh element\n    \"\"\"\n    if crs:\n        kwargs['crs'] = crs\n    elif hasattr(da, 'crs'):\n        try:\n            kwargs['crs'] = process_crs(da.crs)\n        except:\n            param.main.warning('Could not decode projection from crs string %r, '\n                               'defaulting to non-geographic element.' % da.crs)\n\n    coords = list(da.coords)\n    if coords not in (['band', 'y', 'x'], ['y', 'x']):\n        from .element.geo import Dataset, HvDataset\n        el = Dataset if 'crs' in kwargs else HvDataset\n        return el(da, **kwargs)\n\n    if len(coords) == 2:\n        y, x = coords\n        bands = 1\n    else:\n        y, x = coords[1:]\n        bands = len(da.coords[coords[0]])\n\n    if apply_transform:\n        from affine import Affine\n        transform = Affine.from_gdal(*da.attrs['transform'][:6])\n        nx, ny = da.sizes[x], da.sizes[y]\n        xs, ys = np.meshgrid(np.arange(nx)+0.5, np.arange(ny)+0.5) * transform\n        data = (xs, ys)\n    else:\n        xres, yres = da.attrs['res'] if 'res' in da.attrs else (1, 1)\n        xs = da.coords[x][::-1] if xres < 0 else da.coords[x]\n        ys = da.coords[y][::-1] if yres < 0 else da.coords[y]\n\n    data = (xs, ys)\n    for b in range(bands):\n        values = da[b].values\n        if nan_nodata and da.attrs.get('nodatavals', []):\n\n            values = values.astype(float)\n            for d in da.attrs['nodatavals']:\n                values[values==d] = np.NaN\n        data += (values,)\n\n    if 'datatype' not in kwargs:\n        kwargs['datatype'] = ['xarray', 'grid', 'image']\n\n    if xs.ndim > 1:\n        from .element.geo import QuadMesh, HvQuadMesh\n        el = QuadMesh if 'crs' in kwargs else HvQuadMesh\n        el = el(data, [x, y], **kwargs)\n    elif bands < 3:\n        from .element.geo import Image, HvImage\n        el = Image if 'crs' in kwargs else HvImage\n        el = el(data, [x, y], **kwargs)\n    else:\n        from .element.geo import RGB, HvRGB\n        el = RGB if 'crs' in kwargs else HvRGB\n        vdims = el.vdims[:bands]\n        el = el(data, [x, y], vdims, **kwargs)\n    if hasattr(el.data, 'attrs'):\n        el.data.attrs = da.attrs\n    return el"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the index of a geometry in a list of geometries avoiding expensive equality checks of in operator.", "response": "def find_geom(geom, geoms):\n    \"\"\"\n    Returns the index of a geometry in a list of geometries avoiding\n    expensive equality checks of `in` operator.\n    \"\"\"\n    for i, g in enumerate(geoms):\n        if g is geom:\n            return i"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_zoom_level(bounds, domain, levels):\n    area_fraction = min(bounds.area/domain.area, 1)\n    return int(min(round(np.log2(1/area_fraction)), levels))", "response": "Compute a zoom level given a bounds polygon a domain and a number of zoom levels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bounds_to_poly(bounds):\n    x0, y0, x1, y1 = bounds\n    return Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])", "response": "Constructs a shapely Polygon from the provided bounds tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_glyph(self, plot, mapping, properties):\n        tile_source = mapping['tile_source']\n        level = properties.pop('level', 'underlay')\n        renderer = plot.add_tile(tile_source, level=level)\n        renderer.alpha = properties.get('alpha', 1)\n\n        # Remove save tool\n        plot.tools = [t for t in plot.tools if not isinstance(t, SaveTool)]\n        return renderer, tile_source", "response": "Initialize a new glyph object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the given json plan in dict format.", "response": "def get_assignment(self):\n        \"\"\"Parse the given json plan in dict format.\"\"\"\n        try:\n            plan = json.loads(open(self.args.plan_file_path).read())\n            return plan_to_assignment(plan)\n        except IOError:\n            self.log.exception(\n                'Given json file {file} not found.'\n                .format(file=self.args.plan_file_path),\n            )\n            raise\n        except ValueError:\n            self.log.exception(\n                'Given json file {file} could not be decoded.'\n                .format(file=self.args.plan_file_path),\n            )\n            raise\n        except KeyError:\n            self.log.exception(\n                'Given json file {file} could not be parsed in desired format.'\n                .format(file=self.args.plan_file_path),\n            )\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a list of requests to the consumer coordinator for the specified group and payloads.", "response": "def _send_consumer_aware_request(self, group, payloads, encoder_fn, decoder_fn):\n        \"\"\"\n        Send a list of requests to the consumer coordinator for the group\n        specified using the supplied encode/decode functions. As the payloads\n        that use consumer-aware requests do not contain the group (e.g.\n        OffsetFetchRequest), all payloads must be for a single group.\n        Arguments:\n        group: the name of the consumer group (str) the payloads are for\n        payloads: list of object-like entities with topic (str) and\n            partition (int) attributes; payloads with duplicate\n            topic+partition are not supported.\n        encode_fn: a method to encode the list of payloads to a request body,\n            must accept client_id, correlation_id, and payloads as\n            keyword arguments\n        decode_fn: a method to decode a response body into response objects.\n            The response objects must be object-like and have topic\n            and partition attributes\n        Returns:\n        List of response objects in the same order as the supplied payloads\n        \"\"\"\n        # encoders / decoders do not maintain ordering currently\n        # so we need to keep this so we can rebuild order before returning\n        original_ordering = [(p.topic, p.partition) for p in payloads]\n\n        retries = 0\n        broker = None\n        while not broker:\n            try:\n                broker = self._get_coordinator_for_group(group)\n            except (GroupCoordinatorNotAvailableError, GroupLoadInProgressError) as e:\n                if retries == CONSUMER_OFFSET_TOPIC_CREATION_RETRIES:\n                    raise e\n                time.sleep(CONSUMER_OFFSET_RETRY_INTERVAL_SEC)\n            retries += 1\n\n        # Send the list of request payloads and collect the responses and\n        # errors\n        responses = {}\n\n        def failed_payloads(payloads):\n            for payload in payloads:\n                topic_partition = (str(payload.topic), payload.partition)\n                responses[topic_partition] = FailedPayloadsError(payload)\n\n        host, port, afi = get_ip_port_afi(broker.host)\n        try:\n            conn = self._get_conn(host, broker.port, afi)\n        except ConnectionError:\n            failed_payloads(payloads)\n\n        else:\n            request = encoder_fn(payloads=payloads)\n            # decoder_fn=None signal that the server is expected to not\n            # send a response.  This probably only applies to\n            # ProduceRequest w/ acks = 0\n            future = conn.send(request)\n\n            while not future.is_done:\n                for r, f in conn.recv():\n                    f.success(r)\n\n            if future.failed():\n                failed_payloads(payloads)\n\n            elif not request.expect_response():\n                failed_payloads(payloads)\n\n            else:\n                for payload_response in decoder_fn(future.value):\n                    topic_partition = (str(payload_response.topic),\n                                       payload_response.partition)\n                    responses[topic_partition] = payload_response\n\n        # Return responses in the same order as provided\n        return [responses[tp] for tp in original_ordering]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of brokers in the form [ id host ]", "response": "def get_broker_list(cluster_config):\n    \"\"\"Returns a list of brokers in the form [(id: host)]\n\n    :param cluster_config: the configuration of the cluster\n    :type cluster_config: map\n    \"\"\"\n    with ZK(cluster_config) as zk:\n        brokers = sorted(list(zk.get_brokers().items()), key=itemgetter(0))\n        return [(id, data['host']) for id, data in brokers]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_broker_list(brokers, filter_by):\n    filter_by_set = set(filter_by)\n    return [(id, host) for id, host in brokers if id in filter_by_set]", "response": "Returns a sorted list of elements from brokers in the form [ id host ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_requests(hosts, jolokia_port, jolokia_prefix):\n    session = FuturesSession()\n    for host in hosts:\n        url = \"http://{host}:{port}/{prefix}/read/{key}\".format(\n            host=host,\n            port=jolokia_port,\n            prefix=jolokia_prefix,\n            key=UNDER_REPL_KEY,\n        )\n        yield host, session.get(url)", "response": "Generate a generator of requests to fetch the under replicated\n    partition number from the specified hosts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_cluster_status(hosts, jolokia_port, jolokia_prefix):\n    under_replicated = 0\n    missing_brokers = 0\n    for host, request in generate_requests(hosts, jolokia_port, jolokia_prefix):\n        try:\n            response = request.result()\n            if 400 <= response.status_code <= 599:\n                print(\"Got status code {0}. Exiting.\".format(response.status_code))\n                sys.exit(1)\n            json = response.json()\n            under_replicated += json['value']\n        except RequestException as e:\n            print(\"Broker {0} is down: {1}.\"\n                  \"This maybe because it is starting up\".format(host, e), file=sys.stderr)\n            missing_brokers += 1\n        except KeyError:\n            print(\"Cannot find the key, Kafka is probably still starting up\", file=sys.stderr)\n            missing_brokers += 1\n    return under_replicated, missing_brokers", "response": "Read and return the number of under replicated partitions and missing brokers from the specified hosts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the list of brokers that will be restarted.", "response": "def print_brokers(cluster_config, brokers):\n    \"\"\"Print the list of brokers that will be restarted.\n\n    :param cluster_config: the cluster configuration\n    :type cluster_config: map\n    :param brokers: the brokers that will be restarted\n    :type brokers: map of broker ids and host names\n    \"\"\"\n    print(\"Will restart the following brokers in {0}:\".format(cluster_config.name))\n    for id, host in brokers:\n        print(\"  {0}: {1}\".format(id, host))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking the user to confirm the execution of the current user. Return true if the user confirmed false otherwise.", "response": "def ask_confirmation():\n    \"\"\"Ask for confirmation to the user. Return true if the user confirmed\n    the execution, false otherwise.\n\n    :returns: bool\n    \"\"\"\n    while True:\n        print(\"Do you want to restart these brokers? \", end=\"\")\n        choice = input().lower()\n        if choice in ['yes', 'y']:\n            return True\n        elif choice in ['no', 'n']:\n            return False\n        else:\n            print(\"Please respond with 'yes' or 'no'\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_broker(host, connection, start_command, verbose):\n    _, stdout, stderr = connection.sudo_command(start_command)\n    if verbose:\n        report_stdout(host, stdout)\n        report_stderr(host, stderr)", "response": "Execute the start command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop_broker(host, connection, stop_command, verbose):\n    _, stdout, stderr = connection.sudo_command(stop_command)\n    if verbose:\n        report_stdout(host, stdout)\n        report_stderr(host, stderr)", "response": "Execute the stop command on the current node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for_stable_cluster(\n    hosts,\n    jolokia_port,\n    jolokia_prefix,\n    check_interval,\n    check_count,\n    unhealthy_time_limit,\n):\n    \"\"\"\n    Block the caller until the cluster can be considered stable.\n\n    :param hosts: list of brokers ip addresses\n    :type hosts: list of strings\n    :param jolokia_port: HTTP port for Jolokia\n    :type jolokia_port: integer\n    :param jolokia_prefix: HTTP prefix on the server for the Jolokia queries\n    :type jolokia_prefix: string\n    :param check_interval: the number of seconds it will wait between each check\n    :type check_interval: integer\n    :param check_count: the number of times the check should be positive before\n    restarting the next broker\n    :type check_count: integer\n    :param unhealthy_time_limit: the maximum number of seconds it will wait for\n    the cluster to become stable before exiting with error\n    :type unhealthy_time_limit: integer\n    \"\"\"\n    stable_counter = 0\n    max_checks = int(math.ceil(unhealthy_time_limit / check_interval))\n    for i in itertools.count():\n        partitions, brokers = read_cluster_status(\n            hosts,\n            jolokia_port,\n            jolokia_prefix,\n        )\n        if partitions or brokers:\n            stable_counter = 0\n        else:\n            stable_counter += 1\n        print(\n            \"Under replicated partitions: {p_count}, missing brokers: {b_count} ({stable}/{limit})\".format(\n                p_count=partitions,\n                b_count=brokers,\n                stable=stable_counter,\n                limit=check_count,\n            ))\n        if stable_counter >= check_count:\n            print(\"The cluster is stable\")\n            return\n        if i >= max_checks:\n            raise WaitTimeoutException()\n        time.sleep(check_interval)", "response": "Block the caller until the cluster is stable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a rolling restart on the specified brokers.", "response": "def execute_rolling_restart(\n    brokers,\n    jolokia_port,\n    jolokia_prefix,\n    check_interval,\n    check_count,\n    unhealthy_time_limit,\n    skip,\n    verbose,\n    pre_stop_task,\n    post_stop_task,\n    start_command,\n    stop_command,\n    ssh_password=None\n):\n    \"\"\"Execute the rolling restart on the specified brokers. It checks the\n    number of under replicated partitions on each broker, using Jolokia.\n\n    The check is performed at constant intervals, and a broker will be restarted\n    when all the brokers are answering and are reporting zero under replicated\n    partitions.\n\n    :param brokers: the brokers that will be restarted\n    :type brokers: map of broker ids and host names\n    :param jolokia_port: HTTP port for Jolokia\n    :type jolokia_port: integer\n    :param jolokia_prefix: HTTP prefix on the server for the Jolokia queries\n    :type jolokia_prefix: string\n    :param check_interval: the number of seconds it will wait between each check\n    :type check_interval: integer\n    :param check_count: the number of times the check should be positive before\n    restarting the next broker\n    :type check_count: integer\n    :param unhealthy_time_limit: the maximum number of seconds it will wait for\n    the cluster to become stable before exiting with error\n    :type unhealthy_time_limit: integer\n    :param skip: the number of brokers to skip\n    :type skip: integer\n    :param verbose: print commend execution information\n    :type verbose: bool\n    :param pre_stop_task: a list of tasks to execute before running stop\n    :type pre_stop_task: list\n    :param post_stop_task: a list of task to execute after running stop\n    :type post_stop_task: list\n    :param start_command: the start command for kafka\n    :type start_command: string\n    :param stop_command: the stop command for kafka\n    :type stop_command: string\n    :param ssh_password: The ssh password to use if needed\n    :type ssh_password: string\n    \"\"\"\n    all_hosts = [b[1] for b in brokers]\n    for n, host in enumerate(all_hosts[skip:]):\n        with ssh(host=host, forward_agent=True, sudoable=True, max_attempts=3, max_timeout=2,\n                 ssh_password=ssh_password) as connection:\n            execute_task(pre_stop_task, host)\n            wait_for_stable_cluster(\n                all_hosts,\n                jolokia_port,\n                jolokia_prefix,\n                check_interval,\n                1 if n == 0 else check_count,\n                unhealthy_time_limit,\n            )\n            print(\"Stopping {0} ({1}/{2})\".format(host, n + 1, len(all_hosts) - skip))\n            stop_broker(host, connection, stop_command, verbose)\n            execute_task(post_stop_task, host)\n        # we open a new SSH connection in case the hostname has a new IP\n        with ssh(host=host, forward_agent=True, sudoable=True, max_attempts=3, max_timeout=2,\n                 ssh_password=ssh_password) as connection:\n            print(\"Starting {0} ({1}/{2})\".format(host, n + 1, len(all_hosts) - skip))\n            start_broker(host, connection, start_command, verbose)\n    # Wait before terminating the script\n    wait_for_stable_cluster(\n        all_hosts,\n        jolokia_port,\n        jolokia_prefix,\n        check_interval,\n        check_count,\n        unhealthy_time_limit,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_opts(opts, brokers_num):\n    if opts.skip < 0 or opts.skip >= brokers_num:\n        print(\"Error: --skip must be >= 0 and < #brokers\")\n        return True\n    if opts.check_count < 0:\n        print(\"Error: --check-count must be >= 0\")\n        return True\n    if opts.unhealthy_time_limit < 0:\n        print(\"Error: --unhealthy-time-limit must be >= 0\")\n        return True\n    if opts.check_count == 0:\n        print(\"Warning: no check will be performed\")\n    if opts.check_interval < 0:\n        print(\"Error: --check-interval must be >= 0\")\n        return True\n    return False", "response": "Basic option validation. Returns True if the options are valid False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_broker_ids_subset(broker_ids, subset_ids):\n    all_ids = set(broker_ids)\n    valid = True\n    for subset_id in subset_ids:\n        valid = valid and subset_id in all_ids\n        if subset_id not in all_ids:\n            print(\"Error: user specified broker id {0} does not exist in cluster.\".format(subset_id))\n    return valid", "response": "Validate that broker ids to restart exist in the broker ids retrieved\n    from cluster config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread in a list of tasks provided by the user and returns two lists pre_stop_tasks and post_stop_tasks", "response": "def get_task_class(tasks, task_args):\n    \"\"\"Reads in a list of tasks provided by the user,\n    loads the appropiate task, and returns two lists,\n    pre_stop_tasks and post_stop_tasks\n    :param tasks: list of strings locating tasks to load\n    :type tasks: list\n    :param task_args: list of strings to be used as args\n    :type task_args: list\n    \"\"\"\n    pre_stop_tasks = []\n    post_stop_tasks = []\n    task_to_task_args = dict(list(zip(tasks, task_args)))\n    tasks_classes = [PreStopTask, PostStopTask]\n\n    for func, task_args in task_to_task_args.items():\n        for task_class in tasks_classes:\n            imported_class = dynamic_import(func, task_class)\n            if imported_class:\n                if task_class is PreStopTask:\n                    pre_stop_tasks.append(imported_class(task_args))\n                elif task_class is PostStopTask:\n                    post_stop_tasks.append(imported_class(task_args))\n                else:\n                    print(\"ERROR: Class is not a type of Pre/Post StopTask:\" + func)\n                    sys.exit(1)\n    return pre_stop_tasks, post_stop_tasks"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(\n            self,\n            cluster_config,\n            rg_parser,\n            partition_measurer,\n            cluster_balancer,\n            args,\n    ):\n        \"\"\"Initialize cluster_config, args, and zk then call run_command.\"\"\"\n        self.cluster_config = cluster_config\n        self.args = args\n        with ZK(self.cluster_config) as self.zk:\n            self.log.debug(\n                'Starting %s for cluster: %s and zookeeper: %s',\n                self.__class__.__name__,\n                self.cluster_config.name,\n                self.cluster_config.zookeeper,\n            )\n            brokers = self.zk.get_brokers()\n            assignment = self.zk.get_cluster_assignment()\n            pm = partition_measurer(\n                self.cluster_config,\n                brokers,\n                assignment,\n                args,\n            )\n            ct = ClusterTopology(\n                assignment,\n                brokers,\n                pm,\n                rg_parser.get_replication_group,\n            )\n            if len(ct.partitions) == 0:\n                self.log.info(\"The cluster is empty. No actions to perform.\")\n                return\n\n            # Exit if there is an on-going reassignment\n            if self.is_reassignment_pending():\n                self.log.error('Previous reassignment pending.')\n                sys.exit(1)\n\n            self.run_command(ct, cluster_balancer(ct, args))", "response": "Initialize the cluster and run the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_plan(self, plan, allow_rf_change=False):\n        if self.should_execute():\n            result = self.zk.execute_plan(plan, allow_rf_change=allow_rf_change)\n            if not result:\n                self.log.error('Plan execution unsuccessful.')\n                sys.exit(1)\n            else:\n                self.log.info(\n                    'Plan sent to zookeeper for reassignment successfully.',\n                )\n        else:\n            self.log.info('Proposed plan won\\'t be executed (--apply and confirmation needed).')", "response": "Save proposed - plan and execute the same if requested."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef should_execute(self):\n        return self.args.apply and (self.args.no_confirm or self.confirm_execution())", "response": "Confirm if proposed - plan should be executed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_reassignment_pending(self):\n        in_progress_plan = self.zk.get_pending_plan()\n        if in_progress_plan:\n            in_progress_partitions = in_progress_plan['partitions']\n            self.log.info(\n                'Previous re-assignment in progress for {count} partitions.'\n                ' Current partitions in re-assignment queue: {partitions}'\n                .format(\n                    count=len(in_progress_partitions),\n                    partitions=in_progress_partitions,\n                )\n            )\n            return True\n        else:\n            return False", "response": "Return True if there are reassignment tasks pending."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreducing the assignment based on the total actions.", "response": "def get_reduced_assignment(\n        self,\n        original_assignment,\n        cluster_topology,\n        max_partition_movements,\n        max_leader_only_changes,\n        max_movement_size=DEFAULT_MAX_MOVEMENT_SIZE,\n        force_progress=False,\n    ):\n        \"\"\"Reduce the assignment based on the total actions.\n\n        Actions represent actual partition movements\n        and/or changes in preferred leader.\n        Get the difference of original and proposed assignment\n        and take the subset of this plan for given limit.\n\n        Argument(s):\n        original_assignment:    Current assignment of cluster in zookeeper\n        cluster_topology:       Cluster topology containing the new proposed-assignment of cluster\n        max_partition_movements:Maximum number of partition-movements in\n                                final set of actions\n        max_leader_only_changes:Maximum number of actions with leader only changes\n        max_movement_size:      Maximum size, in bytes, to move in final set of actions\n        force_progress:         Whether to force progress if max_movement_size is too small\n        :return:\n        :reduced_assignment:    Final reduced assignment\n        \"\"\"\n        new_assignment = cluster_topology.assignment\n        if (not original_assignment or not new_assignment or\n                max_partition_movements < 0 or max_leader_only_changes < 0 or\n                max_movement_size < 0):\n            return {}\n\n        # The replica set stays the same for leaders only changes\n        leaders_changes = [\n            (t_p, new_assignment[t_p])\n            for t_p, replica in six.iteritems(original_assignment)\n            if replica != new_assignment[t_p] and\n            set(replica) == set(new_assignment[t_p])\n        ]\n\n        # The replica set is different for partitions changes\n        # Here we create a list of tuple ((topic, partion), # replica movements)\n        partition_change_count = [\n            (\n                t_p,\n                len(set(replica) - set(new_assignment[t_p])),\n            )\n            for t_p, replica in six.iteritems(original_assignment)\n            if set(replica) != set(new_assignment[t_p])\n        ]\n\n        self.log.info(\n            \"Total number of actions before reduction: %s.\",\n            len(partition_change_count) + len(leaders_changes),\n        )\n        # Extract reduced plan maximizing uniqueness of topics and ensuring we do not\n        # go over the max_movement_size\n        reduced_actions = self._extract_actions_unique_topics(\n            partition_change_count,\n            max_partition_movements,\n            cluster_topology,\n            max_movement_size,\n        )\n\n        # Ensure progress is made if force_progress=True\n        if len(reduced_actions) == 0 and force_progress:\n            smallest_size = min([cluster_topology.partitions[t_p[0]].size for t_p in partition_change_count])\n            self.log.warning(\n                '--max-movement-size={max_movement_size} is too small, using smallest size'\n                ' in set of partitions to move, {smallest_size} instead to force progress'.format(\n                    max_movement_size=max_movement_size,\n                    smallest_size=smallest_size,\n                )\n            )\n            max_movement_size = smallest_size\n            reduced_actions = self._extract_actions_unique_topics(\n                partition_change_count,\n                max_partition_movements,\n                cluster_topology,\n                max_movement_size,\n            )\n\n        reduced_partition_changes = [\n            (t_p, new_assignment[t_p]) for t_p in reduced_actions\n        ]\n        self.log.info(\n            \"Number of partition changes: %s.\"\n            \" Number of leader-only changes: %s\",\n            len(reduced_partition_changes),\n            min(max_leader_only_changes, len(leaders_changes)),\n        )\n        # Merge leaders and partition changes and generate the assignment\n        reduced_assignment = {\n            t_p: replicas\n            for t_p, replicas in (\n                reduced_partition_changes + leaders_changes[:max_leader_only_changes]\n            )\n        }\n        return reduced_assignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract_actions_unique_topics(self, movement_counts, max_movements, cluster_topology, max_movement_size):\n        # Group actions by topic\n        topic_actions = defaultdict(list)\n        for t_p, replica_change_cnt in movement_counts:\n            topic_actions[t_p[0]].append((t_p, replica_change_cnt))\n\n        # Create reduced assignment minimizing duplication of topics\n        extracted_actions = []\n        curr_movements = 0\n        curr_size = 0\n        action_available = True\n        while curr_movements < max_movements and curr_size <= max_movement_size and action_available:\n            action_available = False\n            for topic, actions in six.iteritems(topic_actions):\n                for action in actions:\n                    action_size = cluster_topology.partitions[action[0]].size\n                    if curr_movements + action[1] > max_movements or curr_size + action_size > max_movement_size:\n                        # Remove action since it won't be possible to use it\n                        actions.remove(action)\n                    else:\n                        # Append (topic, partition) to the list of movements\n                        action_available = True\n                        extracted_actions.append(action[0])\n                        curr_movements += action[1]\n                        curr_size += action_size\n                        actions.remove(action)\n                        break\n        return extracted_actions", "response": "Extract unique topics from the resultant cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef confirm_execution(self):\n        permit = ''\n        while permit.lower() not in ('yes', 'no'):\n            permit = input('Execute Proposed Plan? [yes/no] ')\n        if permit.lower() == 'yes':\n            return True\n        else:\n            return False", "response": "Confirm from your if proposed - plan be executed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_json_plan(self, proposed_layout, proposed_plan_file):\n        with open(proposed_plan_file, 'w') as output:\n            json.dump(proposed_layout, output)", "response": "Dump proposed json plan to given output file for future usage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the preferred leader with one of the given replicas.", "response": "def swap_leader(self, new_leader):\n        \"\"\"Change the preferred leader with one of\n        given replicas.\n\n        Note: Leaders for all the replicas of current\n        partition needs to be changed.\n        \"\"\"\n        # Replica set cannot be changed\n        assert(new_leader in self._replicas)\n        curr_leader = self.leader\n        idx = self._replicas.index(new_leader)\n        self._replicas[0], self._replicas[idx] = \\\n            self._replicas[idx], self._replicas[0]\n        return curr_leader"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace(self, source, dest):\n        for i, broker in enumerate(self.replicas):\n            if broker == source:\n                self.replicas[i] = dest\n                return", "response": "Replace source broker with destination broker in replica set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count_siblings(self, partitions):\n        count = sum(\n            int(self.topic == partition.topic)\n            for partition in partitions\n        )\n        return count", "response": "Count the number of siblings of the given partitions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of topic - partition metadata for all topics in the Kafka broker.", "response": "def get_topic_partition_metadata(hosts):\n    \"\"\"Returns topic-partition metadata from Kafka broker.\n\n    kafka-python 1.3+ doesn't include partition metadata information in\n    topic_partitions so we extract it from metadata ourselves.\n    \"\"\"\n    kafka_client = KafkaToolClient(hosts, timeout=10)\n    kafka_client.load_metadata_for_topics()\n    topic_partitions = kafka_client.topic_partitions\n    resp = kafka_client.send_metadata_request()\n\n    for _, topic, partitions in resp.topics:\n        for partition_error, partition, leader, replicas, isr in partitions:\n            if topic_partitions.get(topic, {}).get(partition) is not None:\n                topic_partitions[topic][partition] = PartitionMetadata(topic, partition, leader,\n                                                                       replicas, isr, partition_error)\n    return topic_partitions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_unavailable_brokers(zk, partition_metadata):\n    topic_data = zk.get_topics(partition_metadata.topic)\n    topic = partition_metadata.topic\n    partition = partition_metadata.partition\n    expected_replicas = set(topic_data[topic]['partitions'][str(partition)]['replicas'])\n    available_replicas = set(partition_metadata.replicas)\n    return expected_replicas - available_replicas", "response": "Returns the set of unavailable brokers from the difference of replica\n    set of given partition to the set of available replicas."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_topic_partition_with_error(cluster_config, error, fetch_unavailable_brokers=False):\n\n    metadata = get_topic_partition_metadata(cluster_config.broker_list)\n    affected_partitions = set()\n    if fetch_unavailable_brokers:\n        unavailable_brokers = set()\n    with ZK(cluster_config) as zk:\n        for partitions in metadata.values():\n            for partition_metadata in partitions.values():\n                if int(partition_metadata.error) == error:\n                    if fetch_unavailable_brokers:\n                        unavailable_brokers |= get_unavailable_brokers(zk, partition_metadata)\n                    affected_partitions.add((partition_metadata.topic, partition_metadata.partition))\n\n    if fetch_unavailable_brokers:\n        return affected_partitions, unavailable_brokers\n    else:\n        return affected_partitions", "response": "Fetches the metadata from the cluster and returns the set of topic - partitions\n    that affected by the specified error."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_current_consumer_offsets(\n    kafka_client,\n    group,\n    topics,\n    raise_on_error=True,\n):\n    \"\"\" Get current consumer offsets.\n\n    NOTE: This method does not refresh client metadata. It is up to the caller\n    to avoid using stale metadata.\n\n    If any partition leader is not available, the request fails for all the\n    other topics. This is the tradeoff of sending all topic requests in batch\n    and save both in performance and Kafka load.\n\n    :param kafka_client: a connected KafkaToolClient\n    :param group: kafka group_id\n    :param topics: topic list or dict {<topic>: [partitions]}\n    :param raise_on_error: if False the method ignores missing topics and\n      missing partitions. It still may fail on the request send.\n    :returns: a dict topic: partition: offset\n    :raises:\n      :py:class:`kafka_utils.util.error.UnknownTopic`: upon missing\n      topics and raise_on_error=True\n\n      :py:class:`kafka_utils.util.error.UnknownPartition`: upon missing\n      partitions and raise_on_error=True\n\n      FailedPayloadsError: upon send request error.\n    \"\"\"\n\n    topics = _verify_topics_and_partitions(kafka_client, topics, raise_on_error)\n\n    group_offset_reqs = [\n        OffsetFetchRequestPayload(topic, partition)\n        for topic, partitions in six.iteritems(topics)\n        for partition in partitions\n    ]\n\n    group_offsets = {}\n\n    send_api = kafka_client.send_offset_fetch_request_kafka\n\n    if group_offset_reqs:\n        # fail_on_error = False does not prevent network errors\n        group_resps = send_api(\n            group=group,\n            payloads=group_offset_reqs,\n            fail_on_error=False,\n            callback=pluck_topic_offset_or_zero_on_unknown,\n        )\n        for resp in group_resps:\n            group_offsets.setdefault(\n                resp.topic,\n                {},\n            )[resp.partition] = resp.offset\n\n    return group_offsets", "response": "Get current consumer offsets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the current topic watermarks for the given topics.", "response": "def get_topics_watermarks(kafka_client, topics, raise_on_error=True):\n    \"\"\" Get current topic watermarks.\n\n    NOTE: This method does not refresh client metadata. It is up to the caller\n    to use avoid using stale metadata.\n\n    If any partition leader is not available, the request fails for all the\n    other topics. This is the tradeoff of sending all topic requests in batch\n    and save both in performance and Kafka load.\n\n    :param kafka_client: a connected KafkaToolClient\n    :param topics: topic list or dict {<topic>: [partitions]}\n    :param raise_on_error: if False the method ignores missing topics\n      and missing partitions. It still may fail on the request send.\n    :returns: a dict topic: partition: Part\n    :raises:\n      :py:class:`~kafka_utils.util.error.UnknownTopic`: upon missing\n      topics and raise_on_error=True\n\n      :py:class:`~kafka_utils.util.error.UnknownPartition`: upon missing\n      partitions and raise_on_error=True\n\n      FailedPayloadsError: upon send request error.\n    \"\"\"\n    topics = _verify_topics_and_partitions(\n        kafka_client,\n        topics,\n        raise_on_error,\n    )\n    highmark_offset_reqs = []\n    lowmark_offset_reqs = []\n\n    for topic, partitions in six.iteritems(topics):\n        # Batch watermark requests\n        for partition in partitions:\n            # Request the the latest offset\n            highmark_offset_reqs.append(\n                OffsetRequestPayload(\n                    topic, partition, -1, max_offsets=1\n                )\n            )\n            # Request the earliest offset\n            lowmark_offset_reqs.append(\n                OffsetRequestPayload(\n                    topic, partition, -2, max_offsets=1\n                )\n            )\n\n    watermark_offsets = {}\n\n    if not (len(highmark_offset_reqs) + len(lowmark_offset_reqs)):\n        return watermark_offsets\n\n    # fail_on_error = False does not prevent network errors\n    highmark_resps = kafka_client.send_offset_request(\n        highmark_offset_reqs,\n        fail_on_error=False,\n        callback=_check_fetch_response_error,\n    )\n    lowmark_resps = kafka_client.send_offset_request(\n        lowmark_offset_reqs,\n        fail_on_error=False,\n        callback=_check_fetch_response_error,\n    )\n\n    # At this point highmark and lowmark should ideally have the same length.\n    assert len(highmark_resps) == len(lowmark_resps)\n    aggregated_offsets = defaultdict(lambda: defaultdict(dict))\n    for resp in highmark_resps:\n        aggregated_offsets[resp.topic][resp.partition]['highmark'] = \\\n            resp.offsets[0]\n    for resp in lowmark_resps:\n        aggregated_offsets[resp.topic][resp.partition]['lowmark'] = \\\n            resp.offsets[0]\n\n    for topic, partition_watermarks in six.iteritems(aggregated_offsets):\n        for partition, watermarks in six.iteritems(partition_watermarks):\n            watermark_offsets.setdefault(\n                topic,\n                {},\n            )[partition] = PartitionOffsets(\n                topic,\n                partition,\n                watermarks['highmark'],\n                watermarks['lowmark'],\n            )\n    return watermark_offsets"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadvance consumer offsets to the latest message in the topic partition.", "response": "def advance_consumer_offsets(\n    kafka_client,\n    group,\n    topics,\n    raise_on_error=True,\n):\n    \"\"\"Advance consumer offsets to the latest message in the topic\n    partition (the high watermark).\n\n    This method shall refresh the client metadata prior to updating\n    the offsets.\n\n    If any partition leader is not available, the request fails for all the\n    other topics. This is the tradeoff of sending all topic requests in batch\n    and save both in performance and Kafka load.\n\n    :param kafka_client: a connected KafkaToolClient\n    :param group: kafka group_id\n    :param topics: topic list or dict {<topic>: [partitions]}\n    :param raise_on_error: if False the method does not raise exceptions\n      on missing topics/partitions. It may still fail on the request send.\n    :returns: a list of errors for each partition offset update that failed.\n    :rtype: list [OffsetCommitError]\n    :raises:\n      :py:class:`kafka_utils.util.error.UnknownTopic`: upon missing\n      topics and raise_on_error=True\n\n      :py:class:`kafka_utils.util.error.UnknownPartition`: upon missing\n      partitions and raise_on_error=True\n\n      FailedPayloadsError: upon send request error.\n    \"\"\"\n    kafka_client.load_metadata_for_topics()\n\n    return _commit_offsets_to_watermark(\n        kafka_client, group, topics,\n        HIGH_WATERMARK, raise_on_error,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rewind_consumer_offsets(\n    kafka_client,\n    group,\n    topics,\n    raise_on_error=True,\n):\n    \"\"\"Rewind consumer offsets to the earliest message in the topic\n    partition (the low watermark).\n\n    This method shall refresh the client metadata prior to updating\n    the offsets.\n\n    If any partition leader is not available, the request fails for all the\n    other topics. This is the tradeoff of sending all topic requests in batch\n    and save both in performance and Kafka load.\n\n    :param kafka_client: a connected KafkaToolClient\n    :param group: kafka group_id\n    :param topics: topic list or dict {<topic>: [partitions]}\n    :param raise_on_error: if False the method does not raise exceptions\n      on missing topics/partitions. It may still fail on the request send.\n    :returns: a list of errors for each partition offset update that failed.\n    :rtype: list [OffsetCommitError]\n    :raises:\n      :py:class:`kafka_utils.util.error.UnknownTopic`: upon missing\n      topics and raise_on_error=True\n\n      :py:class:`kafka_utils.util.error.UnknownPartition`: upon missing\n      partitions and raise_on_error=True\n\n      FailedPayloadsError: upon send request error.\n    \"\"\"\n    kafka_client.load_metadata_for_topics()\n\n    return _commit_offsets_to_watermark(\n        kafka_client, group, topics,\n        LOW_WATERMARK, raise_on_error,\n    )", "response": "Rewind consumer offsets to the earliest message in the topic\n    partition."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset consumer offsets to the specified offsets. This method does not validate the specified offsets, it is up to the caller to specify valid offsets within a topic partition. If any partition leader is not available, the request fails for all the other topics. This is the tradeoff of sending all topic requests in batch and save both in performance and Kafka load. :param kafka_client: a connected KafkaToolClient :param group: kafka group_id :param topics: dict {<topic>: {<partition>: <offset>}} :param raise_on_error: if False the method does not raise exceptions on errors encountered. It may still fail on the request send. :returns: a list of errors for each partition offset update that failed. :rtype: list [OffsetCommitError] :raises: :py:class:`kafka_utils.util.error.UnknownTopic`: upon missing topics and raise_on_error=True :py:class:`kafka_utils.util.error.UnknownPartition`: upon missing partitions and raise_on_error=True :py:class:`exceptions.TypeError`: upon badly formatted input new_offsets FailedPayloadsError: upon send request error.", "response": "def set_consumer_offsets(\n    kafka_client,\n    group,\n    new_offsets,\n    raise_on_error=True,\n):\n    \"\"\"Set consumer offsets to the specified offsets.\n\n    This method does not validate the specified offsets, it is up to\n    the caller to specify valid offsets within a topic partition.\n\n    If any partition leader is not available, the request fails for all the\n    other topics. This is the tradeoff of sending all topic requests in batch\n    and save both in performance and Kafka load.\n\n    :param kafka_client: a connected KafkaToolClient\n    :param group: kafka group_id\n    :param topics: dict {<topic>: {<partition>: <offset>}}\n    :param raise_on_error: if False the method does not raise exceptions\n      on errors encountered. It may still fail on the request send.\n    :returns: a list of errors for each partition offset update that failed.\n    :rtype: list [OffsetCommitError]\n    :raises:\n      :py:class:`kafka_utils.util.error.UnknownTopic`: upon missing\n      topics and raise_on_error=True\n\n      :py:class:`kafka_utils.util.error.UnknownPartition`: upon missing\n      partitions and raise_on_error=True\n\n      :py:class:`exceptions.TypeError`: upon badly formatted input\n      new_offsets\n\n      FailedPayloadsError: upon send request error.\n    \"\"\"\n    valid_new_offsets = _verify_commit_offsets_requests(\n        kafka_client,\n        new_offsets,\n        raise_on_error\n    )\n\n    group_offset_reqs = [\n        OffsetCommitRequestPayload(\n            topic,\n            partition,\n            offset,\n            metadata='',\n        )\n        for topic, new_partition_offsets in six.iteritems(valid_new_offsets)\n        for partition, offset in six.iteritems(new_partition_offsets)\n    ]\n\n    send_api = kafka_client.send_offset_commit_request_kafka\n\n    status = []\n    if group_offset_reqs:\n        status = send_api(\n            group,\n            group_offset_reqs,\n            raise_on_error,\n            callback=_check_commit_response_error\n        )\n\n    return [_f for _f in status\n            if _f and _f.error != 0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying the offsets metadata so that the partition offsets have null payloads.", "response": "def nullify_offsets(offsets):\n    \"\"\"Modify offsets metadata so that the partition offsets\n    have null payloads.\n\n    :param offsets: dict {<topic>: {<partition>: <offset>}}\n    :returns: a dict topic: partition: offset\n    \"\"\"\n    result = {}\n    for topic, partition_offsets in six.iteritems(offsets):\n        result[topic] = _nullify_partition_offsets(partition_offsets)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display_table(headers, table):\n    assert all(len(row) == len(headers) for row in table)\n\n    str_headers = [str(header) for header in headers]\n    str_table = [[str(cell) for cell in row] for row in table]\n    column_lengths = [\n        max(len(header), *(len(row[i]) for row in str_table))\n        for i, header in enumerate(str_headers)\n    ]\n\n    print(\n        \" | \".join(\n            str(header).ljust(length)\n            for header, length in zip(str_headers, column_lengths)\n        )\n    )\n    print(\"-+-\".join(\"-\" * length for length in column_lengths))\n    for row in str_table:\n        print(\n            \" | \".join(\n                str(cell).ljust(length)\n                for cell, length in zip(row, column_lengths)\n            )\n        )", "response": "Print a formatted table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_replica_imbalance(cluster_topologies):\n    assert cluster_topologies\n\n    rg_ids = list(next(six.itervalues(cluster_topologies)).rgs.keys())\n    assert all(\n        set(rg_ids) == set(cluster_topology.rgs.keys())\n        for cluster_topology in six.itervalues(cluster_topologies)\n    )\n\n    rg_imbalances = [\n        stats.get_replication_group_imbalance_stats(\n            list(cluster_topology.rgs.values()),\n            list(cluster_topology.partitions.values()),\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n\n    _display_table_title_multicolumn(\n        'Extra Replica Count',\n        'Replication Group',\n        rg_ids,\n        list(cluster_topologies.keys()),\n        [\n            [erc[rg_id] for rg_id in rg_ids]\n            for _, erc in rg_imbalances\n        ],\n    )\n\n    for name, imbalance in zip(\n            six.iterkeys(cluster_topologies),\n            (imbalance for imbalance, _ in rg_imbalances)\n    ):\n        print(\n            '\\n'\n            '{name}'\n            'Total extra replica count: {imbalance}'\n            .format(\n                name='' if len(cluster_topologies) == 1 else name + '\\n',\n                imbalance=imbalance,\n            )\n        )", "response": "Display replica replication - group distribution imbalance statistics."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display_partition_imbalance(cluster_topologies):\n    broker_ids = list(next(six.itervalues(cluster_topologies)).brokers.keys())\n    assert all(\n        set(broker_ids) == set(cluster_topology.brokers.keys())\n        for cluster_topology in six.itervalues(cluster_topologies)\n    )\n    broker_partition_counts = [\n        stats.get_broker_partition_counts(\n            cluster_topology.brokers[broker_id]\n            for broker_id in broker_ids\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n    broker_weights = [\n        stats.get_broker_weights(\n            cluster_topology.brokers[broker_id]\n            for broker_id in broker_ids\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n\n    _display_table_title_multicolumn(\n        'Partition Count',\n        'Broker',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        broker_partition_counts,\n    )\n\n    print('')\n\n    _display_table_title_multicolumn(\n        'Partition Weight',\n        'Broker',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        broker_weights,\n    )\n\n    for name, bpc, bw in zip(\n            list(cluster_topologies.keys()),\n            broker_partition_counts,\n            broker_weights\n    ):\n        print(\n            '\\n'\n            '{name}'\n            'Partition count imbalance: {net_imbalance}\\n'\n            'Broker weight mean: {weight_mean}\\n'\n            'Broker weight stdev: {weight_stdev}\\n'\n            'Broker weight cv: {weight_cv}'\n            .format(\n                name='' if len(cluster_topologies) == 1 else name + '\\n',\n                net_imbalance=stats.get_net_imbalance(bpc),\n                weight_mean=stats.mean(bw),\n                weight_stdev=stats.stdevp(bw),\n                weight_cv=stats.coefficient_of_variation(bw),\n            )\n        )", "response": "Display partition count and weight imbalance statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display_leader_imbalance(cluster_topologies):\n    broker_ids = list(next(six.itervalues(cluster_topologies)).brokers.keys())\n    assert all(\n        set(broker_ids) == set(cluster_topology.brokers.keys())\n        for cluster_topology in six.itervalues(cluster_topologies)\n    )\n\n    broker_leader_counts = [\n        stats.get_broker_leader_counts(\n            cluster_topology.brokers[broker_id]\n            for broker_id in broker_ids\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n    broker_leader_weights = [\n        stats.get_broker_leader_weights(\n            cluster_topology.brokers[broker_id]\n            for broker_id in broker_ids\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n\n    _display_table_title_multicolumn(\n        'Leader Count',\n        'Brokers',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        broker_leader_counts,\n    )\n\n    print('')\n\n    _display_table_title_multicolumn(\n        'Leader weight',\n        'Brokers',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        broker_leader_weights,\n    )\n\n    for name, blc, blw in zip(\n            list(cluster_topologies.keys()),\n            broker_leader_counts,\n            broker_leader_weights\n    ):\n        print(\n            '\\n'\n            '{name}'\n            'Leader count imbalance: {net_imbalance}\\n'\n            'Broker leader weight mean: {weight_mean}\\n'\n            'Broker leader weight stdev: {weight_stdev}\\n'\n            'Broker leader weight cv: {weight_cv}'\n            .format(\n                name='' if len(cluster_topologies) == 1 else name + '\\n',\n                net_imbalance=stats.get_net_imbalance(blc),\n                weight_mean=stats.mean(blw),\n                weight_stdev=stats.stdevp(blw),\n                weight_cv=stats.coefficient_of_variation(blw),\n            )\n        )", "response": "Display the leader count and weight imbalance statistics."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_topic_broker_imbalance(cluster_topologies):\n    broker_ids = list(next(six.itervalues(cluster_topologies)).brokers.keys())\n    assert all(\n        set(broker_ids) == set(cluster_topology.brokers.keys())\n        for cluster_topology in six.itervalues(cluster_topologies)\n    )\n    topic_names = list(next(six.itervalues(cluster_topologies)).topics.keys())\n    assert all(\n        set(topic_names) == set(cluster_topology.topics.keys())\n        for cluster_topology in six.itervalues(cluster_topologies)\n    )\n\n    imbalances = [\n        stats.get_topic_imbalance_stats(\n            [cluster_topology.brokers[broker_id] for broker_id in broker_ids],\n            [cluster_topology.topics[tname] for tname in topic_names],\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n    weighted_imbalances = [\n        stats.get_weighted_topic_imbalance_stats(\n            [cluster_topology.brokers[broker_id] for broker_id in broker_ids],\n            [cluster_topology.topics[tname] for tname in topic_names],\n        )\n        for cluster_topology in six.itervalues(cluster_topologies)\n    ]\n\n    _display_table_title_multicolumn(\n        'Extra-Topic-Partition Count',\n        'Brokers',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        [\n            [i[1][broker_id] for broker_id in broker_ids]\n            for i in imbalances\n        ]\n    )\n\n    print('')\n\n    _display_table_title_multicolumn(\n        'Weighted Topic Imbalance',\n        'Brokers',\n        broker_ids,\n        list(cluster_topologies.keys()),\n        [\n            [wi[1][broker_id] for broker_id in broker_ids]\n            for wi in weighted_imbalances\n        ]\n    )\n\n    for name, topic_imbalance, weighted_topic_imbalance in zip(\n            six.iterkeys(cluster_topologies),\n            (i[0] for i in imbalances),\n            (wi[0] for wi in weighted_imbalances),\n    ):\n        print(\n            '\\n'\n            '{name}'\n            'Topic partition imbalance count: {topic_imbalance}\\n'\n            'Weighted topic partition imbalance: {weighted_topic_imbalance}'\n            .format(\n                name='' if len(cluster_topologies) == 1 else name + '\\n',\n                topic_imbalance=topic_imbalance,\n                weighted_topic_imbalance=weighted_topic_imbalance,\n            )\n        )", "response": "Display topic broker imbalance statistics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef display_movements_stats(ct, base_assignment):\n    movement_count, movement_size, leader_changes = \\\n        stats.get_partition_movement_stats(ct, base_assignment)\n    print(\n        'Total partition movements: {movement_count}\\n'\n        'Total partition movement size: {movement_size}\\n'\n        'Total leader changes: {leader_changes}'\n        .format(\n            movement_count=movement_count,\n            movement_size=movement_size,\n            leader_changes=leader_changes,\n        )\n    )", "response": "Display how the amount of movement between two assignments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display_assignment_changes(plan_details, to_log=True):\n    curr_plan_list, new_plan_list, total_changes = plan_details\n    action_cnt = '\\n[INFO] Total actions required {0}'.format(total_changes)\n    _log_or_display(to_log, action_cnt)\n    action_cnt = (\n        '[INFO] Total actions that will be executed {0}'\n        .format(len(new_plan_list))\n    )\n    _log_or_display(to_log, action_cnt)\n    changes = ('[INFO] Proposed Changes in current cluster-layout:\\n')\n    _log_or_display(to_log, changes)\n\n    tp_str = 'Topic - Partition'\n    curr_repl_str = 'Previous-Assignment'\n    new_rep_str = 'Proposed-Assignment'\n    tp_list = [tp_repl[0] for tp_repl in curr_plan_list]\n\n    # Display heading\n    msg = '=' * 80\n    _log_or_display(to_log, msg)\n    row = (\n        '{tp:^30s}: {curr_rep_str:^20s} ==> {new_rep_str:^20s}' .format(\n            tp=tp_str,\n            curr_rep_str=curr_repl_str,\n            new_rep_str=new_rep_str,\n        )\n    )\n    _log_or_display(to_log, row)\n    msg = '=' * 80\n    _log_or_display(to_log, msg)\n\n    # Display each topic-partition list with changes\n    tp_list_sorted = sorted(tp_list, key=lambda tp: (tp[0], tp[1]))\n    for tp in tp_list_sorted:\n        curr_repl = [\n            tp_repl[1] for tp_repl in curr_plan_list if tp_repl[0] == tp\n        ][0]\n        proposed_repl = [\n            tp_repl[1] for tp_repl in new_plan_list if tp_repl[0] == tp\n        ][0]\n        tp_str = '{topic} - {partition:<2d}'.format(topic=tp[0], partition=tp[1])\n        row = (\n            '{tp:<30s}: {curr_repl:<20s} ==> {proposed_repl:<20s}'.format(\n                tp=tp_str,\n                curr_repl=curr_repl,\n                proposed_repl=proposed_repl,\n            )\n        )\n        _log_or_display(to_log, row)", "response": "Display current and proposed changes in cluster - layout over brokers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef variance(data, data_mean=None):\n    data_mean = data_mean or mean(data)\n    return sum((x - data_mean) ** 2 for x in data) / len(data)", "response": "Calculates the variance of a sequence of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stdevp(data, data_mean=None, data_variance=None):\n    data_variance = data_variance or variance(data, data_mean)\n    return sqrt(data_variance)", "response": "Calculates the standard deviation of a sequence of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the coefficient of variation of a sequence of numbers.", "response": "def coefficient_of_variation(data, data_mean=None, data_stdev=None):\n    \"\"\"Return the coefficient of variation (CV) of a sequence of numbers.\n    :param data_mean: Precomputed mean of the sequence.\n    :param data_stdevp: Precomputed stdevp of the\n        sequence.\n    \"\"\"\n    data_mean = data_mean or mean(data)\n    data_stdev = data_stdev or stdevp(data, data_mean)\n    if data_mean == 0:\n        return float(\"inf\") if data_stdev != 0 else 0\n    else:\n        return data_stdev / data_mean"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate and return net imbalance based on given count of partitions or leaders per broker.", "response": "def get_net_imbalance(count_per_broker):\n    \"\"\"Calculate and return net imbalance based on given count of\n    partitions or leaders per broker.\n\n    Net-imbalance in case of partitions implies total number of\n    extra partitions from optimal count over all brokers.\n    This is also implies, the minimum number of partition movements\n    required for overall balancing.\n\n    For leaders, net imbalance implies total number of extra brokers\n    as leaders from optimal count.\n    \"\"\"\n    net_imbalance = 0\n    opt_count, extra_allowed = \\\n        compute_optimum(len(count_per_broker), sum(count_per_broker))\n    for count in count_per_broker:\n        extra_cnt, extra_allowed = \\\n            get_extra_element_count(count, opt_count, extra_allowed)\n        net_imbalance += extra_cnt\n    return net_imbalance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_extra_element_count(curr_count, opt_count, extra_allowed_cnt):\n    if curr_count > opt_count:\n        # We still can allow 1 extra count\n        if extra_allowed_cnt > 0:\n            extra_allowed_cnt -= 1\n            extra_cnt = curr_count - opt_count - 1\n        else:\n            extra_cnt = curr_count - opt_count\n    else:\n        extra_cnt = 0\n    return extra_cnt, extra_allowed_cnt", "response": "Evaluate and return extra same element count based on given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_replication_group_imbalance_stats(rgs, partitions):\n    tot_rgs = len(rgs)\n    extra_replica_cnt_per_rg = defaultdict(int)\n    for partition in partitions:\n        # Get optimal replica-count for each partition\n        opt_replica_cnt, extra_replicas_allowed = \\\n            compute_optimum(tot_rgs, partition.replication_factor)\n\n        # Extra replica count for each rg\n        for rg in rgs:\n            replica_cnt_rg = rg.count_replica(partition)\n            extra_replica_cnt, extra_replicas_allowed = \\\n                get_extra_element_count(\n                    replica_cnt_rg,\n                    opt_replica_cnt,\n                    extra_replicas_allowed,\n                )\n            extra_replica_cnt_per_rg[rg.id] += extra_replica_cnt\n\n    # Evaluate net imbalance across all replication-groups\n    net_imbalance = sum(extra_replica_cnt_per_rg.values())\n    return net_imbalance, extra_replica_cnt_per_rg", "response": "Calculate net imbalance across all replication - groups and extra - same - replica count."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns count of topics and partitions on each broker having multiple topic - partition count.", "response": "def get_topic_imbalance_stats(brokers, topics):\n    \"\"\"Return count of topics and partitions on each broker having multiple\n    partitions of same topic.\n\n    :rtype dict(broker_id: same-topic-partition count)\n    Example:\n    Total-brokers (b1, b2): 2\n    Total-partitions of topic t1: 5\n    (b1 has 4 partitions), (b2 has 1 partition)\n    opt-count: 5/2 = 2\n    extra-count: 5%2 = 1\n    i.e. 1 broker can have 2 + 1 = 3 partitions\n    and rest of brokers can have 2 partitions for given topic\n    Extra-partition or imbalance:\n    b1: current-partitions - optimal-count = 4 - 2 - 1(extra allowed) = 1\n    Net-imbalance = 1\n    \"\"\"\n    extra_partition_cnt_per_broker = defaultdict(int)\n    tot_brokers = len(brokers)\n    # Sort the brokers so that the iteration order is deterministic.\n    sorted_brokers = sorted(brokers, key=lambda b: b.id)\n    for topic in topics:\n        # Optimal partition-count per topic per broker\n        total_partition_replicas = \\\n            len(topic.partitions) * topic.replication_factor\n        opt_partition_cnt, extra_partitions_allowed = \\\n            compute_optimum(tot_brokers, total_partition_replicas)\n        # Get extra-partition count per broker for each topic\n        for broker in sorted_brokers:\n            partition_cnt_broker = broker.count_partitions(topic)\n            extra_partitions, extra_partitions_allowed = \\\n                get_extra_element_count(\n                    partition_cnt_broker,\n                    opt_partition_cnt,\n                    extra_partitions_allowed,\n                )\n            extra_partition_cnt_per_broker[broker.id] += extra_partitions\n\n    # Net extra partitions over all brokers\n    net_imbalance = sum(six.itervalues(extra_partition_cnt_per_broker))\n    return net_imbalance, extra_partition_cnt_per_broker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_partition_movement(prev_assignment, curr_assignment):\n    total_movements = 0\n    movements = {}\n    for prev_partition, prev_replicas in six.iteritems(prev_assignment):\n        curr_replicas = curr_assignment[prev_partition]\n        diff = len(set(curr_replicas) - set(prev_replicas))\n        if diff:\n            total_movements += diff\n            movements[prev_partition] = (\n                (set(prev_replicas) - set(curr_replicas)),\n                (set(curr_replicas) - set(prev_replicas)),\n            )\n    return movements, total_movements", "response": "Calculate the partition movements from initial to current assignment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_generated_broker_id(meta_properties_path):\n    try:\n        with open(meta_properties_path, 'r') as f:\n            broker_id = _parse_meta_properties_file(f)\n    except IOError:\n        raise IOError(\n            \"Cannot open meta.properties file: {path}\"\n            .format(path=meta_properties_path),\n        )\n    except ValueError:\n        raise ValueError(\"Broker id not valid\")\n\n    if broker_id is None:\n        raise ValueError(\"Autogenerated broker id missing from data directory\")\n\n    return broker_id", "response": "reads broker_id from meta. properties file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_watermark_for_regex(\n    kafka_client,\n    topic_regex,\n):\n    \"\"\"This method:\n        * refreshes metadata for the kafka client\n        * fetches watermarks\n\n    :param kafka_client: KafkaToolClient instance\n    :param topic: the topic regex\n    :returns: dict <topic>: [ConsumerPartitionOffsets]\n    \"\"\"\n    # Refresh client metadata. We do not use the topic list, because we\n    # don't want to accidentally create the topic if it does not exist.\n    # If Kafka is unavailable, let's retry loading client metadata\n    try:\n        kafka_client.load_metadata_for_topics()\n    except KafkaUnavailableError:\n        kafka_client.load_metadata_for_topics()\n\n    topics_to_be_considered = []\n\n    for topic in kafka_client.topic_partitions:\n        if re.search(topic_regex, topic):\n            topics_to_be_considered.append(topic)\n\n    watermarks = get_topics_watermarks(\n        kafka_client, topics_to_be_considered\n    )\n    return watermarks", "response": "This method gets the watermarks for a given topic regex"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_watermark_for_topic(\n    kafka_client,\n    topic,\n):\n    \"\"\"This method:\n        * refreshes metadata for the kafka client\n        * fetches watermarks\n\n    :param kafka_client: KafkaToolClient instance\n    :param topic: the topic\n    :returns: dict <topic>: [ConsumerPartitionOffsets]\n    \"\"\"\n    # Refresh client metadata. We do not use the topic list, because we\n    # don't want to accidentally create the topic if it does not exist.\n    # If Kafka is unavailable, let's retry loading client metadata\n    try:\n        kafka_client.load_metadata_for_topics()\n    except KafkaUnavailableError:\n        kafka_client.load_metadata_for_topics()\n\n    watermarks = get_topics_watermarks(\n        kafka_client, [topic]\n    )\n    return watermarks", "response": "This method gets the watermarks for a topic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging the offset metadata dictionaries from multiple responses.", "response": "def merge_offsets_metadata(topics, *offsets_responses):\n    \"\"\"Merge the offset metadata dictionaries from multiple responses.\n\n    :param topics: list of topics\n    :param offsets_responses: list of dict topic: partition: offset\n    :returns: dict topic: partition: offset\n    \"\"\"\n    result = dict()\n    for topic in topics:\n        partition_offsets = [\n            response[topic]\n            for response in offsets_responses\n            if topic in response\n        ]\n        result[topic] = merge_partition_offsets(*partition_offsets)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_partition_offsets(*partition_offsets):\n    output = dict()\n    for partition_offset in partition_offsets:\n        for partition, offset in six.iteritems(partition_offset):\n            prev_offset = output.get(partition, 0)\n            output[partition] = max(prev_offset, offset)\n    return output", "response": "Merge the partition offsets of a single topic from multiple responses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbalance replicas across replication - groups.", "response": "def rebalance_replicas(\n            self,\n            max_movement_count=None,\n            max_movement_size=None,\n    ):\n        \"\"\"Balance replicas across replication-groups.\n\n        :param max_movement_count: The maximum number of partitions to move.\n        :param max_movement_size: The maximum total size of the partitions to move.\n\n        :returns: A 2-tuple whose first element is the number of partitions moved\n            and whose second element is the total size of the partitions moved.\n        \"\"\"\n        movement_count = 0\n        movement_size = 0\n        for partition in six.itervalues(self.cluster_topology.partitions):\n            count, size = self._rebalance_partition_replicas(\n                partition,\n                None if not max_movement_count\n                else max_movement_count - movement_count,\n                None if not max_movement_size\n                else max_movement_size - movement_size,\n            )\n            movement_count += count\n            movement_size += size\n\n        return movement_count, movement_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _elect_source_replication_group(\n            self,\n            over_replicated_rgs,\n            partition,\n    ):\n        \"\"\"Decide source replication-group based as group with highest replica\n        count.\n        \"\"\"\n        return max(\n            over_replicated_rgs,\n            key=lambda rg: rg.count_replica(partition),\n        )", "response": "Select the highest replication - group that is over_replicated_rgs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects the destination replication - group based on replica - count.", "response": "def _elect_dest_replication_group(\n            self,\n            replica_count_source,\n            under_replicated_rgs,\n            partition,\n    ):\n        \"\"\"Decide destination replication-group based on replica-count.\"\"\"\n        min_replicated_rg = min(\n            under_replicated_rgs,\n            key=lambda rg: rg.count_replica(partition),\n        )\n        # Locate under-replicated replication-group with lesser\n        # replica count than source replication-group\n        if min_replicated_rg.count_replica(partition) < replica_count_source - 1:\n            return min_replicated_rg\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_consumer_offsets(cls, json_file):\n        with open(json_file, 'r') as consumer_offsets_json:\n            try:\n                parsed_offsets = {}\n                parsed_offsets_data = json.load(consumer_offsets_json)\n                # Create new dict with partition-keys as integers\n                parsed_offsets['groupid'] = parsed_offsets_data['groupid']\n                parsed_offsets['offsets'] = {}\n                for topic, topic_data in six.iteritems(parsed_offsets_data['offsets']):\n                    parsed_offsets['offsets'][topic] = {}\n                    for partition, offset in six.iteritems(topic_data):\n                        parsed_offsets['offsets'][topic][int(partition)] = offset\n                return parsed_offsets\n            except ValueError:\n                print(\n                    \"Error: Given consumer-data json data-file {file} could not be \"\n                    \"parsed\".format(file=json_file),\n                    file=sys.stderr,\n                )\n                raise", "response": "Parse consumer - data json - file and return a dict with the parsed offsets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild complete consumer offsets from parsed current consumer - offsets and lowmarks and highmarks from current - offsets for.", "response": "def build_new_offsets(cls, client, topics_offset_data, topic_partitions, current_offsets):\n        \"\"\"Build complete consumer offsets from parsed current consumer-offsets\n        and lowmarks and highmarks from current-offsets for.\n        \"\"\"\n        new_offsets = defaultdict(dict)\n        try:\n            for topic, partitions in six.iteritems(topic_partitions):\n                # Validate current offsets in range of low and highmarks\n                # Currently we only validate for positive offsets and warn\n                # if out of range of low and highmarks\n                valid_partitions = set()\n                for topic_partition_offsets in current_offsets[topic]:\n                    partition = topic_partition_offsets.partition\n                    valid_partitions.add(partition)\n                    # Skip the partition not present in list\n                    if partition not in topic_partitions[topic]:\n                        continue\n                    lowmark = topic_partition_offsets.lowmark\n                    highmark = topic_partition_offsets.highmark\n                    new_offset = topics_offset_data[topic][partition]\n                    if new_offset < 0:\n                        print(\n                            \"Error: Given offset: {offset} is negative\"\n                            .format(offset=new_offset),\n                            file=sys.stderr,\n                        )\n                        sys.exit(1)\n                    if new_offset < lowmark or new_offset > highmark:\n                        print(\n                            \"Warning: Given offset {offset} for topic-partition \"\n                            \"{topic}:{partition} is outside the range of lowmark \"\n                            \"{lowmark} and highmark {highmark}\".format(\n                                offset=new_offset,\n                                topic=topic,\n                                partition=partition,\n                                lowmark=lowmark,\n                                highmark=highmark,\n                            )\n                        )\n                    new_offsets[topic][partition] = new_offset\n                if not set(partitions).issubset(valid_partitions):\n                    print(\n                        \"Error: Some invalid partitions {partitions} for topic \"\n                        \"{topic} found. Valid partition-list {valid_partitions}. \"\n                        \"Exiting...\".format(\n                            partitions=', '.join([str(p) for p in partitions]),\n                            valid_partitions=', '.join([str(p) for p in valid_partitions]),\n                            topic=topic,\n                        ),\n                        file=sys.stderr,\n                    )\n                    sys.exit(1)\n        except KeyError as ex:\n            print(\n                \"Error: Possible invalid topic or partition. Error msg: {ex}. \"\n                \"Exiting...\".format(ex=ex),\n            )\n            sys.exit(1)\n        return new_offsets"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore_offsets(cls, client, parsed_consumer_offsets):\n        # Fetch current offsets\n        try:\n            consumer_group = parsed_consumer_offsets['groupid']\n            topics_offset_data = parsed_consumer_offsets['offsets']\n            topic_partitions = dict(\n                (topic, [partition for partition in offset_data.keys()])\n                for topic, offset_data in six.iteritems(topics_offset_data)\n            )\n        except IndexError:\n            print(\n                \"Error: Given parsed consumer-offset data {consumer_offsets} \"\n                \"could not be parsed\".format(consumer_offsets=parsed_consumer_offsets),\n                file=sys.stderr,\n            )\n            raise\n        current_offsets = get_consumer_offsets_metadata(\n            client,\n            consumer_group,\n            topic_partitions,\n        )\n        # Build new offsets\n        new_offsets = cls.build_new_offsets(\n            client,\n            topics_offset_data,\n            topic_partitions,\n            current_offsets,\n        )\n\n        # Commit offsets\n        consumer_group = parsed_consumer_offsets['groupid']\n        set_consumer_offsets(client, consumer_group, new_offsets)\n        print(\"Restored to new offsets {offsets}\".format(offsets=dict(new_offsets)))", "response": "Fetch current offsets from kafka validate them against given\n            topic - partitions and commit the new offsets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of a tuple with some elements replaced.", "response": "def tuple_replace(tup, *pairs):\n    \"\"\"Return a copy of a tuple with some elements replaced.\n\n    :param tup: The tuple to be copied.\n    :param pairs: Any number of (index, value) tuples where index is the index\n        of the item to replace and value is the new value of the item.\n    \"\"\"\n    tuple_list = list(tup)\n    for index, value in pairs:\n        tuple_list[index] = value\n    return tuple(tuple_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a copy of a tuple with some elements altered.", "response": "def tuple_alter(tup, *pairs):\n    \"\"\"Return a copy of a tuple with some elements altered.\n\n    :param tup: The tuple to be copied.\n    :param pairs: Any number of (index, func) tuples where index is the index\n        of the item to alter and the new value is func(tup[index]).\n    \"\"\"\n    # timeit says that this is faster than a similar\n    tuple_list = list(tup)\n    for i, f in pairs:\n        tuple_list[i] = f(tuple_list[i])\n    return tuple(tuple_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tuple_remove(tup, *items):\n    tuple_list = list(tup)\n    for item in items:\n        tuple_list.remove(item)\n    return tuple(tuple_list)", "response": "Returns a copy of a tuple with some items removed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef positive_int(string):\n    error_msg = 'Positive integer required, {string} given.'.format(string=string)\n    try:\n        value = int(string)\n    except ValueError:\n        raise ArgumentTypeError(error_msg)\n    if value < 0:\n        raise ArgumentTypeError(error_msg)\n    return value", "response": "Convert string to positive integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts string to positive integer greater than zero.", "response": "def positive_nonzero_int(string):\n    \"\"\"Convert string to positive integer greater than zero.\"\"\"\n    error_msg = 'Positive non-zero integer required, {string} given.'.format(string=string)\n    try:\n        value = int(string)\n    except ValueError:\n        raise ArgumentTypeError(error_msg)\n    if value <= 0:\n        raise ArgumentTypeError(error_msg)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef positive_float(string):\n    error_msg = 'Positive float required, {string} given.'.format(string=string)\n    try:\n        value = float(string)\n    except ValueError:\n        raise ArgumentTypeError(error_msg)\n    if value < 0:\n        raise ArgumentTypeError(error_msg)\n    return value", "response": "Convert string to positive float."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_h(num, suffix='B'):\n    if num is None:  # Show None when data is missing\n        return \"None\"\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)", "response": "Converts a byte value in human readable form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_to_json(data):\n    if sys.stdout.isatty():\n        return json.dumps(data, indent=4, separators=(',', ': '))\n    else:\n        return json.dumps(data)", "response": "Converts data into json\n    If stdout is a tty it performs a pretty print."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_brokers(self, brokers):\n        for broker_id, metadata in six.iteritems(brokers):\n            self.brokers[broker_id] = self._create_broker(broker_id, metadata)", "response": "Build broker objects using broker - ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_broker(self, broker_id, metadata=None):\n        broker = Broker(broker_id, metadata)\n        if not metadata:\n            broker.mark_inactive()\n        rg_id = self.extract_group(broker)\n        group = self.rgs.setdefault(rg_id, ReplicationGroup(rg_id))\n        group.add_broker(broker)\n        broker.replication_group = group\n        return broker", "response": "Create a broker object and assign to a replication group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_partitions(self, assignment):\n        self.partitions = {}\n        for partition_name, replica_ids in six.iteritems(assignment):\n            # Get topic\n            topic_id = partition_name[0]\n            partition_id = partition_name[1]\n            topic = self.topics.setdefault(\n                topic_id,\n                Topic(topic_id, replication_factor=len(replica_ids))\n            )\n\n            # Creating partition object\n            partition = Partition(\n                topic,\n                partition_id,\n                weight=self.partition_measurer.get_weight(partition_name),\n                size=self.partition_measurer.get_size(partition_name),\n            )\n            self.partitions[partition_name] = partition\n            topic.add_partition(partition)\n\n            # Updating corresponding broker objects\n            for broker_id in replica_ids:\n                # Check if broker-id is present in current active brokers\n                if broker_id not in list(self.brokers.keys()):\n                    self.log.warning(\n                        \"Broker %s containing partition %s is not in \"\n                        \"active brokers.\",\n                        broker_id,\n                        partition,\n                    )\n                    self.brokers[broker_id] = self._create_broker(broker_id)\n\n                self.brokers[broker_id].add_partition(partition)", "response": "Builds all partition objects and updates corresponding broker and topic objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of brokers that are not inactive or decommissioned.", "response": "def active_brokers(self):\n        \"\"\"Set of brokers that are not inactive or decommissioned.\"\"\"\n        return {\n            broker for broker in six.itervalues(self.brokers)\n            if not broker.inactive and not broker.decommissioned\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_broker(self, source_id, dest_id):\n        try:\n            source = self.brokers[source_id]\n            dest = self.brokers[dest_id]\n            # Move all partitions from source to destination broker\n            for partition in source.partitions.copy():  # Partitions set changes\n                # We cannot move partition directly since that re-orders the\n                # replicas for the partition\n                source.partitions.remove(partition)\n                dest.partitions.add(partition)\n                # Replace broker in replica\n                partition.replace(source, dest)\n        except KeyError as e:\n            self.log.error(\"Invalid broker id %s.\", e.args[0])\n            raise InvalidBrokerIdError(\n                \"Broker id {} does not exist in cluster\".format(e.args[0])\n            )", "response": "Replace broker in source broker with destination broker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmodify the cluster - topology with given assignment.", "response": "def update_cluster_topology(self, assignment):\n        \"\"\"Modify the cluster-topology with given assignment.\n\n        Change the replica set of partitions as in given assignment.\n\n        :param assignment: dict representing actions to be used to update the current\n        cluster-topology\n        :raises: InvalidBrokerIdError when broker-id is invalid\n        :raises: InvalidPartitionError when partition-name is invalid\n        \"\"\"\n        try:\n            for partition_name, replica_ids in six.iteritems(assignment):\n                try:\n                    new_replicas = [self.brokers[b_id] for b_id in replica_ids]\n                except KeyError:\n                    self.log.error(\n                        \"Invalid replicas %s for topic-partition %s-%s.\",\n                        ', '.join([str(id) for id in replica_ids]),\n                        partition_name[0],\n                        partition_name[1],\n                    )\n                    raise InvalidBrokerIdError(\n                        \"Invalid replicas {0}.\".format(\n                            ', '.join([str(id) for id in replica_ids])\n                        ),\n                    )\n                try:\n                    partition = self.partitions[partition_name]\n                    old_replicas = [broker for broker in partition.replicas]\n\n                    # No change needed. Save ourself some CPU time.\n                    # Replica order matters as the first one is the leader.\n                    if new_replicas == old_replicas:\n                        continue\n\n                    # Remove old partitions from broker\n                    # This also updates partition replicas\n                    for broker in old_replicas:\n                        broker.remove_partition(partition)\n\n                    # Add new partition to brokers\n                    for broker in new_replicas:\n                        broker.add_partition(partition)\n                except KeyError:\n                    self.log.error(\n                        \"Invalid topic-partition %s-%s.\",\n                        partition_name[0],\n                        partition_name[1],\n                    )\n                    raise InvalidPartitionError(\n                        \"Invalid topic-partition {0}-{1}.\"\n                        .format(partition_name[0], partition_name[1]),\n                    )\n        except KeyError:\n            self.log.error(\"Could not parse given assignment {0}\".format(assignment))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plan_to_assignment(plan):\n    assignment = {}\n    for elem in plan['partitions']:\n        assignment[\n            (elem['topic'], elem['partition'])\n        ] = elem['replicas']\n    return assignment", "response": "Convert the plan to the format used by cluster - topology."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assignment_to_plan(assignment):\n    return {\n        'version': 1,\n        'partitions':\n        [{'topic': t_p[0],\n          'partition': t_p[1],\n          'replicas': replica\n          } for t_p, replica in six.iteritems(assignment)]\n    }", "response": "Convert an assignment to the format used by Kafka to\n    describe a reassignment plan."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_plan(\n    new_plan,\n    base_plan=None,\n    is_partition_subset=True,\n    allow_rf_change=False,\n):\n    \"\"\"Verify that the new plan is valid for execution.\n\n    Given kafka-reassignment plan should affirm with following rules:\n    - Plan should have at least one partition for re-assignment\n    - Partition-name list should be subset of base-plan partition-list\n    - Replication-factor for each partition of same topic is same\n    - Replication-factor for each partition remains unchanged\n    - No duplicate broker-ids in each replicas\n    \"\"\"\n    if not _validate_plan(new_plan):\n        _log.error('Invalid proposed-plan.')\n        return False\n\n    # Validate given plan in reference to base-plan\n    if base_plan:\n        if not _validate_plan(base_plan):\n            _log.error('Invalid assignment from cluster.')\n            return False\n        if not _validate_plan_base(\n            new_plan,\n            base_plan,\n            is_partition_subset,\n            allow_rf_change\n        ):\n            return False\n    # Plan validation successful\n    return True", "response": "Validate that the new plan is valid for execution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that given plan is valid comparing with given base - plan.", "response": "def _validate_plan_base(\n    new_plan,\n    base_plan,\n    is_partition_subset=True,\n    allow_rf_change=False,\n):\n    \"\"\"Validate if given plan is valid comparing with given base-plan.\n\n    Validate following assertions:\n    - Partition-check: New partition-set should be subset of base-partition set\n    - Replica-count check: Replication-factor for each partition remains same\n    - Broker-check: New broker-set should be subset of base broker-set\n    \"\"\"\n\n    # Verify that partitions in plan are subset of base plan.\n    new_partitions = set([\n        (p_data['topic'], p_data['partition'])\n        for p_data in new_plan['partitions']\n    ])\n    base_partitions = set([\n        (p_data['topic'], p_data['partition'])\n        for p_data in base_plan['partitions']\n    ])\n    if is_partition_subset:\n        invalid_partitions = list(new_partitions - base_partitions)\n    else:\n        # partition set should be equal\n        invalid_partitions = list(\n            new_partitions.union(base_partitions) -\n            new_partitions.intersection(base_partitions),\n        )\n    if invalid_partitions:\n        _log.error(\n            'Invalid partition(s) found: {p_list}'.format(\n                p_list=invalid_partitions,\n            )\n        )\n        return False\n\n    # Verify replication-factor remains consistent\n    base_partition_replicas = {\n        (p_data['topic'], p_data['partition']): p_data['replicas']\n        for p_data in base_plan['partitions']\n    }\n    new_partition_replicas = {\n        (p_data['topic'], p_data['partition']): p_data['replicas']\n        for p_data in new_plan['partitions']\n    }\n    if not allow_rf_change:\n        invalid_replication_factor = False\n        for new_partition, replicas in six.iteritems(new_partition_replicas):\n            base_replica_cnt = len(base_partition_replicas[new_partition])\n            if len(replicas) != base_replica_cnt:\n                invalid_replication_factor = True\n                _log.error(\n                    'Replication-factor Mismatch: Partition: {partition}: '\n                    'Base-replicas: {expected}, Proposed-replicas: {actual}'\n                    .format(\n                        partition=new_partition,\n                        expected=base_partition_replicas[new_partition],\n                        actual=replicas,\n                    ),\n                )\n        if invalid_replication_factor:\n            return False\n\n    # Validation successful\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates the format of the current language of the current language of the current language.", "response": "def _validate_format(plan):\n    \"\"\"Validate if the format of the plan as expected.\n\n    Validate format of plan on following rules:\n    a) Verify if it ONLY and MUST have keys and value, 'version' and 'partitions'\n    b) Verify if each value of 'partitions' ONLY and MUST have keys 'replicas',\n        'partition', 'topic'\n    c) Verify desired type of each value\n    d) Verify non-empty partitions and replicas\n    Sample-plan format:\n    {\n        \"version\": 1,\n        \"partitions\": [\n            {\"partition\":0, \"topic\":'t1', \"replicas\":[0,1,2]},\n            {\"partition\":0, \"topic\":'t2', \"replicas\":[1,2]},\n            ...\n        ]}\n    \"\"\"\n    # Verify presence of required keys\n    if set(plan.keys()) != set(['version', 'partitions']):\n        _log.error(\n            'Invalid or incomplete keys in given plan. Expected: \"version\", '\n            '\"partitions\". Found:{keys}'\n            .format(keys=', '.join(list(plan.keys()))),\n        )\n        return False\n\n    # Invalid version\n    if plan['version'] != 1:\n        _log.error(\n            'Invalid version of plan {version}'\n            .format(version=plan['version']),\n        )\n        return False\n\n    # Empty partitions\n    if not plan['partitions']:\n        _log.error(\n            '\"partitions\" list found empty\"'\n            .format(version=plan['partitions']),\n        )\n        return False\n\n    # Invalid partitions type\n    if not isinstance(plan['partitions'], list):\n        _log.error('\"partitions\" of type list expected.')\n        return False\n\n    # Invalid partition-data\n    for p_data in plan['partitions']:\n        if set(p_data.keys()) != set(['topic', 'partition', 'replicas']):\n            _log.error(\n                'Invalid keys in partition-data {keys}'\n                .format(keys=', '.join(list(p_data.keys()))),\n            )\n            return False\n        # Check types\n        if not isinstance(p_data['topic'], six.text_type):\n            _log.error(\n                '\"topic\" of type unicode expected {p_data}, found {t_type}'\n                .format(p_data=p_data, t_type=type(p_data['topic'])),\n            )\n            return False\n        if not isinstance(p_data['partition'], int):\n            _log.error(\n                '\"partition\" of type int expected {p_data}, found {p_type}'\n                .format(p_data=p_data, p_type=type(p_data['partition'])),\n            )\n            return False\n        if not isinstance(p_data['replicas'], list):\n            _log.error(\n                '\"replicas\" of type list expected {p_data}, found {r_type}'\n                .format(p_data=p_data, r_type=type(p_data['replicas'])),\n            )\n            return False\n        if not p_data['replicas']:\n            _log.error(\n                'Non-empty \"replicas\" expected: {p_data}'\n                .format(p_data=p_data),\n            )\n            return False\n        # Invalid broker-type\n        for broker in p_data['replicas']:\n            if not isinstance(broker, int):\n                _log.error(\n                    '\"replicas\" of type integer list expected {p_data}'\n                    .format(p_data=p_data),\n                )\n                return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating if given plan is valid based on kafka - cluster - assignment protocols.", "response": "def _validate_plan(plan):\n    \"\"\"Validate if given plan is valid based on kafka-cluster-assignment protocols.\n\n    Validate following parameters:\n    - Correct format of plan\n    - Partition-list should be unique\n    - Every partition of a topic should have same replication-factor\n    - Replicas of a partition should have unique broker-set\n    \"\"\"\n    # Validate format of plan\n    if not _validate_format(plan):\n        return False\n\n    # Verify no duplicate partitions\n    partition_names = [\n        (p_data['topic'], p_data['partition'])\n        for p_data in plan['partitions']\n    ]\n    duplicate_partitions = [\n        partition for partition, count in six.iteritems(Counter(partition_names))\n        if count > 1\n    ]\n    if duplicate_partitions:\n        _log.error(\n            'Duplicate partitions in plan {p_list}'\n            .format(p_list=duplicate_partitions),\n        )\n        return False\n\n    # Verify no duplicate brokers in partition-replicas\n    dup_replica_brokers = []\n    for p_data in plan['partitions']:\n        dup_replica_brokers = [\n            broker\n            for broker, count in Counter(p_data['replicas']).items()\n            if count > 1\n        ]\n        if dup_replica_brokers:\n            _log.error(\n                'Duplicate brokers: ({topic}, {p_id}) in replicas {replicas}'\n                .format(\n                    topic=p_data['topic'],\n                    p_id=p_data['partition'],\n                    replicas=p_data['replicas'],\n                )\n            )\n            return False\n\n    # Verify same replication-factor for every topic\n    topic_replication_factor = {}\n    for partition_info in plan['partitions']:\n        topic = partition_info['topic']\n        replication_factor = len(partition_info['replicas'])\n        if topic in list(topic_replication_factor.keys()):\n            if topic_replication_factor[topic] != replication_factor:\n                _log.error(\n                    'Mismatch in replication-factor of partitions for topic '\n                    '{topic}'.format(topic=topic),\n                )\n                return False\n        else:\n            topic_replication_factor[topic] = replication_factor\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreceives a dict of ( topic_name ConsumerPartitionOffset ) and returns a dict where the topics are sorted by total offset distance.", "response": "def sort_by_distance(cls, consumer_offsets_metadata):\n        \"\"\"Receives a dict of (topic_name: ConsumerPartitionOffset) and returns a\n        similar dict where the topics are sorted by total offset distance.\"\"\"\n        sorted_offsets = sorted(\n            list(consumer_offsets_metadata.items()),\n            key=lambda topic_offsets: sum([o.highmark - o.current for o in topic_offsets[1]])\n        )\n        return OrderedDict(sorted_offsets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sort_by_distance_percentage(cls, consumer_offsets_metadata):\n        sorted_offsets = sorted(\n            list(consumer_offsets_metadata.items()),\n            key=lambda topic_offsets1: sum(\n                [cls.percentage_distance(o.highmark, o.current) for o in topic_offsets1[1]]\n            )\n        )\n        return OrderedDict(sorted_offsets)", "response": "Receives a dict of ( topic_name ConsumerPartitionOffset ) and returns an OrderedDict where the topics are sorted by average offset distance\n        in percentage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_children(self, path, watch=None):\n        _log.debug(\n            \"ZK: Getting children of {path}\".format(path=path),\n        )\n        return self.zk.get_children(path, watch)", "response": "Returns the children of the specified node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, path, watch=None):\n        _log.debug(\n            \"ZK: Getting {path}\".format(path=path),\n        )\n        return self.zk.get(path, watch)", "response": "Returns the data of the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, path, value):\n        _log.debug(\n            \"ZK: Setting {path} to {value}\".format(path=path, value=value)\n        )\n        return self.zk.set(path, value)", "response": "Sets and returns new data for the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_json(self, path, watch=None):\n        data, _ = self.get(path, watch)\n        return load_json(data) if data else None", "response": "Reads the data of the specified node and converts it to json."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget information on all the available brokers.", "response": "def get_brokers(self, names_only=False):\n        \"\"\"Get information on all the available brokers.\n\n        :rtype : dict of brokers\n        \"\"\"\n        try:\n            broker_ids = self.get_children(\"/brokers/ids\")\n        except NoNodeError:\n            _log.info(\n                \"cluster is empty.\"\n            )\n            return {}\n        # Return broker-ids only\n        if names_only:\n            return {int(b_id): None for b_id in broker_ids}\n        return {int(b_id): self.get_broker_metadata(b_id) for b_id in broker_ids}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_topic_config(self, topic):\n        try:\n            config_data = load_json(\n                self.get(\n                    \"/config/topics/{topic}\".format(topic=topic)\n                )[0]\n            )\n        except NoNodeError as e:\n\n            # Kafka version before 0.8.1 does not have \"/config/topics/<topic_name>\" path in ZK and\n            # if the topic exists, return default dict instead of raising an Exception.\n            # Ref: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper.\n\n            topics = self.get_topics(topic_name=topic, fetch_partition_state=False)\n            if len(topics) > 0:\n                _log.info(\"Configuration not available for topic {topic}.\".format(topic=topic))\n                config_data = {\"config\": {}}\n            else:\n                _log.error(\n                    \"topic {topic} not found.\".format(topic=topic)\n                )\n                raise e\n        return config_data", "response": "Get configuration information for specified topic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting configuration information for specified topic.", "response": "def set_topic_config(self, topic, value, kafka_version=(0, 10, )):\n        \"\"\"Set configuration information for specified topic.\n\n        :topic : topic whose configuration needs to be changed\n        :value :  config value with which the topic needs to be\n            updated with. This would be of the form key=value.\n            Example 'cleanup.policy=compact'\n        :kafka_version :tuple kafka version the brokers are running on.\n            Defaults to (0, 10, x). Kafka version 9 and kafka 10\n            support this feature.\n        \"\"\"\n        config_data = dump_json(value)\n\n        try:\n            # Change value\n            return_value = self.set(\n                \"/config/topics/{topic}\".format(topic=topic),\n                config_data\n            )\n            # Create change\n            version = kafka_version[1]\n\n            # this feature is supported in kafka 9 and kafka 10\n            assert version in (9, 10), \"Feature supported with kafka 9 and kafka 10\"\n\n            if version == 9:\n                # https://github.com/apache/kafka/blob/0.9.0.1/\n                #     core/src/main/scala/kafka/admin/AdminUtils.scala#L334\n                change_node = dump_json({\n                    \"version\": 1,\n                    \"entity_type\": \"topics\",\n                    \"entity_name\": topic\n                })\n            else:  # kafka 10\n                # https://github.com/apache/kafka/blob/0.10.2.1/\n                #     core/src/main/scala/kafka/admin/AdminUtils.scala#L574\n                change_node = dump_json({\n                    \"version\": 2,\n                    \"entity_path\": \"topics/\" + topic,\n                })\n\n            self.create(\n                '/config/changes/config_change_',\n                change_node,\n                sequence=True\n            )\n        except NoNodeError as e:\n            _log.error(\n                \"topic {topic} not found.\".format(topic=topic)\n            )\n            raise e\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_topics(\n        self,\n        topic_name=None,\n        names_only=False,\n        fetch_partition_state=True,\n    ):\n        \"\"\"Get information on all the available topics.\n\n        Topic-data format with fetch_partition_state as False :-\n        topic_data = {\n            'version': 1,\n            'partitions': {\n                <p_id>: {\n                    replicas: <broker-ids>\n                }\n            }\n        }\n\n        Topic-data format with fetch_partition_state as True:-\n        topic_data = {\n            'version': 1,\n            'ctime': <timestamp>,\n            'partitions': {\n                <p_id>:{\n                    replicas: [<broker_id>, <broker_id>, ...],\n                    isr: [<broker_id>, <broker_id>, ...],\n                    controller_epoch: <val>,\n                    leader_epoch: <val>,\n                    version: 1,\n                    leader: <broker-id>,\n                    ctime: <timestamp>,\n                }\n            }\n        }\n        Note: By default we also fetch partition-state which results in\n        accessing the zookeeper twice. If just partition-replica information is\n        required fetch_partition_state should be set to False.\n        \"\"\"\n        try:\n            topic_ids = [topic_name] if topic_name else self.get_children(\n                \"/brokers/topics\",\n            )\n        except NoNodeError:\n            _log.error(\n                \"Cluster is empty.\"\n            )\n            return {}\n\n        if names_only:\n            return topic_ids\n        topics_data = {}\n        for topic_id in topic_ids:\n            try:\n                topic_info = self.get(\"/brokers/topics/{id}\".format(id=topic_id))\n                topic_data = load_json(topic_info[0])\n                topic_ctime = topic_info[1].ctime / 1000.0\n                topic_data['ctime'] = topic_ctime\n            except NoNodeError:\n                _log.info(\n                    \"topic '{topic}' not found.\".format(topic=topic_id),\n                )\n                return {}\n            # Prepare data for each partition\n            partitions_data = {}\n            for p_id, replicas in six.iteritems(topic_data['partitions']):\n                partitions_data[p_id] = {}\n                if fetch_partition_state:\n                    # Fetch partition-state from zookeeper\n                    partition_state = self._fetch_partition_state(topic_id, p_id)\n                    partitions_data[p_id] = load_json(partition_state[0])\n                    partitions_data[p_id]['ctime'] = partition_state[1].ctime / 1000.0\n                else:\n                    # Fetch partition-info from zookeeper\n                    partition_info = self._fetch_partition_info(topic_id, p_id)\n                    partitions_data[p_id]['ctime'] = partition_info.ctime / 1000.0\n                partitions_data[p_id]['replicas'] = replicas\n            topic_data['partitions'] = partitions_data\n            topics_data[topic_id] = topic_data\n        return topics_data", "response": "Get information on all the available topics."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting information on all the available consumer - groups.", "response": "def get_consumer_groups(self, consumer_group_id=None, names_only=False):\n        \"\"\"Get information on all the available consumer-groups.\n\n        If names_only is False, only list of consumer-group ids are sent.\n        If names_only is True, Consumer group offset details are returned\n        for all consumer-groups or given consumer-group if given in dict\n        format as:-\n\n        {\n            'group-id':\n            {\n                'topic':\n                {\n                    'partition': offset-value,\n                    ...\n                    ...\n                }\n            }\n        }\n\n        :rtype: dict of consumer-group offset details\n        \"\"\"\n        if consumer_group_id is None:\n            group_ids = self.get_children(\"/consumers\")\n        else:\n            group_ids = [consumer_group_id]\n\n        # Return consumer-group-ids only\n        if names_only:\n            return {g_id: None for g_id in group_ids}\n\n        consumer_offsets = {}\n        for g_id in group_ids:\n            consumer_offsets[g_id] = self.get_group_offsets(g_id)\n        return consumer_offsets"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_group_offsets(self, group, topic=None):\n        group_offsets = {}\n        try:\n            all_topics = self.get_my_subscribed_topics(group)\n        except NoNodeError:\n            # No offset information of given consumer-group\n            _log.warning(\n                \"No topics subscribed to consumer-group {group}.\".format(\n                    group=group,\n                ),\n            )\n            return group_offsets\n        if topic:\n            if topic in all_topics:\n                topics = [topic]\n            else:\n                _log.error(\n                    \"Topic {topic} not found in topic list {topics} for consumer\"\n                    \"-group {consumer_group}.\".format(\n                        topic=topic,\n                        topics=', '.join(topic for topic in all_topics),\n                        consumer_group=group,\n                    ),\n                )\n                return group_offsets\n        else:\n            topics = all_topics\n        for topic in topics:\n            group_offsets[topic] = {}\n            try:\n                partitions = self.get_my_subscribed_partitions(group, topic)\n            except NoNodeError:\n                _log.warning(\n                    \"No partition offsets found for topic {topic}. \"\n                    \"Continuing to next one...\".format(topic=topic),\n                )\n                continue\n            # Fetch offsets for each partition\n            for partition in partitions:\n                path = \"/consumers/{group_id}/offsets/{topic}/{partition}\".format(\n                    group_id=group,\n                    topic=topic,\n                    partition=partition,\n                )\n                try:\n                    # Get current offset\n                    offset_json, _ = self.get(path)\n                    group_offsets[topic][partition] = load_json(offset_json)\n                except NoNodeError:\n                    _log.error(\"Path {path} not found\".format(path=path))\n                    raise\n        return group_offsets", "response": "Fetch group offsets for given topic and partition otherwise all topics\n            and partitions otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fetch_partition_state(self, topic_id, partition_id):\n        state_path = \"/brokers/topics/{topic_id}/partitions/{p_id}/state\"\n        try:\n            partition_state = self.get(\n                state_path.format(topic_id=topic_id, p_id=partition_id),\n            )\n            return partition_state\n        except NoNodeError:\n            return {}", "response": "Fetch partition - state for given topic - partition."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching partition info for given topic - partition.", "response": "def _fetch_partition_info(self, topic_id, partition_id):\n        \"\"\"Fetch partition info for given topic-partition.\"\"\"\n        info_path = \"/brokers/topics/{topic_id}/partitions/{p_id}\"\n        try:\n            _, partition_info = self.get(\n                info_path.format(topic_id=topic_id, p_id=partition_id),\n            )\n            return partition_info\n        except NoNodeError:\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_my_subscribed_topics(self, groupid):\n        path = \"/consumers/{group_id}/offsets\".format(group_id=groupid)\n        return self.get_children(path)", "response": "Get the list of topics that a consumer is subscribed to\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_my_subscribed_partitions(self, groupid, topic):\n        path = \"/consumers/{group_id}/offsets/{topic}\".format(\n            group_id=groupid,\n            topic=topic,\n        )\n        return self.get_children(path)", "response": "Get the list of partitions of a topic\n        that a consumer is subscribed to\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the cluster layout in form of assignment from zookeeper", "response": "def get_cluster_assignment(self):\n        \"\"\"Fetch the cluster layout in form of assignment from zookeeper\"\"\"\n        plan = self.get_cluster_plan()\n        assignment = {}\n        for elem in plan['partitions']:\n            assignment[\n                (elem['topic'], elem['partition'])\n            ] = elem['replicas']\n\n        return assignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(\n        self,\n        path,\n        value='',\n        acl=None,\n        ephemeral=False,\n        sequence=False,\n        makepath=False\n    ):\n        \"\"\"Creates a Zookeeper node.\n\n        :param: path: The zookeeper node path\n        :param: value: Zookeeper node value\n        :param: acl: ACL list\n        :param: ephemeral: Boolean indicating where this node is tied to\n          this session.\n        :param: sequence:  Boolean indicating whether path is suffixed\n          with a unique index.\n        :param: makepath: Whether the path should be created if it doesn't\n          exist.\n        \"\"\"\n        _log.debug(\"ZK: Creating node \" + path)\n        return self.zk.create(path, value, acl, ephemeral, sequence, makepath)", "response": "Creates a Zookeeper node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a Zookeeper node.", "response": "def delete(self, path, recursive=False):\n        \"\"\"Deletes a Zookeeper node.\n\n        :param: path: The zookeeper node path\n        :param: recursive: Recursively delete node and all its children.\n        \"\"\"\n        _log.debug(\"ZK: Deleting node \" + path)\n        return self.zk.delete(path, recursive=recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubmitting a reassignment plan for execution.", "response": "def execute_plan(self, plan, allow_rf_change=False):\n        \"\"\"Submit reassignment plan for execution.\"\"\"\n        reassignment_path = '{admin}/{reassignment_node}'\\\n            .format(admin=ADMIN_PATH, reassignment_node=REASSIGNMENT_NODE)\n        plan_json = dump_json(plan)\n        base_plan = self.get_cluster_plan()\n        if not validate_plan(plan, base_plan, allow_rf_change=allow_rf_change):\n            _log.error('Given plan is invalid. Aborting new reassignment plan ... {plan}'.format(plan=plan))\n            return False\n        # Send proposed-plan to zookeeper\n        try:\n            _log.info('Sending plan to Zookeeper...')\n            self.create(reassignment_path, plan_json, makepath=True)\n            _log.info(\n                'Re-assign partitions node in Zookeeper updated successfully '\n                'with {plan}'.format(plan=plan),\n            )\n            return True\n        except NodeExistsError:\n            _log.warning('Previous plan in progress. Exiting..')\n            _log.warning('Aborting new reassignment plan... {plan}'.format(plan=plan))\n            in_progress_plan = load_json(self.get(reassignment_path)[0])\n            in_progress_partitions = [\n                '{topic}-{p_id}'.format(\n                    topic=p_data['topic'],\n                    p_id=str(p_data['partition']),\n                )\n                for p_data in in_progress_plan['partitions']\n            ]\n            _log.warning(\n                '{count} partition(s) reassignment currently in progress:-'\n                .format(count=len(in_progress_partitions)),\n            )\n            _log.warning(\n                '{partitions}. In Progress reassignment plan...'.format(\n                    partitions=', '.join(in_progress_partitions),\n                ),\n            )\n            return False\n        except Exception as e:\n            _log.error(\n                'Could not re-assign partitions {plan}. Error: {e}'\n                .format(plan=plan, e=e),\n            )\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch cluster plan from zookeeper.", "response": "def get_cluster_plan(self):\n        \"\"\"Fetch cluster plan from zookeeper.\"\"\"\n\n        _log.info('Fetching current cluster-topology from Zookeeper...')\n        cluster_layout = self.get_topics(fetch_partition_state=False)\n        # Re-format cluster-layout\n        partitions = [\n            {\n                'topic': topic_id,\n                'partition': int(p_id),\n                'replicas': partitions_data['replicas']\n            }\n            for topic_id, topic_info in six.iteritems(cluster_layout)\n            for p_id, partitions_data in six.iteritems(topic_info['partitions'])\n        ]\n        return {\n            'version': 1,\n            'partitions': partitions\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_pending_plan(self):\n        reassignment_path = '{admin}/{reassignment_node}'\\\n            .format(admin=ADMIN_PATH, reassignment_node=REASSIGNMENT_NODE)\n        try:\n            result = self.get(reassignment_path)\n            return load_json(result[0])\n        except NoNodeError:\n            return {}", "response": "Read the currently running plan on reassign_partitions node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_output(partitions, verbose):\n    out = {}\n    partitions_count = len(partitions)\n    out['raw'] = {\n        'offline_count': partitions_count,\n    }\n\n    if partitions_count == 0:\n        out['message'] = 'No offline partitions.'\n    else:\n        out['message'] = \"{count} offline partitions.\".format(count=partitions_count)\n        if verbose:\n            lines = (\n                '{}:{}'.format(topic, partition)\n                for (topic, partition) in partitions\n            )\n            out['verbose'] = \"Partitions:\\n\" + \"\\n\".join(lines)\n        else:\n            cmdline = sys.argv[:]\n            cmdline.insert(1, '-v')\n            out['message'] += '\\nTo see all offline partitions run: ' + ' '.join(cmdline)\n\n    if verbose:\n        out['raw']['partitions'] = [\n            {'topic': topic, 'partition': partition}\n            for (topic, partition) in partitions\n        ]\n\n    return out", "response": "Prepares the output dict for the internal use."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the number of offline partitions and returns an error code and the output", "response": "def run_command(self):\n        \"\"\"Checks the number of offline partitions\"\"\"\n        offline = get_topic_partition_with_error(\n            self.cluster_config,\n            LEADER_NOT_AVAILABLE_ERROR,\n        )\n\n        errcode = status_code.OK if not offline else status_code.CRITICAL\n        out = _prepare_output(offline, self.args.verbose)\n        return errcode, out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the group_id of groups committed into Kafka.", "response": "def get_kafka_groups(cls, cluster_config):\n        '''Get the group_id of groups committed into Kafka.'''\n        kafka_group_reader = KafkaGroupReader(cluster_config)\n        return list(kafka_group_reader.read_groups().keys())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a connection to the desired host.", "response": "def ssh(host, forward_agent=False, sudoable=False, max_attempts=1, max_timeout=5, ssh_password=None):\n    \"\"\"Manages a SSH connection to the desired host.\n       Will leverage your ssh config at ~/.ssh/config if available\n\n    :param host: the server to connect to\n    :type host: str\n    :param forward_agent: forward the local agents\n    :type forward_agent: bool\n    :param sudoable: allow sudo commands\n    :type sudoable: bool\n    :param max_attempts: the maximum attempts to connect to the desired host\n    :type max_attempts: int\n    :param max_timeout: the maximum timeout in seconds to sleep between attempts\n    :type max_timeout: int\n    :param ssh_password: SSH password to use if needed\n    :type ssh_password: str\n    :returns a SSH connection to the desired host\n    :rtype: Connection\n\n    :raises MaxConnectionAttemptsError: Exceeded the maximum attempts\n    to establish the SSH connection.\n    \"\"\"\n    with closing(SSHClient()) as client:\n        client.set_missing_host_key_policy(AutoAddPolicy())\n\n        cfg = {\n            \"hostname\": host,\n            \"timeout\": max_timeout,\n        }\n        if ssh_password:\n            cfg['password'] = ssh_password\n\n        ssh_config = SSHConfig()\n        user_config_file = os.path.expanduser(\"~/.ssh/config\")\n        if os.path.exists(user_config_file):\n            with open(user_config_file) as f:\n                ssh_config.parse(f)\n                host_config = ssh_config.lookup(host)\n                if \"user\" in host_config:\n                    cfg[\"username\"] = host_config[\"user\"]\n\n                if \"proxycommand\" in host_config:\n                    cfg[\"sock\"] = ProxyCommand(host_config[\"proxycommand\"])\n\n                if \"identityfile\" in host_config:\n                    cfg['key_filename'] = host_config['identityfile']\n\n                if \"port\" in host_config:\n                    cfg[\"port\"] = int(host_config[\"port\"])\n\n        attempts = 0\n        while attempts < max_attempts:\n            try:\n                attempts += 1\n                client.connect(**cfg)\n                break\n            except socket.error as e:\n                if attempts < max_attempts:\n                    print(\"SSH to host {0} failed, retrying...\".format(host))\n                    time.sleep(max_timeout)\n                else:\n                    print(\"SSH Exception: {0}\".format(e))\n\n        else:\n            raise MaxConnectionAttemptsError(\n                \"Exceeded max attempts to connect to host {0} after {1} retries\".format(host, max_attempts)\n            )\n\n        yield Connection(client, forward_agent, sudoable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints the lines to stdout if lines are present.", "response": "def report_stdout(host, stdout):\n    \"\"\"Take a stdout and print it's lines to output if lines are present.\n\n    :param host: the host where the process is running\n    :type host: str\n    :param stdout: the std out of that process\n    :type stdout: paramiko.channel.Channel\n    \"\"\"\n    lines = stdout.readlines()\n    if lines:\n        print(\"STDOUT from {host}:\".format(host=host))\n        for line in lines:\n            print(line.rstrip(), file=sys.stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the lines to stderr if lines are present.", "response": "def report_stderr(host, stderr):\n    \"\"\"Take a stderr and print it's lines to output if lines are present.\n\n    :param host: the host where the process is running\n    :type host: str\n    :param stderr: the std error of that process\n    :type stderr: paramiko.channel.Channel\n    \"\"\"\n    lines = stderr.readlines()\n    if lines:\n        print(\"STDERR from {host}:\".format(host=host))\n        for line in lines:\n            print(line.rstrip(), file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sudo_command(self, command, bufsize=-1):\n        new_command = \"sudo {0}\".format(command)\n        return self.exec_command(new_command, bufsize)", "response": "Executes a command on the SSH server and returns the stdout and stderr of the command"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a command on the SSH server while preserving underling the SSH agent forwarding and sudo privileges.", "response": "def exec_command(self, command, bufsize=-1, check_status=True):\n        \"\"\"Execute a command on the SSH server while preserving underling\n        agent forwarding and sudo privileges.\n        https://github.com/paramiko/paramiko/blob/1.8/paramiko/client.py#L348\n\n        :param command: the command to execute\n        :type command: str\n        :param bufsize: interpreted the same way as by the built-in C{file()} function in python\n        :type bufsize: int\n        :param check_staus: if enabled, waits for the command to complete and return an exception\n        if the status is non-zero.\n        :type check_staus: bool\n        :returns the stdin, stdout, and stderr of the executing command\n        :rtype: tuple(L{ChannelFile}, L{ChannelFile}, L{ChannelFile})\n\n        :raises SSHException: if the server fails to execute the command\n        \"\"\"\n        channel = self.transport.open_session()\n\n        if self.forward_agent:\n            AgentRequestHandler(channel)\n        if self.sudoable:\n            channel.get_pty()\n\n        channel.exec_command(command)\n        if check_status and channel.recv_exit_status() != 0:\n            raise RuntimeError(\"Command execution error: {}\".format(command))\n\n        stdin = channel.makefile('wb', bufsize)\n        stdout = channel.makefile('rb', bufsize)\n        stderr = channel.makefile_stderr('rb', bufsize)\n        return (stdin, stdout, stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_offsets(\n        cls,\n        consumer_offsets_metadata,\n        topics_dict,\n        json_file,\n        groupid,\n    ):\n        \"\"\"Built offsets for given topic-partitions in required format from current\n        offsets metadata and write to given json-file.\n\n        :param consumer_offsets_metadata: Fetched consumer offsets from kafka.\n        :param topics_dict: Dictionary of topic-partitions.\n        :param json_file: Filename to store consumer-offsets.\n        :param groupid: Current consumer-group.\n        \"\"\"\n        # Build consumer-offset data in desired format\n        current_consumer_offsets = defaultdict(dict)\n        for topic, topic_offsets in six.iteritems(consumer_offsets_metadata):\n            for partition_offset in topic_offsets:\n                current_consumer_offsets[topic][partition_offset.partition] = \\\n                    partition_offset.current\n        consumer_offsets_data = {'groupid': groupid, 'offsets': current_consumer_offsets}\n\n        cls.write_offsets_to_file(json_file, consumer_offsets_data)", "response": "Save offsets for given topic - partitions in required format to given json - file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_offsets_to_file(cls, json_file_name, consumer_offsets_data):\n        # Save consumer-offsets to file\n        with open(json_file_name, \"w\") as json_file:\n            try:\n                json.dump(consumer_offsets_data, json_file)\n            except ValueError:\n                print(\"Error: Invalid json data {data}\".format(data=consumer_offsets_data))\n                raise\n            print(\"Consumer offset data saved in json-file {file}\".format(file=json_file_name))", "response": "Save built consumer - offsets data to given json file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decommission_brokers(self, broker_ids):\n        groups = set()\n        for b_id in broker_ids:\n            try:\n                broker = self.cluster_topology.brokers[b_id]\n            except KeyError:\n                self.log.error(\"Invalid broker id %s.\", b_id)\n                # Raise an error for now. As alternative we may ignore the\n                # invalid id and continue with the others.\n                raise InvalidBrokerIdError(\n                    \"Broker id {} does not exist in cluster\".format(b_id),\n                )\n            broker.mark_decommissioned()\n            groups.add(broker.replication_group)\n\n        for group in groups:\n            self._decommission_brokers_in_group(group)", "response": "Decommission a list of brokers trying to keep the replication group\n        the brokers belong to balanced."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecommissions the marked brokers in a group.", "response": "def _decommission_brokers_in_group(self, group):\n        \"\"\"Decommission the marked brokers of a group.\"\"\"\n        try:\n            group.rebalance_brokers()\n        except EmptyReplicationGroupError:\n            self.log.warning(\"No active brokers left in replication group %s\", group)\n        for broker in group.brokers:\n            if broker.decommissioned and not broker.empty():\n                # In this case we need to reassign the remaining partitions\n                # to other replication groups\n                self.log.info(\n                    \"Broker %s can't be decommissioned within the same \"\n                    \"replication group %s. Moving partitions to other \"\n                    \"replication groups.\",\n                    broker,\n                    broker.replication_group,\n                )\n                self._force_broker_decommission(broker)\n                # Broker should be empty now\n                if not broker.empty():\n                    # Decommission may be impossible if there are not enough\n                    # brokers to redistributed the replicas.\n                    self.log.error(\n                        \"Could not decommission broker %s. \"\n                        \"Partitions %s cannot be reassigned.\",\n                        broker,\n                        broker.partitions,\n                    )\n                    raise BrokerDecommissionError(\"Broker decommission failed.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rebalance_replication_groups(self):\n        # Balance replicas over replication-groups for each partition\n        if any(b.inactive for b in six.itervalues(self.cluster_topology.brokers)):\n            self.log.error(\n                \"Impossible to rebalance replication groups because of inactive \"\n                \"brokers.\"\n            )\n            raise RebalanceError(\n                \"Impossible to rebalance replication groups because of inactive \"\n                \"brokers\"\n            )\n\n        # Balance replica-count over replication-groups\n        self.rebalance_replicas()\n\n        # Balance partition-count over replication-groups\n        self._rebalance_groups_partition_cnt()", "response": "Rebalance partitions over replication - groups."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revoke_leadership(self, broker_ids):\n        for b_id in broker_ids:\n            try:\n                broker = self.cluster_topology.brokers[b_id]\n            except KeyError:\n                self.log.error(\"Invalid broker id %s.\", b_id)\n                raise InvalidBrokerIdError(\n                    \"Broker id {} does not exist in cluster\".format(b_id),\n                )\n            broker.mark_revoked_leadership()\n\n        assert(len(self.cluster_topology.brokers) - len(broker_ids) > 0), \"Not \" \\\n            \"all brokers can be revoked for leadership\"\n        opt_leader_cnt = len(self.cluster_topology.partitions) // (\n            len(self.cluster_topology.brokers) - len(broker_ids)\n        )\n        # Balanced brokers transfer leadership to their under-balanced followers\n        self.rebalancing_non_followers(opt_leader_cnt)\n\n        # If the broker-ids to be revoked from leadership are still leaders for any\n        # partitions, try to forcefully move their leadership to followers if possible\n        pending_brokers = [\n            b for b in six.itervalues(self.cluster_topology.brokers)\n            if b.revoked_leadership and b.count_preferred_replica() > 0\n        ]\n        for b in pending_brokers:\n            self._force_revoke_leadership(b)", "response": "Revoke leadership for given brokers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrevoke the leadership of given broker for any remaining partitions. Algorithm: 1. Find the partitions (owned_partitions) with given broker as leader. 2. For each partition find the eligible followers. Brokers which are not to be revoked from leadership are eligible followers. 3. Select the follower who is leader for minimum partitions. 4. Assign the selected follower as leader. 5. Notify for any pending owned_partitions whose leader cannot be changed. This could be due to replica size 1 or eligible followers are None.", "response": "def _force_revoke_leadership(self, broker):\n        \"\"\"Revoke the leadership of given broker for any remaining partitions.\n\n        Algorithm:\n        1. Find the partitions (owned_partitions) with given broker as leader.\n        2. For each partition find the eligible followers.\n           Brokers which are not to be revoked from leadership are eligible followers.\n        3. Select the follower who is leader for minimum partitions.\n        4. Assign the selected follower as leader.\n        5. Notify for any pending owned_partitions whose leader cannot be changed.\n        This could be due to replica size 1 or eligible followers are None.\n        \"\"\"\n        owned_partitions = list(filter(\n            lambda p: broker is p.leader,\n            broker.partitions,\n        ))\n        for partition in owned_partitions:\n            if len(partition.replicas) == 1:\n                self.log.error(\n                    \"Cannot be revoked leadership for broker {b} for partition {p}. Replica count: 1\"\n                    .format(p=partition, b=broker),\n                )\n                continue\n            eligible_followers = [\n                follower for follower in partition.followers\n                if not follower.revoked_leadership\n            ]\n            if eligible_followers:\n                # Pick follower with least leader-count\n                best_fit_follower = min(\n                    eligible_followers,\n                    key=lambda follower: follower.count_preferred_replica(),\n                )\n                partition.swap_leader(best_fit_follower)\n            else:\n                self.log.error(\n                    \"All replicas for partition {p} on broker {b} are to be revoked for leadership.\".format(\n                        p=partition,\n                        b=broker,\n                    )\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rebalance_leaders(self):\n        opt_leader_cnt = len(self.cluster_topology.partitions) // len(self.cluster_topology.brokers)\n        # Balanced brokers transfer leadership to their under-balanced followers\n        self.rebalancing_non_followers(opt_leader_cnt)", "response": "Re - order brokers such that every broker is assigned as\n        preferred leader evenly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rebalancing_non_followers(self, opt_cnt):\n        # Don't include leaders if they are marked for leadership removal\n        under_brokers = list(filter(\n            lambda b: b.count_preferred_replica() < opt_cnt and not b.revoked_leadership,\n            six.itervalues(self.cluster_topology.brokers),\n        ))\n        if under_brokers:\n            skip_brokers, skip_partitions = [], []\n            for broker in under_brokers:\n                skip_brokers.append(broker)\n                broker.request_leadership(opt_cnt, skip_brokers, skip_partitions)\n\n        over_brokers = list(filter(\n            lambda b: b.count_preferred_replica() > opt_cnt + 1,\n            six.itervalues(self.cluster_topology.brokers),\n        ))\n        # Any over-balanced brokers tries to donate their leadership to followers\n        if over_brokers:\n            skip_brokers, used_edges = [], []\n            for broker in over_brokers:\n                skip_brokers.append(broker)\n                broker.donate_leadership(opt_cnt, skip_brokers, used_edges)", "response": "Transfer leadership to any over - balanced followers on the pretext\n        that are not followers of other leaders."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rebalance_groups_partition_cnt(self):\n        # Segregate replication-groups based on partition-count\n        total_elements = sum(len(rg.partitions) for rg in six.itervalues(self.cluster_topology.rgs))\n        over_loaded_rgs, under_loaded_rgs = separate_groups(\n            list(self.cluster_topology.rgs.values()),\n            lambda rg: len(rg.partitions),\n            total_elements,\n        )\n        if over_loaded_rgs and under_loaded_rgs:\n            self.cluster_topology.log.info(\n                'Over-loaded replication-groups {over_loaded}, under-loaded '\n                'replication-groups {under_loaded} based on partition-count'\n                .format(\n                    over_loaded=[rg.id for rg in over_loaded_rgs],\n                    under_loaded=[rg.id for rg in under_loaded_rgs],\n                )\n            )\n        else:\n            self.cluster_topology.log.info('Replication-groups are balanced based on partition-count.')\n            return\n\n        # Get optimal partition-count per replication-group\n        opt_partition_cnt, _ = compute_optimum(\n            len(self.cluster_topology.rgs),\n            total_elements,\n        )\n        # Balance replication-groups\n        for over_loaded_rg in over_loaded_rgs:\n            for under_loaded_rg in under_loaded_rgs:\n                # Filter unique partition with replica-count > opt-replica-count\n                # in over-loaded-rgs and <= opt-replica-count in under-loaded-rgs\n                eligible_partitions = set(filter(\n                    lambda partition:\n                    over_loaded_rg.count_replica(partition) >\n                    len(partition.replicas) // len(self.cluster_topology.rgs) and\n                    under_loaded_rg.count_replica(partition) <=\n                    len(partition.replicas) // len(self.cluster_topology.rgs),\n                    over_loaded_rg.partitions,\n                ))\n                # Move all possible partitions\n                for eligible_partition in eligible_partitions:\n                    # The difference of partition-count b/w the over-loaded and under-loaded\n                    # replication-groups should be greater than 1 for convergence\n                    if len(over_loaded_rg.partitions) - len(under_loaded_rg.partitions) > 1:\n                        over_loaded_rg.move_partition_replica(\n                            under_loaded_rg,\n                            eligible_partition,\n                        )\n                    else:\n                        break\n                    # Move to next replication-group if either of the groups got\n                    # balanced, otherwise try with next eligible partition\n                    if (len(under_loaded_rg.partitions) == opt_partition_cnt or\n                            len(over_loaded_rg.partitions) == opt_partition_cnt):\n                        break\n                if len(over_loaded_rg.partitions) == opt_partition_cnt:\n                    # Move to next over-loaded replication-group if balanced\n                    break", "response": "Re - balance partition - count across replication - groups."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nincreasing the replication factor for a partition.", "response": "def add_replica(self, partition_name, count=1):\n        \"\"\"Increase the replication-factor for a partition.\n\n        The replication-group to add to is determined as follows:\n            1. Find all replication-groups that have brokers not already\n                replicating the partition.\n            2. Of these, find replication-groups that have fewer than the\n                average number of replicas for this partition.\n            3. Choose the replication-group with the fewest overall partitions.\n\n        :param partition_name: (topic_id, partition_id) of the partition to add\n            replicas of.\n        :param count: The number of replicas to add.\n        :raises InvalidReplicationFactorError when the resulting replication\n        factor is greater than the number of brokers in the cluster.\n        \"\"\"\n        try:\n            partition = self.cluster_topology.partitions[partition_name]\n        except KeyError:\n            raise InvalidPartitionError(\n                \"Partition name {name} not found\".format(name=partition_name),\n            )\n        if partition.replication_factor + count > len(self.cluster_topology.brokers):\n            raise InvalidReplicationFactorError(\n                \"Cannot increase replication factor to {0}. There are only \"\n                \"{1} brokers.\"\n                .format(\n                    partition.replication_factor + count,\n                    len(self.cluster_topology.brokers),\n                )\n            )\n\n        non_full_rgs = [\n            rg\n            for rg in self.cluster_topology.rgs.values()\n            if rg.count_replica(partition) < len(rg.brokers)\n        ]\n        for _ in range(count):\n            total_replicas = sum(\n                rg.count_replica(partition)\n                for rg in non_full_rgs\n            )\n            opt_replicas, _ = compute_optimum(\n                len(non_full_rgs),\n                total_replicas,\n            )\n            under_replicated_rgs = [\n                rg\n                for rg in non_full_rgs\n                if rg.count_replica(partition) < opt_replicas\n            ]\n            candidate_rgs = under_replicated_rgs or non_full_rgs\n            rg = min(candidate_rgs, key=lambda rg: len(rg.partitions))\n\n            rg.add_replica(partition)\n\n            if rg.count_replica(partition) >= len(rg.brokers):\n                non_full_rgs.remove(rg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving one replica from the cluster.", "response": "def remove_replica(self, partition_name, osr_broker_ids, count=1):\n        \"\"\"Remove one replica of a partition from the cluster.\n\n        The replication-group to remove from is determined as follows:\n            1. Find all replication-groups that contain at least one\n                out-of-sync replica for this partition.\n            2. Of these, find replication-groups with more than the average\n                number of replicas of this partition.\n            3. Choose the replication-group with the most overall partitions.\n            4. Repeat steps 1-3 with in-sync replicas\n\n        After this operation, the preferred leader for this partition will\n        be set to the broker that leads the fewest other partitions, even if\n        the current preferred leader is not removed.\n        This is done to keep the number of preferred replicas balanced across\n        brokers in the cluster.\n\n        :param partition_name: (topic_id, partition_id) of the partition to\n            remove replicas of.\n        :param osr_broker_ids: A list of the partition's out-of-sync broker ids.\n        :param count: The number of replicas to remove.\n        :raises: InvalidReplicationFactorError when count is greater than the\n        replication factor of the partition.\n        \"\"\"\n        try:\n            partition = self.cluster_topology.partitions[partition_name]\n        except KeyError:\n            raise InvalidPartitionError(\n                \"Partition name {name} not found\".format(name=partition_name),\n            )\n        if partition.replication_factor <= count:\n            raise InvalidReplicationFactorError(\n                \"Cannot remove {0} replicas. Replication factor is only {1}.\"\n                .format(count, partition.replication_factor)\n            )\n\n        osr = []\n        for broker_id in osr_broker_ids:\n            try:\n                osr.append(self.cluster_topology.brokers[broker_id])\n            except KeyError:\n                raise InvalidBrokerIdError(\n                    \"No broker found with id {bid}\".format(bid=broker_id),\n                )\n\n        non_empty_rgs = [\n            rg\n            for rg in self.cluster_topology.rgs.values()\n            if rg.count_replica(partition) > 0\n        ]\n        rgs_with_osr = [\n            rg\n            for rg in non_empty_rgs\n            if any(b in osr for b in rg.brokers)\n        ]\n\n        for _ in range(count):\n            candidate_rgs = rgs_with_osr or non_empty_rgs\n            total_replicas = sum(\n                rg.count_replica(partition)\n                for rg in candidate_rgs\n            )\n            opt_replica_cnt, _ = compute_optimum(\n                len(candidate_rgs),\n                total_replicas,\n            )\n            over_replicated_rgs = [\n                rg\n                for rg in candidate_rgs\n                if rg.count_replica(partition) > opt_replica_cnt\n            ]\n            candidate_rgs = over_replicated_rgs or candidate_rgs\n            rg = max(candidate_rgs, key=lambda rg: len(rg.partitions))\n\n            osr_in_rg = [b for b in rg.brokers if b in osr]\n            rg.remove_replica(partition, osr_in_rg)\n\n            osr = [b for b in osr if b in partition.replicas]\n            if rg in rgs_with_osr and len(osr_in_rg) == 1:\n                rgs_with_osr.remove(rg)\n            if rg.count_replica(partition) == 0:\n                non_empty_rgs.remove(rg)\n\n        new_leader = min(\n            partition.replicas,\n            key=lambda broker: broker.count_preferred_replica(),\n        )\n        partition.swap_leader(new_leader)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess_topics(source_groupid, source_topics, dest_groupid, topics_dest_group):\n    # Is the new consumer already subscribed to any of these topics?\n    common_topics = [topic for topic in topics_dest_group if topic in source_topics]\n    if common_topics:\n        print(\n            \"Error: Consumer Group ID: {groupid} is already \"\n            \"subscribed to following topics: {topic}.\\nPlease delete this \"\n            \"topics from new group before re-running the \"\n            \"command.\".format(\n                groupid=dest_groupid,\n                topic=', '.join(common_topics),\n            ),\n            file=sys.stderr,\n        )\n        sys.exit(1)\n    # Let's confirm what the user intends to do.\n    if topics_dest_group:\n        in_str = (\n            \"New Consumer Group: {dest_groupid} already \"\n            \"exists.\\nTopics subscribed to by the consumer groups are listed \"\n            \"below:\\n{source_groupid}: {source_group_topics}\\n\"\n            \"{dest_groupid}: {dest_group_topics}\\nDo you intend to copy into\"\n            \"existing consumer destination-group? (y/n)\".format(\n                source_groupid=source_groupid,\n                source_group_topics=source_topics,\n                dest_groupid=dest_groupid,\n                dest_group_topics=topics_dest_group,\n            )\n        )\n        prompt_user_input(in_str)", "response": "Pre - process the topics in source and destination group for duplicates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate path with offset value for each topic - partition of given consumer group.", "response": "def create_offsets(zk, consumer_group, offsets):\n    \"\"\"Create path with offset value for each topic-partition of given consumer\n    group.\n\n    :param zk: Zookeeper client\n    :param consumer_group: Consumer group id for given offsets\n    :type consumer_group: int\n    :param offsets: Offsets of all topic-partitions\n    :type offsets: dict(topic, dict(partition, offset))\n    \"\"\"\n    # Create new offsets\n    for topic, partition_offsets in six.iteritems(offsets):\n        for partition, offset in six.iteritems(partition_offsets):\n            new_path = \"/consumers/{groupid}/offsets/{topic}/{partition}\".format(\n                groupid=consumer_group,\n                topic=topic,\n                partition=partition,\n            )\n            try:\n                zk.create(new_path, value=offset, makepath=True)\n            except NodeExistsError:\n                print(\n                    \"Error: Path {path} already exists. Please re-run the \"\n                    \"command.\".format(path=new_path),\n                    file=sys.stderr,\n                )\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_offsets(zk, consumer_group, topics):\n    source_offsets = defaultdict(dict)\n    for topic, partitions in six.iteritems(topics):\n        for partition in partitions:\n            offset, _ = zk.get(\n                \"/consumers/{groupid}/offsets/{topic}/{partition}\".format(\n                    groupid=consumer_group,\n                    topic=topic,\n                    partition=partition,\n                )\n            )\n            source_offsets[topic][partition] = offset\n    return source_offsets", "response": "Fetch offsets for given topics of given consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_offset_topic_partition_count(kafka_config):\n    metadata = get_topic_partition_metadata(kafka_config.broker_list)\n    if CONSUMER_OFFSET_TOPIC not in metadata:\n        raise UnknownTopic(\"Consumer offset topic is missing.\")\n    return len(metadata[CONSUMER_OFFSET_TOPIC])", "response": "Given a kafka cluster configuration return the number of partitions\n    in the offset topic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_group_partition(group, partition_count):\n    def java_string_hashcode(s):\n        h = 0\n        for c in s:\n            h = (31 * h + ord(c)) & 0xFFFFFFFF\n        return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000\n    return abs(java_string_hashcode(group)) % partition_count", "response": "Given a group name return the partition number of the consumer offset\n    topic containing the data associated to that group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive an initialized KafkaConsumer timestamp and list of topics returns the offsets for the given topic - partition and timestamp.", "response": "def topic_offsets_for_timestamp(consumer, timestamp, topics):\n    \"\"\"Given an initialized KafkaConsumer, timestamp, and list of topics,\n    looks up the offsets for the given topics by timestamp. The returned\n    offset for each partition is the earliest offset whose timestamp is greater than or\n    equal to the given timestamp in the corresponding partition.\n\n    Arguments:\n        consumer (KafkaConsumer): an initialized kafka-python consumer\n        timestamp (int): Unix epoch milliseconds. Unit should be milliseconds\n            since beginning of the epoch (midnight Jan 1, 1970 (UTC))\n        topics (list): List of topics whose offsets are to be fetched.\n    :returns:\n        ``{TopicPartition: OffsetAndTimestamp}``: mapping from partition\n        to the timestamp and offset of the first message with timestamp\n        greater than or equal to the target timestamp.\n        Returns ``{TopicPartition: None}`` for specific topic-partiitons if:\n          1. Timestamps are not supported in messages\n          2. No offsets in the partition after the given timestamp\n          3. No data in the topic-partition\n    :raises:\n        ValueError: If the target timestamp is negative\n        UnsupportedVersionError: If the broker does not support looking\n            up the offsets by timestamp.\n        KafkaTimeoutError: If fetch failed in request_timeout_ms\n    \"\"\"\n    tp_timestamps = {}\n    for topic in topics:\n        topic_partitions = consumer_partitions_for_topic(consumer, topic)\n        for tp in topic_partitions:\n            tp_timestamps[tp] = timestamp\n    return consumer.offsets_for_times(tp_timestamps)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all TopicPartitions for a given topic.", "response": "def consumer_partitions_for_topic(consumer, topic):\n    \"\"\"Returns a list of all TopicPartitions for a given topic.\n\n    Arguments:\n        consumer: an initialized KafkaConsumer\n        topic: a topic name to fetch TopicPartitions for\n\n    :returns:\n        list(TopicPartition): A list of TopicPartitions that belong to the given topic\n    \"\"\"\n    topic_partitions = []\n    partitions = consumer.partitions_for_topic(topic)\n    if partitions is not None:\n        for partition in partitions:\n            topic_partitions.append(TopicPartition(topic, partition))\n    else:\n        logging.error(\n            \"No partitions found for topic {}. Maybe it doesn't exist?\".format(topic),\n        )\n    return topic_partitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef consumer_commit_for_times(consumer, partition_to_offset, atomic=False):\n    no_offsets = set()\n    for tp, offset in six.iteritems(partition_to_offset):\n        if offset is None:\n            logging.error(\n                \"No offsets found for topic-partition {tp}. Either timestamps not supported\"\n                \" for the topic {tp}, or no offsets found after timestamp specified, or there is no\"\n                \" data in the topic-partition.\".format(tp=tp),\n            )\n            no_offsets.add(tp)\n    if atomic and len(no_offsets) > 0:\n        logging.error(\n            \"Commit aborted; offsets were not found for timestamps in\"\n            \" topics {}\".format(\",\".join([str(tp) for tp in no_offsets])),\n        )\n        return\n\n    offsets_metadata = {\n        tp: OffsetAndMetadata(partition_to_offset[tp].offset, metadata=None)\n        for tp in six.iterkeys(partition_to_offset) if tp not in no_offsets\n    }\n\n    if len(offsets_metadata) != 0:\n        consumer.commit(offsets_metadata)", "response": "Commits offsets to Kafka using the given KafkaConsumer and offsets a mapping of TopicPartition to Unix Epoch milliseconds timestamps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cluster_config(\n    cluster_type,\n    cluster_name=None,\n    kafka_topology_base_path=None,\n):\n    \"\"\"Return the cluster configuration.\n    Use the local cluster if cluster_name is not specified.\n\n    :param cluster_type: the type of the cluster\n    :type cluster_type: string\n    :param cluster_name: the name of the cluster\n    :type cluster_name: string\n    :param kafka_topology_base_path: base path to look for <cluster_type>.yaml\n    :type cluster_name: string\n    :returns: the cluster\n    :rtype: ClusterConfig\n    \"\"\"\n    if not kafka_topology_base_path:\n        config_dirs = get_conf_dirs()\n    else:\n        config_dirs = [kafka_topology_base_path]\n\n    topology = None\n    for config_dir in config_dirs:\n        try:\n            topology = TopologyConfiguration(\n                cluster_type,\n                config_dir,\n            )\n        except MissingConfigurationError:\n            pass\n    if not topology:\n        raise MissingConfigurationError(\n            \"No available configuration for type {0}\".format(cluster_type),\n        )\n\n    if cluster_name:\n        return topology.get_cluster_by_name(cluster_name)\n    else:\n        return topology.get_local_cluster()", "response": "Return the cluster configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_configurations(kafka_topology_base_path=None):\n    if not kafka_topology_base_path:\n        config_dirs = get_conf_dirs()\n    else:\n        config_dirs = [kafka_topology_base_path]\n\n    types = set()\n    for config_dir in config_dirs:\n        new_types = [x for x in map(\n            lambda x: os.path.basename(x)[:-5],\n            glob.glob('{0}/*.yaml'.format(config_dir)),\n        ) if x not in types]\n        for cluster_type in new_types:\n            try:\n                topology = TopologyConfiguration(\n                    cluster_type,\n                    config_dir,\n                )\n            except ConfigurationError:\n                continue\n            types.add(cluster_type)\n            yield topology", "response": "Iterate over all the topologies available in config."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_topology_config(self):\n        config_path = os.path.join(\n            self.kafka_topology_path,\n            '{id}.yaml'.format(id=self.cluster_type),\n        )\n        self.log.debug(\"Loading configuration from %s\", config_path)\n        if os.path.isfile(config_path):\n            topology_config = load_yaml_config(config_path)\n        else:\n            raise MissingConfigurationError(\n                \"Topology configuration {0} for cluster {1} \"\n                \"does not exist\".format(\n                    config_path,\n                    self.cluster_type,\n                )\n            )\n        self.log.debug(\"Topology configuration %s\", topology_config)\n        try:\n            self.clusters = topology_config['clusters']\n        except KeyError:\n            self.log.exception(\"Invalid topology file\")\n            raise InvalidConfigurationError(\"Invalid topology file {0}\".format(\n                config_path))\n        if 'local_config' in topology_config:\n            self.local_config = topology_config['local_config']", "response": "Load the topology configuration from a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_to_broker_id(string):\n    error_msg = 'Positive integer or -1 required, {string} given.'.format(string=string)\n    try:\n        value = int(string)\n    except ValueError:\n        raise argparse.ArgumentTypeError(error_msg)\n    if value <= 0 and value != -1:\n        raise argparse.ArgumentTypeError(error_msg)\n    return value", "response": "Convert string to kafka broker_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the command line arguments.", "response": "def parse_args():\n    \"\"\"Parse the command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Check kafka current status',\n    )\n    parser.add_argument(\n        \"--cluster-type\",\n        \"-t\",\n        dest='cluster_type',\n        required=True,\n        help='Type of cluster',\n        default=None,\n    )\n    parser.add_argument(\n        \"--cluster-name\",\n        \"-c\",\n        dest='cluster_name',\n        help='Name of the cluster',\n    )\n    parser.add_argument(\n        '--discovery-base-path',\n        dest='discovery_base_path',\n        type=str,\n        help='Path of the directory containing the <cluster_type>.yaml config',\n    )\n    parser.add_argument(\n        \"--broker-id\",\n        help='The broker id where the check is running. Set to -1 if you use automatic '\n        'broker ids, and it will read the id from data-path instead. This parameter is '\n        'required only in case controller-only or first-broker-only are used.',\n        type=convert_to_broker_id,\n    )\n    parser.add_argument(\n        \"--data-path\",\n        help='Path to the Kafka data folder.',\n    )\n    parser.add_argument(\n        '--controller-only',\n        action=\"store_true\",\n        help='If this parameter is specified, it will do nothing and succeed on '\n        'non-controller brokers. Default: %(default)s',\n    )\n    parser.add_argument(\n        '--first-broker-only',\n        action='store_true',\n        help='If specified, the command will only perform the check if '\n        'broker_id is the lowest broker id in the cluster. If it is not the lowest, '\n        'it will not perform any check and succeed immediately. '\n        'Default: %(default)s',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        help='print verbose execution information. Default: %(default)s',\n        action=\"store_true\",\n        default=False,\n    )\n    parser.add_argument(\n        '-j',\n        '--json',\n        help='Print output in json format. Default: %(default)s',\n        action=\"store_true\",\n        default=False,\n    )\n\n    subparsers = parser.add_subparsers()\n    MinIsrCmd().add_subparser(subparsers)\n    ReplicaUnavailabilityCmd().add_subparser(subparsers)\n    ReplicationFactorCmd().add_subparser(subparsers)\n    OfflineCmd().add_subparser(subparsers)\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run():\n    args = parse_args()\n    logging.basicConfig(level=logging.WARN)\n\n    # to prevent flooding for sensu-check.\n    logging.getLogger('kafka').setLevel(logging.CRITICAL)\n\n    if args.controller_only and args.first_broker_only:\n        terminate(\n            status_code.WARNING,\n            prepare_terminate_message(\n                \"Only one of controller_only and first_broker_only should be used\",\n            ),\n            args.json,\n        )\n\n    if args.controller_only or args.first_broker_only:\n        if args.broker_id is None:\n            terminate(\n                status_code.WARNING,\n                prepare_terminate_message(\"broker_id is not specified\"),\n                args.json,\n            )\n        elif args.broker_id == -1:\n            try:\n                args.broker_id = get_broker_id(args.data_path)\n            except Exception as e:\n                terminate(\n                    status_code.WARNING,\n                    prepare_terminate_message(\"{}\".format(e)),\n                    args.json,\n                )\n\n    try:\n        cluster_config = config.get_cluster_config(\n            args.cluster_type,\n            args.cluster_name,\n            args.discovery_base_path,\n        )\n        code, msg = args.command(cluster_config, args)\n    except ConfigurationError as e:\n        terminate(\n            status_code.CRITICAL,\n            prepare_terminate_message(\"ConfigurationError {0}\".format(e)),\n            args.json,\n        )\n\n    terminate(code, msg, args.json)", "response": "Verify command - line arguments and run commands"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Manage and describe partition layout over brokers of'\n        ' a cluster.',\n    )\n    parser.add_argument(\n        '--cluster-type',\n        '-t',\n        dest='cluster_type',\n        help='Type of the cluster.',\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        '--cluster-name',\n        '-c',\n        dest='cluster_name',\n        help='Name of the cluster (Default to local cluster).',\n    )\n    parser.add_argument(\n        '--discovery-base-path',\n        dest='discovery_base_path',\n        type=str,\n        help='Path of the directory containing the <cluster_type>.yaml config',\n    )\n    parser.add_argument(\n        '--logconf',\n        type=str,\n        help='Path to logging configuration file. Default: log to console.',\n    )\n    parser.add_argument(\n        '--apply',\n        action='store_true',\n        help='Proposed-plan will be executed on confirmation.',\n    )\n    parser.add_argument(\n        '--no-confirm',\n        action='store_true',\n        help='Proposed-plan will be executed without confirmation.'\n             ' --apply flag also required.',\n    )\n    parser.add_argument(\n        '--write-to-file',\n        dest='proposed_plan_file',\n        metavar='<reassignment-plan-file-path>',\n        type=str,\n        help='Write the partition reassignment plan '\n             'to a json file.',\n    )\n    parser.add_argument(\n        '--group-parser',\n        type=str,\n        help='Module containing an implementation of ReplicationGroupParser. '\n        'The module should be specified as path_to_include_to_py_path:module. '\n        'Ex: \"/module/path:module.parser\". '\n        'If not specified the default replication group parser will create '\n        'only one group for all brokers.',\n    )\n    parser.add_argument(\n        '--partition-measurer',\n        type=str,\n        help='Module containing an implementation of PartitionMeasurer. '\n        'The module should be specified as path_to_include_to_py_path:module. '\n        'Default: Assign each partition a weight and size of 1.'\n    )\n    parser.add_argument(\n        '--measurer-args',\n        type=str,\n        action='append',\n        default=[],\n        help='Argument list that is passed to the chosen PartitionMeasurer. '\n        'Ex: --measurer-args \"--n 10\" will pass [\"--n\", \"10\"] to the '\n        'PartitionMeasurer\\'s parse_args method.'\n    )\n    parser.add_argument(\n        '--cluster-balancer',\n        type=str,\n        help='Module containing an implementation of ClusterBalancer. '\n        'The module should be specified as path_to_include_to_py_path:module. '\n        'Default: PartitionCountBalancer.',\n    )\n    parser.add_argument(\n        '--balancer-args',\n        type=str,\n        action='append',\n        default=[],\n        help='Argument list that is passed to the chosen ClusterBalancer. '\n        'Ex: --balancer-args \"--n 10\" will pass [\"--n\", \"10\"] to the '\n        'ClusterBalancer\\'s parse_args method.'\n    )\n    parser.add_argument(\n        '--partition-count-balancer',\n        action='store_const',\n        const=PARTITION_COUNT_BALANCER_MODULE,\n        dest='cluster_balancer',\n        help='Use the number of partitions on each broker to balance the '\n        'cluster.',\n    )\n    parser.add_argument(\n        '--genetic-balancer',\n        action='store_const',\n        const=GENETIC_BALANCER_MODULE,\n        dest='cluster_balancer',\n        help='Use partition metrics and a genetic algorithm to balance the '\n        'cluster.',\n    )\n\n    subparsers = parser.add_subparsers()\n    RebalanceCmd().add_subparser(subparsers)\n    DecommissionCmd().add_subparser(subparsers)\n    RevokeLeadershipCmd().add_subparser(subparsers)\n    StatsCmd().add_subparser(subparsers)\n    StoreAssignmentsCmd().add_subparser(subparsers)\n    ReplaceBrokerCmd().add_subparser(subparsers)\n    SetReplicationFactorCmd().add_subparser(subparsers)\n\n    return parser.parse_args()", "response": "Parse the arguments and return a tuple of the arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_topics_with_wrong_rp(topics, zk, default_min_isr):\n    topics_with_wrong_rf = []\n\n    for topic_name, partitions in topics.items():\n        min_isr = get_min_isr(zk, topic_name) or default_min_isr\n        replication_factor = len(partitions[0].replicas)\n\n        if replication_factor >= min_isr + 1:\n            continue\n\n        topics_with_wrong_rf.append({\n            'replication_factor': replication_factor,\n            'min_isr': min_isr,\n            'topic': topic_name,\n        })\n\n    return topics_with_wrong_rf", "response": "Returns topics with wrong replication factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare the output dictionary for the log output.", "response": "def _prepare_output(topics_with_wrong_rf, verbose):\n    \"\"\"Returns dict with 'raw' and 'message' keys filled.\"\"\"\n    out = {}\n    topics_count = len(topics_with_wrong_rf)\n    out['raw'] = {\n        'topics_with_wrong_replication_factor_count': topics_count,\n    }\n\n    if topics_count == 0:\n        out['message'] = 'All topics have proper replication factor.'\n    else:\n        out['message'] = (\n            \"{0} topic(s) have replication factor lower than specified min ISR + 1.\"\n        ).format(topics_count)\n\n        if verbose:\n            lines = (\n                \"replication_factor={replication_factor} is lower than min_isr={min_isr} + 1 for {topic}\"\n                .format(\n                    min_isr=topic['min_isr'],\n                    topic=topic['topic'],\n                    replication_factor=topic['replication_factor'],\n                )\n                for topic in topics_with_wrong_rf\n            )\n            out['verbose'] = \"Topics:\\n\" + \"\\n\".join(lines)\n    if verbose:\n        out['raw']['topics'] = topics_with_wrong_rf\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck replication factor settings and compare it with the min. isr in the cluster.", "response": "def run_command(self):\n        \"\"\"Replication factor command, checks replication factor settings and compare it with\n        min.isr in the cluster.\"\"\"\n        topics = get_topic_partition_metadata(self.cluster_config.broker_list)\n\n        topics_with_wrong_rf = _find_topics_with_wrong_rp(\n            topics,\n            self.zk,\n            self.args.default_min_isr,\n        )\n\n        errcode = status_code.OK if not topics_with_wrong_rf else status_code.CRITICAL\n        out = _prepare_output(topics_with_wrong_rf, self.args.verbose)\n        return errcode, out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes an OffsetCommitRequest struct into a kafka. protocol. commit. OffsetCommitRequest struct.", "response": "def encode_offset_commit_request_kafka(cls, group, payloads):\n        \"\"\"\n        Encode an OffsetCommitRequest struct\n        Arguments:\n            group: string, the consumer group you are committing offsets for\n            payloads: list of OffsetCommitRequestPayload\n        \"\"\"\n        return kafka.protocol.commit.OffsetCommitRequest[2](\n            consumer_group=group,\n            consumer_group_generation_id=kafka.protocol.commit.OffsetCommitRequest[2].DEFAULT_GENERATION_ID,\n            consumer_id='',\n            retention_time=kafka.protocol.commit.OffsetCommitRequest[2].DEFAULT_RETENTION_TIME,\n            topics=[(\n                topic,\n                [(\n                    partition,\n                    payload.offset,\n                    payload.metadata)\n                    for partition, payload in six.iteritems(topic_payloads)])\n                for topic, topic_payloads in six.iteritems(group_by_topic_and_partition(payloads))])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode GroupCoordinatorResponse. Note that ConsumerMetadataResponse is renamed to GroupCoordinatorResponse in 0.9+ Arguments: response: response to decode", "response": "def decode_consumer_metadata_response(cls, response):\n        \"\"\"\n        Decode GroupCoordinatorResponse. Note that ConsumerMetadataResponse is\n        renamed to GroupCoordinatorResponse in 0.9+\n        Arguments:\n            response: response to decode\n        \"\"\"\n        return ConsumerMetadataResponse(\n            response.error_code,\n            response.coordinator_id,\n            response.host,\n            response.port,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decommission_brokers(self, broker_ids):\n        decommission_brokers = []\n        for broker_id in broker_ids:\n            try:\n                broker = self.cluster_topology.brokers[broker_id]\n                broker.mark_decommissioned()\n                decommission_brokers.append(broker)\n            except KeyError:\n                raise InvalidBrokerIdError(\n                    \"No broker found with id {broker_id}\".format(broker_id=broker_id)\n                )\n\n        partitions = defaultdict(int)\n\n        # Remove all partitions from decommissioned brokers.\n        for broker in decommission_brokers:\n            broker_partitions = list(broker.partitions)\n            for partition in broker_partitions:\n                broker.remove_partition(partition)\n                partitions[partition.name] += 1\n\n        active_brokers = self.cluster_topology.active_brokers\n\n        # Create state from the initial cluster topology.\n        self.state = _State(self.cluster_topology, brokers=active_brokers)\n\n        # Add partition replicas to active brokers one-by-one.\n        for partition_name in sorted(six.iterkeys(partitions)):  # repeatability\n            partition = self.cluster_topology.partitions[partition_name]\n            replica_count = partitions[partition_name]\n            try:\n                self.add_replica(partition_name, replica_count)\n            except InvalidReplicationFactorError:\n                raise BrokerDecommissionError(\n                    \"Not enough active brokers in the cluster. \"\n                    \"Partition {partition} has replication-factor {rf}, \"\n                    \"but only {brokers} active brokers remain.\"\n                    .format(\n                        partition=partition_name,\n                        rf=partition.replication_factor + replica_count,\n                        brokers=len(active_brokers)\n                    )\n                )", "response": "This method is used to remove all partitions from decommissioned brokers and add them one - by - one back to the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_replica(self, partition_name, count=1):\n        try:\n            partition = self.cluster_topology.partitions[partition_name]\n        except KeyError:\n            raise InvalidPartitionError(\n                \"Partition name {name} not found.\".format(name=partition_name),\n            )\n\n        active_brokers = self.cluster_topology.active_brokers\n\n        if partition.replication_factor + count > len(active_brokers):\n            raise InvalidReplicationFactorError(\n                \"Cannot increase replication factor from {rf} to {new_rf}.\"\n                \" There are only {brokers} active brokers.\"\n                .format(\n                    rf=partition.replication_factor,\n                    new_rf=partition.replication_factor + count,\n                    brokers=len(active_brokers),\n                )\n            )\n\n        partition_index = self.state.partition_indices[partition]\n\n        for _ in range(count):\n            # Find eligible replication-groups.\n            non_full_rgs = [\n                rg for rg in six.itervalues(self.cluster_topology.rgs)\n                if rg.count_replica(partition) < len(rg.active_brokers)\n            ]\n            # Since replicas can only be added to non-full rgs, only consider\n            # replicas on those rgs when determining which rgs are\n            # under-replicated.\n            replica_count = sum(\n                rg.count_replica(partition)\n                for rg in non_full_rgs\n            )\n            opt_replicas, _ = compute_optimum(\n                len(non_full_rgs),\n                replica_count,\n            )\n            under_replicated_rgs = [\n                rg for rg in non_full_rgs\n                if rg.count_replica(partition) < opt_replicas\n            ] or non_full_rgs\n\n            # Add the replica to every eligible broker, as follower and leader\n            new_states = []\n            for rg in under_replicated_rgs:\n                for broker in rg.active_brokers:\n                    if broker not in partition.replicas:\n                        broker_index = self.state.brokers.index(broker)\n                        new_state = self.state.add_replica(\n                            partition_index,\n                            broker_index,\n                        )\n                        new_state_leader = new_state.move_leadership(\n                            partition_index,\n                            broker_index,\n                        )\n                        new_states.extend([new_state, new_state_leader])\n\n            # Update cluster topology with highest scoring state.\n            self.state = sorted(new_states, key=self._score, reverse=True)[0]\n            self.cluster_topology.update_cluster_topology(self.state.pending_assignment)\n\n            # Update the internal state to match.\n            self.state.clear_pending_assignment()", "response": "Add a replica to every available broker in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a replica from every broker.", "response": "def remove_replica(self, partition_name, osr_broker_ids, count=1):\n        \"\"\"Removing a replica is done by trying to remove a replica from every\n        broker and choosing the resulting state with the highest fitness score.\n        Out-of-sync replicas will always be removed before in-sync replicas.\n\n        :param partition_name: (topic_id, partition_id) of the partition to remove replicas of.\n        :param osr_broker_ids: A list of the partition's out-of-sync broker ids.\n        :param count: The number of replicas to remove.\n        \"\"\"\n        try:\n            partition = self.cluster_topology.partitions[partition_name]\n        except KeyError:\n            raise InvalidPartitionError(\n                \"Partition name {name} not found.\".format(name=partition_name),\n            )\n\n        if partition.replication_factor - count < 1:\n            raise InvalidReplicationFactorError(\n                \"Cannot decrease replication factor from {rf} to {new_rf}.\"\n                \"Replication factor must be at least 1.\"\n                .format(\n                    rf=partition.replication_factor,\n                    new_rf=partition.replication_factor - count,\n                )\n            )\n\n        osr = {\n            broker for broker in partition.replicas\n            if broker.id in osr_broker_ids\n        }\n\n        # Create state from current cluster topology.\n        state = _State(self.cluster_topology)\n        partition_index = state.partitions.index(partition)\n\n        for _ in range(count):\n            # Find eligible replication groups.\n            non_empty_rgs = [\n                rg for rg in six.itervalues(self.cluster_topology.rgs)\n                if rg.count_replica(partition) > 0\n            ]\n            rgs_with_osr = [\n                rg for rg in non_empty_rgs\n                if any(b in osr for b in rg.brokers)\n            ]\n            candidate_rgs = rgs_with_osr or non_empty_rgs\n            # Since replicas will only be removed from the candidate rgs, only\n            # count replicas on those rgs when determining which rgs are\n            # over-replicated.\n            replica_count = sum(\n                rg.count_replica(partition)\n                for rg in candidate_rgs\n            )\n            opt_replicas, _ = compute_optimum(\n                len(candidate_rgs),\n                replica_count,\n            )\n            over_replicated_rgs = [\n                rg for rg in candidate_rgs\n                if rg.count_replica(partition) > opt_replicas\n            ] or candidate_rgs\n            candidate_rgs = over_replicated_rgs or candidate_rgs\n\n            # Remove the replica from every eligible broker.\n            new_states = []\n            for rg in candidate_rgs:\n                osr_brokers = {\n                    broker for broker in rg.brokers\n                    if broker in osr\n                }\n                candidate_brokers = osr_brokers or rg.brokers\n                for broker in candidate_brokers:\n                    if broker in partition.replicas:\n                        broker_index = state.brokers.index(broker)\n                        new_states.append(\n                            state.remove_replica(partition_index, broker_index)\n                        )\n\n            # Update cluster topology with highest scoring state.\n            state = sorted(new_states, key=self._score, reverse=True)[0]\n            self.cluster_topology.update_cluster_topology(state.assignment)\n            osr = {b for b in osr if b in partition.replicas}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _explore(self, pop):\n        new_pop = set(pop)\n        exploration_per_state = self.args.max_exploration // len(pop)\n\n        mutations = []\n        if self.args.brokers:\n            mutations.append(self._move_partition)\n        if self.args.leaders:\n            mutations.append(self._move_leadership)\n\n        for state in pop:\n            for _ in range(exploration_per_state):\n                new_state = random.choice(mutations)(state)\n                if new_state:\n                    new_pop.add(new_state)\n\n        return new_pop", "response": "Exploration phase : Finds a set of candidate states based on the current population."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nattempt to move a random partition to a random broker.", "response": "def _move_partition(self, state):\n        \"\"\"Attempt to move a random partition to a random broker. If the\n        chosen movement is not possible, None is returned.\n\n        :param state: The starting state.\n\n        :return: The resulting State object if a movement is found. None if\n            no movement is found.\n        \"\"\"\n        partition = random.randint(0, len(self.cluster_topology.partitions) - 1)\n\n        # Choose distinct source and destination brokers.\n        source = random.choice(state.replicas[partition])\n        dest = random.randint(0, len(self.cluster_topology.brokers) - 1)\n        if dest in state.replicas[partition]:\n            return None\n        source_rg = state.broker_rg[source]\n        dest_rg = state.broker_rg[dest]\n\n        # Ensure replicas remain balanced across replication groups.\n        if source_rg != dest_rg:\n            source_rg_replicas = state.rg_replicas[source_rg][partition]\n            dest_rg_replicas = state.rg_replicas[dest_rg][partition]\n            if source_rg_replicas <= dest_rg_replicas:\n                return None\n\n        # Ensure movement size capacity is not surpassed\n        partition_size = state.partition_sizes[partition]\n        if (self.args.max_movement_size is not None and\n                state.movement_size + partition_size >\n                self.args.max_movement_size):\n            return None\n\n        return state.move(partition, source, dest)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _move_leadership(self, state):\n        partition = random.randint(0, len(self.cluster_topology.partitions) - 1)\n\n        # Moving zero weight partitions will not improve balance for any of the\n        # balance criteria. Disallow these movements here to avoid wasted\n        # effort.\n        if state.partition_weights[partition] == 0:\n            return None\n        if len(state.replicas[partition]) <= 1:\n            return None\n        dest_index = random.randint(1, len(state.replicas[partition]) - 1)\n        dest = state.replicas[partition][dest_index]\n        if (self.args.max_leader_changes is not None and\n                state.leader_movement_count >= self.args.max_leader_changes):\n            return None\n\n        return state.move_leadership(partition, dest)", "response": "Attempt to move a random partition to a random broker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prune(self, pop_candidates):\n        return set(\n            sorted(pop_candidates, key=self._score, reverse=True)\n            [:self.args.max_pop]\n        )", "response": "Prune the set of candidate states that are not in the next set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscoring a state based on how balanced it is. A higher score represents a more balanced state.", "response": "def _score(self, state, score_movement=True):\n        \"\"\"Score a state based on how balanced it is. A higher score represents\n        a more balanced state.\n\n        :param state: The state to score.\n        \"\"\"\n        score = 0\n        max_score = 0\n        if state.total_weight:\n            # Coefficient of variance is a value between 0 and the sqrt(n)\n            # where n is the length of the series (the number of brokers)\n            # so those parameters are scaled by (1 / sqrt(# or brokers)) to\n            # get a value between 0 and 1.\n            #\n            # Since smaller imbalance values are preferred use 1 - x so that\n            # higher scores correspond to more balanced states.\n            score += self.args.partition_weight_cv_score_weight * \\\n                (1 - state.broker_weight_cv / sqrt(len(state.brokers)))\n            score += self.args.leader_weight_cv_score_weight * \\\n                (1 - state.broker_leader_weight_cv / sqrt(len(state.brokers)))\n            score += self.args.topic_broker_imbalance_score_weight * \\\n                (1 - state.weighted_topic_broker_imbalance)\n            score += self.args.broker_partition_count_score_weight * \\\n                (1 - state.broker_partition_count_cv / sqrt(len(state.brokers)))\n            score += self.args.broker_leader_count_score_weight * \\\n                (1 - state.broker_leader_count_cv / sqrt(len(state.brokers)))\n            max_score += self.args.partition_weight_cv_score_weight\n            max_score += self.args.leader_weight_cv_score_weight\n            max_score += self.args.topic_broker_imbalance_score_weight\n            max_score += self.args.broker_partition_count_score_weight\n            max_score += self.args.broker_leader_count_score_weight\n\n        if self.args.max_movement_size is not None and score_movement:\n            # Avoid potential divide-by-zero error\n            max_movement = max(self.args.max_movement_size, 1)\n            score += self.args.movement_size_score_weight * \\\n                (1 - state.movement_size / max_movement)\n            max_score += self.args.movement_size_score_weight\n\n        if self.args.max_leader_changes is not None and score_movement:\n            # Avoid potential divide-by-zero error\n            max_leader = max(self.args.max_leader_changes, 1)\n            score += self.args.leader_change_score_weight * \\\n                (1 - state.leader_movement_count / max_leader)\n            max_score += self.args.leader_change_score_weight\n\n        return score / max_score"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new state that is the result of moving a single partition from source to dest.", "response": "def move(self, partition, source, dest):\n        \"\"\"Return a new state that is the result of moving a single partition.\n\n        :param partition: The partition index of the partition to move.\n        :param source: The broker index of the broker to move the partition\n            from.\n        :param dest: The broker index of the broker to move the partition to.\n        \"\"\"\n        new_state = copy(self)\n\n        # Update the partition replica tuple\n        source_index = self.replicas[partition].index(source)\n        new_state.replicas = tuple_alter(\n            self.replicas,\n            (partition, lambda replicas: tuple_replace(\n                replicas,\n                (source_index, dest),\n            )),\n        )\n\n        new_state.pending_partitions = self.pending_partitions + (partition, )\n\n        # Update the broker weights\n        partition_weight = self.partition_weights[partition]\n\n        new_state.broker_weights = tuple_alter(\n            self.broker_weights,\n            (source, lambda broker_weight: broker_weight - partition_weight),\n            (dest, lambda broker_weight: broker_weight + partition_weight),\n        )\n\n        # Update the broker partition count\n        new_state.broker_partition_counts = tuple_alter(\n            self.broker_partition_counts,\n            (source, lambda partition_count: partition_count - 1),\n            (dest, lambda partition_count: partition_count + 1),\n        )\n\n        # Update the broker leader weights\n        if source_index == 0:\n            new_state.broker_leader_weights = tuple_alter(\n                self.broker_leader_weights,\n                (source, lambda lw: lw - partition_weight),\n                (dest, lambda lw: lw + partition_weight),\n            )\n            new_state.broker_leader_counts = tuple_alter(\n                self.broker_leader_counts,\n                (source, lambda leader_count: leader_count - 1),\n                (dest, lambda leader_count: leader_count + 1),\n            )\n            new_state.leader_movement_count += 1\n\n        # Update the topic broker counts\n        topic = self.partition_topic[partition]\n\n        new_state.topic_broker_count = tuple_alter(\n            self.topic_broker_count,\n            (topic, lambda broker_count: tuple_alter(\n                broker_count,\n                (source, lambda count: count - 1),\n                (dest, lambda count: count + 1),\n            )),\n        )\n\n        # Update the topic broker imbalance\n        new_state.topic_broker_imbalance = tuple_replace(\n            self.topic_broker_imbalance,\n            (topic, new_state._calculate_topic_imbalance(topic)),\n        )\n\n        new_state._weighted_topic_broker_imbalance = (\n            self._weighted_topic_broker_imbalance +\n            self.topic_weights[topic] * (\n                new_state.topic_broker_imbalance[topic] -\n                self.topic_broker_imbalance[topic]\n            )\n        )\n\n        # Update the replication group replica counts\n        source_rg = self.broker_rg[source]\n        dest_rg = self.broker_rg[dest]\n        if source_rg != dest_rg:\n            new_state.rg_replicas = tuple_alter(\n                self.rg_replicas,\n                (source_rg, lambda replica_counts: tuple_alter(\n                    replica_counts,\n                    (partition, lambda replica_count: replica_count - 1),\n                )),\n                (dest_rg, lambda replica_counts: tuple_alter(\n                    replica_counts,\n                    (partition, lambda replica_count: replica_count + 1),\n                )),\n            )\n\n        # Update the movement sizes\n        new_state.movement_size += self.partition_sizes[partition]\n        new_state.movement_count += 1\n\n        return new_state"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_leadership(self, partition, new_leader):\n        new_state = copy(self)\n\n        # Update the partition replica tuple\n        source = new_state.replicas[partition][0]\n        new_leader_index = self.replicas[partition].index(new_leader)\n        new_state.replicas = tuple_alter(\n            self.replicas,\n            (partition, lambda replicas: tuple_replace(\n                replicas,\n                (0, replicas[new_leader_index]),\n                (new_leader_index, replicas[0]),\n            )),\n        )\n\n        new_state.pending_partitions = self.pending_partitions + (partition, )\n\n        # Update the leader count\n        new_state.broker_leader_counts = tuple_alter(\n            self.broker_leader_counts,\n            (source, lambda leader_count: leader_count - 1),\n            (new_leader, lambda leader_count: leader_count + 1),\n        )\n\n        # Update the broker leader weights\n        partition_weight = self.partition_weights[partition]\n        new_state.broker_leader_weights = tuple_alter(\n            self.broker_leader_weights,\n            (source, lambda leader_weight: leader_weight - partition_weight),\n            (new_leader, lambda leader_weight: leader_weight + partition_weight),\n        )\n\n        # Update the total leader movement size\n        new_state.leader_movement_count += 1\n\n        return new_state", "response": "Returns a new state that is the result of changing the leadership of the broker of the given partition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assignment(self):\n        return {\n            partition.name: [\n                self.brokers[bid].id for bid in self.replicas[pid]\n            ]\n            for pid, partition in enumerate(self.partitions)\n        }", "response": "Return the partition assignment that this state represents."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pending_assignment(self):\n        return {\n            self.partitions[pid].name: [\n                self.brokers[bid].id for bid in self.replicas[pid]\n            ]\n            for pid in set(self.pending_partitions)\n        }", "response": "Return the pending partition assignment that this state represents."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prepare_output(partitions, unavailable_brokers, verbose):\n    partitions_count = len(partitions)\n    out = {}\n    out['raw'] = {\n        'replica_unavailability_count': partitions_count,\n    }\n\n    if partitions_count == 0:\n        out['message'] = 'All replicas available for communication.'\n    else:\n        out['message'] = \"{replica_unavailability} replicas unavailable for communication. \" \\\n            \"Unavailable Brokers: {unavailable_brokers}\".format(\n            replica_unavailability=partitions_count,\n            unavailable_brokers=', '.join([str(e) for e in unavailable_brokers]),\n        )\n        if verbose:\n            lines = (\n                '{}:{}'.format(topic, partition)\n                for (topic, partition) in partitions\n            )\n            out['verbose'] = \"Partitions:\\n\" + \"\\n\".join(lines)\n\n    if verbose:\n        out['raw']['partitions'] = [\n            {'topic': topic, 'partition': partition}\n            for (topic, partition) in partitions\n        ]\n\n    return out", "response": "Prepares the output dict for the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_min_isr(zk, topic):\n    ISR_CONF_NAME = 'min.insync.replicas'\n    try:\n        config = zk.get_topic_config(topic)\n    except NoNodeError:\n        return None\n    if ISR_CONF_NAME in config['config']:\n        return int(config['config'][ISR_CONF_NAME])\n    else:\n        return None", "response": "Return the min - isr for the given topic or None if not specified."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_metadata_response(topics, zk, default_min_isr):\n    not_in_sync_partitions = []\n    for topic_name, partitions in topics.items():\n        min_isr = get_min_isr(zk, topic_name) or default_min_isr\n        if min_isr is None:\n            continue\n        for metadata in partitions.values():\n            cur_isr = len(metadata.isr)\n            if cur_isr < min_isr:\n                not_in_sync_partitions.append({\n                    'isr': cur_isr,\n                    'min_isr': min_isr,\n                    'topic': metadata.topic,\n                    'partition': metadata.partition,\n                })\n\n    return not_in_sync_partitions", "response": "Returns not in sync partitions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the output dict for the internal use.", "response": "def _prepare_output(partitions, verbose):\n    \"\"\"Returns dict with 'raw' and 'message' keys filled.\"\"\"\n    out = {}\n    partitions_count = len(partitions)\n    out['raw'] = {\n        'not_enough_replicas_count': partitions_count,\n    }\n\n    if partitions_count == 0:\n        out['message'] = 'All replicas in sync.'\n    else:\n        out['message'] = (\n            \"{0} partition(s) have the number of replicas in \"\n            \"sync that is lower than the specified min ISR.\"\n        ).format(partitions_count)\n\n        if verbose:\n            lines = (\n                \"isr={isr} is lower than min_isr={min_isr} for {topic}:{partition}\"\n                .format(\n                    isr=p['isr'],\n                    min_isr=p['min_isr'],\n                    topic=p['topic'],\n                    partition=p['partition'],\n                )\n                for p in partitions\n            )\n            out['verbose'] = \"Partitions:\\n\" + \"\\n\".join(lines)\n    if verbose:\n        out['raw']['partitions'] = partitions\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_partition(self, partition):\n        if partition in self._partitions:\n            # Remove partition from set\n            self._partitions.remove(partition)\n            # Remove broker from replica list of partition\n            partition.replicas.remove(self)\n        else:\n            raise ValueError(\n                'Partition: {topic_id}:{partition_id} not found in broker '\n                '{broker_id}'.format(\n                    topic_id=partition.topic.id,\n                    partition_id=partition.partition_id,\n                    broker_id=self._id,\n                )\n            )", "response": "Removes partition from partition list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds partition to partition list.", "response": "def add_partition(self, partition):\n        \"\"\"Add partition to partition list.\"\"\"\n        assert(partition not in self._partitions)\n        # Add partition to existing set\n        self._partitions.add(partition)\n        # Add broker to replica list\n        partition.add_replica(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmove partition to destination broker and adjust replicas.", "response": "def move_partition(self, partition, broker_destination):\n        \"\"\"Move partition to destination broker and adjust replicas.\"\"\"\n        self.remove_partition(partition)\n        broker_destination.add_partition(partition)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of partitions for given topic.", "response": "def count_partitions(self, topic):\n        \"\"\"Return count of partitions for given topic.\"\"\"\n        return sum(1 for p in topic.partitions if p in self.partitions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_preferred_partition(self, broker, sibling_distance):\n        # Only partitions not having replica in broker are valid\n        # Get best fit partition, based on avoiding partition from same topic\n        # and partition with least siblings in destination-broker.\n        eligible_partitions = self.partitions - broker.partitions\n        if eligible_partitions:\n            pref_partition = min(\n                eligible_partitions,\n                key=lambda source_partition:\n                    sibling_distance[source_partition.topic],\n            )\n            return pref_partition\n        else:\n            return None", "response": "Returns the preferred partition for the topic with the minimum distance between destination and source."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef donate_leadership(self, opt_count, skip_brokers, used_edges):\n        owned_partitions = list(filter(\n            lambda p: self is p.leader and len(p.replicas) > 1,\n            self.partitions,\n        ))\n        for partition in owned_partitions:\n            # Skip using same partition with broker if already used before\n            potential_new_leaders = list(filter(\n                lambda f: f not in skip_brokers,\n                partition.followers,\n            ))\n            for follower in potential_new_leaders:\n                # Don't swap the broker-pair if already swapped before\n                # in same partition\n                if (partition, self, follower) in used_edges:\n                    continue\n                partition.swap_leader(follower)\n                used_edges.append((partition, follower, self))\n                # new-leader didn't unbalance\n                if follower.count_preferred_replica() <= opt_count + 1:\n                    # over-broker balanced\n                    # If over-broker is the one which needs to be revoked from leadership\n                    # it's considered balanced only if its preferred replica count is 0\n                    if (self.count_preferred_replica() <= opt_count + 1 and not self.revoked_leadership) or \\\n                            (self.count_preferred_replica() == 0 and self.revoked_leadership):\n                        return\n                    else:\n                        # Try next-partition, not another follower\n                        break\n                else:  # new-leader (broker) became over-balanced\n                    skip_brokers.append(follower)\n                    follower.donate_leadership(opt_count, skip_brokers, used_edges)\n                    # new-leader couldn't be balanced, revert\n                    if follower.count_preferred_replica() > opt_count + 1:\n                        used_edges.append((partition, follower, self))\n                        partition.swap_leader(self)\n                        # Try next leader or partition\n                        continue\n                    else:\n                        # New-leader was successfully balanced\n                        used_edges.append((partition, follower, self))\n                        # New-leader can be reused\n                        skip_brokers.remove(follower)\n                        # If broker is the one which needs to be revoked from leadership\n                        # it's considered balanced only if its preferred replica count is 0\n                        if (self.count_preferred_replica() <= opt_count + 1 and not self.revoked_leadership) or \\\n                                (self.count_preferred_replica() == 0 and self.revoked_leadership):\n                            # Now broker is balanced\n                            return\n                        else:\n                            # Try next-partition, not another follower\n                            break", "response": "This method tries to donate the leadership of the broker - pair to one of its followers and then tries to donate the leadership to one of its followers until they become balanced."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_command(self, cluster_topology, cluster_balancer):\n\n        # The ideal weight of each broker is total_weight / broker_count.\n        # It should be possible to remove partitions from each broker until\n        # the weight of the broker is less than this ideal value, otherwise it\n        # is impossible to balance the cluster. If --max-movement-size is too\n        # small, exit with an error.\n        if self.args.max_movement_size:\n            total_weight = sum(\n                partition.weight\n                for partition in six.itervalues(cluster_topology.partitions)\n            )\n            broker_count = len(cluster_topology.brokers)\n            optimal_weight = total_weight / broker_count\n\n            broker, max_unmovable_on_one_broker = max((\n                (broker, sum(\n                    partition.weight\n                    for partition in broker.partitions\n                    if partition.size > self.args.max_movement_size\n                ))\n                for broker in cluster_topology.brokers.values()),\n                key=lambda t: t[1],\n            )\n\n            if max_unmovable_on_one_broker >= optimal_weight:\n                sorted_partitions = sorted(\n                    [\n                        partition\n                        for partition in broker.partitions\n                        if partition.size > self.args.max_movement_size\n                    ],\n                    reverse=True,\n                    key=lambda partition: partition.size,\n                )\n\n                for partition in sorted_partitions:\n                    max_unmovable_on_one_broker -= partition.weight\n                    if max_unmovable_on_one_broker <= optimal_weight:\n                        required_max_movement_size = partition.size\n                        break\n\n                self.log.error(\n                    'Max movement size {max_movement_size} is too small, it is'\n                    ' not be possible to balance the cluster. A max movement'\n                    ' size of {required} or higher is required.'.format(\n                        max_movement_size=self.args.max_movement_size,\n                        required=required_max_movement_size,\n                    )\n                )\n                sys.exit(1)\n        elif self.args.auto_max_movement_size:\n            self.args.max_movement_size = max(\n                partition.size\n                for partition in six.itervalues(cluster_topology.partitions)\n            )\n            self.log.info(\n                'Auto-max-movement-size: using {max_movement_size} as'\n                ' max-movement-size.'.format(\n                    max_movement_size=self.args.max_movement_size,\n                )\n            )\n\n        base_assignment = cluster_topology.assignment\n        base_score = cluster_balancer.score()\n        rg_imbalance, _ = get_replication_group_imbalance_stats(\n            list(cluster_topology.rgs.values()),\n            list(cluster_topology.partitions.values())\n        )\n\n        cluster_balancer.rebalance()\n\n        assignment = cluster_topology.assignment\n        score = cluster_balancer.score()\n        new_rg_imbalance, _ = get_replication_group_imbalance_stats(\n            list(cluster_topology.rgs.values()),\n            list(cluster_topology.partitions.values())\n        )\n\n        if self.args.show_stats:\n            display_cluster_topology_stats(cluster_topology, base_assignment)\n            if base_score is not None and score is not None:\n                print('\\nScore before: %f' % base_score)\n                print('Score after:  %f' % score)\n                print('Score improvement: %f' % (score - base_score))\n\n        if not validate_plan(\n            assignment_to_plan(assignment),\n            assignment_to_plan(base_assignment),\n        ):\n            self.log.error('Invalid latest-cluster assignment. Exiting.')\n            sys.exit(1)\n\n        if self.args.score_improvement_threshold:\n            if base_score is None or score is None:\n                self.log.error(\n                    '%s cannot assign scores so --score-improvement-threshold'\n                    ' cannot be used.',\n                    cluster_balancer.__class__.__name__,\n                )\n                return\n            else:\n                score_improvement = score - base_score\n                if score_improvement >= self.args.score_improvement_threshold:\n                    self.log.info(\n                        'Score improvement %f is greater than the threshold %f.'\n                        ' Continuing to apply the assignment.',\n                        score_improvement,\n                        self.args.score_improvement_threshold,\n                    )\n                elif new_rg_imbalance < rg_imbalance:\n                    self.log.info(\n                        'Score improvement %f is less than the threshold %f,'\n                        ' but replica balance has improved. Continuing to'\n                        ' apply the assignment.',\n                        score_improvement,\n                        self.args.score_improvement_threshold,\n                    )\n                else:\n                    self.log.info(\n                        'Score improvement %f is less than the threshold %f.'\n                        ' Assignment will not be applied.',\n                        score_improvement,\n                        self.args.score_improvement_threshold,\n                    )\n                    return\n\n        # Reduce the proposed assignment based on max_partition_movements\n        # and max_leader_changes\n        reduced_assignment = self.get_reduced_assignment(\n            base_assignment,\n            cluster_topology,\n            self.args.max_partition_movements,\n            self.args.max_leader_changes,\n        )\n        if reduced_assignment:\n            self.process_assignment(reduced_assignment)\n        else:\n            self.log.info(\"Cluster already balanced. No actions to perform.\")", "response": "Run the executable proposed plan for display or execution."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ssh_client(host):\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(host)\n    return ssh", "response": "Start an ssh client."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the log files depending on their modification time. :param data_path: the path to the Kafka data directory :type data_path: str :param minutes: check the files modified in the last N minutes :type minutes: int :param start_time: check the files modified after start_time :type start_time: str :param end_time: check the files modified before end_time :type end_time: str :returns: the find command :rtype: str", "response": "def find_files_cmd(data_path, minutes, start_time, end_time):\n    \"\"\"Find the log files depending on their modification time.\n\n    :param data_path: the path to the Kafka data directory\n    :type data_path: str\n    :param minutes: check the files modified in the last N minutes\n    :type minutes: int\n    :param start_time: check the files modified after start_time\n    :type start_time: str\n    :param end_time: check the files modified before end_time\n    :type end_time: str\n    :returns: the find command\n    :rtype: str\n    \"\"\"\n    if minutes:\n        return FIND_MINUTES_COMMAND.format(\n            data_path=data_path,\n            minutes=minutes,\n        )\n    if start_time:\n        if end_time:\n            return FIND_RANGE_COMMAND.format(\n                data_path=data_path,\n                start_time=start_time,\n                end_time=end_time,\n            )\n        else:\n            return FIND_START_COMMAND.format(\n                data_path=data_path,\n                start_time=start_time,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_corrupted_files_cmd(java_home, files):\n    files_str = \",\".join(files)\n    check_command = CHECK_COMMAND.format(\n        ionice=IONICE,\n        java_home=java_home,\n        files=files_str,\n    )\n    # One line per message can generate several MB/s of data\n    # Use pre-filtering on the server side to reduce it\n    command = \"{check_command} | {reduce_output}\".format(\n        check_command=check_command,\n        reduce_output=REDUCE_OUTPUT,\n    )\n    return command", "response": "Returns the command to check the corruption of the specified files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a command on the specified host and return a list of output lines.", "response": "def get_output_lines_from_command(host, command):\n    \"\"\"Execute a command on the specified host, returning a list of\n    output lines.\n\n    :param host: the host name\n    :type host: str\n    :param command: the command\n    :type commmand: str\n    \"\"\"\n    with closing(ssh_client(host)) as ssh:\n        _, stdout, stderr = ssh.exec_command(command)\n        lines = stdout.read().splitlines()\n        report_stderr(host, stderr)\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all the Kafka log files on the given broker that have been modified by the given time range.", "response": "def find_files(data_path, brokers, minutes, start_time, end_time):\n    \"\"\"Find all the Kafka log files on the broker that have been modified\n    in the speficied time range.\n\n    start_time and end_time should be in the format specified\n    by TIME_FORMAT_REGEX.\n\n    :param data_path: the path to the lof files on the broker\n    :type data_path: str\n    :param brokers: the brokers\n    :type brokers: list of (broker_id, host) pairs\n    :param minutes: check the files modified in the last N minutes\n    :type minutes: int\n    :param start_time: check the files modified after start_time\n    :type start_time: str\n    :param end_time: check the files modified before end_time\n    :type end_time: str\n    :returns: the files\n    :rtype: list of (broker, host, file_path) tuples\n    \"\"\"\n    command = find_files_cmd(data_path, minutes, start_time, end_time)\n    pool = Pool(len(brokers))\n    result = pool.map(\n        partial(get_output_lines_from_command, command=command),\n        [host for broker, host in brokers])\n    return [(broker, host, files)\n            for (broker, host), files\n            in zip(brokers, result)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_output(host, output):\n    current_file = None\n    for line in output.readlines():\n        file_name_search = FILE_PATH_REGEX.search(line)\n        if file_name_search:\n            current_file = file_name_search.group(1)\n            continue\n        if INVALID_MESSAGE_REGEX.match(line) or INVALID_BYTES_REGEX.match(line):\n            print_line(host, current_file, line, \"ERROR\")\n        elif VALID_MESSAGE_REGEX.match(line) or \\\n                line.startswith('Starting offset:'):\n            continue\n        else:\n            print_line(host, current_file, line, \"UNEXPECTED OUTPUT\")", "response": "Parse the output of the dump tool and print warnings or error messages\n    accordingly."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a line to stdout.", "response": "def print_line(host, path, line, line_type):\n    \"\"\"Print a dump tool line to stdout.\n\n    :param host: the source host\n    :type host: str\n    :param path: the path to the file that is being analyzed\n    :type path: str\n    :param line: the line to be printed\n    :type line: str\n    :param line_type: a header for the line\n    :type line_type: str\n    \"\"\"\n    print(\n        \"{ltype} Host: {host}, File: {path}\".format(\n            ltype=line_type,\n            host=host,\n            path=path,\n        )\n    )\n    print(\"{ltype} Output: {line}\".format(ltype=line_type, line=line))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the files on the host.", "response": "def check_files_on_host(java_home, host, files, batch_size):\n    \"\"\"Check the files on the host. Files are grouped together in groups\n    of batch_size files. The dump class will be executed on each batch,\n    sequentially.\n\n    :param java_home: the JAVA_HOME of the broker\n    :type java_home: str\n    :param host: the host where the tool will be executed\n    :type host: str\n    :param files: the list of files to be analyzed\n    :type files: list of str\n    :param batch_size: the size of each batch\n    :type batch_size: int\n    \"\"\"\n    with closing(ssh_client(host)) as ssh:\n        for i, batch in enumerate(chunks(files, batch_size)):\n            command = check_corrupted_files_cmd(java_home, batch)\n            _, stdout, stderr = ssh.exec_command(command)\n            report_stderr(host, stderr)\n            print(\n                \"  {host}: file {n_file} of {total}\".format(\n                    host=host,\n                    n_file=(i * DEFAULT_BATCH_SIZE),\n                    total=len(files),\n                )\n            )\n            parse_output(host, stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_partition_leaders(cluster_config):\n    client = KafkaClient(cluster_config.broker_list)\n    result = {}\n    for topic, topic_data in six.iteritems(client.topic_partitions):\n        for partition, p_data in six.iteritems(topic_data):\n            topic_partition = topic + \"-\" + str(partition)\n            result[topic_partition] = p_data.leader\n    return result", "response": "Return the current leaders of all partitions. Partitions are\n    returned as a topic - partition string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tp_from_file(file_path):\n    match = TP_FROM_FILE_REGEX.match(file_path)\n    if not match:\n        print(\"File path is not valid: \" + file_path)\n        sys.exit(1)\n    return match.group(1)", "response": "Return the name of the topic - partition given the path to the file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_leader_files(cluster_config, broker_files):\n    print(\"Filtering leaders\")\n    leader_of = get_partition_leaders(cluster_config)\n    result = []\n    for broker, host, files in broker_files:\n        filtered = []\n        for file_path in files:\n            tp = get_tp_from_file(file_path)\n            if tp not in leader_of or leader_of[tp] == broker:\n                filtered.append(file_path)\n        result.append((broker, host, filtered))\n        print(\n            \"Broker: {broker}, leader of {l_count} over {f_count} files\".format(\n                broker=broker,\n                l_count=len(filtered),\n                f_count=len(files),\n            )\n        )\n    return result", "response": "Given a list of broker files filter out all the files that are in the replicas."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the integrity of the Kafka log files in a cluster.", "response": "def check_cluster(\n    cluster_config,\n    data_path,\n    java_home,\n    check_replicas,\n    batch_size,\n    minutes,\n    start_time,\n    end_time,\n):\n    \"\"\"Check the integrity of the Kafka log files in a cluster.\n\n    start_time and end_time should be in the format specified\n    by TIME_FORMAT_REGEX.\n\n    :param data_path: the path to the log folder on the broker\n    :type data_path: str\n    :param java_home: the JAVA_HOME of the broker\n    :type java_home: str\n    :param check_replicas: also checks the replica files\n    :type check_replicas: bool\n    :param batch_size: the size of the batch\n    :type batch_size: int\n    :param minutes: check the files modified in the last N minutes\n    :type minutes: int\n    :param start_time: check the files modified after start_time\n    :type start_time: str\n    :param end_time: check the files modified before end_time\n    :type end_time: str\n    \"\"\"\n    brokers = get_broker_list(cluster_config)\n    broker_files = find_files(data_path, brokers, minutes, start_time, end_time)\n    if not check_replicas:  # remove replicas\n        broker_files = filter_leader_files(cluster_config, broker_files)\n    processes = []\n    print(\"Starting {n} parallel processes\".format(n=len(broker_files)))\n    try:\n        for broker, host, files in broker_files:\n            print(\n                \"  Broker: {host}, {n} files to check\".format(\n                    host=host,\n                    n=len(files)),\n            )\n            p = Process(\n                name=\"dump_process_\" + host,\n                target=check_files_on_host,\n                args=(java_home, host, files, batch_size),\n            )\n            p.start()\n            processes.append(p)\n        print(\"Processes running:\")\n        for process in processes:\n            process.join()\n    except KeyboardInterrupt:\n        print(\"Terminating all processes\")\n        for process in processes:\n            process.terminate()\n            process.join()\n        print(\"All processes terminated\")\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_args(args):\n    if not args.minutes and not args.start_time:\n        print(\"Error: missing --minutes or --start-time\")\n        return False\n    if args.minutes and args.start_time:\n        print(\"Error: --minutes shouldn't be specified if --start-time is used\")\n        return False\n    if args.end_time and not args.start_time:\n        print(\"Error: --end-time can't be used without --start-time\")\n        return False\n    if args.minutes and args.minutes <= 0:\n        print(\"Error: --minutes must be > 0\")\n        return False\n    if args.start_time and not TIME_FORMAT_REGEX.match(args.start_time):\n        print(\"Error: --start-time format is not valid\")\n        print(\"Example format: '2015-11-26 11:00:00'\")\n        return False\n    if args.end_time and not TIME_FORMAT_REGEX.match(args.end_time):\n        print(\"Error: --end-time format is not valid\")\n        print(\"Example format: '2015-11-26 11:00:00'\")\n        return False\n    if args.batch_size <= 0:\n        print(\"Error: --batch-size must be > 0\")\n        return False\n    return True", "response": "Basic option validation. Returns False if the options are not valid True otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_command(self, ct, cluster_balancer):\n        if self.args.topic in ct.topics:\n            topic = ct.topics[self.args.topic]\n        else:\n            self.log.error(\n                \"Topic {topic} not found. Exiting.\"\n                .format(topic=self.args.topic),\n            )\n            sys.exit(1)\n\n        if topic.replication_factor == self.args.replication_factor:\n            self.log.info(\n                \"Topic {topic} already has replication factor {rf}. \"\n                \"No action to perform.\"\n                .format(topic=topic.id, rf=self.args.replication_factor),\n            )\n            return\n\n        if self.args.replication_factor > len(ct.brokers):\n            self.log.error(\n                \"Replication factor {rf} is greater than the total number of \"\n                \"brokers {brokers}. Exiting.\"\n                .format(\n                    rf=self.args.replication_factor,\n                    brokers=len(ct.brokers)\n                ),\n            )\n            sys.exit(1)\n\n        base_assignment = ct.assignment\n\n        changes_per_partition = abs(\n            self.args.replication_factor - topic.replication_factor\n        )\n\n        if topic.replication_factor < self.args.replication_factor:\n            self.log.info(\n                \"Increasing topic {topic} replication factor from {old_rf} to \"\n                \"{new_rf}.\"\n                .format(\n                    topic=topic.id,\n                    old_rf=topic.replication_factor,\n                    new_rf=self.args.replication_factor,\n                ),\n            )\n            for partition in topic.partitions:\n                cluster_balancer.add_replica(\n                    partition.name,\n                    changes_per_partition,\n                )\n        else:\n            self.log.info(\n                \"Decreasing topic {topic} replication factor from {old_rf} to \"\n                \"{new_rf}.\"\n                .format(\n                    topic=topic.id,\n                    old_rf=topic.replication_factor,\n                    new_rf=self.args.replication_factor,\n                ),\n            )\n            topic_data = self.zk.get_topics(topic.id)[topic.id]\n            for partition in topic.partitions:\n                partition_data = topic_data['partitions'][str(partition.partition_id)]\n                isr = partition_data['isr']\n                osr_broker_ids = [b.id for b in partition.replicas if b.id not in isr]\n                if osr_broker_ids:\n                    self.log.info(\n                        \"The out of sync replica(s) {osr_broker_ids} will be \"\n                        \"prioritized for removal.\"\n                        .format(osr_broker_ids=osr_broker_ids)\n                    )\n                cluster_balancer.remove_replica(\n                    partition.name,\n                    osr_broker_ids,\n                    changes_per_partition,\n                )\n\n        # Each replica addition/removal for each partition counts for one\n        # partition movement\n        partition_movement_count = len(topic.partitions) * changes_per_partition\n\n        reduced_assignment = self.get_reduced_assignment(\n            base_assignment,\n            ct,\n            max_partition_movements=partition_movement_count,\n            max_leader_only_changes=0,\n        )\n        self.process_assignment(reduced_assignment, allow_rf_change=True)", "response": "Get executable proposed plan(if any) for display or execution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of group objects, and a function to extract the number of elements for each of them, return the list of groups that have an excessive number of elements (when compared to a uniform distribution), a list of groups with insufficient elements, and a list of groups that already have the optimal number of elements. :param list groups: list of group objects :param func key: function to retrieve the current number of elements from the group object :param int total: total number of elements to distribute Example: .. code-block:: python smart_separate_groups([11, 9, 10, 14], lambda g: g) => ([14], [10, 9], [11])", "response": "def _smart_separate_groups(groups, key, total):\n    \"\"\"Given a list of group objects, and a function to extract the number of\n    elements for each of them, return the list of groups that have an excessive\n    number of elements (when compared to a uniform distribution), a list of\n    groups with insufficient elements, and a list of groups that already have\n    the optimal number of elements.\n\n    :param list groups: list of group objects\n    :param func key: function to retrieve the current number of elements from the group object\n    :param int total: total number of elements to distribute\n\n    Example:\n        .. code-block:: python\n           smart_separate_groups([11,  9, 10, 14], lambda g: g) => ([14], [10, 9], [11])\n    \"\"\"\n    optimum, extra = compute_optimum(len(groups), total)\n    over_loaded, under_loaded, optimal = [], [], []\n    for group in sorted(groups, key=key, reverse=True):\n        n_elements = key(group)\n        additional_element = 1 if extra else 0\n        if n_elements > optimum + additional_element:\n            over_loaded.append(group)\n        elif n_elements == optimum + additional_element:\n            optimal.append(group)\n        elif n_elements < optimum + additional_element:\n            under_loaded.append(group)\n        extra -= additional_element\n    return over_loaded, under_loaded, optimal"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nseparate the group into overloaded and under - loaded groups.", "response": "def separate_groups(groups, key, total):\n    \"\"\"Separate the group into overloaded and under-loaded groups.\n\n    The revised over-loaded groups increases the choice space for future\n    selection of most suitable group based on search criteria.\n\n    For example:\n    Given the groups (a:4, b:4, c:3, d:2) where the number represents the number\n    of elements for each group.\n    smart_separate_groups sets 'a' and 'c' as optimal, 'b' as over-loaded\n    and 'd' as under-loaded.\n\n    separate-groups combines 'a' with 'b' as over-loaded, allowing to select\n    between these two groups to transfer the element to 'd'.\n\n    :param groups: list of groups\n    :param key: function to retrieve element count from group\n    :param total: total number of elements to distribute\n    :returns: sorted lists of over loaded (descending) and under\n        loaded (ascending) group\n    \"\"\"\n    optimum, extra = compute_optimum(len(groups), total)\n    over_loaded, under_loaded, optimal = _smart_separate_groups(groups, key, total)\n    # If every group is optimal return\n    if not extra:\n        return over_loaded, under_loaded\n    # Some groups in optimal may have a number of elements that is optimum + 1.\n    # In this case they should be considered over_loaded.\n    potential_under_loaded = [\n        group for group in optimal\n        if key(group) == optimum\n    ]\n    potential_over_loaded = [\n        group for group in optimal\n        if key(group) > optimum\n    ]\n    revised_under_loaded = under_loaded + potential_under_loaded\n    revised_over_loaded = over_loaded + potential_over_loaded\n    return (\n        sorted(revised_over_loaded, key=key, reverse=True),\n        sorted(revised_under_loaded, key=key),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef active_brokers(self):\n        return {\n            broker\n            for broker in self._brokers\n            if not broker.inactive and not broker.decommissioned\n        }", "response": "Return set of brokers that are not inactive or decommissioned."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_broker(self, broker):\n        if broker not in self._brokers:\n            self._brokers.add(broker)\n        else:\n            self.log.warning(\n                'Broker {broker_id} already present in '\n                'replication-group {rg_id}'.format(\n                    broker_id=broker.id,\n                    rg_id=self._id,\n                )\n            )", "response": "Add broker to current broker - list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn count of replicas of given partition.", "response": "def count_replica(self, partition):\n        \"\"\"Return count of replicas of given partition.\"\"\"\n        return sum(1 for b in partition.replicas if b in self.brokers)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire_partition(self, partition, source_broker):\n        broker_dest = self._elect_dest_broker(partition)\n        if not broker_dest:\n            raise NotEligibleGroupError(\n                \"No eligible brokers to accept partition {p}\".format(p=partition),\n            )\n        source_broker.move_partition(partition, broker_dest)", "response": "Move a partition from one of the eligible brokers\n        of the replication group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_partition(self, rg_destination, victim_partition):\n        # Select best-fit source and destination brokers for partition\n        # Best-fit is based on partition-count and presence/absence of\n        # Same topic-partition over brokers\n        broker_source, broker_destination = self._select_broker_pair(\n            rg_destination,\n            victim_partition,\n        )\n        # Actual-movement of victim-partition\n        self.log.debug(\n            'Moving partition {p_name} from broker {broker_source} to '\n            'replication-group:broker {rg_dest}:{dest_broker}'.format(\n                p_name=victim_partition.name,\n                broker_source=broker_source.id,\n                dest_broker=broker_destination.id,\n                rg_dest=rg_destination.id,\n            ),\n        )\n        broker_source.move_partition(victim_partition, broker_destination)", "response": "Move partition from source broker to destination broker"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect broker pair based on partitionCounts and presence of partition over the broker.", "response": "def _select_broker_pair(self, rg_destination, victim_partition):\n        \"\"\"Select best-fit source and destination brokers based on partition\n        count and presence of partition over the broker.\n\n        * Get overloaded and underloaded brokers\n        Best-fit Selection Criteria:\n        Source broker: Select broker containing the victim-partition with\n        maximum partitions.\n        Destination broker: NOT containing the victim-partition with minimum\n        partitions. If no such broker found, return first broker.\n\n        This helps in ensuring:-\n        * Topic-partitions are distributed across brokers.\n        * Partition-count is balanced across replication-groups.\n        \"\"\"\n        broker_source = self._elect_source_broker(victim_partition)\n        broker_destination = rg_destination._elect_dest_broker(victim_partition)\n        return broker_source, broker_destination"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _elect_source_broker(self, victim_partition, broker_subset=None):\n        broker_subset = broker_subset or self._brokers\n        over_loaded_brokers = sorted(\n            [\n                broker\n                for broker in broker_subset\n                if victim_partition in broker.partitions and not broker.inactive\n            ],\n            key=lambda b: len(b.partitions),\n            reverse=True,\n        )\n        if not over_loaded_brokers:\n            return None\n\n        broker_topic_partition_cnt = [\n            (broker, broker.count_partitions(victim_partition.topic))\n            for broker in over_loaded_brokers\n        ]\n        max_count_pair = max(\n            broker_topic_partition_cnt,\n            key=lambda ele: ele[1],\n        )\n        return max_count_pair[0]", "response": "Select the source broker for a given victim partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _elect_dest_broker(self, victim_partition):\n        under_loaded_brokers = sorted(\n            [\n                broker\n                for broker in self._brokers\n                if (victim_partition not in broker.partitions and\n                    not broker.inactive and\n                    not broker.decommissioned)\n            ],\n            key=lambda b: len(b.partitions)\n        )\n        if not under_loaded_brokers:\n            return None\n\n        broker_topic_partition_cnt = [\n            (broker, broker.count_partitions(victim_partition.topic))\n            for broker in under_loaded_brokers\n            if victim_partition not in broker.partitions\n        ]\n        min_count_pair = min(\n            broker_topic_partition_cnt,\n            key=lambda ele: ele[1],\n        )\n        return min_count_pair[0]", "response": "Select the destination broker for the given victim partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the target brokers based on the given brokers and the given distance.", "response": "def _get_target_brokers(self, over_loaded_brokers, under_loaded_brokers, sibling_distance):\n        \"\"\"Pick best-suitable source-broker, destination-broker and partition to\n        balance partition-count over brokers in given replication-group.\n        \"\"\"\n        # Sort given brokers to ensure determinism\n        over_loaded_brokers = sorted(\n            over_loaded_brokers,\n            key=lambda b: len(b.partitions),\n            reverse=True,\n        )\n        under_loaded_brokers = sorted(\n            under_loaded_brokers,\n            key=lambda b: len(b.partitions),\n        )\n        # pick pair of brokers from source and destination brokers with\n        # minimum same-partition-count\n        # Set result in format: (source, dest, preferred-partition)\n        target = (None, None, None)\n        min_distance = sys.maxsize\n        best_partition = None\n        for source in over_loaded_brokers:\n            for dest in under_loaded_brokers:\n                # A decommissioned broker can have less partitions than\n                # destination. We consider it a valid source because we want to\n                # move all the partitions out from it.\n                if (len(source.partitions) - len(dest.partitions) > 1 or\n                        source.decommissioned):\n                    best_partition = source.get_preferred_partition(\n                        dest,\n                        sibling_distance[dest][source],\n                    )\n                    # If no eligible partition continue with next broker.\n                    if best_partition is None:\n                        continue\n                    distance = sibling_distance[dest][source][best_partition.topic]\n                    if distance < min_distance:\n                        min_distance = distance\n                        target = (source, dest, best_partition)\n                else:\n                    # If relatively-unbalanced then all brokers in destination\n                    # will be thereafter, return from here.\n                    break\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_sibling_distance(self):\n        sibling_distance = defaultdict(lambda: defaultdict(dict))\n        topics = {p.topic for p in self.partitions}\n        for source in self.brokers:\n            for dest in self.brokers:\n                if source != dest:\n                    for topic in topics:\n                        sibling_distance[dest][source][topic] = \\\n                            dest.count_partitions(topic) - \\\n                            source.count_partitions(topic)\n        return sibling_distance", "response": "Generate a dict containing the distance computed as difference in\n        in number of partitions of each topic from under_loaded_brokers\n        to over_loaded_brokers\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_sibling_distance(self, sibling_distance, dest, topic):\n        for source in six.iterkeys(sibling_distance[dest]):\n            sibling_distance[dest][source][topic] = \\\n                dest.count_partitions(topic) - \\\n                source.count_partitions(topic)\n        return sibling_distance", "response": "Update the sibling distance for topic and destination broker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove partition to under - loaded replication - group if possible.", "response": "def move_partition_replica(self, under_loaded_rg, eligible_partition):\n        \"\"\"Move partition to under-loaded replication-group if possible.\"\"\"\n        # Evaluate possible source and destination-broker\n        source_broker, dest_broker = self._get_eligible_broker_pair(\n            under_loaded_rg,\n            eligible_partition,\n        )\n        if source_broker and dest_broker:\n            self.log.debug(\n                'Moving partition {p_name} from broker {source_broker} to '\n                'replication-group:broker {rg_dest}:{dest_broker}'.format(\n                    p_name=eligible_partition.name,\n                    source_broker=source_broker.id,\n                    dest_broker=dest_broker.id,\n                    rg_dest=under_loaded_rg.id,\n                ),\n            )\n            # Move partition if eligible brokers found\n            source_broker.move_partition(eligible_partition, dest_broker)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_eligible_broker_pair(self, under_loaded_rg, eligible_partition):\n        under_brokers = list(filter(\n            lambda b: eligible_partition not in b.partitions,\n            under_loaded_rg.brokers,\n        ))\n        over_brokers = list(filter(\n            lambda b: eligible_partition in b.partitions,\n            self.brokers,\n        ))\n\n        # Get source and destination broker\n        source_broker, dest_broker = None, None\n        if over_brokers:\n            source_broker = max(\n                over_brokers,\n                key=lambda broker: len(broker.partitions),\n            )\n        if under_brokers:\n            dest_broker = min(\n                under_brokers,\n                key=lambda broker: len(broker.partitions),\n            )\n        return (source_broker, dest_broker)", "response": "Evaluate and return source and destination broker - pair from over - loaded\n            and under - loaded replication - group and eligible_partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_result(res):\n    if not isinstance(res, dict):\n        raise ValueError('Value should be of dict type')\n\n    result = set([])\n\n    for _, v in res.items():\n        for value in v:\n            result.add(value)\n\n    return list(result)", "response": "Merge all items in res into a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the first key in a command dictionary.", "response": "def first_key(res):\n    \"\"\"\n    Returns the first result for the given command.\n\n    If more then 1 result is returned then a `RedisClusterException` is raised.\n    \"\"\"\n    if not isinstance(res, dict):\n        raise ValueError('Value should be of dict type')\n\n    if len(res.keys()) != 1:\n        raise RedisClusterException(\"More then 1 result from command\")\n\n    return list(res.values())[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clusterdown_wrapper(func):\n\n    @wraps(func)\n    async def inner(*args, **kwargs):\n        for _ in range(0, 3):\n            try:\n                return await func(*args, **kwargs)\n            except ClusterDownError:\n                # Try again with the new cluster setup. All other errors\n                # should be raised.\n                pass\n\n        # If it fails 3 times then raise exception back to caller\n        raise ClusterDownError(\"CLUSTERDOWN error. Unable to rebuild the cluster\")\n\n    return inner", "response": "A wrapper for the CLUSTERDOWN error handling."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the results of Redis s DEBUG OBJECT command into a Python dict", "response": "def parse_debug_object(response):\n    \"Parse the results of Redis's DEBUG OBJECT command into a Python dict\"\n    # The 'type' of the object is the first item in the response, but isn't\n    # prefixed with a name\n    response = nativestr(response)\n    response = 'type:' + response\n    response = dict([kv.split(':') for kv in response.split()])\n\n    # parse some expected int values from the string response\n    # note: this cmd isn't spec'd so these may not appear in all redis versions\n    int_fields = ('refcount', 'serializedlength', 'lru', 'lru_seconds_idle')\n    for field in int_fields:\n        if field in response:\n            response[field] = int(response[field])\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_info(response):\n    \"Parse the result of Redis's INFO command into a Python dict\"\n    info = {}\n    response = nativestr(response)\n\n    def get_value(value):\n        if ',' not in value or '=' not in value:\n            try:\n                if '.' in value:\n                    return float(value)\n                else:\n                    return int(value)\n            except ValueError:\n                return value\n        else:\n            sub_dict = {}\n            for item in value.split(','):\n                k, v = item.rsplit('=', 1)\n                sub_dict[k] = get_value(v)\n            return sub_dict\n\n    for line in response.splitlines():\n        if line and not line.startswith('#'):\n            if line.find(':') != -1:\n                key, value = line.split(':', 1)\n                info[key] = get_value(value)\n            else:\n                # if the line isn't splittable, append it to the \"__raw__\" key\n                info.setdefault('__raw__', []).append(line)\n\n    return info", "response": "Parse the result of Redis s INFO command into a Python dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def slaveof(self, host=None, port=None):\n        if host is None and port is None:\n            return await self.execute_command('SLAVEOF', b('NO'), b('ONE'))\n        return await self.execute_command('SLAVEOF', host, port)", "response": "Set the server to be a replicated slave of the instance identified by the host and port."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the most recent items from the slowlog.", "response": "async def slowlog_get(self, num=None):\n        \"\"\"\n        Get the entries from the slowlog. If ``num`` is specified, get the\n        most recent ``num`` items.\n        \"\"\"\n        args = ['SLOWLOG GET']\n        if num is not None:\n            args.append(num)\n        return await self.execute_command(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a cache object using default identity generator serializer and compressor.", "response": "def cache(self, name, cache_class=Cache,\n              identity_generator_class=IdentityGenerator,\n              compressor_class=Compressor,\n              serializer_class=Serializer, *args, **kwargs):\n        \"\"\"\n        Return a cache object using default identity generator,\n        serializer and compressor.\n\n        ``name`` is used to identify the series of your cache\n        ``cache_class`` Cache is for normal use and HerdCache\n        is used in case of Thundering Herd Problem\n        ``identity_generator_class`` is the class used to generate\n        the real unique key in cache, can be overwritten to\n        meet your special needs. It should provide `generate` API\n        ``compressor_class`` is the class used to compress cache in redis,\n        can be overwritten with API `compress` and `decompress` retained.\n        ``serializer_class`` is the class used to serialize\n        content before compress, can be overwritten with API\n        `serialize` and `deserialize` retained.\n        \"\"\"\n        return cache_class(self, app=name,\n                           identity_generator_class=identity_generator_class,\n                           compressor_class=compressor_class,\n                           serializer_class=serializer_class,\n                           *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lock(self, name, timeout=None, sleep=0.1, blocking_timeout=None,\n             lock_class=None, thread_local=True):\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.        \"\"\"\n        if lock_class is None:\n            if self._use_lua_lock is None:\n                # the first time .lock() is called, determine if we can use\n                # Lua by attempting to register the necessary scripts\n                try:\n                    LuaLock.register_scripts(self)\n                    self._use_lua_lock = True\n                except ResponseError:\n                    self._use_lua_lock = False\n            lock_class = self._use_lua_lock and LuaLock or Lock\n        return lock_class(self, name, timeout=timeout, sleep=sleep,\n                          blocking_timeout=blocking_timeout,\n                          thread_local=thread_local)", "response": "This method returns a new Lock object with the specified key name and timeout and sleep."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def hincrby(self, name, key, amount=1):\n        \"Increment the value of ``key`` in hash ``name`` by ``amount``\"\n        return await self.execute_command('HINCRBY', name, key, amount)", "response": "Increment the value of key in hash name by amount"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def hincrbyfloat(self, name, key, amount=1.0):\n        return await self.execute_command('HINCRBYFLOAT', name, key, amount)", "response": "Increment the value of key in hash name by floating amount."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def hset(self, name, key, value):\n        return await self.execute_command('HSET', name, key, value)", "response": "Set the value of key to value within hash name. Returns 0 if the key does not exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the value of key to value within hash name. Returns 1 if the key does not exist. Returns 0 if the key does not exist.", "response": "async def hsetnx(self, name, key, value):\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name`` if ``key`` does not\n        exist.  Returns 1 if HSETNX created a field, otherwise 0.\n        \"\"\"\n        return await self.execute_command('HSETNX', name, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the value of each corresponding key to value within hash name.", "response": "async def hmset(self, name, mapping):\n        \"\"\"\n        Set key to value within hash ``name`` for each corresponding\n        key and value from the ``mapping`` dict.\n        \"\"\"\n        if not mapping:\n            raise DataError(\"'hmset' with 'mapping' of length 0\")\n        items = []\n        for pair in iteritems(mapping):\n            items.extend(pair)\n        return await self.execute_command('HMSET', name, *items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def transaction(self, func, *watches, **kwargs):\n        shard_hint = kwargs.pop('shard_hint', None)\n        value_from_callable = kwargs.pop('value_from_callable', False)\n        watch_delay = kwargs.pop('watch_delay', None)\n        async with await self.pipeline(True, shard_hint) as pipe:\n            while True:\n                try:\n                    if watches:\n                        await pipe.watch(*watches)\n                    func_value = await func(pipe)\n                    exec_value = await pipe.execute()\n                    return func_value if value_from_callable else exec_value\n                except WatchError:\n                    if watch_delay is not None and watch_delay > 0:\n                        await asyncio.sleep(\n                            watch_delay,\n                            loop=self.connection_pool.loop\n                        )\n                    continue", "response": "Execute a callable func while watching all keys specified in watches."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a bytestring representation of the value", "response": "def encode(self, value):\n        \"\"\"Return a bytestring representation of the value\"\"\"\n        if isinstance(value, bytes):\n            return value\n        elif isinstance(value, int):\n            value = b(str(value))\n        elif isinstance(value, float):\n            value = b(repr(value))\n        elif not isinstance(value, str):\n            value = str(value)\n        if isinstance(value, str):\n            value = value.encode()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the slots cache by asking all startup nodes and then checking if there are any nodes that are still connected to the master.", "response": "async def initialize(self):\n        \"\"\"\n        Init the slots cache by asking all startup nodes what the current cluster configuration is\n\n        TODO: Currently the last node will have the last say about how the configuration is setup.\n        Maybe it should stop to try after it have correctly covered all slots or when one node is reached\n        and it could execute CLUSTER SLOTS command.\n        \"\"\"\n        nodes_cache = {}\n        tmp_slots = {}\n\n        all_slots_covered = False\n        disagreements = []\n        startup_nodes_reachable = False\n\n        nodes = self.orig_startup_nodes\n\n        # With this option the client will attempt to connect to any of the previous set of nodes instead of the original set of nodes\n        if self.nodemanager_follow_cluster:\n            nodes = self.startup_nodes\n\n        for node in nodes:\n            try:\n                r = self.get_redis_link(host=node['host'], port=node['port'])\n                cluster_slots = await r.cluster_slots()\n                startup_nodes_reachable = True\n            except ConnectionError:\n                continue\n            except Exception:\n                raise RedisClusterException('ERROR sending \"cluster slots\" command to redis server: {0}'.format(node))\n\n            all_slots_covered = True\n\n            # If there's only one server in the cluster, its ``host`` is ''\n            # Fix it to the host in startup_nodes\n            if len(cluster_slots) == 1 and len(self.startup_nodes) == 1:\n                single_node_slots = cluster_slots.get((0, self.RedisClusterHashSlots - 1))[0]\n                if len(single_node_slots['host']) == 0:\n                    single_node_slots['host'] = self.startup_nodes[0]['host']\n                    single_node_slots['server_type'] = 'master'\n\n            # No need to decode response because StrictRedis should handle that for us...\n            for min_slot, max_slot in cluster_slots:\n                nodes = cluster_slots.get((min_slot, max_slot))\n                master_node, slave_nodes = nodes[0], nodes[1:]\n\n                if master_node['host'] == '':\n                    master_node['host'] = node['host']\n                self.set_node_name(master_node)\n                nodes_cache[master_node['name']] = master_node\n\n                for i in range(min_slot, max_slot + 1):\n                    if i not in tmp_slots:\n                        tmp_slots[i] = [master_node]\n\n                        for slave_node in slave_nodes:\n                            self.set_node_name(slave_node)\n                            nodes_cache[slave_node['name']] = slave_node\n                            tmp_slots[i].append(slave_node)\n                    else:\n                        # Validate that 2 nodes want to use the same slot cache setup\n                        if tmp_slots[i][0]['name'] != node['name']:\n                            disagreements.append('{0} vs {1} on slot: {2}'.format(\n                                tmp_slots[i][0]['name'], node['name'], i),\n                            )\n\n                            if len(disagreements) > 5:\n                                raise RedisClusterException('startup_nodes could not agree on a valid slots cache. {0}'\n                                                            .format(', '.join(disagreements)))\n\n                self.populate_startup_nodes()\n                self.refresh_table_asap = False\n\n            if self._skip_full_coverage_check:\n                need_full_slots_coverage = False\n            else:\n                need_full_slots_coverage = await self.cluster_require_full_coverage(nodes_cache)\n\n            # Validate if all slots are covered or if we should try next startup node\n            for i in range(0, self.RedisClusterHashSlots):\n                if i not in tmp_slots and need_full_slots_coverage:\n                    all_slots_covered = False\n\n            if all_slots_covered:\n                # All slots are covered and application can continue to execute\n                break\n\n        if not startup_nodes_reachable:\n            raise RedisClusterException('Redis Cluster cannot be connected. '\n                                        'Please provide at least one reachable node.')\n\n        if not all_slots_covered:\n            raise RedisClusterException('Not all slots are covered after query all startup_nodes. '\n                                        '{0} of {1} covered...'.format(len(tmp_slots), self.RedisClusterHashSlots))\n\n        # Set the tmp variables to the real variables\n        self.slots = tmp_slots\n        self.nodes = nodes_cache\n        self.reinitialize_counter = 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if all nodes have cluster - require - full - coverage", "response": "async def cluster_require_full_coverage(self, nodes_cache):\n        \"\"\"\n        if exists 'cluster-require-full-coverage no' config on redis servers,\n        then even all slots are not covered, cluster still will be able to\n        respond\n        \"\"\"\n        nodes = nodes_cache or self.nodes\n\n        async def node_require_full_coverage(node):\n            r_node = self.get_redis_link(host=node['host'], port=node['port'])\n            node_config = await r_node.config_get('cluster-require-full-coverage')\n            return 'yes' in node_config.values()\n\n        # at least one node should have cluster-require-full-coverage yes\n        for node in nodes.values():\n            if await node_require_full_coverage(node):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate data for a node.", "response": "def set_node(self, host, port, server_type=None):\n        \"\"\"\n        Update data for a node.\n        \"\"\"\n        node_name = \"{0}:{1}\".format(host, port)\n        node = {\n            'host': host,\n            'port': port,\n            'name': node_name,\n            'server_type': server_type\n        }\n        self.nodes[node_name] = node\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef populate_startup_nodes(self):\n        for item in self.startup_nodes:\n            self.set_node_name(item)\n\n        for n in self.nodes.values():\n            if n not in self.startup_nodes:\n                self.startup_nodes.append(n)\n\n        # freeze it so we can set() it\n        uniq = {frozenset(node.items()) for node in self.startup_nodes}\n        # then thaw it back out into a list of dicts\n        self.startup_nodes = [dict(node) for node in uniq]", "response": "Populate the startup nodes list with the nodes that are not in the startup nodes list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new ConnectionPool configured from a given URL.", "response": "def from_url(cls, url, db=None, decode_components=False, **kwargs):\n        \"\"\"\n        Return a connection pool configured from the given URL.\n\n        For example::\n\n        redis://[:password]@localhost:6379/0\n        rediss://[:password]@localhost:6379/0\n        unix://[:password]@/path/to/socket.sock?db=0\n\n        Three URL schemes are supported:\n\n        - ```redis://``\n        <http://www.iana.org/assignments/uri-schemes/prov/redis>`_ creates a\n        normal TCP socket connection\n        - ```rediss://``\n        <http://www.iana.org/assignments/uri-schemes/prov/rediss>`_ creates a\n        SSL wrapped TCP socket connection\n        - ``unix://`` creates a Unix Domain Socket connection\n\n        There are several ways to specify a database number. The parse function\n        will return the first specified option:\n        1. A ``db`` querystring option, e.g. redis://localhost?db=0\n        2. If using the redis:// scheme, the path argument of the url, e.g.\n        redis://localhost/0\n        3. The ``db`` argument to this function.\n\n        If none of these options are specified, db=0 is used.\n\n        The ``decode_components`` argument allows this function to work with\n        percent-encoded URLs. If this argument is set to ``True`` all ``%xx``\n        escapes will be replaced by their single-character equivalents after\n        the URL has been parsed. This only applies to the ``hostname``,\n        ``path``, and ``password`` components.\n\n        Any additional querystring arguments and keyword arguments will be\n        passed along to the ConnectionPool class's initializer. The querystring\n        arguments ``connect_timeout`` and ``stream_timeout`` if supplied\n        are parsed as float values. The arguments ``retry_on_timeout`` are\n        parsed to boolean values that accept True/False, Yes/No values to indicate state.\n        Invalid types cause a ``UserWarning`` to be raised.\n        In the case of conflicting arguments, querystring arguments always win.\n        \"\"\"\n        url = urlparse(url)\n        qs = url.query\n\n        url_options = {}\n\n        for name, value in iter(parse_qs(qs).items()):\n            if value and len(value) > 0:\n                parser = URL_QUERY_ARGUMENT_PARSERS.get(name)\n                if parser:\n                    try:\n                        url_options[name] = parser(value[0])\n                    except (TypeError, ValueError):\n                        warnings.warn(UserWarning(\n                            \"Invalid value for `%s` in connection URL.\" % name\n                        ))\n                else:\n                    url_options[name] = value[0]\n\n        if decode_components:\n            password = unquote(url.password) if url.password else None\n            path = unquote(url.path) if url.path else None\n            hostname = unquote(url.hostname) if url.hostname else None\n        else:\n            password = url.password\n            path = url.path\n            hostname = url.hostname\n\n        # We only support redis:// and unix:// schemes.\n        if url.scheme == 'unix':\n            url_options.update({\n                'password': password,\n                'path': path,\n                'connection_class': UnixDomainSocketConnection,\n            })\n\n        else:\n            url_options.update({\n                'host': hostname,\n                'port': int(url.port or 6379),\n                'password': password,\n            })\n\n            # If there's a path argument, use it as the db argument if a\n            # querystring value wasn't specified\n            if 'db' not in url_options and path:\n                try:\n                    url_options['db'] = int(path.replace('/', ''))\n                except (AttributeError, ValueError):\n                    pass\n\n            if url.scheme == 'rediss':\n                keyfile = url_options.pop('ssl_keyfile', None)\n                certfile = url_options.pop('ssl_certfile', None)\n                cert_reqs = url_options.pop('ssl_cert_reqs', None)\n                ca_certs = url_options.pop('ssl_ca_certs', None)\n                url_options['ssl_context'] = RedisSSLContext(keyfile, certfile, cert_reqs, ca_certs).get()\n\n        # last shot at the db value\n        url_options['db'] = int(url_options.get('db', db or 0))\n\n        # update the arguments from the URL values\n        kwargs.update(url_options)\n        return cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_connection(self, *args, **kwargs):\n        \"Get a connection from the pool\"\n        self._checkpid()\n        try:\n            connection = self._available_connections.pop()\n        except IndexError:\n            connection = self.make_connection()\n        self._in_use_connections.add(connection)\n        return connection", "response": "Get a connection from the pool"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrelease the connection back to the pool", "response": "def release(self, connection):\n        \"Releases the connection back to the pool\"\n        self._checkpid()\n        if connection.pid != self.pid:\n            return\n        self._in_use_connections.remove(connection)\n        # discard connection with unread response\n        if connection.awaiting_response:\n            connection.disconnect()\n            self._created_connections -= 1\n        else:\n            self._available_connections.append(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the connection pool back to a clean state.", "response": "def reset(self):\n        \"\"\"\n        Resets the connection pool back to a clean state.\n        \"\"\"\n        self.pid = os.getpid()\n        self._created_connections = 0\n        self._created_connections_per_node = {}  # Dict(Node, Int)\n        self._available_connections = {}  # Dict(Node, List)\n        self._in_use_connections = {}  # Dict(Node, Set)\n        self._check_lock = threading.Lock()\n        self.initialized = False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_connection(self, node):\n        if self.count_all_num_connections(node) >= self.max_connections:\n            if self.max_connections_per_node:\n                raise RedisClusterException(\"Too many connection ({0}) for node: {1}\"\n                                            .format(self.count_all_num_connections(node),\n                                                    node['name']))\n\n            raise RedisClusterException(\"Too many connections\")\n\n        self._created_connections_per_node.setdefault(node['name'], 0)\n        self._created_connections_per_node[node['name']] += 1\n        connection = self.connection_class(host=node[\"host\"],\n                                           port=node[\"port\"],\n                                           **self.connection_kwargs)\n\n        # Must store node in the connection to make it eaiser to track\n        connection.node = node\n\n        return connection", "response": "Create a new connection for the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreleases the connection back to the pool.", "response": "def release(self, connection):\n        \"\"\"\n        Releases the connection back to the pool\n        \"\"\"\n        self._checkpid()\n        if connection.pid != self.pid:\n            return\n\n        # Remove the current connection from _in_use_connection and add it back to the available pool\n        # There is cases where the connection is to be removed but it will not exist and there\n        # must be a safe way to remove\n        i_c = self._in_use_connections.get(connection.node[\"name\"], set())\n        if connection in i_c:\n            i_c.remove(connection)\n        else:\n            pass\n        # discard connection with unread response\n        if connection.awaiting_response:\n            connection.disconnect()\n            # reduce node connection count in case of too many connection error raised\n            if self.max_connections_per_node and self._created_connections_per_node.get(connection.node['name']):\n                self._created_connections_per_node[connection.node['name']] -= 1\n        else:\n            self._available_connections.setdefault(connection.node[\"name\"], []).append(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisconnects all the available and in - use connections.", "response": "def disconnect(self):\n        \"\"\"\n        Nothing that requires any overwrite.\n        \"\"\"\n        all_conns = chain(\n            self._available_connections.values(),\n            self._in_use_connections.values(),\n        )\n\n        for node_connections in all_conns:\n            for connection in node_connections:\n                connection.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_random_connection(self):\n        if self._available_connections:\n            node_name = random.choice(list(self._available_connections.keys()))\n            conn_list = self._available_connections[node_name]\n            # check it in case of empty connection list\n            if conn_list:\n                return conn_list.pop()\n        for node in self.nodes.random_startup_node_iter():\n            connection = self.get_connection_by_node(node)\n\n            if connection:\n                return connection\n\n        raise Exception(\"Cant reach a single startup node.\")", "response": "Get a random redis connection to the redis server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a redis object that is connected to a specific slot.", "response": "def get_connection_by_slot(self, slot):\n        \"\"\"\n        Determine what server a specific slot belongs to and return a redis object that is connected\n        \"\"\"\n        self._checkpid()\n\n        try:\n            return self.get_connection_by_node(self.get_node_by_slot(slot))\n        except KeyError:\n            return self.get_random_connection()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connection_by_node(self, node):\n        self._checkpid()\n        self.nodes.set_node_name(node)\n\n        try:\n            # Try to get connection from existing pool\n            connection = self._available_connections.get(node[\"name\"], []).pop()\n        except IndexError:\n            connection = self.make_connection(node)\n\n        self._in_use_connections.setdefault(node[\"name\"], set()).add(connection)\n\n        return connection", "response": "get a connection by node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode the value so that it s identical to what we ll read off the connection", "response": "def encode(self, value):\n        \"\"\"\n        Encode the value so that it's identical to what we'll\n        read off the connection\n        \"\"\"\n        if self.decode_responses and isinstance(value, bytes):\n            value = value.decode(self.encoding)\n        elif not self.decode_responses and isinstance(value, str):\n            value = value.encode(self.encoding)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def parse_response(self, block=True, timeout=0):\n        \"Parse the response from a publish/subscribe command\"\n        connection = self.connection\n        if connection is None:\n            raise RuntimeError(\n                'pubsub connection not set: '\n                'did you forget to call subscribe() or psubscribe()?')\n        coro = self._execute(connection, connection.read_response)\n        if not block and timeout > 0:\n            try:\n                return await asyncio.wait_for(coro, timeout)\n            except Exception:\n                return None\n        return await coro", "response": "Parse the response from a publish / subscribe command"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubscribe to channel patterns.", "response": "async def psubscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_patterns = {}\n        new_patterns.update(dict.fromkeys(map(self.encode, args)))\n        for pattern, handler in iteritems(kwargs):\n            new_patterns[self.encode(pattern)] = handler\n        ret_val = await self.execute_command('PSUBSCRIBE', *iterkeys(new_patterns))\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        self.patterns.update(new_patterns)\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsubscribing to channels. Channels supplied as keyword arguments expect a channel name as the key and a callable as the value.", "response": "async def subscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value. A channel's\n        callable will be invoked automatically when a message is received on\n        that channel rather than producing a message via ``listen()`` or\n        ``get_message()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_channels = {}\n        new_channels.update(dict.fromkeys(map(self.encode, args)))\n        for channel, handler in iteritems(kwargs):\n            new_channels[self.encode(channel)] = handler\n        ret_val = await self.execute_command('SUBSCRIBE', *iterkeys(new_channels))\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        self.channels.update(new_channels)\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlistens for messages on channels this client has been subscribed to", "response": "async def listen(self):\n        \"Listen for messages on channels this client has been subscribed to\"\n        if self.subscribed:\n            return self.handle_message(await self.parse_response(block=True))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_message(self, ignore_subscribe_messages=False, timeout=0):\n        response = await self.parse_response(block=False, timeout=timeout)\n        if response:\n            return self.handle_message(response, ignore_subscribe_messages)\n        return None", "response": "Get the next available message from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_message(self, response, ignore_subscribe_messages=False):\n        message_type = nativestr(response[0])\n        if message_type == 'pmessage':\n            message = {\n                'type': message_type,\n                'pattern': response[1],\n                'channel': response[2],\n                'data': response[3]\n            }\n        else:\n            message = {\n                'type': message_type,\n                'pattern': None,\n                'channel': response[1],\n                'data': response[2]\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            subscribed_dict = None\n            if message_type == 'punsubscribe':\n                subscribed_dict = self.patterns\n            else:\n                subscribed_dict = self.channels\n            try:\n                del subscribed_dict[message['channel']]\n            except KeyError:\n                pass\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            handler = None\n            if message_type == 'pmessage':\n                handler = self.patterns.get(message['pattern'], None)\n            else:\n                handler = self.channels.get(message['channel'], None)\n            if handler:\n                handler(message)\n                return None\n        else:\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message", "response": "Parses a pub / sub message and returns a parsed version of the message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def execute_command(self, *args, **kwargs):\n        # NOTE: don't parse the response in this function -- it could pull a\n        # legitimate message off the stack if the connection is already\n        # subscribed to one or more channels\n        await self.connection_pool.initialize()\n\n        if self.connection is None:\n            self.connection = self.connection_pool.get_connection(\n                'pubsub',\n                channel=args[1],\n            )\n            # register a callback that re-subscribes to any channels we\n            # were listening to when we were disconnected\n            self.connection.register_connect_callback(self.on_connect)\n        connection = self.connection\n        await self._execute(connection, connection.send_command, *args)", "response": "Execute a publish or subscribe command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gen_identity(self, key, param=None):\n        if self.identity_generator and param is not None:\n            if self.serializer:\n                param = self.serializer.serialize(param)\n            if self.compressor:\n                param = self.compressor.compress(param)\n            identity = self.identity_generator.generate(key, param)\n        else:\n            identity = key\n        return identity", "response": "generate identity according to key and param given"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npack the content using serializer and compressor", "response": "def _pack(self, content):\n        \"\"\"pack the content using serializer and compressor\"\"\"\n        if self.serializer:\n            content = self.serializer.serialize(content)\n        if self.compressor:\n            content = self.compressor.compress(content)\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _unpack(self, content):\n        if self.compressor:\n            try:\n                content = self.compressor.decompress(content)\n            except CompressError:\n                pass\n        if self.serializer:\n            content = self.serializer.deserialize(content)\n        return content", "response": "unpack cache using serializer and compressor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def delete(self, key, param=None):\n        identity = self._gen_identity(key, param)\n        return await self.client.delete(identity)", "response": "delete cache corresponding to identity\n        generated from key and param"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def delete_pattern(self, pattern, count=None):\n        cursor = '0'\n        count_deleted = 0\n        while cursor != 0:\n            cursor, identities = await self.client.scan(\n                cursor=cursor, match=pattern, count=count\n            )\n            count_deleted += await self.client.delete(*identities)\n        return count_deleted", "response": "delete cache according to pattern in redis"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseeing if specific identity exists", "response": "async def exist(self, key, param=None):\n        \"\"\"see if specific identity exists\"\"\"\n        identity = self._gen_identity(key, param)\n        return await self.client.exists(identity)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting time to live of a specific identity", "response": "async def ttl(self, key, param=None):\n        \"\"\"get time to live of a specific identity\"\"\"\n        identity = self._gen_identity(key, param)\n        return await self.client.ttl(identity)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def set(self, key, value, param=None, expire_time=None, herd_timeout=None):\n        identity = self._gen_identity(key, param)\n        expected_expired_ts = int(time.time())\n        if expire_time:\n            expected_expired_ts += expire_time\n        expected_expired_ts += herd_timeout or self.default_herd_timeout\n        value = self._pack([value, expected_expired_ts])\n        return await self.client.set(identity, value, ex=expire_time)", "response": "Set the content of a key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a cache entry from Herd.", "response": "async def get(self, key, param=None, extend_herd_timeout=None):\n        \"\"\"\n        Use key or identity generate from key and param to\n        get cached content and expire time.\n        Compare expire time with time.now(), return None and\n        set cache with extended timeout if cache is expired,\n        else, return unpacked content\n        \"\"\"\n        identity = self._gen_identity(key, param)\n        res = await self.client.get(identity)\n        if res:\n            res, timeout = self._unpack(res)\n            now = int(time.time())\n            if timeout <= now:\n                extend_timeout = extend_herd_timeout or self.extend_herd_timeout\n                expected_expired_ts = now + extend_timeout\n                value = self._pack([res, expected_expired_ts])\n                await self.client.set(identity, value, extend_timeout)\n                return None\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the PubSub channels returned by the cluster_pubsub_channels command.", "response": "def parse_cluster_pubsub_channels(res, **options):\n    \"\"\"\n    Result callback, handles different return types\n    switchable by the `aggregate` flag.\n    \"\"\"\n    aggregate = options.get('aggregate', True)\n    if not aggregate:\n        return res\n    return merge_result(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_cluster_pubsub_numpat(res, **options):\n    aggregate = options.get('aggregate', True)\n    if not aggregate:\n        return res\n\n    numpat = 0\n    for node, node_numpat in res.items():\n        numpat += node_numpat\n    return numpat", "response": "Parse the PubSub numpat of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresulting callback handles different return types switchable by the aggregate flag.", "response": "def parse_cluster_pubsub_numsub(res, **options):\n    \"\"\"\n    Result callback, handles different return types\n    switchable by the `aggregate` flag.\n    \"\"\"\n    aggregate = options.get('aggregate', True)\n    if not aggregate:\n        return res\n\n    numsub_d = dict()\n    for _, numsub_tups in res.items():\n        for channel, numsubbed in numsub_tups:\n            try:\n                numsub_d[channel] += numsubbed\n            except KeyError:\n                numsub_d[channel] = numsubbed\n\n    ret_numsub = []\n    for channel, numsub in numsub_d.items():\n        ret_numsub.append((channel, numsub))\n    return ret_numsub"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new entry to the specified key in a stream.", "response": "async def xadd(self, name: str, entry: dict,\n                   max_len=None, stream_id='*',\n                   approximate=True) -> str:\n        \"\"\"\n        Appends the specified stream entry to the stream at the specified key.\n        If the key does not exist, as a side effect of running\n        this command the key is created with a stream value.\n        Available since 5.0.0.\n        Time complexity: O(log(N)) with N being the number of items already into the stream.\n\n        :param name: name of the stream\n        :param entry: key-values to be appended to the stream\n        :param max_len: max length of the stream\n        length will not be limited max_len is set to None\n        notice: max_len should be int greater than 0,\n        if set to 0 or negative, the stream length will not be limited\n\n        :param stream_id: id of the options appended to the stream.\n        The XADD command will auto-generate a unique id for you\n        if the id argument specified is the * character.\n        ID are specified by two numbers separated by a \"-\" character\n\n        :param approximate: whether redis will limit\n        the stream with given max length exactly, if set to True,\n        there will be a few tens of entries more,\n        but never less than 1000 items\n\n        :return: id auto generated or the specified id given.\n        notice: specified id without \"-\" character will be completed like \"id-0\"\n        \"\"\"\n        pieces = []\n        if max_len is not None:\n            if not isinstance(max_len, int) or max_len < 1:\n                raise RedisError(\"XADD maxlen must be a positive integer\")\n            pieces.append('MAXLEN')\n            if approximate:\n                pieces.append('~')\n            pieces.append(str(max_len))\n        pieces.append(stream_id)\n        for kv in entry.items():\n            pieces.extend(list(kv))\n        return await self.execute_command('XADD', name, *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def xrange(self, name: str, start='-', end='+', count=None) -> list:\n\n        pieces = [start, end]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise RedisError(\"XRANGE count must be a positive integer\")\n            pieces.append(\"COUNT\")\n            pieces.append(str(count))\n        return await self.execute_command('XRANGE', name, *pieces)", "response": "Read stream values within an interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading data from one or more streams and return the entries in that stream.", "response": "async def xread(self, count=None, block=None, **streams) -> dict:\n        \"\"\"\n        Available since 5.0.0.\n\n        Time complexity:\n        For each stream mentioned: O(log(N)+M) with N being the number\n        of elements in the stream and M the number of elements being returned.\n        If M is constant (e.g. always asking for the first 10 elements with COUNT),\n        you can consider it O(log(N)). On the other side, XADD will pay the O(N)\n        time in order to serve the N clients blocked on the stream getting new data.\n\n        Read data from one or multiple streams,\n        only returning entries with an ID greater\n        than the last received ID reported by the caller.\n\n        :param count: int, if set, only return this many items, beginning with the\n               earliest available.\n        :param block: int, milliseconds we want to block before timing out,\n                if the BLOCK option is not used, the command is synchronous\n        :param streams: stream_name - stream_id mapping\n        :return dict like {stream_name: [(stream_id: entry), ...]}\n        \"\"\"\n        pieces = []\n        if block is not None:\n            if not isinstance(block, int) or block < 1:\n                raise RedisError(\"XREAD block must be a positive integer\")\n            pieces.append(\"BLOCK\")\n            pieces.append(str(block))\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise RedisError(\"XREAD count must be a positive integer\")\n            pieces.append(\"COUNT\")\n            pieces.append(str(count))\n        pieces.append(\"STREAMS\")\n        ids = []\n        for partial_stream in streams.items():\n            pieces.append(partial_stream[0])\n            ids.append(partial_stream[1])\n        pieces.extend(ids)\n        return await self.execute_command('XREAD', *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from a consumer group.", "response": "async def xreadgroup(self, group: str, consumer_id: str,\n                         count=None, block=None, **streams):\n        \"\"\"\n        Available since 5.0.0.\n\n        Time complexity:\n        For each stream mentioned: O(log(N)+M) with N being the number of elements\n        in the stream and M the number of elements being returned.\n        If M is constant (e.g. always asking for the first 10 elements with COUNT),\n        you can consider it O(log(N)). On the other side,\n        XADD will pay the O(N) time in order to serve\n        the N clients blocked on the stream getting new data.\n\n        Read data from one or multiple streams via the consumer group,\n        only returning entries with an ID greater\n        than the last received ID reported by the caller.\n\n        :param group: the name of the consumer group\n        :param consumer_id: the name of the consumer that is attempting to read\n        :param count: int, if set, only return this many items, beginning with the\n               earliest available.\n        :param block: int, milliseconds we want to block before timing out,\n                if the BLOCK option is not used, the command is synchronous\n        :param streams: stream_name - stream_id mapping\n        :return dict like {stream_name: [(stream_id: entry), ...]}\n        \"\"\"\n        pieces = ['GROUP', group, consumer_id]\n        if block is not None:\n            if not isinstance(block, int) or block < 1:\n                raise RedisError(\"XREAD block must be a positive integer\")\n            pieces.append(\"BLOCK\")\n            pieces.append(str(block))\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise RedisError(\"XREAD count must be a positive integer\")\n            pieces.append(\"COUNT\")\n            pieces.append(str(count))\n        pieces.append(\"STREAMS\")\n        ids = []\n        for partial_stream in streams.items():\n            pieces.append(partial_stream[0])\n            ids.append(partial_stream[1])\n        pieces.extend(ids)\n        return await self.execute_command('XREADGROUP', *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the pending entries of a given consumer group.", "response": "async def xpending(self, name: str, group: str,\n                       start='-', end='+', count=None, consumer=None) -> list:\n        \"\"\"\n        Available since 5.0.0.\n\n        Time complexity:\n        O(log(N)+M) with N being the number of elements in the consumer\n        group pending entries list, and M the number of elements being returned.\n        When the command returns just the summary it runs in O(1)\n        time assuming the list of consumers is small,\n        otherwise there is additional O(N) time needed to iterate every consumer.\n\n        Fetching data from a stream via a consumer group,\n        and not acknowledging such data,\n        has the effect of creating pending entries.\n        The XPENDING command is the interface to inspect the list of pending messages.\n\n        :param name: name of the stream\n        :param group: name of the consumer group\n        :param start: first stream ID. defaults to '-',\n               meaning the earliest available.\n        :param end: last stream ID. defaults to '+',\n                meaning the latest available.\n        :param count: int, number of entries\n                [NOTICE] only when count is set to int,\n                start & end options will have effect\n                and detail of pending entries will be returned\n        :param consumer: str, consumer of the stream in the group\n                [NOTICE] only when count is set to int,\n                this option can be appended to\n                query pending entries of given consumer\n        \"\"\"\n        pieces = [name, group]\n        if count is not None:\n            pieces.extend([start, end, count])\n            if consumer is not None:\n                pieces.append(str(consumer))\n        # todo: may there be a parse function\n        return await self.execute_command('XPENDING', *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntrims the set of entries in the given key from the given length.", "response": "async def xtrim(self, name: str, max_len: int, approximate=True) -> int:\n        \"\"\"\n        [NOTICE] Not officially released yet\n\n        XTRIM is designed to accept different trimming strategies,\n        even if currently only MAXLEN is implemented.\n\n        :param name: name of the stream\n        :param max_len: max length of the stream after being trimmed\n        :param approximate: whether redis will limit\n        the stream with given max length exactly, if set to True,\n        there will be a few tens of entries more,\n        but never less than 1000 items:\n\n        :return: number of entries trimmed\n        \"\"\"\n        pieces = ['MAXLEN']\n        if approximate:\n            pieces.append('~')\n        pieces.append(max_len)\n        return await self.execute_command('XTRIM', name, *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving items from the given stream.", "response": "async def xdel(self, name: str, stream_id: str) -> int:\n        \"\"\"\n        [NOTICE] Not officially released yet\n        [NOTICE] In the current implementation, memory is not\n        really reclaimed until a macro node is completely empty,\n        so you should not abuse this feature.\n\n        remove items from the middle of a stream, just by ID.\n\n        :param name: name of the stream\n        :param stream_id: id of the options appended to the stream.\n        \"\"\"\n        return await self.execute_command('XDEL', name, stream_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting information about the consumers of a given stream and consumer group.", "response": "async def xinfo_consumers(self, name: str, group: str) -> list:\n        \"\"\"\n        [NOTICE] Not officially released yet\n\n        XINFO command is an observability interface that can be used\n        with sub-commands in order to get information\n        about streams or consumer groups.\n\n        :param name: name of the stream\n        :param group: name of the consumer group\n        \"\"\"\n        return await self.execute_command('XINFO CONSUMERS', name, group)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new consumer group.", "response": "async def xgroup_create(self, name: str, group: str, stream_id='$') -> bool:\n        \"\"\"\n        [NOTICE] Not officially released yet\n        XGROUP is used in order to create, destroy and manage consumer groups.\n        :param name: name of the stream\n        :param group: name of the consumer group\n        :param stream_id:\n            If we provide $ as we did, then only new messages arriving\n            in the stream from now on will be provided to the consumers in the group.\n            If we specify 0 instead the consumer group will consume all the messages\n            in the stream history to start with.\n            Of course, you can specify any other valid ID\n        \"\"\"\n        return await self.execute_command('XGROUP CREATE', name, group, stream_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def xgroup_set_id(self, name: str, group: str, stream_id: str) -> bool:\n        return await self.execute_command('XGROUP SETID', name, group, stream_id)", "response": "Set the ID of the current consumer group to the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndestroys the consumer group with the given name and group.", "response": "async def xgroup_destroy(self, name: str, group: str) -> int:\n        \"\"\"\n        [NOTICE] Not officially released yet\n        XGROUP is used in order to create, destroy and manage consumer groups.\n        :param name: name of the stream\n        :param group: name of the consumer group\n        \"\"\"\n        return await self.execute_command('XGROUP DESTROY', name, group)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a consumer from a group", "response": "async def xgroup_del_consumer(self, name: str, group: str, consumer: str) -> int:\n        \"\"\"\n        [NOTICE] Not officially released yet\n        XGROUP is used in order to create, destroy and manage consumer groups.\n        :param name: name of the stream\n        :param group: name of the consumer group\n        :param consumer: name of the consumer\n        \"\"\"\n        return await self.execute_command('XGROUP DELCONSUMER', name, group, consumer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def brpoplpush(self, src, dst, timeout=0):\n        if timeout is None:\n            timeout = 0\n        return await self.execute_command('BRPOPLPUSH', src, dst, timeout)", "response": "Pops a value off the tail of src and pushes it on the head of dst."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def linsert(self, name, where, refvalue, value):\n        return await self.execute_command('LINSERT', name, where, refvalue, value)", "response": "Insert value in list name immediately before or after refvalue. Returns the new length on success or - 1 on failure."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def ltrim(self, name, start, end):\n        return await self.execute_command('LTRIM', name, start, end)", "response": "Trim the list name from start to end."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopping a value off the tail of src and puts it on the head of dst.", "response": "async def brpoplpush(self, src, dst, timeout=0):\n        \"\"\"\n        Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n\n        Cluster impl:\n            Call brpop() then send the result into lpush()\n\n            Operation is no longer atomic.\n        \"\"\"\n        try:\n            value = await self.brpop(src, timeout=timeout)\n            if value is None:\n                return None\n        except TimeoutError:\n            # Timeout was reached\n            return None\n\n        await self.lpush(dst, value[1])\n        return value[1]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def rpoplpush(self, src, dst):\n        value = await self.rpop(src)\n\n        if value:\n            await self.lpush(dst, value)\n            return value\n\n        return None", "response": "Atomically RPOP a value off of the src list and atomically LPUSH it on to the dst list. Returns None if there is no such value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts and return the list set or sorted set at name.", "response": "async def sort(self, name, start=None, num=None, by=None, get=None, desc=False, alpha=False, store=None, groups=None):\n        \"\"\"Sort and return the list, set or sorted set at ``name``.\n\n        :start: and :num:\n            allow for paging through the sorted data\n\n        :by:\n            allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        :get:\n            allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where int he key\n            the item value is located\n\n        :desc:\n            allows for reversing the sort\n\n        :alpha:\n            allows for sorting lexicographically rather than numerically\n\n        :store:\n            allows for storing the result of the sort into the key `store`\n\n        ClusterImpl:\n            A full implementation of the server side sort mechanics because many of the\n            options work on multiple keys that can exist on multiple servers.\n        \"\"\"\n        if (start is None and num is not None) or \\\n                (start is not None and num is None):\n            raise RedisError(\"RedisError: ``start`` and ``num`` must both be specified\")\n        try:\n            data_type = b(await self.type(name))\n\n            if data_type == b(\"none\"):\n                return []\n            elif data_type == b(\"set\"):\n                data = list(await self.smembers(name))[:]\n            elif data_type == b(\"list\"):\n                data = await self.lrange(name, 0, -1)\n            else:\n                raise RedisClusterException(\"Unable to sort data type : {0}\".format(data_type))\n            if by is not None:\n                # _sort_using_by_arg mutates data so we don't\n                # need need a return value.\n                data = await self._sort_using_by_arg(data, by, alpha)\n            elif not alpha:\n                data.sort(key=self._strtod_key_func)\n            else:\n                data.sort()\n            if desc:\n                data = data[::-1]\n            if not (start is None and num is None):\n                data = data[start:start + num]\n\n            if get:\n                data = await self._retrive_data_from_sort(data, get)\n\n            if store is not None:\n                if data_type == b(\"set\"):\n                    await self.delete(store)\n                    await self.rpush(store, *data)\n                elif data_type == b(\"list\"):\n                    await self.delete(store)\n                    await self.rpush(store, *data)\n                else:\n                    raise RedisClusterException(\"Unable to store sorted data for data type : {0}\".format(data_type))\n\n                return len(data)\n\n            if groups:\n                if not get or isinstance(get, str) or len(get) < 2:\n                    raise DataError('when using \"groups\" the \"get\" argument '\n                                    'must be specified and contain at least '\n                                    'two keys')\n                n = len(get)\n                return list(zip(*[data[i::n] for i in range(n)]))\n            else:\n                return data\n        except KeyError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _get_single_item(self, k, g):\n        if getattr(k, \"decode\", None):\n            k = k.decode(\"utf-8\")\n\n        if '*' in g:\n            g = g.replace('*', k)\n            if '->' in g:\n                key, hash_key = g.split('->')\n                single_item = await self.get(key, {}).get(hash_key)\n            else:\n                single_item = await self.get(g)\n        elif '#' in g:\n            single_item = k\n        else:\n            single_item = None\n        return b(single_item)", "response": "Get a single item from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint error because some pipelined commands should be blocked when running in cluster-mode", "response": "def block_pipeline_command(func):\n    \"\"\"\n    Prints error because some pipelined commands should be blocked when running in cluster-mode\n    \"\"\"\n\n    def inner(*args, **kwargs):\n        raise RedisClusterException(\n            \"ERROR: Calling pipelined function {0} is blocked when running redis in cluster mode...\".format(\n                func.__name__))\n\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a command immediately and return the result.", "response": "async def immediate_execute_command(self, *args, **options):\n        \"\"\"\n        Execute a command immediately, but don't auto-retry on a\n        ConnectionError if we're already WATCHing a variable. Used when\n        issuing WATCH or subsequent commands retrieving their values but before\n        MULTI is called.\n        \"\"\"\n        command_name = args[0]\n        conn = self.connection\n        # if this is the first call, we need a connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            self.connection = conn\n        try:\n            await conn.send_command(*args)\n            return await self.parse_response(conn, command_name, **options)\n        except (ConnectionError, TimeoutError) as e:\n            conn.disconnect()\n            if not conn.retry_on_timeout and isinstance(e, TimeoutError):\n                raise\n            # if we're not already watching, we can safely retry the command\n            try:\n                if not self.watching:\n                    await conn.send_command(*args)\n                    return await self.parse_response(conn, command_name, **options)\n            except ConnectionError:\n                # the retry failed so cleanup.\n                conn.disconnect()\n                await self.reset()\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def execute(self, raise_on_error=True):\n        \"Execute all the commands in the current pipeline\"\n        stack = self.command_stack\n        if not stack:\n            return []\n        if self.scripts:\n            await self.load_scripts()\n        if self.transaction or self.explicit_transaction:\n            exec = self._execute_transaction\n        else:\n            exec = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n\n        try:\n            return await exec(conn, stack, raise_on_error)\n        except (ConnectionError, TimeoutError) as e:\n            conn.disconnect()\n            if not conn.retry_on_timeout and isinstance(e, TimeoutError):\n                raise\n            # if we were watching a variable, the watch is no longer valid\n            # since this connection has died. raise a WatchError, which\n            # indicates the user should retry his transaction. If this is more\n            # than a temporary failure, the WATCH that the user next issues\n            # will fail, propegating the real ConnectionError\n            if self.watching:\n                raise WatchError(\"A ConnectionError occured on while watching \"\n                                 \"one or more keys\")\n            # otherwise, it's safe to retry since the transaction isn't\n            # predicated on any state\n            return await exec(conn, stack, raise_on_error)\n        finally:\n            await self.reset()", "response": "Execute all the commands in the current pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _determine_slot(self, *args):\n        if len(args) <= 1:\n            raise RedisClusterException(\"No way to dispatch this command to Redis Cluster. Missing key.\")\n        command = args[0]\n\n        if command in ['EVAL', 'EVALSHA']:\n            numkeys = args[2]\n            keys = args[3: 3 + numkeys]\n            slots = {self.connection_pool.nodes.keyslot(key) for key in keys}\n            if len(slots) != 1:\n                raise RedisClusterException(\"{0} - all keys must map to the same key slot\".format(command))\n            return slots.pop()\n\n        key = args[1]\n\n        return self.connection_pool.nodes.keyslot(key)", "response": "Determine what slot based on command and args"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the pipeline to empty.", "response": "def reset(self):\n        \"\"\"\n        Reset back to empty pipeline.\n        \"\"\"\n        self.command_stack = []\n\n        self.scripts = set()\n        self.watches = []\n        # clean up the other instance attributes\n        self.watching = False\n        self.explicit_transaction = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a bunch of commands to the Redis cluster.", "response": "async def send_cluster_commands(self, stack, raise_on_error=True, allow_redirections=True):\n        \"\"\"\n        Send a bunch of cluster commands to the redis cluster.\n\n        `allow_redirections` If the pipeline should follow `ASK` & `MOVED` responses\n        automatically. If set to false it will raise RedisClusterException.\n        \"\"\"\n        # the first time sending the commands we send all of the commands that were queued up.\n        # if we have to run through it again, we only retry the commands that failed.\n        attempt = sorted(stack, key=lambda x: x.position)\n\n        # build a list of node objects based on node names we need to\n        nodes = {}\n\n        # as we move through each command that still needs to be processed,\n        # we figure out the slot number that command maps to, then from the slot determine the node.\n        for c in attempt:\n            # refer to our internal node -> slot table that tells us where a given\n            # command should route to.\n            slot = self._determine_slot(*c.args)\n            node = self.connection_pool.get_node_by_slot(slot)\n\n            # little hack to make sure the node name is populated. probably could clean this up.\n            self.connection_pool.nodes.set_node_name(node)\n\n            # now that we know the name of the node ( it's just a string in the form of host:port )\n            # we can build a list of commands for each node.\n            node_name = node['name']\n            if node_name not in nodes:\n                nodes[node_name] = NodeCommands(self.parse_response, self.connection_pool.get_connection_by_node(node))\n\n            nodes[node_name].append(c)\n\n        # send the commands in sequence.\n        # we  write to all the open sockets for each node first, before reading anything\n        # this allows us to flush all the requests out across the network essentially in parallel\n        # so that we can read them all in parallel as they come back.\n        # we dont' multiplex on the sockets as they come available, but that shouldn't make too much difference.\n        node_commands = nodes.values()\n        for n in node_commands:\n            await n.write()\n\n        for n in node_commands:\n            await n.read()\n\n        # release all of the redis connections we allocated earlier back into the connection pool.\n        # we used to do this step as part of a try/finally block, but it is really dangerous to\n        # release connections back into the pool if for some reason the socket has data still left in it\n        # from a previous operation. The write and read operations already have try/catch around them for\n        # all known types of errors including connection and socket level errors.\n        # So if we hit an exception, something really bad happened and putting any of\n        # these connections back into the pool is a very bad idea.\n        # the socket might have unread buffer still sitting in it, and then the\n        # next time we read from it we pass the buffered result back from a previous\n        # command and every single request after to that connection will always get\n        # a mismatched result. (not just theoretical, I saw this happen on production x.x).\n        for n in nodes.values():\n            self.connection_pool.release(n.connection)\n\n        # if the response isn't an exception it is a valid response from the node\n        # we're all done with that command, YAY!\n        # if we have more commands to attempt, we've run into problems.\n        # collect all the commands we are allowed to retry.\n        # (MOVED, ASK, or connection errors or timeout errors)\n        attempt = sorted([c for c in attempt if isinstance(c.result, ERRORS_ALLOW_RETRY)], key=lambda x: x.position)\n        if attempt and allow_redirections:\n            # RETRY MAGIC HAPPENS HERE!\n            # send these remaing comamnds one at a time using `execute_command`\n            # in the main client. This keeps our retry logic in one place mostly,\n            # and allows us to be more confident in correctness of behavior.\n            # at this point any speed gains from pipelining have been lost\n            # anyway, so we might as well make the best attempt to get the correct\n            # behavior.\n            #\n            # The client command will handle retries for each individual command\n            # sequentially as we pass each one into `execute_command`. Any exceptions\n            # that bubble out should only appear once all retries have been exhausted.\n            #\n            # If a lot of commands have failed, we'll be setting the\n            # flag to rebuild the slots table from scratch. So MOVED errors should\n            # correct themselves fairly quickly.\n            await self.connection_pool.nodes.increment_reinitialize_counter(len(attempt))\n            for c in attempt:\n                try:\n                    # send each command individually like we do in the main client.\n                    c.result = await super(StrictClusterPipeline, self).execute_command(*c.args, **c.options)\n                except RedisError as e:\n                    c.result = e\n\n        # turn the response back into a simple flat array that corresponds\n        # to the sequence of commands issued in the stack in pipeline.execute()\n        response = [c.result for c in sorted(stack, key=lambda x: x.position)]\n\n        if raise_on_error:\n            self.raise_first_error(stack)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwatch the values at keys names", "response": "async def _watch(self, node, conn, names):\n        \"Watches the values at keys ``names``\"\n        for name in names:\n            slot = self._determine_slot('WATCH', name)\n            dist_node = self.connection_pool.get_node_by_slot(slot)\n            if node.get('name') != dist_node['name']:\n                # raise error if commands in a transaction can not hash to same node\n                if len(node) > 0:\n                    raise ClusterTransactionError(\"Keys in request don't hash to the same node\")\n        if self.explicit_transaction:\n            raise RedisError('Cannot issue a WATCH after a MULTI')\n        await conn.send_command('WATCH', *names)\n        return await conn.read_response()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def write(self):\n        connection = self.connection\n        commands = self.commands\n\n        # We are going to clobber the commands with the write, so go ahead\n        # and ensure that nothing is sitting there from a previous run.\n        for c in commands:\n            c.result = None\n\n        # build up all commands into a single request to increase network perf\n        # send all the commands and catch connection and timeout errors.\n        try:\n            await connection.send_packed_command(connection.pack_commands([c.args for c in commands]))\n        except (ConnectionError, TimeoutError) as e:\n            for c in commands:\n                c.result = e", "response": "Write all the items in the cache to StrictRedis."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the specified bit field and returns its old value.", "response": "def set(self, type, offset, value):\n        \"\"\"\n        Set the specified bit field and returns its old value.\n        \"\"\"\n        self._command_stack.extend(['SET', type, offset, value])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, type, offset):\n        self._command_stack.extend(['GET', type, offset])\n        return self", "response": "Returns the specified bit field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef incrby(self, type, offset, increment):\n        self._command_stack.extend(['INCRBY', type, offset, increment])\n        return self", "response": "Increments or decrements the specified bit field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of bits in the value of key.", "response": "async def bitcount(self, key, start=None, end=None):\n        \"\"\"\n        Returns the count of set bits in the value of ``key``.  Optional\n        ``start`` and ``end`` paramaters indicate which bytes to consider\n        \"\"\"\n        params = [key]\n        if start is not None and end is not None:\n            params.append(start)\n            params.append(end)\n        elif (start is not None and end is None) or \\\n                (end is not None and start is None):\n            raise RedisError(\"Both start and end must be specified\")\n        return await self.execute_command('BITCOUNT', *params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def getrange(self, key, start, end):\n        return await self.execute_command('GETRANGE', key, start, end)", "response": "Get the substring of the string stored at key determined by the offsets start and end."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def mget(self, keys, *args):\n        args = list_or_args(keys, args)\n        return await self.execute_command('MGET', *args)", "response": "Returns a list of values ordered identically to keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the keys and values based on a mapping.", "response": "async def msetnx(self, *args, **kwargs):\n        \"\"\"\n        Sets key/values based on a mapping if none of the keys are already set.\n        Mapping can be supplied as a single dictionary argument or as kwargs.\n        Returns a boolean indicating if the operation was successful.\n        \"\"\"\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSETNX requires **kwargs or a single '\n                                 'dict arg')\n            kwargs.update(args[0])\n        items = []\n        for pair in iteritems(kwargs):\n            items.extend(pair)\n        return await self.execute_command('MSETNX', *items)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def psetex(self, name, time_ms, value):\n        if isinstance(time_ms, datetime.timedelta):\n            ms = int(time_ms.microseconds / 1000)\n            time_ms = (time_ms.seconds + time_ms.days * 24 * 3600) * 1000 + ms\n        return await self.execute_command('PSETEX', name, time_ms, value)", "response": "Set the value of key name to value that expires in time_ms milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def setex(self, name, time, value):\n        if isinstance(time, datetime.timedelta):\n            time = time.seconds + time.days * 24 * 3600\n        return await self.execute_command('SETEX', name, time, value)", "response": "Set the value of key name to value that expires in time. time can be represented by an integer or a Python timedelta object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def substr(self, name, start, end=-1):\n        return await self.execute_command('SUBSTR', name, start, end)", "response": "Return a substring of the string at key name. start and end are 0 - based integers specifying the portion of the string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of values ordered identically to keys.", "response": "async def mget(self, keys, *args):\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n\n        Cluster impl:\n            Itterate all keys and send GET for each key.\n            This will go alot slower than a normal mget call in StrictRedis.\n\n            Operation is no longer atomic.\n        \"\"\"\n        res = list()\n        for arg in list_or_args(keys, args):\n            res.append(await self.get(arg))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def mset(self, *args, **kwargs):\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSET requires **kwargs or a single dict arg')\n            kwargs.update(args[0])\n\n        for pair in iteritems(kwargs):\n            await self.set(pair[0], pair[1])\n\n        return True", "response": "Set the key - value pairs based on a mapping."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def msetnx(self, *args, **kwargs):\n        if args:\n            if len(args) != 1 or not isinstance(args[0], dict):\n                raise RedisError('MSETNX requires **kwargs or a single dict arg')\n            kwargs.update(args[0])\n\n        # Itterate over all items and fail fast if one value is True.\n        for k, _ in kwargs.items():\n            if await self.get(k):\n                return False\n\n        return await self.mset(**kwargs)", "response": "Set the keys and values based on a mapping."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zset_score_pairs(response, **options):\n    if not response or not options['withscores']:\n        return response\n    score_cast_func = options.get('score_cast_func', float)\n    it = iter(response)\n    return list(zip(it, map(score_cast_func, it)))", "response": "Convert the response to a list of pairs of value score pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding any number of elements in the specified set to the key name.", "response": "async def zadd(self, name, *args, **kwargs):\n        \"\"\"\n        Set any number of score, element-name pairs to the key ``name``. Pairs\n        can be specified in two ways:\n\n        As *args, in the form of: score1, name1, score2, name2, ...\n        or as **kwargs, in the form of: name1=score1, name2=score2, ...\n\n        The following example would add four values to the 'my-key' key:\n        redis.zadd('my-key', 1.1, 'name1', 2.2, 'name2', name3=3.3, name4=4.4)\n        \"\"\"\n        pieces = []\n        if args:\n            if len(args) % 2 != 0:\n                raise RedisError(\"ZADD requires an equal number of \"\n                                 \"values and scores\")\n            pieces.extend(args)\n        for pair in iteritems(kwargs):\n            pieces.append(pair[1])\n            pieces.append(pair[0])\n        return await self.execute_command('ZADD', name, *pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndiffering from zadd in that you can set either 'XX' or 'NX' option as described here: https://redis.io/commands/zadd. Only for Redis 3.0.2 or later. The following example would add four values to the 'my-key' key: redis.zaddoption('my-key', 'XX', 1.1, 'name1', 2.2, 'name2', name3=3.3, name4=4.4) redis.zaddoption('my-key', 'NX CH', name1=2.2)", "response": "async def zaddoption(self, name, option=None, *args, **kwargs):\n        \"\"\"\n        Differs from zadd in that you can set either 'XX' or 'NX' option as\n        described here: https://redis.io/commands/zadd. Only for Redis 3.0.2 or\n        later.\n\n        The following example would add four values to the 'my-key' key:\n        redis.zaddoption('my-key', 'XX', 1.1, 'name1', 2.2, 'name2', name3=3.3, name4=4.4)\n        redis.zaddoption('my-key', 'NX CH', name1=2.2)\n        \"\"\"\n        if not option:\n            raise RedisError(\"ZADDOPTION must take options\")\n        options = set(opt.upper() for opt in option.split())\n        if options - VALID_ZADD_OPTIONS:\n            raise RedisError(\"ZADD only takes XX, NX, CH, or INCR\")\n        if 'NX' in options and 'XX' in options:\n            raise RedisError(\"ZADD only takes one of XX or NX\")\n        pieces = list(options)\n        members = []\n        if args:\n            if len(args) % 2 != 0:\n                raise RedisError(\"ZADD requires an equal number of \"\n                                 \"values and scores\")\n            members.extend(args)\n        for pair in iteritems(kwargs):\n            members.append(pair[1])\n            members.append(pair[0])\n        if 'INCR' in options and len(members) != 2:\n            raise RedisError(\"ZADD with INCR only takes one score-name pair\")\n        return await self.execute_command('ZADD', name, *pieces, *members)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def zrange(self, name, start, end, desc=False, withscores=False,\n                     score_cast_func=float):\n        \"\"\"\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``desc`` a boolean indicating whether to sort the results descendingly\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        if desc:\n            return await self.zrevrange(name, start, end, withscores,\n                                        score_cast_func)\n        pieces = ['ZRANGE', name, start, end]\n        if withscores:\n            pieces.append(b('WITHSCORES'))\n        options = {\n            'withscores': withscores,\n            'score_cast_func': score_cast_func\n        }\n        return await self.execute_command(*pieces, **options)", "response": "Return a range of values from sorted set name between start and end."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the lexicographical range of values from sorted set name between min and max.", "response": "async def zrangebylex(self, name, min, max, start=None, num=None):\n        \"\"\"\n        Return the lexicographical range of values from sorted set ``name``\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n        \"\"\"\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise RedisError(\"``start`` and ``num`` must both be specified\")\n        pieces = ['ZRANGEBYLEX', name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([b('LIMIT'), start, num])\n        return await self.execute_command(*pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving all elements in the sorted set name with scores between min and max.", "response": "async def zremrangebyscore(self, name, min, max):\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with scores\n        between ``min`` and ``max``. Returns the number of elements removed.\n        \"\"\"\n        return await self.execute_command('ZREMRANGEBYSCORE', name, min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a range of values from the sorted set name with scores between min and max in descending order.", "response": "async def zrevrangebyscore(self, name, max, min, start=None, num=None,\n                               withscores=False, score_cast_func=float):\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max`` in descending order.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise RedisError(\"``start`` and ``num`` must both be specified\")\n        pieces = ['ZREVRANGEBYSCORE', name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([b('LIMIT'), start, num])\n        if withscores:\n            pieces.append(b('WITHSCORES'))\n        options = {\n            'withscores': withscores,\n            'score_cast_func': score_cast_func\n        }\n        return await self.execute_command(*pieces, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort the response as a list of n - element tuples with n being the value found in options. groups", "response": "def sort_return_tuples(response, **options):\n    \"\"\"\n    If ``groups`` is specified, return the response as a list of\n    n-element tuples with n being the value found in options['groups']\n    \"\"\"\n    if not response or not options['groups']:\n        return response\n    n = options['groups']\n    return list(zip(*[response[i::n] for i in range(n)]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets an expire flag on key name for time seconds.", "response": "async def expire(self, name, time):\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` seconds. ``time``\n        can be represented by an integer or a Python timedelta object.\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = time.seconds + time.days * 24 * 3600\n        return await self.execute_command('EXPIRE', name, time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def pexpire(self, name, time):\n        if isinstance(time, datetime.timedelta):\n            ms = int(time.microseconds / 1000)\n            time = (time.seconds + time.days * 24 * 3600) * 1000 + ms\n        return await self.execute_command('PEXPIRE', name, time)", "response": "Set an expire flag on key name for time milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def sort(self, name, start=None, num=None, by=None, get=None,\n             desc=False, alpha=False, store=None, groups=False):\n        \"\"\"\n        Sort and return the list, set or sorted set at ``name``.\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where int he key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        ``store`` allows for storing the result of the sort into\n            the key ``store``\n\n        ``groups`` if set to True and if ``get`` contains at least two\n            elements, sort will return a list of tuples, each containing the\n            values fetched from the arguments to ``get``.\n\n        \"\"\"\n        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise RedisError(\"``start`` and ``num`` must both be specified\")\n\n        pieces = [name]\n        if by is not None:\n            pieces.append(b('BY'))\n            pieces.append(by)\n        if start is not None and num is not None:\n            pieces.append(b('LIMIT'))\n            pieces.append(start)\n            pieces.append(num)\n        if get is not None:\n            # If get is a string assume we want to get a single value.\n            # Otherwise assume it's an interable and we want to get multiple\n            # values. We can't just iterate blindly because strings are\n            # iterable.\n            if isinstance(get, str):\n                pieces.append(b('GET'))\n                pieces.append(get)\n            else:\n                for g in get:\n                    pieces.append(b('GET'))\n                    pieces.append(g)\n        if desc:\n            pieces.append(b('DESC'))\n        if alpha:\n            pieces.append(b('ALPHA'))\n        if store is not None:\n            pieces.append(b('STORE'))\n            pieces.append(store)\n\n        if groups:\n            if not get or isinstance(get, str) or len(get) < 2:\n                raise DataError('when using \"groups\" the \"get\" argument '\n                                'must be specified and contain at least '\n                                'two keys')\n\n        options = {'groups': len(get) if groups else None}\n        return await self.execute_command('SORT', *pieces, **options)", "response": "Sorts the items in the specified key - item store or sorted set at name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrename a key src to dst", "response": "async def rename(self, src, dst):\n        \"\"\"\n        Rename key ``src`` to ``dst``\n\n        Cluster impl:\n            This operation is no longer atomic because each key must be querried\n            then set in separate calls because they maybe will change cluster node\n        \"\"\"\n        if src == dst:\n            raise ResponseError(\"source and destination objects are the same\")\n\n        data = await self.dump(src)\n\n        if data is None:\n            raise ResponseError(\"no such key\")\n\n        ttl = await self.pttl(src)\n\n        if ttl is None or ttl < 1:\n            ttl = 0\n\n        await self.delete(dst)\n        await self.restore(dst, ttl, data)\n        await self.delete(src)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete one or more keys specified by names Cluster impl : Delete one or more keys specified by names", "response": "async def delete(self, *names):\n        \"\"\"\n        \"Delete one or more keys specified by ``names``\"\n\n        Cluster impl:\n            Iterate all keys and send DELETE for each key.\n            This will go a lot slower than a normal delete call in StrictRedis.\n\n            Operation is no longer atomic.\n        \"\"\"\n        count = 0\n\n        for arg in names:\n            count += await self.execute_command('DEL', arg)\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def renamenx(self, src, dst):\n        if not await self.exists(dst):\n            return await self.rename(src, dst)\n\n        return False", "response": "Rename key src to dst."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the specified geospatial items to the specified key identified by the name argument.", "response": "async def geoadd(self, name, *values):\n        \"\"\"\n        Add the specified geospatial items to the specified key identified\n        by the ``name`` argument. The Geospatial items are given as ordered\n        members of the ``values`` argument, each item or place is formed by\n        the triad latitude, longitude and name.\n        \"\"\"\n        if len(values) % 3 != 0:\n            raise RedisError(\"GEOADD requires places with lon, lat and name\"\n                             \" values\")\n        return await self.execute_command('GEOADD', name, *values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def georadius(self, name, longitude, latitude, radius, unit=None,\n                        withdist=False, withcoord=False, withhash=False, count=None,\n                        sort=None, store=None, store_dist=None):\n        \"\"\"\n        Return the members of the specified key identified by the\n        ``name`` argument which are within the borders of the area specified\n        with the ``latitude`` and ``longitude`` location and the maximum\n        distance from the center specified by the ``radius`` value.\n\n        The units must be one of the following : m, km mi, ft. By default\n\n        ``withdist`` indicates to return the distances of each place.\n\n        ``withcoord`` indicates to return the latitude and longitude of\n        each place.\n\n        ``withhash`` indicates to return the geohash string of each place.\n\n        ``count`` indicates to return the number of elements up to N.\n\n        ``sort`` indicates to return the places in a sorted way, ASC for\n        nearest to fairest and DESC for fairest to nearest.\n\n        ``store`` indicates to save the places names in a sorted set named\n        with a specific key, each element of the destination sorted set is\n        populated with the score got from the original geo sorted set.\n\n        ``store_dist`` indicates to save the places names in a sorted set\n        named with a specific key, instead of ``store`` the sorted set\n        destination score is set with the distance.\n        \"\"\"\n        return await self._georadiusgeneric('GEORADIUS',\n                                            name, longitude, latitude, radius,\n                                            unit=unit, withdist=withdist,\n                                            withcoord=withcoord, withhash=withhash,\n                                            count=count, sort=sort, store=store,\n                                            store_dist=store_dist)", "response": "Return members of the specified key in georadius."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def rotate_slaves(self):\n        \"Round-robin slave balancer\"\n        slaves = await self.sentinel_manager.discover_slaves(self.service_name)\n        slave_address = list()\n        if slaves:\n            if self.slave_rr_counter is None:\n                self.slave_rr_counter = random.randint(0, len(slaves) - 1)\n            for _ in range(len(slaves)):\n                self.slave_rr_counter = (self.slave_rr_counter + 1) % len(slaves)\n                slave_address.append(slaves[self.slave_rr_counter])\n            return slave_address\n        # Fallback to the master connection\n        try:\n            return await self.get_master_address()\n        except MasterNotFoundError:\n            pass\n        raise SlaveNotFoundError('No slave found for %r' % (self.service_name))", "response": "Round - robin slave balancer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef master_for(self, service_name, redis_class=StrictRedis,\n                   connection_pool_class=SentinelConnectionPool, **kwargs):\n        \"\"\"\n        Returns a redis client instance for the ``service_name`` master.\n\n        A SentinelConnectionPool class is used to retrive the master's\n        address before establishing a new connection.\n\n        NOTE: If the master's address has changed, any cached connections to\n        the old master are closed.\n\n        By default clients will be a redis.StrictRedis instance. Specify a\n        different class to the ``redis_class`` argument if you desire\n        something different.\n\n        The ``connection_pool_class`` specifies the connection pool to use.\n        The SentinelConnectionPool will be used by default.\n\n        All other keyword arguments are merged with any connection_kwargs\n        passed to this class and passed to the connection pool as keyword\n        arguments to be used to initialize Redis connections.\n        \"\"\"\n        kwargs['is_master'] = True\n        connection_kwargs = dict(self.connection_kwargs)\n        connection_kwargs.update(kwargs)\n        return redis_class(connection_pool=connection_pool_class(\n            service_name, self, **connection_kwargs))", "response": "Returns a redis client instance for the master for the given service_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Redis client instance for the given service_name slave.", "response": "def slave_for(self, service_name, redis_class=StrictRedis,\n                  connection_pool_class=SentinelConnectionPool, **kwargs):\n        \"\"\"\n        Returns redis client instance for the ``service_name`` slave(s).\n\n        A SentinelConnectionPool class is used to retrive the slave's\n        address before establishing a new connection.\n\n        By default clients will be a redis.StrictRedis instance. Specify a\n        different class to the ``redis_class`` argument if you desire\n        something different.\n\n        The ``connection_pool_class`` specifies the connection pool to use.\n        The SentinelConnectionPool will be used by default.\n\n        All other keyword arguments are merged with any connection_kwargs\n        passed to this class and passed to the connection pool as keyword\n        arguments to be used to initialize Redis connections.\n        \"\"\"\n        kwargs['is_master'] = False\n        connection_kwargs = dict(self.connection_kwargs)\n        connection_kwargs.update(kwargs)\n        return redis_class(connection_pool=connection_pool_class(\n            service_name, self, **connection_kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nacquires a shared distributed lock.", "response": "async def acquire(self, blocking=None, blocking_timeout=None):\n        \"\"\"\n        Use Redis to hold a shared, distributed lock named ``name``.\n        Returns True once the lock is acquired.\n\n        If ``blocking`` is False, always return immediately. If the lock\n        was acquired, return True, otherwise return False.\n\n        ``blocking_timeout`` specifies the maximum number of seconds to\n        wait trying to acquire the lock.\n        \"\"\"\n        sleep = self.sleep\n        token = b(uuid.uuid1().hex)\n        if blocking is None:\n            blocking = self.blocking\n        if blocking_timeout is None:\n            blocking_timeout = self.blocking_timeout\n        stop_trying_at = None\n        if blocking_timeout is not None:\n            stop_trying_at = mod_time.time() + blocking_timeout\n        while True:\n            if await self.do_acquire(token):\n                self.local.token = token\n                return True\n            if not blocking:\n                return False\n            if stop_trying_at is not None and mod_time.time() > stop_trying_at:\n                return False\n            await asyncio.sleep(sleep, loop=self.redis.connection_pool.loop)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def release(self):\n        \"Releases the already acquired lock\"\n        expected_token = self.local.token\n        if expected_token is None:\n            raise LockError(\"Cannot release an unlocked lock\")\n        self.local.token = None\n        await self.do_release(expected_token)", "response": "Releases the already acquired lock"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def extend(self, additional_time):\n        if self.local.token is None:\n            raise LockError(\"Cannot extend an unlocked lock\")\n        if self.timeout is None:\n            raise LockError(\"Cannot extend a lock with no timeout\")\n        return await self.do_extend(additional_time)", "response": "Adds more time to an already acquired lock."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacquire a shared distributed lock.", "response": "async def acquire(self, blocking=None, blocking_timeout=None):\n        \"\"\"\n        Use Redis to hold a shared, distributed lock named ``name``.\n        Returns True once the lock is acquired.\n\n        If ``blocking`` is False, always return immediately. If the lock\n        was acquired, return True, otherwise return False.\n\n        ``blocking_timeout`` specifies the maximum number of seconds to\n        wait trying to acquire the lock. It should not be greater than\n        expire time of the lock\n        \"\"\"\n        sleep = self.sleep\n        token = b(uuid.uuid1().hex)\n        if blocking is None:\n            blocking = self.blocking\n        if blocking_timeout is None:\n            blocking_timeout = self.blocking_timeout\n        blocking_timeout = blocking_timeout or self.timeout\n        stop_trying_at = mod_time.time() + min(blocking_timeout, self.timeout)\n\n        while True:\n            if await self.do_acquire(token):\n                lock_acquired_at = mod_time.time()\n                if await self.check_lock_in_slaves(token):\n                    check_finished_at = mod_time.time()\n                    # if time expends on acquiring lock is greater than given time\n                    # the lock should be released manually\n                    if check_finished_at > stop_trying_at:\n                        await self.do_release(token)\n                        return False\n                    self.local.token = token\n                    # validity time is considered to be the\n                    # initial validity time minus the time elapsed during check\n                    await self.do_extend(lock_acquired_at - check_finished_at)\n                    return True\n                else:\n                    await self.do_release(token)\n                    return False\n            if not blocking or mod_time.time() > stop_trying_at:\n                return False\n            await asyncio.sleep(sleep, loop=self.redis.connection_pool.loop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a mapping of nodes to slots nodes", "response": "def _nodes_slots_to_slots_nodes(self, mapping):\n        \"\"\"\n        Converts a mapping of\n        {id: <node>, slots: (slot1, slot2)}\n        to\n        {slot1: <node>, slot2: <node>}\n\n        Operation is expensive so use with caution\n        \"\"\"\n        out = {}\n        for node in mapping:\n            for slot in node['slots']:\n                out[str(slot)] = node['id']\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete a set of hash slots from the cluster.", "response": "async def cluster_delslots(self, *slots):\n        \"\"\"\n        Set hash slots as unbound in the cluster.\n        It determines by it self what node the slot is in and sends it there\n\n        Returns a list of the results for each processed slot.\n        \"\"\"\n        cluster_nodes = self._nodes_slots_to_slots_nodes(await self.cluster_nodes())\n        res = list()\n        for slot in slots:\n            res.append(await self.execute_command('CLUSTER DELSLOTS', slot, node_id=cluster_nodes[slot]))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def cluster_failover(self, node_id, option):\n        if not isinstance(option, str) or option.upper() not in {'FORCE', 'TAKEOVER'}:\n            raise ClusterError('Wrong option provided')\n        return await self.execute_command('CLUSTER FAILOVER', option, node_id=node_id)", "response": "This command allows a slave to perform a manual failover of its master"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def cluster_meet(self, node_id, host, port):\n        return await self.execute_command('CLUSTER MEET', host, port, node_id=node_id)", "response": "Send a MEET command to the node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def cluster_reset(self, node_id, soft=True):\n        option = 'SOFT' if soft else 'HARD'\n        return await self.execute_command('CLUSTER RESET', option, node_id=node_id)", "response": "Reset the set of keys in a Redis Cluster"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend CLUSTER RESET to all nodes in the cluster", "response": "async def cluster_reset_all_nodes(self, soft=True):\n        \"\"\"\n        Send CLUSTER RESET to all nodes in the cluster\n\n        If 'soft' is True then it will send 'SOFT' argument\n        If 'soft' is False then it will send 'HARD' argument\n\n        Sends to all nodes in the cluster\n        \"\"\"\n        option = 'SOFT' if soft else 'HARD'\n        res = list()\n        for node in await self.cluster_nodes():\n            res.append(\n                await self.execute_command(\n                    'CLUSTER RESET', option, node_id=node['id']\n                ))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbinds an hash slot to a specific node", "response": "async def cluster_setslot(self, node_id, slot_id, state):\n        \"\"\"\n        Bind an hash slot to a specific node\n\n        Sends to specified node\n        \"\"\"\n        if state.upper() in {'IMPORTING', 'MIGRATING', 'NODE'} and node_id is not None:\n            return await self.execute_command('CLUSTER SETSLOT', slot_id, state, node_id)\n        elif state.upper() == 'STABLE':\n            return await self.execute_command('CLUSTER SETSLOT', slot_id, 'STABLE')\n        else:\n            raise RedisError('Invalid slot state: {0}'.format(state))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def execute(self, keys=[], args=[], client=None):\n        \"Execute the script, passing any required ``args``\"\n        if client is None:\n            client = self.registered_client\n        args = tuple(keys) + tuple(args)\n        # make sure the Redis server knows about the script\n        if isinstance(client, BasePipeline):\n            # make sure this script is good to go on pipeline\n            client.scripts.add(self)\n        try:\n            return await client.evalsha(self.sha, len(keys), *args)\n        except NoScriptError:\n            # Maybe the client is pointed to a differnet server than the client\n            # that created this instance?\n            # Overwrite the sha just in case there was a discrepancy.\n            self.sha = await client.script_load(self.script)\n            return await client.evalsha(self.sha, len(keys), *args)", "response": "Execute the script passing any required args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def pfmerge(self, dest, *sources):\n        all_k = []\n\n        # Fetch all HLL objects via GET and store them client side as strings\n        all_hll_objects = list()\n        for hll_key in sources:\n            all_hll_objects.append(await self.get(hll_key))\n\n        # Randomize a keyslot hash that should be used inside {} when doing SET\n        random_hash_slot = self._random_id()\n\n        # Special handling of dest variable if it allready exists, then it shold be included in the HLL merge\n        # dest can exists anywhere in the cluster.\n        dest_data = await self.get(dest)\n\n        if dest_data:\n            all_hll_objects.append(dest_data)\n\n        # SET all stored HLL objects with SET {RandomHash}RandomKey hll_obj\n        for hll_object in all_hll_objects:\n            k = self._random_good_hashslot_key(random_hash_slot)\n            all_k.append(k)\n            await self.set(k, hll_object)\n\n        # Do regular PFMERGE operation and store value in random key in {RandomHash}\n        tmp_dest = self._random_good_hashslot_key(random_hash_slot)\n        await self.execute_command(\"PFMERGE\", tmp_dest, *all_k)\n\n        # Do GET and SET so that result will be stored in the destination object any where in the cluster\n        parsed_dest = await self.get(tmp_dest)\n        await self.set(dest, parsed_dest)\n\n        # Cleanup tmp variables\n        await self.delete(tmp_dest)\n\n        for k in all_k:\n            await self.delete(k)\n\n        return True", "response": "Merge N different HyperLogLogs into a single HyperLogLog."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _random_id(self, size=16, chars=string.ascii_uppercase + string.digits):\n        return ''.join(random.choice(chars) for _ in range(size))", "response": "Generates a random id based on size and chars variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new master to Sentinel to be monitored", "response": "async def sentinel_monitor(self, name, ip, port, quorum):\n        \"Add a new master to Sentinel to be monitored\"\n        return await self.execute_command('SENTINEL MONITOR', name, ip, port, quorum)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def sentinel_set(self, name, option, value):\n        \"Set Sentinel monitoring parameters for a given master\"\n        return await self.execute_command('SENTINEL SET', name, option, value)", "response": "Set Sentinel monitoring parameters for a given master"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def sdiff(self, keys, *args):\n        \"Return the difference of sets specified by ``keys``\"\n        args = list_or_args(keys, args)\n        return await self.execute_command('SDIFF', *args)", "response": "Return the difference of sets specified by keys"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the difference of sets specified by keys into a new set named dest. Returns the number of keys in the new set.", "response": "async def sdiffstore(self, dest, keys, *args):\n        \"\"\"\n        Store the difference of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        \"\"\"\n        args = list_or_args(keys, args)\n        return await self.execute_command('SDIFFSTORE', dest, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the intersection of sets specified by keys", "response": "async def sinter(self, keys, *args):\n        \"Return the intersection of sets specified by ``keys``\"\n        args = list_or_args(keys, args)\n        return await self.execute_command('SINTER', *args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def sinterstore(self, dest, keys, *args):\n        args = list_or_args(keys, args)\n        return await self.execute_command('SINTERSTORE', dest, *args)", "response": "Store the intersection of sets specified by keys into a new set named dest. Returns the number of keys in the new set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove and return a random member of set name.", "response": "async def spop(self, name, count=None):\n        \"\"\"\n        Remove and return a random member of set ``name``\n        ``count`` should be type of int and default set to 1.\n        If ``count`` is supplied, pops a list of ``count`` random\n+        members of set ``name``\n        \"\"\"\n        if count and isinstance(count, int):\n            return await self.execute_command('SPOP', name, count)\n        else:\n            return await self.execute_command('SPOP', name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def srandmember(self, name, number=None):\n        args = number and [number] or []\n        return await self.execute_command('SRANDMEMBER', name, *args)", "response": "Returns a random member of set name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def sunion(self, keys, *args):\n        \"Return the union of sets specified by ``keys``\"\n        args = list_or_args(keys, args)\n        return await self.execute_command('SUNION', *args)", "response": "Return the union of sets specified by keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def sunionstore(self, dest, keys, *args):\n        args = list_or_args(keys, args)\n        return await self.execute_command('SUNIONSTORE', dest, *args)", "response": "Store the union of sets specified by keys into a new set named dest. Returns the number of keys in the new set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the difference of sets specified by keys into a new set named dest. Returns the number of keys in the new set.", "response": "async def sdiffstore(self, dest, keys, *args):\n        \"\"\"\n        Store the difference of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        Overwrites dest key if it exists.\n\n        Cluster impl:\n            Use sdiff() --> Delete dest key --> store result in dest key\n        \"\"\"\n        res = await self.sdiff(keys, *args)\n        await self.delete(dest)\n\n        if not res:\n            return 0\n        return await self.sadd(dest, *res)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def sinter(self, keys, *args):\n        k = list_or_args(keys, args)\n        res = await self.smembers(k[0])\n\n        for arg in k[1:]:\n            res &= await self.smembers(arg)\n\n        return res", "response": "Return the intersection of sets specified by keys and any given set of arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing sinterstore to store the intersection of sets specified by keys into a new set named dest. Returns the number of keys in the new set.", "response": "async def sinterstore(self, dest, keys, *args):\n        \"\"\"\n        Store the intersection of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        Cluster impl:\n            Use sinter() --> Delete dest key --> store result in dest key\n        \"\"\"\n        res = await self.sinter(keys, *args)\n        await self.delete(dest)\n\n        if res:\n            await self.sadd(dest, *res)\n            return len(res)\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def smove(self, src, dst, value):\n        res = await self.srem(src, value)\n\n        # Only add the element if existed in src set\n        if res == 1:\n            await self.sadd(dst, value)\n\n        return res", "response": "Move value from set src to set dst atomically."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def sunionstore(self, dest, keys, *args):\n        res = await self.sunion(keys, *args)\n        await self.delete(dest)\n\n        return await self.sadd(dest, *res)", "response": "Store the union of sets specified by keys into a new set named dest."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def parse_response(self, connection, command_name, **options):\n        \"Parses a response from the Redis server\"\n        response = await connection.read_response()\n        if command_name in self.response_callbacks:\n            callback = self.response_callbacks[command_name]\n            return callback(response, **options)\n        return response", "response": "Parses a response from the Redis server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new pipeline that can queue multiple commands for later execution.", "response": "async def pipeline(self, transaction=True, shard_hint=None):\n        \"\"\"\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        \"\"\"\n        from aredis.pipeline import StrictPipeline\n        pipeline = StrictPipeline(self.connection_pool, self.response_callbacks,\n                                  transaction, shard_hint)\n        await pipeline.reset()\n        return pipeline"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_url(cls, url, db=None, skip_full_coverage_check=False, **kwargs):\n        connection_pool = ClusterConnectionPool.from_url(url, db=db, **kwargs)\n        return cls(connection_pool=connection_pool, skip_full_coverage_check=skip_full_coverage_check)", "response": "Returns a Redis client object configured from the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges result with command.", "response": "def _merge_result(self, command, res, **kwargs):\n        \"\"\"\n        `res` is a dict with the following structure Dict(NodeName, CommandResult)\n        \"\"\"\n        if command in self.result_callbacks:\n            return self.result_callbacks[command](res, **kwargs)\n\n        # Default way to handle result\n        return first_key(res)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def execute_command(self, *args, **kwargs):\n        if not self.connection_pool.initialized:\n            await self.connection_pool.initialize()\n        if not args:\n            raise RedisClusterException(\"Unable to determine command to use\")\n\n        command = args[0]\n\n        node = self.determine_node(*args, **kwargs)\n        if node:\n            return await self.execute_command_on_nodes(node, *args, **kwargs)\n\n        # If set externally we must update it before calling any commands\n        if self.refresh_table_asap:\n            await self.connection_pool.nodes.initialize()\n            self.refresh_table_asap = False\n\n        redirect_addr = None\n        asking = False\n\n        try_random_node = False\n        slot = self._determine_slot(*args)\n        ttl = int(self.RedisClusterRequestTTL)\n\n        while ttl > 0:\n            ttl -= 1\n\n            if asking:\n                node = self.connection_pool.nodes.nodes[redirect_addr]\n                r = self.connection_pool.get_connection_by_node(node)\n            elif try_random_node:\n                r = self.connection_pool.get_random_connection()\n                try_random_node = False\n            else:\n                if self.refresh_table_asap:\n                    # MOVED\n                    node = self.connection_pool.get_master_node_by_slot(slot)\n                else:\n                    node = self.connection_pool.get_node_by_slot(slot)\n                r = self.connection_pool.get_connection_by_node(node)\n\n            try:\n                if asking:\n                    await r.send_command('ASKING')\n                    await self.parse_response(r, \"ASKING\", **kwargs)\n                    asking = False\n\n                await r.send_command(*args)\n                return await self.parse_response(r, command, **kwargs)\n            except (RedisClusterException, BusyLoadingError):\n                raise\n            except (CancelledError, ConnectionError, TimeoutError):\n                try_random_node = True\n\n                if ttl < self.RedisClusterRequestTTL / 2:\n                    await asyncio.sleep(0.1)\n            except ClusterDownError as e:\n                self.connection_pool.disconnect()\n                self.connection_pool.reset()\n                self.refresh_table_asap = True\n\n                raise e\n            except MovedError as e:\n                # Reinitialize on ever x number of MovedError.\n                # This counter will increase faster when the same client object\n                # is shared between multiple threads. To reduce the frequency you\n                # can set the variable 'reinitialize_steps' in the constructor.\n                self.refresh_table_asap = True\n                await self.connection_pool.nodes.increment_reinitialize_counter()\n\n                node = self.connection_pool.nodes.set_node(e.host, e.port, server_type='master')\n                self.connection_pool.nodes.slots[e.slot_id][0] = node\n            except TryAgainError as e:\n                if ttl < self.RedisClusterRequestTTL / 2:\n                    await asyncio.sleep(0.05)\n            except AskError as e:\n                redirect_addr, asking = \"{0}:{1}\".format(e.host, e.port), True\n            finally:\n                self.connection_pool.release(r)\n\n        raise ClusterError('TTL exhausted.')", "response": "Send a command to a node in the cluster and return the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncluster impl: Pipelines do not work in cluster mode the same way they do in normal mode. Create a clone of this object so that simulating pipelines will work correctly. Each command will be called directly when used and when calling execute() will only return the result stack. cluster transaction can only be run with commands in the same node, otherwise error will be raised.", "response": "async def pipeline(self, transaction=None, shard_hint=None, watches=None):\n        \"\"\"\n        Cluster impl:\n            Pipelines do not work in cluster mode the same way they do in normal mode.\n            Create a clone of this object so that simulating pipelines will work correctly.\n            Each command will be called directly when used and when calling execute() will only return the result stack.\n        cluster transaction can only be run with commands in the same node, otherwise error will be raised.\n        \"\"\"\n        await self.connection_pool.initialize()\n        if shard_hint:\n            raise RedisClusterException(\"shard_hint is deprecated in cluster mode\")\n\n        from aredis.pipeline import StrictClusterPipeline\n        return StrictClusterPipeline(\n            connection_pool=self.connection_pool,\n            startup_nodes=self.connection_pool.nodes.startup_nodes,\n            result_callbacks=self.result_callbacks,\n            response_callbacks=self.response_callbacks,\n            transaction=transaction,\n            watches=watches\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when the stream connects", "response": "def on_connect(self, connection):\n        \"Called when the stream connects\"\n        self._stream = connection._reader\n        self._buffer = SocketBuffer(self._stream, self._read_size)\n        if connection.decode_responses:\n            self.encoding = connection.encoding"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_disconnect(self):\n        \"Called when the stream disconnects\"\n        if self._stream is not None:\n            self._stream = None\n        if self._buffer is not None:\n            self._buffer.close()\n            self._buffer = None\n        self.encoding = None", "response": "Called when the stream disconnects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nseeing if there s data that can be read.", "response": "async def can_read(self):\n        \"See if there's data that can be read.\"\n        if not (self._reader and self._writer):\n            await self.connect()\n        return self._parser.can_read()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def send_packed_command(self, command):\n        \"Send an already packed command to the Redis server\"\n        if not self._writer:\n            await self.connect()\n        try:\n            if isinstance(command, str):\n                command = [command]\n            self._writer.writelines(command)\n        except asyncio.futures.TimeoutError:\n            self.disconnect()\n            raise TimeoutError(\"Timeout writing to socket\")\n        except Exception:\n            e = sys.exc_info()[1]\n            self.disconnect()\n            if len(e.args) == 1:\n                errno, errmsg = 'UNKNOWN', e.args[0]\n            else:\n                errno = e.args[0]\n                errmsg = e.args[1]\n            raise ConnectionError(\"Error %s while writing to socket. %s.\" %\n                                  (errno, errmsg))\n        except:\n            self.disconnect()\n            raise", "response": "Send an already packed command to the Redis server"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self):\n        \"Disconnects from the Redis server\"\n        self._parser.on_disconnect()\n        try:\n            self._writer.close()\n        except Exception:\n            pass\n        self._reader = None\n        self._writer = None", "response": "Disconnects from the Redis server"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pack_command(self, *args):\n        \"Pack a series of arguments into the Redis protocol\"\n        output = []\n        # the client might have included 1 or more literal arguments in\n        # the command name, e.g., 'CONFIG GET'. The Redis server expects these\n        # arguments to be sent separately, so split the first argument\n        # manually. All of these arguements get wrapped in the Token class\n        # to prevent them from being encoded.\n        command = args[0]\n        if ' ' in command:\n            args = tuple([b(s) for s in command.split()]) + args[1:]\n        else:\n            args = (b(command),) + args[1:]\n\n        buff = SYM_EMPTY.join(\n            (SYM_STAR, b(str(len(args))), SYM_CRLF))\n        for arg in map(self.encode, args):\n            # to avoid large string mallocs, chunk the command into the\n            # output list if we're sending large values\n            if len(buff) > 6000 or len(arg) > 6000:\n                buff = SYM_EMPTY.join(\n                    (buff, SYM_DOLLAR, b(str(len(arg))), SYM_CRLF))\n                output.append(buff)\n                output.append(b(arg))\n                buff = SYM_CRLF\n            else:\n                buff = SYM_EMPTY.join((buff, SYM_DOLLAR, b(str(len(arg))),\n                                       SYM_CRLF, b(arg), SYM_CRLF))\n        output.append(buff)\n        return output", "response": "Pack a series of arguments into the Redis protocol"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npacking multiple commands into the Redis protocol", "response": "def pack_commands(self, commands):\n        \"Pack multiple commands into the Redis protocol\"\n        output = []\n        pieces = []\n        buffer_length = 0\n\n        for cmd in commands:\n            for chunk in self.pack_command(*cmd):\n                pieces.append(chunk)\n                buffer_length += len(chunk)\n\n            if buffer_length > 6000:\n                output.append(SYM_EMPTY.join(pieces))\n                buffer_length = 0\n                pieces = []\n\n        if pieces:\n            output.append(SYM_EMPTY.join(pieces))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the connection send READONLY if it is set during object initialization.", "response": "async def on_connect(self):\n        \"\"\"\n        Initialize the connection, authenticate and select a database and send READONLY if it is\n        set during object initialization.\n        \"\"\"\n        if self.db:\n            warnings.warn('SELECT DB is not allowed in cluster mode')\n            self.db = ''\n        await super(ClusterConnection, self).on_connect()\n        if self.readonly:\n            await self.send_command('READONLY')\n            if nativestr(await self.read_response()) != 'OK':\n                raise ConnectionError('READONLY command failed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_schema(self, schema):\n        if isinstance(schema, SchemaBuilder):\n            schema_uri = schema.schema_uri\n            schema = schema.to_schema()\n            if schema_uri is None:\n                del schema['$schema']\n        elif isinstance(schema, SchemaNode):\n            schema = schema.to_schema()\n\n        if '$schema' in schema:\n            self.schema_uri = self.schema_uri or schema['$schema']\n            schema = dict(schema)\n            del schema['$schema']\n        self._root_node.add_schema(schema)", "response": "Add a schema to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a schema based on previous inputs.", "response": "def to_schema(self):\n        \"\"\"\n        Generate a schema based on previous inputs.\n\n        :rtype: ``dict``\n        \"\"\"\n        schema = self._base_schema()\n        schema.update(self._root_node.to_schema())\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a schema and convert it directly to serialized JSON.", "response": "def to_json(self, *args, **kwargs):\n        \"\"\"\n        Generate a schema and convert it directly to serialized JSON.\n\n        :rtype: ``str``\n        \"\"\"\n        return json.dumps(self.to_schema(), *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_long_docs(*filenames):\n    docs = []\n    for filename in filenames:\n        with open(filename, 'r') as f:\n            docs.append(f.read())\n\n    return \"\\n\\n\".join(docs)", "response": "Build rst description from a set of files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_json_strings(raw_text):\n    strings = re.split('}\\s*(?={)', raw_text)\n\n    # put back the stripped character\n    json_strings = [string + '}' for string in strings[:-1]]\n\n    # the last one doesn't need to be modified\n    json_strings.append(strings[-1])\n\n    return json_strings", "response": "Detects the JSON strings in the raw text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_schema(self, schema):\n\n        # serialize instances of SchemaNode before parsing\n        if isinstance(schema, SchemaNode):\n            schema = schema.to_schema()\n\n        for subschema in self._get_subschemas(schema):\n            # delegate to SchemaType object\n            schema_generator = self._get_generator_for_schema(subschema)\n            schema_generator.add_schema(subschema)\n\n        # return self for easy method chaining\n        return self", "response": "Add a schema to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_object(self, obj):\n\n        # delegate to SchemaType object\n        schema_generator = self._get_generator_for_object(obj)\n        schema_generator.add_object(obj)\n\n        # return self for easy method chaining\n        return self", "response": "Modify the schema to accommodate an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the current schema to a dict.", "response": "def to_schema(self):\n        \"\"\"\n        Convert the current schema to a `dict`.\n        \"\"\"\n        types = set()\n        generated_schemas = []\n        for schema_generator in self._schema_generators:\n            generated_schema = schema_generator.to_schema()\n            if len(generated_schema) == 1 and 'type' in generated_schema:\n                types.add(generated_schema['type'])\n            else:\n                generated_schemas.append(generated_schema)\n\n        if types:\n            if len(types) == 1:\n                (types,) = types\n            else:\n                types = sorted(types)\n            generated_schemas = [{'type': types}] + generated_schemas\n        if len(generated_schemas) == 1:\n            (result_schema,) = generated_schemas\n        elif generated_schemas:\n            result_schema = {'anyOf': generated_schemas}\n        else:\n            result_schema = {}\n\n        return result_schema"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconfigure the specified Flask app to enforce SSL.", "response": "def init_app(self, app):\n        \"\"\"Configures the specified Flask app to enforce SSL.\"\"\"\n        app.config.setdefault('SSLIFY_AGE', self.defaults['age'])\n        app.config.setdefault('SSLIFY_SUBDOMAINS', self.defaults['subdomains'])\n        app.config.setdefault('SSLIFY_PERMANENT', self.defaults['permanent'])\n        app.config.setdefault('SSLIFY_SKIPS', self.defaults['skips'])\n\n        app.before_request(self.redirect_to_ssl)\n        app.after_request(self.set_hsts_header)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the proper HSTS policy.", "response": "def hsts_header(self):\n        \"\"\"Returns the proper HSTS policy.\"\"\"\n        hsts_policy = 'max-age={0}'.format(self.hsts_age)\n\n        if self.hsts_include_subdomains:\n            hsts_policy += '; includeSubDomains'\n\n        return hsts_policy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the skip list.", "response": "def skip(self):\n        \"\"\"Checks the skip list.\"\"\"\n        # Should we skip?\n        if self.skip_list and isinstance(self.skip_list, list):\n            for skip in self.skip_list:\n                if request.path.startswith('/{0}'.format(skip)):\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nredirecting incoming requests to HTTPS.", "response": "def redirect_to_ssl(self):\n        \"\"\"Redirect incoming requests to HTTPS.\"\"\"\n        # Should we redirect?\n        criteria = [\n            request.is_secure,\n            current_app.debug,\n            current_app.testing,\n            request.headers.get('X-Forwarded-Proto', 'http') == 'https'\n        ]\n\n        if not any(criteria) and not self.skip:\n            if request.url.startswith('http://'):\n                url = request.url.replace('http://', 'https://', 1)\n                code = 302\n                if self.permanent:\n                    code = 301\n                r = redirect(url, code=code)\n                return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets hue to set duration in ms", "response": "def set_hue(self, hue, duration=0, rapid=False):\n        \"\"\" hue to set\n            duration in ms\"\"\"\n        color = self.get_color()\n        color2 = (hue, color[1], color[2], color[3])\n        try:\n            if rapid:\n                self.fire_and_forget(LightSetColor, {\"color\": color2, \"duration\": duration}, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetColor, {\"color\": color2, \"duration\": duration})\n        except WorkflowException as e:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_saturation(self, saturation, duration=0, rapid=False):\n        color = self.get_color()\n        color2 = (color[0], saturation, color[2], color[3])\n        try:\n            if rapid:\n                self.fire_and_forget(LightSetColor, {\"color\": color2, \"duration\": duration}, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetColor, {\"color\": color2, \"duration\": duration})\n        except WorkflowException as e:\n            raise", "response": "set saturation to set\n            duration in ms"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_brightness(self, brightness, duration=0, rapid=False):\n        color = self.get_color()\n        color2 = (color[0], color[1], brightness, color[3])\n        try:\n            if rapid:\n                self.fire_and_forget(LightSetColor, {\"color\": color2, \"duration\": duration}, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetColor, {\"color\": color2, \"duration\": duration})\n        except WorkflowException as e:\n            raise", "response": "set the brightness of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets color temperature to set duration in ms", "response": "def set_colortemp(self, kelvin, duration=0, rapid=False):\n        \"\"\" kelvin: color temperature to set\n            duration in ms\"\"\"\n        color = self.get_color()\n        color2 = (color[0], color[1], color[2], kelvin)\n        try:\n            if rapid:\n                self.fire_and_forget(LightSetColor, {\"color\": color2, \"duration\": duration}, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetColor, {\"color\": color2, \"duration\": duration})\n        except WorkflowException as e:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rotate(edges, step):\n        step = Step(step)\n        result = set()\n        movement = {\n            \"U\": \"RFLB\", \n            \"D\": \"LFRB\", \n            \"R\": \"FUBD\", \n            \"L\": \"FDBU\", \n            \"F\": \"URDL\", \n            \"B\": \"ULDR\", \n            }[step.face]\n        movement = {\n            movement[i]: movement[(i + step.is_clockwise + (-1 * step.is_counter_clockwise) + (2 * step.is_180)) % 4]\n            for i in range(4)\n            }\n        for edge in edges:\n            if step.face not in edge:\n                result.add(edge.copy())\n            else:\n                k = (set(edge.facings.keys()) - {step.face}).pop()\n                new_edge = Edge(**{\n                    step.face: edge[step.face], \n                    movement[k]: edge[k], \n                    })\n                result.add(new_edge)\n        return result", "response": "Simulate the cube rotation by updating four edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cross_goal(state):\n        centres, edges = state\n        for edge in edges:\n            if \"D\" not in edge.facings:\n                return False\n            if edge[\"D\"] != centres[\"D\"][\"D\"]:\n                return False\n            k = \"\".join(edge.facings.keys()).replace(\"D\", \"\")\n            if edge[k] != centres[k][k]:\n                return False\n        return True", "response": "The goal function for cross solving search."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the state value of the cross solving search.", "response": "def cross_state_value(state):\n        \"\"\"\n        Compute the state value of the cross solving search.\n        \"\"\"\n        centres, edges = state\n        value = 0\n        for edge in edges:\n            if \"U\" in edge:\n                if edge[\"U\"] == centres[\"D\"][\"D\"]:\n                    value += 1\n                else:\n                    value += 2\n            elif \"D\" in edge:\n                if edge[\"D\"] != centres[\"D\"][\"D\"]:\n                    value += 3\n            else:\n                value += 1\n        edgeposes = {}\n        counts = {f: 0 for f in \"LFRB\"}\n        ngedges = []\n        for edge in edges:\n            if \"U\" in edge and edge[\"U\"] == centres[\"D\"][\"D\"]:\n                k = \"\".join(edge.facings.keys()).replace(\"U\", \"\")\n                edgeposes[k] = edge[k]\n                counts[k] += 1\n            elif \"D\" in edge and edge[\"D\"] == centres[\"D\"][\"D\"]:\n                k = \"\".join(edge.facings.keys()).replace(\"D\", \"\")\n                edgeposes[k] = edge[k]\n                counts[k] += 1\n            elif \"U\" in edge or \"D\" in edge:\n                ngedges.append(edge)\n            else:\n                for k, s in edge:\n                    if s != centres[\"D\"][\"D\"]:\n                        edgeposes[k] = s\n                        counts[k] += 1\n                        break\n        for edge in ngedges:\n            idx = \"LFRB\".index(edge[centres[\"D\"].colour])\n            for i in [-1, 1]:\n                if \"LFRB\"[(idx+1)%4] not in edgeposes:\n                    k = \"\".join(edge.facings.keys()).replace(\"LFRB\"[idx], \"\")\n                    edgeposes[\"LFRB\"[(idx+1)%4]] = edge[k]\n                    counts[\"LFRB\"[(idx+1)%4]] += 1\n                    break\n            else:\n                k = \"\".join(edge.facings.keys()).replace(\"LFRB\"[idx], \"\")\n                if counts[\"LFRB\"[(idx-1)%4]] > counts[\"LFRB\"[(idx+1)%4]]:\n                    edgeposes[\"LFRB\"[(idx-1)%4]] = edge[k]\n                else:\n                    edgeposes[\"LFRB\"[(idx+1)%4]] = edge[k]\n        relative_pos = {f: centres[f][f] for f in \"LFRB\"}\n        if len(edgeposes) == 4:\n            for i in range(4):\n                edgeposes[\"L\"], edgeposes[\"F\"], edgeposes[\"R\"], edgeposes[\"B\"] = \\\n                    edgeposes[\"F\"], edgeposes[\"R\"], edgeposes[\"B\"], edgeposes[\"L\"]\n                if edgeposes == relative_pos:\n                    break\n            else:\n                value += 5\n        else:\n            value += 3\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_solved(self):\n        return self.cross_goal(({f: self.cube[f] for f in \"LUFDRB\"}, \n            self.cube.select_type(\"edge\") & self.cube.has_colour(self.cube[\"D\"].colour)))", "response": "Check if the cross of Cube is solved."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrecognise the PLL case of Cube.", "response": "def recognise(self):\n        \"\"\"\n        Recognise the PLL case of Cube.\n        \"\"\"\n        result = \"\"\n        for side in \"LFRB\":\n            for square in self.cube.get_face(side)[0]:\n                for _side in \"LFRB\":\n                    if square.colour == self.cube[_side].colour:\n                        result += _side\n                        break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solve(self):\n        if not isinstance(self.cube, Cube):\n            raise ValueError(\"Use Solver.feed(cube) to feed the cube to solver.\")\n        for i in range(4):\n            rec_id = self.recognise()\n            if rec_id in algo_dict:\n                self.cube(algo_dict[rec_id])\n                return Formula((Step(\"y\") * i) or []) + algo_dict[rec_id]\n            self.cube(Step(\"y\"))\n        raise ValueError(\"Invalid cube.\")", "response": "Solve PLL of Cube."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_solved(self):\n        for side in \"LUFDRB\":\n            sample = self.cube[side].facings[side]\n            for square in sum(self.cube.get_face(side), []):\n                if square != sample:\n                    return False\n        return True", "response": "Check if Cube is solved."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recognise(self):\n        if not isinstance(self.cube, Cube):\n            raise ValueError(\"Use Solver.feed(cube) to feed the cube to solver.\")\n        result = \"\"\n        for face in \"LFRB\":\n            for square in self.cube.get_face(face)[0]:\n                result += str(int(square == self.cube[\"U\"][\"U\"]))\n        if result not in algo_dict:\n            raise ValueError(\"Invalid Cube, probably didn't solve F2L, or wrong input value.\\nUse Solver.feed(cube) to reset the cube.\")\n        self.case = result\n        return result", "response": "Recognise which is Cube s OLL case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef solve(self):\n        if not isinstance(self.cube, Cube):\n            raise ValueError(\"Use Solver.feed(cube) to feed the cube to solver.\")\n        self.recognise()\n        self.cube(algo_dict[self.case])\n        return algo_dict[self.case]", "response": "Solve the OLL. Returns an Formula."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef feed(self, cube, pair):\n        self.cube = cube\n        if pair not in [\"FR\", \"RB\", \"BL\", \"LF\"]:\n            pair = [\"FR\", \"RB\", \"BL\", \"LF\"][[\"RF\", \"BR\", \"LB\", \"FL\"].index(pair)]\n        self.pair = pair", "response": "Feeds the current Cube to the solver."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_pair(self):\n        colours = (\n            self.cube[self.pair[0]].colour, \n            self.cube[self.pair[1]].colour, \n            self.cube[\"D\"].colour\n            )\n        result_corner = self.cube.children.copy()\n        for c in colours[:2]:\n            result_corner &= self.cube.has_colour(c)\n        result_edge = result_corner & self.cube.select_type(\"edge\")\n        result_corner &= self.cube.has_colour(colours[2])\n        return (list(result_corner)[0], list(result_edge)[0])", "response": "Get the F2L pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimated_position(self):\n        corner = {\"D\":self.cube[\"D\"][\"D\"]}\n        edge = {}\n        for cubie in (corner, edge):\n            for face in self.pair:\n                cubie.update({face:self.cube[face][face]})\n        return (Corner(**corner), Edge(**edge))", "response": "Get the estimated position of the pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_slot(self):\n        corner, edge = self.get_pair()\n        corner_slot, edge_slot = corner.location.replace(\"D\", \"\", 1), edge.location\n        if \"U\" not in corner_slot and corner_slot not in [\"FR\", \"RB\", \"BL\", \"LF\"]:\n            corner_slot = [\"FR\", \"RB\", \"BL\", \"LF\"][[\"RF\", \"BR\", \"LB\", \"FL\"].index(corner_slot)]\n        if \"U\" not in edge_slot and edge_slot not in [\"FR\", \"RB\", \"BL\", \"LF\"]:\n            edge_slot = [\"FR\", \"RB\", \"BL\", \"LF\"][[\"RF\", \"BR\", \"LB\", \"FL\"].index(edge_slot)]\n        if \"U\" in corner_slot and \"U\" in edge_slot:\n            return (\"SLOTFREE\", (None, None), (corner, edge))\n        if \"U\" in corner_slot:\n            return (\"CSLOTFREE\", (None, edge_slot), (corner, edge))\n        if \"U\" in edge_slot:\n            return (\"ESLOTFREE\", (corner_slot, None), (corner, edge))\n        if corner_slot not in [edge_slot, edge_slot[::-1]]:\n            return (\"DIFFSLOT\", (corner_slot, edge_slot), (corner, edge))\n        if (corner, edge) == self.estimated_position():\n            return (\"SOLVED\", (corner_slot, edge_slot), (corner, edge))\n        return (\"WRONGSLOT\", (corner_slot, edge_slot), (corner, edge))", "response": "Get the slot name of this pair."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combining_goal(state):\n        ((corner, edge), (L, U, F, D, R, B)) = state\n        if \"U\" not in corner or \"U\" not in edge: return False\n        if set(edge).issubset(set(corner)): return True\n        elif set(edge.facings.keys()).issubset(set(corner.facings.keys())): return False\n        opposite = {\"L\":\"R\", \"R\":\"L\", \"F\":\"B\", \"B\":\"F\"}\n        edge_facings = list(edge)\n        for i, (face, square) in enumerate(edge_facings):\n            if face == \"U\":\n                if square != corner[opposite[edge_facings[(i+1)%2][0]]]:\n                    return False\n            else:\n                if square != corner[\"U\"]:\n                    return False\n        return True", "response": "Check if two Cubies are combined on the U face."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rotate(pair, step):\n        step = Step(step)\n        movement = {\n            \"U\": \"RFLB\", \n            \"D\": \"LFRB\", \n            \"R\": \"FUBD\", \n            \"L\": \"FDBU\", \n            \"F\": \"URDL\", \n            \"B\": \"ULDR\", \n            }[step.face]\n        movement = {\n            movement[i]: movement[(i + step.is_clockwise + (-1 * step.is_counter_clockwise) + (2 * step.is_180)) % 4]\n            for i in range(4)\n            }\n        for cubie in pair:\n            if step.face not in cubie:\n                if cubie.type == \"edge\":\n                    result_edge = cubie.copy()\n                else:\n                    result_corner = cubie.copy()\n            else:\n                result = {}\n                for face, square in cubie:\n                    if face not in movement:\n                        result[face] = square\n                    else:\n                        result[movement[face]] = square\n                if len(result) == 2:\n                    result_edge = Edge(**result)\n                else:\n                    result_corner = Corner(**result)\n        return (result_corner, result_edge)", "response": "Simulate the cube rotation by updating the pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding a list of successors for combining F2L pair.", "response": "def combining_successors(state, last_action=()):\n        \"\"\"\n        Successors function for finding path of combining F2L pair.\n        \"\"\"\n        ((corner, edge), (L, U, F, D, R, B)) = state\n        U_turns = [Formula(\"U\"), Formula(\"U'\"), Formula(\"U2\")] if len(last_action) != 1 else []\n        R_turns = [Formula(\"R U R'\"), Formula(\"R U' R'\"), Formula(\"R U2 R'\")] if \"R\" not in last_action else []\n        F_turns = [Formula(\"F' U F\"), Formula(\"F' U' F\"), Formula(\"F' U2 F\")] if \"F\" not in last_action else []\n        for act in (U_turns + R_turns + F_turns):\n            new = (corner, edge)\n            for q in act:\n                new = F2LPairSolver._rotate(new, q)\n            yield act, (new, (L, U, F, D, R, B))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef combining_search(self):\n        start = (\n            self.get_pair(), \n            (\n                self.cube[\"L\"], \n                self.cube[\"U\"], \n                self.cube[\"F\"], \n                self.cube[\"D\"], \n                self.cube[\"R\"], \n                self.cube[\"B\"], \n                ), \n            )\n        return sum(path_actions(a_star_search(start, \n                       self.combining_successors, \n                       lambda x: len(x), \n                       self.combining_goal)), Formula())", "response": "Search the path for combining the pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsolve the entire F2L.", "response": "def solve(self):\n        \"\"\"\n        Solve the entire F2L. (Generator)\n        \"\"\"\n        for i in range(4):\n            for slot in [\"FR\", \"RB\", \"BL\", \"LF\"]:\n                solver = F2LPairSolver(self.cube, slot)\n                if not solver.is_solved():\n                    yield tuple([self.cube[slot[i]].colour for i in range(2)]), solver.solve()\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if Cube s F2L is solved.", "response": "def is_solved(self):\n        \"\"\"\n        Check if Cube's F2L is solved.\n        \"\"\"\n        if self.cube.D == [[Square(self.cube[\"D\"].colour)] * 3] * 3:\n            for face in \"LFRB\":\n                if self.cube.get_face(face)[1:] != [[Square(self.cube[face].colour)] * 3] * 2:\n                    return False\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authorization_code(self, client_id, client_secret, code,\n                           redirect_uri, grant_type='authorization_code'):\n        \"\"\"Authorization code grant\n\n        This is the OAuth 2.0 grant that regular web apps utilize in order\n        to access an API. Use this endpoint to exchange an Authorization Code\n        for a Token.\n\n        Args:\n            grant_type (str): Denotes the flow you're using. For authorization code\n            use authorization_code\n\n            client_id (str): your application's client Id\n\n            client_secret (str): your application's client Secret\n\n            code (str): The Authorization Code received from the /authorize Calls\n\n            redirect_uri (str, optional): This is required only if it was set at\n            the GET /authorize endpoint. The values must match\n\n        Returns:\n            access_token, id_token\n        \"\"\"\n\n        return self.post(\n            'https://{}/oauth/token'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'client_secret': client_secret,\n                'code': code,\n                'grant_type': grant_type,\n                'redirect_uri': redirect_uri,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "This endpoint is used to exchange an authorization code for an API call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef client_credentials(self, client_id, client_secret, audience,\n                           grant_type='client_credentials'):\n        \"\"\"Client credentials grant\n\n        This is the OAuth 2.0 grant that server processes utilize in\n        order to access an API. Use this endpoint to directly request\n        an access_token by using the Application Credentials (a Client Id and\n        a Client Secret).\n\n        Args:\n            grant_type (str): Denotes the flow you're using. For client credentials\n            use client_credentials\n\n            client_id (str): your application's client Id\n\n            client_secret (str): your application's client Secret\n\n            audience (str): The unique identifier of the target API you want to access.\n\n        Returns:\n            access_token\n        \"\"\"\n\n        return self.post(\n            'https://{}/oauth/token'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'client_secret': client_secret,\n                'audience': audience,\n                'grant_type': grant_type,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "This endpoint returns an access token that can be used to request a client credentials for an application."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging in a user to the server.", "response": "def login(self, client_id, client_secret, username, password, scope, realm,\n              audience, grant_type='http://auth0.com/oauth/grant-type/password-realm'):\n        \"\"\"Calls oauth/token endpoint with password-realm grant type\n\n\n        This is the OAuth 2.0 grant that highly trusted apps utilize in order\n        to access an API. In this flow the end-user is asked to fill in credentials\n        (username/password) typically using an interactive form in the user-agent\n        (browser). This information is later on sent to the client and Auth0.\n        It is therefore imperative that the client is absolutely trusted with\n        this information.\n\n        Args:\n            grant_type (str): Denotes the flow you're using. For password realm\n            use http://auth0.com/oauth/grant-type/password-realm\n\n            client_id (str): your application's client Id\n\n            client_secret (str): your application's client Secret\n\n            audience (str): The unique identifier of the target API you want to access.\n\n            username (str): Resource owner's identifier\n\n            password (str): resource owner's Secret\n\n            scope(str): String value of the different scopes the client is asking for.\n            Multiple scopes are separated with whitespace.\n\n            realm (str): String value of the realm the user belongs.\n            Set this if you want to add realm support at this grant.\n\n        Returns:\n            access_token, id_token\n        \"\"\"\n\n        return self.post(\n            'https://{}/oauth/token'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'username': username,\n                'password': password,\n                'realm': realm,\n                'client_secret': client_secret,\n                'scope': scope,\n                'audience': audience,\n                'grant_type': grant_type\n            },\n            headers={'Content-Type': 'application/json'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh_token(self, client_id, client_secret, refresh_token, grant_type='refresh_token'):\n\n        return self.post(\n            'https://{}/oauth/token'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'client_secret': client_secret,\n                'refresh_token': refresh_token,\n                'grant_type': grant_type\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "Calls oauth / token endpoint with refresh token grant type and returns the new access token and id_token."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, id, fields=None, include_fields=True):\n\n        params = {'fields': fields and ','.join(fields) or None,\n                  'include_fields': str(include_fields).lower()}\n\n        return self.client.get(self._url(id), params=params)", "response": "Retrieve a single connection by id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmodify a connection object.", "response": "def update(self, id, body):\n        \"\"\"Modifies a connection.\n\n        Args:\n           id: Id of the connection.\n\n           body (dict): Specifies which fields are to be modified, and to what\n              values.\n              See: https://auth0.com/docs/api/management/v2#!/Connections/patch_connections_by_id\n\n        Returns:\n           The modified connection object.\n        \"\"\"\n\n        return self.client.patch(self._url(id), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new connection.", "response": "def create(self, body):\n        \"\"\"Creates a new connection.\n\n        Args:\n            body (dict): Attributes used to create the connection. Mandatory\n                attributes are: 'name' and 'strategy'.\n                See: https://auth0.com/docs/api/management/v2#!/Connections/post_connections\n        \"\"\"\n\n        return self.client.post(self._url(), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_user_by_email(self, id, email):\n        return self.client.delete(self._url(id) + '/users', params={'email': email})", "response": "Deletes a specified connection user by its email."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, user_id, client_id, type, fields=None, include_fields=True):\n\n        params = {\n            'fields': fields and ','.join(fields) or None,\n            'include_fields': str(include_fields).lower(),\n            'user_id': user_id,\n            'client_id': client_id,\n            'type': type,\n        }\n        return self.client.get(self._url(), params=params)", "response": "List device credentials.\n\n        Args:\n            user_id (str): The user_id of the devices to retrieve.\n\n            client_id (str): The client_id of the devices to retrieve.\n\n            type (str): The type of credentials (public_key, refresh_token).\n\n            fields (list, optional): A list of fields to include or exclude\n                (depending on include_fields) from the result, empty to\n                retrieve all fields\n\n            include_fields (bool, optional): True if the fields specified are\n                to be included in the result, False otherwise\n                (defaults to true)\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Device_Credentials/get_device_credentials"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, id):\n        url = self._url('%s' % (id))\n        return self.client.get(url)", "response": "Retrieves a specific custom domain by its ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a grant. .", "response": "def delete(self, id):\n        \"\"\"Deletes a grant.\n\n        Args:\n           id (str): The id of the custom domain to delete\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Custom_Domains/delete_custom_domains_by_id\n        \"\"\"\n        url = self._url('%s' % (id))\n        return self.client.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures a new custom domain with the given body", "response": "def create_new(self, body):\n        \"\"\"Configure a new custom domain\n\n        Args:\n           body (str): The domain, tye and verification method in json\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Custom_Domains/post_custom_domains\n        \"\"\"\n        return self.client.post(self._url(), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies a custom domain record set", "response": "def verify(self, id):\n        \"\"\"Verify a custom domain\n\n        Args:\n           id (str): The id of the custom domain to delete\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Custom_Domains/post_verify\n        \"\"\"\n        url = self._url('%s/verify' % (id))\n        return self.client.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef saml_metadata(self, client_id):\n\n        return self.get(url='https://{}/samlp/metadata/{}'.format(self.domain,\n                                                                  client_id))", "response": "Get the SAML metadata for a specific client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wsfed_metadata(self):\n\n        url = 'https://{}/wsfed/FederationMetadata' \\\n              '/2007-06/FederationMetadata.xml'\n\n        return self.get(url=url.format(self.domain))", "response": "Returns the WS - Federation Metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a list of all the applications in the cluster.", "response": "def all(self, fields=None, include_fields=True, page=None, per_page=None, extra_params=None):\n        \"\"\"Retrieves a list of all the applications.\n\n        Important: The client_secret and encryption_key attributes can only be\n        retrieved with the read:client_keys scope.\n\n        Args:\n           fields (list of str, optional): A list of fields to include or\n              exclude from the result (depending on include_fields). Empty to\n              retrieve all fields.\n\n           include_fields (bool, optional): True if the fields specified are\n              to be included in the result, False otherwise.\n\n           page (int): The result's page number (zero based).\n\n           per_page (int, optional): The amount of entries per page.\n\n           extra_params (dictionary, optional): The extra parameters to add to\n             the request. The fields, include_fields, page and per_page values\n             specified as parameters take precedence over the ones defined here.\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Clients/get_clients\n        \"\"\"\n        params = extra_params or {}\n        params['fields'] = fields and ','.join(fields) or None\n        params['include_fields'] = str(include_fields).lower()\n        params['page'] = page\n        params['per_page'] = per_page\n\n        return self.client.get(self._url(), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotate_secret(self, id):\n\n        params = {'id': id }\n\n        url = self._url('%s/rotate-secret' % id)\n        return self.client.get(url, params=params)", "response": "Rotate a client secret."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts flow sending an email.", "response": "def email(self, client_id, email, send='link', auth_params=None):\n        \"\"\"Start flow sending an email.\n\n        Given the user email address, it will send an email with:\n\n          - A link (default, send:\"link\"). You can then authenticate with\n            this user opening the link and he will be automatically logged in\n            to the application. Optionally, you can append/override\n            parameters to the link (like scope, redirect_uri, protocol,\n            response_type, etc.) using auth_params dict.\n\n          - A verification code (send:\"code\"). You can then authenticate with\n            this user using email as username and code as password.\n\n        Args:\n            client_id (str): Client Id of the application.\n\n            email (str): Email address.\n\n            send (str, optional): Can be: 'link' or 'code'. Defaults to 'link'.\n\n            auth_params (dict, optional): Parameters to append or override.\n        \"\"\"\n\n        return self.post(\n            'https://{}/passwordless/start'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'connection': 'email',\n                'email': email,\n                'send': send,\n                'authParams': auth_params\n            },\n            headers={'Content-Type': 'application/json'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sms(self, client_id, phone_number):\n\n        return self.post(\n            'https://{}/passwordless/start'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'connection': 'sms',\n                'phone_number': phone_number,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "Start flow sending an SMS message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlogin using phone number verification code.", "response": "def sms_login(self, client_id, phone_number, code, scope='openid'):\n        \"\"\"Login using phone number/verification code.\n        \"\"\"\n\n        return self.post(\n            'https://{}/oauth/ro'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'connection': 'sms',\n                'grant_type': 'password',\n                'username': phone_number,\n                'password': code,\n                'scope': scope,\n            },\n            headers={'Content-Type': 'application/json'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authorize(self, client_id, audience=None, state=None, redirect_uri=None,\n                  response_type='code', scope='openid'):\n        \"\"\"Authorization code grant\n\n        This is the OAuth 2.0 grant that regular web apps utilize in order to access an API.\n        \"\"\"\n        params = {\n            'client_id': client_id,\n            'audience': audience,\n            'response_type': response_type,\n            'scope': scope,\n            'state': state,\n            'redirect_uri': redirect_uri\n        }\n\n        return self.get(\n            'https://{}/authorize'.format(self.domain),\n            params=params)", "response": "This method returns the response from the OAuth 2. 0 authorization grant that can be used to access an API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all(self, page=None, per_page=None, include_totals=False):\n\n        params = {\n            'page': page,\n            'per_page': per_page,\n            'include_totals': str(include_totals).lower()\n        }\n\n        return self.client.get(self._url(), params=params)", "response": "Retrieves all resource servers in a given page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_by_identifier(self, identifier):\n\n        params = {'identifier': identifier}\n\n        return self.client.get(self._url(), params=params)", "response": "Gets blocks by identifier"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunblock the user_blocks record with the given identifier.", "response": "def unblock_by_identifier(self, identifier):\n        \"\"\"Unblocks by identifier\n\n        Args:\n           identifier (str): Should be any of: username, phone_number, email.\n\n        See: https://auth0.com/docs/api/management/v2#!/User_Blocks/delete_user_blocks\n        \"\"\"\n\n        params = {'identifier': identifier}\n\n        return self.client.delete(self._url(), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nobtains a delegation token.", "response": "def get_token(self, client_id, target, api_type, grant_type,\n                  id_token=None, refresh_token=None, scope='openid'):\n\n        \"\"\"Obtain a delegation token.\n        \"\"\"\n\n        if id_token and refresh_token:\n            raise ValueError('Only one of id_token or refresh_token '\n                             'can be None')\n\n        data = {\n            'client_id': client_id,\n            'grant_type': grant_type,\n            'target': target,\n            'scope': scope,\n            'api_type': api_type,\n        }\n\n        if id_token:\n            data.update({'id_token': id_token})\n        elif refresh_token:\n            data.update({'refresh_token': refresh_token})\n        else:\n            raise ValueError('Either id_token or refresh_token must '\n                             'have a value')\n\n        return self.post(\n            'https://{}/delegation'.format(self.domain),\n            headers={'Content-Type': 'application/json'},\n            data=data\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating Guardian factor by name.", "response": "def update_factor(self, name, body):\n        \"\"\"Update Guardian factor\n        Useful to enable / disable factor\n\n        Args:\n            name (str): Either push-notification or sms\n            body (dict): Attributes to modify.\n            See: https://auth0.com/docs/api/management/v2#!/Guardian/put_factors_by_name\n        \"\"\"\n        url = self._url('factors/{}'.format(name))\n        return self.client.put(url, data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the SMS templates.", "response": "def update_templates(self, body):\n        \"\"\"Update enrollment and verification SMS templates.\n\n        Useful to send custom messages on sms enrollment and verification\n\n        Args:\n            body (dict): Attributes to modify.\n            See: https://auth0.com/docs/api/management/v2#!/Guardian/put_templates\n        \"\"\"\n\n        return self.client.put(self._url('factors/sms/templates'), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_enrollment(self, id):\n        url = self._url('enrollments/{}'.format(id))\n        return self.client.get(url)", "response": "Retrieves an enrollment. A user can check its type and related metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_enrollment(self, id):\n        url = self._url('enrollments/{}'.format(id))\n        return self.client.delete(url)", "response": "Deletes an enrollment.\n\n        Useful when you want to force re-enroll.\n\n        Args:\n           id (str): The id of the device account to update\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Guardian/delete_enrollments_by_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an enrollment ticket for user_id", "response": "def create_enrollment_ticket(self, body):\n        \"\"\"Creates an enrollment ticket for user_id\n\n        A useful way to send an email to a user, with a link that lead to\n            start the enrollment process\n\n        Args:\n            body (dict): Details of the user to send the ticket to.\n            See: https://auth0.com/docs/api/management/v2#!/Guardian/post_ticket\n        \"\"\"\n        return self.client.post(self._url('enrollments/ticket'), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget Guardian SNS or SMS factor providers.", "response": "def get_factor_providers(self, factor_name, name):\n        \"\"\"Get Guardian SNS or SMS factor providers.\n\n        Returns provider configuration\n        Args:\n           factor_name (str): Either push-notification or sms\n           name (str): Name of the provider\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Guardian/get_sns\n             https://auth0.com/docs/api/management/v2#!/Guardian/get_twilio\n\"\"\"\n        url = self._url('factors/{}/providers/{}'.format(factor_name, name))\n        return self.client.get(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_factor_providers(self, factor_name, name, body):\n        url = self._url('factors/{}/providers/{}'.format(factor_name, name))\n        return self.client.put(url, data=body)", "response": "Update Guardian factor providers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrevoke a Refresh Token if it has been compromised.", "response": "def revoke_refresh_token(self, client_id, token, client_secret=None):\n        \"\"\"Revokes a Refresh Token if it has been compromised\n           \n           Each revocation request invalidates not only the specific token, but all other tokens \n           based on the same authorization grant. This means that all Refresh Tokens that have \n           been issued for the same user, application, and audience will be revoked.\n\n           Args:\n                client_id (str): The Client ID for your Application\n\n                token (str): The Refresh Token you want to revoke\n\n                client_secret (str, optional): The Client Secret for your Application.\n                        Required for confidential applications.\n                        See: https://auth0.com/docs/applications/application-types#confidential-applications\n            \n            See: https://auth0.com/docs/api/authentication#refresh-token\n        \"\"\"\n        body = {\n            'client_id': client_id,\n            'token': token,\n            'client_secret': client_secret\n        }\n\n        return self.post(\n            'https://{}/oauth/revoke'.format(self.domain), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unset(self, key):\n        params = {\n            'key': key\n        }\n        return self.client.delete(self._url(), params=params)", "response": "Removes the rules config for a given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the rules config for a given key.", "response": "def set(self, key, value):\n        \"\"\"Sets the rules config for a given key.\n\n        Args:\n            key (str): rules config key to set\n\n            value (str): value to set for the rules config key\n\n        See: https://auth0.com/docs/api/management/v2#!/Rules_Configs/put_rules_configs_by_key\n        \"\"\"\n        url = self._url('{}'.format(key))\n        body = {'value': value}\n        return self.client.put(url, data=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef daily_stats(self, from_date=None, to_date=None):\n\n        return self.client.get(self._url('daily'), params={'from': from_date,\n                                                           'to': to_date})", "response": "Gets the daily stats for a particular period."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_email_verification(self, body):\n        return self.client.post(self._url('email-verification'), data=body)", "response": "Create an email verification ticket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_pswd_change(self, body):\n        return self.client.post(self._url('password-change'), data=body)", "response": "Create password change ticket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching log events in the CanonJS API.", "response": "def search(self, page=0, per_page=50, sort=None, q=None,\n               include_totals=True, fields=None, from_param=None, take=None,\n               include_fields=True):\n        \"\"\"Search log events.\n\n        Args:\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            sort (str, optional): The field to use for sorting.\n                1 == ascending and -1 == descending. (e.g: date:1)\n\n            q (str, optional): Query in Lucene query string syntax.\n\n            fields (list of str, optional): A list of fields to include or\n                exclude from the result (depending on include_fields). Empty to\n                retrieve all fields.\n\n            include_fields (bool, optional): True if the fields specified are\n                to be included in the result, False otherwise.\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n            from_param (str, optional): Log Event Id to start retrieving logs. You can\n                limit the amount of logs using the take parameter\n\n            take (int, optional): The total amount of entries to retrieve when\n                using the from parameter.\n\n        https://auth0.com/docs/api/management/v2#!/Logs/get_logs\n        \"\"\"\n        params = {\n            'per_page': per_page,\n            'page': page,\n            'include_totals': str(include_totals).lower(),\n            'sort': sort,\n            'fields': fields and ','.join(fields) or None,\n            'include_fields': str(include_fields).lower(),\n            'q': q,\n            'from': from_param,\n            'take': take\n        }\n        return self.client.get(self._url(), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving all client grants for a given audience page per page and client id.", "response": "def all(self, audience=None, page=None, per_page=None, include_totals=False, client_id=None):\n        \"\"\"Retrieves all client grants.\n\n        Args:\n            audience (str, optional): URL encoded audience of a Resource Server\n                to filter\n\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n            client_id (string, optional): The id of a client to filter\n\n        See: https://auth0.com/docs/api/management/v2#!/Client_Grants/get_client_grants\n        \"\"\"\n\n        params = {\n            'audience': audience,\n            'page': page,\n            'per_page': per_page,\n            'include_totals': str(include_totals).lower(),\n            'client_id': client_id,\n        }\n\n        return self.client.get(self._url(), params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login(self, client_id, access_token, connection, scope='openid'):\n\n        return self.post(\n            'https://{}/oauth/access_token'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'access_token': access_token,\n                'connection': connection,\n                'scope': scope,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "Login using a social provider s access token and return a dict with the access_token and id_token keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, aud=None):\n\n        params = {\n            'aud': aud\n        }\n\n        return self.client.get(self.url, params=params)", "response": "Retrieves the jti and aud of all tokens in the blacklist."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a token to the blacklist.", "response": "def create(self, jti, aud=''):\n        \"\"\"Adds a token to the blacklist.\n\n        Args:\n            jti (str): the jti of the JWT to blacklist.\n            aud (str, optional): The JWT's aud claim. The client_id of the\n                application for which it was issued.\n\n            body (dict):\n            \tSee: https://auth0.com/docs/api/management/v2#!/Blacklists/post_tokens\n        \"\"\"\n\n        return self.client.post(self.url, data={'jti': jti, 'aud': aud})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, page=0, per_page=25, sort=None, connection=None, q=None,\n             search_engine=None, include_totals=True, fields=None,\n             include_fields=True):\n        \"\"\"List or search users.\n\n        Args:\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            sort (str, optional): The field to use for sorting.\n                1 == ascending and -1 == descending. (e.g: email:1)\n\n            connection (str, optional): Connection filter.\n\n            q (str, optional): Query in Lucene query string syntax. Only fields\n                in app_metadata, user_metadata or the normalized user profile\n                are searchable.\n\n            search_engine (str, optional): The version of the search_engine to use\n                when querying for users. Will default to the latest version available.\n                See: https://auth0.com/docs/users/search\n\n            fields (list of str, optional): A list of fields to include or\n                exclude from the result (depending on include_fields). Empty to\n                retrieve all fields.\n\n            include_fields (bool, optional): True if the fields specified are\n                to be include in the result, False otherwise.\n\n        See: https://auth0.com/docs/api/management/v2#!/Users/get_users\n        \"\"\"\n        params = {\n            'per_page': per_page,\n            'page': page,\n            'include_totals': str(include_totals).lower(),\n            'sort': sort,\n            'connection': connection,\n            'fields': fields and ','.join(fields) or None,\n            'include_fields': str(include_fields).lower(),\n            'q': q,\n            'search_engine': search_engine\n        }\n        return self.client.get(self._url(), params=params)", "response": "List or search users."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_multifactor(self, id, provider):\n        url = self._url('{}/multifactor/{}'.format(id, provider))\n        return self.client.delete(url)", "response": "Delete a user s multifactor provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlink_user_account(self, id, provider, user_id):\n        url = self._url('{}/identities/{}/{}'.format(id, provider, user_id))\n        return self.client.delete(url)", "response": "Unlink a user account by user id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef link_user_account(self, user_id, body):\n        url = self._url('{}/identities'.format(user_id))\n        return self.client.post(url, data=body)", "response": "Link the user account specified by the id param of the URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef regenerate_recovery_code(self, user_id):\n        url = self._url('{}/recovery-code-regeneration'.format(user_id))\n        return self.client.post(url)", "response": "Removes the current recovery token generates and returns a new one\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve all Guardian enrollments for a user.", "response": "def get_guardian_enrollments(self, user_id):\n        \"\"\"Retrieves all Guardian enrollments.\n\n        Args:\n            user_id (str):  The user_id of the user to retrieve\n\n        See: https://auth0.com/docs/api/management/v2#!/Users/get_enrollments\n        \"\"\"\n        url = self._url('{}/enrollments'.format(user_id))\n        return self.client.get(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve every log event for a specific user id.", "response": "def get_log_events(self, user_id, page=0, per_page=50, sort=None,\n                       include_totals=False):\n        \"\"\"Retrieve every log event for a specific user id\n\n        Args:\n            user_id (str):  The user_id of the logs to retrieve\n\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n                Default: 50. Max value: 100\n\n            sort (str, optional):  The field to use for sorting. Use field:order\n                where order is 1 for ascending and -1 for descending.\n                For example date:-1\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n            See: https://auth0.com/docs/api/management/v2#!/Users/get_logs_by_user\n        \"\"\"\n\n        params = {\n            'per_page': per_page,\n            'page': page,\n            'include_totals': str(include_totals).lower(),\n            'sort': sort\n        }\n\n        url = self._url('{}/logs'.format(user_id))\n        return self.client.get(url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all(self, stage='login_success', enabled=True, fields=None,\n            include_fields=True, page=None, per_page=None, include_totals=False):\n        \"\"\"Retrieves a list of all rules.\n\n        Args:\n            stage (str, optional):  Retrieves rules that match the execution\n                stage (defaults to login_success).\n\n            enabled (bool, optional): If provided, retrieves rules that match\n                the value, otherwise all rules are retrieved.\n\n            fields (list, optional): A list of fields to include or exclude\n                (depending on include_fields) from the result, empty to\n                retrieve all fields.\n\n            include_fields (bool, optional): True if the fields specified are\n                to be included in the result, False otherwise\n                (defaults to true).\n\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n        See: https://auth0.com/docs/api/management/v2#!/Rules/get_rules\n        \"\"\"\n\n        params = {\n            'stage': stage,\n            'fields': fields and ','.join(fields) or None,\n            'include_fields': str(include_fields).lower(),\n            'page': page,\n            'per_page': per_page,\n            'include_totals': str(include_totals).lower()\n        }\n\n        # since the default is True, this is here to disable the filter\n        if enabled is not None:\n            params['enabled'] = str(enabled).lower()\n\n        return self.client.get(self._url(), params=params)", "response": "Retrieves a list of all rules in the specified stage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_failed_job(self, id):\n        url = self._url('{}/errors'.format(id))\n        return self.client.get(url)", "response": "Get the failed job error details"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_results(self, job_id):\n        url = self._url('%s/results' % job_id)\n        return self.client.get(url)", "response": "Get the results of a job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_users(self, body):\n        return self.client.post(self._url('users-exports'), data=body)", "response": "Export all users to a file using a long running job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_users(self, connection_id, file_obj, upsert=False):\n        return self.client.file_post(self._url('users-imports'),\n                                     data={'connection_id': connection_id, 'upsert': str(upsert).lower()},\n                                     files={'users': file_obj})", "response": "Imports users to a connection from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends an email to the specified user that asks them to click a link to the verify their email address.", "response": "def send_verification_email(self, body):\n        \"\"\"Send verification email.\n\n        Send an email to the specified user that asks them to click a link to\n        verify their email address.\n\n        Args:\n            body (dict): Please see: https://auth0.com/docs/api/v2#!/Jobs/post_verification_email\n        \"\"\"\n        return self.client.post(self._url('verification-email'), data=body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef config(self, body):\n        return self.client.post(self._url(), data=body)", "response": "Configure the email provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef userinfo(self, access_token):\n\n        \"\"\"Returns the user information based on the Auth0 access token.\n        This endpoint will work only if openid was granted as a scope for the access_token.\n\n        Args:\n            access_token (str): Auth0 access token (obtained during login).\n\n        Returns:\n            The user profile.\n        \"\"\"\n\n        return self.get(\n            url='https://{}/userinfo'.format(self.domain),\n            headers={'Authorization': 'Bearer {}'.format(access_token)}\n        )", "response": "Returns the user information based on the Auth0 access token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the user profile based on the user s jwt", "response": "def tokeninfo(self, jwt):\n\n        \"\"\"Returns user profile based on the user's jwt\n\n        Validates a JSON Web Token (signature and expiration) and returns the\n        user information associated with the user id (sub property) of\n        the token.\n\n        Args:\n            jwt (str): User's jwt\n\n        Returns:\n            The user profile.\n        \"\"\"\n        warnings.warn(\"/tokeninfo will be deprecated in future releases\", DeprecationWarning)\n        return self.post(\n            url='https://{}/tokeninfo'.format(self.domain),\n            data={'id_token': jwt},\n            headers={'Content-Type': 'application/json'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all(self, page=None, per_page=None, include_totals=False, extra_params=None):\n        params = extra_params or {}\n        params.update({\n            'page': page,\n            'per_page': per_page,\n            'include_totals': str(include_totals).lower()\n        })\n\n        return self.client.get(self._url(), params=params)", "response": "Retrieves all grants.\n\n        Args:\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n           extra_params (dictionary, optional): The extra parameters to add to\n             the request. The page, per_page, and include_totals values\n             specified as parameters take precedence over the ones defined here.\n            \n        See: https://auth0.com/docs/api/management/v2#!/Grants/get_grants"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef login(self, client_id, username, password, connection, id_token=None,\n              grant_type='password', device=None, scope='openid'):\n        \"\"\"Login using username and password\n\n        Given the user credentials and the connection specified, it will do\n        the authentication on the provider and return a dict with the\n        access_token and id_token. This endpoint only works for database\n        connections, passwordless connections, Active Directory/LDAP,\n        Windows Azure AD and ADFS.\n        \"\"\"\n        warnings.warn(\"/oauth/ro will be deprecated in future releases\", DeprecationWarning)\n        return self.post(\n            'https://{}/oauth/ro'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'username': username,\n                'password': password,\n                'id_token': id_token,\n                'connection': connection,\n                'device': device,\n                'grant_type': grant_type,\n                'scope': scope,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "Login using username and password"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_password(self, client_id, email, connection, password=None):\n\n        return self.post(\n            'https://{}/dbconnections/change_password'.format(self.domain),\n            data={\n                'client_id': client_id,\n                'email': email,\n                'password': password,\n                'connection': connection,\n            },\n            headers={'Content-Type': 'application/json'}\n        )", "response": "Asks to change a password for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary with keyword arguments that can be used to build the cffi package.", "response": "def keywords_with_side_effects(argv):\n    \"\"\"\n    Get a dictionary with setup keywords that (can) have side effects.\n\n    :param argv: A list of strings with command line arguments.\n    :returns: A dictionary with keyword arguments for the ``setup()`` function.\n\n    This setup.py script uses the setuptools 'setup_requires' feature because\n    this is required by the cffi package to compile extension modules. The\n    purpose of ``keywords_with_side_effects()`` is to avoid triggering the cffi\n    build process as a result of setup.py invocations that don't need the cffi\n    module to be built (setup.py serves the dual purpose of exposing package\n    metadata).\n\n    All of the options listed by ``python setup.py --help`` that print\n    information should be recognized here. The commands ``clean``,\n    ``egg_info``, ``register``, ``sdist`` and ``upload`` are also recognized.\n    Any combination of these options and commands is also supported.\n\n    This function was originally based on the `setup.py script`_ of SciPy (see\n    also the discussion in `pip issue #25`_).\n\n    .. _pip issue #25: https://github.com/pypa/pip/issues/25\n    .. _setup.py script: https://github.com/scipy/scipy/blob/master/setup.py\n    \"\"\"\n    no_setup_requires_arguments = (\n        '-h', '--help',\n        '-n', '--dry-run',\n        '-q', '--quiet',\n        '-v', '--verbose',\n        '-V', '--version',\n        '--author',\n        '--author-email',\n        '--classifiers',\n        '--contact',\n        '--contact-email',\n        '--description',\n        '--egg-base',\n        '--fullname',\n        '--help-commands',\n        '--keywords',\n        '--licence',\n        '--license',\n        '--long-description',\n        '--maintainer',\n        '--maintainer-email',\n        '--name',\n        '--no-user-cfg',\n        '--obsoletes',\n        '--platforms',\n        '--provides',\n        '--requires',\n        '--url',\n        'clean',\n        'egg_info',\n        'register',\n        'sdist',\n        'upload',\n    )\n\n    def is_short_option(argument):\n        \"\"\"Check whether a command line argument is a short option.\"\"\"\n        return len(argument) >= 2 and argument[0] == '-' and argument[1] != '-'\n\n    def expand_short_options(argument):\n        \"\"\"Expand combined short options into canonical short options.\"\"\"\n        return ('-' + char for char in argument[1:])\n\n    def argument_without_setup_requirements(argv, i):\n        \"\"\"Check whether a command line argument needs setup requirements.\"\"\"\n        if argv[i] in no_setup_requires_arguments:\n            # Simple case: An argument which is either an option or a command\n            # which doesn't need setup requirements.\n            return True\n        elif (is_short_option(argv[i]) and\n              all(option in no_setup_requires_arguments\n                  for option in expand_short_options(argv[i]))):\n            # Not so simple case: Combined short options none of which need\n            # setup requirements.\n            return True\n        elif argv[i - 1:i] == ['--egg-base']:\n            # Tricky case: --egg-info takes an argument which should not make\n            # us use setup_requires (defeating the purpose of this code).\n            return True\n        else:\n            return False\n\n    if all(argument_without_setup_requirements(argv, i)\n           for i in range(1, len(argv))):\n        return {\n            \"cmdclass\": {\n                \"build\": DummyCFFIBuild,\n                \"install\": DummyCFFIInstall,\n                \"test\": DummyPyTest,\n            }\n        }\n    else:\n        return {\n            \"setup_requires\": [CFFI_DEPENDENCY],\n            \"cmdclass\": {\n                \"test\": PyTest,\n            },\n            \"cffi_modules\": CFFI_MODULES,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencrypt according to the selected encryption and hashing functions.", "response": "def encrypt(self, k, a, m):\n        \"\"\" Encrypt according to the selected encryption and hashing\n        functions.\n\n        :param k: Encryption key (optional)\n        :param a: Additional Authentication Data\n        :param m: Plaintext\n\n        Returns a dictionary with the computed data.\n        \"\"\"\n        hkey = k[:_inbytes(self.keysize)]\n        ekey = k[_inbytes(self.keysize):]\n\n        # encrypt\n        iv = _randombits(self.blocksize)\n        cipher = Cipher(algorithms.AES(ekey), modes.CBC(iv),\n                        backend=self.backend)\n        encryptor = cipher.encryptor()\n        padder = PKCS7(self.blocksize).padder()\n        padded_data = padder.update(m) + padder.finalize()\n        e = encryptor.update(padded_data) + encryptor.finalize()\n\n        # mac\n        t = self._mac(hkey, a, iv, e)\n\n        return (iv, e, t)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrypt(self, k, a, iv, e, t):\n        hkey = k[:_inbytes(self.keysize)]\n        dkey = k[_inbytes(self.keysize):]\n\n        # verify mac\n        if not constant_time.bytes_eq(t, self._mac(hkey, a, iv, e)):\n            raise InvalidSignature('Failed to verify MAC')\n\n        # decrypt\n        cipher = Cipher(algorithms.AES(dkey), modes.CBC(iv),\n                        backend=self.backend)\n        decryptor = cipher.decryptor()\n        d = decryptor.update(e) + decryptor.finalize()\n        unpadder = PKCS7(self.blocksize).unpadder()\n        return unpadder.update(d) + unpadder.finalize()", "response": "Decrypt according to the selected encryption and hashing\n        functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencrypt accoriding to the selected encryption key and additional authentication data.", "response": "def encrypt(self, k, a, m):\n        \"\"\" Encrypt accoriding to the selected encryption and hashing\n        functions.\n\n        :param k: Encryption key (optional)\n        :param a: Additional Authentication Data\n        :param m: Plaintext\n\n        Returns a dictionary with the computed data.\n        \"\"\"\n        iv = _randombits(96)\n        cipher = Cipher(algorithms.AES(k), modes.GCM(iv),\n                        backend=self.backend)\n        encryptor = cipher.encryptor()\n        encryptor.authenticate_additional_data(a)\n        e = encryptor.update(m) + encryptor.finalize()\n\n        return (iv, e, encryptor.tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decrypt(self, k, a, iv, e, t):\n        cipher = Cipher(algorithms.AES(k), modes.GCM(iv, t),\n                        backend=self.backend)\n        decryptor = cipher.decryptor()\n        decryptor.authenticate_additional_data(a)\n        return decryptor.update(e) + decryptor.finalize()", "response": "Decrypt the given encryption key and additional authenticated data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new object from the standard JSON format.", "response": "def from_json(cls, key):\n        \"\"\"Creates a RFC 7517 JWK from the standard JSON format.\n\n        :param key: The RFC 7517 representation of a JWK.\n        \"\"\"\n        obj = cls()\n        try:\n            jkey = json_decode(key)\n        except Exception as e:  # pylint: disable=broad-except\n            raise InvalidJWKValue(e)\n        obj.import_key(**jkey)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the key in the standard JSON format.", "response": "def export(self, private_key=True):\n        \"\"\"Exports the key in the standard JSON format.\n        Exports the key regardless of type, if private_key is False\n        and the key is_symmetric an exceptionis raised.\n\n        :param private_key(bool): Whether to export the private key.\n                                  Defaults to True.\n        \"\"\"\n        if private_key is True:\n            # Use _export_all for backwards compatibility, as this\n            # function allows to export symmetrict keys too\n            return self._export_all()\n        else:\n            return self.export_public()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_curve(self, arg):\n        k = self._key\n        if self._params['kty'] not in ['EC', 'OKP']:\n            raise InvalidJWKType('Not an EC or OKP key')\n        if arg and k['crv'] != arg:\n            raise InvalidJWKValue('Curve requested is \"%s\", but '\n                                  'key curve is \"%s\"' % (arg, k['crv']))\n\n        return self._get_curve_by_name(k['crv'])", "response": "Gets the Elliptic Curve associated with the key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the key object associated to the requested operation.", "response": "def get_op_key(self, operation=None, arg=None):\n        \"\"\"Get the key object associated to the requested opration.\n        For example the public RSA key for the 'verify' operation or\n        the private EC key for the 'decrypt' operation.\n\n        :param operation: The requested operation.\n         The valid set of operations is availble in the\n         :data:`JWKOperationsRegistry` registry.\n        :param arg: an optional, context specific, argument\n         For example a curve name.\n\n        :raises InvalidJWKOperation: if the operation is unknown or\n         not permitted with this key.\n        :raises InvalidJWKUsage: if the use constraints do not permit\n         the operation.\n        \"\"\"\n        validops = self._params.get('key_ops',\n                                    list(JWKOperationsRegistry.keys()))\n        if validops is not list:\n            validops = [validops]\n        if operation is None:\n            if self._params['kty'] == 'oct':\n                return self._key['k']\n            raise InvalidJWKOperation(operation, validops)\n        elif operation == 'sign':\n            self._check_constraints('sig', operation)\n            return self._get_private_key(arg)\n        elif operation == 'verify':\n            self._check_constraints('sig', operation)\n            return self._get_public_key(arg)\n        elif operation == 'encrypt' or operation == 'wrapKey':\n            self._check_constraints('enc', operation)\n            return self._get_public_key(arg)\n        elif operation == 'decrypt' or operation == 'unwrapKey':\n            self._check_constraints('enc', operation)\n            return self._get_private_key(arg)\n        else:\n            raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimporting a key from data loaded from a PEM file.", "response": "def import_from_pem(self, data, password=None):\n        \"\"\"Imports a key from data loaded from a PEM file.\n        The key may be encrypted with a password.\n        Private keys (PKCS#8 format), public keys, and X509 certificate's\n        public keys can be imported with this interface.\n\n        :param data(bytes): The data contained in a PEM file.\n        :param password(bytes): An optional password to unwrap the key.\n        \"\"\"\n\n        try:\n            key = serialization.load_pem_private_key(\n                data, password=password, backend=default_backend())\n        except ValueError as e:\n            if password is not None:\n                raise e\n            try:\n                key = serialization.load_pem_public_key(\n                    data, backend=default_backend())\n            except ValueError:\n                try:\n                    cert = x509.load_pem_x509_certificate(\n                        data, backend=default_backend())\n                    key = cert.public_key()\n                except ValueError:\n                    raise e\n\n        self.import_from_pyca(key)\n        self._params['kid'] = self.thumbprint()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports the keys to a data buffer suitable to be stored as PEM files.", "response": "def export_to_pem(self, private_key=False, password=False):\n        \"\"\"Exports keys to a data buffer suitable to be stored as a PEM file.\n        Either the public or the private key can be exported to a PEM file.\n        For private keys the PKCS#8 format is used. If a password is provided\n        the best encryption method available as determined by the cryptography\n        module is used to wrap the key.\n\n        :param private_key: Whether the private key should be exported.\n         Defaults to `False` which means the public key is exported by default.\n        :param password(bytes): A password for wrapping the private key.\n         Defaults to False which will cause the operation to fail. To avoid\n         encryption the user must explicitly pass None, otherwise the user\n         needs to provide a password in a bytes buffer.\n        \"\"\"\n        e = serialization.Encoding.PEM\n        if private_key:\n            if not self.has_private:\n                raise InvalidJWKType(\"No private key available\")\n            f = serialization.PrivateFormat.PKCS8\n            if password is None:\n                a = serialization.NoEncryption()\n            elif isinstance(password, bytes):\n                a = serialization.BestAvailableEncryption(password)\n            elif password is False:\n                raise ValueError(\"The password must be None or a bytes string\")\n            else:\n                raise TypeError(\"The password string must be bytes\")\n            return self._get_private_key().private_bytes(\n                encoding=e, format=f, encryption_algorithm=a)\n        else:\n            if not self.has_public:\n                raise InvalidJWKType(\"No public key available\")\n            f = serialization.PublicFormat.SubjectPublicKeyInfo\n            return self._get_public_key().public_bytes(encoding=e, format=f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_pem(cls, data, password=None):\n        obj = cls()\n        obj.import_from_pem(data, password)\n        return obj", "response": "Creates a key from PKCS#8 formatted data loaded from a PEM file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the thumbprint as specified by RFC 7638.", "response": "def thumbprint(self, hashalg=hashes.SHA256()):\n        \"\"\"Returns the key thumbprint as specified by RFC 7638.\n\n        :param hashalg: A hash function (defaults to SHA256)\n        \"\"\"\n\n        t = {'kty': self._params['kty']}\n        for name, val in iteritems(JWKValuesRegistry[t['kty']]):\n            if val.required:\n                t[name] = self._key[name]\n        digest = hashes.Hash(hashalg, backend=default_backend())\n        digest.update(bytes(json_encode(t).encode('utf8')))\n        return base64url_encode(digest.finalize())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, elem):\n        if not isinstance(elem, JWK):\n            raise TypeError('Only JWK objects are valid elements')\n        set.add(self, elem)", "response": "Adds a JWK object to the set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self, private_keys=True):\n        exp_dict = dict()\n        for k, v in iteritems(self):\n            if k == 'keys':\n                keys = list()\n                for jwk in v:\n                    keys.append(json_decode(jwk.export(private_keys)))\n                v = keys\n            exp_dict[k] = v\n        return json_encode(exp_dict)", "response": "Exports a RFC 7517 keyset using the standard JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_keyset(self, keyset):\n        try:\n            jwkset = json_decode(keyset)\n        except Exception:  # pylint: disable=broad-except\n            raise InvalidJWKValue()\n\n        if 'keys' not in jwkset:\n            raise InvalidJWKValue()\n\n        for k, v in iteritems(jwkset):\n            if k == 'keys':\n                for jwk in v:\n                    self['keys'].add(JWK(**jwk))\n            else:\n                self[k] = v", "response": "Imports a JOSE Keyset into the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a recipient to the object.", "response": "def add_recipient(self, key, header=None):\n        \"\"\"Encrypt the plaintext with the given key.\n\n        :param key: A JWK key or password of appropriate type for the 'alg'\n         provided in the JOSE Headers.\n        :param header: A JSON string representing the per-recipient header.\n\n        :raises ValueError: if the plaintext is missing or not of type bytes.\n        :raises ValueError: if the compression type is unknown.\n        :raises InvalidJWAAlgorithm: if the 'alg' provided in the JOSE\n         headers is missing or unknown, or otherwise not implemented.\n        \"\"\"\n        if self.plaintext is None:\n            raise ValueError('Missing plaintext')\n        if not isinstance(self.plaintext, bytes):\n            raise ValueError(\"Plaintext must be 'bytes'\")\n\n        if isinstance(header, dict):\n            header = json_encode(header)\n\n        jh = self._get_jose_header(header)\n        alg, enc = self._get_alg_enc_from_headers(jh)\n\n        rec = dict()\n        if header:\n            rec['header'] = header\n\n        wrapped = alg.wrap(key, enc.wrap_key_size, self.cek, jh)\n        self.cek = wrapped['cek']\n\n        if 'ek' in wrapped:\n            rec['encrypted_key'] = wrapped['ek']\n\n        if 'header' in wrapped:\n            h = json_decode(rec.get('header', '{}'))\n            nh = self._merge_headers(h, wrapped['header'])\n            rec['header'] = json_encode(nh)\n\n        if 'ciphertext' not in self.objects:\n            self._encrypt(alg, enc, jh)\n\n        if 'recipients' in self.objects:\n            self.objects['recipients'].append(rec)\n        elif 'encrypted_key' in self.objects or 'header' in self.objects:\n            self.objects['recipients'] = list()\n            n = dict()\n            if 'encrypted_key' in self.objects:\n                n['encrypted_key'] = self.objects.pop('encrypted_key')\n            if 'header' in self.objects:\n                n['header'] = self.objects.pop('header')\n            self.objects['recipients'].append(n)\n            self.objects['recipients'].append(rec)\n        else:\n            self.objects.update(rec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self, compact=False):\n\n        if 'ciphertext' not in self.objects:\n            raise InvalidJWEOperation(\"No available ciphertext\")\n\n        if compact:\n            for invalid in 'aad', 'unprotected':\n                if invalid in self.objects:\n                    raise InvalidJWEOperation(\n                        \"Can't use compact encoding when the '%s' parameter\"\n                        \"is set\" % invalid)\n            if 'protected' not in self.objects:\n                raise InvalidJWEOperation(\n                    \"Can't use compat encoding without protected headers\")\n            else:\n                ph = json_decode(self.objects['protected'])\n                for required in 'alg', 'enc':\n                    if required not in ph:\n                        raise InvalidJWEOperation(\n                            \"Can't use compat encoding, '%s' must be in the \"\n                            \"protected header\" % required)\n            if 'recipients' in self.objects:\n                if len(self.objects['recipients']) != 1:\n                    raise InvalidJWEOperation(\"Invalid number of recipients\")\n                rec = self.objects['recipients'][0]\n            else:\n                rec = self.objects\n            if 'header' in rec:\n                # The AESGCMKW algorithm generates data (iv, tag) we put in the\n                # per-recipient unpotected header by default. Move it to the\n                # protected header and re-encrypt the payload, as the protected\n                # header is used as additional authenticated data.\n                h = json_decode(rec['header'])\n                ph = json_decode(self.objects['protected'])\n                nph = self._merge_headers(h, ph)\n                self.objects['protected'] = json_encode(nph)\n                jh = self._get_jose_header()\n                alg, enc = self._get_alg_enc_from_headers(jh)\n                self._encrypt(alg, enc, jh)\n                del rec['header']\n\n            return '.'.join([base64url_encode(self.objects['protected']),\n                             base64url_encode(rec.get('encrypted_key', '')),\n                             base64url_encode(self.objects['iv']),\n                             base64url_encode(self.objects['ciphertext']),\n                             base64url_encode(self.objects['tag'])])\n        else:\n            obj = self.objects\n            enc = {'ciphertext': base64url_encode(obj['ciphertext']),\n                   'iv': base64url_encode(obj['iv']),\n                   'tag': base64url_encode(self.objects['tag'])}\n            if 'protected' in obj:\n                enc['protected'] = base64url_encode(obj['protected'])\n            if 'unprotected' in obj:\n                enc['unprotected'] = json_decode(obj['unprotected'])\n            if 'aad' in obj:\n                enc['aad'] = base64url_encode(obj['aad'])\n            if 'recipients' in obj:\n                enc['recipients'] = list()\n                for rec in obj['recipients']:\n                    e = dict()\n                    if 'encrypted_key' in rec:\n                        e['encrypted_key'] = \\\n                            base64url_encode(rec['encrypted_key'])\n                    if 'header' in rec:\n                        e['header'] = json_decode(rec['header'])\n                    enc['recipients'].append(e)\n            else:\n                if 'encrypted_key' in obj:\n                    enc['encrypted_key'] = \\\n                        base64url_encode(obj['encrypted_key'])\n                if 'header' in obj:\n                    enc['header'] = json_decode(obj['header'])\n            return json_encode(enc)", "response": "Serializes the object into a JWE token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decrypt(self, key):\n\n        if 'ciphertext' not in self.objects:\n            raise InvalidJWEOperation(\"No available ciphertext\")\n        self.decryptlog = list()\n\n        if 'recipients' in self.objects:\n            for rec in self.objects['recipients']:\n                try:\n                    self._decrypt(key, rec)\n                except Exception as e:  # pylint: disable=broad-except\n                    self.decryptlog.append('Failed: [%s]' % repr(e))\n        else:\n            try:\n                self._decrypt(key, self.objects)\n            except Exception as e:  # pylint: disable=broad-except\n                self.decryptlog.append('Failed: [%s]' % repr(e))\n\n        if not self.plaintext:\n            raise InvalidJWEData('No recipient matched the provided '\n                                 'key' + repr(self.decryptlog))", "response": "Decrypt a JWE token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deserialize(self, raw_jwe, key=None):\n\n        self.objects = dict()\n        self.plaintext = None\n        self.cek = None\n\n        o = dict()\n        try:\n            try:\n                djwe = json_decode(raw_jwe)\n                o['iv'] = base64url_decode(djwe['iv'])\n                o['ciphertext'] = base64url_decode(djwe['ciphertext'])\n                o['tag'] = base64url_decode(djwe['tag'])\n                if 'protected' in djwe:\n                    p = base64url_decode(djwe['protected'])\n                    o['protected'] = p.decode('utf-8')\n                if 'unprotected' in djwe:\n                    o['unprotected'] = json_encode(djwe['unprotected'])\n                if 'aad' in djwe:\n                    o['aad'] = base64url_decode(djwe['aad'])\n                if 'recipients' in djwe:\n                    o['recipients'] = list()\n                    for rec in djwe['recipients']:\n                        e = dict()\n                        if 'encrypted_key' in rec:\n                            e['encrypted_key'] = \\\n                                base64url_decode(rec['encrypted_key'])\n                        if 'header' in rec:\n                            e['header'] = json_encode(rec['header'])\n                        o['recipients'].append(e)\n                else:\n                    if 'encrypted_key' in djwe:\n                        o['encrypted_key'] = \\\n                            base64url_decode(djwe['encrypted_key'])\n                    if 'header' in djwe:\n                        o['header'] = json_encode(djwe['header'])\n\n            except ValueError:\n                c = raw_jwe.split('.')\n                if len(c) != 5:\n                    raise InvalidJWEData()\n                p = base64url_decode(c[0])\n                o['protected'] = p.decode('utf-8')\n                ekey = base64url_decode(c[1])\n                if ekey != b'':\n                    o['encrypted_key'] = base64url_decode(c[1])\n                o['iv'] = base64url_decode(c[2])\n                o['ciphertext'] = base64url_decode(c[3])\n                o['tag'] = base64url_decode(c[4])\n\n            self.objects = o\n\n        except Exception as e:  # pylint: disable=broad-except\n            raise InvalidJWEData('Invalid format', repr(e))\n\n        if key:\n            self.decrypt(key)", "response": "Deserialize a JWE token into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a signature for the current record set", "response": "def sign(self):\n        \"\"\"Generates a signature\"\"\"\n        payload = self._payload()\n        sigin = b'.'.join([self.protected.encode('utf-8'), payload])\n        signature = self.engine.sign(self.key, sigin)\n        return {'protected': self.protected,\n                'payload': payload,\n                'signature': base64url_encode(signature)}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify a signature :raises InvalidJWSSignature: if the verification fails.", "response": "def verify(self, signature):\n        \"\"\"Verifies a signature\n\n        :raises InvalidJWSSignature: if the verification fails.\n        \"\"\"\n        try:\n            payload = self._payload()\n            sigin = b'.'.join([self.protected.encode('utf-8'), payload])\n            self.engine.verify(self.key, sigin, signature)\n        except Exception as e:  # pylint: disable=broad-except\n            raise InvalidJWSSignature('Verification failed', repr(e))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify(self, key, alg=None):\n\n        self.verifylog = list()\n        self.objects['valid'] = False\n        obj = self.objects\n        if 'signature' in obj:\n            try:\n                self._verify(alg, key,\n                             obj['payload'],\n                             obj['signature'],\n                             obj.get('protected', None),\n                             obj.get('header', None))\n                obj['valid'] = True\n            except Exception as e:  # pylint: disable=broad-except\n                self.verifylog.append('Failed: [%s]' % repr(e))\n\n        elif 'signatures' in obj:\n            for o in obj['signatures']:\n                try:\n                    self._verify(alg, key,\n                                 obj['payload'],\n                                 o['signature'],\n                                 o.get('protected', None),\n                                 o.get('header', None))\n                    # Ok if at least one verifies\n                    obj['valid'] = True\n                except Exception as e:  # pylint: disable=broad-except\n                    self.verifylog.append('Failed: [%s]' % repr(e))\n        else:\n            raise InvalidJWSSignature('No signatures availble')\n\n        if not self.is_valid:\n            raise InvalidJWSSignature('Verification failed for all '\n                                      'signatures' + repr(self.verifylog))", "response": "Verifies a JWS token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_signature(self, key, alg=None, protected=None, header=None):\n\n        if not self.objects.get('payload', None):\n            raise InvalidJWSObject('Missing Payload')\n\n        b64 = True\n\n        p = dict()\n        if protected:\n            if isinstance(protected, dict):\n                p = protected\n                protected = json_encode(p)\n            else:\n                p = json_decode(protected)\n\n        # If b64 is present we must enforce criticality\n        if 'b64' in list(p.keys()):\n            crit = p.get('crit', [])\n            if 'b64' not in crit:\n                raise InvalidJWSObject('b64 header must always be critical')\n            b64 = p['b64']\n\n        if 'b64' in self.objects:\n            if b64 != self.objects['b64']:\n                raise InvalidJWSObject('Mixed b64 headers on signatures')\n\n        h = None\n        if header:\n            if isinstance(header, dict):\n                h = header\n                header = json_encode(header)\n            else:\n                h = json_decode(header)\n\n        p = self._merge_check_headers(p, h)\n\n        if 'alg' in p:\n            if alg is None:\n                alg = p['alg']\n            elif alg != p['alg']:\n                raise ValueError('\"alg\" value mismatch, specified \"alg\" '\n                                 'does not match JOSE header value')\n\n        if alg is None:\n            raise ValueError('\"alg\" not specified')\n\n        c = JWSCore(alg, key, protected, self.objects['payload'])\n        sig = c.sign()\n\n        o = dict()\n        o['signature'] = base64url_decode(sig['signature'])\n        if protected:\n            o['protected'] = protected\n        if header:\n            o['header'] = h\n        o['valid'] = True\n\n        if 'signatures' in self.objects:\n            self.objects['signatures'].append(o)\n        elif 'signature' in self.objects:\n            self.objects['signatures'] = list()\n            n = dict()\n            n['signature'] = self.objects.pop('signature')\n            if 'protected' in self.objects:\n                n['protected'] = self.objects.pop('protected')\n            if 'header' in self.objects:\n                n['header'] = self.objects.pop('header')\n            if 'valid' in self.objects:\n                n['valid'] = self.objects.pop('valid')\n            self.objects['signatures'].append(n)\n            self.objects['signatures'].append(o)\n        else:\n            self.objects.update(o)\n            self.objects['b64'] = b64", "response": "Adds a new signature to the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing the object into a JWS token.", "response": "def serialize(self, compact=False):\n        \"\"\"Serializes the object into a JWS token.\n\n        :param compact(boolean): if True generates the compact\n         representation, otherwise generates a standard JSON format.\n\n        :raises InvalidJWSOperation: if the object cannot serialized\n         with the compact representation and `compat` is True.\n        :raises InvalidJWSSignature: if no signature has been added\n         to the object, or no valid signature can be found.\n        \"\"\"\n        if compact:\n            if 'signatures' in self.objects:\n                raise InvalidJWSOperation(\"Can't use compact encoding with \"\n                                          \"multiple signatures\")\n            if 'signature' not in self.objects:\n                raise InvalidJWSSignature(\"No available signature\")\n            if not self.objects.get('valid', False):\n                raise InvalidJWSSignature(\"No valid signature found\")\n            if 'protected' in self.objects:\n                protected = base64url_encode(self.objects['protected'])\n            else:\n                protected = ''\n            if self.objects.get('payload', False):\n                if self.objects.get('b64', True):\n                    payload = base64url_encode(self.objects['payload'])\n                else:\n                    if isinstance(self.objects['payload'], bytes):\n                        payload = self.objects['payload'].decode('utf-8')\n                    else:\n                        payload = self.objects['payload']\n                    if '.' in payload:\n                        raise InvalidJWSOperation(\n                            \"Can't use compact encoding with unencoded \"\n                            \"payload that uses the . character\")\n            else:\n                payload = ''\n            return '.'.join([protected, payload,\n                             base64url_encode(self.objects['signature'])])\n        else:\n            obj = self.objects\n            sig = dict()\n            if self.objects.get('payload', False):\n                if self.objects.get('b64', True):\n                    sig['payload'] = base64url_encode(self.objects['payload'])\n                else:\n                    sig['payload'] = self.objects['payload']\n            if 'signature' in obj:\n                if not obj.get('valid', False):\n                    raise InvalidJWSSignature(\"No valid signature found\")\n                sig['signature'] = base64url_encode(obj['signature'])\n                if 'protected' in obj:\n                    sig['protected'] = base64url_encode(obj['protected'])\n                if 'header' in obj:\n                    sig['header'] = obj['header']\n            elif 'signatures' in obj:\n                sig['signatures'] = list()\n                for o in obj['signatures']:\n                    if not o.get('valid', False):\n                        continue\n                    s = {'signature': base64url_encode(o['signature'])}\n                    if 'protected' in o:\n                        s['protected'] = base64url_encode(o['protected'])\n                    if 'header' in o:\n                        s['header'] = o['header']\n                    sig['signatures'].append(s)\n                if len(sig['signatures']) == 0:\n                    raise InvalidJWSSignature(\"No valid signature found\")\n            else:\n                raise InvalidJWSSignature(\"No available signature\")\n            return json_encode(sig)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsign the payload. Creates a JWS token with the header as the JWS protected header and the claims as the payload. See (:class:`jwcrypto.jws.JWS`) for details on the exceptions that may be reaised. :param key: A (:class:`jwcrypto.jwk.JWK`) key.", "response": "def make_signed_token(self, key):\n        \"\"\"Signs the payload.\n\n        Creates a JWS token with the header as the JWS protected header and\n        the claims as the payload. See (:class:`jwcrypto.jws.JWS`) for\n        details on the exceptions that may be reaised.\n\n        :param key: A (:class:`jwcrypto.jwk.JWK`) key.\n        \"\"\"\n\n        t = JWS(self.claims)\n        t.add_signature(key, protected=self.header)\n        self.token = t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencrypt the payload. Creates a JWE token with the header as the JWE protected header and the claims as the plaintext. See (:class:`jwcrypto.jwe.JWE`) for details on the exceptions that may be reaised. :param key: A (:class:`jwcrypto.jwk.JWK`) key.", "response": "def make_encrypted_token(self, key):\n        \"\"\"Encrypts the payload.\n\n        Creates a JWE token with the header as the JWE protected header and\n        the claims as the plaintext. See (:class:`jwcrypto.jwe.JWE`) for\n        details on the exceptions that may be reaised.\n\n        :param key: A (:class:`jwcrypto.jwk.JWK`) key.\n        \"\"\"\n\n        t = JWE(self.claims, self.header)\n        t.add_recipient(key)\n        self.token = t"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deserialize(self, jwt, key=None):\n        c = jwt.count('.')\n        if c == 2:\n            self.token = JWS()\n        elif c == 4:\n            self.token = JWE()\n        else:\n            raise ValueError(\"Token format unrecognized\")\n\n        # Apply algs restrictions if any, before performing any operation\n        if self._algs:\n            self.token.allowed_algs = self._algs\n\n        self.deserializelog = list()\n        # now deserialize and also decrypt/verify (or raise) if we\n        # have a key\n        if key is None:\n            self.token.deserialize(jwt, None)\n        elif isinstance(key, JWK):\n            self.token.deserialize(jwt, key)\n            self.deserializelog.append(\"Success\")\n        elif isinstance(key, JWKSet):\n            self.token.deserialize(jwt, None)\n            if 'kid' in self.token.jose_header:\n                kid_key = key.get_key(self.token.jose_header['kid'])\n                if not kid_key:\n                    raise JWTMissingKey('Key ID %s not in key set'\n                                        % self.token.jose_header['kid'])\n                self.token.deserialize(jwt, kid_key)\n            else:\n                for k in key:\n                    try:\n                        self.token.deserialize(jwt, k)\n                        self.deserializelog.append(\"Success\")\n                        break\n                    except Exception as e:  # pylint: disable=broad-except\n                        keyid = k.key_id\n                        if keyid is None:\n                            keyid = k.thumbprint()\n                        self.deserializelog.append('Key [%s] failed: [%s]' % (\n                            keyid, repr(e)))\n                        continue\n                if \"Success\" not in self.deserializelog:\n                    raise JWTMissingKey('No working key found in key set')\n        else:\n            raise ValueError(\"Unrecognized Key Type\")\n\n        if key is not None:\n            self.header = self.token.jose_header\n            self.claims = self.token.payload.decode('utf-8')\n            self._check_provided_claims()", "response": "Deserialize a raw JWT token into a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates current model using validation dataset.", "response": "def validate(self, n_viz=9):\n        \"\"\"Validate current model using validation dataset.\n\n        Parameters\n        ----------\n        n_viz: int\n            Number fo visualization.\n\n        Returns\n        -------\n        log: dict\n            Log values.\n        \"\"\"\n        iter_valid = copy.copy(self.iter_valid)\n        losses, lbl_trues, lbl_preds = [], [], []\n        vizs = []\n        dataset = iter_valid.dataset\n        desc = 'valid [iteration=%08d]' % self.iteration\n        for batch in tqdm.tqdm(iter_valid, desc=desc, total=len(dataset),\n                               ncols=80, leave=False):\n            img, lbl_true = zip(*batch)\n            batch = map(datasets.transform_lsvrc2012_vgg16, batch)\n            with chainer.no_backprop_mode(), \\\n                    chainer.using_config('train', False):\n                in_vars = utils.batch_to_vars(batch, device=self.device)\n                loss = self.model(*in_vars)\n            losses.append(float(loss.data))\n            score = self.model.score\n            lbl_pred = chainer.functions.argmax(score, axis=1)\n            lbl_pred = chainer.cuda.to_cpu(lbl_pred.data)\n            for im, lt, lp in zip(img, lbl_true, lbl_pred):\n                lbl_trues.append(lt)\n                lbl_preds.append(lp)\n                if len(vizs) < n_viz:\n                    viz = utils.visualize_segmentation(\n                        lbl_pred=lp, lbl_true=lt,\n                        img=im, n_class=self.model.n_class)\n                    vizs.append(viz)\n        # save visualization\n        out_viz = osp.join(self.out, 'visualizations_valid',\n                           'iter%08d.jpg' % self.iteration)\n        if not osp.exists(osp.dirname(out_viz)):\n            os.makedirs(osp.dirname(out_viz))\n        viz = utils.get_tile_image(vizs)\n        skimage.io.imsave(out_viz, viz)\n        # generate log\n        acc = utils.label_accuracy_score(\n            lbl_trues, lbl_preds, self.model.n_class)\n        self._write_log(**{\n            'epoch': self.epoch,\n            'iteration': self.iteration,\n            'elapsed_time': time.time() - self.stamp_start,\n            'valid/loss': np.mean(losses),\n            'valid/acc': acc[0],\n            'valid/acc_cls': acc[1],\n            'valid/mean_iu': acc[2],\n            'valid/fwavacc': acc[3],\n        })\n        self._save_model()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(self):\n        self.stamp_start = time.time()\n        for iteration, batch in tqdm.tqdm(enumerate(self.iter_train),\n                                          desc='train', total=self.max_iter,\n                                          ncols=80):\n            self.epoch = self.iter_train.epoch\n            self.iteration = iteration\n\n            ############\n            # validate #\n            ############\n\n            if self.interval_validate and \\\n                    self.iteration % self.interval_validate == 0:\n                self.validate()\n\n            #########\n            # train #\n            #########\n\n            batch = map(datasets.transform_lsvrc2012_vgg16, batch)\n            in_vars = utils.batch_to_vars(batch, device=self.device)\n            self.model.zerograds()\n            loss = self.model(*in_vars)\n\n            if loss is not None:\n                loss.backward()\n                self.optimizer.update()\n\n                lbl_true = zip(*batch)[1]\n                lbl_pred = chainer.functions.argmax(self.model.score, axis=1)\n                lbl_pred = chainer.cuda.to_cpu(lbl_pred.data)\n                acc = utils.label_accuracy_score(\n                    lbl_true, lbl_pred, self.model.n_class)\n                self._write_log(**{\n                    'epoch': self.epoch,\n                    'iteration': self.iteration,\n                    'elapsed_time': time.time() - self.stamp_start,\n                    'train/loss': float(loss.data),\n                    'train/acc': acc[0],\n                    'train/acc_cls': acc[1],\n                    'train/mean_iu': acc[2],\n                    'train/fwavacc': acc[3],\n                })\n\n            if iteration >= self.max_iter:\n                self._save_model()\n                break", "response": "Train the network using the training dataset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tile_images(imgs, tile_shape, concatenated_image):\n    y_num, x_num = tile_shape\n    one_width = imgs[0].shape[1]\n    one_height = imgs[0].shape[0]\n    if concatenated_image is None:\n        if len(imgs[0].shape) == 3:\n            n_channels = imgs[0].shape[2]\n            assert all(im.shape[2] == n_channels for im in imgs)\n            concatenated_image = np.zeros(\n                (one_height * y_num, one_width * x_num, n_channels),\n                dtype=np.uint8,\n            )\n        else:\n            concatenated_image = np.zeros(\n                (one_height * y_num, one_width * x_num), dtype=np.uint8)\n    for y in six.moves.range(y_num):\n        for x in six.moves.range(x_num):\n            i = x + y * x_num\n            if i >= len(imgs):\n                pass\n            else:\n                concatenated_image[y * one_height:(y + 1) * one_height,\n                                   x * one_width:(x + 1) * one_width] = imgs[i]\n    return concatenated_image", "response": "Concatenate images that are in the same shape as the image list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new image list that is concatenated with the given images.", "response": "def get_tile_image(imgs, tile_shape=None, result_img=None, margin_color=None):\n    \"\"\"Concatenate images whose sizes are different.\n\n    @param imgs: image list which should be concatenated\n    @param tile_shape: shape for which images should be concatenated\n    @param result_img: numpy array to put result image\n    \"\"\"\n    def resize(*args, **kwargs):\n        # anti_aliasing arg cannot be passed to skimage<0.14\n        # use LooseVersion to allow 0.14dev.\n        if LooseVersion(skimage.__version__) < LooseVersion('0.14'):\n            kwargs.pop('anti_aliasing', None)\n        return skimage.transform.resize(*args, **kwargs)\n\n    def get_tile_shape(img_num):\n        x_num = 0\n        y_num = int(math.sqrt(img_num))\n        while x_num * y_num < img_num:\n            x_num += 1\n        return y_num, x_num\n\n    if tile_shape is None:\n        tile_shape = get_tile_shape(len(imgs))\n\n    # get max tile size to which each image should be resized\n    max_height, max_width = np.inf, np.inf\n    for img in imgs:\n        max_height = min([max_height, img.shape[0]])\n        max_width = min([max_width, img.shape[1]])\n\n    # resize and concatenate images\n    for i, img in enumerate(imgs):\n        h, w = img.shape[:2]\n        dtype = img.dtype\n        h_scale, w_scale = max_height / h, max_width / w\n        scale = min([h_scale, w_scale])\n        h, w = int(scale * h), int(scale * w)\n        img = resize(\n            image=img,\n            output_shape=(h, w),\n            mode='reflect',\n            preserve_range=True,\n            anti_aliasing=True,\n        ).astype(dtype)\n        if len(img.shape) == 3:\n            img = centerize(img, (max_height, max_width, 3), margin_color)\n        else:\n            img = centerize(img, (max_height, max_width), margin_color)\n        imgs[i] = img\n    return _tile_images(imgs, tile_shape, result_img)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvisualize segmentation. Parameters ---------- img: ndarray Input image to predict label. lbl_true: ndarray Ground truth of the label. lbl_pred: ndarray Label predicted. n_class: int Number of classes. label_names: dict or list Names of each label value. Key or index is label_value and value is its name. Returns ------- img_array: ndarray Visualized image.", "response": "def visualize_segmentation(**kwargs):\n    \"\"\"Visualize segmentation.\n\n    Parameters\n    ----------\n    img: ndarray\n        Input image to predict label.\n    lbl_true: ndarray\n        Ground truth of the label.\n    lbl_pred: ndarray\n        Label predicted.\n    n_class: int\n        Number of classes.\n    label_names: dict or list\n        Names of each label value.\n        Key or index is label_value and value is its name.\n\n    Returns\n    -------\n    img_array: ndarray\n        Visualized image.\n    \"\"\"\n    img = kwargs.pop('img', None)\n    lbl_true = kwargs.pop('lbl_true', None)\n    lbl_pred = kwargs.pop('lbl_pred', None)\n    n_class = kwargs.pop('n_class', None)\n    label_names = kwargs.pop('label_names', None)\n    if kwargs:\n        raise RuntimeError(\n            'Unexpected keys in kwargs: {}'.format(kwargs.keys()))\n\n    if lbl_true is None and lbl_pred is None:\n        raise ValueError('lbl_true or lbl_pred must be not None.')\n\n    lbl_true = copy.deepcopy(lbl_true)\n    lbl_pred = copy.deepcopy(lbl_pred)\n\n    mask_unlabeled = None\n    viz_unlabeled = None\n    if lbl_true is not None:\n        mask_unlabeled = lbl_true == -1\n        lbl_true[mask_unlabeled] = 0\n        viz_unlabeled = (\n            np.random.random((lbl_true.shape[0], lbl_true.shape[1], 3)) * 255\n        ).astype(np.uint8)\n        if lbl_pred is not None:\n            lbl_pred[mask_unlabeled] = 0\n\n    vizs = []\n\n    if lbl_true is not None:\n        viz_trues = [\n            img,\n            label2rgb(lbl_true, label_names=label_names, n_labels=n_class),\n            label2rgb(lbl_true, img, label_names=label_names,\n                      n_labels=n_class),\n        ]\n        viz_trues[1][mask_unlabeled] = viz_unlabeled[mask_unlabeled]\n        viz_trues[2][mask_unlabeled] = viz_unlabeled[mask_unlabeled]\n        vizs.append(get_tile_image(viz_trues, (1, 3)))\n\n    if lbl_pred is not None:\n        viz_preds = [\n            img,\n            label2rgb(lbl_pred, label_names=label_names, n_labels=n_class),\n            label2rgb(lbl_pred, img, label_names=label_names,\n                      n_labels=n_class),\n        ]\n        if mask_unlabeled is not None and viz_unlabeled is not None:\n            viz_preds[1][mask_unlabeled] = viz_unlabeled[mask_unlabeled]\n            viz_preds[2][mask_unlabeled] = viz_unlabeled[mask_unlabeled]\n        vizs.append(get_tile_image(viz_preds, (1, 3)))\n\n    if len(vizs) == 1:\n        return vizs[0]\n    elif len(vizs) == 2:\n        return get_tile_image(vizs, (2, 1))\n    else:\n        raise RuntimeError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_upsampling_filter(size):\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    filter = (1 - abs(og[0] - center) / factor) * \\\n             (1 - abs(og[1] - center) / factor)\n    return filter", "response": "Make a 2D bilinear kernel suitable for upsampling"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new version of the object.", "response": "def create(self, output_path, dry_run=False, output_format=None, compresslevel=None):\n        \"\"\"\n        Create the archive at output_file_path.\n\n        Type of the archive is determined either by extension of output_file_path or by output_format.\n        Supported formats are: gz, zip, bz2, xz, tar, tgz, txz\n\n        @param output_path: Output file path.\n        @type output_path: str\n\n        @param dry_run: Determines whether create should do nothing but print what it would archive.\n        @type dry_run: bool\n\n        @param output_format: Determines format of the output archive. If None, format is determined from extension\n            of output_file_path.\n        @type output_format: str\n        \"\"\"\n        if output_format is None:\n            file_name, file_ext = path.splitext(output_path)\n            output_format = file_ext[len(extsep):].lower()\n            self.LOG.debug(\"Output format is not explicitly set, determined format is {0}.\".format(output_format))\n\n        if not dry_run:\n            if output_format in self.ZIPFILE_FORMATS:\n                from zipfile import ZipFile, ZipInfo, ZIP_DEFLATED\n\n                if compresslevel is not None:\n                    if sys.version_info > (3, 7):\n                        archive = ZipFile(path.abspath(output_path), 'w', compresslevel=compresslevel)\n                    else:\n                        raise ValueError(\"Compression level for zip archives requires Python 3.7+\")\n                else:\n                    archive = ZipFile(path.abspath(output_path), 'w')\n\n                def add_file(file_path, arcname):\n                    if not path.islink(file_path):\n                        archive.write(file_path, arcname, ZIP_DEFLATED)\n                    else:\n                        i = ZipInfo(arcname)\n                        i.create_system = 3\n                        i.external_attr = 0xA1ED0000\n                        archive.writestr(i, readlink(file_path))\n            elif output_format in self.TARFILE_FORMATS:\n                import tarfile\n\n                mode = self.TARFILE_FORMATS[output_format]\n\n                if compresslevel is not None:\n                    try:\n                        archive = tarfile.open(path.abspath(output_path), mode, compresslevel=compresslevel)\n                    except TypeError:\n                        raise ValueError(\"{0} cannot be compressed\".format(output_format))\n                else:\n                    archive = tarfile.open(path.abspath(output_path), mode)\n\n                def add_file(file_path, arcname):\n                    archive.add(file_path, arcname)\n            else:\n                raise ValueError(\"unknown format: {0}\".format(output_format))\n\n            def archiver(file_path, arcname):\n                self.LOG.debug(\"{0} => {1}\".format(file_path, arcname))\n                add_file(file_path, arcname)\n        else:\n            archive = None\n\n            def archiver(file_path, arcname):\n                self.LOG.info(\"{0} => {1}\".format(file_path, arcname))\n\n        self.archive_all_files(archiver)\n\n        if archive is not None:\n            archive.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_file_excluded(self, repo_abspath, repo_file_path):\n        next(self._check_attr_gens[repo_abspath])\n        attrs = self._check_attr_gens[repo_abspath].send(repo_file_path)\n        return attrs['export-ignore'] == 'set'", "response": "Checks whether a file at a given path is excluded."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\narchiving all files in the current directory.", "response": "def archive_all_files(self, archiver):\n        \"\"\"\n        Archive all files using archiver.\n\n        @param archiver: Callable that accepts 2 arguments:\n            abspath to file on the system and relative path within archive.\n        @type archiver: Callable\n        \"\"\"\n        for file_path in self.extra:\n            archiver(path.abspath(file_path), path.join(self.prefix, file_path))\n\n        for file_path in self.walk_git_files():\n            archiver(path.join(self.main_repo_abspath, file_path), path.join(self.prefix, file_path))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk_git_files(self, repo_path=''):\n        repo_abspath = path.join(self.main_repo_abspath, repo_path)\n        assert repo_abspath not in self._check_attr_gens\n        self._check_attr_gens[repo_abspath] = self.check_attr(repo_abspath, ['export-ignore'])\n\n        try:\n            repo_file_paths = self.run_git_shell(\n                'git ls-files -z --cached --full-name --no-empty-directory',\n                repo_abspath\n            ).split('\\0')[:-1]\n\n            for repo_file_path in repo_file_paths:\n                repo_file_abspath = path.join(repo_abspath, repo_file_path)  # absolute file path\n                main_repo_file_path = path.join(repo_path, repo_file_path)  # relative to main_repo_abspath\n\n                # Only list symlinks and files.\n                if not path.islink(repo_file_abspath) and path.isdir(repo_file_abspath):\n                    continue\n\n                if self.is_file_excluded(repo_abspath, repo_file_path):\n                    continue\n\n                yield main_repo_file_path\n\n            if self.force_sub:\n                self.run_git_shell('git submodule init', repo_abspath)\n                self.run_git_shell('git submodule update', repo_abspath)\n\n            try:\n                repo_gitmodules_abspath = path.join(repo_abspath, \".gitmodules\")\n\n                with open(repo_gitmodules_abspath) as f:\n                    lines = f.readlines()\n\n                for l in lines:\n                    m = re.match(\"^\\\\s*path\\\\s*=\\\\s*(.*)\\\\s*$\", l)\n\n                    if m:\n                        repo_submodule_path = m.group(1)  # relative to repo_path\n                        main_repo_submodule_path = path.join(repo_path, repo_submodule_path)  # relative to main_repo_abspath\n\n                        if self.is_file_excluded(repo_abspath, repo_submodule_path):\n                            continue\n\n                        for main_repo_submodule_file_path in self.walk_git_files(main_repo_submodule_path):\n                            repo_submodule_file_path = path.relpath(main_repo_submodule_file_path, repo_path)  # relative to repo_path\n                            if self.is_file_excluded(repo_abspath, repo_submodule_file_path):\n                                continue\n\n                            yield main_repo_submodule_file_path\n            except IOError:\n                pass\n        finally:\n            self._check_attr_gens[repo_abspath].close()\n            del self._check_attr_gens[repo_abspath]", "response": "Walks the git repository and returns an iterator that yields a file path relative to the main_repo_abspath."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator that returns attributes for given paths relative to repo_abspath.", "response": "def check_attr(self, repo_abspath, attrs):\n        \"\"\"\n        Generator that returns attributes for given paths relative to repo_abspath.\n\n        >>> g = GitArchiver.check_attr('repo_path', ['export-ignore'])\n        >>> next(g)\n        >>> attrs = g.send('relative_path')\n        >>> print(attrs['export-ignore'])\n\n        @param repo_abspath: Absolute path to a git repository.\n        @type repo_abspath: str\n\n        @param attrs: Attributes to check.\n        @type attrs: [str]\n\n        @rtype: generator\n        \"\"\"\n        def make_process():\n            env = dict(environ, GIT_FLUSH='1')\n            cmd = 'git check-attr --stdin -z {0}'.format(' '.join(attrs))\n            return Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, cwd=repo_abspath, env=env)\n\n        def read_attrs(process, repo_file_path):\n            process.stdin.write(repo_file_path.encode('utf-8') + b'\\0')\n            process.stdin.flush()\n\n            # For every attribute check-attr will output: <path> NUL <attribute> NUL <info> NUL\n            path, attr, info = b'', b'', b''\n            nuls_count = 0\n            nuls_expected = 3 * len(attrs)\n\n            while nuls_count != nuls_expected:\n                b = process.stdout.read(1)\n\n                if b == b'' and process.poll() is not None:\n                    raise RuntimeError(\"check-attr exited prematurely\")\n                elif b == b'\\0':\n                    nuls_count += 1\n\n                    if nuls_count % 3 == 0:\n                        yield map(self.decode_git_output, (path, attr, info))\n                        path, attr, info = b'', b'', b''\n                elif nuls_count % 3 == 0:\n                    path += b\n                elif nuls_count % 3 == 1:\n                    attr += b\n                elif nuls_count % 3 == 2:\n                    info += b\n\n        def read_attrs_old(process, repo_file_path):\n            \"\"\"\n            Compatibility with versions 1.8.5 and below that do not recognize -z for output.\n            \"\"\"\n            process.stdin.write(repo_file_path.encode('utf-8') + b'\\0')\n            process.stdin.flush()\n\n            # For every attribute check-attr will output: <path>: <attribute>: <info>\\n\n            # where <path> is c-quoted\n\n            path, attr, info = b'', b'', b''\n            lines_count = 0\n            lines_expected = len(attrs)\n\n            while lines_count != lines_expected:\n                line = process.stdout.readline()\n\n                info_start = line.rfind(b': ')\n                if info_start == -1:\n                    raise RuntimeError(\"unexpected output of check-attr: {0}\".format(line))\n\n                attr_start = line.rfind(b': ', 0, info_start)\n                if attr_start == -1:\n                    raise RuntimeError(\"unexpected output of check-attr: {0}\".format(line))\n\n                info = line[info_start + 2:len(line) - 1]  # trim leading \": \" and trailing \\n\n                attr = line[attr_start + 2:info_start]  # trim leading \": \"\n                path = line[:attr_start]\n\n                yield map(self.decode_git_output, (path, attr, info))\n                lines_count += 1\n\n        if not attrs:\n            return\n\n        process = make_process()\n\n        try:\n            while True:\n                repo_file_path = yield\n                repo_file_attrs = {}\n\n                if self.git_version is None or self.git_version > (1, 8, 5):\n                    reader = read_attrs\n                else:\n                    reader = read_attrs_old\n\n                for path, attr, value in reader(process, repo_file_path):\n                    repo_file_attrs[attr] = value\n\n                yield repo_file_attrs\n        finally:\n            process.stdin.close()\n            process.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning git shell command and decodes output into unicode string.", "response": "def run_git_shell(cls, cmd, cwd=None):\n        \"\"\"\n        Runs git shell command, reads output and decodes it into unicode string.\n\n        @param cmd: Command to be executed.\n        @type cmd: str\n\n        @type cwd: str\n        @param cwd: Working directory.\n\n        @rtype: str\n        @return: Output of the command.\n\n        @raise CalledProcessError:  Raises exception if return code of the command is non-zero.\n        \"\"\"\n        p = Popen(cmd, shell=True, stdout=PIPE, cwd=cwd)\n        output, _ = p.communicate()\n        output = cls.decode_git_output(output)\n\n        if p.returncode:\n            if sys.version_info > (2, 6):\n                raise CalledProcessError(returncode=p.returncode, cmd=cmd, output=output)\n            else:\n                raise CalledProcessError(returncode=p.returncode, cmd=cmd)\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_git_version(cls):\n        try:\n            output = cls.run_git_shell('git version')\n        except CalledProcessError:\n            cls.LOG.warning(\"Unable to get Git version.\")\n            return None\n\n        try:\n            version = output.split()[2]\n        except IndexError:\n            cls.LOG.warning(\"Unable to parse Git version \\\"%s\\\".\", output)\n            return None\n\n        try:\n            return tuple(int(v) for v in version.split('.'))\n        except ValueError:\n            cls.LOG.warning(\"Unable to parse Git version \\\"%s\\\".\", version)\n            return None", "response": "Returns version of git current shell points to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def request(method, uri, **kwargs):\n    '''Base function for one time http requests.\n\n    Args:\n        method (str): The http method to use. For example 'GET'\n        uri (str): The url of the resource.\n            Example: 'https://example.com/stuff'\n        kwargs: Any number of arguments supported, found here:\n            http://asks.rtfd.io/en/latest/overview-of-funcs-and-args.html\n\n    Returns:\n        Response (asks.Response): The Response object.\n    '''\n    c_interact = kwargs.pop('persist_cookies', None)\n    ssl_context = kwargs.pop('ssl_context', None)\n    async with Session(persist_cookies=c_interact, ssl_context=ssl_context) as s:\n        r = await s.request(method, url=uri, **kwargs)\n        return r", "response": "Base function for one time http requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef requote_uri(uri):\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        # Unquote only the unreserved characters\n        # Then quote only illegal characters (do not quote reserved,\n        # unreserved, or '%')\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except ValueError:\n        # We couldn't unquote the given URI, so let's try quoting it, but\n        # there may be unquoted '%'s in the URI. We need to make sure they're\n        # properly quoted so they do not cause issues elsewhere.\n        return quote(uri, safe=safe_without_percent)", "response": "Re - quote the given URI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a request to the Central Hub.", "response": "async def make_request(self, redirect=False):\n        '''\n        Acts as the central hub for preparing requests to be sent, and\n        returning them upon completion. Generally just pokes through\n        self's attribs and makes decisions about what to do.\n\n        Returns:\n            sock: The socket to be returned to the calling session's\n                pool.\n            Response: The response object, after any redirects. If there were\n                redirects, the redirect responses will be stored in the final\n                response object's `.history`.\n        '''\n        h11_connection = h11.Connection(our_role=h11.CLIENT)\n        (self.scheme,\n            self.host,\n            self.path,\n            self.uri_parameters,\n            self.query,\n            _) = urlparse(self.uri)\n\n        if not redirect:\n            self.initial_scheme = self.scheme\n            self.initial_netloc = self.host\n\n        # leave default the host on 80 / 443\n        # otherwise use the base host with :port appended.\n        host = (self.host if (self.port == '80' or\n                              self.port == '443')\n                else self.host.split(':')[0] + ':' + self.port)\n\n        # default header construction\n        asks_headers = c_i_dict([('Host', host),\n                                 ('Connection', 'keep-alive'),\n                                 ('Accept-Encoding', 'gzip, deflate'),\n                                 ('Accept', '*/*'),\n                                 ('Content-Length', '0'),\n                                 ('User-Agent', 'python-asks/2.2.2')\n                                 ])\n\n        # check for a CookieTracker object, and if it's there inject\n        # the relevant cookies in to the (next) request.\n        # What the fuck is this shit.\n        if self.persist_cookies is not None:\n            self.cookies.update(\n                self.persist_cookies.get_additional_cookies(\n                    self.host, self.path))\n\n        # formulate path / query and intended extra querys for use in uri\n        self._build_path()\n\n        # handle building the request body, if any\n        body = ''\n        if any((self.data, self.files, self.json is not None)):\n            content_type, content_len, body = await self._formulate_body()\n            asks_headers['Content-Type'] = content_type\n            asks_headers['Content-Length'] = content_len\n\n        # add custom headers, if any\n        # note that custom headers take precedence\n        if self.headers is not None:\n            asks_headers.update(self.headers)\n\n        # add auth\n        if self.auth is not None:\n            asks_headers.update(await self._auth_handler_pre())\n            asks_headers.update(await self._auth_handler_post_get_auth())\n\n        # add cookies\n        if self.cookies:\n            cookie_str = ''\n            for k, v in self.cookies.items():\n                cookie_str += '{}={}; '.format(k, v)\n            asks_headers['Cookie'] = cookie_str[:-1]\n\n        # Construct h11 body object, if any body.\n        if body:\n            if not isinstance(body, bytes):\n                body = bytes(body, self.encoding)\n                asks_headers['Content-Length'] = str(len(body))\n            req_body = h11.Data(data=body)\n        else:\n            req_body = None\n\n        # Construct h11 request object.\n        req = h11.Request(method=self.method,\n                          target=self.path,\n                          headers=asks_headers.items())\n\n        # call i/o handling func\n        response_obj = await self._request_io(req, req_body, h11_connection)\n\n        # check to see if the final socket object is suitable to be returned\n        # to the calling session's connection pool.\n        # We don't want to return sockets that are of a difference schema or\n        # different top level domain, as they are less likely to be useful.\n        if redirect:\n            if not (self.scheme == self.initial_scheme and\n               self.host == self.initial_netloc):\n                self.sock._active = False\n\n        if self.streaming:\n            return None, response_obj\n\n        return self.sock, response_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes care of the i/o side of the request once it's been built, and calls a couple of cleanup functions to check for redirects / store cookies and the likes. Args: h11_request (h11.Request): A h11.Request object h11_body (h11.Data): A h11.Data object, representing the request body. h11_connection (h11.Connection): The h11 connection for the request. Returns: (Response): The final response object, including any response objects in `.history` generated by redirects. Notes: This function sets off a possible call to `_redirect` which is semi-recursive.", "response": "async def _request_io(self, h11_request, h11_body, h11_connection):\n        '''\n        Takes care of the i/o side of the request once it's been built,\n        and calls a couple of cleanup functions to check for redirects / store\n        cookies and the likes.\n\n        Args:\n            h11_request (h11.Request): A h11.Request object\n            h11_body (h11.Data): A h11.Data object, representing the request\n                                 body.\n            h11_connection (h11.Connection): The h11 connection for the request.\n\n        Returns:\n            (Response): The final response object, including any response\n                        objects in `.history` generated by redirects.\n\n        Notes:\n            This function sets off a possible call to `_redirect` which\n            is semi-recursive.\n        '''\n        await self._send(h11_request, h11_body, h11_connection)\n        response_obj = await self._catch_response(h11_connection)\n        parse_cookies(response_obj, self.host)\n\n        # If there's a cookie tracker object, store any cookies we\n        # might've picked up along our travels.\n        if self.persist_cookies is not None:\n            self.persist_cookies._store_cookies(response_obj)\n\n        # Have a crack at guessing the encoding of the response.\n        response_obj._guess_encoding()\n\n        # Check to see if there's a PostResponseAuth set, and does magic.\n        if self.auth is not None:\n            response_obj = await self._auth_handler_post_check_retry(\n                response_obj)\n\n        # check redirects\n        if self.method != 'HEAD':\n            if self.max_redirects < 0:\n                raise TooManyRedirects\n            response_obj = await self._redirect(response_obj)\n        response_obj.history = self.history_objects\n\n        return response_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild the actual request path with accompanying query if any.", "response": "def _build_path(self):\n        '''\n        Constructs the actual request URL with accompanying query if any.\n\n        Returns:\n            None: But does modify self.path, which contains the final\n                request path sent to the server.\n\n        '''\n        if not self.path:\n            self.path = '/'\n\n        if self.uri_parameters:\n            self.path = self.path + ';' + requote_uri(self.uri_parameters)\n\n        if self.query:\n            self.path = (self.path + '?' + self.query)\n\n        if self.params:\n            try:\n                if self.query:\n                    self.path = self.path + self._dict_to_query(\n                        self.params, base_query=True)\n                else:\n                    self.path = self.path + self._dict_to_query(self.params)\n            except AttributeError:\n                self.path = self.path + '?' + self.params\n\n        self.path = requote_uri(self.path)\n\n        self.req_url = urlunparse(\n            (self.scheme, self.host, (self.path or ''), '', '', ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls the _check_redirect method of the supplied response object in order to determine if the http status code indicates a redirect. Returns: Response: May or may not be the result of recursive calls due to redirects! Notes: If it does redirect, it calls the appropriate method with the redirect location, returning the response object. Furthermore, if there is a redirect, this function is recursive in a roundabout way, storing the previous response object in `.history_objects`.", "response": "async def _redirect(self, response_obj):\n        '''\n        Calls the _check_redirect method of the supplied response object\n        in order to determine if the http status code indicates a redirect.\n\n        Returns:\n            Response: May or may not be the result of recursive calls due\n            to redirects!\n\n        Notes:\n            If it does redirect, it calls the appropriate method with the\n            redirect location, returning the response object. Furthermore,\n            if there is a redirect, this function is recursive in a roundabout\n            way, storing the previous response object in `.history_objects`.\n        '''\n        redirect, force_get, location = False, None, None\n        if 300 <= response_obj.status_code < 400:\n            if response_obj.status_code == 303:\n                self.data, self.json, self.files = None, None, None\n            if response_obj.status_code in [301, 305]:\n                # redirect / force GET / location\n                redirect = True\n                force_get = False\n            else:\n                redirect = True\n                force_get = True\n            location = response_obj.headers['Location']\n\n        if redirect:\n            allow_redirect = True\n            redirect_uri = urlparse(location.strip())\n            # relative redirect\n            if not redirect_uri.netloc:\n                self.uri = urlunparse(\n                    (self.scheme, self.host, *redirect_uri[2:]))\n\n            # absolute-redirect\n            else:\n                location = location.strip()\n                if self.auth is not None:\n                    if not self.auth_off_domain:\n                        allow_redirect = self._location_auth_protect(location)\n                self.uri = location\n                l_scheme, l_netloc, *_ = urlparse(location)\n                if l_scheme != self.scheme or l_netloc != self.host:\n                    await self._get_new_sock()\n\n            # follow redirect with correct http method type\n            if force_get:\n                self.history_objects.append(response_obj)\n                self.method = 'GET'\n            else:\n                self.history_objects.append(response_obj)\n            self.max_redirects -= 1\n\n            try:\n                if response_obj.headers['connection'].lower() == 'close':\n                    await self._get_new_sock()\n            except KeyError:\n                pass\n            if allow_redirect:\n                _, response_obj = await self.make_request()\n        return response_obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _get_new_sock(self):\n        '''\n        On 'Connection: close' headers we've to create a new connection.\n        This reaches in to the parent session and pulls a switcheroo, dunking\n        the current connection and requesting a new one.\n        '''\n        self.sock._active = False\n        self.sock = await self.session._grab_connection(self.uri)\n        self.port = self.sock.port", "response": "Get a new socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformulate the request body.", "response": "async def _formulate_body(self):\n        '''\n        Takes user supplied data / files and forms it / them\n        appropriately, returning the contents type, len,\n        and the request body its self.\n\n        Returns:\n            The str mime type for the Content-Type header.\n            The len of the body.\n            The body as a str.\n        '''\n        c_type, body = None, ''\n        multipart_ctype = 'multipart/form-data; boundary={}'.format(_BOUNDARY)\n\n        if self.data is not None:\n            if self.files or self.json is not None:\n                raise TypeError('data arg cannot be used in conjunction with'\n                                'files or json arg.')\n            c_type = 'application/x-www-form-urlencoded'\n            try:\n                body = self._dict_to_query(self.data, params=False)\n            except AttributeError:\n                body = self.data\n                c_type = self.mimetype or 'text/plain'\n\n        elif self.files is not None:\n            if self.data or self.json is not None:\n                raise TypeError('files arg cannot be used in conjunction with'\n                                'data or json arg.')\n            c_type = multipart_ctype\n            body = await self._multipart(self.files)\n\n        elif self.json is not None:\n            if self.data or self.files:\n                raise TypeError('json arg cannot be used in conjunction with'\n                                'data or files arg.')\n            c_type = 'application/json'\n            body = _json.dumps(self.json)\n\n        return c_type, str(len(body)), body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dict_to_query(data, params=True, base_query=False):\n        '''\n        Turns python dicts in to valid body-queries or queries for use directly\n        in the request url. Unlike the stdlib quote() and it's variations,\n        this also works on iterables like lists which are normally not valid.\n\n        The use of lists in this manner is not a great idea unless\n        the server supports it. Caveat emptor.\n\n        Returns:\n            Query part of url (or body).\n        '''\n        query = []\n\n        for k, v in data.items():\n            if v is None:\n                continue\n            if isinstance(v, (str, Number)):\n                query.append('='.join(quote_plus(x) for x in (k, str(v))))\n            elif isinstance(v, dict):\n                for key in v:\n                    query.append('='.join(quote_plus(x) for x in (k, key)))\n            elif hasattr(v, '__iter__'):\n                for elm in v:\n                    query.append('='.join(quote_plus(x) for x in (k,\n                                 quote_plus('+'.join(str(elm).split())))))\n\n        if params and query:\n            if not base_query:\n                return requote_uri('?' + '&'.join(query))\n            else:\n                return requote_uri('&' + '&'.join(query))\n\n        return requote_uri('&'.join(query))", "response": "Turn python dicts in to valid body - queries or queries for use directly\n        in the request url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat a dict of filename - > filepaths as multipart requests.", "response": "async def _multipart(self, files_dict):\n        '''\n        Forms multipart requests from a dict with name, path k/vs. Name\n        does not have to be the actual file name.\n\n        Args:\n            files_dict (dict): A dict of `filename:filepath`s, to be sent\n            as multipart files.\n\n        Returns:\n            multip_pkg (str): The strings representation of the content body,\n            multipart formatted.\n        '''\n        boundary = bytes(_BOUNDARY, self.encoding)\n        hder_format = 'Content-Disposition: form-data; name=\"{}\"'\n        hder_format_io = '; filename=\"{}\"'\n\n        multip_pkg = b''\n\n        num_of_parts = len(files_dict)\n\n        for index, kv in enumerate(files_dict.items(), start=1):\n            multip_pkg += (b'--' + boundary + b'\\r\\n')\n            k, v = kv\n\n            try:\n                pkg_body = await self._file_manager(v)\n                multip_pkg += bytes(hder_format.format(k) +\n                                    hder_format_io.format(basename(v)),\n                                    self.encoding)\n                mime_type = mimetypes.guess_type(basename(v))\n                if not mime_type[1]:\n                    mime_type = 'application/octet-stream'\n                else:\n                    mime_type = '/'.join(mime_type)\n                multip_pkg += bytes('; Content-Type: ' + mime_type,\n                                    self.encoding)\n                multip_pkg += b'\\r\\n'*2 + pkg_body\n\n            except (TypeError, FileNotFoundError):\n                pkg_body = bytes(v, self.encoding) + b'\\r\\n'\n                multip_pkg += bytes(hder_format.format(k) +\n                                    '\\r\\n'*2, self.encoding)\n                multip_pkg += pkg_body\n\n            if index == num_of_parts:\n                multip_pkg += b'--' + boundary + b'--\\r\\n'\n        return multip_pkg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _catch_response(self, h11_connection):\n        '''\n        Instantiates the parser which manages incoming data, first getting\n        the headers, storing cookies, and then parsing the response's body,\n        if any.\n\n        This function also instances the Response class in which the response\n        status line, headers, cookies, and body is stored.\n\n        It should be noted that in order to remain preformant, if the user\n        wishes to do any file IO it should use async files or risk long wait\n        times and risk connection issues server-side when using callbacks.\n\n        If a callback is used, the response's body will be None.\n\n        Returns:\n            The most recent response object.\n        '''\n\n        response = await self._recv_event(h11_connection)\n\n        resp_data = {'encoding': self.encoding,\n                     'method': self.method,\n                     'status_code': response.status_code,\n                     'reason_phrase': str(response.reason, 'utf-8'),\n                     'http_version': str(response.http_version, 'utf-8'),\n                     'headers': c_i_dict(\n                        [(str(name, 'utf-8'), str(value, 'utf-8'))\n                         for name, value in response.headers]),\n                     'body': b'',\n                     'url': self.req_url\n                     }\n\n        for header in response.headers:\n            if header[0] == b'set-cookie':\n                try:\n                    resp_data['headers']['set-cookie'].append(str(header[1],\n                                                                  'utf-8'))\n                except (KeyError, AttributeError):\n                    resp_data['headers']['set-cookie'] = [str(header[1],\n                                                          'utf-8')]\n\n        # check whether we should receive body according to RFC 7230\n        # https://tools.ietf.org/html/rfc7230#section-3.3.3\n        get_body = False\n        try:\n            if int(resp_data['headers']['content-length']) > 0:\n                get_body = True\n        except KeyError:\n            try:\n                if 'chunked' in resp_data['headers']['transfer-encoding'].lower():\n                    get_body = True\n            except KeyError:\n                if resp_data['headers'].get('connection', '').lower() == 'close':\n                    get_body = True\n\n        if get_body:\n            if self.callback is not None:\n                endof = await self._body_callback(h11_connection)\n\n            elif self.stream:\n                if not ((self.scheme == self.initial_scheme and\n                        self.host == self.initial_netloc) or\n                        resp_data['headers']['connection'].lower() == 'close'):\n                    self.sock._active = False\n\n                resp_data['body'] = StreamBody(\n                    h11_connection,\n                    self.sock,\n                    resp_data['headers'].get('content-encoding', None),\n                    resp_data['encoding'])\n\n                self.streaming = True\n\n            else:\n                while True:\n                    data = await self._recv_event(h11_connection)\n\n                    if isinstance(data, h11.Data):\n                        resp_data['body'] += data.data\n\n                    elif isinstance(data, h11.EndOfMessage):\n                        break\n\n        else:\n            endof = await self._recv_event(h11_connection)\n            assert isinstance(endof, h11.EndOfMessage)\n\n        if self.streaming:\n            return StreamResponse(**resp_data)\n\n        return Response(**resp_data)", "response": "This function is used to catch the response from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend the request and body to the ether.", "response": "async def _send(self, request_bytes, body_bytes, h11_connection):\n        '''\n        Takes a package and body, combines then, then shoots 'em off in to\n        the ether.\n\n        Args:\n            package (list of str): The header package.\n            body (str): The str representation of the body.\n        '''\n        await self.sock.send_all(h11_connection.send(request_bytes))\n        if body_bytes is not None:\n            await self.sock.send_all(h11_connection.send(body_bytes))\n        await self.sock.send_all(h11_connection.send(h11.EndOfMessage()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _auth_handler_post_get_auth(self):\n        '''\n        If the user supplied auth does rely on a response\n        (is a PostResponseAuth object) then we call the auth's __call__\n        returning a dict to update the request's headers with, as long\n        as there is an appropriate 401'd response object to calculate auth\n        details from.\n        '''\n        # pylint: disable=not-callable\n        if isinstance(self.auth, PostResponseAuth):\n            if self.history_objects:\n                authable_resp = self.history_objects[-1]\n                if authable_resp.status_code == 401:\n                    if not self.auth.auth_attempted:\n                        self.auth.auth_attempted = True\n                        return await self.auth(authable_resp, self)\n        return {}", "response": "This is the callback function for the auth handler when the request is made to get the auth details from the last response object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _auth_handler_post_check_retry(self, response_obj):\n        '''\n        The other half of _auth_handler_post_check_retry (what a mouthful).\n        If auth has not yet been attempted and the most recent response\n        object is a 401, we store that response object and retry the request\n        in exactly the same manner as before except with the correct auth.\n\n        If it fails a second time, we simply return the failed response.\n        '''\n        if isinstance(self.auth, PostResponseAuth):\n            if response_obj.status_code == 401:\n                if not self.auth.auth_attempted:\n                    self.history_objects.append(response_obj)\n                    _, r = await self.make_request()\n                    self.auth.auth_attempted = False\n                    return r\n                else:\n                    response_obj.history = self.history_objects\n                    return response_obj\n        return response_obj", "response": "This is the second half of the auth handler. It tries to retry the request if the auth has not yet been attempted."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _location_auth_protect(self, location):\n        '''\n        Checks to see if the new location is\n            1. The same top level domain\n            2. As or more secure than the current connection type\n\n        Returns:\n            True (bool): If the current top level domain is the same\n                and the connection type is equally or more secure.\n                False otherwise.\n        '''\n        netloc_sans_port = self.host.split(':')[0]\n        netloc_sans_port = netloc_sans_port.replace(\n            (re.match(_WWX_MATCH, netloc_sans_port)[0]), '')\n\n        base_domain = '.'.join(netloc_sans_port.split('.')[-2:])\n\n        l_scheme, l_netloc, _, _, _, _ = urlparse(location)\n        location_sans_port = l_netloc.split(':')[0]\n        location_sans_port = location_sans_port.replace(\n            (re.match(_WWX_MATCH, location_sans_port)[0]), '')\n\n        location_domain = '.'.join(location_sans_port.split('.')[-2:])\n\n        if base_domain == location_domain:\n            if l_scheme < self.scheme:\n                return False\n            else:\n                return True", "response": "Checks to see if the new location is the same top level domain and is more secure than the current connection type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _open_connection_http(self, location):\n        '''\n        Creates a normal async socket, returns it.\n        Args:\n            location (tuple(str, int)): A tuple of net location (eg\n                '127.0.0.1' or 'example.org') and port (eg 80 or 25000).\n        '''\n        sock = await connect_tcp(location[0], location[1], bind_host=self.source_address)\n        sock._active = True\n        return sock", "response": "Creates a normal async socket returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an async SSL socket returns it.", "response": "async def _open_connection_https(self, location):\n        '''\n        Creates an async SSL socket, returns it.\n        Args:\n            location (tuple(str, int)): A tuple of net location (eg\n                '127.0.0.1' or 'example.org') and port (eg 80 or 25000).\n        '''\n        sock = await connect_tcp(location[0],\n                                 location[1],\n                                 ssl_context=self.ssl_context or ssl.SSLContext(),\n                                 bind_host=self.source_address,\n                                 autostart_tls=True)\n        sock._active = True\n        return sock"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _connect(self, host_loc):\n        '''\n        Simple enough stuff to figure out where we should connect, and creates\n        the appropriate connection.\n        '''\n        scheme, host, path, parameters, query, fragment = urlparse(\n            host_loc)\n        if parameters or query or fragment:\n            raise ValueError('Supplied info beyond scheme, host.' +\n                             ' Host should be top level only: ', path)\n\n        host, port = get_netloc_port(scheme, host)\n\n        if scheme == 'http':\n            return await self._open_connection_http(\n                (host, int(port))), port\n        else:\n            return await self._open_connection_https(\n                (host, int(port))), port", "response": "Connect to a specific host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def request(self, method, url=None, *, path='', retries=1,\n                      connection_timeout=60, **kwargs):\n        '''\n        This is the template for all of the `http method` methods for\n        the Session.\n\n        Args:\n            method (str): A http method, such as 'GET' or 'POST'.\n            url (str): The url the request should be made to.\n            path (str): An optional kw-arg for use in Session method calls,\n                for specifying a particular path. Usually to be used in\n                conjunction with the base_location/endpoint paradigm.\n            kwargs: Any number of the following:\n                        data (dict or str): Info to be processed as a\n                            body-bound query.\n                        params (dict or str): Info to be processed as a\n                            url-bound query.\n                        headers (dict): User HTTP headers to be used in the\n                            request.\n                        encoding (str): The str representation of the codec to\n                            process the request under.\n                        json (dict): A dict to be formatted as json and sent in\n                            the request body.\n                        files (dict): A dict of `filename:filepath`s to be sent\n                            as multipart.\n                        cookies (dict): A dict of `name:value` cookies to be\n                            passed in request.\n                        callback (func): A callback function to be called on\n                            each bytechunk of of the response body.\n                        timeout (int or float): A numeric representation of the\n                            longest time to wait on a complete response once a\n                            request has been sent.\n                        retries (int): The number of attempts to try against\n                            connection errors.\n                        max_redirects (int): The maximum number of redirects\n                            allowed.\n                        persist_cookies (True or None): Passing True\n                            instantiates a CookieTracker object to manage the\n                            return of cookies to the server under the relevant\n                            domains.\n                        auth (child of AuthBase): An object for handling auth\n                            construction.\n\n        When you call something like Session.get() or asks.post(), you're\n        really calling a partial method that has the 'method' argument\n        pre-completed.\n        '''\n        timeout = kwargs.get('timeout', None)\n        req_headers = kwargs.pop('headers', None)\n\n        if self.headers is not None:\n            headers = copy(self.headers)\n        if req_headers is not None:\n            headers.update(req_headers)\n        req_headers = headers\n\n        async with self.sema:\n            if url is None:\n                url = self._make_url() + path\n\n            retry = False\n\n            sock = None\n            try:\n                sock = await timeout_manager(\n                    connection_timeout, self._grab_connection, url)\n                port = sock.port\n\n                req_obj = RequestProcessor(\n                    self,\n                    method,\n                    url,\n                    port,\n                    headers=req_headers,\n                    encoding=self.encoding,\n                    sock=sock,\n                    persist_cookies=self._cookie_tracker,\n                    **kwargs\n                )\n\n                try:\n                    if timeout is None:\n                        sock, r = await req_obj.make_request()\n                    else:\n                        sock, r = await timeout_manager(timeout, req_obj.make_request)\n                except BadHttpResponse:\n                    if timeout is None:\n                        sock, r = await req_obj.make_request()\n                    else:\n                        sock, r = await timeout_manager(timeout, req_obj.make_request)\n\n                if sock is not None:\n                    try:\n                        if r.headers['connection'].lower() == 'close':\n                            sock._active = False\n                            await sock.close()\n                    except KeyError:\n                        pass\n                    await self.return_to_pool(sock)\n\n            # ConnectionErrors are special. They are the only kind of exception\n            # we ever want to suppress. All other exceptions are re-raised or\n            # raised through another exception.\n            except ConnectionError as e:\n                if retries > 0:\n                    retry = True\n                    retries -= 1\n                else:\n                    raise e\n\n            except Exception as e:\n                if sock:\n                    await self._handle_exception(e, sock)\n                raise\n\n            # any BaseException is considered unlawful murder, and\n            # Session.cleanup should be called to tidy up sockets.\n            except BaseException as e:\n                if sock:\n                    await sock.close()\n                raise e\n\n        if retry:\n            return (await self.request(method,\n                                       url,\n                                       path=path,\n                                       retries=retries,\n                                       headers=headers,\n                                       **kwargs))\n\n        return r", "response": "This is the asynchronous method that handles all of the HTTP methods for the Session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles exceptions raised by the server.", "response": "async def _handle_exception(self, e, sock):\n        \"\"\"\n        Given an exception, we want to handle it appropriately. Some exceptions we\n        prefer to shadow with an asks exception, and some we want to raise directly.\n        In all cases we clean up the underlying socket.\n        \"\"\"\n        if isinstance(e, (RemoteProtocolError, AssertionError)):\n            await sock.close()\n            raise BadHttpResponse('Invalid HTTP response from server.') from e\n\n        if isinstance(e, Exception):\n            await sock.close()\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the response s body as a python dict", "response": "def json(self, **kwargs):\n        '''\n        If the response's body is valid json, we load it as a python dict\n        and return it.\n        '''\n        body = self._decompress(self.encoding)\n        return _json.loads(body, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nraise BadStatus if one occurred.", "response": "def raise_for_status(self):\n        '''\n        Raise BadStatus if one occurred.\n        '''\n        if 400 <= self.status_code < 500:\n            raise BadStatus('{} Client Error: {} for url: {}'.format(self.status_code, self.reason_phrase, self.url), self.status_code)\n        elif 500 <= self.status_code < 600:\n            raise BadStatus('{} Server Error: {} for url: {}'.format(self.status_code, self.reason_phrase, self.url), self.status_code)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_cookies(response, host):\n    cookie_pie = []\n    try:\n        for cookie in response.headers['set-cookie']:\n            cookie_jar = {}\n            name_val, *rest = cookie.split(';')\n            name, value = name_val.split('=', 1)\n            cookie_jar['name'] = name.strip()\n            cookie_jar['value'] = value\n            for item in rest:\n                try:\n                    name, value = item.split('=')\n                    if value.startswith('.'):\n                        value = value[1:]\n                    cookie_jar[name.lower().lstrip()] = value\n                except ValueError:\n                    cookie_jar[item.lower().lstrip()] = True\n            cookie_pie.append(cookie_jar)\n        response.cookies = [Cookie(host, x) for x in cookie_pie]\n    except KeyError:\n        pass", "response": "Parses the response. cookies and adds them to the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload all the photos coming from the redactor editor and returns all the files", "response": "def upload_photos(request):\n    '''\n    takes all the images coming from the redactor editor and\n    stores it in the database and returns all the files\n    '''\n    upurl = ''\n    if request.FILES.get(\"upload\"):\n        f = request.FILES.get(\"upload\")\n        obj = Image_File.objects.create(upload=f, is_image=True)\n        obj.save()\n        thumbnail_name = 'thumb' + f.name\n        if getattr(settings, 'AWS_ENABLED', False):\n            image_file = requests.get(obj.upload.url, stream=True)\n            with open(thumbnail_name, 'wb') as destination:\n                for chunk in image_file.iter_content():\n                    destination.write(chunk)\n        else:\n            image_file = f\n            with open(thumbnail_name, 'wb') as destination:\n                for chunk in image_file.chunks():\n                    destination.write(chunk)\n        im = Image.open(destination.name)\n        size = (128, 128)\n        im.thumbnail(size)\n        im.save(thumbnail_name)\n        with open(thumbnail_name, 'rb') as imdata:\n            obj.thumbnail.save(thumbnail_name, File(imdata))\n        obj.save()\n        os.remove(os.path.join(settings.BASE_DIR, thumbnail_name))\n        upurl = \"/\" + obj.upload.url\n    return HttpResponse(\n        \"\"\"<script type='text/javascript'>\n        window.parent.CKEDITOR.tools.callFunction({0}, '{1}');\n        </script>\"\"\".format(request.GET['CKEditorFuncNum'], upurl)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all the images from the data base", "response": "def recent_photos(request):\n    ''' returns all the images from the data base '''\n    imgs = []\n    for obj in Image_File.objects.filter(is_image=True).order_by(\"-date_created\"):\n        upurl = \"/\" + obj.upload.url\n        thumburl = \"\"\n        if obj.thumbnail:\n            thumburl = \"/\" + obj.thumbnail.url\n        imgs.append({'src': upurl, 'thumb': thumburl, 'is_image': True})\n    return render_to_response('dashboard/browse.html', {'files': imgs})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unmarshal(self, v):\n        if self.units:\n            # Note that we don't want to cast to type in this case!\n            if not isinstance(v, Quantity):\n                v = self.units(v)\n        elif not isinstance(v, self.type):\n            v = self.type(v)\n        return v", "response": "Unmarshalls a value from parsed JSON structure to native python representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unmarshal(self, v):\n        if not isinstance(v, date):\n            # 2012-12-13\n            v = datetime.strptime(v, \"%Y-%m-%d\").date()\n        return v", "response": "Unmarshalls a string representation of a date in 2012 - 12 - 13 format to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a timestamp in 2012 - 12 - 13T03 - 43 : 19Z format to a datetime. datetime object.", "response": "def unmarshal(self, v):\n        \"\"\"\n        Convert a timestamp in \"2012-12-13T03:43:19Z\" format to a `datetime.datetime` object.\n        \"\"\"\n        if not isinstance(v, datetime):\n            if isinstance(v, six.integer_types):\n                v = arrow.get(v)\n            else:\n                try:\n                    # Most dates are in this format 2012-12-13T03:43:19Z\n                    v = datetime.strptime(v, \"%Y-%m-%dT%H:%M:%SZ\")\n                except ValueError:\n                    # ... but not all.\n                    v = arrow.get(v).datetime\n            # Translate to specified TZ\n            v = v.replace(tzinfo=self.tzinfo)\n\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef marshal(self, v):\n        return \"{lat},{lon}\".format(lat=v.lat, lon=v.lon) if v else None", "response": "Turn this value into format for wire ( JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unmarshal(self, v):\n        if not isinstance(v, tzinfo):\n            # (GMT-08:00) America/Los_Angeles\n            tzname = v.split(' ', 1)[1]\n            v = pytz.timezone(tzname)\n        return v", "response": "Convert a timestamp in format \"(GMT - 08 - 00 ) to\n a pytz. timestamp object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unmarshal(self, v):\n        if not isinstance(v, timedelta):\n            v = timedelta(seconds=v)\n        return v", "response": "Unmarshalls a value from parsed JSON structure to native python representation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn this value into API format.", "response": "def marshal(self, v):\n        \"\"\"\n        Turn this value into API format.\n\n        Do a reverse dictionary lookup on choices to find the original value. If\n        there are no keys or too many keys for now we raise a NotImplementedError\n        as marshal is not used anywhere currently. In the future we will want to\n        fail gracefully.\n        \"\"\"\n        if v:\n            orig = [i for i in self.choices if self.choices[i] == v]\n            if len(orig) == 1:\n                return orig[0]\n            elif len(orig) == 0:\n                # No such choice\n                raise NotImplementedError(\"No such reverse choice {0} for field {1}.\".format(v, self))\n            else:\n                # Too many choices. We could return one possible choice (e.g. orig[0]).\n                raise NotImplementedError(\"Too many reverse choices {0} for value {1} for field {2}\".format(orig, v, self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unmarshal(self, v):\n        try:\n            return self.choices[v]\n        except KeyError:\n            self.log.warning(\"No such choice {0} for field {1}.\".format(v, self))\n            # Just return the value from the API\n            return v", "response": "Unmarshalls the value from Strava API format to useful python representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncast the specified value to the entity type.", "response": "def unmarshal(self, value, bind_client=None):\n        \"\"\"\n        Cast the specified value to the entity type.\n        \"\"\"\n        #self.log.debug(\"Unmarshall {0!r}: {1!r}\".format(self, value))\n        if not isinstance(value, self.type):\n            o = self.type()\n            if bind_client is not None and hasattr(o.__class__, 'bind_client'):\n                o.bind_client = bind_client\n\n            if isinstance(value, dict):\n                for (k, v) in value.items():\n                    if not hasattr(o.__class__, k):\n                        self.log.warning(\"Unable to set attribute {0} on entity {1!r}\".format(k, o))\n                    else:\n                        #self.log.debug(\"Setting attribute {0} on entity {1!r}\".format(k, o))\n                        setattr(o, k, v)\n                value = o\n            else:\n                raise Exception(\"Unable to unmarshall object {0!r}\".format(value))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef marshal(self, values):\n        if values is not None:\n            return [super(EntityCollection, self).marshal(v) for v in values]", "response": "Turn a list of entities into a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unmarshal(self, values, bind_client=None):\n        if values is not None:\n            return [super(EntityCollection, self).unmarshal(v, bind_client=bind_client) for v in values]", "response": "Unmarshall the list of items into a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the URL needed to authorize a Strava user to access a Strava user s information.", "response": "def authorization_url(self, client_id, redirect_uri, approval_prompt='auto',\n                          scope=None, state=None):\n        \"\"\"\n        Get the URL needed to authorize your application to access a Strava user's information.\n\n        :param client_id: The numeric developer client id.\n        :type client_id: int\n\n        :param redirect_uri: The URL that Strava will redirect to after successful (or failed) authorization.\n        :type redirect_uri: str\n\n        :param approval_prompt: Whether to prompt for approval even if approval already granted to app.\n                                Choices are 'auto' or 'force'.  (Default is 'auto')\n        :type approval_prompt: str\n\n        :param scope: The access scope required.  Omit to imply \"public\".\n                      Valid values are 'read', 'read_all', 'profile:read_all', 'profile:write', 'profile:read_all',\n                      'activity:read_all', 'activity:write'\n        :type scope: str\n\n        :param state: An arbitrary variable that will be returned to your application in the redirect URI.\n        :type state: str\n\n        :return: The URL to use for authorization link.\n        :rtype: str\n        \"\"\"\n        return self.protocol.authorization_url(client_id=client_id,\n                                               redirect_uri=redirect_uri,\n                                               approval_prompt=approval_prompt,\n                                               scope=scope, state=state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexchange the temporary authorization code for a temporary access token and a refresh token.", "response": "def exchange_code_for_token(self, client_id, client_secret, code):\n        \"\"\"\n        Exchange the temporary authorization code (returned with redirect from strava authorization URL)\n        for a temporary access token and a refresh token (used to obtain the next access token later on).\n\n        :param client_id: The numeric developer client id.\n        :type client_id: int\n\n        :param client_secret: The developer client secret\n        :type client_secret: str\n\n        :param code: The temporary authorization code\n        :type code: str\n\n        :return: Dictionary containing the access_token, refresh_token\n                 and expires_at (number of seconds since Epoch when the provided access token will expire)\n        :rtype: dict\n        \"\"\"\n        return self.protocol.exchange_code_for_token(client_id=client_id,\n                                                     client_secret=client_secret,\n                                                     code=code)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexchange the temporary authorization code and a temporary access token and a refresh token.", "response": "def refresh_access_token(self, client_id, client_secret, refresh_token):\n        \"\"\"\n        Exchange the temporary authorization code (returned with redirect from strava authorization URL)\n        for a temporary access token and a refresh token (used to obtain the next access token later on).\n\n        :param client_id: The numeric developer client id.\n        :type client_id: int\n\n        :param client_secret: The developer client secret\n        :type client_secret: str\n\n        :param refresh_token: The refresh token obtain from a previous authorization request\n        :type refresh_token: str\n\n        :return: Dictionary containing the access_token, refresh_token\n                 and expires_at (number of seconds since Epoch when the provided access token will expire)\n        :rtype: dict\n        \"\"\"\n        return self.protocol.refresh_access_token(client_id=client_id,\n                                                  client_secret=client_secret,\n                                                  refresh_token=refresh_token)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the specified datetime value to a unix epoch timestamp.", "response": "def _utc_datetime_to_epoch(self, activity_datetime):\n        \"\"\"\n        Convert the specified datetime value to a unix epoch timestamp (seconds since epoch).\n\n        :param activity_datetime: A string which may contain tzinfo (offset) or a datetime object (naive datetime will\n                                    be considered to be UTC).\n        :return: Epoch timestamp.\n        :rtype: int\n        \"\"\"\n        if isinstance(activity_datetime, str):\n            activity_datetime = arrow.get(activity_datetime).datetime\n        assert isinstance(activity_datetime, datetime)\n        if activity_datetime.tzinfo:\n            activity_datetime = activity_datetime.astimezone(pytz.utc)\n\n        return calendar.timegm(activity_datetime.timetuple())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting activities for authenticated user sorted by newest first.", "response": "def get_activities(self, before=None, after=None, limit=None):\n        \"\"\"\n        Get activities for authenticated user sorted by newest first.\n\n        http://strava.github.io/api/v3/activities/\n\n\n        :param before: Result will start with activities whose start date is\n                       before specified date. (UTC)\n        :type before: datetime.datetime or str or None\n\n        :param after: Result will start with activities whose start date is after\n                      specified value. (UTC)\n        :type after: datetime.datetime or str or None\n\n        :param limit: How many maximum activities to return.\n        :type limit: int or None\n\n        :return: An iterator of :class:`stravalib.model.Activity` objects.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n\n        if before:\n            before = self._utc_datetime_to_epoch(before)\n\n        if after:\n            after = self._utc_datetime_to_epoch(after)\n\n        params = dict(before=before, after=after)\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/athlete/activities',\n                                           **params)\n\n        return BatchedResultsIterator(entity=model.Activity,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_athlete(self, athlete_id=None):\n        if athlete_id is None:\n            raw = self.protocol.get('/athlete')\n        else:\n            raise NotImplementedError(\"The /athletes/{id} endpoint was removed by Strava.  \"\n                                      \"See https://developers.strava.com/docs/january-2018-update/\")\n\n            # raw = self.protocol.get('/athletes/{athlete_id}', athlete_id=athlete_id)\n\n        return model.Athlete.deserialize(raw, bind_client=self)", "response": "Gets the specified athlete."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_athlete_friends(self, athlete_id=None, limit=None):\n        if athlete_id is None:\n            result_fetcher = functools.partial(self.protocol.get, '/athlete/friends')\n        else:\n            raise NotImplementedError(\"The /athletes/{id}/friends endpoint was removed by Strava.  \"\n                                      \"See https://developers.strava.com/docs/january-2018-update/\")\n            # result_fetcher = functools.partial(self.protocol.get,\n            #                                    '/athletes/{id}/friends',\n            #                                    id=athlete_id)\n\n        return BatchedResultsIterator(entity=model.Athlete,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)", "response": "Gets friends for current or specified athlete."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_athlete(self, city=None, state=None, country=None, sex=None, weight=None):\n        params = {'city': city,\n                  'state': state,\n                  'country': country,\n                  'sex': sex}\n        params = {k: v for (k, v) in params.items() if v is not None}\n        if weight is not None:\n            params['weight'] = float(weight)\n\n        raw_athlete = self.protocol.put('/athlete', **params)\n        return model.Athlete.deserialize(raw_athlete, bind_client=self)", "response": "Update the properties of the authorized athlete."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Q / KOMs / CRs for specified athlete.", "response": "def get_athlete_koms(self, athlete_id, limit=None):\n        \"\"\"\n        Gets Q/KOMs/CRs for specified athlete.\n\n        KOMs are returned as `stravalib.model.SegmentEffort` objects.\n\n        http://strava.github.io/api/v3/athlete/#koms\n\n        :param athlete_id: The ID of the athlete.\n        :type athlete_id: int\n\n        :param limit: Maximum number of KOM segment efforts to return (default unlimited).\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.SegmentEffort` objects.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/athletes/{id}/koms',\n                                           id=athlete_id)\n\n        return BatchedResultsIterator(entity=model.SegmentEffort,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_athlete_stats(self, athlete_id=None):\n        if athlete_id is None:\n            athlete_id = self.get_athlete().id\n\n        raw = self.protocol.get('/athletes/{id}/stats', id=athlete_id)\n        # TODO: Better error handling - this will return a 401 if this athlete\n        #       is not the authenticated athlete.\n\n        return model.AthleteStats.deserialize(raw)", "response": "Returns the Statistics for the authenticated athlete."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all clubs for the currently authenticated athlete.", "response": "def get_athlete_clubs(self):\n        \"\"\"\n        List the clubs for the currently authenticated athlete.\n\n        http://strava.github.io/api/v3/clubs/#get-athletes\n\n        :return: A list of :class:`stravalib.model.Club`\n        :rtype: :py:class:`list`\n        \"\"\"\n        club_structs = self.protocol.get('/athlete/clubs')\n        return [model.Club.deserialize(raw, bind_client=self) for raw in club_structs]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_club(self, club_id):\n        raw = self.protocol.get(\"/clubs/{id}\", id=club_id)\n        return model.Club.deserialize(raw, bind_client=self)", "response": "Returns a specific club object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the member objects for the specified club ID.", "response": "def get_club_members(self, club_id, limit=None):\n        \"\"\"\n        Gets the member objects for specified club ID.\n\n        http://strava.github.io/api/v3/clubs/#get-members\n\n        :param club_id: The numeric ID for the club.\n        :type club_id: int\n\n        :param limit: Maximum number of athletes to return. (default unlimited)\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.Athlete` objects.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/clubs/{id}/members',\n                                           id=club_id)\n\n        return BatchedResultsIterator(entity=model.Athlete, bind_client=self,\n                                      result_fetcher=result_fetcher, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_club_activities(self, club_id, limit=None):\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/clubs/{id}/activities',\n                                           id=club_id)\n\n        return BatchedResultsIterator(entity=model.Activity, bind_client=self,\n                                      result_fetcher=result_fetcher, limit=limit)", "response": "Gets the activities associated with a given club."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_activity(self, activity_id, include_all_efforts=False):\n        raw = self.protocol.get('/activities/{id}', id=activity_id,\n                                include_all_efforts=include_all_efforts)\n        return model.Activity.deserialize(raw, bind_client=self)", "response": "Gets the specified activity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_activity(self, name, activity_type, start_date_local, elapsed_time,\n                        description=None, distance=None):\n        \"\"\"\n        Create a new manual activity.\n\n        If you would like to create an activity from an uploaded GPS file, see the\n        :meth:`stravalib.client.Client.upload_activity` method instead.\n\n        :param name: The name of the activity.\n        :type name: str\n\n        :param activity_type: The activity type (case-insensitive).\n                              Possible values: ride, run, swim, workout, hike, walk, nordicski,\n                              alpineski, backcountryski, iceskate, inlineskate, kitesurf, rollerski,\n                              windsurf, workout, snowboard, snowshoe\n        :type activity_type: str\n\n        :param start_date_local: Local date/time of activity start. (TZ info will be ignored)\n        :type start_date_local: :class:`datetime.datetime` or string in ISO8601 format.\n\n        :param elapsed_time: The time in seconds or a :class:`datetime.timedelta` object.\n        :type elapsed_time: :class:`datetime.timedelta` or int (seconds)\n\n        :param description: The description for the activity.\n        :type description: str\n\n        :param distance: The distance in meters (float) or a :class:`units.quantity.Quantity` instance.\n        :type distance: :class:`units.quantity.Quantity` or float (meters)\n        \"\"\"\n        if isinstance(elapsed_time, timedelta):\n            elapsed_time = unithelper.timedelta_to_seconds(elapsed_time)\n\n        if isinstance(distance, Quantity):\n            distance = float(unithelper.meters(distance))\n\n        if isinstance(start_date_local, datetime):\n            start_date_local = start_date_local.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        if not activity_type.lower() in [t.lower() for t in model.Activity.TYPES]:\n            raise ValueError(\"Invalid activity type: {0}.  Possible values: {1!r}\".format(activity_type, model.Activity.TYPES))\n\n        params = dict(name=name, type=activity_type, start_date_local=start_date_local,\n                      elapsed_time=elapsed_time)\n\n        if description is not None:\n            params['description'] = description\n\n        if distance is not None:\n            params['distance'] = distance\n\n        raw_activity = self.protocol.post('/activities', **params)\n\n        return model.Activity.deserialize(raw_activity, bind_client=self)", "response": "Create a new activity in the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the properties of a specific activity.", "response": "def update_activity(self, activity_id, name=None, activity_type=None,\n                        private=None, commute=None, trainer=None, gear_id=None,\n                        description=None,device_name=None):\n        \"\"\"\n        Updates the properties of a specific activity.\n\n        http://strava.github.io/api/v3/activities/#put-updates\n\n        :param activity_id: The ID of the activity to update.\n        :type activity_id: int\n\n        :param name: The name of the activity.\n        :param activity_type: The activity type (case-insensitive).\n                              Possible values: ride, run, swim, workout, hike,\n                              walk, nordicski, alpineski, backcountryski,\n                              iceskate, inlineskate, kitesurf, rollerski,\n                              windsurf, workout, snowboard, snowshoe\n        :param private: Whether the activity is private.\n        :param commute: Whether the activity is a commute.\n        :param trainer: Whether this is a trainer activity.\n        :param gear_id: Alpha-numeric ID of gear (bike, shoes) used on this activity.\n        :param description: Description for the activity.\n        :param device_name: Device name for the activity\n\n        :return: The updated activity.\n        :rtype: :class:`stravalib.model.Activity`\n        \"\"\"\n\n        # Convert the kwargs into a params dict\n        params = {}\n\n        if name is not None:\n            params['name'] = name\n\n        if activity_type is not None:\n            if not activity_type.lower() in [t.lower() for t in model.Activity.TYPES]:\n                raise ValueError(\"Invalid activity type: {0}.  Possible values: {1!r}\".format(activity_type, model.Activity.TYPES))\n            params['type'] = activity_type\n\n        if private is not None:\n            params['private'] = int(private)\n\n        if commute is not None:\n            params['commute'] = int(commute)\n\n        if trainer is not None:\n            params['trainer'] = int(trainer)\n\n        if gear_id is not None:\n            params['gear_id'] = gear_id\n\n        if description is not None:\n            params['description'] = description\n            \n        if device_name is not None:\n            params['device_name'] = device_name\n\n        raw_activity = self.protocol.put('/activities/{activity_id}', activity_id=activity_id, **params)\n\n        return model.Activity.deserialize(raw_activity, bind_client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload_activity(self, activity_file, data_type, name=None, description=None,\n                        activity_type=None, private=None, external_id=None):\n        \"\"\"\n        Uploads a GPS file (tcx, gpx) to create a new activity for current athlete.\n\n        http://strava.github.io/api/v3/athlete/#get-details\n\n        :param activity_file: The file object to upload or file contents.\n        :type activity_file: file or str\n\n        :param data_type: File format for upload. Possible values: fit, fit.gz, tcx, tcx.gz, gpx, gpx.gz\n        :type data_type: str\n\n        :param name: (optional) if not provided, will be populated using start date and location, if available\n        :type name: str\n\n        :param description: (optional) The description for the activity\n        :type name: str\n\n        :param activity_type: (optional) case-insensitive type of activity.\n                              possible values: ride, run, swim, workout, hike, walk,\n                              nordicski, alpineski, backcountryski, iceskate, inlineskate,\n                              kitesurf, rollerski, windsurf, workout, snowboard, snowshoe\n                              Type detected from file overrides, uses athlete's default type if not specified\n        :type activity_type: str\n\n        :param private: (optional) set to True to mark the resulting activity as private, 'view_private' permissions will be necessary to view the activity\n        :type private: bool\n\n        :param external_id: (optional) An arbitrary unique identifier may be specified which will be included in status responses.\n        :type external_id: str\n        \"\"\"\n        if not hasattr(activity_file, 'read'):\n            if isinstance(activity_file, six.string_types):\n                activity_file = BytesIO(activity_file.encode('utf-8'))\n            elif isinstance(activity_file, str):\n                activity_file = BytesIO(activity_file)\n            else:\n                raise TypeError(\"Invalid type specified for activity_file: {0}\".format(type(activity_file)))\n\n        valid_data_types = ('fit', 'fit.gz', 'tcx', 'tcx.gz', 'gpx', 'gpx.gz')\n        if not data_type in valid_data_types:\n            raise ValueError(\"Invalid data type {0}. Possible values {1!r}\".format(data_type, valid_data_types))\n\n        params = {'data_type': data_type}\n        if name is not None:\n            params['name'] = name\n        if description is not None:\n            params['description'] = description\n        if activity_type is not None:\n            if not activity_type.lower() in [t.lower() for t in model.Activity.TYPES]:\n                raise ValueError(\"Invalid activity type: {0}.  Possible values: {1!r}\".format(activity_type, model.Activity.TYPES))\n            params['activity_type'] = activity_type\n        if private is not None:\n            params['private'] = int(private)\n        if external_id is not None:\n            params['external_id'] = external_id\n\n        initial_response = self.protocol.post('/uploads',\n                                              files={'file': activity_file},\n                                              check_for_errors=False,\n                                              **params)\n\n        return ActivityUploader(self, response=initial_response)", "response": "Uploads a GPS file to the athlete and returns the ID of the newly created activity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the list of zones for the activity.", "response": "def get_activity_zones(self, activity_id):\n        \"\"\"\n        Gets zones for activity.\n\n        Requires premium account.\n\n        http://strava.github.io/api/v3/activities/#zones\n\n        :param activity_id: The activity for which to zones.\n        :type activity_id: int\n\n        :return: An list of :class:`stravalib.model.ActivityComment` objects.\n        :rtype: :py:class:`list`\n        \"\"\"\n        zones = self.protocol.get('/activities/{id}/zones', id=activity_id)\n        # We use a factory to give us the correct zone based on type.\n        return [model.BaseActivityZone.deserialize(z, bind_client=self) for z in zones]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the comments for an activity.", "response": "def get_activity_comments(self, activity_id, markdown=False, limit=None):\n        \"\"\"\n        Gets the comments for an activity.\n\n        http://strava.github.io/api/v3/comments/#list\n\n        :param activity_id: The activity for which to fetch comments.\n        :type activity_id: int\n\n        :param markdown: Whether to include markdown in comments (default is false/filterout).\n        :type markdown: bool\n\n        :param limit: Max rows to return (default unlimited).\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.ActivityComment` objects.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n        result_fetcher = functools.partial(self.protocol.get, '/activities/{id}/comments',\n                                           id=activity_id, markdown=int(markdown))\n\n        return BatchedResultsIterator(entity=model.ActivityComment,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_activity_kudos(self, activity_id, limit=None):\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/activities/{id}/kudos',\n                                           id=activity_id)\n\n        return BatchedResultsIterator(entity=model.ActivityKudos,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)", "response": "Gets the kudos for an activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the photos from an activity.", "response": "def get_activity_photos(self, activity_id, size=None, only_instagram=False):\n        \"\"\"\n        Gets the photos from an activity.\n\n        http://strava.github.io/api/v3/photos/\n\n        :param activity_id: The activity for which to fetch kudos.\n        :type activity_id: int\n\n        :param size: the requested size of the activity's photos. URLs for the photos will be returned that best match\n                    the requested size. If not included, the smallest size is returned\n        :type size: int\n\n        :param only_instagram: Parameter to preserve legacy behavior of only returning Instagram photos.\n        :type only_instagram: bool\n\n        :return: An iterator of :class:`stravalib.model.ActivityPhoto` objects.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n        params = {}\n\n        if not only_instagram:\n            params['photo_sources'] = 'true'\n\n        if size is not None:\n            params['size'] = size\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/activities/{id}/photos',\n                                           id=activity_id, **params)\n\n        return BatchedResultsIterator(entity=model.ActivityPhoto,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_activity_laps(self, activity_id):\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/activities/{id}/laps',\n                                           id=activity_id)\n\n        return BatchedResultsIterator(entity=model.ActivityLap,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher)", "response": "Gets the laps from an activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting details for an item of gear.", "response": "def get_gear(self, gear_id):\n        \"\"\"\n        Get details for an item of gear.\n\n        http://strava.github.io/api/v3/gear/#show\n\n        :param gear_id: The gear id.\n        :type gear_id: str\n\n        :return: The Bike or Shoe subclass object.\n        :rtype: :class:`stravalib.model.Gear`\n        \"\"\"\n        return model.Gear.deserialize(self.protocol.get('/gear/{id}', id=gear_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_segment_effort(self, effort_id):\n        return model.SegmentEffort.deserialize(self.protocol.get('/segment_efforts/{id}',\n                                                                 id=effort_id))", "response": "Get a specific segment effort by ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a specific segment by ID.", "response": "def get_segment(self, segment_id):\n        \"\"\"\n        Gets a specific segment by ID.\n\n        http://strava.github.io/api/v3/segments/#retrieve\n\n        :param segment_id: The segment to fetch.\n        :type segment_id: int\n\n        :return: A segment object.\n        :rtype: :class:`stravalib.model.Segment`\n        \"\"\"\n        return model.Segment.deserialize(self.protocol.get('/segments/{id}',\n                                         id=segment_id), bind_client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a summary representation of the segments starred by the authenticated user. Pagination is supported.", "response": "def get_starred_segments(self, limit=None):\n        \"\"\"\n        Returns a summary representation of the segments starred by the\n         authenticated user. Pagination is supported.\n\n        http://strava.github.io/api/v3/segments/#starred\n\n        :param limit: (optional), limit number of starred segments returned.\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.Segment` starred by authenticated user.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n\n        params = {}\n        if limit is not None:\n            params[\"limit\"] = limit\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/segments/starred')\n\n        return BatchedResultsIterator(entity=model.Segment,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a summary representation of the segments starred by the specified athlete.", "response": "def get_athlete_starred_segments(self, athlete_id, limit=None):\n        \"\"\"\n        Returns a summary representation of the segments starred by the\n         specified athlete. Pagination is supported.\n\n        http://strava.github.io/api/v3/segments/#starred\n\n        :param athlete_id: The ID of the athlete.\n        :type athlete_id: int\n\n        :param limit: (optional), limit number of starred segments returned.\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.Segment` starred by authenticated user.\n        :rtype: :class:`BatchedResultsIterator`\n        \"\"\"\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/athletes/{id}/segments/starred',\n                                           id=athlete_id)\n\n        return BatchedResultsIterator(entity=model.Segment,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_segment_leaderboard(self, segment_id, gender=None, age_group=None, weight_class=None,\n                                following=None, club_id=None, timeframe=None, top_results_limit=None,\n                                page=None, context_entries = None):\n        \"\"\"\n        Gets the leaderboard for a segment.\n\n        http://strava.github.io/api/v3/segments/#leaderboard\n\n        Note that by default Strava will return the top 10 results, and if the current user has ridden\n        that segment, the current user's result along with the two results above in rank and the two\n        results below will be included.  The top X results can be configured by setting the top_results_limit\n        parameter; however, the other 5 results will be included if the current user has ridden that segment.\n        (i.e. if you specify top_results_limit=15, you will get a total of 20 entries back.)\n\n        :param segment_id: ID of the segment.\n        :type segment_id: int\n\n        :param gender: (optional) 'M' or 'F'\n        :type gender: str\n\n        :param age_group: (optional) '0_24', '25_34', '35_44', '45_54', '55_64', '65_plus'\n        :type age_group: str\n\n        :param weight_class: (optional) pounds '0_124', '125_149', '150_164', '165_179', '180_199', '200_plus'\n                             or kilograms '0_54', '55_64', '65_74', '75_84', '85_94', '95_plus'\n        :type weight_class: str\n\n        :param following: (optional) Limit to athletes current user is following.\n        :type following: bool\n\n        :param club_id: (optional) limit to specific club\n        :type club_id: int\n\n        :param timeframe: (optional)  'this_year', 'this_month', 'this_week', 'today'\n        :type timeframe: str\n\n        :param top_results_limit: (optional, strava default is 10 + 5 from end) How many of leading leaderboard entries to display.\n                            See description for why this is a little confusing.\n        :type top_results_limit: int\n\n        :param page: (optional, strava default is 1) Page number of leaderboard to return, sorted by highest ranking leaders\n        :type page: int\n\n        :param context_entries: (optional, strava default is 2, max is 15) number of entries surrounding requesting athlete to return\n        :type context_entries: int\n\n        :return: The SegmentLeaderboard for the specified page (default: 1)\n        :rtype: :class:`stravalib.model.SegmentLeaderboard`\n\n        \"\"\"\n        params = {}\n        if gender is not None:\n            if gender.upper() not in ('M', 'F'):\n                raise ValueError(\"Invalid gender: {0}. Possible values: 'M' or 'F'\".format(gender))\n            params['gender'] = gender\n\n        valid_age_groups = ('0_24', '25_34', '35_44', '45_54', '55_64', '65_plus')\n        if age_group is not None:\n            if not age_group in valid_age_groups:\n                raise ValueError(\"Invalid age group: {0}.  Possible values: {1!r}\".format(age_group, valid_age_groups))\n            params['age_group'] = age_group\n\n        valid_weight_classes = ('0_124', '125_149', '150_164', '165_179', '180_199', '200_plus',\n                                '0_54', '55_64', '65_74', '75_84', '85_94', '95_plus')\n        if weight_class is not None:\n            if not weight_class in valid_weight_classes:\n                raise ValueError(\"Invalid weight class: {0}.  Possible values: {1!r}\".format(weight_class, valid_weight_classes))\n            params['weight_class'] = weight_class\n\n        if following is not None:\n            params['following'] = int(following)\n\n        if club_id is not None:\n            params['club_id'] = club_id\n\n        if timeframe is not None:\n            valid_timeframes = 'this_year', 'this_month', 'this_week', 'today'\n            if not timeframe in valid_timeframes:\n                raise ValueError(\"Invalid timeframe: {0}.  Possible values: {1!r}\".format(timeframe, valid_timeframes))\n            params['date_range'] = timeframe\n\n        if top_results_limit is not None:\n            params['per_page'] = top_results_limit\n\n        if page is not None:\n            params['page'] = page\n\n        if context_entries is not None:\n            params['context_entries'] = context_entries\n\n        return model.SegmentLeaderboard.deserialize(self.protocol.get('/segments/{id}/leaderboard',\n                                                                      id=segment_id,\n                                                                      **params),\n                                                    bind_client=self)", "response": "Gets the leaderboard for a given segment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_segment_efforts(self, segment_id, athlete_id=None,\n                            start_date_local=None, end_date_local=None,\n                            limit=None):\n        \"\"\"\n        Gets all efforts on a particular segment sorted by start_date_local\n\n        Returns an array of segment effort summary representations sorted by\n        start_date_local ascending or by elapsed_time if an athlete_id is\n        provided.\n\n        If no filtering parameters is provided all efforts for the segment\n        will be returned.\n\n        Date range filtering is accomplished using an inclusive start and end time,\n        thus start_date_local and end_date_local must be sent together. For open\n        ended ranges pick dates significantly in the past or future. The\n        filtering is done over local time for the segment, so there is no need\n        for timezone conversion. For example, all efforts on Jan. 1st, 2014\n        for a segment in San Francisco, CA can be fetched using\n        2014-01-01T00:00:00Z and 2014-01-01T23:59:59Z.\n\n        http://strava.github.io/api/v3/segments/#all_efforts\n\n        :param segment_id: ID of the segment.\n        :type segment_id: param\n\n        :int athlete_id: (optional) ID of athlete.\n        :type athlete_id: int\n\n        :param start_date_local: (optional) efforts before this date will be excluded.\n                                            Either as ISO8601 or datetime object\n        :type start_date_local: datetime.datetime or str\n\n        :param end_date_local: (optional) efforts after this date will be excluded.\n                                           Either as ISO8601 or datetime object\n        :type end_date_local: datetime.datetime or str\n\n        :param limit: (optional), limit number of efforts.\n        :type limit: int\n\n        :return: An iterator of :class:`stravalib.model.SegmentEffort` efforts on a segment.\n        :rtype: :class:`BatchedResultsIterator`\n\n        \"\"\"\n        params = {\"segment_id\": segment_id}\n\n        if athlete_id is not None:\n            params['athlete_id'] = athlete_id\n\n        if start_date_local:\n            if isinstance(start_date_local, six.string_types):\n                start_date_local = arrow.get(start_date_local).naive\n            params[\"start_date_local\"] = start_date_local.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        if end_date_local:\n            if isinstance(end_date_local, six.string_types):\n                end_date_local = arrow.get(end_date_local).naive\n            params[\"end_date_local\"] = end_date_local.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n        if limit is not None:\n            params[\"limit\"] = limit\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/segments/{segment_id}/all_efforts',\n                                           **params)\n\n        return BatchedResultsIterator(entity=model.BaseEffort, bind_client=self,\n                                      result_fetcher=result_fetcher, limit=limit)", "response": "This method returns a list of all effort summary representations of a particular segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of up to 10 segments.", "response": "def explore_segments(self, bounds, activity_type=None, min_cat=None, max_cat=None):\n        \"\"\"\n        Returns an array of up to 10 segments.\n\n        http://strava.github.io/api/v3/segments/#explore\n\n        :param bounds: list of bounding box corners lat/lon [sw.lat, sw.lng, ne.lat, ne.lng] (south,west,north,east)\n        :type bounds: list of 4 floats or list of 2 (lat,lon) tuples\n\n        :param activity_type: (optional, default is riding)  'running' or 'riding'\n        :type activity_type: str\n\n        :param min_cat: (optional) Minimum climb category filter\n        :type min_cat: int\n\n        :param max_cat: (optional) Maximum climb category filter\n        :type max_cat: int\n\n        :return: An list of :class:`stravalib.model.Segment`.\n        :rtype: :py:class:`list`\n\n        \"\"\"\n        if len(bounds) == 2:\n            bounds = (bounds[0][0], bounds[0][1], bounds[1][0], bounds[1][1])\n        elif len(bounds) != 4:\n            raise ValueError(\"Invalid bounds specified: {0!r}. Must be list of 4 float values or list of 2 (lat,lon) tuples.\")\n\n        params = {'bounds': ','.join(str(b) for b in bounds)}\n\n        valid_activity_types = ('riding', 'running')\n        if activity_type is not None:\n            if activity_type not in ('riding', 'running'):\n                raise ValueError('Invalid activity type: {0}.  Possible values: {1!r}'.format(activity_type, valid_activity_types))\n            params['activity_type'] = activity_type\n\n        if min_cat is not None:\n            params['min_cat'] = min_cat\n        if max_cat is not None:\n            params['max_cat'] = max_cat\n\n        raw = self.protocol.get('/segments/explore', **params)\n        return [model.SegmentExplorerResult.deserialize(v, bind_client=self)\n                for v in raw['segments']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_activity_streams(self, activity_id, types=None,\n                             resolution=None, series_type=None):\n        \"\"\"\n        Returns an streams for an activity.\n\n        http://strava.github.io/api/v3/streams/#activity\n\n        Streams represent the raw data of the uploaded file. External\n        applications may only access this information for activities owned\n        by the authenticated athlete.\n\n        Streams are available in 11 different types. If the stream is not\n        available for a particular activity it will be left out of the request\n        results.\n\n        Streams types are: time, latlng, distance, altitude, velocity_smooth,\n                           heartrate, cadence, watts, temp, moving, grade_smooth\n\n        http://strava.github.io/api/v3/streams/#activity\n\n        :param activity_id: The ID of activity.\n        :type activity_id: int\n\n        :param types: (optional) A list of the the types of streams to fetch.\n        :type types: list\n\n        :param resolution: (optional, default is 'all') indicates desired number\n                            of data points. 'low' (100), 'medium' (1000),\n                            'high' (10000) or 'all'.\n        :type resolution: str\n\n        :param series_type: (optional, default is 'distance'.  Relevant only if\n                             using resolution either 'time' or 'distance'.\n                             Used to index the streams if the stream is being\n                             reduced.\n        :type series_type: str\n\n        :return: An dictionary of :class:`stravalib.model.Stream` from the activity or None if there are no streams.\n        :rtype: :py:class:`dict`\n        \"\"\"\n\n        # stream are comma seperated list\n        if types is not None:\n            types = \",\".join(types)\n\n        params = {}\n        if resolution is not None:\n            params[\"resolution\"] = resolution\n\n        if series_type is not None:\n            params[\"series_type\"] = series_type\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/activities/{id}/streams/{types}'.format(id=activity_id, types=types),\n                                           **params)\n\n        streams = BatchedResultsIterator(entity=model.Stream,\n                                         bind_client=self,\n                                         result_fetcher=result_fetcher)\n\n        # Pack streams into dictionary\n        try:\n            return {i.type: i for i in streams}\n        except exc.ObjectNotFound:\n            return None", "response": "Returns a dictionary of streams for an activity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_effort_streams(self, effort_id, types=None, resolution=None,\n                           series_type=None):\n        \"\"\"\n        Returns an streams for an effort.\n\n        http://strava.github.io/api/v3/streams/#effort\n\n        Streams represent the raw data of the uploaded file. External\n        applications may only access this information for activities owned\n        by the authenticated athlete.\n\n        Streams are available in 11 different types. If the stream is not\n        available for a particular activity it will be left out of the request\n        results.\n\n        Streams types are: time, latlng, distance, altitude, velocity_smooth,\n                           heartrate, cadence, watts, temp, moving, grade_smooth\n\n        http://strava.github.io/api/v3/streams/#effort\n\n        :param effort_id: The ID of effort.\n        :type effort_id: int\n\n        :param types: (optional) A list of the the types of streams to fetch.\n        :type types: list\n\n        :param resolution: (optional, default is 'all') indicates desired number\n                            of data points. 'low' (100), 'medium' (1000),\n                            'high' (10000) or 'all'.\n        :type resolution: str\n\n        :param series_type: (optional, default is 'distance'.  Relevant only if\n                             using resolution either 'time' or 'distance'.\n                             Used to index the streams if the stream is being\n                             reduced.\n        :type series_type: str\n\n        :return: An dictionary of :class:`stravalib.model.Stream` from the effort.\n        :rtype: :py:class:`dict`\n\n        \"\"\"\n\n        # stream are comma seperated list\n        if types is not None:\n            types = \",\".join(types)\n\n        params = {}\n        if resolution is not None:\n            params[\"resolution\"] = resolution\n\n        if series_type is not None:\n            params[\"series_type\"] = series_type\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/segment_efforts/{id}/streams/{types}'.format(id=effort_id, types=types),\n                                           **params)\n\n        streams = BatchedResultsIterator(entity=model.Stream,\n                                         bind_client=self,\n                                         result_fetcher=result_fetcher)\n\n        # Pack streams into dictionary\n        return {i.type: i for i in streams}", "response": "Returns a dictionary of the streams for an effort."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a running race for a given identifier.", "response": "def get_running_race(self, race_id):\n        \"\"\"\n        Gets a running race for a given identifier.t\n\n        http://strava.github.io/api/v3/running_races/#list\n\n        :param race_id: id for the race\n\n        :rtype: :class:`stravalib.model.RunningRace`\n        \"\"\"        \n        raw = self.protocol.get('/running_races/{id}', id=race_id)\n        return model.RunningRace.deserialize(raw, bind_client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_running_races(self, year=None):\n        if year is None:\n            year = datetime.datetime.now().year\n    \n        params = {\"year\": year}\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/running_races',\n                                           **params)\n\n        return BatchedResultsIterator(entity=model.RunningRace, bind_client=self,\n                                      result_fetcher=result_fetcher)", "response": "Gets a running races for a given year."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_routes(self, athlete_id=None, limit=None):\n        if athlete_id is None:\n            athlete_id = self.get_athlete().id\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/athletes/{id}/routes'.format(id=athlete_id))\n\n        return BatchedResultsIterator(entity=model.Route,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher,\n                                      limit=limit)", "response": "Gets the routes list for an authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets specified route. Will be detail-level if owned by authenticated user; otherwise summary-level. https://strava.github.io/api/v3/routes/#retreive :param route_id: The ID of route to fetch. :type route_id: int :rtype: :class:`stravalib.model.Route`", "response": "def get_route(self, route_id):\n        \"\"\"\n        Gets specified route.\n\n        Will be detail-level if owned by authenticated user; otherwise summary-level.\n\n        https://strava.github.io/api/v3/routes/#retreive\n\n        :param route_id: The ID of route to fetch.\n        :type route_id: int\n\n        :rtype: :class:`stravalib.model.Route`\n        \"\"\"\n        raw = self.protocol.get('/routes/{id}', id=route_id)\n        return model.Route.deserialize(raw, bind_client=self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of streams for a route.", "response": "def get_route_streams(self, route_id):\n        \"\"\"\n        Returns streams for a route.\n\n        http://strava.github.io/api/v3/streams/#routes\n\n        Streams represent the raw data of the saved route. External\n        applications may access this information for all public routes and for\n        the private routes of the authenticated athlete.\n\n        The 3 available route stream types `distance`, `altitude` and `latlng`\n        are always returned.\n\n        http://strava.github.io/api/v3/streams/#routes\n\n        :param activity_id: The ID of activity.\n        :type activity_id: int\n\n        :return: A dictionary of :class:`stravalib.model.Stream`from the route.\n        :rtype: :py:class:`dict`\n\n        \"\"\"\n\n        result_fetcher = functools.partial(self.protocol.get,\n                                           '/routes/{id}/streams/'.format(id=route_id))\n\n        streams = BatchedResultsIterator(entity=model.Stream,\n                                         bind_client=self,\n                                         result_fetcher=result_fetcher)\n\n        # Pack streams into dictionary\n        return {i.type: i for i in streams}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_subscription(self, client_id, client_secret, callback_url,\n                            object_type=model.Subscription.OBJECT_TYPE_ACTIVITY,\n                            aspect_type=model.Subscription.ASPECT_TYPE_CREATE,\n                            verify_token=model.Subscription.VERIFY_TOKEN_DEFAULT):\n        \"\"\"\n        Creates a webhook event subscription.\n\n        http://strava.github.io/api/partner/v3/events/#create-a-subscription\n\n        :param client_id: application's ID, obtained during registration\n        :type client_id: int\n\n        :param client_secret: application's secret, obtained during registration\n        :type client_secret: str\n\n        :param callback_url: callback URL where Strava will first send a GET request to validate, then subsequently send POST requests with updates\n        :type callback_url: str\n\n        :param object_type: object_type (currently only `activity` is supported)\n        :type object_type: str\n\n        :param aspect_type: object_type (currently only `create` is supported)\n        :type aspect_type: str\n\n        :param verify_token: a token you can use to verify Strava's GET callback request\n        :type verify_token: str\n\n        :return: An instance of :class:`stravalib.model.Subscription`.\n        :rtype: :class:`stravalib.model.Subscription`\n\n        Notes:\n\n        `object_type` and `aspect_type` are given defaults because there is currently only one valid value for each.\n\n        `verify_token` is set to a default in the event that the author doesn't want to specify one.\n\n        The appliction must have permission to make use of the webhook API. Access can be requested by contacting developers -at- strava.com.\n        \"\"\"\n        params = dict(client_id=client_id, client_secret=client_secret,\n                      object_type=object_type, aspect_type=aspect_type,\n                      callback_url=callback_url, verify_token=verify_token)\n        raw = self.protocol.post('/push_subscriptions', use_webhook_server=True,\n                                 **params)\n        return model.Subscription.deserialize(raw, bind_client=self)", "response": "Creates a new subscription for the given application."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates callback request and return valid response with challenge.", "response": "def handle_subscription_callback(self, raw,\n                                     verify_token=model.Subscription.VERIFY_TOKEN_DEFAULT):\n        \"\"\"\n        Validate callback request and return valid response with challenge.\n\n        :return: The JSON response expected by Strava to the challenge request.\n        :rtype: Dict[str, str]\n        \"\"\"\n        callback = model.SubscriptionCallback.deserialize(raw)\n        callback.validate(verify_token)\n        response_raw = {'hub.challenge': callback.hub_challenge}\n        return response_raw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_subscriptions(self, client_id, client_secret):\n        result_fetcher = functools.partial(self.protocol.get, '/push_subscriptions', client_id=client_id,\n                                           client_secret=client_secret, use_webhook_server=True)\n\n        return BatchedResultsIterator(entity=model.Subscription,\n                                      bind_client=self,\n                                      result_fetcher=result_fetcher)", "response": "List current webhook event subscriptions in place for the current application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_subscription(self, subscription_id, client_id, client_secret):\n        self.protocol.delete('/push_subscriptions/{id}', id=subscription_id,\n                             client_id=client_id, client_secret=client_secret, use_webhook_server=True)", "response": "Unsubscribe from webhook events for an existing subscription."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill_buffer(self):\n        # If we cannot fetch anymore from the server then we're done here.\n        if self._all_results_fetched:\n            self._eof()\n\n        raw_results = self.result_fetcher(page=self._page, per_page=self.per_page)\n\n        entities = []\n        for raw in raw_results:\n            entities.append(self.entity.deserialize(raw, bind_client=self.bind_client))\n\n        self._buffer = collections.deque(entities)\n\n        self.log.debug(\"Requested page {0} (got: {1} items)\".format(self._page,\n                                                                    len(self._buffer)))\n        if len(self._buffer) < self.per_page:\n            self._all_results_fetched = True\n\n        self._page += 1", "response": "Fills the internal buffer with the contents of the API response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_from_response(self, response, raise_exc=True):\n        self.upload_id = response.get('id')\n        self.external_id = response.get('external_id')\n        self.activity_id = response.get('activity_id')\n        self.status = response.get('status') or response.get('message')\n\n        if response.get('error'):\n            self.error = response.get('error')\n        elif response.get('errors'):\n            # This appears to be an undocumented API; ths is a bit of a hack for now.\n            self.error = str(response.get('errors'))\n        else:\n            self.error = None\n\n        if raise_exc:\n            self.raise_for_error()", "response": "Updates internal state of the object from the response object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npolls the internal state of the object.", "response": "def poll(self):\n        \"\"\"\n        Update internal state from polling strava.com.\n\n        :raise stravalib.exc.ActivityUploadFailed: If the poll returns an error.\n        \"\"\"\n        response = self.client.protocol.get('/uploads/{upload_id}',\n                                            upload_id=self.upload_id,\n                                            check_for_errors=False)\n\n        self.update_from_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait(self, timeout=None, poll_interval=1.0):\n        start = time.time()\n        while self.activity_id is None:\n            self.poll()\n            time.sleep(poll_interval)\n            if timeout and (time.time() - start) > timeout:\n                raise exc.TimeoutExceeded()\n        # If we got this far, we must have an activity!\n        return self.client.get_activity(self.activity_id)", "response": "Wait for the upload to complete or raise TimeoutExceededException."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a namedtuple with values for short - and long usage and limit rates found in provided HTTP response headers.", "response": "def get_rates_from_response_headers(headers):\n    \"\"\"\n    Returns a namedtuple with values for short - and long usage and limit rates found in provided HTTP response headers\n    :param headers: HTTP response headers\n    :type headers: dict\n    :return: namedtuple with request rates or None if no rate-limit headers present in response.\n    :rtype: Optional[RequestRate]\n    \"\"\"\n    try:\n        usage_rates = [int(v) for v in headers['X-RateLimit-Usage'].split(',')]\n        limit_rates = [int(v) for v in headers['X-RateLimit-Limit'].split(',')]\n\n        return RequestRate(short_usage=usage_rates[0], long_usage=usage_rates[1],\n                           short_limit=limit_rates[0], long_limit=limit_rates[1])\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_seconds_until_next_quarter(now=None):\n    if now is None:\n        now = arrow.utcnow()\n    return 899 - (now - now.replace(minute=(now.minute // 15) * 15, second=0, microsecond=0)).seconds", "response": "Returns the number of seconds until the next quarter of an hour."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_seconds_until_next_day(now=None):\n    if now is None:\n        now = arrow.utcnow()\n    return (now.ceil('day') - now).seconds", "response": "Returns the number of seconds until the next day."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new object based on serialized (dict) struct.", "response": "def deserialize(cls, v, bind_client=None):\n        \"\"\"\n        Creates a new object based on serialized (dict) struct.\n        \"\"\"\n        if v is None:\n            return None\n        o = cls(bind_client=bind_client)\n        o.from_dict(v)\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef members(self):\n        if self._members is None:\n            self.assert_bind_client()\n            self._members = self.bind_client.get_club_members(self.id)\n        return self._members", "response": "An iterator of the members of this club."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deserialize(cls, v):\n        if v is None:\n            return None\n        if cls == Gear and v.get('resource_state') == 3:\n            if 'frame_type' in v:\n                o = Bike()\n            else:\n                o = Shoe()\n        else:\n            o = cls()\n        o.from_dict(v)\n        return o", "response": "Deserialize a dictionary into a new object based on serialized ( dict ) struct."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the athlete is the authenticated athlete.", "response": "def is_authenticated_athlete(self):\n        \"\"\"\n        :return: Boolean as to whether the athlete is the authenticated athlete.\n        \"\"\"\n        if self._is_authenticated is None:\n            if self.resource_state == DETAILED:\n                # If the athlete is in detailed state it must be the authenticated athlete\n                self._is_authenticated = True\n            else:\n                # We need to check this athlete's id matches the authenticated athlete's id\n                self.assert_bind_client()\n                authenticated_athlete = self.bind_client.get_athlete()\n                self._is_authenticated = authenticated_athlete.id == self.id\n        return self._is_authenticated"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator of the athlete s friends.", "response": "def friends(self):\n        \"\"\"\n        :return: Iterator of :class:`stravalib.model.Athlete` friend objects for this athlete.\n        \"\"\"\n        if self._friends is None:\n            self.assert_bind_client()\n            if self.friend_count > 0:\n                self._friends = self.bind_client.get_athlete_friends(self.id)\n            else:\n                # Shortcut if we know there aren't any\n                self._friends = []\n        return self._friends"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef followers(self):\n        if self._followers is None:\n            self.assert_bind_client()\n            if self.follower_count > 0:\n                self._followers = self.bind_client.get_athlete_followers(self.id)\n            else:\n                # Shortcut if we know there aren't any\n                self._followers = []\n        return self._followers", "response": "Return an iterator of the athlete s followers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stats(self):\n        if not self.is_authenticated_athlete():\n            raise exc.NotAuthenticatedAthlete(\"Statistics are only available for the authenticated athlete\")\n        if self._stats is None:\n            self.assert_bind_client()\n            self._stats = self.bind_client.get_athlete_stats(self.id)\n        return self._stats", "response": "Gets the statistics for the athlete."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the segment associated with this instance.", "response": "def segment(self):\n        \"\"\" Associated (full) :class:`stravalib.model.Segment` object. \"\"\"\n        if self._segment is None:\n            self.assert_bind_client()\n            if self.id is not None:\n                self._segment = self.bind_client.get_segment(self.id)\n        return self._segment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef leaderboard(self):\n        if self._leaderboard is None:\n            self.assert_bind_client()\n            if self.id is not None:\n                self._leaderboard = self.bind_client.get_segment_leaderboard(self.id)\n        return self._leaderboard", "response": "Returns the leaderboard object for this segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef comments(self):\n        if self._comments is None:\n            self.assert_bind_client()\n            if self.comment_count > 0:\n                self._comments = self.bind_client.get_activity_comments(self.id)\n            else:\n                # Shortcut if we know there aren't any\n                self._comments = []\n        return self._comments", "response": "Return an iterator of Comment objects for this activity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of ActivityZone objects for this activity.", "response": "def zones(self):\n        \"\"\"\n        :class:`list` of :class:`stravalib.model.ActivityZone` objects for this activity.\n        \"\"\"\n        if self._zones is None:\n            self.assert_bind_client()\n            self._zones = self.bind_client.get_activity_zones(self.id)\n        return self._zones"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kudos(self):\n        if self._kudos is None:\n            self.assert_bind_client()\n            self._kudos = self.bind_client.get_activity_kudos(self.id)\n        return self._kudos", "response": "Return a list of ActivityKudos objects for this activity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of photos for this activity.", "response": "def full_photos(self):\n        \"\"\"\n        Gets a list of photos using default options.\n\n        :class:`list` of :class:`stravalib.model.ActivityPhoto` objects for this activity.\n        \"\"\"\n        if self._photos is None:\n            if self.total_photo_count > 0:\n                self.assert_bind_client()\n                self._photos = self.bind_client.get_activity_photos(self.id, only_instagram=False)\n            else:\n                self._photos = []\n        return self._photos"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef related(self):\n        if self._related is None:\n            if self.athlete_count - 1 > 0:\n                self.assert_bind_client()\n                self._related = self.bind_client.get_related_activities(self.id)\n            else:\n                self._related = []\n        return self._related", "response": "Iterator of Activty objects for this activity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deserialize(cls, v, bind_client=None):\n        if v is None:\n            return None\n        az_classes = {'heartrate': HeartrateActivityZone,\n                      'power': PowerActivityZone,\n                      'pace': PaceActivityZone}\n        try:\n            clazz = az_classes[v['type']]\n        except KeyError:\n            raise ValueError(\"Unsupported activity zone type: {0}\".format(v['type']))\n        else:\n            o = clazz(bind_client=bind_client)\n            o.from_dict(v)\n            return o", "response": "Deserialize an activity zone record into an object based on serialized ( dict ) struct."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set(self, key, value, timeout=None):\n        key = self.make_key(key)\n        if timeout is None:\n            timeout = self.default_timeout\n\n        if self.debug:\n            return True\n\n        pickled_value = pickle.dumps(value)\n        self.metrics['writes'] += 1\n        if timeout:\n            return self.database.setex(key, int(timeout), pickled_value)\n        else:\n            return self.database.set(key, pickled_value)", "response": "Cache the given value in the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the given key from the cache.", "response": "def delete(self, key):\n        \"\"\"Remove the given key from the cache.\"\"\"\n        if not self.debug:\n            self.database.delete(self.make_key(key))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving all cached objects from the database.", "response": "def flush(self):\n        \"\"\"Remove all cached objects from the database.\"\"\"\n        keys = list(self.keys())\n        if keys:\n            return self.database.delete(*keys)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cached(self, key_fn=_key_fn, timeout=None, metrics=False):\n        def decorator(fn):\n            def make_key(args, kwargs):\n                return '%s:%s' % (fn.__name__, key_fn(args, kwargs))\n\n            def bust(*args, **kwargs):\n                return self.delete(make_key(args, kwargs))\n\n            _metrics = {\n                'hits': 0,\n                'misses': 0,\n                'avg_hit_time': 0,\n                'avg_miss_time': 0}\n\n            @wraps(fn)\n            def inner(*args, **kwargs):\n                start = time.time()\n                is_cache_hit = True\n                key = make_key(args, kwargs)\n                res = self.get(key)\n                if res is None:\n                    res = fn(*args, **kwargs)\n                    self.set(key, res, timeout)\n                    is_cache_hit = False\n\n                if metrics:\n                    dur = time.time() - start\n                    if is_cache_hit:\n                        _metrics['hits'] += 1\n                        _metrics['avg_hit_time'] += (dur / _metrics['hits'])\n                    else:\n                        _metrics['misses'] += 1\n                        _metrics['avg_miss_time'] += (dur / _metrics['misses'])\n\n                return res\n\n            inner.bust = bust\n            inner.make_key = make_key\n            if metrics:\n                inner.metrics = _metrics\n            return inner\n        return decorator", "response": "A decorator that caches the result of a function with the given args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the double metaphone codes for given string", "response": "def dm(st) :\r\n    \"\"\"dm(string) -> (string, string or None)\r\n    returns the double metaphone codes for given string - always a tuple\r\n    there are no checks done on the input string, but it should be a single\r\n    word or name.\"\"\"\r\n    st = decode(st)\r\n    # st is short for string. I usually prefer descriptive over short,\r\n    # but this var is used a lot!\r\n    st = st.upper()\r\n    is_slavo_germanic = (st.find('W') > -1 or st.find('K') > -1 or\r\n                        st.find('CZ') > -1 or st.find('WITZ') > -1)\r\n    length = len(st)\r\n    first = 2\r\n    # so we can index beyond the begining and end of the input string\r\n    st = ('-') * first + st + (' ' * 5)\r\n    last = first + length -1\r\n    pos = first # pos is short for position\r\n    pri = sec = '' # primary and secondary metaphone codes\r\n    #skip these silent letters when at start of word\r\n    if st[first:first+2] in GNKN:\r\n        pos += 1\r\n    # Initial 'X' is pronounced 'Z' e.g. 'Xavier'\r\n    if st[first] == 'X' :\r\n        pri = sec = 'S' #'Z' maps to 'S'\r\n        pos += 1\r\n    # main loop through chars in st\r\n    while pos <= last :\r\n        #print str(pos) + '\\t' + st[pos]\r\n        ch = st[pos] # ch is short for character\r\n        # nxt (short for next characters in metaphone code) is set to  a\r\n        # tuple of the next characters in\r\n        # the primary and secondary codes and how many characters to move\r\n        # forward in the string.\r\n        # the secondary code letter is given only when it is different than\r\n        # the primary.\r\n        # This is just a trick to make the code easier to write and read.\r\n        #\r\n        # default action is to add nothing and move to next char\r\n        nxt = (None, 1)\r\n        if ch in VOWELS :\r\n            nxt = (None, 1)\r\n            if pos == first : # all init VOWELS now map to 'A'\r\n                nxt = ('A', 1)\r\n        elif ch == 'B' :\r\n            #\"-mb\", e.g\", \"dumb\", already skipped over... see 'M' below\r\n            if st[pos+1] == 'B' :\r\n                nxt = ('P', 2)\r\n            else :\r\n                nxt = ('P', 1)\r\n        elif ch == 'C' :\r\n            # various germanic\r\n            if (pos > (first + 1) and st[pos-2] not in VOWELS and\r\n                    st[pos-1:pos+2] == 'ACH' and\r\n                    (st[pos+2] not in ['I', 'E'] or\r\n                     st[pos-2:pos+4] in ['BACHER', 'MACHER'])):\r\n                nxt = ('K', 2)\r\n            # special case 'CAESAR'\r\n            elif pos == first and st[first:first+6] == 'CAESAR' :\r\n                nxt = ('S', 2)\r\n            elif st[pos:pos+4] == 'CHIA' : #italian 'chianti'\r\n                nxt = ('K', 2)\r\n            elif st[pos:pos+2] == 'CH' :\r\n                # find 'michael'\r\n                if pos > first and st[pos:pos+4] == 'CHAE' :\r\n                    nxt = ('K', 'X', 2)\r\n                elif pos == first and (st[pos+1:pos+6] in ['HARAC', 'HARIS'] or \\\r\n                   st[pos+1:pos+4] in [\"HOR\", \"HYM\", \"HIA\", \"HEM\"]) and st[first:first+5] != 'CHORE' :\r\n                    nxt = ('K', 2)\r\n                #germanic, greek, or otherwise 'ch' for 'kh' sound\r\n                elif st[first:first+4] in ['VAN ', 'VON '] or st[first:first+3] == 'SCH' \\\r\n                   or st[pos-2:pos+4] in [\"ORCHES\", \"ARCHIT\", \"ORCHID\"] \\\r\n                   or st[pos+2] in ['T', 'S'] \\\r\n                   or ((st[pos-1] in [\"A\", \"O\", \"U\", \"E\"] or pos == first) \\\r\n                   and st[pos+2] in [\"L\", \"R\", \"N\", \"M\", \"B\", \"H\", \"F\", \"V\", \"W\", \" \"]) :\r\n                    nxt = ('K', 1)\r\n                else :\r\n                    if pos > first :\r\n                        if st[first:first+2] == 'MC' :\r\n                            nxt = ('K', 2)\r\n                        else :\r\n                            nxt = ('X', 'K', 2)\r\n                    else :\r\n                        nxt = ('X', 2)\r\n            #e.g, 'czerny'\r\n            elif st[pos:pos+2] == 'CZ' and st[pos-2:pos+2] != 'WICZ' :\r\n                nxt = ('S', 'X', 2)\r\n            #e.g., 'focaccia'\r\n            elif st[pos+1:pos+4] == 'CIA' :\r\n                nxt = ('X', 3)\r\n            #double 'C', but not if e.g. 'McClellan'\r\n            elif st[pos:pos+2] == 'CC' and not (pos == (first +1) and st[first] == 'M') :\r\n                #'bellocchio' but not 'bacchus'\r\n                if st[pos+2] in [\"I\", \"E\", \"H\"] and st[pos+2:pos+4] != 'HU' :\r\n                    #'accident', 'accede' 'succeed'\r\n                    if (pos == (first +1) and st[first] == 'A') or \\\r\n                       st[pos-1:pos+4] in ['UCCEE', 'UCCES'] :\r\n                        nxt = ('KS', 3)\r\n                    #'bacci', 'bertucci', other italian\r\n                    else:\r\n                        nxt = ('X', 3)\r\n                else :\r\n                    nxt = ('K', 2)\r\n            elif st[pos:pos+2] in [\"CK\", \"CG\", \"CQ\"] :\r\n                nxt = ('K', 'K', 2)\r\n            elif st[pos:pos+2] in [\"CI\", \"CE\", \"CY\"] :\r\n                #italian vs. english\r\n                if st[pos:pos+3] in [\"CIO\", \"CIE\", \"CIA\"] :\r\n                    nxt = ('S', 'X', 2)\r\n                else :\r\n                    nxt = ('S', 2)\r\n            else :\r\n                #name sent in 'mac caffrey', 'mac gregor\r\n                if st[pos+1:pos+3] in [\" C\", \" Q\", \" G\"] :\r\n                    nxt = ('K', 3)\r\n                else :\r\n                    if st[pos+1] in [\"C\", \"K\", \"Q\"] and st[pos+1:pos+3] not in [\"CE\", \"CI\"] :\r\n                        nxt = ('K', 2)\r\n                    else : # default for 'C'\r\n                        nxt = ('K', 1)\r\n        #elif ch == CCCC:\r\n        #\tnxt = ('S', 1)\r\n        elif ch == 'D' :\r\n            if st[pos:pos+2] == 'DG' :\r\n                if st[pos+2] in ['I', 'E', 'Y'] : #e.g. 'edge'\r\n                    nxt = ('J', 3)\r\n                else :\r\n                    nxt = ('TK', 2)\r\n            elif st[pos:pos+2] in ['DT', 'DD'] :\r\n                nxt = ('T', 2)\r\n            else :\r\n                nxt = ('T', 1)\r\n        elif ch == 'F' :\r\n            if st[pos+1] == 'F' :\r\n                nxt = ('F', 2)\r\n            else :\r\n                nxt = ('F', 1)\r\n        elif ch == 'G' :\r\n            if st[pos+1] == 'H' :\r\n                if pos > first and st[pos-1] not in VOWELS :\r\n                    nxt = ('K', 2)\r\n                elif pos < (first + 3) :\r\n                    if pos == first : #'ghislane', ghiradelli\r\n                        if st[pos+2] == 'I' :\r\n                            nxt = ('J', 2)\r\n                        else :\r\n                            nxt = ('K', 2)\r\n                #Parker's rule (with some further refinements) - e.g., 'hugh'\r\n                elif (pos > (first + 1) and st[pos-2] in ['B', 'H', 'D'] ) \\\r\n                   or (pos > (first + 2) and st[pos-3] in ['B', 'H', 'D'] ) \\\r\n                   or (pos > (first + 3) and st[pos-4] in ['B', 'H'] ) :\r\n                    nxt = (None, 2)\r\n                else :\r\n                    # e.g., 'laugh', 'McLaughlin', 'cough', 'gough', 'rough', 'tough'\r\n                    if pos > (first + 2) and st[pos-1] == 'U' \\\r\n                       and st[pos-3] in [\"C\", \"G\", \"L\", \"R\", \"T\"] :\r\n                        nxt = ('F', 2)\r\n                    else :\r\n                        if pos > first and st[pos-1] != 'I' :\r\n                            nxt = ('K', 2)\r\n            elif st[pos+1] == 'N' :\r\n                if pos == (first +1) and st[first] in VOWELS and not is_slavo_germanic :\r\n                    nxt = ('KN', 'N', 2)\r\n                else :\r\n                    # not e.g. 'cagney'\r\n                    if st[pos+2:pos+4] != 'EY' and st[pos+1] != 'Y' and not is_slavo_germanic :\r\n                        nxt = ('N', 'KN', 2)\r\n                    else :\r\n                        nxt = ('KN', 2)\r\n            # 'tagliaro'\r\n            elif st[pos+1:pos+3] == 'LI' and not is_slavo_germanic :\r\n                nxt = ('KL', 'L', 2)\r\n            # -ges-,-gep-,-gel-, -gie- at beginning\r\n            elif pos == first and (st[pos+1] == 'Y' \\\r\n               or st[pos+1:pos+3] in [\"ES\", \"EP\", \"EB\", \"EL\", \"EY\", \"IB\", \"IL\", \"IN\", \"IE\", \"EI\", \"ER\"]) :\r\n                nxt = ('K', 'J', 2)\r\n            # -ger-,  -gy-\r\n            elif (st[pos+1:pos+2] == 'ER' or st[pos+1] == 'Y') \\\r\n               and st[first:first+6] not in [\"DANGER\", \"RANGER\", \"MANGER\"] \\\r\n               and st[pos-1] not in ['E', 'I'] and st[pos-1:pos+2] not in ['RGY', 'OGY'] :\r\n                nxt = ('K', 'J', 2)\r\n            # italian e.g, 'biaggi'\r\n            elif st[pos+1] in ['E', 'I', 'Y'] or st[pos-1:pos+3] in [\"AGGI\", \"OGGI\"] :\r\n                # obvious germanic\r\n                if st[first:first+4] in ['VON ', 'VAN '] or st[first:first+3] == 'SCH' \\\r\n                   or st[pos+1:pos+3] == 'ET' :\r\n                    nxt = ('K', 2)\r\n                else :\r\n                    # always soft if french ending\r\n                    if st[pos+1:pos+5] == 'IER ' :\r\n                        nxt = ('J', 2)\r\n                    else :\r\n                        nxt = ('J', 'K', 2)\r\n            elif st[pos+1] == 'G' :\r\n                nxt = ('K', 2)\r\n            else :\r\n                nxt = ('K', 1)\r\n        elif ch == 'H' :\r\n            # only keep if first & before vowel or btw. 2 VOWELS\r\n            if (pos == first or st[pos-1] in VOWELS) and st[pos+1] in VOWELS :\r\n                nxt = ('H', 2)\r\n            else : # (also takes care of 'HH')\r\n                nxt = (None, 1)\r\n        elif ch == 'J' :\r\n            # obvious spanish, 'jose', 'san jacinto'\r\n            if st[pos:pos+4] == 'JOSE' or st[first:first+4] == 'SAN ' :\r\n                if (pos == first and st[pos+4] == ' ') or st[first:first+4] == 'SAN ' :\r\n                    nxt = ('H',)\r\n                else :\r\n                    nxt = ('J', 'H')\r\n            elif pos == first and st[pos:pos+4] != 'JOSE' :\r\n                nxt = ('J', 'A') # Yankelovich/Jankelowicz\r\n            else :\r\n                # spanish pron. of e.g. 'bajador'\r\n                if st[pos-1] in VOWELS and not is_slavo_germanic \\\r\n                   and st[pos+1] in ['A', 'O'] :\r\n                    nxt = ('J', 'H')\r\n                else :\r\n                    if pos == last :\r\n                        nxt = ('J', ' ')\r\n                    else :\r\n                        if st[pos+1] not in [\"L\", \"T\", \"K\", \"S\", \"N\", \"M\", \"B\", \"Z\"] \\\r\n                           and st[pos-1] not in [\"S\", \"K\", \"L\"] :\r\n                            nxt = ('J',)\r\n                        else :\r\n                            nxt = (None, )\r\n            if st[pos+1] == 'J' :\r\n                nxt = nxt + (2,)\r\n            else :\r\n                nxt = nxt + (1,)\r\n        elif ch == 'K' :\r\n            if st[pos+1] == 'K' :\r\n                nxt = ('K', 2)\r\n            else :\r\n                nxt = ('K', 1)\r\n        elif ch == 'L' :\r\n            if st[pos+1] == 'L' :\r\n                # spanish e.g. 'cabrillo', 'gallegos'\r\n                if (pos == (last - 2) and st[pos-1:pos+3] in [\"ILLO\", \"ILLA\", \"ALLE\"]) \\\r\n                   or ((st[last-1:last+1] in [\"AS\", \"OS\"] or st[last] in [\"A\", \"O\"]) \\\r\n                   and st[pos-1:pos+3] == 'ALLE') :\r\n                    nxt = ('L', '', 2)\r\n                else :\r\n                    nxt = ('L', 2)\r\n            else :\r\n                nxt = ('L', 1)\r\n        elif ch == 'M' :\r\n            if st[pos+1:pos+4] == 'UMB' \\\r\n               and (pos + 1 == last or st[pos+2:pos+4] == 'ER') \\\r\n               or st[pos+1] == 'M' :\r\n                nxt = ('M', 2)\r\n            else :\r\n                nxt = ('M', 1)\r\n        elif ch == 'N' :\r\n            if st[pos+1] == 'N' :\r\n                nxt = ('N', 2)\r\n            else :\r\n                nxt = ('N', 1)\r\n        elif ch == NNNN:\r\n            nxt = ('N', 1)\r\n        elif ch == 'P' :\r\n            if st[pos+1] == 'H' :\r\n                nxt = ('F', 2)\r\n            elif st[pos+1] in ['P', 'B'] : # also account for \"campbell\", \"raspberry\"\r\n                nxt = ('P', 2)\r\n            else :\r\n                nxt = ('P', 1)\r\n        elif ch == 'Q' :\r\n            if st[pos+1] == 'Q' :\r\n                nxt = ('K', 2)\r\n            else :\r\n                nxt = ('K', 1)\r\n        elif ch == 'R' :\r\n            # french e.g. 'rogier', but exclude 'hochmeier'\r\n            if pos == last and not is_slavo_germanic \\\r\n               and st[pos-2:pos] == 'IE' and st[pos-4:pos-2] not in ['ME', 'MA'] :\r\n                nxt = ('', 'R')\r\n            else :\r\n                nxt = ('R',)\r\n            if st[pos+1] == 'R' :\r\n                nxt = nxt + (2,)\r\n            else :\r\n                nxt = nxt + (1,)\r\n        elif ch == 'S' :\r\n            # special cases 'island', 'isle', 'carlisle', 'carlysle'\r\n            if st[pos-1:pos+2] in ['ISL', 'YSL'] :\r\n                nxt = (None, 1)\r\n            # special case 'sugar-'\r\n            elif pos == first and st[first:first+5] == 'SUGAR' :\r\n                nxt =('X', 'S', 1)\r\n            elif st[pos:pos+2] == 'SH' :\r\n                # germanic\r\n                if st[pos+1:pos+5] in [\"HEIM\", \"HOEK\", \"HOLM\", \"HOLZ\"] :\r\n                    nxt = ('S', 2)\r\n                else :\r\n                    nxt = ('X', 2)\r\n            # italian & armenian\r\n            elif st[pos:pos+3] in [\"SIO\", \"SIA\"] or st[pos:pos+4] == 'SIAN' :\r\n                if not is_slavo_germanic :\r\n                    nxt = ('S', 'X', 3)\r\n                else :\r\n                    nxt = ('S', 3)\r\n            # german & anglicisations, e.g. 'smith' match 'schmidt', 'snider'\r\n            # match 'schneider'\r\n            # also, -sz- in slavic language altho in hungarian it is\r\n            # pronounced 's'\r\n            elif (pos == first and st[pos+1] in [\"M\", \"N\", \"L\", \"W\"]) or st[pos+1] == 'Z' :\r\n                nxt = ('S', 'X')\r\n                if st[pos+1] == 'Z' :\r\n                    nxt = nxt + (2,)\r\n                else :\r\n                    nxt = nxt + (1,)\r\n            elif st[pos:pos+2] == 'SC' :\r\n                # Schlesinger's rule\r\n                if st[pos+2] == 'H' :\r\n                    # dutch origin, e.g. 'school', 'schooner'\r\n                    if st[pos+3:pos+5] in [\"OO\", \"ER\", \"EN\", \"UY\", \"ED\", \"EM\"] :\r\n                        # 'schermerhorn', 'schenker'\r\n                        if st[pos+3:pos+5] in ['ER', 'EN'] :\r\n                            nxt = ('X', 'SK', 3)\r\n                        else :\r\n                            nxt = ('SK', 3)\r\n                    else :\r\n                        if pos == first and st[first+3] not in VOWELS and st[first+3] != 'W' :\r\n                            nxt = ('X', 'S', 3)\r\n                        else :\r\n                            nxt = ('X', 3)\r\n                elif st[pos+2] in ['I', 'E', 'Y'] :\r\n                    nxt = ('S', 3)\r\n                else :\r\n                    nxt = ('SK', 3)\r\n            # french e.g. 'resnais', 'artois'\r\n            elif pos == last and st[pos-2:pos] in ['AI', 'OI'] :\r\n                nxt = ('', 'S', 1)\r\n            else :\r\n                nxt = ('S',)\r\n                if st[pos+1] in ['S', 'Z'] :\r\n                    nxt = nxt + (2,)\r\n                else :\r\n                    nxt = nxt + (1,)\r\n        elif ch == 'T' :\r\n            if st[pos:pos+4] == 'TION' :\r\n                nxt = ('X', 3)\r\n            elif st[pos:pos+3] in ['TIA', 'TCH'] :\r\n                nxt = ('X', 3)\r\n            elif st[pos:pos+2] == 'TH' or st[pos:pos+3] == 'TTH' :\r\n                # special case 'thomas', 'thames' or germanic\r\n                if st[pos+2:pos+4] in ['OM', 'AM'] or st[first:first+4] in ['VON ', 'VAN '] \\\r\n                   or st[first:first+3] == 'SCH' :\r\n                    nxt = ('T', 2)\r\n                else :\r\n                    nxt = ('0', 'T', 2)\r\n            elif st[pos+1] in ['T', 'D'] :\r\n                nxt = ('T', 2)\r\n            else :\r\n                nxt = ('T', 1)\r\n        elif ch == 'V' :\r\n            if st[pos+1] == 'V' :\r\n                nxt = ('F', 2)\r\n            else :\r\n                nxt = ('F', 1)\r\n        elif ch == 'W' :\r\n            # can also be in middle of word\r\n            if st[pos:pos+2] == 'WR' :\r\n                nxt = ('R', 2)\r\n            elif pos == first and (st[pos+1] in VOWELS or st[pos:pos+2] == 'WH') :\r\n                # Wasserman should match Vasserman\r\n                if st[pos+1] in VOWELS :\r\n                    nxt = ('A', 'F', 1)\r\n                else :\r\n                    nxt = ('A', 1)\r\n            # Arnow should match Arnoff\r\n            elif (pos == last and st[pos-1] in VOWELS) \\\r\n               or st[pos-1:pos+5] in [\"EWSKI\", \"EWSKY\", \"OWSKI\", \"OWSKY\"] \\\r\n               or st[first:first+3] == 'SCH' :\r\n                nxt = ('', 'F', 1)\r\n            # polish e.g. 'filipowicz'\r\n            elif st[pos:pos+4] in [\"WICZ\", \"WITZ\"] :\r\n                nxt = ('TS', 'FX', 4)\r\n            else : # default is to skip it\r\n                nxt = (None, 1)\r\n        elif ch == 'X' :\r\n            # french e.g. breaux\r\n            nxt = (None,)\r\n            if not(pos == last and (st[pos-3:pos] in [\"IAU\", \"EAU\"] \\\r\n               or st[pos-2:pos] in ['AU', 'OU'])):\r\n                nxt = ('KS',)\r\n            if st[pos+1] in ['C', 'X'] :\r\n                nxt = nxt + (2,)\r\n            else :\r\n                nxt = nxt + (1,)\r\n        elif ch == 'Z' :\r\n            # chinese pinyin e.g. 'zhao'\r\n            if st[pos+1] == 'H' :\r\n                nxt = ('J',)\r\n            elif st[pos+1:pos+3] in [\"ZO\", \"ZI\", \"ZA\"] \\\r\n               or (is_slavo_germanic and pos > first and st[pos-1] != 'T') :\r\n                nxt = ('S', 'TS')\r\n            else :\r\n                nxt = ('S',)\r\n            if st[pos+1] == 'Z' :\r\n                nxt = nxt + (2,)\r\n            else :\r\n                nxt = nxt + (1,)\r\n        # ----------------------------------\r\n        # --- end checking letters------\r\n        # ----------------------------------\r\n        #print str(nxt)\r\n        if len(nxt) == 2 :\r\n            if nxt[0] :\r\n                pri += nxt[0]\r\n                sec += nxt[0]\r\n            pos += nxt[1]\r\n        elif len(nxt) == 3 :\r\n            if nxt[0] :\r\n                pri += nxt[0]\r\n            if nxt[1] :\r\n                sec += nxt[1]\r\n            pos += nxt[2]\r\n    if pri == sec :\r\n        return (pri, None)\r\n    else :\r\n        return (pri, sec)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring data in the autocomplete index.", "response": "def store(self, obj_id, title=None, data=None, obj_type=None):\n        \"\"\"\n        Store data in the autocomplete index.\n\n        :param obj_id: Either a unique identifier for the object\n            being indexed or the word/phrase to be indexed.\n        :param title: The word or phrase to be indexed. If not\n            provided, the ``obj_id`` will be used as the title.\n        :param data: Arbitrary data to index, which will be\n            returned when searching for results. If not provided,\n            this value will default to the title being indexed.\n        :param obj_type: Optional object type. Since results can be\n            boosted by type, you might find it useful to specify this\n            when storing multiple types of objects.\n\n        You have the option of storing several types of data as\n        defined by the parameters. At the minimum, you can specify\n        an ``obj_id``, which will be the word or phrase you wish to\n        index. Alternatively, if for instance you were indexing blog\n        posts, you might specify all parameters.\n        \"\"\"\n        if title is None:\n            title = obj_id\n        if data is None:\n            data = title\n        obj_type = obj_type or ''\n\n        if self._use_json:\n            data = json.dumps(data)\n\n        combined_id = self.object_key(obj_id, obj_type)\n\n        if self.exists(obj_id, obj_type):\n            stored_title = self._title_data[combined_id]\n            if stored_title == title:\n                self._data[combined_id] = data\n                return\n            else:\n                self.remove(obj_id, obj_type)\n\n        self._data[combined_id] = data\n        self._title_data[combined_id] = title\n\n        clean_title = ' '.join(self.tokenize_title(title))\n        title_score = self.score_token(clean_title)\n\n        for idx, word in enumerate(self.tokenize_title(title)):\n            word_score = self.score_token(word)\n            position_score = word_score + (self._offset * idx)\n            key_score = position_score + title_score\n            for substring in self.substrings(word):\n                self.database.zadd(self.word_key(substring),\n                                   {combined_id: key_score})\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving an object identified by the given object_id and obj_type from the search index.", "response": "def remove(self, obj_id, obj_type=None):\n        \"\"\"\n        Remove an object identified by the given ``obj_id`` (and\n        optionally ``obj_type``) from the search index.\n\n        :param obj_id: The object's unique identifier.\n        :param obj_type: The object's type.\n        \"\"\"\n        if not self.exists(obj_id, obj_type):\n            raise KeyError('Object not found.')\n\n        combined_id = self.object_key(obj_id, obj_type)\n        title = self._title_data[combined_id]\n\n        for word in self.tokenize_title(title):\n            for substring in self.substrings(word):\n                key = self.word_key(substring)\n                if not self.database.zrange(key, 1, 2):\n                    self.database.delete(key)\n                else:\n                    self.database.zrem(key, combined_id)\n\n        del self._data[combined_id]\n        del self._title_data[combined_id]\n        del self._boosts[combined_id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exists(self, obj_id, obj_type=None):\n        return self.object_key(obj_id, obj_type) in self._data", "response": "Returns whether the given object exists in the search index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef boost_object(self, obj_id=None, obj_type=None, multiplier=1.1,\n                     relative=True):\n        \"\"\"\n        Boost search results for the given object or type by the\n        amount specified. When the ``multiplier`` is greater than\n        1, the results will percolate to the top. Values between\n        0 and 1 will percolate results to the bottom.\n\n        Either an ``obj_id`` or ``obj_type`` (or both) must be\n        specified.\n\n        :param obj_id: An object's unique identifier (optional).\n        :param obj_type: The object's type (optional).\n        :param multiplier: A positive floating-point number.\n        :param relative: If ``True``, then any pre-existing saved\n            boost will be updated using the given multiplier.\n\n        Examples:\n\n        .. code-block:: python\n\n            # Make all objects of type=photos percolate to top.\n            ac.boost_object(obj_type='photo', multiplier=2.0)\n\n            # Boost a particularly popular blog entry.\n            ac.boost_object(\n                popular_entry.id,\n                'entry',\n                multipler=5.0,\n                relative=False)\n        \"\"\"\n        combined_id = self.object_key(obj_id or '', obj_type or '')\n        if relative:\n            current = float(self._boosts[combined_id] or 1.0)\n            self._boosts[combined_id] = current * multiplier\n        else:\n            self._boosts[combined_id] = multiplier", "response": "Boosts the blog entry for the given object or type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(self, phrase, limit=None, boosts=None, chunk_size=1000):\n        cleaned = self.tokenize_title(phrase, stopwords=False)\n        if not cleaned:\n            return\n\n        all_boosts = self._load_saved_boosts()\n        if PY3 and boosts:\n            for key in boosts:\n                all_boosts[encode(key)] = boosts[key]\n        elif boosts:\n            all_boosts.update(boosts)\n\n        if len(cleaned) == 1 and not all_boosts:\n            result_key = self.word_key(cleaned[0])\n        else:\n            result_key = self.get_cache_key(cleaned, all_boosts)\n            if result_key not in self.database:\n                self.database.zinterstore(\n                    result_key,\n                    list(map(self.word_key, cleaned)))\n            self.database.expire(result_key, self._cache_timeout)\n\n        results = self.database.ZSet(result_key)\n        if all_boosts:\n            for raw_id, score in results[0:0, True]:\n                orig_score = score\n                for identifier in raw_id.split(encode('\\x01'), 1):\n                    if identifier and identifier in all_boosts:\n                        score *= 1 / all_boosts[identifier]\n\n                if orig_score != score:\n                    results[raw_id] = score\n\n        for result in self._load_objects(results, limit, chunk_size):\n            yield result", "response": "Perform a search for the given phrase."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all the data stored in the autocomplete index.", "response": "def list_data(self):\n        \"\"\"\n        Return all the data stored in the autocomplete index. If the data was\n        stored as serialized JSON, then it will be de-serialized before being\n        returned.\n\n        :rtype: list\n        \"\"\"\n        fn = (lambda v: json.loads(decode(v))) if self._use_json else decode\n        return map(fn, self._data.values())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self, batch_size=1000):\n        keys = self.database.keys(self.namespace + ':*')\n        for i in range(0, len(keys), batch_size):\n            self.database.delete(*keys[i:i + batch_size])", "response": "Flushes the database to disk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_document(self, document_id):\n        key = 'doc.%s.%s' % (self.name, decode(document_id))\n        return decode_dict(self.db.hgetall(key))", "response": "Get the content and metadata for a given document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a document to the index.", "response": "def add(self, key, content, **metadata):\n        \"\"\"\n        :param key: Document unique identifier.\n        :param str content: Content to store and index for search.\n        :param metadata: Arbitrary key/value pairs to store for document.\n\n        Add a document to the search index.\n        \"\"\"\n        self.members.add(key)\n        document_hash = self._get_hash(key)\n        document_hash.update(content=content, **metadata)\n\n        for word, score in self.tokenizer.tokenize(content).items():\n            word_key = self.get_key(word)\n            word_key[key] = -score"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, key, preserve_data=False):\n        if self.members.remove(key) != 1:\n            raise KeyError('Document with key \"%s\" not found.' % key)\n        document_hash = self._get_hash(key)\n        content = decode(document_hash['content'])\n        if not preserve_data:\n            document_hash.clear()\n\n        for word in self.tokenizer.tokenize(content):\n            word_key = self.get_key(word)\n            del word_key[key]\n            if len(word_key) == 0:\n                word_key.clear()", "response": "Removes the document with the given key from the search index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the content of a document in the cache.", "response": "def update(self, key, content, **metadata):\n        \"\"\"\n        :param key: Document unique identifier.\n        :param str content: Content to store and index for search.\n        :param metadata: Arbitrary key/value pairs to store for document.\n\n        Update the given document. Existing metadata will be preserved and,\n        optionally, updated with the provided metadata.\n        \"\"\"\n        self.remove(key, preserve_data=True)\n        self.add(key, content, **metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace the given document with the given content.", "response": "def replace(self, key, content, **metadata):\n        \"\"\"\n        :param key: Document unique identifier.\n        :param str content: Content to store and index for search.\n        :param metadata: Arbitrary key/value pairs to store for document.\n\n        Update the given document. Existing metadata will not be removed and\n        replaced with the provided metadata.\n        \"\"\"\n        self.remove(key)\n        self.add(key, content, **metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(self, query):\n        return [self.get_document(key) for key, _ in self._search(query)]", "response": "Search the index for matching content."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stem(self, words):\n        stemmer = PorterStemmer()\n        _stem = stemmer.stem\n        for word in words:\n            yield _stem(word, 0, len(word) - 1)", "response": "Yields the stem of the specified words."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding the set of words that are metaphone.", "response": "def metaphone(self, words):\n        \"\"\"\n        Apply the double metaphone algorithm to the given words.\n        Using metaphone allows the search index to tolerate\n        misspellings and small typos.\n\n        Example::\n\n            >>> from walrus.search.metaphone import dm as metaphone\n            >>> print metaphone('walrus')\n            ('ALRS', 'FLRS')\n\n            >>> print metaphone('python')\n            ('P0N', 'PTN')\n\n            >>> print metaphone('pithonn')\n            ('P0N', 'PTN')\n        \"\"\"\n        for word in words:\n            r = 0\n            for w in double_metaphone(word):\n                if w:\n                    w = w.strip()\n                    if w:\n                        r += 1\n                        yield w\n            if not r:\n                yield word"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tokenize(self, value):\n        words = self.split_phrase(decode(value).lower())\n        if self._stopwords:\n            words = [w for w in words if w not in self._stopwords]\n        if self._min_word_length:\n            words = [w for w in words if len(w) >= self._min_word_length]\n\n        fraction = 1. / (len(words) + 1)  # Prevent division by zero.\n\n        # Apply optional transformations.\n        if self._use_stemmer:\n            words = self.stem(words)\n        if self._use_metaphone:\n            words = self.metaphone(words)\n\n        scores = {}\n        for word in words:\n            scores.setdefault(word, 0)\n            scores[word] += fraction\n        return scores", "response": "Splits the incoming value into tokens and processes each token optionally stemming or running metaphone."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpires the given key in the given number of seconds. If ttl is None then the key will be persisted.", "response": "def expire(self, ttl=None):\n        \"\"\"\n        Expire the given key in the given number of seconds.\n        If ``ttl`` is ``None``, then any expiry will be cleared\n        and key will be persisted.\n        \"\"\"\n        if ttl is not None:\n            self.database.expire(self.key, ttl)\n        else:\n            self.database.persist(self.key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpires the given key in the given number of milliseconds. If ttl is not None then the key will be persisted.", "response": "def pexpire(self, ttl=None):\n        \"\"\"\n        Expire the given key in the given number of milliseconds.\n        If ``ttl`` is ``None``, then any expiry will be cleared\n        and key will be persisted.\n        \"\"\"\n        if ttl is not None:\n            self.database.pexpire(self.key, ttl)\n        else:\n            self.database.persist(self.key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self, pattern, count=None):\n        return self._scan(match=pattern, count=count)", "response": "Search the keys of the given hash using the specified pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the hash using the given dictionary or key value pairs.", "response": "def update(self, *args, **kwargs):\n        \"\"\"\n        Update the hash using the given dictionary or key/value pairs.\n        \"\"\"\n        if args:\n            self.database.hmset(self.key, *args)\n        else:\n            self.database.hmset(self.key, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef incr(self, key, incr_by=1):\n        return self.database.hincrby(self.key, key, incr_by)", "response": "Increment the key by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef incr_float(self, key, incr_by=1.):\n        return self.database.hincrbyfloat(self.key, key, incr_by)", "response": "Increment the value of the key by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all the key value pairs in the the hash.", "response": "def as_dict(self, decode=False):\n        \"\"\"\n        Return a dictionary containing all the key/value pairs in the\n        hash.\n        \"\"\"\n        res = self.database.hgetall(self.key)\n        return decode_dict(res) if decode else res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and populate a Hash object from a dictionary.", "response": "def from_dict(cls, database, key, data, clear=False):\n        \"\"\"\n        Create and populate a Hash object from a data dictionary.\n        \"\"\"\n        hsh = cls(database, key)\n        if clear:\n            hsh.clear()\n        hsh.update(data)\n        return hsh"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the first item from the list blocking until the first available item becomes an item.", "response": "def bpopleft(self, timeout=0):\n        \"\"\"\n        Remove the first item from the list, blocking until an item becomes\n        available or timeout is reached (0 for no timeout, default).\n        \"\"\"\n        ret = self.database.blpop(self.key, timeout)\n        if ret is not None:\n            return ret[1]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the last item from the list blocking until the item becomes available.", "response": "def bpopright(self, timeout=0):\n        \"\"\"\n        Remove the last item from the list, blocking until an item becomes\n        available or timeout is reached (0 for no timeout, default).\n        \"\"\"\n        ret = self.database.blpop(self.key, timeout)\n        if ret is not None:\n            return ret[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_list(self, decode=False):\n        items = self.database.lrange(self.key, 0, -1)\n        return [_decode(item) for item in items] if decode else items", "response": "Return all the items in the cache as a list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_list(cls, database, key, data, clear=False):\n        lst = cls(database, key)\n        if clear:\n            lst.clear()\n        lst.extend(data)\n        return lst", "response": "Create and populate a List object from a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diffstore(self, dest, *others):\n        keys = [self.key]\n        keys.extend([other.key for other in others])\n        self.database.sdiffstore(dest, keys)\n        return self.database.Set(dest)", "response": "Store the difference of the current set and one or more set instances in a new key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interstore(self, dest, *others):\n        keys = [self.key]\n        keys.extend([other.key for other in others])\n        self.database.sinterstore(dest, keys)\n        return self.database.Set(dest)", "response": "Store the intersection of the current set and one or more set instances in a new key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the union of the current set and one or more set instances in a new key.", "response": "def unionstore(self, dest, *others):\n        \"\"\"\n        Store the union of the current set and one or more\n        others in a new key.\n\n        :param dest: the name of the key to store union\n        :param others: One or more :py:class:`Set` instances\n        :returns: A :py:class:`Set` referencing ``dest``.\n        \"\"\"\n        keys = [self.key]\n        keys.extend([other.key for other in others])\n        self.database.sunionstore(dest, keys)\n        return self.database.Set(dest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all the items in the collection as a Python set.", "response": "def as_set(self, decode=False):\n        \"\"\"\n        Return a Python set containing all the items in the collection.\n        \"\"\"\n        items = self.database.smembers(self.key)\n        return set(_decode(item) for item in items) if decode else items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_set(cls, database, key, data, clear=False):\n        s = cls(database, key)\n        if clear:\n            s.clear()\n        s.add(*data)\n        return s", "response": "Create and populate a Set object from a data set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the given item and score pairs to the ZSet.", "response": "def add(self, _mapping=None, **kwargs):\n        \"\"\"\n        Add the given item/score pairs to the ZSet. Arguments are\n        specified as ``item1, score1, item2, score2...``.\n        \"\"\"\n        if _mapping is not None:\n            _mapping.update(kwargs)\n            mapping = _mapping\n        else:\n            mapping = _mapping\n        return self.database.zadd(self.key, mapping)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the rank of the given item.", "response": "def rank(self, item, reverse=False):\n        \"\"\"Return the rank of the given item.\"\"\"\n        fn = reverse and self.database.zrevrank or self.database.zrank\n        return fn(self.key, item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of items between the given bounds.", "response": "def count(self, low, high=None):\n        \"\"\"\n        Return the number of items between the given bounds.\n        \"\"\"\n        if high is None:\n            high = low\n        return self.database.zcount(self.key, low, high)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lex_count(self, low, high):\n        return self.database.zlexcount(self.key, low, high)", "response": "Count the number of members in a sorted set between a given lexicographical range."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef range(self, low, high, with_scores=False, desc=False, reverse=False):\n        if reverse:\n            return self.database.zrevrange(self.key, low, high, with_scores)\n        else:\n            return self.database.zrange(self.key, low, high, desc, with_scores)", "response": "Return a range of items between low and high."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range_by_lex(self, low, high, start=None, num=None, reverse=False):\n        if reverse:\n            fn = self.database.zrevrangebylex\n            low, high = high, low\n        else:\n            fn = self.database.zrangebylex\n        return fn(self.key, low, high, start, num)", "response": "Return a range of members in a sorted set by lexicographical range."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove elements from the ZSet by their rank.", "response": "def remove_by_rank(self, low, high=None):\n        \"\"\"\n        Remove elements from the ZSet by their rank (relative position).\n\n        :param low: Lower bound.\n        :param high: Upper bound.\n        \"\"\"\n        if high is None:\n            high = low\n        return self.database.zremrangebyrank(self.key, low, high)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove elements from the ZSet by their score.", "response": "def remove_by_score(self, low, high=None):\n        \"\"\"\n        Remove elements from the ZSet by their score.\n\n        :param low: Lower bound.\n        :param high: Upper bound.\n        \"\"\"\n        if high is None:\n            high = low\n        return self.database.zremrangebyscore(self.key, low, high)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrement the score of an item in the ZSet.", "response": "def incr(self, key, incr_by=1.):\n        \"\"\"\n        Increment the score of an item in the ZSet.\n\n        :param key: Item to increment.\n        :param incr_by: Amount to increment item's score.\n        \"\"\"\n        return self.database.zincrby(self.key, incr_by, key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore the intersection of the current zset and one or more zsets in a new key.", "response": "def interstore(self, dest, *others, **kwargs):\n        \"\"\"\n        Store the intersection of the current zset and one or more\n        others in a new key.\n\n        :param dest: the name of the key to store intersection\n        :param others: One or more :py:class:`ZSet` instances\n        :returns: A :py:class:`ZSet` referencing ``dest``.\n        \"\"\"\n        keys = [self.key]\n        keys.extend([other.key for other in others])\n        self.database.zinterstore(dest, keys, **kwargs)\n        return self.database.ZSet(dest)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unionstore(self, dest, *others, **kwargs):\n        keys = [self.key]\n        keys.extend([other.key for other in others])\n        self.database.zunionstore(dest, keys, **kwargs)\n        return self.database.ZSet(dest)", "response": "Store the union of the current set and one or more objects in a new key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bpopmin(self, timeout=0):\n        res = self.database.bzpopmin(self.key, timeout)\n        if res is not None:\n            return (res[1], res[2])", "response": "Atomically remove the lowest - scoring item from the set blocking until the item becomes available. Returns a 2 - tuple of item and score."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_items(self, decode=False):\n        items = self.database.zrange(self.key, 0, -1, withscores=True)\n        if decode:\n            items = [(_decode(k), score) for k, score in items]\n        return items", "response": "Return a list of 2 - tuples consisting of key and score."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(cls, database, key, data, clear=False):\n        zset = cls(database, key)\n        if clear:\n            zset.clear()\n        zset.add(data)\n        return zset", "response": "Create and populate a ZSet object from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(self, dest, *others):\n        items = [self.key]\n        items.extend([other.key for other in others])\n        self.database.pfmerge(dest, *items)\n        return HyperLogLog(self.database, dest)", "response": "Merge two or more HyperLogLog instances into a new one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, value):\n        self.database.run_script(\n            'array_append',\n            keys=[self.key],\n            args=[value])", "response": "Append a new value to the end of the array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend(self, values):\n        self.database.run_script(\n            'array_extend',\n            keys=[self.key],\n            args=values)", "response": "Extend the array appending the given values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an item from the array.", "response": "def pop(self, idx=None):\n        \"\"\"\n        Remove an item from the array. By default this will be the\n        last item by index, but any index can be specified.\n        \"\"\"\n        if idx is not None:\n            return self.database.run_script(\n                'array_remove',\n                keys=[self.key],\n                args=[idx])\n        else:\n            return self.database.run_script(\n                'array_pop',\n                keys=[self.key],\n                args=[])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_list(self, decode=False):\n        return [_decode(i) for i in self] if decode else list(self)", "response": "Return a list of items in the array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_list(cls, database, key, data, clear=False):\n        arr = cls(database, key)\n        if clear:\n            arr.clear()\n        arr.extend(data)\n        return arr", "response": "Create and populate an Array object from a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding data to a stream.", "response": "def add(self, data, id='*', maxlen=None, approximate=True):\n        \"\"\"\n        Add data to a stream.\n\n        :param dict data: data to add to stream\n        :param id: identifier for message ('*' to automatically append)\n        :param maxlen: maximum length for stream\n        :param approximate: allow stream max length to be approximate\n        :returns: the added message id.\n        \"\"\"\n        return self.database.xadd(self.key, data, id, maxlen, approximate)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef range(self, start='-', stop='+', count=None):\n        return self.database.xrange(self.key, start, stop, count)", "response": "Read a range of values from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef revrange(self, start='+', stop='-', count=None):\n        return self.database.xrevrange(self.key, start, stop, count)", "response": "Read a range of values from a stream in reverse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the next count messages from the cache.", "response": "def read(self, count=None, block=None, last_id=None):\n        \"\"\"\n        Monitor stream for new data.\n\n        :param int count: limit number of messages returned\n        :param int block: milliseconds to block, 0 for indefinitely\n        :param last_id: Last id read (an exclusive lower-bound). If the '$'\n            value is given, we will only read values added *after* our command\n            started blocking.\n        :returns: a list of (message id, data) 2-tuples.\n        \"\"\"\n        if last_id is None: last_id = '0-0'\n        resp = self.database.xread({self.key: _decode(last_id)}, count, block)\n\n            # resp is a 2-tuple of stream name -> message list.\n        return resp[0][1] if resp else []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrim the stream to the given count of messages.", "response": "def trim(self, count, approximate=True):\n        \"\"\"\n        Trim the stream to the given \"count\" of messages, discarding the oldest\n        messages first.\n\n        :param count: maximum size of stream\n        :param approximate: allow size to be approximate\n        \"\"\"\n        return self.database.xtrim(self.key, count, approximate)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacknowledge that the messages were processed by the parent.", "response": "def ack(self, *id_list):\n        \"\"\"\n        Acknowledge that the message(s) were been processed by the consumer\n        associated with the parent :py:class:`ConsumerGroup`.\n\n        :param id_list: one or more message ids to acknowledge\n        :returns: number of messages marked acknowledged\n        \"\"\"\n        return self.database.xack(self.key, self.group, *id_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclaims pending but unacknowledged messages for this stream within the parent group.", "response": "def claim(self, *id_list, **kwargs):\n        \"\"\"\n        Claim pending - but unacknowledged - messages for this stream within\n        the context of the parent :py:class:`ConsumerGroup`.\n\n        :param id_list: one or more message ids to acknowledge\n        :param min_idle_time: minimum idle time in milliseconds (keyword-arg).\n        :returns: list of (message id, data) 2-tuples of messages that were\n            successfully claimed\n        \"\"\"\n        min_idle_time = kwargs.pop('min_idle_time', None) or 0\n        if kwargs: raise ValueError('incorrect arguments for claim()')\n        return self.database.xclaim(self.key, self.group, self._consumer,\n                                    min_idle_time, id_list)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pending(self, start='-', stop='+', count=1000, consumer=None):\n        return self.database.xpending_range(self.key, self.group, start, stop,\n                                            count, consumer)", "response": "List pending messages within the consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmonitors the stream for new messages within the context of the parent.", "response": "def read(self, count=None, block=None, last_id=None):\n        \"\"\"\n        Monitor the stream for new messages within the context of the parent\n        :py:class:`ConsumerGroup`.\n\n        :param int count: limit number of messages returned\n        :param int block: milliseconds to block, 0 for indefinitely.\n        :param str last_id: optional last ID, by default uses the special\n            token \">\", which reads the oldest unread message.\n        :returns: a list of (message id, data) 2-tuples.\n        \"\"\"\n        key = {self.key: '>' if last_id is None else last_id}\n        resp = self.database.xreadgroup(self.group, self._consumer, key, count,\n                                        block)\n        return resp[0][1] if resp else []"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the last - read message id for the stream within the context of the current ConsumerGroup.", "response": "def set_id(self, id='$'):\n        \"\"\"\n        Set the last-read message id for the stream within the context of the\n        parent :py:class:`ConsumerGroup`. By default this will be the special\n        \"$\" identifier, meaning all messages are marked as having been read.\n\n        :param id: id of last-read message (or \"$\").\n        \"\"\"\n        return self.database.xgroup_setid(self.key, self.group, id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_consumer(self, consumer=None):\n        if consumer is None: consumer = self._consumer\n        return self.database.xgroup_delconsumer(self.key, self.group, consumer)", "response": "Remove a specific consumer from a consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consumer(self, name):\n        return type(self)(self.database, self.name, self.keys, name)", "response": "Create a new consumer for the given consumer name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the consumer group and register it with the consumer group s stream keys.", "response": "def create(self, ensure_keys_exist=True, mkstream=False):\n        \"\"\"\n        Create the consumer group and register it with the group's stream keys.\n\n        :param ensure_keys_exist: Ensure that the streams exist before creating\n            the consumer group. Streams that do not exist will be created.\n        :param mkstream: Use the \"MKSTREAM\" option to ensure stream exists (may\n            require unstable version of Redis).\n        \"\"\"\n        if ensure_keys_exist:\n            for key in self.keys:\n                if not self.database.exists(key):\n                    msg_id = self.database.xadd(key, {'': ''}, id=b'0-1')\n                    self.database.xdel(key, msg_id)\n                elif self.database.type(key) != b'stream':\n                    raise ValueError('Consumer group key \"%s\" exists and is '\n                                     'not a stream. To prevent data-loss '\n                                     'this key will not be deleted.')\n\n        resp = {}\n\n        # Mapping of key -> last-read message ID.\n        for key, value in self.keys.items():\n            try:\n                resp[key] = self.database.xgroup_create(key, self.name, value,\n                                                        mkstream)\n            except ResponseError as exc:\n                if exception_message(exc).startswith('BUSYGROUP'):\n                    resp[key] = False\n                else:\n                    raise\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef destroy(self):\n        resp = {}\n        for key in self.keys:\n            resp[key] = self.database.xgroup_destroy(key, self.name)\n        return resp", "response": "Destroy the consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for the database. xreadgroup method.", "response": "def read(self, count=None, block=None, consumer=None):\n        \"\"\"\n        Read unseen messages from all streams in the consumer group. Wrapper\n        for :py:class:`Database.xreadgroup` method.\n\n        :param int count: limit number of messages returned\n        :param int block: milliseconds to block, 0 for indefinitely.\n        :param consumer: consumer name\n        :returns: a list of (stream key, messages) tuples, where messages is\n            a list of (message id, data) 2-tuples.\n        \"\"\"\n        if consumer is None: consumer = self._consumer\n        return self.database.xreadgroup(self.name, consumer, self._read_keys,\n                                        count, block)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_id(self, id='$'):\n        accum = {}\n        for key in self.keys:\n            accum[key] = self.database.xgroup_setid(key, self.name, id)\n        return accum", "response": "Set the last - read message id for each consumer group. By default this will be the special \"$\" identifier."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_info(self):\n        accum = {}\n        for key in self.keys:\n            accum[key] = self.database.xinfo_stream(key)\n        return accum", "response": "Retrieve information for each stream managed by the consumer group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nincrementing a bitfield by a given amount.", "response": "def incrby(self, fmt, offset, increment, overflow=None):\n        \"\"\"\n        Increment a bitfield by a given amount.\n\n        :param fmt: format-string for the bitfield being updated, e.g. u8 for\n            an unsigned 8-bit integer.\n        :param int offset: offset (in number of bits).\n        :param int increment: value to increment the bitfield by.\n        :param str overflow: overflow algorithm. Defaults to WRAP, but other\n            acceptable values are SAT and FAIL. See the Redis docs for\n            descriptions of these algorithms.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        if overflow is not None and overflow != self._last_overflow:\n            self._last_overflow = overflow\n            self.operations.append(('OVERFLOW', overflow))\n\n        self.operations.append(('INCRBY', fmt, offset, increment))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrement a bitfield by a given amount.", "response": "def incrby(self, fmt, offset, increment, overflow=None):\n        \"\"\"\n        Increment a bitfield by a given amount.\n\n        :param fmt: format-string for the bitfield being updated, e.g. u8 for\n            an unsigned 8-bit integer.\n        :param int offset: offset (in number of bits).\n        :param int increment: value to increment the bitfield by.\n        :param str overflow: overflow algorithm. Defaults to WRAP, but other\n            acceptable values are SAT and FAIL. See the Redis docs for\n            descriptions of these algorithms.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        bfo = BitFieldOperation(self.database, self.key)\n        return bfo.incrby(fmt, offset, increment, overflow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, fmt, offset):\n        bfo = BitFieldOperation(self.database, self.key)\n        return bfo.get(fmt, offset)", "response": "Get the value of a given bitfield."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, fmt, offset, value):\n        bfo = BitFieldOperation(self.database, self.key)\n        return bfo.set(fmt, offset, value)", "response": "Set the value of a given bitfield at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bit_count(self, start=None, end=None):\n        return self.database.bitcount(self.key, start, end)", "response": "Count the number of bits set in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_bit(self, offset, value):\n        return self.database.setbit(self.key, offset, value)", "response": "Set the value of the given bit at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an item to the bloomfilter.", "response": "def add(self, data):\n        \"\"\"\n        Add an item to the bloomfilter.\n\n        :param bytes data: a bytestring representing the item to add.\n        \"\"\"\n        bfo = BitFieldOperation(self.database, self.key)\n        for bit_index in self._get_seeds(data):\n            bfo.set('u1', bit_index, 1)\n        bfo.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contains(self, data):\n        bfo = BitFieldOperation(self.database, self.key)\n        for bit_index in self._get_seeds(data):\n            bfo.get('u1', bit_index)\n        return all(bfo.execute())", "response": "Check if an item has been added to the bloomfilter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cons(self, i):\r\n        if (self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or\r\n            self.b[i] == 'o' or self.b[i] == 'u'):\r\n            return 0\r\n        if self.b[i] == 'y':\r\n            if i == self.k0:\r\n                return 1\r\n            else:\r\n                return (not self.cons(i - 1))\r\n        return 1", "response": "Returns 1 if the ith character is a consonant."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef m(self):\r\n        n = 0\r\n        i = self.k0\r\n        while 1:\r\n            if i > self.j:\r\n                return n\r\n            if not self.cons(i):\r\n                break\r\n            i = i + 1\r\n        i = i + 1\r\n        while 1:\r\n            while 1:\r\n                if i > self.j:\r\n                    return n\r\n                if self.cons(i):\r\n                    break\r\n                i = i + 1\r\n            i = i + 1\r\n            n = n + 1\r\n            while 1:\r\n                if i > self.j:\r\n                    return n\r\n                if not self.cons(i):\r\n                    break\r\n                i = i + 1\r\n            i = i + 1", "response": "m returns the number of consonant sequences between k0 and j"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vowelinstem(self):\r\n        for i in range(self.k0, self.j + 1):\r\n            if not self.cons(i):\r\n                return 1\r\n        return 0", "response": "returns 1 if the user has a vowel in the hierarchy 0 otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef step3(self):\r\n        if self.b[self.k] == 'e':\r\n            if self.ends(\"icate\"):     self.r(\"ic\")\r\n            elif self.ends(\"ative\"):   self.r(\"\")\r\n            elif self.ends(\"alize\"):   self.r(\"al\")\r\n        elif self.b[self.k] == 'i':\r\n            if self.ends(\"iciti\"):     self.r(\"ic\")\r\n        elif self.b[self.k] == 'l':\r\n            if self.ends(\"ical\"):      self.r(\"ic\")\r\n            elif self.ends(\"ful\"):     self.r(\"\")\r\n        elif self.b[self.k] == 's':\r\n            if self.ends(\"ness\"):      self.r(\"\")", "response": "step 3 deletes the ice - full -ness etc. similar strategy\r\n            to step2."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef step4(self):\r\n        if self.b[self.k - 1] == 'a':\r\n            if self.ends(\"al\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'c':\r\n            if self.ends(\"ance\"): pass\r\n            elif self.ends(\"ence\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'e':\r\n            if self.ends(\"er\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'i':\r\n            if self.ends(\"ic\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'l':\r\n            if self.ends(\"able\"): pass\r\n            elif self.ends(\"ible\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'n':\r\n            if self.ends(\"ant\"): pass\r\n            elif self.ends(\"ement\"): pass\r\n            elif self.ends(\"ment\"): pass\r\n            elif self.ends(\"ent\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'o':\r\n            if (self.ends(\"ion\") and\r\n                (self.b[self.j] == 's' or self.b[self.j] == 't')):\r\n                pass\r\n            elif self.ends(\"ou\"):\r\n                pass\r\n            # takes care of -ous\r\n            else:\r\n                return\r\n        elif self.b[self.k - 1] == 's':\r\n            if self.ends(\"ism\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 't':\r\n            if self.ends(\"ate\"): pass\r\n            elif self.ends(\"iti\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'u':\r\n            if self.ends(\"ous\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'v':\r\n            if self.ends(\"ive\"): pass\r\n            else: return\r\n        elif self.b[self.k - 1] == 'z':\r\n            if self.ends(\"ize\"): pass\r\n            else: return\r\n        else:\r\n            return\r\n        if self.m() > 1:\r\n            self.k = self.j", "response": "Returns a new version of\r\n            based on the current state of the current sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores multiple subject - predicate - object triples in the database.", "response": "def store_many(self, items):\n        \"\"\"\n        Store multiple subject-predicate-object triples in the database.\n\n        :param items: A list of (subj, pred, obj) 3-tuples.\n        \"\"\"\n        with self.walrus.atomic():\n            for item in items:\n                self.store(*item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the given subj - pred - obj triple from the database.", "response": "def delete(self, s, p, o):\n        \"\"\"Remove the given subj-pred-obj triple from the database.\"\"\"\n        with self.walrus.atomic():\n            for key in self.keys_for_values(s, p, o):\n                del self._z[key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries the cache for all the entries in the given set of keys.", "response": "def query(self, s=None, p=None, o=None):\n        \"\"\"\n        Return all triples that satisfy the given expression. You may specify\n        all or none of the fields (s, p, and o). For instance, if I wanted\n        to query for all the people who live in Kansas, I might write:\n\n        .. code-block:: python\n\n            for triple in graph.query(p='lives', o='Kansas'):\n                print triple['s'], 'lives in Kansas!'\n        \"\"\"\n        start, end = self.keys_for_query(s, p, o)\n        if end is None:\n            if start in self._z:\n                yield {'s': s, 'p': p, 'o': o}\n            else:\n                raise StopIteration\n        else:\n            for key in self._z.range_by_lex('[' + start, '[' + end):\n                keys, p1, p2, p3 = decode(key).split('::')\n                yield dict(zip(keys, (p1, p2, p3)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self, *conditions):\n        results = {}\n\n        for condition in conditions:\n            if isinstance(condition, tuple):\n                query = dict(zip('spo', condition))\n            else:\n                query = condition.copy()\n            materialized = {}\n            targets = []\n\n            for part in ('s', 'p', 'o'):\n                if isinstance(query[part], Variable):\n                    variable = query.pop(part)\n                    materialized[part] = set()\n                    targets.append((variable, part))\n\n            # Potentially rather than popping all the variables, we could use\n            # the result values from a previous condition and do O(results)\n            # loops looking for a single variable.\n            for result in self.query(**query):\n                ok = True\n                for var, part in targets:\n                    if var in results and result[part] not in results[var]:\n                        ok = False\n                        break\n\n                if ok:\n                    for var, part in targets:\n                        materialized[part].add(result[part])\n\n            for var, part in targets:\n                if var in results:\n                    results[var] &= materialized[part]\n                else:\n                    results[var] = materialized[part]\n\n        return dict((var.name, vals) for (var, vals) in results.items())", "response": "Search for all values that satisfy the condition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef commit_transaction(self):\n        with self._transaction_lock:\n            local = self._transaction_local\n            if not local.pipes:\n                raise ValueError('No transaction is currently active.')\n            return local.commit()", "response": "Commits the currently active transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_transaction(self):\n        with self._transaction_lock:\n            local = self._transaction_local\n            if not local.pipes:\n                raise ValueError('No transaction is currently active.')\n            local.abort()", "response": "Clears the currently active transaction."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_script(self, script_name, keys=None, args=None):\n        return self._scripts[script_name](keys, args)", "response": "Execute a walrus script with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a rich object for the given key.", "response": "def get_key(self, key):\n        \"\"\"\n        Return a rich object for the given key. For instance, if\n        a hash key is requested, then a :py:class:`Hash` will be\n        returned.\n\n        :param str key: Key to retrieve.\n        :returns: A hash, set, list, zset or array.\n        \"\"\"\n        return self.__mapping.get(self.type(key), self.__getitem__)(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache(self, name='cache', default_timeout=3600):\n        return Cache(self, name=name, default_timeout=default_timeout)", "response": "Create a new Cache instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Graph instance with the given name and optional arguments.", "response": "def graph(self, name, *args, **kwargs):\n        \"\"\"\n        Creates a :py:class:`Graph` instance.\n\n        :param str name: The namespace for the graph metadata.\n        :returns: a :py:class:`Graph` instance.\n        \"\"\"\n        return Graph(self, name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a named lock instance.", "response": "def lock(self, name, ttl=None, lock_id=None):\n        \"\"\"\n        Create a named :py:class:`Lock` instance. The lock implements\n        an API similar to the standard library's ``threading.Lock``,\n        and can also be used as a context manager or decorator.\n\n        :param str name: The name of the lock.\n        :param int ttl: The time-to-live for the lock in milliseconds\n            (optional). If the ttl is ``None`` then the lock will not\n            expire.\n        :param str lock_id: Optional identifier for the lock instance.\n        \"\"\"\n        return Lock(self, name, ttl, lock_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rate_limit(self, name, limit=5, per=60, debug=False):\n        return RateLimit(self, name, limit, per, debug)", "response": "Rate limit implementation. Allows up to limit events every per second."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef consumer_group(self, group, keys, consumer=None):\n        return ConsumerGroup(self, group, keys, consumer=consumer)", "response": "Create a new ConsumerGroup instance for the given keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new time - series object for the given key - list.", "response": "def time_series(self, group, keys, consumer=None):\n        \"\"\"\n        Create a named :py:class:`TimeSeries` consumer-group for the\n        given key(s). TimeSeries objects are almost identical to\n        :py:class:`ConsumerGroup` except they offer a higher level of\n        abstraction and read/write message ids as datetimes.\n\n        :param group: name of consumer group\n        :param keys: stream identifier(s) to monitor. May be a single stream\n            key, a list of stream keys, or a key-to-minimum id mapping. The\n            minimum id for each stream should be considered an exclusive\n            lower-bound. The '$' value can also be used to only read values\n            added *after* our command started blocking.\n        :param consumer: name for consumer within group\n        :returns: a :py:class:`TimeSeries` instance\n        \"\"\"\n        return TimeSeries(self, group, keys, consumer=consumer)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cas(self, key, value, new_value):\n        return self.run_script('cas', keys=[key], args=[value, new_value])", "response": "Perform an atomic compare - and - set on the value in key using a prefix\n        match on the provided value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstreams Redis activity one line at a time to the given callback.", "response": "def stream_log(self, callback, connection_id='monitor'):\n        \"\"\"\n        Stream Redis activity one line at a time to the given\n        callback.\n\n        :param callback: A function that accepts a single argument,\n            the Redis command.\n        \"\"\"\n        conn = self.connection_pool.get_connection(connection_id, None)\n        conn.send_command('monitor')\n        while callback(conn.read_response()):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquire(self, block=True):\n        while True:\n            acquired = self.database.run_script(\n                'lock_acquire',\n                keys=[self.key],\n                args=[self._lock_id, self.ttl])\n            if acquired == 1 or not block:\n                return acquired == 1\n\n            # Perform a blocking pop on the event key. When a lock\n            # is released, a value is pushed into the list, which\n            # signals listeners that the lock is available.\n            self.database.blpop(self.event, self.ttl)", "response": "Acquire the lock. The lock will be held until it is released\n        by calling :py:meth:`Lock.release`. If the lock was\n        initialized with a ``ttl``, then the lock will be released\n        automatically after the given number of milliseconds.\n\n        By default this method will block until the lock becomes\n        free (either by being released or expiring). The blocking is\n        accomplished by performing a blocking left-pop on a list, as\n        opposed to a spin-loop.\n\n        If you specify ``block=False``, then the method will return\n        ``False`` if the lock could not be acquired.\n\n        :param bool block: Whether to block while waiting to acquire\n            the lock.\n        :returns: Returns ``True`` if the lock was acquired."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef release(self):\n        unlocked = self.database.run_script(\n            'lock_release',\n            keys=[self.key, self.event],\n            args=[self._lock_id])\n        return unlocked != 0", "response": "Release the lock.\n\n        :returns: Returns ``True`` if the lock was released."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self):\n        self.database.delete(self.key)\n        self.database.delete(self.event)", "response": "Clear the lock and clear the event table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_key(self, *parts):\n        separator = getattr(self.model_class, 'index_separator', '.')\n        parts = map(decode, parts)\n        return '%s%s' % (self._base_key, separator.join(map(str, parts)))", "response": "Generate a namespaced key for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef incr(self, field, incr_by=1):\n        model_hash = self.to_hash()\n\n        # Remove the value from the index.\n        for index in field.get_indexes():\n            index.remove(self)\n\n        if isinstance(incr_by, int):\n            new_val = model_hash.incr(field.name, incr_by)\n        else:\n            new_val = model_hash.incr_float(field.name, incr_by)\n        setattr(self, field.name, new_val)\n\n        # Re-index the new value.\n        for index in field.get_indexes():\n            index.save(self)\n\n        return new_val", "response": "Increment the value stored in the given field by the specified amount."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator that successively yields saved model instances.", "response": "def all(cls):\n        \"\"\"\n        Return an iterator that successively yields saved model\n        instances. Models are saved in an unordered :py:class:`Set`,\n        so the iterator will return them in arbitrary order.\n\n        Example::\n\n            for note in Note.all():\n                print note.content\n\n        To return models in sorted order, see :py:meth:`Model.query`.\n        Example returning all records, sorted newest to oldest::\n\n            for note in Note.query(order_by=Note.timestamp.desc()):\n                print note.timestamp, note.content\n        \"\"\"\n        for result in cls._query.all_index():\n            yield cls.load(result, convert_key=False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery the blog entries for the given class.", "response": "def query(cls, expression=None, order_by=None):\n        \"\"\"\n        Return model instances matching the given expression (if\n        specified). Additionally, matching instances can be returned\n        sorted by field value.\n\n        Example::\n\n            # Get administrators sorted by username.\n            admin_users = User.query(\n                (User.admin == True),\n                order_by=User.username)\n\n            # List blog entries newest to oldest.\n            entries = Entry.query(order_by=Entry.timestamp.desc())\n\n            # Perform a complex filter.\n            values = StatData.query(\n                (StatData.timestamp < datetime.date.today()) &\n                ((StatData.type == 'pv') | (StatData.type == 'cv')))\n\n        :param expression: A boolean expression to filter by.\n        :param order_by: A field whose value should be used to\n            sort returned instances.\n        \"\"\"\n        if expression is not None:\n            executor = Executor(cls.__database__)\n            result = executor.execute(expression)\n        else:\n            result = cls._query.all_index()\n\n        if order_by is not None:\n            desc = False\n            if isinstance(order_by, Desc):\n                desc = True\n                order_by = order_by.node\n\n            alpha = not isinstance(order_by, _ScalarField)\n            result = cls.__database__.sort(\n                result.key,\n                by='*->%s' % order_by.name,\n                alpha=alpha,\n                desc=desc)\n        elif isinstance(result, ZSet):\n            result = result.iterator(reverse=True)\n\n        for hash_id in result:\n            yield cls.load(hash_id, convert_key=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_delete(cls, expression=None):\n        if expression is not None:\n            executor = Executor(cls.__database__)\n            result = executor.execute(expression)\n        else:\n            result = cls._query.all_index()\n\n        for hash_id in result:\n            cls.load(hash_id, convert_key=False).delete()", "response": "Query the database for all model instances matching the given expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls, expression):\n        executor = Executor(cls.__database__)\n        result = executor.execute(expression)\n        if len(result) != 1:\n            raise ValueError('Got %s results, expected 1.' % len(result))\n        return cls.load(result._first_or_any(), convert_key=False)", "response": "Retrieve the model instance matching the given expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a model instance by primary key.", "response": "def load(cls, primary_key, convert_key=True):\n        \"\"\"\n        Retrieve a model instance by primary key.\n\n        :param primary_key: The primary key of the model instance.\n        :returns: Corresponding :py:class:`Model` instance.\n        :raises: ``KeyError`` if object with given primary key does\n            not exist.\n        \"\"\"\n        if convert_key:\n            primary_key = cls._query.get_primary_hash_key(primary_key)\n        if not cls.__database__.hash_exists(primary_key):\n            raise KeyError('Object not found.')\n        raw_data = cls.__database__.hgetall(primary_key)\n        if PY3:\n            raw_data = decode_dict_keys(raw_data)\n        data = {}\n        for name, field in cls._fields.items():\n            if isinstance(field, _ContainerField):\n                continue\n            elif name in raw_data:\n                data[name] = field.python_value(raw_data[name])\n            else:\n                data[name] = None\n\n        return cls(**data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the given model instance.", "response": "def delete(self, for_update=False):\n        \"\"\"\n        Delete the given model instance.\n        \"\"\"\n        hash_key = self.get_hash_id()\n        try:\n            original_instance = self.load(hash_key, convert_key=False)\n        except KeyError:\n            return\n\n        # Remove from the `all` index.\n        all_index = self._query.all_index()\n        all_index.remove(hash_key)\n\n        # Remove from the secondary indexes.\n        for field in self._indexes:\n            for index in field.get_indexes():\n                index.remove(original_instance)\n\n        if not for_update:\n            for field in self._fields.values():\n                if isinstance(field, _ContainerField):\n                    field._delete(self)\n\n        # Remove the object itself.\n        self.__database__.delete(hash_key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self):\n        pk_field = self._fields[self._primary_key]\n        if not self._data.get(self._primary_key):\n            setattr(self, self._primary_key, pk_field._generate_key())\n            require_delete = False\n        else:\n            require_delete = True\n\n        if require_delete:\n            self.delete(for_update=True)\n\n        data = self._get_data_dict()\n        hash_obj = self.to_hash()\n        hash_obj.clear()\n        hash_obj.update(data)\n\n        all_index = self._query.all_index()\n        all_index.add(self.get_hash_id())\n\n        for field in self._indexes:\n            for index in field.get_indexes():\n                index.save(self)", "response": "Save the given model instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef limit(self, key):\n        if self._debug:\n            return False\n\n        counter = self.database.List(self.name + ':' + key)\n        n = len(counter)\n        is_limited = False\n        if n < self._limit:\n            counter.prepend(str(time.time()))\n        else:\n            oldest = float(counter[-1])\n            if time.time() - oldest < self._per:\n                is_limited = True\n            else:\n                counter.prepend(str(time.time()))\n            del counter[:self._limit]\n        counter.pexpire(int(self._per * 2000))\n        return is_limited", "response": "Function to limit the number of events for a given key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rate_limited(self, key_function=None):\n        if key_function is None:\n            def key_function(*args, **kwargs):\n                data = pickle.dumps((args, sorted(kwargs.items())))\n                return hashlib.md5(data).hexdigest()\n\n        def decorator(fn):\n            @wraps(fn)\n            def inner(*args, **kwargs):\n                key = key_function(*args, **kwargs)\n                if self.limit(key):\n                    raise RateLimitException(\n                        'Call to %s exceeded %s events in %s seconds.' % (\n                            fn.__name__, self._limit, self._per))\n                return fn(*args, **kwargs)\n            return inner\n        return decorator", "response": "Decorator that will prevent calls to the decorated function when the number of events has been exceeded for the given time period."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, count=None, block=None):\n        resp = super(TimeSeries, self).read(count, block)\n        return xread_to_messages(resp)", "response": "Wrapper for the Database. xreadgroup method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _log_multipart(self, body, logging_context):\n        try:\n            body_str = body.decode()\n        except UnicodeDecodeError:\n            self.logger.log(self.log_level, \"(multipart/form)\", logging_context)\n            return\n\n        parts = body_str.split(self.boundary)\n        last = len(parts) - 1\n        for i, part in enumerate(parts):\n            if 'Content-Type:' in part:\n                match = BINARY_REGEX.search(part)\n                if match and match.group(2) in BINARY_TYPES and not match.group(4) in ('', '\\r\\n'):\n                    part = match.expand(r'\\1\\2/\\3\\r\\n\\r\\n(binary data)\\r\\n')\n\n            if i != last:\n                part = part + self.boundary\n\n            self.logger.log(self.log_level, part, logging_context)", "response": "Log the content of the multipart request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nasserts that there are no missing values in the DataFrame.", "response": "def none_missing(df, columns=None):\n    \"\"\"\n    Asserts that there are no missing values (NaNs) in the DataFrame.\n\n    Parameters\n    ----------\n    df : DataFrame\n    columns : list\n      list of columns to restrict the check to\n\n    Returns\n    -------\n    df : DataFrame\n      same as the original\n    \"\"\"\n    if columns is None:\n        columns = df.columns\n    try:\n        assert not df[columns].isnull().any().any()\n    except AssertionError as e:\n        missing = df[columns].isnull()\n        msg = generic.bad_locations(missing)\n        e.args = msg\n        raise\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests that the DataFrame df is monotonic.", "response": "def is_monotonic(df, items=None, increasing=None, strict=False):\n    \"\"\"\n    Asserts that the DataFrame is monotonic.\n\n    Parameters\n    ==========\n\n    df : Series or DataFrame\n    items : dict\n        mapping columns to conditions (increasing, strict)\n    increasing : None or bool\n        None is either increasing or decreasing.\n    strict : whether the comparison should be strict\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    if items is None:\n        items = {k: (increasing, strict) for k in df}\n\n    for col, (increasing, strict) in items.items():\n        s = pd.Index(df[col])\n        if increasing:\n            good = getattr(s, 'is_monotonic_increasing')\n        elif increasing is None:\n            good = getattr(s, 'is_monotonic') | getattr(s, 'is_monotonic_decreasing')\n        else:\n            good = getattr(s, 'is_monotonic_decreasing')\n        if strict:\n            if increasing:\n                good = good & (s.to_series().diff().dropna() > 0).all()\n            elif increasing is None:\n                good = good & ((s.to_series().diff().dropna() > 0).all() |\n                               (s.to_series().diff().dropna() < 0).all())\n            else:\n                good = good & (s.to_series().diff().dropna() < 0).all()\n        if not good:\n            raise AssertionError\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasserting that the DataFrame is of a known shape.", "response": "def is_shape(df, shape):\n    \"\"\"\n    Asserts that the DataFrame is of a known shape.\n\n    Parameters\n    ==========\n\n    df : DataFrame\n    shape : tuple\n      (n_rows, n_columns). Use None or -1 if you don't care\n      about a dimension.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    try:\n        check = np.all(np.equal(df.shape, shape) | (np.equal(shape, [-1, -1]) |\n                                                    np.equal(shape, [None, None])))\n        assert check\n    except AssertionError as e:\n        msg = (\"Expected shape: {}\\n\"\n               \"\\t\\tActual shape:   {}\".format(shape, df.shape))\n        e.args = (msg,)\n        raise\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique(df, columns=None):\n    if columns is None:\n        columns = df.columns\n    for col in columns:\n        if not df[col].is_unique:\n            raise AssertionError(\"Column {!r} contains non-unique values\".format(col))\n    return df", "response": "Assert that the DataFrame only has unique values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unique_index(df):\n    try:\n        assert df.index.is_unique\n    except AssertionError as e:\n        e.args = df.index.get_duplicates()\n        raise\n    return df", "response": "Returns a DataFrame with the unique items in the index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassert that df is a subset of items", "response": "def within_set(df, items=None):\n    \"\"\"\n    Assert that df is a subset of items\n\n    Parameters\n    ==========\n    df : DataFrame\n    items : dict\n      mapping of columns (k) to array-like of values (v) that\n      ``df[k]`` is expected to be a subset of\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    for k, v in items.items():\n        if not df[k].isin(v).all():\n            bad = df.loc[~df[k].isin(v), k]\n            raise AssertionError('Not in set', bad)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef within_range(df, items=None):\n    for k, (lower, upper) in items.items():\n        if (lower > df[k]).any() or (upper < df[k]).any():\n            bad = (lower > df[k]) | (upper < df[k])\n            raise AssertionError(\"Outside range\", bad)\n    return df", "response": "Assert that a DataFrame is within a range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nasserting that every value in the DataFrame is within n standard deviations of its column s mean.", "response": "def within_n_std(df, n=3):\n    \"\"\"\n    Assert that every value is within ``n`` standard\n    deviations of its column's mean.\n\n    Parameters\n    ==========\n    df : DataFame\n    n : int\n      number of standard deviations from the mean\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n    means = df.mean()\n    stds = df.std()\n    inliers = (np.abs(df[means.index] - means) < n * stds)\n    if not np.all(inliers):\n        msg = generic.bad_locations(~inliers)\n        raise AssertionError(msg)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef has_dtypes(df, items):\n    dtypes = df.dtypes\n    for k, v in items.items():\n        if not dtypes[k] == v:\n            raise AssertionError(\"{} has the wrong dtype. Should be ({}), is ({})\".format(k, v,dtypes[k]))\n    return df", "response": "Assert that a DataFrame has the correct dtypes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts that a single - to - many relationship is preserved between two tables.", "response": "def one_to_many(df, unitcol, manycol):\n    \"\"\"\n    Assert that a many-to-one relationship is preserved between two\n    columns. For example, a retail store will have have distinct\n    departments, each with several employees. If each employee may\n    only work in a single department, then the relationship of the\n    department to the employees is one to many.\n\n    Parameters\n    ==========\n    df : DataFrame\n    unitcol : str\n        The column that encapulates the groups in ``manycol``.\n    manycol : str\n        The column that must remain unique in the distict pairs\n        between ``manycol`` and ``unitcol``\n\n    Returns\n    =======\n    df : DataFrame\n\n    \"\"\"\n    subset = df[[manycol, unitcol]].drop_duplicates()\n    for many in subset[manycol].unique():\n        if subset[subset[manycol] == many].shape[0] > 1:\n            msg = \"{} in {} has multiple values for {}\".format(many, manycol, unitcol)\n            raise AssertionError(msg)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_same_as(df, df_to_compare, **kwargs):\n    try:\n        tm.assert_frame_equal(df, df_to_compare, **kwargs)\n    except AssertionError as exc:\n        six.raise_from(AssertionError(\"DataFrames are not equal\"), exc)\n    return df", "response": "Assert that two pandas dataframes are the same as the given one."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies that all the entries in df are true.", "response": "def verify_all(df, check, *args, **kwargs):\n    \"\"\"\n    Verify that all the entries in ``check(df, *args, **kwargs)``\n    are true.\n    \"\"\"\n    result = check(df, *args, **kwargs)\n    try:\n        assert np.all(result)\n    except AssertionError as e:\n        msg = \"{} not true for all\".format(check.__name__)\n        e.args = (msg, df[~result])\n        raise\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that any of the entries in df are true.", "response": "def verify_any(df, check, *args, **kwargs):\n    \"\"\"\n    Verify that any of the entries in ``check(df, *args, **kwargs)``\n    is true\n    \"\"\"\n    result = check(df, *args, **kwargs)\n    try:\n        assert np.any(result)\n    except AssertionError as e:\n        msg = '{} not true for any'.format(check.__name__)\n        e.args = (msg, df)\n        raise\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the input block for INPUT token.", "response": "def process_input(self, data, input_prompt, lineno):\n        \"\"\"\n        Process data block for INPUT token.\n\n        \"\"\"\n        decorator, input, rest = data\n        image_file = None\n        image_directive = None\n\n        is_verbatim = decorator=='@verbatim' or self.is_verbatim\n        is_doctest = (decorator is not None and \\\n                     decorator.startswith('@doctest')) or self.is_doctest\n        is_suppress = decorator=='@suppress' or self.is_suppress\n        is_okexcept = decorator=='@okexcept' or self.is_okexcept\n        is_okwarning = decorator=='@okwarning' or self.is_okwarning\n        is_savefig = decorator is not None and \\\n                     decorator.startswith('@savefig')\n\n        # set the encodings to be used by DecodingStringIO\n        # to convert the execution output into unicode if\n        # needed. this attrib is set by IpythonDirective.run()\n        # based on the specified block options, defaulting to ['ut\n        self.cout.set_encodings(self.output_encoding)\n\n        input_lines = input.split('\\n')\n\n        if len(input_lines) > 1:\n           if input_lines[-1] != \"\":\n               input_lines.append('') # make sure there's a blank line\n                                       # so splitter buffer gets reset\n\n        continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))\n\n        if is_savefig:\n            image_file, image_directive = self.process_image(decorator)\n\n        ret = []\n        is_semicolon = False\n\n        # Hold the execution count, if requested to do so.\n        if is_suppress and self.hold_count:\n            store_history = False\n        else:\n            store_history = True\n\n        # Note: catch_warnings is not thread safe\n        with warnings.catch_warnings(record=True) as ws:\n            for i, line in enumerate(input_lines):\n                if line.endswith(';'):\n                    is_semicolon = True\n\n                if i == 0:\n                    # process the first input line\n                    if is_verbatim:\n                        self.process_input_line('')\n                        self.IP.execution_count += 1 # increment it anyway\n                    else:\n                        # only submit the line in non-verbatim mode\n                        self.process_input_line(line, store_history=store_history)\n                    formatted_line = '%s %s'%(input_prompt, line)\n                else:\n                    # process a continuation line\n                    if not is_verbatim:\n                        self.process_input_line(line, store_history=store_history)\n\n                    formatted_line = '%s %s'%(continuation, line)\n\n                if not is_suppress:\n                    ret.append(formatted_line)\n\n        if not is_suppress and len(rest.strip()) and is_verbatim:\n            # the \"rest\" is the standard output of the\n            # input, which needs to be added in\n            # verbatim mode\n            ret.append(rest)\n\n        self.cout.seek(0)\n        output = self.cout.read()\n        if not is_suppress and not is_semicolon:\n            ret.append(output)\n        elif is_semicolon: # get spacing right\n            ret.append('')\n\n        # context information\n        filename = self.state.document.current_source\n        lineno = self.state.document.current_line\n\n        # output any exceptions raised during execution to stdout\n        # unless :okexcept: has been specified.\n        if not is_okexcept and \"Traceback\" in output:\n            s =  \"\\nException in %s at block ending on line %s\\n\" % (filename, lineno)\n            s += \"Specify :okexcept: as an option in the ipython:: block to suppress this message\\n\"\n            sys.stdout.write('\\n\\n>>>' + ('-' * 73))\n            sys.stdout.write(s)\n            sys.stdout.write(output)\n            sys.stdout.write('<<<' + ('-' * 73) + '\\n\\n')\n\n        # output any warning raised during execution to stdout\n        # unless :okwarning: has been specified.\n        if not is_okwarning:\n            for w in ws:\n                s =  \"\\nWarning in %s at block ending on line %s\\n\" % (filename, lineno)\n                s += \"Specify :okwarning: as an option in the ipython:: block to suppress this message\\n\"\n                sys.stdout.write('\\n\\n>>>' + ('-' * 73))\n                sys.stdout.write(s)\n                sys.stdout.write('-' * 76 + '\\n')\n                s=warnings.formatwarning(w.message, w.category,\n                                         w.filename, w.lineno, w.line)\n                sys.stdout.write(s)\n                sys.stdout.write('<<<' + ('-' * 73) + '\\n')\n\n        self.cout.truncate(0)\n        return (ret, input_lines, output, is_doctest, decorator, image_file,\n                    image_directive)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_output(self, data, output_prompt,\n                       input_lines, output, is_doctest, decorator, image_file):\n        \"\"\"\n        Process data block for OUTPUT token.\n\n        \"\"\"\n        TAB = ' ' * 4\n\n        if is_doctest and output is not None:\n\n            found = output\n            found = found.strip()\n            submitted = data.strip()\n\n            if self.directive is None:\n                source = 'Unavailable'\n                content = 'Unavailable'\n            else:\n                source = self.directive.state.document.current_source\n                content = self.directive.content\n                # Add tabs and join into a single string.\n                content = '\\n'.join([TAB + line for line in content])\n\n            # Make sure the output contains the output prompt.\n            ind = found.find(output_prompt)\n            if ind < 0:\n                e = ('output does not contain output prompt\\n\\n'\n                     'Document source: {0}\\n\\n'\n                     'Raw content: \\n{1}\\n\\n'\n                     'Input line(s):\\n{TAB}{2}\\n\\n'\n                     'Output line(s):\\n{TAB}{3}\\n\\n')\n                e = e.format(source, content, '\\n'.join(input_lines),\n                             repr(found), TAB=TAB)\n                raise RuntimeError(e)\n            found = found[len(output_prompt):].strip()\n\n            # Handle the actual doctest comparison.\n            if decorator.strip() == '@doctest':\n                # Standard doctest\n                if found != submitted:\n                    e = ('doctest failure\\n\\n'\n                         'Document source: {0}\\n\\n'\n                         'Raw content: \\n{1}\\n\\n'\n                         'On input line(s):\\n{TAB}{2}\\n\\n'\n                         'we found output:\\n{TAB}{3}\\n\\n'\n                         'instead of the expected:\\n{TAB}{4}\\n\\n')\n                    e = e.format(source, content, '\\n'.join(input_lines),\n                                 repr(found), repr(submitted), TAB=TAB)\n                    raise RuntimeError(e)\n            else:\n                self.custom_doctest(decorator, input_lines, found, submitted)", "response": "Process the output block for OUTPUT token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a block from the block parser and return a list of processed lines", "response": "def process_block(self, block):\n        \"\"\"\n        process block from the block_parser and return a list of processed lines\n        \"\"\"\n        ret = []\n        output = None\n        input_lines = None\n        lineno = self.IP.execution_count\n\n        input_prompt = self.promptin % lineno\n        output_prompt = self.promptout % lineno\n        image_file = None\n        image_directive = None\n\n        for token, data in block:\n            if token == COMMENT:\n                out_data = self.process_comment(data)\n            elif token == INPUT:\n                (out_data, input_lines, output, is_doctest, decorator,\n                    image_file, image_directive) = \\\n                          self.process_input(data, input_prompt, lineno)\n            elif token == OUTPUT:\n                out_data = \\\n                    self.process_output(data, output_prompt,\n                                        input_lines, output, is_doctest,\n                                        decorator, image_file)\n            if out_data:\n                ret.extend(out_data)\n\n        # save the image files\n        if image_file is not None:\n            self.save_image(image_file)\n\n        return ret, image_directive"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_pyplot(self):\n        # We are here if the @figure pseudo decorator was used. Thus, it's\n        # possible that we could be here even if python_mplbackend were set to\n        # `None`. That's also strange and perhaps worthy of raising an\n        # exception, but for now, we just set the backend to 'agg'.\n\n        if not self._pyplot_imported:\n            if 'matplotlib.backends' not in sys.modules:\n                # Then ipython_matplotlib was set to None but there was a\n                # call to the @figure decorator (and ipython_execlines did\n                # not set a backend).\n                #raise Exception(\"No backend was set, but @figure was used!\")\n                import matplotlib\n                matplotlib.use('agg')\n\n            # Always import pyplot into embedded shell.\n            self.process_input_line('import matplotlib.pyplot as plt',\n                                    store_history=False)\n            self._pyplot_imported = True", "response": "Ensures that matplotlib is imported into the embedded IPython shell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a JSON object from a URL.", "response": "def _fetch_remote_json(service_url, params=None, use_http_post=False):\n    \"\"\"Retrieves a JSON object from a URL.\"\"\"\n    if not params:\n        params = {}\n\n    request_url, response = _fetch_remote(service_url, params, use_http_post)\n    if six.PY3:\n        str_response = response.read().decode('utf-8')\n        return (request_url, json.loads(str_response, parse_float=Decimal))\n    return (request_url, json.load(response, parse_float=Decimal))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a file from a URL.", "response": "def _fetch_remote_file(service_url, params=None, use_http_post=False):\n    \"\"\"Retrieves a file from a URL.\n\n    Returns a tuple (mimetype, filename, data)\n    \"\"\"\n    if not params:\n        params = {}\n\n    request_url, response = _fetch_remote(service_url, params, use_http_post)\n    dummy, params = cgi.parse_header(\n            response.headers.get('Content-Disposition', ''))\n    fn = params['filename']\n\n    return (response.headers.get('content-type'),\n            fn, response.read(), response.geturl())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a human - readable location to lat - lng.", "response": "def geocode_location(location, sensor=False, api_key=None):\n    \"\"\"Converts a human-readable location to lat-lng.\n\n    Returns a dict with lat and lng keys.\n\n    keyword arguments:\n    location -- A human-readable location, e.g 'London, England'\n    sensor   -- Boolean flag denoting if the location came from a device using\n                its' location sensor (default False)\n    api_key  -- A valid Google Places API key. \n\n    raises:\n    GooglePlacesError -- if the geocoder fails to find a location.\n    \"\"\"\n    params = {'address': location, 'sensor': str(sensor).lower()}\n    if api_key is not None:\n        params['key'] = api_key\n    url, geo_response = _fetch_remote_json(\n            GooglePlaces.GEOCODE_API_URL, params)\n    _validate_response(url, geo_response)\n    if geo_response['status'] == GooglePlaces.RESPONSE_STATUS_ZERO_RESULTS:\n        error_detail = ('Lat/Lng for location \\'%s\\' can\\'t be determined.' %\n                        location)\n        raise GooglePlacesError(error_detail)\n    return geo_response['results'][0]['geometry']['location']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_place_details(place_id, api_key, sensor=False,\n                       language=lang.ENGLISH):\n    \"\"\"Gets a detailed place response.\n\n    keyword arguments:\n    place_id -- The unique identifier for the required place.\n    \"\"\"\n    url, detail_response = _fetch_remote_json(GooglePlaces.DETAIL_API_URL,\n                                              {'placeid': place_id,\n                                               'sensor': str(sensor).lower(),\n                                               'key': api_key,\n                                               'language': language})\n    _validate_response(url, detail_response)\n    return detail_response['result']", "response": "Gets a detailed place response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_place_photo(photoreference, api_key, maxheight=None, maxwidth=None,\n                       sensor=False):\n    \"\"\"Gets a place's photo by reference.\n    See detailed documentation at https://developers.google.com/places/documentation/photos\n\n    Arguments:\n    photoreference -- The unique Google reference for the required photo.\n\n    Keyword arguments:\n    maxheight -- The maximum desired photo height in pixels\n    maxwidth -- The maximum desired photo width in pixels\n\n    You must specify one of this keyword arguments. Acceptable value is an\n    integer between 1 and 1600.\n    \"\"\"\n\n    params = {'photoreference': photoreference,\n              'sensor': str(sensor).lower(),\n              'key': api_key}\n\n    if maxheight:\n        params['maxheight'] = maxheight\n\n    if maxwidth:\n        params['maxwidth'] = maxwidth\n\n    return _fetch_remote_file(GooglePlaces.PHOTO_API_URL, params)", "response": "Gets a place s photo by reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that the response from Google was successful.", "response": "def _validate_response(url, response):\n    \"\"\"Validates that the response from Google was successful.\"\"\"\n    if response['status'] not in [GooglePlaces.RESPONSE_STATUS_OK,\n                                  GooglePlaces.RESPONSE_STATUS_ZERO_RESULTS]:\n        error_detail = ('Request to URL %s failed with response code: %s' %\n                        (url, response['status']))\n        raise GooglePlacesError(error_detail)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a nearby search using the Google Places API.", "response": "def nearby_search(self, language=lang.ENGLISH, keyword=None, location=None,\n               lat_lng=None, name=None, radius=3200, rankby=ranking.PROMINENCE,\n               sensor=False, type=None, types=[], pagetoken=None):\n        \"\"\"Perform a nearby search using the Google Places API.\n\n        One of either location, lat_lng or pagetoken are required, the rest of \n        the keyword arguments are optional.\n\n        keyword arguments:\n        keyword  -- A term to be matched against all available fields, including\n                    but not limited to name, type, and address (default None)\n        location -- A human readable location, e.g 'London, England'\n                    (default None)\n        language -- The language code, indicating in which language the\n                    results should be returned, if possible. (default lang.ENGLISH)\n        lat_lng  -- A dict containing the following keys: lat, lng\n                    (default None)\n        name     -- A term to be matched against the names of the Places.\n                    Results will be restricted to those containing the passed\n                    name value. (default None)\n        radius   -- The radius (in meters) around the location/lat_lng to\n                    restrict the search to. The maximum is 50000 meters.\n                    (default 3200)\n        rankby   -- Specifies the order in which results are listed :\n                    ranking.PROMINENCE (default) or ranking.DISTANCE\n                    (imply no radius argument).\n        sensor   -- Indicates whether or not the Place request came from a\n                    device using a location sensor (default False).\n        type     -- Optional type param used to indicate place category.\n        types    -- An optional list of types, restricting the results to\n                    Places (default []). If there is only one item the request\n                    will be send as type param.\n        pagetoken-- Optional parameter to force the search result to return the next\n                    20 results from a previously run search. Setting this parameter \n                    will execute a search with the same parameters used previously. \n                    (default None)\n        \"\"\"\n        if location is None and lat_lng is None and pagetoken is None:\n            raise ValueError('One of location, lat_lng or pagetoken must be passed in.')\n        if rankby == 'distance':\n            # As per API docs rankby == distance:\n            #  One or more of keyword, name, or types is required.\n            if keyword is None and types == [] and name is None:\n                raise ValueError('When rankby = googleplaces.ranking.DISTANCE, ' +\n                                 'name, keyword or types kwargs ' +\n                                 'must be specified.')\n        self._sensor = sensor\n        radius = (radius if radius <= GooglePlaces.MAXIMUM_SEARCH_RADIUS\n                  else GooglePlaces.MAXIMUM_SEARCH_RADIUS)\n        lat_lng_str = self._generate_lat_lng_string(lat_lng, location)\n        self._request_params = {'location': lat_lng_str}\n        if rankby == 'prominence':\n            self._request_params['radius'] = radius\n        else:\n            self._request_params['rankby'] = rankby\n        if type:\n            self._request_params['type'] = type\n        elif types:\n            if len(types) == 1:\n                self._request_params['type'] = types[0]\n            elif len(types) > 1:\n                self._request_params['types'] = '|'.join(types)\n        if keyword is not None:\n            self._request_params['keyword'] = keyword\n        if name is not None:\n            self._request_params['name'] = name\n        if pagetoken is not None:\n            self._request_params['pagetoken'] = pagetoken\n        if language is not None:\n            self._request_params['language'] = language\n        self._add_required_param_keys()\n        url, places_response = _fetch_remote_json(\n                GooglePlaces.NEARBY_SEARCH_API_URL, self._request_params)\n        _validate_response(url, places_response)\n        return GooglePlacesSearchResult(self, places_response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a text search using the Google Places API.", "response": "def text_search(self, query=None, language=lang.ENGLISH, lat_lng=None,\n                    radius=3200, type=None, types=[], location=None, pagetoken=None):\n        \"\"\"Perform a text search using the Google Places API.\n\n        Only the one of the query or pagetoken kwargs are required, the rest of the \n        keyword arguments are optional.\n\n        keyword arguments:\n        lat_lng  -- A dict containing the following keys: lat, lng\n                    (default None)\n        location -- A human readable location, e.g 'London, England'\n                    (default None)\n        pagetoken-- Optional parameter to force the search result to return the next\n                    20 results from a previously run search. Setting this parameter \n                    will execute a search with the same parameters used previously. \n                    (default None)\n        radius   -- The radius (in meters) around the location/lat_lng to\n                    restrict the search to. The maximum is 50000 meters.\n                    (default 3200)\n        query    -- The text string on which to search, for example:\n                    \"Restaurant in New York\".\n        type     -- Optional type param used to indicate place category.\n        types    -- An optional list of types, restricting the results to\n                    Places (default []). If there is only one item the request\n                    will be send as type param.\n        \"\"\"\n        self._request_params = {'query': query}\n        if lat_lng is not None or location is not None:\n            lat_lng_str = self._generate_lat_lng_string(lat_lng, location)\n            self._request_params['location'] = lat_lng_str\n        self._request_params['radius'] = radius\n        if type:\n            self._request_params['type'] = type\n        elif types:\n            if len(types) == 1:\n                self._request_params['type'] = types[0]\n            elif len(types) > 1:\n                self._request_params['types'] = '|'.join(types)\n        if language is not None:\n            self._request_params['language'] = language\n        if pagetoken is not None:\n            self._request_params['pagetoken'] = pagetoken\n        self._add_required_param_keys()\n        url, places_response = _fetch_remote_json(\n                GooglePlaces.TEXT_SEARCH_API_URL, self._request_params)\n        _validate_response(url, places_response)\n        return GooglePlacesSearchResult(self, places_response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming an autocomplete search using the Google Places API.", "response": "def autocomplete(self, input, lat_lng=None, location=None, radius=3200,\n                     language=lang.ENGLISH, types=None, components=[]):\n        \"\"\"\n        Perform an autocomplete search using the Google Places API.\n\n        Only the input kwarg is required, the rest of the keyword arguments\n        are optional.\n\n        keyword arguments:\n        input    -- The text string on which to search, for example:\n                    \"Hattie B's\".\n        lat_lng  -- A dict containing the following keys: lat, lng\n                    (default None)\n        location -- A human readable location, e.g 'London, England'\n                    (default None)\n        radius   -- The radius (in meters) around the location to which the\n                    search is to be restricted. The maximum is 50000 meters.\n                    (default 3200)\n        language -- The language code, indicating in which language the\n                    results should be returned, if possible. (default lang.ENGLISH)\n        types    -- A type to search against. See `types.py` \"autocomplete types\"\n                    for complete list\n                    https://developers.google.com/places/documentation/autocomplete#place_types.\n        components -- An optional grouping of places to which you would\n                    like to restrict your results. An array containing one or\n                    more tuples of:\n                    * country: matches a country name or a two letter ISO 3166-1 country code.\n                    eg: [('country','US')]\n        \"\"\"\n        self._request_params = {'input': input}\n        if lat_lng is not None or location is not None:\n            lat_lng_str = self._generate_lat_lng_string(lat_lng, location)\n            self._request_params['location'] = lat_lng_str\n        self._request_params['radius'] = radius\n        if types:\n            self._request_params['types'] = types\n        if len(components) > 0:\n            self._request_params['components'] = '|'.join(['{}:{}'.format(\n                                                     c[0],c[1]) for c in components])\n        if language is not None:\n            self._request_params['language'] = language\n        self._add_required_param_keys()\n        url, places_response = _fetch_remote_json(\n                GooglePlaces.AUTOCOMPLETE_API_URL, self._request_params)\n        _validate_response(url, places_response)\n        return GoogleAutocompleteSearchResult(self, places_response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef radar_search(self, sensor=False, keyword=None, name=None,\n                     language=lang.ENGLISH, lat_lng=None, opennow=False,\n                     radius=3200, type=None, types=[], location=None):\n        \"\"\"Perform a radar search using the Google Places API.\n\n        One of lat_lng or location are required, the rest of the keyword\n        arguments are optional.\n\n        keyword arguments:\n        keyword  -- A term to be matched against all available fields, including\n                    but not limited to name, type, and address (default None)\n        name     -- A term to be matched against the names of Places. Results will\n                    be restricted to those containing the passed name value.\n        language -- The language code, indicating in which language the\n                    results should be returned, if possible. (default lang.ENGLISH)\n        lat_lng  -- A dict containing the following keys: lat, lng\n                    (default None)\n        location -- A human readable location, e.g 'London, England'\n                    (default None)\n        radius   -- The radius (in meters) around the location/lat_lng to\n                    restrict the search to. The maximum is 50000 meters.\n                    (default 3200)\n        opennow  -- Returns only those Places that are open for business at the time\n                    the query is sent. (default False)\n        sensor   -- Indicates whether or not the Place request came from a\n                    device using a location sensor (default False).\n        type     -- Optional type param used to indicate place category\n        types    -- An optional list of types, restricting the results to\n                    Places (default []). If there is only one item the request\n                    will be send as type param\n        \"\"\"\n        if keyword is None and name is None and len(types) is 0:\n            raise ValueError('One of keyword, name or types must be supplied.')\n        if location is None and lat_lng is None:\n            raise ValueError('One of location or lat_lng must be passed in.')\n        try:\n            radius = int(radius)\n        except:\n            raise ValueError('radius must be passed supplied as an integer.')\n        if sensor not in [True, False]:\n            raise ValueError('sensor must be passed in as a boolean value.')\n\n        self._request_params = {'radius': radius}\n        self._sensor = sensor\n        self._request_params['location'] = self._generate_lat_lng_string(\n                lat_lng, location)\n        if keyword is not None:\n            self._request_params['keyword'] = keyword\n        if name is not None:\n            self._request_params['name'] = name\n        if type:\n            self._request_params['type'] = type\n        elif types:\n            if len(types) == 1:\n                self._request_params['type'] = types[0]\n            elif len(types) > 1:\n                self._request_params['types'] = '|'.join(types)\n        if language is not None:\n            self._request_params['language'] = language\n        if opennow is True:\n            self._request_params['opennow'] = 'true'\n        self._add_required_param_keys()\n        url, places_response = _fetch_remote_json(\n                GooglePlaces.RADAR_SEARCH_API_URL, self._request_params)\n        _validate_response(url, places_response)\n        return GooglePlacesSearchResult(self, places_response)", "response": "Perform a radar search using the Google Places API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkin(self, place_id, sensor=False):\n        data = {'placeid': place_id}\n        url, checkin_response = _fetch_remote_json(\n                GooglePlaces.CHECKIN_API_URL % (str(sensor).lower(),\n                        self.api_key), json.dumps(data), use_http_post=True)\n        _validate_response(url, checkin_response)", "response": "Checks in a user to a place."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a detailed place object.", "response": "def get_place(self, place_id, sensor=False, language=lang.ENGLISH):\n        \"\"\"Gets a detailed place object.\n\n        keyword arguments:\n        place_id -- The unique Google identifier for the required place.\n        sensor    -- Boolean flag denoting if the location came from a\n                     device using its' location sensor (default False).\n        language -- The language code, indicating in which language the\n                    results should be returned, if possible. (default lang.ENGLISH)\n        \"\"\"\n        place_details = _get_place_details(place_id,\n                self.api_key, sensor, language=language)\n        return Place(self, place_details)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_place(self, **kwargs):\n        required_kwargs = {'name': [str], 'lat_lng': [dict],\n                           'accuracy': [int], 'types': [str, list]}\n        request_params = {}\n        for key in required_kwargs:\n            if key not in kwargs or kwargs[key] is None:\n                raise ValueError('The %s argument is required.' % key)\n            expected_types = required_kwargs[key]\n            type_is_valid = False\n            for expected_type in expected_types:\n                if isinstance(kwargs[key], expected_type):\n                    type_is_valid = True\n                    break\n            if not type_is_valid:\n                raise ValueError('Invalid value for %s' % key)\n            if key is not 'lat_lng':\n                request_params[key] = kwargs[key]\n\n        if len(kwargs['name']) > 255:\n            raise ValueError('The place name must not exceed 255 characters ' +\n                             'in length.')\n        try:\n            kwargs['lat_lng']['lat']\n            kwargs['lat_lng']['lng']\n            request_params['location'] = kwargs['lat_lng']\n        except KeyError:\n            raise ValueError('Invalid keys for lat_lng.')\n\n        request_params['language'] = (kwargs.get('language')\n                if kwargs.get('language') is not None else\n                lang.ENGLISH)\n\n        sensor = (kwargs.get('sensor')\n                       if kwargs.get('sensor') is not None else\n                       False)\n\n        # At some point Google might support multiple types, so this supports\n        # strings and lists.\n        if isinstance(kwargs['types'], str):\n            request_params['types'] = [kwargs['types']]\n        else:\n            request_params['types'] = kwargs['types']\n        url, add_response = _fetch_remote_json(\n                GooglePlaces.ADD_API_URL % (str(sensor).lower(),\n                self.api_key), json.dumps(request_params), use_http_post=True)\n        _validate_response(url, add_response)\n        return {'place_id': add_response['place_id'],\n                'id': add_response['id']}", "response": "Adds a new place to the Google Places database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a place from the Google Places database.", "response": "def delete_place(self, place_id, sensor=False):\n        \"\"\"Deletes a place from the Google Places database.\n\n        keyword arguments:\n        place_id   -- The textual identifier that uniquely identifies this\n                      Place, returned from a Place Search request.\n        sensor     -- Boolean flag denoting if the location came from a device\n                      using its location sensor (default False).\n        \"\"\"\n\n        request_params = {'place_id': place_id}\n        url, delete_response = _fetch_remote_json(\n                GooglePlaces.DELETE_API_URL % (str(sensor).lower(),\n                self.api_key), json.dumps(request_params), use_http_post=True)\n        _validate_response(url, delete_response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of feature types describing the given result.", "response": "def types(self):\n        \"\"\"\n        Returns a list of feature types describing the given result.\n        \"\"\"\n        if self._types == '' and self.details != None and 'types' in self.details:\n            self._icon = self.details['types']\n        return self._types"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_details(self, language=None):\n        if self._place is None:\n            if language is None:\n                try:\n                    language = self._query_instance._request_params['language']\n                except KeyError:\n                    language = lang.ENGLISH\n            place = _get_place_details(\n                    self.place_id, self._query_instance.api_key,\n                    self._query_instance.sensor, language=language)\n            self._place = Place(self._query_instance, place)", "response": "Retrieves full information on the place matching the place_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the URL of a recommended icon for display.", "response": "def icon(self):\n        \"\"\"Returns the URL of a recommended icon for display.\"\"\"\n        if self._icon == '' and self.details != None and 'icon' in self.details:\n            self._icon = self.details['icon']\n        return self._icon"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef name(self):\n        if self._name == '' and self.details != None and 'name' in self.details:\n            self._name = self.details['name']\n        return self._name", "response": "Returns the human - readable name of the place."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vicinity(self):\n        if self._vicinity == '' and self.details != None and 'vicinity' in self.details:\n            self._vicinity = self.details['vicinity']\n        return self._vicinity", "response": "Returns a feature name of a nearby location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rating(self):\n        if self._rating == '' and self.details != None and 'rating' in self.details:\n            self._rating = self.details['rating']\n        return self._rating", "response": "Returns the Place s rating from 0. 0 to 5. 0 based on user reviews."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking in an anonymous user in.", "response": "def checkin(self):\n        \"\"\"Checks in an anonymous user in.\"\"\"\n        self._query_instance.checkin(self.place_id,\n                                     self._query_instance.sensor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves full information on the place matching the place_id.", "response": "def get_details(self, language=None):\n        \"\"\"Retrieves full information on the place matching the place_id.\n\n        Further attributes will be made available on the instance once this\n        method has been invoked.\n\n        keyword arguments:\n        language -- The language code, indicating in which language the\n                    results should be returned, if possible. This value defaults\n                    to the language that was used to generate the\n                    GooglePlacesSearchResult instance.\n        \"\"\"\n        if self._details is None:\n            if language is None:\n                try:\n                    language = self._query_instance._request_params['language']\n                except KeyError:\n                    language = lang.ENGLISH\n            self._details = _get_place_details(\n                    self.place_id, self._query_instance.api_key,\n                    self._query_instance.sensor, language=language)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, maxheight=None, maxwidth=None, sensor=False):\n        if not maxheight and not maxwidth:\n            raise GooglePlacesError('You must specify maxheight or maxwidth!')\n\n        result = _get_place_photo(self.photo_reference,\n                                  self._query_instance.api_key,\n                                  maxheight=maxheight, maxwidth=maxwidth,\n                                  sensor=sensor)\n\n        self.mimetype, self.filename, self.data, self.url = result", "response": "Fetch photo from API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_png(data, size, level=6, output=None):\n    # type: (bytes, Tuple[int, int], int, Optional[str]) -> Optional[bytes]\n    \"\"\"\n    Dump data to a PNG file.  If `output` is `None`, create no file but return\n    the whole PNG data.\n\n    :param bytes data: RGBRGB...RGB data.\n    :param tuple size: The (width, height) pair.\n    :param int level: PNG compression level.\n    :param str output: Output file name.\n    \"\"\"\n\n    width, height = size\n    line = width * 3\n    png_filter = struct.pack(\">B\", 0)\n    scanlines = b\"\".join(\n        [png_filter + data[y * line : y * line + line] for y in range(height)]\n    )\n\n    magic = struct.pack(\">8B\", 137, 80, 78, 71, 13, 10, 26, 10)\n\n    # Header: size, marker, data, CRC32\n    ihdr = [b\"\", b\"IHDR\", b\"\", b\"\"]\n    ihdr[2] = struct.pack(\">2I5B\", width, height, 8, 2, 0, 0, 0)\n    ihdr[3] = struct.pack(\">I\", zlib.crc32(b\"\".join(ihdr[1:3])) & 0xFFFFFFFF)\n    ihdr[0] = struct.pack(\">I\", len(ihdr[2]))\n\n    # Data: size, marker, data, CRC32\n    idat = [b\"\", b\"IDAT\", zlib.compress(scanlines, level), b\"\"]\n    idat[3] = struct.pack(\">I\", zlib.crc32(b\"\".join(idat[1:3])) & 0xFFFFFFFF)\n    idat[0] = struct.pack(\">I\", len(idat[2]))\n\n    # Footer: size, marker, None, CRC32\n    iend = [b\"\", b\"IEND\", b\"\", b\"\"]\n    iend[3] = struct.pack(\">I\", zlib.crc32(iend[1]) & 0xFFFFFFFF)\n    iend[0] = struct.pack(\">I\", len(iend[2]))\n\n    if not output:\n        # Returns raw bytes of the whole PNG data\n        return magic + b\"\".join(ihdr + idat + iend)\n\n    with open(output, \"wb\") as fileh:\n        fileh.write(magic)\n        fileh.write(b\"\".join(ihdr))\n        fileh.write(b\"\".join(idat))\n        fileh.write(b\"\".join(iend))\n\n    return None", "response": "Dump data to a PNG file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets all ctypes functions and attach them to attributes.", "response": "def _set_cfunctions(self):\n        # type: () -> None\n        \"\"\" Set all ctypes functions and attach them to attributes. \"\"\"\n\n        def cfactory(func, argtypes, restype):\n            # type: (str, List[Any], Any) -> None\n            \"\"\" Factorize ctypes creations. \"\"\"\n            self._cfactory(\n                attr=self.core, func=func, argtypes=argtypes, restype=restype\n            )\n\n        uint32 = ctypes.c_uint32\n        void = ctypes.c_void_p\n        size_t = ctypes.c_size_t\n        pointer = ctypes.POINTER\n\n        cfactory(\n            func=\"CGGetActiveDisplayList\",\n            argtypes=[uint32, pointer(uint32), pointer(uint32)],\n            restype=ctypes.c_int32,\n        )\n        cfactory(func=\"CGDisplayBounds\", argtypes=[uint32], restype=CGRect)\n        cfactory(func=\"CGRectStandardize\", argtypes=[CGRect], restype=CGRect)\n        cfactory(func=\"CGRectUnion\", argtypes=[CGRect, CGRect], restype=CGRect)\n        cfactory(func=\"CGDisplayRotation\", argtypes=[uint32], restype=ctypes.c_float)\n        cfactory(\n            func=\"CGWindowListCreateImage\",\n            argtypes=[CGRect, uint32, uint32, uint32],\n            restype=void,\n        )\n        cfactory(func=\"CGImageGetWidth\", argtypes=[void], restype=size_t)\n        cfactory(func=\"CGImageGetHeight\", argtypes=[void], restype=size_t)\n        cfactory(func=\"CGImageGetDataProvider\", argtypes=[void], restype=void)\n        cfactory(func=\"CGDataProviderCopyData\", argtypes=[void], restype=void)\n        cfactory(func=\"CFDataGetBytePtr\", argtypes=[void], restype=void)\n        cfactory(func=\"CFDataGetLength\", argtypes=[void], restype=ctypes.c_uint64)\n        cfactory(func=\"CGImageGetBytesPerRow\", argtypes=[void], restype=size_t)\n        cfactory(func=\"CGImageGetBitsPerPixel\", argtypes=[void], restype=size_t)\n        cfactory(func=\"CGDataProviderRelease\", argtypes=[void], restype=void)\n        cfactory(func=\"CFRelease\", argtypes=[void], restype=void)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets positions of monitors.", "response": "def monitors(self):\n        # type: () -> Monitors\n        \"\"\" Get positions of monitors (see parent class). \"\"\"\n\n        if not self._monitors:\n            int_ = int\n            core = self.core\n\n            # All monitors\n            # We need to update the value with every single monitor found\n            # using CGRectUnion.  Else we will end with infinite values.\n            all_monitors = CGRect()\n            self._monitors.append({})\n\n            # Each monitors\n            display_count = ctypes.c_uint32(0)\n            active_displays = (ctypes.c_uint32 * self.max_displays)()\n            core.CGGetActiveDisplayList(\n                self.max_displays, active_displays, ctypes.byref(display_count)\n            )\n            rotations = {0.0: \"normal\", 90.0: \"right\", -90.0: \"left\"}\n            for idx in range(display_count.value):\n                display = active_displays[idx]\n                rect = core.CGDisplayBounds(display)\n                rect = core.CGRectStandardize(rect)\n                width, height = rect.size.width, rect.size.height\n                rot = core.CGDisplayRotation(display)\n                if rotations[rot] in [\"left\", \"right\"]:\n                    width, height = height, width\n                self._monitors.append(\n                    {\n                        \"left\": int_(rect.origin.x),\n                        \"top\": int_(rect.origin.y),\n                        \"width\": int_(width),\n                        \"height\": int_(height),\n                    }\n                )\n\n                # Update AiO monitor's values\n                all_monitors = core.CGRectUnion(all_monitors, rect)\n\n            # Set the AiO monitor's values\n            self._monitors[0] = {\n                \"left\": int_(all_monitors.origin.x),\n                \"top\": int_(all_monitors.origin.y),\n                \"width\": int_(all_monitors.size.width),\n                \"height\": int_(all_monitors.size.height),\n            }\n\n        return self._monitors"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngrabs the screen shot for the specified monitor.", "response": "def grab(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        \"\"\"\n        See :meth:`MSSMixin.grab <mss.base.MSSMixin.grab>` for full details.\n        \"\"\"\n\n        # pylint: disable=too-many-locals\n\n        # Convert PIL bbox style\n        if isinstance(monitor, tuple):\n            monitor = {\n                \"left\": monitor[0],\n                \"top\": monitor[1],\n                \"width\": monitor[2] - monitor[0],\n                \"height\": monitor[3] - monitor[1],\n            }\n\n        core = self.core\n        rect = CGRect(\n            (monitor[\"left\"], monitor[\"top\"]), (monitor[\"width\"], monitor[\"height\"])\n        )\n\n        image_ref = core.CGWindowListCreateImage(rect, 1, 0, 0)\n        if not image_ref:\n            raise ScreenShotError(\"CoreGraphics.CGWindowListCreateImage() failed.\")\n\n        width = int(core.CGImageGetWidth(image_ref))\n        height = int(core.CGImageGetHeight(image_ref))\n        prov = copy_data = None\n        try:\n            prov = core.CGImageGetDataProvider(image_ref)\n            copy_data = core.CGDataProviderCopyData(prov)\n            data_ref = core.CFDataGetBytePtr(copy_data)\n            buf_len = core.CFDataGetLength(copy_data)\n            raw = ctypes.cast(data_ref, ctypes.POINTER(ctypes.c_ubyte * buf_len))\n            data = bytearray(raw.contents)\n\n            # Remove padding per row\n            bytes_per_row = int(core.CGImageGetBytesPerRow(image_ref))\n            bytes_per_pixel = int(core.CGImageGetBitsPerPixel(image_ref))\n            bytes_per_pixel = (bytes_per_pixel + 7) // 8\n\n            if bytes_per_pixel * width != bytes_per_row:\n                cropped = bytearray()\n                for row in range(height):\n                    start = row * bytes_per_row\n                    end = start + width * bytes_per_pixel\n                    cropped.extend(data[start:end])\n                data = cropped\n        finally:\n            if prov:\n                core.CGDataProviderRelease(prov)\n            if copy_data:\n                core.CFRelease(copy_data)\n\n        return self.cls_image(data, monitor, size=Size(width, height))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pixels(self):\n        # type: () -> Pixels\n        \"\"\"\n        :return list: RGB tuples.\n        \"\"\"\n\n        if not self.__pixels:\n            rgb_tuples = zip(\n                self.raw[2::4], self.raw[1::4], self.raw[0::4]\n            )  # type: Iterator[Pixel]\n            self.__pixels = list(zip(*[iter(rgb_tuples)] * self.width))  # type: ignore\n\n        return self.__pixels", "response": "Returns a list of RGB tuples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rgb(self):\n        # type: () -> bytes\n        \"\"\"\n        Compute RGB values from the BGRA raw pixels.\n\n        :return bytes: RGB pixels.\n        \"\"\"\n\n        if not self.__rgb:\n            rgb = bytearray(self.height * self.width * 3)\n            raw = self.raw\n            rgb[0::3] = raw[2::4]\n            rgb[1::3] = raw[1::4]\n            rgb[2::3] = raw[0::4]\n            self.__rgb = bytes(rgb)\n\n        return self.__rgb", "response": "Compute RGB values from the BGRA raw pixels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value at a given position.", "response": "def pixel(self, coord_x, coord_y):\n        # type: (int, int) -> Pixel\n        \"\"\"\n        Returns the pixel value at a given position.\n\n        :param int coord_x: The x coordinate.\n        :param int coord_y: The y coordinate.\n        :return tuple: The pixel value as (R, G, B).\n        \"\"\"\n\n        try:\n            return self.pixels[coord_y][coord_x]  # type: ignore\n        except IndexError:\n            raise ScreenShotError(\n                \"Pixel location ({}, {}) is out of range.\".format(coord_x, coord_y)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(args=None):\n    # type: (Optional[List[str]]) -> int\n    \"\"\" Main logic. \"\"\"\n\n    cli_args = ArgumentParser()\n    cli_args.add_argument(\n        \"-c\",\n        \"--coordinates\",\n        default=\"\",\n        type=str,\n        help=\"the part of the screen to capture: top, left, width, height\",\n    )\n    cli_args.add_argument(\n        \"-l\",\n        \"--level\",\n        default=6,\n        type=int,\n        choices=list(range(10)),\n        help=\"the PNG compression level\",\n    )\n    cli_args.add_argument(\n        \"-m\", \"--monitor\", default=0, type=int, help=\"the monitor to screen shot\"\n    )\n    cli_args.add_argument(\n        \"-o\", \"--output\", default=\"monitor-{mon}.png\", help=\"the output file name\"\n    )\n    cli_args.add_argument(\n        \"-q\",\n        \"--quiet\",\n        default=False,\n        action=\"store_true\",\n        help=\"do not print created files\",\n    )\n    cli_args.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\n\n    options = cli_args.parse_args(args)\n    kwargs = {\"mon\": options.monitor, \"output\": options.output}\n    if options.coordinates:\n        try:\n            top, left, width, height = options.coordinates.split(\",\")\n        except ValueError:\n            print(\"Coordinates syntax: top, left, width, height\")\n            return 2\n\n        kwargs[\"mon\"] = {\n            \"top\": int(top),\n            \"left\": int(left),\n            \"width\": int(width),\n            \"height\": int(height),\n        }\n        if options.output == \"monitor-{mon}.png\":\n            kwargs[\"output\"] = \"sct-{top}x{left}_{width}x{height}.png\"\n\n    try:\n        with mss() as sct:\n            if options.coordinates:\n                output = kwargs[\"output\"].format(**kwargs[\"mon\"])\n                sct_img = sct.grab(kwargs[\"mon\"])\n                to_png(sct_img.rgb, sct_img.size, level=options.level, output=output)\n                if not options.quiet:\n                    print(os.path.realpath(output))\n            else:\n                for file_name in sct.save(**kwargs):\n                    if not options.quiet:\n                        print(os.path.realpath(file_name))\n            return 0\n    except ScreenShotError:\n        return 1", "response": "Entry point for the\n archive script."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the error handler for the current program.", "response": "def error_handler(_, event):\n    # type: (Any, Any) -> int\n    \"\"\" Specifies the program's supplied error handler. \"\"\"\n\n    evt = event.contents\n    ERROR.details = {\n        \"type\": evt.type,\n        \"serial\": evt.serial,\n        \"error_code\": evt.error_code,\n        \"request_code\": evt.request_code,\n        \"minor_code\": evt.minor_code,\n    }\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(retval, func, args):\n    # type: (int, Any, Tuple[Any, Any]) -> Optional[Tuple[Any, Any]]\n    \"\"\" Validate the returned value of a Xlib or XRANDR function. \"\"\"\n\n    if retval != 0 and not ERROR.details:\n        return args\n\n    err = \"{}() failed\".format(func.__name__)\n    details = {\"retval\": retval, \"args\": args}\n    raise ScreenShotError(err, details=details)", "response": "Validate the return value of a Xlib or XRANDR function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets all ctypes functions and attach them to attributes.", "response": "def _set_cfunctions(self):\n        \"\"\"\n        Set all ctypes functions and attach them to attributes.\n        See https://tronche.com/gui/x/xlib/function-index.html for details.\n        \"\"\"\n\n        def cfactory(attr=self.xlib, func=None, argtypes=None, restype=None):\n            # type: (Any, str, List[Any], Any) -> None\n            \"\"\" Factorize ctypes creations. \"\"\"\n            self._cfactory(\n                attr=attr,\n                errcheck=validate,\n                func=func,\n                argtypes=argtypes,\n                restype=restype,\n            )\n\n        void = ctypes.c_void_p\n        c_int = ctypes.c_int\n        uint = ctypes.c_uint\n        ulong = ctypes.c_ulong\n        c_long = ctypes.c_long\n        char_p = ctypes.c_char_p\n        pointer = ctypes.POINTER\n\n        cfactory(func=\"XSetErrorHandler\", argtypes=[void], restype=c_int)\n        cfactory(\n            func=\"XGetErrorText\",\n            argtypes=[pointer(Display), c_int, char_p, c_int],\n            restype=void,\n        )\n        cfactory(func=\"XOpenDisplay\", argtypes=[char_p], restype=pointer(Display))\n        cfactory(\n            func=\"XDefaultRootWindow\",\n            argtypes=[pointer(Display)],\n            restype=pointer(XWindowAttributes),\n        )\n        cfactory(\n            func=\"XGetWindowAttributes\",\n            argtypes=[\n                pointer(Display),\n                pointer(XWindowAttributes),\n                pointer(XWindowAttributes),\n            ],\n            restype=c_int,\n        )\n        cfactory(\n            func=\"XGetImage\",\n            argtypes=[\n                pointer(Display),\n                pointer(Display),\n                c_int,\n                c_int,\n                uint,\n                uint,\n                ulong,\n                c_int,\n            ],\n            restype=pointer(XImage),\n        )\n        cfactory(func=\"XDestroyImage\", argtypes=[pointer(XImage)], restype=void)\n\n        # A simple benchmark calling 10 times those 2 functions:\n        # XRRGetScreenResources():        0.1755971429956844 s\n        # XRRGetScreenResourcesCurrent(): 0.0039125580078689 s\n        # The second is faster by a factor of 44! So try to use it first.\n        try:\n            cfactory(\n                attr=self.xrandr,\n                func=\"XRRGetScreenResourcesCurrent\",\n                argtypes=[pointer(Display), pointer(Display)],\n                restype=pointer(XRRScreenResources),\n            )\n        except AttributeError:\n            cfactory(\n                attr=self.xrandr,\n                func=\"XRRGetScreenResources\",\n                argtypes=[pointer(Display), pointer(Display)],\n                restype=pointer(XRRScreenResources),\n            )\n            self.xrandr.XRRGetScreenResourcesCurrent = self.xrandr.XRRGetScreenResources\n\n        cfactory(\n            attr=self.xrandr,\n            func=\"XRRGetCrtcInfo\",\n            argtypes=[pointer(Display), pointer(XRRScreenResources), c_long],\n            restype=pointer(XRRCrtcInfo),\n        )\n        cfactory(\n            attr=self.xrandr,\n            func=\"XRRFreeScreenResources\",\n            argtypes=[pointer(XRRScreenResources)],\n            restype=void,\n        )\n        cfactory(\n            attr=self.xrandr,\n            func=\"XRRFreeCrtcInfo\",\n            argtypes=[pointer(XRRCrtcInfo)],\n            restype=void,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets more information about the latest X server error.", "response": "def get_error_details(self):\n        # type: () -> Optional[Dict[str, Any]]\n        \"\"\" Get more information about the latest X server error. \"\"\"\n\n        details = {}  # type: Dict[str, Any]\n\n        if ERROR.details:\n            details = {\"xerror_details\": ERROR.details}\n            ERROR.details = None\n            xserver_error = ctypes.create_string_buffer(1024)\n            self.xlib.XGetErrorText(\n                MSS.display,\n                details.get(\"xerror_details\", {}).get(\"error_code\", 0),\n                xserver_error,\n                len(xserver_error),\n            )\n            xerror = xserver_error.value.decode(\"utf-8\")\n            if xerror != \"0\":\n                details[\"xerror\"] = xerror\n\n        return details"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting positions of monitors.", "response": "def monitors(self):\n        # type: () -> Monitors\n        \"\"\" Get positions of monitors (see parent class property). \"\"\"\n\n        if not self._monitors:\n            display = MSS.display\n            int_ = int\n            xrandr = self.xrandr\n\n            # All monitors\n            gwa = XWindowAttributes()\n            self.xlib.XGetWindowAttributes(display, self.root, ctypes.byref(gwa))\n            self._monitors.append(\n                {\n                    \"left\": int_(gwa.x),\n                    \"top\": int_(gwa.y),\n                    \"width\": int_(gwa.width),\n                    \"height\": int_(gwa.height),\n                }\n            )\n\n            # Each monitors\n            mon = xrandr.XRRGetScreenResourcesCurrent(display, self.drawable).contents\n            crtcs = mon.crtcs\n            for idx in range(mon.ncrtc):\n                crtc = xrandr.XRRGetCrtcInfo(display, mon, crtcs[idx]).contents\n                if crtc.noutput == 0:\n                    xrandr.XRRFreeCrtcInfo(crtc)\n                    continue\n\n                self._monitors.append(\n                    {\n                        \"left\": int_(crtc.x),\n                        \"top\": int_(crtc.y),\n                        \"width\": int_(crtc.width),\n                        \"height\": int_(crtc.height),\n                    }\n                )\n                xrandr.XRRFreeCrtcInfo(crtc)\n            xrandr.XRRFreeScreenResources(mon)\n\n        return self._monitors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngrab all pixels from a monitor.", "response": "def grab(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        \"\"\" Retrieve all pixels from a monitor. Pixels have to be RGB. \"\"\"\n\n        # Convert PIL bbox style\n        if isinstance(monitor, tuple):\n            monitor = {\n                \"left\": monitor[0],\n                \"top\": monitor[1],\n                \"width\": monitor[2] - monitor[0],\n                \"height\": monitor[3] - monitor[1],\n            }\n\n        ximage = self.xlib.XGetImage(\n            MSS.display,\n            self.drawable,\n            monitor[\"left\"],\n            monitor[\"top\"],\n            monitor[\"width\"],\n            monitor[\"height\"],\n            PLAINMASK,\n            ZPIXMAP,\n        )\n\n        try:\n            bits_per_pixel = ximage.contents.bits_per_pixel\n            if bits_per_pixel != 32:\n                raise ScreenShotError(\n                    \"[XImage] bits per pixel value not (yet?) implemented: {}.\".format(\n                        bits_per_pixel\n                    )\n                )\n\n            raw_data = ctypes.cast(\n                ximage.contents.data,\n                ctypes.POINTER(\n                    ctypes.c_ubyte * monitor[\"height\"] * monitor[\"width\"] * 4\n                ),\n            )\n            data = bytearray(raw_data.contents)\n        finally:\n            # Free\n            self.xlib.XDestroyImage(ximage)\n\n        return self.cls_image(data, monitor)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, mon=0, output=\"monitor-{mon}.png\", callback=None):\n        # type: (int, str, Callable[[str], None]) -> Iterator[str]\n        \"\"\"\n        Grab a screen shot and save it to a file.\n\n        :param int mon: The monitor to screen shot (default=0).\n                        -1: grab one screen shot of all monitors\n                         0: grab one screen shot by monitor\n                        N: grab the screen shot of the monitor N\n\n        :param str output: The output filename.\n\n            It can take several keywords to customize the filename:\n            - `{mon}`: the monitor number\n            - `{top}`: the screen shot y-coordinate of the upper-left corner\n            - `{left}`: the screen shot x-coordinate of the upper-left corner\n            - `{width}`: the screen shot's width\n            - `{height}`: the screen shot's height\n            - `{date}`: the current date using the default formatter\n\n            As it is using the `format()` function, you can specify\n            formatting options like `{date:%Y-%m-%s}`.\n\n        :param callable callback: Callback called before saving the\n            screen shot to a file.  Take the `output` argument as parameter.\n\n        :return generator: Created file(s).\n        \"\"\"\n\n        monitors = self.monitors\n        if not monitors:\n            raise ScreenShotError(\"No monitor found.\")\n\n        if mon == 0:\n            # One screen shot by monitor\n            for idx, monitor in enumerate(monitors[1:], 1):\n                fname = output.format(mon=idx, date=datetime.now(), **monitor)\n                if callable(callback):\n                    callback(fname)\n                sct = self.grab(monitor)\n                to_png(sct.rgb, sct.size, level=self.compression_level, output=fname)\n                yield fname\n        else:\n            # A screen shot of all monitors together or\n            # a screen shot of the monitor N.\n            mon = 0 if mon == -1 else mon\n            try:\n                monitor = monitors[mon]\n            except IndexError:\n                raise ScreenShotError(\"Monitor {!r} does not exist.\".format(mon))\n\n            output = output.format(mon=mon, date=datetime.now(), **monitor)\n            if callable(callback):\n                callback(output)\n            sct = self.grab(monitor)\n            to_png(sct.rgb, sct.size, level=self.compression_level, output=output)\n            yield output", "response": "Save a screen shot of the current state of the screen shot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shot(self, **kwargs):\n        # type: (Any) -> str\n        \"\"\"\n        Helper to save the screen shot of the 1st monitor, by default.\n        You can pass the same arguments as for ``save``.\n        \"\"\"\n\n        kwargs[\"mon\"] = kwargs.get(\"mon\", 1)\n        return next(self.save(**kwargs))", "response": "Helper to save the screen shot of the 1st monitor by default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset all ctypes functions and attach them to the attributes.", "response": "def _set_cfunctions(self):\n        \"\"\" Set all ctypes functions and attach them to attributes. \"\"\"\n\n        void = ctypes.c_void_p\n        pointer = ctypes.POINTER\n\n        self._cfactory(\n            attr=self.user32, func=\"GetSystemMetrics\", argtypes=[INT], restype=INT\n        )\n        self._cfactory(\n            attr=self.user32,\n            func=\"EnumDisplayMonitors\",\n            argtypes=[HDC, void, self.monitorenumproc, LPARAM],\n            restype=BOOL,\n        )\n        self._cfactory(\n            attr=self.user32, func=\"GetWindowDC\", argtypes=[HWND], restype=HDC\n        )\n\n        self._cfactory(\n            attr=self.gdi32, func=\"GetDeviceCaps\", argtypes=[HWND, INT], restype=INT\n        )\n        self._cfactory(\n            attr=self.gdi32, func=\"CreateCompatibleDC\", argtypes=[HDC], restype=HDC\n        )\n        self._cfactory(\n            attr=self.gdi32,\n            func=\"CreateCompatibleBitmap\",\n            argtypes=[HDC, INT, INT],\n            restype=HBITMAP,\n        )\n        self._cfactory(\n            attr=self.gdi32,\n            func=\"SelectObject\",\n            argtypes=[HDC, HGDIOBJ],\n            restype=HGDIOBJ,\n        )\n        self._cfactory(\n            attr=self.gdi32,\n            func=\"BitBlt\",\n            argtypes=[HDC, INT, INT, INT, INT, HDC, INT, INT, DWORD],\n            restype=BOOL,\n        )\n        self._cfactory(\n            attr=self.gdi32, func=\"DeleteObject\", argtypes=[HGDIOBJ], restype=INT\n        )\n        self._cfactory(\n            attr=self.gdi32,\n            func=\"GetDIBits\",\n            argtypes=[HDC, HBITMAP, UINT, UINT, void, pointer(BITMAPINFO), UINT],\n            restype=BOOL,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting DPI aware on Hi - DPI monitors.", "response": "def _set_dpi_awareness(self):\n        \"\"\" Set DPI aware to capture full screen on Hi-DPI monitors. \"\"\"\n\n        version = sys.getwindowsversion()[:2]  # pylint: disable=no-member\n        if version >= (6, 3):\n            # Windows 8.1+\n            # Here 2 = PROCESS_PER_MONITOR_DPI_AWARE, which means:\n            #     per monitor DPI aware. This app checks for the DPI when it is\n            #     created and adjusts the scale factor whenever the DPI changes.\n            #     These applications are not automatically scaled by the system.\n            ctypes.windll.shcore.SetProcessDpiAwareness(2)\n        elif (6, 0) <= version < (6, 3):\n            # Windows Vista, 7, 8 and Server 2012\n            self.user32.SetProcessDPIAware()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef monitors(self):\n        # type: () -> Monitors\n        \"\"\" Get positions of monitors (see parent class). \"\"\"\n\n        if not self._monitors:\n            int_ = int\n            user32 = self.user32\n            get_system_metrics = user32.GetSystemMetrics\n\n            # All monitors\n            self._monitors.append(\n                {\n                    \"left\": int_(get_system_metrics(76)),  # SM_XVIRTUALSCREEN\n                    \"top\": int_(get_system_metrics(77)),  # SM_YVIRTUALSCREEN\n                    \"width\": int_(get_system_metrics(78)),  # SM_CXVIRTUALSCREEN\n                    \"height\": int_(get_system_metrics(79)),  # SM_CYVIRTUALSCREEN\n                }\n            )\n\n            # Each monitors\n            def _callback(monitor, data, rect, dc_):\n                # types: (int, HDC, LPRECT, LPARAM) -> int\n                \"\"\"\n                Callback for monitorenumproc() function, it will return\n                a RECT with appropriate values.\n                \"\"\"\n                # pylint: disable=unused-argument\n\n                rct = rect.contents\n                self._monitors.append(\n                    {\n                        \"left\": int_(rct.left),\n                        \"top\": int_(rct.top),\n                        \"width\": int_(rct.right - rct.left),\n                        \"height\": int_(rct.bottom - rct.top),\n                    }\n                )\n                return 1\n\n            callback = self.monitorenumproc(_callback)\n            user32.EnumDisplayMonitors(0, 0, callback, 0)\n\n        return self._monitors", "response": "Get positions of monitors (see parent class)."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngrabbing all pixels from a monitor and return a ScreenShot object.", "response": "def grab(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        \"\"\" Retrieve all pixels from a monitor. Pixels have to be RGB.\n\n            In the code, there are few interesting things:\n\n            [1] bmi.bmiHeader.biHeight = -height\n\n            A bottom-up DIB is specified by setting the height to a\n            positive number, while a top-down DIB is specified by\n            setting the height to a negative number.\n            https://msdn.microsoft.com/en-us/library/ms787796.aspx\n            https://msdn.microsoft.com/en-us/library/dd144879%28v=vs.85%29.aspx\n\n\n            [2] bmi.bmiHeader.biBitCount = 32\n                image_data = create_string_buffer(height * width * 4)\n\n            We grab the image in RGBX mode, so that each word is 32bit\n            and we have no striding, then we transform to RGB.\n            Inspired by https://github.com/zoofIO/flexx\n\n\n            [3] bmi.bmiHeader.biClrUsed = 0\n                bmi.bmiHeader.biClrImportant = 0\n\n            When biClrUsed and biClrImportant are set to zero, there\n            is \"no\" color table, so we can read the pixels of the bitmap\n            retrieved by gdi32.GetDIBits() as a sequence of RGB values.\n            Thanks to http://stackoverflow.com/a/3688682\n        \"\"\"\n\n        # Convert PIL bbox style\n        if isinstance(monitor, tuple):\n            monitor = {\n                \"left\": monitor[0],\n                \"top\": monitor[1],\n                \"width\": monitor[2] - monitor[0],\n                \"height\": monitor[3] - monitor[1],\n            }\n\n        srcdc, memdc = MSS.srcdc, MSS.memdc\n        width, height = monitor[\"width\"], monitor[\"height\"]\n\n        if (self._bbox[\"height\"], self._bbox[\"width\"]) != (height, width):\n            self._bbox = monitor\n            self._bmi.bmiHeader.biWidth = width\n            self._bmi.bmiHeader.biHeight = -height  # Why minus? [1]\n            self._data = ctypes.create_string_buffer(width * height * 4)  # [2]\n            if MSS.bmp:\n                self.gdi32.DeleteObject(MSS.bmp)\n            MSS.bmp = self.gdi32.CreateCompatibleBitmap(srcdc, width, height)\n            self.gdi32.SelectObject(memdc, MSS.bmp)\n\n        self.gdi32.BitBlt(\n            memdc,\n            0,\n            0,\n            width,\n            height,\n            srcdc,\n            monitor[\"left\"],\n            monitor[\"top\"],\n            SRCCOPY | CAPTUREBLT,\n        )\n        bits = self.gdi32.GetDIBits(\n            memdc, MSS.bmp, 0, height, self._data, self._bmi, DIB_RGB_COLORS\n        )\n        if bits != height:\n            raise ScreenShotError(\"gdi32.GetDIBits() failed.\")\n\n        return self.cls_image(bytearray(self._data), monitor)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mss(**kwargs):\n    # type: (Any) -> MSSMixin\n    \"\"\" Factory returning a proper MSS class instance.\n\n        It detects the plateform we are running on\n        and choose the most adapted mss_class to take\n        screenshots.\n\n        It then proxies its arguments to the class for\n        instantiation.\n    \"\"\"\n\n    os_ = platform.system().lower()\n\n    if os_ == \"darwin\":\n        from . import darwin\n\n        return darwin.MSS(**kwargs)\n\n    if os_ == \"linux\":\n        from . import linux\n\n        return linux.MSS(**kwargs)\n\n    if os_ == \"windows\":\n        from . import windows\n\n        return windows.MSS(**kwargs)\n\n    raise ScreenShotError(\"System {!r} not (yet?) implemented.\".format(os_))", "response": "Factory returning a proper MSS class instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert HSL representation towards RGB", "response": "def hsl2rgb(hsl):\n    \"\"\"Convert HSL representation towards RGB\n\n    :param h: Hue, position around the chromatic circle (h=1 equiv h=0)\n    :param s: Saturation, color saturation (0=full gray, 1=full color)\n    :param l: Ligthness, Overhaul lightness (0=full black, 1=full white)\n    :rtype: 3-uple for RGB values in float between 0 and 1\n\n    Hue, Saturation, Range from Lightness is a float between 0 and 1\n\n    Note that Hue can be set to any value but as it is a rotation\n    around the chromatic circle, any value above 1 or below 0 can\n    be expressed by a value between 0 and 1 (Note that h=0 is equiv\n    to h=1).\n\n    This algorithm came from:\n    http://www.easyrgb.com/index.php?X=MATH&H=19#text19\n\n    Here are some quick notion of HSL to RGB conversion:\n\n    >>> from colour import hsl2rgb\n\n    With a lightness put at 0, RGB is always rgbblack\n\n    >>> hsl2rgb((0.0, 0.0, 0.0))\n    (0.0, 0.0, 0.0)\n    >>> hsl2rgb((0.5, 0.0, 0.0))\n    (0.0, 0.0, 0.0)\n    >>> hsl2rgb((0.5, 0.5, 0.0))\n    (0.0, 0.0, 0.0)\n\n    Same for lightness put at 1, RGB is always rgbwhite\n\n    >>> hsl2rgb((0.0, 0.0, 1.0))\n    (1.0, 1.0, 1.0)\n    >>> hsl2rgb((0.5, 0.0, 1.0))\n    (1.0, 1.0, 1.0)\n    >>> hsl2rgb((0.5, 0.5, 1.0))\n    (1.0, 1.0, 1.0)\n\n    With saturation put at 0, the RGB should be equal to Lightness:\n\n    >>> hsl2rgb((0.0, 0.0, 0.25))\n    (0.25, 0.25, 0.25)\n    >>> hsl2rgb((0.5, 0.0, 0.5))\n    (0.5, 0.5, 0.5)\n    >>> hsl2rgb((0.5, 0.0, 0.75))\n    (0.75, 0.75, 0.75)\n\n    With saturation put at 1, and lightness put to 0.5, we can find\n    normal full red, green, blue colors:\n\n    >>> hsl2rgb((0 , 1.0, 0.5))\n    (1.0, 0.0, 0.0)\n    >>> hsl2rgb((1 , 1.0, 0.5))\n    (1.0, 0.0, 0.0)\n    >>> hsl2rgb((1.0/3 , 1.0, 0.5))\n    (0.0, 1.0, 0.0)\n    >>> hsl2rgb((2.0/3 , 1.0, 0.5))\n    (0.0, 0.0, 1.0)\n\n    Of course:\n    >>> hsl2rgb((0.0, 2.0, 0.5))  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    ValueError: Saturation must be between 0 and 1.\n\n    And:\n    >>> hsl2rgb((0.0, 0.0, 1.5))  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    ValueError: Lightness must be between 0 and 1.\n\n    \"\"\"\n    h, s, l = [float(v) for v in hsl]\n\n    if not (0.0 - FLOAT_ERROR <= s <= 1.0 + FLOAT_ERROR):\n        raise ValueError(\"Saturation must be between 0 and 1.\")\n    if not (0.0 - FLOAT_ERROR <= l <= 1.0 + FLOAT_ERROR):\n        raise ValueError(\"Lightness must be between 0 and 1.\")\n\n    if s == 0:\n        return l, l, l\n\n    if l < 0.5:\n        v2 = l * (1.0 + s)\n    else:\n        v2 = (l + s) - (s * l)\n\n    v1 = 2.0 * l - v2\n\n    r = _hue2rgb(v1, v2, h + (1.0 / 3))\n    g = _hue2rgb(v1, v2, h)\n    b = _hue2rgb(v1, v2, h - (1.0 / 3))\n\n    return r, g, b"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting RGB representation into HSL representation.", "response": "def rgb2hsl(rgb):\n    \"\"\"Convert RGB representation towards HSL\n\n    :param r: Red amount (float between 0 and 1)\n    :param g: Green amount (float between 0 and 1)\n    :param b: Blue amount (float between 0 and 1)\n    :rtype: 3-uple for HSL values in float between 0 and 1\n\n    This algorithm came from:\n    http://www.easyrgb.com/index.php?X=MATH&H=19#text19\n\n    Here are some quick notion of RGB to HSL conversion:\n\n    >>> from colour import rgb2hsl\n\n    Note that if red amount is equal to green and blue, then you\n    should have a gray value (from black to white).\n\n\n    >>> rgb2hsl((1.0, 1.0, 1.0))  # doctest: +ELLIPSIS\n    (..., 0.0, 1.0)\n    >>> rgb2hsl((0.5, 0.5, 0.5))  # doctest: +ELLIPSIS\n    (..., 0.0, 0.5)\n    >>> rgb2hsl((0.0, 0.0, 0.0))  # doctest: +ELLIPSIS\n    (..., 0.0, 0.0)\n\n    If only one color is different from the others, it defines the\n    direct Hue:\n\n    >>> rgb2hsl((0.5, 0.5, 1.0))  # doctest: +ELLIPSIS\n    (0.66..., 1.0, 0.75)\n    >>> rgb2hsl((0.2, 0.1, 0.1))  # doctest: +ELLIPSIS\n    (0.0, 0.33..., 0.15...)\n\n    Having only one value set, you can check that:\n\n    >>> rgb2hsl((1.0, 0.0, 0.0))\n    (0.0, 1.0, 0.5)\n    >>> rgb2hsl((0.0, 1.0, 0.0))  # doctest: +ELLIPSIS\n    (0.33..., 1.0, 0.5)\n    >>> rgb2hsl((0.0, 0.0, 1.0))  # doctest: +ELLIPSIS\n    (0.66..., 1.0, 0.5)\n\n    Regression check upon very close values in every component of\n    red, green and blue:\n\n    >>> rgb2hsl((0.9999999999999999, 1.0, 0.9999999999999994))\n    (0.0, 0.0, 0.999...)\n\n    Of course:\n\n    >>> rgb2hsl((0.0, 2.0, 0.5))  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    ValueError: Green must be between 0 and 1. You provided 2.0.\n\n    And:\n    >>> rgb2hsl((0.0, 0.0, 1.5))  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    ValueError: Blue must be between 0 and 1. You provided 1.5.\n\n    \"\"\"\n    r, g, b = [float(v) for v in rgb]\n\n    for name, v in {'Red': r, 'Green': g, 'Blue': b}.items():\n        if not (0 - FLOAT_ERROR <= v <= 1 + FLOAT_ERROR):\n            raise ValueError(\"%s must be between 0 and 1. You provided %r.\"\n                             % (name, v))\n\n    vmin = min(r, g, b)  ## Min. value of RGB\n    vmax = max(r, g, b)  ## Max. value of RGB\n    diff = vmax - vmin   ## Delta RGB value\n\n    vsum = vmin + vmax\n\n    l = vsum / 2\n\n    if diff < FLOAT_ERROR:  ## This is a gray, no chroma...\n        return (0.0, 0.0, l)\n\n    ##\n    ## Chromatic data...\n    ##\n\n    ## Saturation\n    if l < 0.5:\n        s = diff / vsum\n    else:\n        s = diff / (2.0 - vsum)\n\n    dr = (((vmax - r) / 6) + (diff / 2)) / diff\n    dg = (((vmax - g) / 6) + (diff / 2)) / diff\n    db = (((vmax - b) / 6) + (diff / 2)) / diff\n\n    if r == vmax:\n        h = db - dg\n    elif g == vmax:\n        h = (1.0 / 3) + dr - db\n    elif b == vmax:\n        h = (2.0 / 3) + dg - dr\n\n    if h < 0: h += 1\n    if h > 1: h -= 1\n\n    return (h, s, l)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _hue2rgb(v1, v2, vH):\n\n    while vH < 0: vH += 1\n    while vH > 1: vH -= 1\n\n    if 6 * vH < 1: return v1 + (v2 - v1) * 6 * vH\n    if 2 * vH < 1: return v2\n    if 3 * vH < 2: return v1 + (v2 - v1) * ((2.0 / 3) - vH) * 6\n\n    return v1", "response": "Private function to convert hue values to RGB values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming RGB tuple to hex RGB representation", "response": "def rgb2hex(rgb, force_long=False):\n    \"\"\"Transform RGB tuple to hex RGB representation\n\n    :param rgb: RGB 3-uple of float between 0 and 1\n    :rtype: 3 hex char or 6 hex char string representation\n\n    Usage\n    -----\n\n    >>> from colour import rgb2hex\n\n    >>> rgb2hex((0.0,1.0,0.0))\n    '#0f0'\n\n    Rounding try to be as natural as possible:\n\n    >>> rgb2hex((0.0,0.999999,1.0))\n    '#0ff'\n\n    And if not possible, the 6 hex char representation is used:\n\n    >>> rgb2hex((0.23,1.0,1.0))\n    '#3bffff'\n\n    >>> rgb2hex((0.0,0.999999,1.0), force_long=True)\n    '#00ffff'\n\n    \"\"\"\n\n    hx = ''.join([\"%02x\" % int(c * 255 + 0.5 - FLOAT_ERROR)\n                  for c in rgb])\n\n    if not force_long and hx[0::2] == hx[1::2]:\n        hx = ''.join(hx[0::2])\n\n    return \"#%s\" % hx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming hex RGB representation to RGB tuple", "response": "def hex2rgb(str_rgb):\n    \"\"\"Transform hex RGB representation to RGB tuple\n\n    :param str_rgb: 3 hex char or 6 hex char string representation\n    :rtype: RGB 3-uple of float between 0 and 1\n\n    >>> from colour import hex2rgb\n\n    >>> hex2rgb('#00ff00')\n    (0.0, 1.0, 0.0)\n\n    >>> hex2rgb('#0f0')\n    (0.0, 1.0, 0.0)\n\n    >>> hex2rgb('#aaa')  # doctest: +ELLIPSIS\n    (0.66..., 0.66..., 0.66...)\n\n    >>> hex2rgb('#aa')  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    ValueError: Invalid value '#aa' provided for rgb color.\n\n    \"\"\"\n\n    try:\n        rgb = str_rgb[1:]\n\n        if len(rgb) == 6:\n            r, g, b = rgb[0:2], rgb[2:4], rgb[4:6]\n        elif len(rgb) == 3:\n            r, g, b = rgb[0] * 2, rgb[1] * 2, rgb[2] * 2\n        else:\n            raise ValueError()\n    except:\n        raise ValueError(\"Invalid value %r provided for rgb color.\"\n                         % str_rgb)\n\n    return tuple([float(int(v, 16)) / 255 for v in (r, g, b)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hex2web(hex):\n    dec_rgb = tuple(int(v * 255) for v in hex2rgb(hex))\n    if dec_rgb in RGB_TO_COLOR_NAMES:\n        ## take the first one\n        color_name = RGB_TO_COLOR_NAMES[dec_rgb][0]\n        ## Enforce full lowercase for single worded color name.\n        return color_name if len(re.sub(r\"[^A-Z]\", \"\", color_name)) > 1 \\\n               else color_name.lower()\n\n    # Hex format is verified by hex2rgb function. And should be 3 or 6 digit\n    if len(hex) == 7:\n        if hex[1] == hex[2] and \\\n           hex[3] == hex[4] and \\\n           hex[5] == hex[6]:\n            return '#' + hex[1] + hex[3] + hex[5]\n    return hex", "response": "Converts HEX representation to WEB representation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef web2hex(web, force_long=False):\n    if web.startswith('#'):\n        if (LONG_HEX_COLOR.match(web) or\n            (not force_long and SHORT_HEX_COLOR.match(web))):\n            return web.lower()\n        elif SHORT_HEX_COLOR.match(web) and force_long:\n            return '#' + ''.join([(\"%s\" % (t, )) * 2 for t in web[1:]])\n        raise AttributeError(\n            \"%r is not in web format. Need 3 or 6 hex digit.\" % web)\n\n    web = web.lower()\n    if web not in COLOR_NAME_TO_RGB:\n        raise ValueError(\"%r is not a recognized color.\" % web)\n\n    ## convert dec to hex:\n\n    return rgb2hex([float(int(v)) / 255 for v in COLOR_NAME_TO_RGB[web]],\n                   force_long)", "response": "Converts WEB representation to HEX"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of nb color HSL tuples between begin_hsl and end_hsl.", "response": "def color_scale(begin_hsl, end_hsl, nb):\n    \"\"\"Returns a list of nb color HSL tuples between begin_hsl and end_hsl\n\n    >>> from colour import color_scale\n\n    >>> [rgb2hex(hsl2rgb(hsl)) for hsl in color_scale((0, 1, 0.5),\n    ...                                               (1, 1, 0.5), 3)]\n    ['#f00', '#0f0', '#00f', '#f00']\n\n    >>> [rgb2hex(hsl2rgb(hsl))\n    ...  for hsl in color_scale((0, 0, 0),\n    ...                         (0, 0, 1),\n    ...                         15)]  # doctest: +ELLIPSIS\n    ['#000', '#111', '#222', ..., '#ccc', '#ddd', '#eee', '#fff']\n\n    Of course, asking for negative values is not supported:\n\n    >>> color_scale((0, 1, 0.5), (1, 1, 0.5), -2)\n    Traceback (most recent call last):\n    ...\n    ValueError: Unsupported negative number of colors (nb=-2).\n\n    \"\"\"\n\n    if nb < 0:\n        raise ValueError(\n            \"Unsupported negative number of colors (nb=%r).\" % nb)\n\n    step = tuple([float(end_hsl[i] - begin_hsl[i]) / nb for i in range(0, 3)]) \\\n           if nb > 0 else (0, 0, 0)\n\n    def mul(step, value):\n        return tuple([v * value for v in step])\n\n    def add_v(step, step2):\n        return tuple([v + step2[i] for i, v in enumerate(step)])\n\n    return [add_v(begin_hsl, mul(step, r)) for r in range(0, nb + 1)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a color representation from the string representation of an object.", "response": "def RGB_color_picker(obj):\n    \"\"\"Build a color representation from the string representation of an object\n\n    This allows to quickly get a color from some data, with the\n    additional benefit that the color will be the same as long as the\n    (string representation of the) data is the same::\n\n        >>> from colour import RGB_color_picker, Color\n\n    Same inputs produce the same result::\n\n        >>> RGB_color_picker(\"Something\") == RGB_color_picker(\"Something\")\n        True\n\n    ... but different inputs produce different colors::\n\n        >>> RGB_color_picker(\"Something\") != RGB_color_picker(\"Something else\")\n        True\n\n    In any case, we still get a ``Color`` object::\n\n        >>> isinstance(RGB_color_picker(\"Something\"), Color)\n        True\n\n    \"\"\"\n\n    ## Turn the input into a by 3-dividable string. SHA-384 is good because it\n    ## divides into 3 components of the same size, which will be used to\n    ## represent the RGB values of the color.\n    digest = hashlib.sha384(str(obj).encode('utf-8')).hexdigest()\n\n    ## Split the digest into 3 sub-strings of equivalent size.\n    subsize = int(len(digest) / 3)\n    splitted_digest = [digest[i * subsize: (i + 1) * subsize]\n                       for i in range(3)]\n\n    ## Convert those hexadecimal sub-strings into integer and scale them down\n    ## to the 0..1 range.\n    max_value = float(int(\"f\" * subsize, 16))\n    components = (\n        int(d, 16)     ## Make a number from a list with hex digits\n        / max_value    ## Scale it down to [0.0, 1.0]\n        for d in splitted_digest)\n\n    return Color(rgb2hex(components))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pep517_subprocess_runner(cmd, cwd=None, extra_environ=None):\n    # type: (List[AnyStr], Optional[AnyStr], Optional[Mapping[S, S]]) -> None\n    \"\"\"The default method of calling the wrapper subprocess.\"\"\"\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n\n    run(\n        cmd,\n        cwd=cwd,\n        env=env,\n        block=True,\n        combine_stderr=True,\n        return_object=False,\n        write_to_stdout=False,\n        nospin=True,\n    )", "response": "The default method of calling the wrapper subprocess."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a setup. py script with a target egg_base if provided.", "response": "def run_setup(script_path, egg_base=None):\n    # type: (str, Optional[str]) -> Distribution\n    \"\"\"Run a `setup.py` script with a target **egg_base** if provided.\n\n    :param S script_path: The path to the `setup.py` script to run\n    :param Optional[S] egg_base: The metadata directory to build in\n    :raises FileNotFoundError: If the provided `script_path` does not exist\n    :return: The metadata dictionary\n    :rtype: Dict[Any, Any]\n    \"\"\"\n\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(script_path)\n    target_cwd = os.path.dirname(os.path.abspath(script_path))\n    if egg_base is None:\n        egg_base = os.path.join(target_cwd, \"reqlib-metadata\")\n    with temp_path(), cd(target_cwd), _suppress_distutils_logs():\n        # This is for you, Hynek\n        # see https://github.com/hynek/environ_config/blob/69b1c8a/setup.py\n        args = [\"egg_info\"]\n        if egg_base:\n            args += [\"--egg-base\", egg_base]\n        script_name = os.path.basename(script_path)\n        g = {\"__file__\": script_name, \"__name__\": \"__main__\"}\n        sys.path.insert(0, target_cwd)\n        local_dict = {}\n        if sys.version_info < (3, 5):\n            save_argv = sys.argv\n        else:\n            save_argv = sys.argv.copy()\n        try:\n            global _setup_distribution, _setup_stop_after\n            _setup_stop_after = \"run\"\n            sys.argv[0] = script_name\n            sys.argv[1:] = args\n            with open(script_name, \"rb\") as f:\n                contents = f.read()\n                if six.PY3:\n                    contents.replace(br\"\\r\\n\", br\"\\n\")\n                else:\n                    contents.replace(r\"\\r\\n\", r\"\\n\")\n                if sys.version_info < (3, 5):\n                    exec(contents, g, local_dict)\n                else:\n                    exec(contents, g)\n        # We couldn't import everything needed to run setup\n        except Exception:\n            python = os.environ.get(\"PIP_PYTHON_PATH\", sys.executable)\n            out, _ = run(\n                [python, \"setup.py\"] + args,\n                cwd=target_cwd,\n                block=True,\n                combine_stderr=False,\n                return_object=False,\n                nospin=True,\n            )\n        finally:\n            _setup_stop_after = None\n            sys.argv = save_argv\n            _setup_distribution = get_metadata(egg_base, metadata_type=\"egg\")\n        dist = _setup_distribution\n    return dist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean(ctx):\n    ctx.run(f\"python setup.py clean\")\n    dist = ROOT.joinpath(\"dist\")\n    build = ROOT.joinpath(\"build\")\n    print(f\"[clean] Removing {dist} and {build}\")\n    if dist.exists():\n        shutil.rmtree(str(dist))\n    if build.exists():\n        shutil.rmtree(str(build))", "response": "Clean previously built package artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(ctx, type_, repo, prebump=PREBUMP, yes=False):\n    if prebump not in REL_TYPES:\n        raise ValueError(f\"{type_} not in {REL_TYPES}\")\n    prebump = REL_TYPES.index(prebump)\n\n    version = bump_version(ctx, type_, log=True)\n    # Needs to happen before Towncrier deletes fragment files.\n    tag_release(version, yes=yes)\n\n    ctx.run(f\"python setup.py sdist bdist_wheel\")\n\n    dist_pattern = f'{PACKAGE_NAME.replace(\"-\", \"[-_]\")}-*'\n    artifacts = list(ROOT.joinpath(\"dist\").glob(dist_pattern))\n    filename_display = \"\\n\".join(f\"  {a}\" for a in artifacts)\n    print(f\"[release] Will upload:\\n{filename_display}\")\n    if not yes:\n        try:\n            input(\"[release] Release ready. ENTER to upload, CTRL-C to abort: \")\n        except KeyboardInterrupt:\n            print(\"\\nAborted!\")\n            return\n\n    arg_display = \" \".join(f'\"{n}\"' for n in artifacts)\n    ctx.run(f'twine upload --repository=\"{repo}\" {arg_display}')\n\n    version = _prebump(version, prebump)\n    _write_version(version)\n\n    ctx.run(f'git commit -am \"Prebump to {version}\"')", "response": "Create a new release."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef profile(ctx, filepath, calltree=False):\n\n    filepath = pathlib.Path(filepath)\n    if not filepath.is_file():\n        log(\"profile\", f\"no such script {filepath!s}\", LogLevel.ERROR)\n    else:\n        if calltree:\n            log(\"profile\", f\"profiling script {filepath!s} calltree\")\n            ctx.run(\n                (\n                    f\"python -m cProfile -o .profile.cprof {filepath!s}\"\n                    \" && pyprof2calltree -k -i .profile.cprof\"\n                    \" && rm -rf .profile.cprof\"\n                )\n            )\n        else:\n            log(\"profile\", f\"profiling script {filepath!s}\")\n            ctx.run(f\"vprof -c cmhp {filepath!s}\")", "response": "Run and profile a given Python script."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _strip_marker_elem(elem_name, elements):\n\n    extra_indexes = []\n    preceding_operators = [\"and\"] if elem_name == \"extra\" else [\"and\", \"or\"]\n    for i, element in enumerate(elements):\n        if isinstance(element, list):\n            cancelled = _strip_marker_elem(elem_name, element)\n            if cancelled:\n                extra_indexes.append(i)\n        elif isinstance(element, tuple) and element[0].value == elem_name:\n            extra_indexes.append(i)\n    for i in reversed(extra_indexes):\n        del elements[i]\n        if i > 0 and elements[i - 1] in preceding_operators:\n            # Remove the \"and\" before it.\n            del elements[i - 1]\n        elif elements:\n            # This shouldn't ever happen, but is included for completeness.\n            # If there is not an \"and\" before this element, try to remove the\n            # operator after it.\n            del elements[0]\n    return not elements", "response": "Remove the supplied element from the marker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a new marker which is cleaned according to strip_func", "response": "def _get_stripped_marker(marker, strip_func):\n    \"\"\"Build a new marker which is cleaned according to `strip_func`\"\"\"\n\n    if not marker:\n        return None\n    marker = _ensure_marker(marker)\n    elements = marker._markers\n    strip_func(elements)\n    if elements:\n        return marker\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollecting extra ==... operands from a marker. Returns a list of str. Each str is a speficied extra in this marker.", "response": "def get_contained_extras(marker):\n    \"\"\"Collect \"extra == ...\" operands from a marker.\n\n    Returns a list of str. Each str is a speficied extra in this marker.\n    \"\"\"\n    if not marker:\n        return set()\n    extras = set()\n    marker = _ensure_marker(marker)\n    _markers_collect_extras(marker._markers, extras)\n    return extras"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_contained_pyversions(marker):\n\n    collection = []\n    if not marker:\n        return set()\n    marker = _ensure_marker(marker)\n    # Collect the (Variable, Op, Value) tuples and string joiners from the marker\n    _markers_collect_pyversions(marker._markers, collection)\n    marker_str = \" and \".join(sorted(collection))\n    if not marker_str:\n        return set()\n    # Use the distlib dictionary parser to create a dictionary 'trie' which is a bit\n    # easier to reason about\n    marker_dict = distlib.markers.parse_marker(marker_str)[0]\n    version_set = set()\n    pyversions, _ = parse_marker_dict(marker_dict)\n    if isinstance(pyversions, set):\n        version_set.update(pyversions)\n    elif pyversions is not None:\n        version_set.add(pyversions)\n    # Each distinct element in the set was separated by an \"and\" operator in the marker\n    # So we will need to reduce them with an intersection here rather than a union\n    # in order to find the boundaries\n    versions = set()\n    if version_set:\n        versions = reduce(lambda x, y: x & y, version_set)\n    return versions", "response": "Collect all python_version operands from a marker.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contains_extra(marker):\n    if not marker:\n        return False\n    marker = _ensure_marker(marker)\n    return _markers_contains_extra(marker._markers)", "response": "Check whehter a marker contains an extra ==... operand."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef contains_pyversion(marker):\n\n    if not marker:\n        return False\n    marker = _ensure_marker(marker)\n    return _markers_contains_pyversion(marker._markers)", "response": "Check whether a marker contains a python_version operand."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrewriting import xxx and from xxx import for vendored_libs", "response": "def rewrite_file_imports(item, vendored_libs, vendor_dir):\n    \"\"\"Rewrite 'import xxx' and 'from xxx import' for vendored_libs\"\"\"\n    text = item.read_text(encoding='utf-8')\n    renames = LIBRARY_RENAMES\n    for k in LIBRARY_RENAMES.keys():\n        if k not in vendored_libs:\n            vendored_libs.append(k)\n    for lib in vendored_libs:\n        to_lib = lib\n        if lib in renames:\n            to_lib = renames[lib]\n        text = re.sub(\n            r'([\\n\\s]*)import %s([\\n\\s\\.]+)' % lib,\n            r'\\1import %s\\2' % to_lib,\n            text,\n        )\n        text = re.sub(\n            r'([\\n\\s]*)from %s([\\s\\.])+' % lib,\n            r'\\1from %s\\2' % to_lib,\n            text,\n        )\n        text = re.sub(\n            r\"(\\n\\s*)__import__\\('%s([\\s'\\.])+\" % lib,\n            r\"\\1__import__('%s\\2\" % to_lib,\n            text,\n        )\n    item.write_text(text, encoding='utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distance(a, b):\n    R = 3963  # radius of Earth (miles)\n    lat1, lon1 = math.radians(a[0]), math.radians(a[1])\n    lat2, lon2 = math.radians(b[0]), math.radians(b[1])\n    return math.acos(math.sin(lat1) * math.sin(lat2) +\n                     math.cos(lat1) * math.cos(lat2) * math.cos(lon1 - lon2)) * R", "response": "Calculates distance between two latitude - longitude coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the length of the route.", "response": "def energy(self):\n        \"\"\"Calculates the length of the route.\"\"\"\n        e = 0\n        for i in range(len(self.state)):\n            e += self.distance_matrix[self.state[i-1]][self.state[i]]\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning x rounded to n significant figures.", "response": "def round_figures(x, n):\n    \"\"\"Returns x rounded to n significant figures.\"\"\"\n    return round(x, int(n - math.ceil(math.log10(abs(x)))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave state to pickle", "response": "def save_state(self, fname=None):\n        \"\"\"Saves state to pickle\"\"\"\n        if not fname:\n            date = datetime.datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n            fname = date + \"_energy_\" + str(self.energy()) + \".state\"\n        with open(fname, \"wb\") as fh:\n            pickle.dump(self.state, fh)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads state from pickle file", "response": "def load_state(self, fname=None):\n        \"\"\"Loads state from pickle\"\"\"\n        with open(fname, 'rb') as fh:\n            self.state = pickle.load(fh)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_schedule(self, schedule):\n        self.Tmax = schedule['tmax']\n        self.Tmin = schedule['tmin']\n        self.steps = int(schedule['steps'])\n        self.updates = int(schedule['updates'])", "response": "Takes the output from auto and sets the attributes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an exact copy of the provided state", "response": "def copy_state(self, state):\n        \"\"\"Returns an exact copy of the provided state\n        Implemented according to self.copy_strategy, one of\n\n        * deepcopy : use copy.deepcopy (slow but reliable)\n        * slice: use list slices (faster but only works if state is list-like)\n        * method: use the state's copy() method\n        \"\"\"\n        if self.copy_strategy == 'deepcopy':\n            return copy.deepcopy(state)\n        elif self.copy_strategy == 'slice':\n            return state[:]\n        elif self.copy_strategy == 'method':\n            return state.copy()\n        else:\n            raise RuntimeError('No implementation found for ' +\n                               'the self.copy_strategy \"%s\"' %\n                               self.copy_strategy)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_update(self, step, T, E, acceptance, improvement):\n\n        elapsed = time.time() - self.start\n        if step == 0:\n            print(' Temperature        Energy    Accept   Improve     Elapsed   Remaining',\n                  file=sys.stderr)\n            print('\\r%12.5f  %12.2f                      %s            ' %\n                  (T, E, time_string(elapsed)), file=sys.stderr, end=\"\\r\")\n            sys.stderr.flush()\n        else:\n            remain = (self.steps - step) * (elapsed / step)\n            print('\\r%12.5f  %12.2f  %7.2f%%  %7.2f%%  %s  %s\\r' %\n                  (T, E, 100.0 * acceptance, 100.0 * improvement,\n                   time_string(elapsed), time_string(remain)), file=sys.stderr, end=\"\\r\")\n            sys.stderr.flush()", "response": "Default update prints the current temperature energy acceptance rate and remaining time."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nminimize the energy of a system by simulated annealing.", "response": "def anneal(self):\n        \"\"\"Minimizes the energy of a system by simulated annealing.\n\n        Parameters\n        state : an initial arrangement of the system\n\n        Returns\n        (state, energy): the best state and energy found.\n        \"\"\"\n        step = 0\n        self.start = time.time()\n\n        # Precompute factor for exponential cooling from Tmax to Tmin\n        if self.Tmin <= 0.0:\n            raise Exception('Exponential cooling requires a minimum \"\\\n                \"temperature greater than zero.')\n        Tfactor = -math.log(self.Tmax / self.Tmin)\n\n        # Note initial state\n        T = self.Tmax\n        E = self.energy()\n        prevState = self.copy_state(self.state)\n        prevEnergy = E\n        self.best_state = self.copy_state(self.state)\n        self.best_energy = E\n        trials, accepts, improves = 0, 0, 0\n        if self.updates > 0:\n            updateWavelength = self.steps / self.updates\n            self.update(step, T, E, None, None)\n\n        # Attempt moves to new states\n        while step < self.steps and not self.user_exit:\n            step += 1\n            T = self.Tmax * math.exp(Tfactor * step / self.steps)\n            self.move()\n            E = self.energy()\n            dE = E - prevEnergy\n            trials += 1\n            if dE > 0.0 and math.exp(-dE / T) < random.random():\n                # Restore previous state\n                self.state = self.copy_state(prevState)\n                E = prevEnergy\n            else:\n                # Accept new state and compare to best state\n                accepts += 1\n                if dE < 0.0:\n                    improves += 1\n                prevState = self.copy_state(self.state)\n                prevEnergy = E\n                if E < self.best_energy:\n                    self.best_state = self.copy_state(self.state)\n                    self.best_energy = E\n            if self.updates > 1:\n                if (step // updateWavelength) > ((step - 1) // updateWavelength):\n                    self.update(\n                        step, T, E, accepts / trials, improves / trials)\n                    trials, accepts, improves = 0, 0, 0\n\n        self.state = self.copy_state(self.best_state)\n        if self.save_state_on_exit:\n            self.save_state()\n\n        # Return best state and energy\n        return self.best_state, self.best_energy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef auto(self, minutes, steps=2000):\n\n        def run(T, steps):\n            \"\"\"Anneals a system at constant temperature and returns the state,\n            energy, rate of acceptance, and rate of improvement.\"\"\"\n            E = self.energy()\n            prevState = self.copy_state(self.state)\n            prevEnergy = E\n            accepts, improves = 0, 0\n            for _ in range(steps):\n                self.move()\n                E = self.energy()\n                dE = E - prevEnergy\n                if dE > 0.0 and math.exp(-dE / T) < random.random():\n                    self.state = self.copy_state(prevState)\n                    E = prevEnergy\n                else:\n                    accepts += 1\n                    if dE < 0.0:\n                        improves += 1\n                    prevState = self.copy_state(self.state)\n                    prevEnergy = E\n            return E, float(accepts) / steps, float(improves) / steps\n\n        step = 0\n        self.start = time.time()\n\n        # Attempting automatic simulated anneal...\n        # Find an initial guess for temperature\n        T = 0.0\n        E = self.energy()\n        self.update(step, T, E, None, None)\n        while T == 0.0:\n            step += 1\n            self.move()\n            T = abs(self.energy() - E)\n\n        # Search for Tmax - a temperature that gives 98% acceptance\n        E, acceptance, improvement = run(T, steps)\n\n        step += steps\n        while acceptance > 0.98:\n            T = round_figures(T / 1.5, 2)\n            E, acceptance, improvement = run(T, steps)\n            step += steps\n            self.update(step, T, E, acceptance, improvement)\n        while acceptance < 0.98:\n            T = round_figures(T * 1.5, 2)\n            E, acceptance, improvement = run(T, steps)\n            step += steps\n            self.update(step, T, E, acceptance, improvement)\n        Tmax = T\n\n        # Search for Tmin - a temperature that gives 0% improvement\n        while improvement > 0.0:\n            T = round_figures(T / 1.5, 2)\n            E, acceptance, improvement = run(T, steps)\n            step += steps\n            self.update(step, T, E, acceptance, improvement)\n        Tmin = T\n\n        # Calculate anneal duration\n        elapsed = time.time() - self.start\n        duration = round_figures(int(60.0 * minutes * step / elapsed), 2)\n\n        # Don't perform anneal, just return params\n        return {'tmax': Tmax, 'tmin': Tmin, 'steps': duration, 'updates': self.updates}", "response": "Auto - simulated annealing landscape and animatinals a system at a constant temperature."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, shapefile=None):\r\n        if shapefile:\r\n            (shapeName, ext) = os.path.splitext(shapefile)\r\n            self.shapeName = shapeName\r\n            try:\r\n                self.shp = open(\"%s.shp\" % shapeName, \"rb\")\r\n            except IOError:\r\n                raise ShapefileException(\"Unable to open %s.shp\" % shapeName)\r\n            try:\r\n                self.shx = open(\"%s.shx\" % shapeName, \"rb\")\r\n            except IOError:\r\n                raise ShapefileException(\"Unable to open %s.shx\" % shapeName)\r\n            try:\r\n                self.dbf = open(\"%s.dbf\" % shapeName, \"rb\")\r\n            except IOError:\r\n                raise ShapefileException(\"Unable to open %s.dbf\" % shapeName)\r\n        if self.shp:\r\n            self.__shpHeader()\r\n        if self.dbf:\r\n            self.__dbfHeader()", "response": "Opens a shapefile from a filename or file - like object. Normally this method would be called by the\r\n        constructor with the file object or file name as an argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __shpHeader(self):\r\n        if not self.shp:\r\n            raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object. (no shp file found\")\r\n        shp = self.shp\r\n        # File length (16-bit word * 2 = bytes)\r\n        shp.seek(24)\r\n        self.shpLength = unpack(\">i\", shp.read(4))[0] * 2\r\n        # Shape type\r\n        shp.seek(32)\r\n        self.shapeType= unpack(\"<i\", shp.read(4))[0]\r\n        # The shapefile's bounding box (lower left, upper right)\r\n        self.bbox = _Array('d', unpack(\"<4d\", shp.read(32)))\r\n        # Elevation\r\n        self.elevation = _Array('d', unpack(\"<2d\", shp.read(16)))\r\n        # Measure\r\n        self.measure = _Array('d', unpack(\"<2d\", shp.read(16)))", "response": "Reads the header information from a. shp or. shx file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __shape(self):\r\n        f = self.__getFileObj(self.shp)\r\n        record = _Shape()\r\n        nParts = nPoints = zmin = zmax = mmin = mmax = None\r\n        (recNum, recLength) = unpack(\">2i\", f.read(8))\r\n        shapeType = unpack(\"<i\", f.read(4))[0]\r\n        record.shapeType = shapeType\r\n        # For Null shapes create an empty points list for consistency\r\n        if shapeType == 0:\r\n            record.points = []\r\n        # All shape types capable of having a bounding box\r\n        elif shapeType in (3,5,8,13,15,18,23,25,28,31):\r\n            record.bbox = _Array('d', unpack(\"<4d\", f.read(32)))\r\n        # Shape types with parts\r\n        if shapeType in (3,5,13,15,23,25,31):\r\n            nParts = unpack(\"<i\", f.read(4))[0]\r\n        # Shape types with points\r\n        if shapeType in (3,5,8,13,15,23,25,31):\r\n            nPoints = unpack(\"<i\", f.read(4))[0]\r\n        # Read parts\r\n        if nParts:\r\n            record.parts = _Array('i', unpack(\"<%si\" % nParts, f.read(nParts * 4)))\r\n        # Read part types for Multipatch - 31\r\n        if shapeType == 31:\r\n            record.partTypes = _Array('i', unpack(\"<%si\" % nParts, f.read(nParts * 4)))\r\n        # Read points - produces a list of [x,y] values\r\n        if nPoints:\r\n            record.points = [_Array('d', unpack(\"<2d\", f.read(16))) for p in range(nPoints)]\r\n        # Read z extremes and values\r\n        if shapeType in (13,15,18,31):\r\n            (zmin, zmax) = unpack(\"<2d\", f.read(16))\r\n            record.z = _Array('d', unpack(\"<%sd\" % nPoints, f.read(nPoints * 8)))\r\n        # Read m extremes and values\r\n        if shapeType in (13,15,18,23,25,28,31):\r\n            (mmin, mmax) = unpack(\"<2d\", f.read(16))\r\n            # Measure values less than -10e38 are nodata values according to the spec\r\n            record.m = []\r\n            for m in _Array('d', unpack(\"%sd\" % nPoints, f.read(nPoints * 8))):\r\n                if m > -10e38:\r\n                    record.m.append(m)\r\n                else:\r\n                    record.m.append(None)\r\n        # Read a single point\r\n        if shapeType in (1,11,21):\r\n            record.points = [_Array('d', unpack(\"<2d\", f.read(16)))]\r\n        # Read a single Z value\r\n        if shapeType == 11:\r\n            record.z = unpack(\"<d\", f.read(8))\r\n        # Read a single M value\r\n        if shapeType in (11,21):\r\n            record.m = unpack(\"<d\", f.read(8))\r\n        return record", "response": "Returns the header info and geometry for a single shape."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __shapeIndex(self, i=None):\r\n        shx = self.shx\r\n        if not shx:\r\n            return None\r\n        if not self._offsets:\r\n            # File length (16-bit word * 2 = bytes) - header length\r\n            shx.seek(24)\r\n            shxRecordLength = (unpack(\">i\", shx.read(4))[0] * 2) - 100\r\n            numRecords = shxRecordLength // 8\r\n            # Jump to the first record.\r\n            shx.seek(100)\r\n            for r in range(numRecords):\r\n                # Offsets are 16-bit words just like the file length\r\n                self._offsets.append(unpack(\">i\", shx.read(4))[0] * 2)\r\n                shx.seek(shx.tell() + 4)\r\n        if not i == None:\r\n            return self._offsets[i]", "response": "Returns the offset in a. shp file for a shape based on information\r\n            in the. shx index file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shape(self, i=0):\r\n        shp = self.__getFileObj(self.shp)\r\n        i = self.__restrictIndex(i)\r\n        offset = self.__shapeIndex(i)\r\n        if not offset:\r\n            # Shx index not available so use the full list.\r\n            shapes = self.shapes()\r\n            return shapes[i]\r\n        shp.seek(offset)\r\n        return self.__shape()", "response": "Returns a shape object for a shape in the geometry\r\n record file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shapes(self):\r\n        shp = self.__getFileObj(self.shp)\r\n        shp.seek(100)\r\n        shapes = []\r\n        while shp.tell() < self.shpLength:\r\n            shapes.append(self.__shape())\r\n        return shapes", "response": "Returns all the shapes in a shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __dbfHeaderLength(self):\r\n        if not self.__dbfHdrLength:\r\n            if not self.dbf:\r\n                raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object. (no dbf file found)\")\r\n            dbf = self.dbf\r\n            (self.numRecords, self.__dbfHdrLength) = \\\r\n                    unpack(\"<xxxxLH22x\", dbf.read(32))\r\n        return self.__dbfHdrLength", "response": "Retrieves the length of a dbf file header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a single DBF header. Xbase - related code borrows heavily from ActiveState Python Cookbook Recipe 362715 by Raymond Hettinger", "response": "def __dbfHeader(self):\r\n        \"\"\"Reads a dbf header. Xbase-related code borrows heavily from ActiveState Python Cookbook Recipe 362715 by Raymond Hettinger\"\"\"\r\n        if not self.dbf:\r\n            raise ShapefileException(\"Shapefile Reader requires a shapefile or file-like object. (no dbf file found)\")\r\n        dbf = self.dbf\r\n        headerLength = self.__dbfHeaderLength()\r\n        numFields = (headerLength - 33) // 32\r\n        for field in range(numFields):\r\n            fieldDesc = list(unpack(\"<11sc4xBB14x\", dbf.read(32)))\r\n            name = 0\r\n            idx = 0\r\n            if b(\"\\x00\") in fieldDesc[name]:\r\n                idx = fieldDesc[name].index(b(\"\\x00\"))\r\n            else:\r\n                idx = len(fieldDesc[name]) - 1\r\n            fieldDesc[name] = fieldDesc[name][:idx]\r\n            fieldDesc[name] = u(fieldDesc[name])\r\n            fieldDesc[name] = fieldDesc[name].lstrip()\r\n            fieldDesc[1] = u(fieldDesc[1])\r\n            self.fields.append(fieldDesc)\r\n        terminator = dbf.read(1)\r\n        assert terminator == b(\"\\r\")\r\n        self.fields.insert(0, ('DeletionFlag', 'C', 1, 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the size of a. shp geometry record.", "response": "def __recordFmt(self):\r\n        \"\"\"Calculates the size of a .shp geometry record.\"\"\"\r\n        if not self.numRecords:\r\n            self.__dbfHeader()\r\n        fmt = ''.join(['%ds' % fieldinfo[2] for fieldinfo in self.fields])\r\n        fmtSize = calcsize(fmt)\r\n        return (fmt, fmtSize)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __record(self):\r\n        f = self.__getFileObj(self.dbf)\r\n        recFmt = self.__recordFmt()\r\n        recordContents = unpack(recFmt[0], f.read(recFmt[1]))\r\n        if recordContents[0] != b(' '):\r\n            # deleted record\r\n            return None\r\n        record = []\r\n        for (name, typ, size, deci), value in zip(self.fields,\r\n                                                                                                recordContents):\r\n            if name == 'DeletionFlag':\r\n                continue\r\n            elif not value.strip():\r\n                record.append(value)\r\n                continue\r\n            elif typ == \"N\":\r\n                value = value.replace(b('\\0'), b('')).strip()\r\n                if value == b(''):\r\n                    value = 0\r\n                elif deci:\r\n                    try:\r\n                        value = float(value)\r\n                    except ValueError:\r\n                        value = 0\r\n                else:\r\n                    value = int(value)\r\n            elif typ == b('D'):\r\n                try:\r\n                    y, m, d = int(value[:4]), int(value[4:6]), int(value[6:8])\r\n                    value = [y, m, d]\r\n                except:\r\n                    value = value.strip()\r\n            elif typ == b('L'):\r\n                value = (value in b('YyTt') and b('T')) or \\\r\n                                        (value in b('NnFf') and b('F')) or b('?')\r\n            else:\r\n                value = u(value)\r\n                value = value.strip()\r\n            record.append(value)\r\n        return record", "response": "Reads and returns a dbf record row as a list of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef record(self, i=0):\r\n        f = self.__getFileObj(self.dbf)\r\n        if not self.numRecords:\r\n            self.__dbfHeader()\r\n        i = self.__restrictIndex(i)\r\n        recSize = self.__recordFmt()[1]\r\n        f.seek(0)\r\n        f.seek(self.__dbfHeaderLength() + (i * recSize))\r\n        return self.__record()", "response": "Returns a specific dbf record based on the supplied index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef records(self):\r\n        if not self.numRecords:\r\n            self.__dbfHeader()\r\n        records = []\r\n        f = self.__getFileObj(self.dbf)\r\n        f.seek(self.__dbfHeaderLength())\r\n        for i in range(self.numRecords):\r\n            r = self.__record()\r\n            if r:\r\n                records.append(r)\r\n        return records", "response": "Returns all records in a dbf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shapeRecord(self, i=0):\r\n        i = self.__restrictIndex(i)\r\n        return _ShapeRecord(shape=self.shape(i),\r\n                                                        record=self.record(i))", "response": "Returns a combination geometry and attribute record for the\r\n        supplied record index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shapeRecords(self):\r\n        shapeRecords = []\r\n        return [_ShapeRecord(shape=rec[0], record=rec[1]) \\\r\n                                for rec in zip(self.shapes(), self.records())]", "response": "Returns a list of combination geometry or attribute records for\r\n        all records in a shapefile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __shpFileLength(self):\r\n        # Start with header length\r\n        size = 100\r\n        # Calculate size of all shapes\r\n        for s in self._shapes:\r\n            # Add in record header and shape type fields\r\n            size += 12\r\n            # nParts and nPoints do not apply to all shapes\r\n            #if self.shapeType not in (0,1):\r\n            #       nParts = len(s.parts)\r\n            #       nPoints = len(s.points)\r\n            if hasattr(s,'parts'):\r\n                nParts = len(s.parts)\r\n            if hasattr(s,'points'):\r\n                nPoints = len(s.points)\r\n            # All shape types capable of having a bounding box\r\n            if self.shapeType in (3,5,8,13,15,18,23,25,28,31):\r\n                size += 32\r\n            # Shape types with parts\r\n            if self.shapeType in (3,5,13,15,23,25,31):\r\n                # Parts count\r\n                size += 4\r\n                # Parts index array\r\n                size += nParts * 4\r\n            # Shape types with points\r\n            if self.shapeType in (3,5,8,13,15,23,25,31):\r\n                # Points count\r\n                size += 4\r\n                # Points array\r\n                size += 16 * nPoints\r\n            # Calc size of part types for Multipatch (31)\r\n            if self.shapeType == 31:\r\n                size += nParts * 4\r\n            # Calc z extremes and values\r\n            if self.shapeType in (13,15,18,31):\r\n                # z extremes\r\n                size += 16\r\n                # z array\r\n                size += 8 * nPoints\r\n            # Calc m extremes and values\r\n            if self.shapeType in (23,25,31):\r\n                # m extremes\r\n                size += 16\r\n                # m array\r\n                size += 8 * nPoints\r\n            # Calc a single point\r\n            if self.shapeType in (1,11,21):\r\n                size += 16\r\n            # Calc a single Z value\r\n            if self.shapeType == 11:\r\n                size += 8\r\n            # Calc a single M value\r\n            if self.shapeType in (11,21):\r\n                size += 8\r\n        # Calculate size as 16-bit words\r\n        size //= 2\r\n        return size", "response": "Calculates the length of the shp file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the specified header type to the specified file - like object.", "response": "def __shapefileHeader(self, fileObj, headerType='shp'):\r\n        \"\"\"Writes the specified header type to the specified file-like object.\r\n        Several of the shapefile formats are so similar that a single generic\r\n        method to read or write them is warranted.\"\"\"\r\n        f = self.__getFileObj(fileObj)\r\n        f.seek(0)\r\n        # File code, Unused bytes\r\n        f.write(pack(\">6i\", 9994,0,0,0,0,0))\r\n        # File length (Bytes / 2 = 16-bit words)\r\n        if headerType == 'shp':\r\n            f.write(pack(\">i\", self.__shpFileLength()))\r\n        elif headerType == 'shx':\r\n            f.write(pack('>i', ((100 + (len(self._shapes) * 8)) // 2)))\r\n        # Version, Shape type\r\n        f.write(pack(\"<2i\", 1000, self.shapeType))\r\n        # The shapefile's bounding box (lower left, upper right)\r\n        if self.shapeType != 0:\r\n            try:\r\n                f.write(pack(\"<4d\", *self.bbox()))\r\n            except error:\r\n                raise ShapefileException(\"Failed to write shapefile bounding box. Floats required.\")\r\n        else:\r\n            f.write(pack(\"<4d\", 0,0,0,0))\r\n        # Elevation\r\n        z = self.zbox()\r\n        # Measure\r\n        m = self.mbox()\r\n        try:\r\n            f.write(pack(\"<4d\", z[0], z[1], m[0], m[1]))\r\n        except error:\r\n            raise ShapefileException(\"Failed to write shapefile elevation and measure values. Floats required.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __dbfHeader(self):\r\n        f = self.__getFileObj(self.dbf)\r\n        f.seek(0)\r\n        version = 3\r\n        year, month, day = time.localtime()[:3]\r\n        year -= 1900\r\n        # Remove deletion flag placeholder from fields\r\n        for field in self.fields:\r\n            if field[0].startswith(\"Deletion\"):\r\n                self.fields.remove(field)\r\n        numRecs = len(self.records)\r\n        numFields = len(self.fields)\r\n        headerLength = numFields * 32 + 33\r\n        recordLength = sum([int(field[2]) for field in self.fields]) + 1\r\n        header = pack('<BBBBLHH20x', version, year, month, day, numRecs,\r\n                headerLength, recordLength)\r\n        f.write(header)\r\n        # Field descriptors\r\n        for field in self.fields:\r\n            name, fieldType, size, decimal = field\r\n            name = b(name)\r\n            name = name.replace(b(' '), b('_'))\r\n            name = name.ljust(11).replace(b(' '), b('\\x00'))\r\n            fieldType = b(fieldType)\r\n            size = int(size)\r\n            fld = pack('<11sc4xBB14x', name, fieldType, size, decimal)\r\n            f.write(fld)\r\n        # Terminator\r\n        f.write(b('\\r'))", "response": "Writes the dbf header and field descriptors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the shp records.", "response": "def __shpRecords(self):\r\n        \"\"\"Write the shp records\"\"\"\r\n        f = self.__getFileObj(self.shp)\r\n        f.seek(100)\r\n        recNum = 1\r\n        for s in self._shapes:\r\n            self._offsets.append(f.tell())\r\n            # Record number, Content length place holder\r\n            f.write(pack(\">2i\", recNum, 0))\r\n            recNum += 1\r\n            start = f.tell()\r\n            # Shape Type\r\n            f.write(pack(\"<i\", s.shapeType))\r\n            # All shape types capable of having a bounding box\r\n            if s.shapeType in (3,5,8,13,15,18,23,25,28,31):\r\n                try:\r\n                    f.write(pack(\"<4d\", *self.__bbox([s])))\r\n                except error:\r\n                    raise ShapefileException(\"Falied to write bounding box for record %s. Expected floats.\" % recNum)\r\n            # Shape types with parts\r\n            if s.shapeType in (3,5,13,15,23,25,31):\r\n                # Number of parts\r\n                f.write(pack(\"<i\", len(s.parts)))\r\n            # Shape types with multiple points per record\r\n            if s.shapeType in (3,5,8,13,15,23,25,31):\r\n                # Number of points\r\n                f.write(pack(\"<i\", len(s.points)))\r\n            # Write part indexes\r\n            if s.shapeType in (3,5,13,15,23,25,31):\r\n                for p in s.parts:\r\n                    f.write(pack(\"<i\", p))\r\n            # Part types for Multipatch (31)\r\n            if s.shapeType == 31:\r\n                for pt in s.partTypes:\r\n                    f.write(pack(\"<i\", pt))\r\n            # Write points for multiple-point records\r\n            if s.shapeType in (3,5,8,13,15,23,25,31):\r\n                try:\r\n                    [f.write(pack(\"<2d\", *p[:2])) for p in s.points]\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write points for record %s. Expected floats.\" % recNum)\r\n            # Write z extremes and values\r\n            if s.shapeType in (13,15,18,31):\r\n                try:\r\n                    f.write(pack(\"<2d\", *self.__zbox([s])))\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write elevation extremes for record %s. Expected floats.\" % recNum)\r\n                try:\r\n                    [f.write(pack(\"<d\", p[2])) for p in s.points]\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write elevation values for record %s. Expected floats.\" % recNum)\r\n            # Write m extremes and values\r\n            if s.shapeType in (23,25,31):\r\n                try:\r\n                    f.write(pack(\"<2d\", *self.__mbox([s])))\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write measure extremes for record %s. Expected floats\" % recNum)\r\n                try:\r\n                    [f.write(pack(\"<d\", p[3])) for p in s.points]\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write measure values for record %s. Expected floats\" % recNum)\r\n            # Write a single point\r\n            if s.shapeType in (1,11,21):\r\n                try:\r\n                    f.write(pack(\"<2d\", s.points[0][0], s.points[0][1]))\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write point for record %s. Expected floats.\" % recNum)\r\n            # Write a single Z value\r\n            if s.shapeType == 11:\r\n                try:\r\n                    f.write(pack(\"<1d\", s.points[0][2]))\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write elevation value for record %s. Expected floats.\" % recNum)\r\n            # Write a single M value\r\n            if s.shapeType in (11,21):\r\n                try:\r\n                    f.write(pack(\"<1d\", s.points[0][3]))\r\n                except error:\r\n                    raise ShapefileException(\"Failed to write measure value for record %s. Expected floats.\" % recNum)\r\n            # Finalize record length as 16-bit words\r\n            finish = f.tell()\r\n            length = (finish - start) // 2\r\n            self._lengths.append(length)\r\n            # start - 4 bytes is the content length field\r\n            f.seek(start-4)\r\n            f.write(pack(\">i\", length))\r\n            f.seek(finish)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __shxRecords(self):\r\n        f = self.__getFileObj(self.shx)\r\n        f.seek(100)\r\n        for i in range(len(self._shapes)):\r\n            f.write(pack(\">i\", self._offsets[i] // 2))\r\n            f.write(pack(\">i\", self._lengths[i]))", "response": "Writes the shx records."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __dbfRecords(self):\r\n        f = self.__getFileObj(self.dbf)\r\n        for record in self.records:\r\n            if not self.fields[0][0].startswith(\"Deletion\"):\r\n                f.write(b(' ')) # deletion flag\r\n            for (fieldName, fieldType, size, dec), value in zip(self.fields, record):\r\n                fieldType = fieldType.upper()\r\n                size = int(size)\r\n                if fieldType.upper() == \"N\":\r\n                    value = str(value).rjust(size)\r\n                elif fieldType == 'L':\r\n                    value = str(value)[0].upper()\r\n                else:\r\n                    value = str(value)[:size].ljust(size)\r\n                assert len(value) == size\r\n                value = b(value)\r\n                f.write(value)", "response": "Writes the dbf records."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef point(self, x, y, z=0, m=0):\r\n        pointShape = _Shape(self.shapeType)\r\n        pointShape.points.append([x, y, z, m])\r\n        self._shapes.append(pointShape)", "response": "Creates a point shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new polygon shape that contains multiple collections of points and even multipoints.", "response": "def poly(self, parts=[], shapeType=POLYGON, partTypes=[]):\r\n        \"\"\"Creates a shape that has multiple collections of points (parts)\r\n        including lines, polygons, and even multipoint shapes. If no shape type\r\n        is specified it defaults to 'polygon'. If no part types are specified\r\n        (which they normally won't be) then all parts default to the shape type.\r\n        \"\"\"\r\n        polyShape = _Shape(shapeType)\r\n        polyShape.parts = []\r\n        polyShape.points = []\r\n        for part in parts:\r\n            polyShape.parts.append(len(polyShape.points))\r\n            for point in part:\r\n                # Ensure point is list\r\n                if not isinstance(point, list):\r\n                    point = list(point)\r\n                # Make sure point has z and m values\r\n                while len(point) < 4:\r\n                    point.append(0)\r\n                polyShape.points.append(point)\r\n        if polyShape.shapeType == 31:\r\n            if not partTypes:\r\n                for part in parts:\r\n                    partTypes.append(polyShape.shapeType)\r\n            polyShape.partTypes = partTypes\r\n        self._shapes.append(polyShape)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef field(self, name, fieldType=\"C\", size=\"50\", decimal=0):\r\n        self.fields.append((name, fieldType, size, decimal))", "response": "Adds a dbf field descriptor to the shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new dbf attribute record.", "response": "def record(self, *recordList, **recordDict):\r\n        \"\"\"Creates a dbf attribute record. You can submit either a sequence of\r\n        field values or keyword arguments of field names and values. Before\r\n        adding records you must add fields for the record values using the\r\n        fields() method. If the record values exceed the number of fields the\r\n        extra ones won't be added. In the case of using keyword arguments to specify\r\n        field/value pairs only fields matching the already registered fields\r\n        will be added.\"\"\"\r\n        record = []\r\n        fieldCount = len(self.fields)\r\n        # Compensate for deletion flag\r\n        if self.fields[0][0].startswith(\"Deletion\"): fieldCount -= 1\r\n        if recordList:\r\n            [record.append(recordList[i]) for i in range(fieldCount)]\r\n        elif recordDict:\r\n            for field in self.fields:\r\n                if field[0] in recordDict:\r\n                    val = recordDict[field[0]]\r\n                    if val:\r\n                        record.append(val)\r\n                    else:\r\n                        record.append(\"\")\r\n        if record:\r\n            self.records.append(record)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef saveShp(self, target):\r\n        if not hasattr(target, \"write\"):\r\n            target = os.path.splitext(target)[0] + '.shp'\r\n        if not self.shapeType:\r\n            self.shapeType = self._shapes[0].shapeType\r\n        self.shp = self.__getFileObj(target)\r\n        self.__shapefileHeader(self.shp, headerType='shp')\r\n        self.__shpRecords()", "response": "Save an shp file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef saveShx(self, target):\r\n        if not hasattr(target, \"write\"):\r\n            target = os.path.splitext(target)[0] + '.shx'\r\n        if not self.shapeType:\r\n            self.shapeType = self._shapes[0].shapeType\r\n        self.shx = self.__getFileObj(target)\r\n        self.__shapefileHeader(self.shx, headerType='shx')\r\n        self.__shxRecords()", "response": "Save an shx file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef saveDbf(self, target):\r\n        if not hasattr(target, \"write\"):\r\n            target = os.path.splitext(target)[0] + '.dbf'\r\n        self.dbf = self.__getFileObj(target)\r\n        self.__dbfHeader()\r\n        self.__dbfRecords()", "response": "Save a dbf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, target=None, shp=None, shx=None, dbf=None):\r\n        # TODO: Create a unique filename for target if None.\r\n        if shp:\r\n            self.saveShp(shp)\r\n        if shx:\r\n            self.saveShx(shx)\r\n        if dbf:\r\n            self.saveDbf(dbf)\r\n        elif target:\r\n            self.saveShp(target)\r\n            self.shp.close()\r\n            self.saveShx(target)\r\n            self.shx.close()\r\n            self.saveDbf(target)\r\n            self.dbf.close()", "response": "Save the shapefile data to three files or three file - like objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the specified part of any shape by specifying a shapeType part number or point number.", "response": "def delete(self, shape=None, part=None, point=None):\r\n        \"\"\"Deletes the specified part of any shape by specifying a shape\r\n        number, part number, or point number.\"\"\"\r\n        # shape, part, point\r\n        if shape and part and point:\r\n            del self._shapes[shape][part][point]\r\n        # shape, part\r\n        elif shape and part and not point:\r\n            del self._shapes[shape][part]\r\n        # shape\r\n        elif shape and not part and not point:\r\n            del self._shapes[shape]\r\n        # point\r\n        elif not shape and not part and point:\r\n            for s in self._shapes:\r\n                if s.shapeType == 1:\r\n                    del self._shapes[point]\r\n                else:\r\n                    for part in s.parts:\r\n                        del s[part][point]\r\n        # part, point\r\n        elif not shape and part and point:\r\n            for s in self._shapes:\r\n                del s[part][point]\r\n        # part\r\n        elif not shape and part and not point:\r\n            for s in self._shapes:\r\n                del s[part]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef point(self, x=None, y=None, z=None, m=None, shape=None, part=None, point=None, addr=None):\r\n        # shape, part, point\r\n        if shape and part and point:\r\n            try: self._shapes[shape]\r\n            except IndexError: self._shapes.append([])\r\n            try: self._shapes[shape][part]\r\n            except IndexError: self._shapes[shape].append([])\r\n            try: self._shapes[shape][part][point]\r\n            except IndexError: self._shapes[shape][part].append([])\r\n            p = self._shapes[shape][part][point]\r\n            if x: p[0] = x\r\n            if y: p[1] = y\r\n            if z: p[2] = z\r\n            if m: p[3] = m\r\n            self._shapes[shape][part][point] = p\r\n        # shape, part\r\n        elif shape and part and not point:\r\n            try: self._shapes[shape]\r\n            except IndexError: self._shapes.append([])\r\n            try: self._shapes[shape][part]\r\n            except IndexError: self._shapes[shape].append([])\r\n            points = self._shapes[shape][part]\r\n            for i in range(len(points)):\r\n                p = points[i]\r\n                if x: p[0] = x\r\n                if y: p[1] = y\r\n                if z: p[2] = z\r\n                if m: p[3] = m\r\n                self._shapes[shape][part][i] = p\r\n        # shape\r\n        elif shape and not part and not point:\r\n            try: self._shapes[shape]\r\n            except IndexError: self._shapes.append([])\r\n\r\n        # point\r\n        # part\r\n        if addr:\r\n            shape, part, point = addr\r\n            self._shapes[shape][part][point] = [x, y, z, m]\r\n        else:\r\n            Writer.point(self, x, y, z, m)\r\n        if self.autoBalance:\r\n            self.balance()", "response": "Creates or updates a specific point shape. The arguments allows for updating a specific point by shape part point of any\r\n            shape type. The arguments allows for updating a specific point by shape part point of any\r\n            shape type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef balance(self):\r\n        if len(self.records) > len(self._shapes):\r\n            self.null()\r\n        elif len(self.records) < len(self._shapes):\r\n            self.record()", "response": "Adds a corresponding empty attribute or null geometry record depending on which type of record was created."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __fieldNorm(self, fieldName):\r\n        if len(fieldName) > 11: fieldName = fieldName[:11]\r\n        fieldName = fieldName.upper()\r\n        fieldName.replace(' ', '_')", "response": "Normalizes a dbf field name to fit within the spec and the\r\n        expectations of certain ESRI software."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diff_main(self, text1, text2, checklines=True, deadline=None):\n    # Set a deadline by which time the diff must be complete.\n    if deadline == None:\n      # Unlike in most languages, Python counts time in seconds.\n      if self.Diff_Timeout <= 0:\n        deadline = sys.maxsize\n      else:\n        deadline = time.time() + self.Diff_Timeout\n\n    # Check for null inputs.\n    if text1 == None or text2 == None:\n      raise ValueError(\"Null inputs. (diff_main)\")\n\n    # Check for equality (speedup).\n    if text1 == text2:\n      if text1:\n        return [(self.DIFF_EQUAL, text1)]\n      return []\n\n    # Trim off common prefix (speedup).\n    commonlength = self.diff_commonPrefix(text1, text2)\n    commonprefix = text1[:commonlength]\n    text1 = text1[commonlength:]\n    text2 = text2[commonlength:]\n\n    # Trim off common suffix (speedup).\n    commonlength = self.diff_commonSuffix(text1, text2)\n    if commonlength == 0:\n      commonsuffix = ''\n    else:\n      commonsuffix = text1[-commonlength:]\n      text1 = text1[:-commonlength]\n      text2 = text2[:-commonlength]\n\n    # Compute the diff on the middle block.\n    diffs = self.diff_compute(text1, text2, checklines, deadline)\n\n    # Restore the prefix and suffix.\n    if commonprefix:\n      diffs[:0] = [(self.DIFF_EQUAL, commonprefix)]\n    if commonsuffix:\n      diffs.append((self.DIFF_EQUAL, commonsuffix))\n    self.diff_cleanupMerge(diffs)\n    return diffs", "response": "Simplifies the problem by finding the differences between two texts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the differences between two texts.", "response": "def diff_compute(self, text1, text2, checklines, deadline):\n    \"\"\"Find the differences between two texts.  Assumes that the texts do not\n      have any common prefix or suffix.\n\n    Args:\n      text1: Old string to be diffed.\n      text2: New string to be diffed.\n      checklines: Speedup flag.  If false, then don't run a line-level diff\n        first to identify the changed areas.\n        If true, then run a faster, slightly less optimal diff.\n      deadline: Time when the diff should be complete by.\n\n    Returns:\n      Array of changes.\n    \"\"\"\n    if not text1:\n      # Just add some text (speedup).\n      return [(self.DIFF_INSERT, text2)]\n\n    if not text2:\n      # Just delete some text (speedup).\n      return [(self.DIFF_DELETE, text1)]\n\n    if len(text1) > len(text2):\n      (longtext, shorttext) = (text1, text2)\n    else:\n      (shorttext, longtext) = (text1, text2)\n    i = longtext.find(shorttext)\n    if i != -1:\n      # Shorter text is inside the longer text (speedup).\n      diffs = [(self.DIFF_INSERT, longtext[:i]), (self.DIFF_EQUAL, shorttext),\n               (self.DIFF_INSERT, longtext[i + len(shorttext):])]\n      # Swap insertions for deletions if diff is reversed.\n      if len(text1) > len(text2):\n        diffs[0] = (self.DIFF_DELETE, diffs[0][1])\n        diffs[2] = (self.DIFF_DELETE, diffs[2][1])\n      return diffs\n\n    if len(shorttext) == 1:\n      # Single character string.\n      # After the previous speedup, the character can't be an equality.\n      return [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]\n\n    # Check to see if the problem can be split in two.\n    hm = self.diff_halfMatch(text1, text2)\n    if hm:\n      # A half-match was found, sort out the return data.\n      (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\n      # Send both pairs off for separate processing.\n      diffs_a = self.diff_main(text1_a, text2_a, checklines, deadline)\n      diffs_b = self.diff_main(text1_b, text2_b, checklines, deadline)\n      # Merge the results.\n      return diffs_a + [(self.DIFF_EQUAL, mid_common)] + diffs_b\n\n    if checklines and len(text1) > 100 and len(text2) > 100:\n      return self.diff_lineMode(text1, text2, deadline)\n\n    return self.diff_bisect(text1, text2, deadline)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo a quick line - level diff on both strings and return a list of changes.", "response": "def diff_lineMode(self, text1, text2, deadline):\n    \"\"\"Do a quick line-level diff on both strings, then rediff the parts for\n      greater accuracy.\n      This speedup can produce non-minimal diffs.\n\n    Args:\n      text1: Old string to be diffed.\n      text2: New string to be diffed.\n      deadline: Time when the diff should be complete by.\n\n    Returns:\n      Array of changes.\n    \"\"\"\n\n    # Scan the text on a line-by-line basis first.\n    (text1, text2, linearray) = self.diff_linesToChars(text1, text2)\n\n    diffs = self.diff_main(text1, text2, False, deadline)\n\n    # Convert the diff back to original text.\n    self.diff_charsToLines(diffs, linearray)\n    # Eliminate freak matches (e.g. blank lines)\n    self.diff_cleanupSemantic(diffs)\n\n    # Rediff any replacement blocks, this time character-by-character.\n    # Add a dummy entry at the end.\n    diffs.append((self.DIFF_EQUAL, ''))\n    pointer = 0\n    count_delete = 0\n    count_insert = 0\n    text_delete = ''\n    text_insert = ''\n    while pointer < len(diffs):\n      if diffs[pointer][0] == self.DIFF_INSERT:\n        count_insert += 1\n        text_insert += diffs[pointer][1]\n      elif diffs[pointer][0] == self.DIFF_DELETE:\n        count_delete += 1\n        text_delete += diffs[pointer][1]\n      elif diffs[pointer][0] == self.DIFF_EQUAL:\n        # Upon reaching an equality, check for prior redundancies.\n        if count_delete >= 1 and count_insert >= 1:\n          # Delete the offending records and add the merged ones.\n          subDiff = self.diff_main(text_delete, text_insert, False, deadline)\n          diffs[pointer - count_delete - count_insert : pointer] = subDiff\n          pointer = pointer - count_delete - count_insert + len(subDiff)\n        count_insert = 0\n        count_delete = 0\n        text_delete = ''\n        text_insert = ''\n\n      pointer += 1\n\n    diffs.pop()  # Remove the dummy entry at the end.\n\n    return diffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff_bisect(self, text1, text2, deadline):\n\n    # Cache the text lengths to prevent multiple calls.\n    text1_length = len(text1)\n    text2_length = len(text2)\n    max_d = (text1_length + text2_length + 1) // 2\n    v_offset = max_d\n    v_length = 2 * max_d\n    v1 = [-1] * v_length\n    v1[v_offset + 1] = 0\n    v2 = v1[:]\n    delta = text1_length - text2_length\n    # If the total number of characters is odd, then the front path will\n    # collide with the reverse path.\n    front = (delta % 2 != 0)\n    # Offsets for start and end of k loop.\n    # Prevents mapping of space beyond the grid.\n    k1start = 0\n    k1end = 0\n    k2start = 0\n    k2end = 0\n    for d in range(max_d):\n      # Bail out if deadline is reached.\n      if time.time() > deadline:\n        break\n\n      # Walk the front path one step.\n      for k1 in range(-d + k1start, d + 1 - k1end, 2):\n        k1_offset = v_offset + k1\n        if k1 == -d or (k1 != d and\n            v1[k1_offset - 1] < v1[k1_offset + 1]):\n          x1 = v1[k1_offset + 1]\n        else:\n          x1 = v1[k1_offset - 1] + 1\n        y1 = x1 - k1\n        while (x1 < text1_length and y1 < text2_length and\n               text1[x1] == text2[y1]):\n          x1 += 1\n          y1 += 1\n        v1[k1_offset] = x1\n        if x1 > text1_length:\n          # Ran off the right of the graph.\n          k1end += 2\n        elif y1 > text2_length:\n          # Ran off the bottom of the graph.\n          k1start += 2\n        elif front:\n          k2_offset = v_offset + delta - k1\n          if k2_offset >= 0 and k2_offset < v_length and v2[k2_offset] != -1:\n            # Mirror x2 onto top-left coordinate system.\n            x2 = text1_length - v2[k2_offset]\n            if x1 >= x2:\n              # Overlap detected.\n              return self.diff_bisectSplit(text1, text2, x1, y1, deadline)\n\n      # Walk the reverse path one step.\n      for k2 in range(-d + k2start, d + 1 - k2end, 2):\n        k2_offset = v_offset + k2\n        if k2 == -d or (k2 != d and\n            v2[k2_offset - 1] < v2[k2_offset + 1]):\n          x2 = v2[k2_offset + 1]\n        else:\n          x2 = v2[k2_offset - 1] + 1\n        y2 = x2 - k2\n        while (x2 < text1_length and y2 < text2_length and\n               text1[-x2 - 1] == text2[-y2 - 1]):\n          x2 += 1\n          y2 += 1\n        v2[k2_offset] = x2\n        if x2 > text1_length:\n          # Ran off the left of the graph.\n          k2end += 2\n        elif y2 > text2_length:\n          # Ran off the top of the graph.\n          k2start += 2\n        elif not front:\n          k1_offset = v_offset + delta - k2\n          if k1_offset >= 0 and k1_offset < v_length and v1[k1_offset] != -1:\n            x1 = v1[k1_offset]\n            y1 = v_offset + x1 - k1_offset\n            # Mirror x2 onto top-left coordinate system.\n            x2 = text1_length - x2\n            if x1 >= x2:\n              # Overlap detected.\n              return self.diff_bisectSplit(text1, text2, x1, y1, deadline)\n\n    # Diff took too long and hit the deadline or\n    # number of diffs equals number of characters, no commonality at all.\n    return [(self.DIFF_DELETE, text1), (self.DIFF_INSERT, text2)]", "response": "This function finds the middle snake of a diff and returns the recursively constructed diff."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the location of the middle snake split the diff in two parts and recurse.", "response": "def diff_bisectSplit(self, text1, text2, x, y, deadline):\n    \"\"\"Given the location of the 'middle snake', split the diff in two parts\n    and recurse.\n\n    Args:\n      text1: Old string to be diffed.\n      text2: New string to be diffed.\n      x: Index of split point in text1.\n      y: Index of split point in text2.\n      deadline: Time at which to bail if not yet complete.\n\n    Returns:\n      Array of diff tuples.\n    \"\"\"\n    text1a = text1[:x]\n    text2a = text2[:y]\n    text1b = text1[x:]\n    text2b = text2[y:]\n\n    # Compute both diffs serially.\n    diffs = self.diff_main(text1a, text2a, False, deadline)\n    diffsb = self.diff_main(text1b, text2b, False, deadline)\n\n    return diffs + diffsb"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_linesToChars(self, text1, text2):\n    lineArray = []  # e.g. lineArray[4] == \"Hello\\n\"\n    lineHash = {}   # e.g. lineHash[\"Hello\\n\"] == 4\n\n    # \"\\x00\" is a valid character, but various debuggers don't like it.\n    # So we'll insert a junk entry to avoid generating a null character.\n    lineArray.append('')\n\n    def diff_linesToCharsMunge(text):\n      \"\"\"Split a text into an array of strings.  Reduce the texts to a string\n      of hashes where each Unicode character represents one line.\n      Modifies linearray and linehash through being a closure.\n\n      Args:\n        text: String to encode.\n\n      Returns:\n        Encoded string.\n      \"\"\"\n      chars = []\n      # Walk the text, pulling out a substring for each line.\n      # text.split('\\n') would would temporarily double our memory footprint.\n      # Modifying text would create many large strings to garbage collect.\n      lineStart = 0\n      lineEnd = -1\n      while lineEnd < len(text) - 1:\n        lineEnd = text.find('\\n', lineStart)\n        if lineEnd == -1:\n          lineEnd = len(text) - 1\n        line = text[lineStart:lineEnd + 1]\n\n        if line in lineHash:\n          chars.append(chr(lineHash[line]))\n        else:\n          if len(lineArray) == maxLines:\n            # Bail out at 1114111 because chr(1114112) throws.\n            line = text[lineStart:]\n            lineEnd = len(text)\n          lineArray.append(line)\n          lineHash[line] = len(lineArray) - 1\n          chars.append(chr(len(lineArray) - 1))\n        lineStart = lineEnd + 1\n      return \"\".join(chars)\n\n    # Allocate 2/3rds of the space for text1, the rest for text2.\n    maxLines = 666666\n    chars1 = diff_linesToCharsMunge(text1)\n    maxLines = 1114111\n    chars2 = diff_linesToCharsMunge(text2)\n    return (chars1, chars2, lineArray)", "response": "This function takes two texts and returns a list of strings that are encoded by the diff_linesToChars method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diff_charsToLines(self, diffs, lineArray):\n    for i in range(len(diffs)):\n      text = []\n      for char in diffs[i][1]:\n        text.append(lineArray[ord(char)])\n      diffs[i] = (diffs[i][0], \"\".join(text))", "response": "Rehydrate the text in a diff from a string of line hashes to real lines\n    of text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diff_commonPrefix(self, text1, text2):\n    # Quick check for common null cases.\n    if not text1 or not text2 or text1[0] != text2[0]:\n      return 0\n    # Binary search.\n    # Performance analysis: https://neil.fraser.name/news/2007/10/09/\n    pointermin = 0\n    pointermax = min(len(text1), len(text2))\n    pointermid = pointermax\n    pointerstart = 0\n    while pointermin < pointermid:\n      if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n        pointermin = pointermid\n        pointerstart = pointermin\n      else:\n        pointermax = pointermid\n      pointermid = (pointermax - pointermin) // 2 + pointermin\n    return pointermid", "response": "Determine the common prefix of two strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diff_commonSuffix(self, text1, text2):\n    # Quick check for common null cases.\n    if not text1 or not text2 or text1[-1] != text2[-1]:\n      return 0\n    # Binary search.\n    # Performance analysis: https://neil.fraser.name/news/2007/10/09/\n    pointermin = 0\n    pointermax = min(len(text1), len(text2))\n    pointermid = pointermax\n    pointerend = 0\n    while pointermin < pointermid:\n      if (text1[-pointermid:len(text1) - pointerend] ==\n          text2[-pointermid:len(text2) - pointerend]):\n        pointermin = pointermid\n        pointerend = pointermin\n      else:\n        pointermax = pointermid\n      pointermid = (pointermax - pointermin) // 2 + pointermin\n    return pointermid", "response": "Determine the common suffix of two strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff_commonOverlap(self, text1, text2):\n    # Cache the text lengths to prevent multiple calls.\n    text1_length = len(text1)\n    text2_length = len(text2)\n    # Eliminate the null case.\n    if text1_length == 0 or text2_length == 0:\n      return 0\n    # Truncate the longer string.\n    if text1_length > text2_length:\n      text1 = text1[-text2_length:]\n    elif text1_length < text2_length:\n      text2 = text2[:text1_length]\n    text_length = min(text1_length, text2_length)\n    # Quick check for the worst case.\n    if text1 == text2:\n      return text_length\n\n    # Start by looking for a single character match\n    # and increase length until no match is found.\n    # Performance analysis: https://neil.fraser.name/news/2010/11/04/\n    best = 0\n    length = 1\n    while True:\n      pattern = text1[-length:]\n      found = text2.find(pattern)\n      if found == -1:\n        return best\n      length += found\n      if found == 0 or text1[-length:] == text2[:length]:\n        best = length\n        length += 1", "response": "Determine if the suffix of one string is the prefix of another string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diff_halfMatch(self, text1, text2):\n    if self.Diff_Timeout <= 0:\n      # Don't risk returning a non-optimal diff if we have unlimited time.\n      return None\n    if len(text1) > len(text2):\n      (longtext, shorttext) = (text1, text2)\n    else:\n      (shorttext, longtext) = (text1, text2)\n    if len(longtext) < 4 or len(shorttext) * 2 < len(longtext):\n      return None  # Pointless.\n\n    def diff_halfMatchI(longtext, shorttext, i):\n      \"\"\"Does a substring of shorttext exist within longtext such that the\n      substring is at least half the length of longtext?\n      Closure, but does not reference any external variables.\n\n      Args:\n        longtext: Longer string.\n        shorttext: Shorter string.\n        i: Start index of quarter length substring within longtext.\n\n      Returns:\n        Five element Array, containing the prefix of longtext, the suffix of\n        longtext, the prefix of shorttext, the suffix of shorttext and the\n        common middle.  Or None if there was no match.\n      \"\"\"\n      seed = longtext[i:i + len(longtext) // 4]\n      best_common = ''\n      j = shorttext.find(seed)\n      while j != -1:\n        prefixLength = self.diff_commonPrefix(longtext[i:], shorttext[j:])\n        suffixLength = self.diff_commonSuffix(longtext[:i], shorttext[:j])\n        if len(best_common) < suffixLength + prefixLength:\n          best_common = (shorttext[j - suffixLength:j] +\n              shorttext[j:j + prefixLength])\n          best_longtext_a = longtext[:i - suffixLength]\n          best_longtext_b = longtext[i + prefixLength:]\n          best_shorttext_a = shorttext[:j - suffixLength]\n          best_shorttext_b = shorttext[j + prefixLength:]\n        j = shorttext.find(seed, j + 1)\n\n      if len(best_common) * 2 >= len(longtext):\n        return (best_longtext_a, best_longtext_b,\n                best_shorttext_a, best_shorttext_b, best_common)\n      else:\n        return None\n\n    # First check if the second quarter is the seed for a half-match.\n    hm1 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 3) // 4)\n    # Check again based on the third quarter.\n    hm2 = diff_halfMatchI(longtext, shorttext, (len(longtext) + 1) // 2)\n    if not hm1 and not hm2:\n      return None\n    elif not hm2:\n      hm = hm1\n    elif not hm1:\n      hm = hm2\n    else:\n      # Both matched.  Select the longest.\n      if len(hm1[4]) > len(hm2[4]):\n        hm = hm1\n      else:\n        hm = hm2\n\n    # A half-match was found, sort out the return data.\n    if len(text1) > len(text2):\n      (text1_a, text1_b, text2_a, text2_b, mid_common) = hm\n    else:\n      (text2_a, text2_b, text1_a, text1_b, mid_common) = hm\n    return (text1_a, text1_b, text2_a, text2_b, mid_common)", "response": "This speedup can produce non - minimal diffs if the longest substring of text1 and text2 are at least half the length of text2. This speedup can produce non - minimal diffs if self. Diff_Timeout is not set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_cleanupSemantic(self, diffs):\n    changes = False\n    equalities = []  # Stack of indices where equalities are found.\n    lastEquality = None  # Always equal to diffs[equalities[-1]][1]\n    pointer = 0  # Index of current position.\n    # Number of chars that changed prior to the equality.\n    length_insertions1, length_deletions1 = 0, 0\n    # Number of chars that changed after the equality.\n    length_insertions2, length_deletions2 = 0, 0\n    while pointer < len(diffs):\n      if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality found.\n        equalities.append(pointer)\n        length_insertions1, length_insertions2 = length_insertions2, 0\n        length_deletions1, length_deletions2 = length_deletions2, 0\n        lastEquality = diffs[pointer][1]\n      else:  # An insertion or deletion.\n        if diffs[pointer][0] == self.DIFF_INSERT:\n          length_insertions2 += len(diffs[pointer][1])\n        else:\n          length_deletions2 += len(diffs[pointer][1])\n        # Eliminate an equality that is smaller or equal to the edits on both\n        # sides of it.\n        if (lastEquality and (len(lastEquality) <=\n            max(length_insertions1, length_deletions1)) and\n            (len(lastEquality) <= max(length_insertions2, length_deletions2))):\n          # Duplicate record.\n          diffs.insert(equalities[-1], (self.DIFF_DELETE, lastEquality))\n          # Change second copy to insert.\n          diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n              diffs[equalities[-1] + 1][1])\n          # Throw away the equality we just deleted.\n          equalities.pop()\n          # Throw away the previous equality (it needs to be reevaluated).\n          if len(equalities):\n            equalities.pop()\n          if len(equalities):\n            pointer = equalities[-1]\n          else:\n            pointer = -1\n          # Reset the counters.\n          length_insertions1, length_deletions1 = 0, 0\n          length_insertions2, length_deletions2 = 0, 0\n          lastEquality = None\n          changes = True\n      pointer += 1\n\n    # Normalize the diff.\n    if changes:\n      self.diff_cleanupMerge(diffs)\n    self.diff_cleanupSemanticLossless(diffs)\n\n    # Find any overlaps between deletions and insertions.\n    # e.g: <del>abcxxx</del><ins>xxxdef</ins>\n    #   -> <del>abc</del>xxx<ins>def</ins>\n    # e.g: <del>xxxabc</del><ins>defxxx</ins>\n    #   -> <ins>def</ins>xxx<del>abc</del>\n    # Only extract an overlap if it is as big as the edit ahead or behind it.\n    pointer = 1\n    while pointer < len(diffs):\n      if (diffs[pointer - 1][0] == self.DIFF_DELETE and\n          diffs[pointer][0] == self.DIFF_INSERT):\n        deletion = diffs[pointer - 1][1]\n        insertion = diffs[pointer][1]\n        overlap_length1 = self.diff_commonOverlap(deletion, insertion)\n        overlap_length2 = self.diff_commonOverlap(insertion, deletion)\n        if overlap_length1 >= overlap_length2:\n          if (overlap_length1 >= len(deletion) / 2.0 or\n              overlap_length1 >= len(insertion) / 2.0):\n            # Overlap found.  Insert an equality and trim the surrounding edits.\n            diffs.insert(pointer, (self.DIFF_EQUAL,\n                                   insertion[:overlap_length1]))\n            diffs[pointer - 1] = (self.DIFF_DELETE,\n                                  deletion[:len(deletion) - overlap_length1])\n            diffs[pointer + 1] = (self.DIFF_INSERT,\n                                  insertion[overlap_length1:])\n            pointer += 1\n        else:\n          if (overlap_length2 >= len(deletion) / 2.0 or\n              overlap_length2 >= len(insertion) / 2.0):\n            # Reverse overlap found.\n            # Insert an equality and swap and trim the surrounding edits.\n            diffs.insert(pointer, (self.DIFF_EQUAL, deletion[:overlap_length2]))\n            diffs[pointer - 1] = (self.DIFF_INSERT,\n                                  insertion[:len(insertion) - overlap_length2])\n            diffs[pointer + 1] = (self.DIFF_DELETE, deletion[overlap_length2:])\n            pointer += 1\n        pointer += 1\n      pointer += 1", "response": "This function cleans up semantically trivial equalities."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking for single edits surrounded on both sides by equalities which can be shifted sideways to align the edit to a word boundary. e.g: The c<ins>at c</ins>ame. -> The <ins>cat </ins>came. Args: diffs: Array of diff tuples.", "response": "def diff_cleanupSemanticLossless(self, diffs):\n    \"\"\"Look for single edits surrounded on both sides by equalities\n    which can be shifted sideways to align the edit to a word boundary.\n    e.g: The c<ins>at c</ins>ame. -> The <ins>cat </ins>came.\n\n    Args:\n      diffs: Array of diff tuples.\n    \"\"\"\n\n    def diff_cleanupSemanticScore(one, two):\n      \"\"\"Given two strings, compute a score representing whether the\n      internal boundary falls on logical boundaries.\n      Scores range from 6 (best) to 0 (worst).\n      Closure, but does not reference any external variables.\n\n      Args:\n        one: First string.\n        two: Second string.\n\n      Returns:\n        The score.\n      \"\"\"\n      if not one or not two:\n        # Edges are the best.\n        return 6\n\n      # Each port of this function behaves slightly differently due to\n      # subtle differences in each language's definition of things like\n      # 'whitespace'.  Since this function's purpose is largely cosmetic,\n      # the choice has been made to use each language's native features\n      # rather than force total conformity.\n      char1 = one[-1]\n      char2 = two[0]\n      nonAlphaNumeric1 = not char1.isalnum()\n      nonAlphaNumeric2 = not char2.isalnum()\n      whitespace1 = nonAlphaNumeric1 and char1.isspace()\n      whitespace2 = nonAlphaNumeric2 and char2.isspace()\n      lineBreak1 = whitespace1 and (char1 == \"\\r\" or char1 == \"\\n\")\n      lineBreak2 = whitespace2 and (char2 == \"\\r\" or char2 == \"\\n\")\n      blankLine1 = lineBreak1 and self.BLANKLINEEND.search(one)\n      blankLine2 = lineBreak2 and self.BLANKLINESTART.match(two)\n\n      if blankLine1 or blankLine2:\n        # Five points for blank lines.\n        return 5\n      elif lineBreak1 or lineBreak2:\n        # Four points for line breaks.\n        return 4\n      elif nonAlphaNumeric1 and not whitespace1 and whitespace2:\n        # Three points for end of sentences.\n        return 3\n      elif whitespace1 or whitespace2:\n        # Two points for whitespace.\n        return 2\n      elif nonAlphaNumeric1 or nonAlphaNumeric2:\n        # One point for non-alphanumeric.\n        return 1\n      return 0\n\n    pointer = 1\n    # Intentionally ignore the first and last element (don't need checking).\n    while pointer < len(diffs) - 1:\n      if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n          diffs[pointer + 1][0] == self.DIFF_EQUAL):\n        # This is a single edit surrounded by equalities.\n        equality1 = diffs[pointer - 1][1]\n        edit = diffs[pointer][1]\n        equality2 = diffs[pointer + 1][1]\n\n        # First, shift the edit as far left as possible.\n        commonOffset = self.diff_commonSuffix(equality1, edit)\n        if commonOffset:\n          commonString = edit[-commonOffset:]\n          equality1 = equality1[:-commonOffset]\n          edit = commonString + edit[:-commonOffset]\n          equality2 = commonString + equality2\n\n        # Second, step character by character right, looking for the best fit.\n        bestEquality1 = equality1\n        bestEdit = edit\n        bestEquality2 = equality2\n        bestScore = (diff_cleanupSemanticScore(equality1, edit) +\n            diff_cleanupSemanticScore(edit, equality2))\n        while edit and equality2 and edit[0] == equality2[0]:\n          equality1 += edit[0]\n          edit = edit[1:] + equality2[0]\n          equality2 = equality2[1:]\n          score = (diff_cleanupSemanticScore(equality1, edit) +\n              diff_cleanupSemanticScore(edit, equality2))\n          # The >= encourages trailing rather than leading whitespace on edits.\n          if score >= bestScore:\n            bestScore = score\n            bestEquality1 = equality1\n            bestEdit = edit\n            bestEquality2 = equality2\n\n        if diffs[pointer - 1][1] != bestEquality1:\n          # We have an improvement, save it back to the diff.\n          if bestEquality1:\n            diffs[pointer - 1] = (diffs[pointer - 1][0], bestEquality1)\n          else:\n            del diffs[pointer - 1]\n            pointer -= 1\n          diffs[pointer] = (diffs[pointer][0], bestEdit)\n          if bestEquality2:\n            diffs[pointer + 1] = (diffs[pointer + 1][0], bestEquality2)\n          else:\n            del diffs[pointer + 1]\n            pointer -= 1\n      pointer += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing the number of edits by eliminating operationally trivial equalities. Args: diffs: Array of diff tuples.", "response": "def diff_cleanupEfficiency(self, diffs):\n    \"\"\"Reduce the number of edits by eliminating operationally trivial\n    equalities.\n\n    Args:\n      diffs: Array of diff tuples.\n    \"\"\"\n    changes = False\n    equalities = []  # Stack of indices where equalities are found.\n    lastEquality = None  # Always equal to diffs[equalities[-1]][1]\n    pointer = 0  # Index of current position.\n    pre_ins = False  # Is there an insertion operation before the last equality.\n    pre_del = False  # Is there a deletion operation before the last equality.\n    post_ins = False  # Is there an insertion operation after the last equality.\n    post_del = False  # Is there a deletion operation after the last equality.\n    while pointer < len(diffs):\n      if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality found.\n        if (len(diffs[pointer][1]) < self.Diff_EditCost and\n            (post_ins or post_del)):\n          # Candidate found.\n          equalities.append(pointer)\n          pre_ins = post_ins\n          pre_del = post_del\n          lastEquality = diffs[pointer][1]\n        else:\n          # Not a candidate, and can never become one.\n          equalities = []\n          lastEquality = None\n\n        post_ins = post_del = False\n      else:  # An insertion or deletion.\n        if diffs[pointer][0] == self.DIFF_DELETE:\n          post_del = True\n        else:\n          post_ins = True\n\n        # Five types to be split:\n        # <ins>A</ins><del>B</del>XY<ins>C</ins><del>D</del>\n        # <ins>A</ins>X<ins>C</ins><del>D</del>\n        # <ins>A</ins><del>B</del>X<ins>C</ins>\n        # <ins>A</del>X<ins>C</ins><del>D</del>\n        # <ins>A</ins><del>B</del>X<del>C</del>\n\n        if lastEquality and ((pre_ins and pre_del and post_ins and post_del) or\n                             ((len(lastEquality) < self.Diff_EditCost / 2) and\n                              (pre_ins + pre_del + post_ins + post_del) == 3)):\n          # Duplicate record.\n          diffs.insert(equalities[-1], (self.DIFF_DELETE, lastEquality))\n          # Change second copy to insert.\n          diffs[equalities[-1] + 1] = (self.DIFF_INSERT,\n              diffs[equalities[-1] + 1][1])\n          equalities.pop()  # Throw away the equality we just deleted.\n          lastEquality = None\n          if pre_ins and pre_del:\n            # No changes made which could affect previous entry, keep going.\n            post_ins = post_del = True\n            equalities = []\n          else:\n            if len(equalities):\n              equalities.pop()  # Throw away the previous equality.\n            if len(equalities):\n              pointer = equalities[-1]\n            else:\n              pointer = -1\n            post_ins = post_del = False\n          changes = True\n      pointer += 1\n\n    if changes:\n      self.diff_cleanupMerge(diffs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreorder and merge like edit sections.", "response": "def diff_cleanupMerge(self, diffs):\n    \"\"\"Reorder and merge like edit sections.  Merge equalities.\n    Any edit section can move as long as it doesn't cross an equality.\n\n    Args:\n      diffs: Array of diff tuples.\n    \"\"\"\n    diffs.append((self.DIFF_EQUAL, ''))  # Add a dummy entry at the end.\n    pointer = 0\n    count_delete = 0\n    count_insert = 0\n    text_delete = ''\n    text_insert = ''\n    while pointer < len(diffs):\n      if diffs[pointer][0] == self.DIFF_INSERT:\n        count_insert += 1\n        text_insert += diffs[pointer][1]\n        pointer += 1\n      elif diffs[pointer][0] == self.DIFF_DELETE:\n        count_delete += 1\n        text_delete += diffs[pointer][1]\n        pointer += 1\n      elif diffs[pointer][0] == self.DIFF_EQUAL:\n        # Upon reaching an equality, check for prior redundancies.\n        if count_delete + count_insert > 1:\n          if count_delete != 0 and count_insert != 0:\n            # Factor out any common prefixies.\n            commonlength = self.diff_commonPrefix(text_insert, text_delete)\n            if commonlength != 0:\n              x = pointer - count_delete - count_insert - 1\n              if x >= 0 and diffs[x][0] == self.DIFF_EQUAL:\n                diffs[x] = (diffs[x][0], diffs[x][1] +\n                            text_insert[:commonlength])\n              else:\n                diffs.insert(0, (self.DIFF_EQUAL, text_insert[:commonlength]))\n                pointer += 1\n              text_insert = text_insert[commonlength:]\n              text_delete = text_delete[commonlength:]\n            # Factor out any common suffixes.\n            commonlength = self.diff_commonSuffix(text_insert, text_delete)\n            if commonlength != 0:\n              diffs[pointer] = (diffs[pointer][0], text_insert[-commonlength:] +\n                  diffs[pointer][1])\n              text_insert = text_insert[:-commonlength]\n              text_delete = text_delete[:-commonlength]\n          # Delete the offending records and add the merged ones.\n          new_ops = []\n          if len(text_delete) != 0:\n            new_ops.append((self.DIFF_DELETE, text_delete))\n          if len(text_insert) != 0:\n            new_ops.append((self.DIFF_INSERT, text_insert))\n          pointer -= count_delete + count_insert\n          diffs[pointer : pointer + count_delete + count_insert] = new_ops\n          pointer += len(new_ops) + 1\n        elif pointer != 0 and diffs[pointer - 1][0] == self.DIFF_EQUAL:\n          # Merge this equality with the previous one.\n          diffs[pointer - 1] = (diffs[pointer - 1][0],\n                                diffs[pointer - 1][1] + diffs[pointer][1])\n          del diffs[pointer]\n        else:\n          pointer += 1\n\n        count_insert = 0\n        count_delete = 0\n        text_delete = ''\n        text_insert = ''\n\n    if diffs[-1][1] == '':\n      diffs.pop()  # Remove the dummy entry at the end.\n\n    # Second pass: look for single edits surrounded on both sides by equalities\n    # which can be shifted sideways to eliminate an equality.\n    # e.g: A<ins>BA</ins>C -> <ins>AB</ins>AC\n    changes = False\n    pointer = 1\n    # Intentionally ignore the first and last element (don't need checking).\n    while pointer < len(diffs) - 1:\n      if (diffs[pointer - 1][0] == self.DIFF_EQUAL and\n          diffs[pointer + 1][0] == self.DIFF_EQUAL):\n        # This is a single edit surrounded by equalities.\n        if diffs[pointer][1].endswith(diffs[pointer - 1][1]):\n          # Shift the edit over the previous equality.\n          if diffs[pointer - 1][1] != \"\":\n            diffs[pointer] = (diffs[pointer][0],\n                diffs[pointer - 1][1] +\n                diffs[pointer][1][:-len(diffs[pointer - 1][1])])\n            diffs[pointer + 1] = (diffs[pointer + 1][0],\n                                  diffs[pointer - 1][1] + diffs[pointer + 1][1])\n          del diffs[pointer - 1]\n          changes = True\n        elif diffs[pointer][1].startswith(diffs[pointer + 1][1]):\n          # Shift the edit over the next equality.\n          diffs[pointer - 1] = (diffs[pointer - 1][0],\n                                diffs[pointer - 1][1] + diffs[pointer + 1][1])\n          diffs[pointer] = (diffs[pointer][0],\n              diffs[pointer][1][len(diffs[pointer + 1][1]):] +\n              diffs[pointer + 1][1])\n          del diffs[pointer + 1]\n          changes = True\n      pointer += 1\n\n    # If shifts were made, the diff needs reordering and another shift sweep.\n    if changes:\n      self.diff_cleanupMerge(diffs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute and return the equivalent location in text1. loc is a location in text2 compute and return the equivalent location in text1. loc is a location in text2 compute and return the equivalent location in text2.", "response": "def diff_xIndex(self, diffs, loc):\n    \"\"\"loc is a location in text1, compute and return the equivalent location\n    in text2.  e.g. \"The cat\" vs \"The big cat\", 1->1, 5->8\n\n    Args:\n      diffs: Array of diff tuples.\n      loc: Location within text1.\n\n    Returns:\n      Location within text2.\n    \"\"\"\n    chars1 = 0\n    chars2 = 0\n    last_chars1 = 0\n    last_chars2 = 0\n    for x in range(len(diffs)):\n      (op, text) = diffs[x]\n      if op != self.DIFF_INSERT:  # Equality or deletion.\n        chars1 += len(text)\n      if op != self.DIFF_DELETE:  # Equality or insertion.\n        chars2 += len(text)\n      if chars1 > loc:  # Overshot the location.\n        break\n      last_chars1 = chars1\n      last_chars2 = chars2\n\n    if len(diffs) != x and diffs[x][0] == self.DIFF_DELETE:\n      # The location was deleted.\n      return last_chars2\n    # Add the remaining len(character).\n    return last_chars2 + (loc - last_chars1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a diff array into a pretty HTML report.", "response": "def diff_prettyHtml(self, diffs):\n    \"\"\"Convert a diff array into a pretty HTML report.\n\n    Args:\n      diffs: Array of diff tuples.\n\n    Returns:\n      HTML representation.\n    \"\"\"\n    html = []\n    for (op, data) in diffs:\n      text = (data.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\")\n                 .replace(\">\", \"&gt;\").replace(\"\\n\", \"&para;<br>\"))\n      if op == self.DIFF_INSERT:\n        html.append(\"<ins style=\\\"background:#e6ffe6;\\\">%s</ins>\" % text)\n      elif op == self.DIFF_DELETE:\n        html.append(\"<del style=\\\"background:#ffe6e6;\\\">%s</del>\" % text)\n      elif op == self.DIFF_EQUAL:\n        html.append(\"<span>%s</span>\" % text)\n    return \"\".join(html)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff_text1(self, diffs):\n    text = []\n    for (op, data) in diffs:\n      if op != self.DIFF_INSERT:\n        text.append(data)\n    return \"\".join(text)", "response": "Compute and return the source text for the first diff."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_text2(self, diffs):\n    text = []\n    for (op, data) in diffs:\n      if op != self.DIFF_DELETE:\n        text.append(data)\n    return \"\".join(text)", "response": "Compute and return the destination text for the given diff tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diff_levenshtein(self, diffs):\n    levenshtein = 0\n    insertions = 0\n    deletions = 0\n    for (op, data) in diffs:\n      if op == self.DIFF_INSERT:\n        insertions += len(data)\n      elif op == self.DIFF_DELETE:\n        deletions += len(data)\n      elif op == self.DIFF_EQUAL:\n        # A deletion and an insertion is one substitution.\n        levenshtein += max(insertions, deletions)\n        insertions = 0\n        deletions = 0\n    levenshtein += max(insertions, deletions)\n    return levenshtein", "response": "Compute the Levenshtein distance of the number of inserted deleted or a set of substituted characters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the full diff between two strings.", "response": "def diff_fromDelta(self, text1, delta):\n    \"\"\"Given the original text1, and an encoded string which describes the\n    operations required to transform text1 into text2, compute the full diff.\n\n    Args:\n      text1: Source string for the diff.\n      delta: Delta text.\n\n    Returns:\n      Array of diff tuples.\n\n    Raises:\n      ValueError: If invalid input.\n    \"\"\"\n    diffs = []\n    pointer = 0  # Cursor in text1\n    tokens = delta.split(\"\\t\")\n    for token in tokens:\n      if token == \"\":\n        # Blank tokens are ok (from a trailing \\t).\n        continue\n      # Each token begins with a one character parameter which specifies the\n      # operation of this token (delete, insert, equality).\n      param = token[1:]\n      if token[0] == \"+\":\n        param = urllib.parse.unquote(param)\n        diffs.append((self.DIFF_INSERT, param))\n      elif token[0] == \"-\" or token[0] == \"=\":\n        try:\n          n = int(param)\n        except ValueError:\n          raise ValueError(\"Invalid number in diff_fromDelta: \" + param)\n        if n < 0:\n          raise ValueError(\"Negative number in diff_fromDelta: \" + param)\n        text = text1[pointer : pointer + n]\n        pointer += n\n        if token[0] == \"=\":\n          diffs.append((self.DIFF_EQUAL, text))\n        else:\n          diffs.append((self.DIFF_DELETE, text))\n      else:\n        # Anything else is an error.\n        raise ValueError(\"Invalid diff operation in diff_fromDelta: \" +\n            token[0])\n    if pointer != len(text1):\n      raise ValueError(\n          \"Delta length (%d) does not equal source text length (%d).\" %\n         (pointer, len(text1)))\n    return diffs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlocating the best instance of pattern in text near loc.", "response": "def match_main(self, text, pattern, loc):\n    \"\"\"Locate the best instance of 'pattern' in 'text' near 'loc'.\n\n    Args:\n      text: The text to search.\n      pattern: The pattern to search for.\n      loc: The location to search around.\n\n    Returns:\n      Best match index or -1.\n    \"\"\"\n    # Check for null inputs.\n    if text == None or pattern == None:\n      raise ValueError(\"Null inputs. (match_main)\")\n\n    loc = max(0, min(loc, len(text)))\n    if text == pattern:\n      # Shortcut (potentially not guaranteed by the algorithm)\n      return 0\n    elif not text:\n      # Nothing to match.\n      return -1\n    elif text[loc:loc + len(pattern)] == pattern:\n      # Perfect match at the perfect spot!  (Includes case of null pattern)\n      return loc\n    else:\n      # Do a fuzzy compare.\n      match = self.match_bitap(text, pattern, loc)\n      return match"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_bitap(self, text, pattern, loc):\n    # Python doesn't have a maxint limit, so ignore this check.\n    #if self.Match_MaxBits != 0 and len(pattern) > self.Match_MaxBits:\n    #  raise ValueError(\"Pattern too long for this application.\")\n\n    # Initialise the alphabet.\n    s = self.match_alphabet(pattern)\n\n    def match_bitapScore(e, x):\n      \"\"\"Compute and return the score for a match with e errors and x location.\n      Accesses loc and pattern through being a closure.\n\n      Args:\n        e: Number of errors in match.\n        x: Location of match.\n\n      Returns:\n        Overall score for match (0.0 = good, 1.0 = bad).\n      \"\"\"\n      accuracy = float(e) / len(pattern)\n      proximity = abs(loc - x)\n      if not self.Match_Distance:\n        # Dodge divide by zero error.\n        return proximity and 1.0 or accuracy\n      return accuracy + (proximity / float(self.Match_Distance))\n\n    # Highest score beyond which we give up.\n    score_threshold = self.Match_Threshold\n    # Is there a nearby exact match? (speedup)\n    best_loc = text.find(pattern, loc)\n    if best_loc != -1:\n      score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n      # What about in the other direction? (speedup)\n      best_loc = text.rfind(pattern, loc + len(pattern))\n      if best_loc != -1:\n        score_threshold = min(match_bitapScore(0, best_loc), score_threshold)\n\n    # Initialise the bit arrays.\n    matchmask = 1 << (len(pattern) - 1)\n    best_loc = -1\n\n    bin_max = len(pattern) + len(text)\n    # Empty initialization added to appease pychecker.\n    last_rd = None\n    for d in range(len(pattern)):\n      # Scan for the best match each iteration allows for one more error.\n      # Run a binary search to determine how far from 'loc' we can stray at\n      # this error level.\n      bin_min = 0\n      bin_mid = bin_max\n      while bin_min < bin_mid:\n        if match_bitapScore(d, loc + bin_mid) <= score_threshold:\n          bin_min = bin_mid\n        else:\n          bin_max = bin_mid\n        bin_mid = (bin_max - bin_min) // 2 + bin_min\n\n      # Use the result from this iteration as the maximum for the next.\n      bin_max = bin_mid\n      start = max(1, loc - bin_mid + 1)\n      finish = min(loc + bin_mid, len(text)) + len(pattern)\n\n      rd = [0] * (finish + 2)\n      rd[finish + 1] = (1 << d) - 1\n      for j in range(finish, start - 1, -1):\n        if len(text) <= j - 1:\n          # Out of range.\n          charMatch = 0\n        else:\n          charMatch = s.get(text[j - 1], 0)\n        if d == 0:  # First pass: exact match.\n          rd[j] = ((rd[j + 1] << 1) | 1) & charMatch\n        else:  # Subsequent passes: fuzzy match.\n          rd[j] = (((rd[j + 1] << 1) | 1) & charMatch) | (\n              ((last_rd[j + 1] | last_rd[j]) << 1) | 1) | last_rd[j + 1]\n        if rd[j] & matchmask:\n          score = match_bitapScore(d, j - 1)\n          # This match will almost certainly be better than any existing match.\n          # But check anyway.\n          if score <= score_threshold:\n            # Told you so.\n            score_threshold = score\n            best_loc = j - 1\n            if best_loc > loc:\n              # When passing loc, don't exceed our current distance from loc.\n              start = max(1, 2 * loc - best_loc)\n            else:\n              # Already passed loc, downhill from here on in.\n              break\n      # No hope for a (better) match at greater error levels.\n      if match_bitapScore(d + 1, loc) > score_threshold:\n        break\n      last_rd = rd\n    return best_loc", "response": "Locate the best instance of pattern in text near loc using the Bitap algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_alphabet(self, pattern):\n    s = {}\n    for char in pattern:\n      s[char] = 0\n    for i in range(len(pattern)):\n      s[pattern[i]] |= 1 << (len(pattern) - i - 1)\n    return s", "response": "Initialise the alphabet for the Bitap algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincreases the context until it is unique and don t let the pattern expand beyond Match_MaxBits.", "response": "def patch_addContext(self, patch, text):\n    \"\"\"Increase the context until it is unique,\n    but don't let the pattern expand beyond Match_MaxBits.\n\n    Args:\n      patch: The patch to grow.\n      text: Source text.\n    \"\"\"\n    if len(text) == 0:\n      return\n    pattern = text[patch.start2 : patch.start2 + patch.length1]\n    padding = 0\n\n    # Look for the first and last matches of pattern in text.  If two different\n    # matches are found, increase the pattern length.\n    while (text.find(pattern) != text.rfind(pattern) and (self.Match_MaxBits ==\n        0 or len(pattern) < self.Match_MaxBits - self.Patch_Margin -\n        self.Patch_Margin)):\n      padding += self.Patch_Margin\n      pattern = text[max(0, patch.start2 - padding) :\n                     patch.start2 + patch.length1 + padding]\n    # Add one chunk for good luck.\n    padding += self.Patch_Margin\n\n    # Add the prefix.\n    prefix = text[max(0, patch.start2 - padding) : patch.start2]\n    if prefix:\n      patch.diffs[:0] = [(self.DIFF_EQUAL, prefix)]\n    # Add the suffix.\n    suffix = text[patch.start2 + patch.length1 :\n                  patch.start2 + patch.length1 + padding]\n    if suffix:\n      patch.diffs.append((self.DIFF_EQUAL, suffix))\n\n    # Roll back the start points.\n    patch.start1 -= len(prefix)\n    patch.start2 -= len(prefix)\n    # Extend lengths.\n    patch.length1 += len(prefix) + len(suffix)\n    patch.length2 += len(prefix) + len(suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing a list of patches to turn text1 into text2.", "response": "def patch_make(self, a, b=None, c=None):\n    \"\"\"Compute a list of patches to turn text1 into text2.\n    Use diffs if provided, otherwise compute it ourselves.\n    There are four ways to call this function, depending on what data is\n    available to the caller:\n    Method 1:\n    a = text1, b = text2\n    Method 2:\n    a = diffs\n    Method 3 (optimal):\n    a = text1, b = diffs\n    Method 4 (deprecated, use method 3):\n    a = text1, b = text2, c = diffs\n\n    Args:\n      a: text1 (methods 1,3,4) or Array of diff tuples for text1 to\n          text2 (method 2).\n      b: text2 (methods 1,4) or Array of diff tuples for text1 to\n          text2 (method 3) or undefined (method 2).\n      c: Array of diff tuples for text1 to text2 (method 4) or\n          undefined (methods 1,2,3).\n\n    Returns:\n      Array of Patch objects.\n    \"\"\"\n    text1 = None\n    diffs = None\n    if isinstance(a, str) and isinstance(b, str) and c is None:\n      # Method 1: text1, text2\n      # Compute diffs from text1 and text2.\n      text1 = a\n      diffs = self.diff_main(text1, b, True)\n      if len(diffs) > 2:\n        self.diff_cleanupSemantic(diffs)\n        self.diff_cleanupEfficiency(diffs)\n    elif isinstance(a, list) and b is None and c is None:\n      # Method 2: diffs\n      # Compute text1 from diffs.\n      diffs = a\n      text1 = self.diff_text1(diffs)\n    elif isinstance(a, str) and isinstance(b, list) and c is None:\n      # Method 3: text1, diffs\n      text1 = a\n      diffs = b\n    elif (isinstance(a, str) and isinstance(b, str) and\n          isinstance(c, list)):\n      # Method 4: text1, text2, diffs\n      # text2 is not used.\n      text1 = a\n      diffs = c\n    else:\n      raise ValueError(\"Unknown call format to patch_make.\")\n\n    if not diffs:\n      return []  # Get rid of the None case.\n    patches = []\n    patch = patch_obj()\n    char_count1 = 0  # Number of characters into the text1 string.\n    char_count2 = 0  # Number of characters into the text2 string.\n    prepatch_text = text1  # Recreate the patches to determine context info.\n    postpatch_text = text1\n    for x in range(len(diffs)):\n      (diff_type, diff_text) = diffs[x]\n      if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:\n        # A new patch starts here.\n        patch.start1 = char_count1\n        patch.start2 = char_count2\n      if diff_type == self.DIFF_INSERT:\n        # Insertion\n        patch.diffs.append(diffs[x])\n        patch.length2 += len(diff_text)\n        postpatch_text = (postpatch_text[:char_count2] + diff_text +\n                          postpatch_text[char_count2:])\n      elif diff_type == self.DIFF_DELETE:\n        # Deletion.\n        patch.length1 += len(diff_text)\n        patch.diffs.append(diffs[x])\n        postpatch_text = (postpatch_text[:char_count2] +\n                          postpatch_text[char_count2 + len(diff_text):])\n      elif (diff_type == self.DIFF_EQUAL and\n            len(diff_text) <= 2 * self.Patch_Margin and\n            len(patch.diffs) != 0 and len(diffs) != x + 1):\n        # Small equality inside a patch.\n        patch.diffs.append(diffs[x])\n        patch.length1 += len(diff_text)\n        patch.length2 += len(diff_text)\n\n      if (diff_type == self.DIFF_EQUAL and\n          len(diff_text) >= 2 * self.Patch_Margin):\n        # Time for a new patch.\n        if len(patch.diffs) != 0:\n          self.patch_addContext(patch, prepatch_text)\n          patches.append(patch)\n          patch = patch_obj()\n          # Unlike Unidiff, our patch lists have a rolling context.\n          # https://github.com/google/diff-match-patch/wiki/Unidiff\n          # Update prepatch text & pos to reflect the application of the\n          # just completed patch.\n          prepatch_text = postpatch_text\n          char_count1 = char_count2\n\n      # Update the current character count.\n      if diff_type != self.DIFF_INSERT:\n        char_count1 += len(diff_text)\n      if diff_type != self.DIFF_DELETE:\n        char_count2 += len(diff_text)\n\n    # Pick up the leftover patch if not empty.\n    if len(patch.diffs) != 0:\n      self.patch_addContext(patch, prepatch_text)\n      patches.append(patch)\n    return patches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_deepCopy(self, patches):\n    patchesCopy = []\n    for patch in patches:\n      patchCopy = patch_obj()\n      # No need to deep copy the tuples since they are immutable.\n      patchCopy.diffs = patch.diffs[:]\n      patchCopy.start1 = patch.start1\n      patchCopy.start2 = patch.start2\n      patchCopy.length1 = patch.length1\n      patchCopy.length2 = patch.length2\n      patchesCopy.append(patchCopy)\n    return patchesCopy", "response": "Given an array of patches return another array that is identical."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch_apply(self, patches, text):\n    if not patches:\n      return (text, [])\n\n    # Deep copy the patches so that no changes are made to originals.\n    patches = self.patch_deepCopy(patches)\n\n    nullPadding = self.patch_addPadding(patches)\n    text = nullPadding + text + nullPadding\n    self.patch_splitMax(patches)\n\n    # delta keeps track of the offset between the expected and actual location\n    # of the previous patch.  If there are patches expected at positions 10 and\n    # 20, but the first patch was found at 12, delta is 2 and the second patch\n    # has an effective expected position of 22.\n    delta = 0\n    results = []\n    for patch in patches:\n      expected_loc = patch.start2 + delta\n      text1 = self.diff_text1(patch.diffs)\n      end_loc = -1\n      if len(text1) > self.Match_MaxBits:\n        # patch_splitMax will only provide an oversized pattern in the case of\n        # a monster delete.\n        start_loc = self.match_main(text, text1[:self.Match_MaxBits],\n                                    expected_loc)\n        if start_loc != -1:\n          end_loc = self.match_main(text, text1[-self.Match_MaxBits:],\n              expected_loc + len(text1) - self.Match_MaxBits)\n          if end_loc == -1 or start_loc >= end_loc:\n            # Can't find valid trailing context.  Drop this patch.\n            start_loc = -1\n      else:\n        start_loc = self.match_main(text, text1, expected_loc)\n      if start_loc == -1:\n        # No match found.  :(\n        results.append(False)\n        # Subtract the delta for this failed patch from subsequent patches.\n        delta -= patch.length2 - patch.length1\n      else:\n        # Found a match.  :)\n        results.append(True)\n        delta = start_loc - expected_loc\n        if end_loc == -1:\n          text2 = text[start_loc : start_loc + len(text1)]\n        else:\n          text2 = text[start_loc : end_loc + self.Match_MaxBits]\n        if text1 == text2:\n          # Perfect match, just shove the replacement text in.\n          text = (text[:start_loc] + self.diff_text2(patch.diffs) +\n                      text[start_loc + len(text1):])\n        else:\n          # Imperfect match.\n          # Run a diff to get a framework of equivalent indices.\n          diffs = self.diff_main(text1, text2, False)\n          if (len(text1) > self.Match_MaxBits and\n              self.diff_levenshtein(diffs) / float(len(text1)) >\n              self.Patch_DeleteThreshold):\n            # The end points match, but the content is unacceptably bad.\n            results[-1] = False\n          else:\n            self.diff_cleanupSemanticLossless(diffs)\n            index1 = 0\n            for (op, data) in patch.diffs:\n              if op != self.DIFF_EQUAL:\n                index2 = self.diff_xIndex(diffs, index1)\n              if op == self.DIFF_INSERT:  # Insertion\n                text = text[:start_loc + index2] + data + text[start_loc +\n                                                               index2:]\n              elif op == self.DIFF_DELETE:  # Deletion\n                text = text[:start_loc + index2] + text[start_loc +\n                    self.diff_xIndex(diffs, index1 + len(data)):]\n              if op != self.DIFF_DELETE:\n                index1 += len(data)\n    # Strip the padding off.\n    text = text[len(nullPadding):-len(nullPadding)]\n    return (text, results)", "response": "Merge a set of patches onto the text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd some padding on text start and end so that edges can match.", "response": "def patch_addPadding(self, patches):\n    \"\"\"Add some padding on text start and end so that edges can match\n    something.  Intended to be called only from within patch_apply.\n\n    Args:\n      patches: Array of Patch objects.\n\n    Returns:\n      The padding string added to each side.\n    \"\"\"\n    paddingLength = self.Patch_Margin\n    nullPadding = \"\"\n    for x in range(1, paddingLength + 1):\n      nullPadding += chr(x)\n\n    # Bump all the patches forward.\n    for patch in patches:\n      patch.start1 += paddingLength\n      patch.start2 += paddingLength\n\n    # Add some padding on start of first diff.\n    patch = patches[0]\n    diffs = patch.diffs\n    if not diffs or diffs[0][0] != self.DIFF_EQUAL:\n      # Add nullPadding equality.\n      diffs.insert(0, (self.DIFF_EQUAL, nullPadding))\n      patch.start1 -= paddingLength  # Should be 0.\n      patch.start2 -= paddingLength  # Should be 0.\n      patch.length1 += paddingLength\n      patch.length2 += paddingLength\n    elif paddingLength > len(diffs[0][1]):\n      # Grow first equality.\n      extraLength = paddingLength - len(diffs[0][1])\n      newText = nullPadding[len(diffs[0][1]):] + diffs[0][1]\n      diffs[0] = (diffs[0][0], newText)\n      patch.start1 -= extraLength\n      patch.start2 -= extraLength\n      patch.length1 += extraLength\n      patch.length2 += extraLength\n\n    # Add some padding on end of last diff.\n    patch = patches[-1]\n    diffs = patch.diffs\n    if not diffs or diffs[-1][0] != self.DIFF_EQUAL:\n      # Add nullPadding equality.\n      diffs.append((self.DIFF_EQUAL, nullPadding))\n      patch.length1 += paddingLength\n      patch.length2 += paddingLength\n    elif paddingLength > len(diffs[-1][1]):\n      # Grow last equality.\n      extraLength = paddingLength - len(diffs[-1][1])\n      newText = diffs[-1][1] + nullPadding[:extraLength]\n      diffs[-1] = (diffs[-1][0], newText)\n      patch.length1 += extraLength\n      patch.length2 += extraLength\n\n    return nullPadding"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of patches and a maximum number of bits split them up into smaller pieces.", "response": "def patch_splitMax(self, patches):\n    \"\"\"Look through the patches and break up any which are longer than the\n    maximum limit of the match algorithm.\n    Intended to be called only from within patch_apply.\n\n    Args:\n      patches: Array of Patch objects.\n    \"\"\"\n    patch_size = self.Match_MaxBits\n    if patch_size == 0:\n      # Python has the option of not splitting strings due to its ability\n      # to handle integers of arbitrary precision.\n      return\n    for x in range(len(patches)):\n      if patches[x].length1 <= patch_size:\n        continue\n      bigpatch = patches[x]\n      # Remove the big old patch.\n      del patches[x]\n      x -= 1\n      start1 = bigpatch.start1\n      start2 = bigpatch.start2\n      precontext = ''\n      while len(bigpatch.diffs) != 0:\n        # Create one of several smaller patches.\n        patch = patch_obj()\n        empty = True\n        patch.start1 = start1 - len(precontext)\n        patch.start2 = start2 - len(precontext)\n        if precontext:\n          patch.length1 = patch.length2 = len(precontext)\n          patch.diffs.append((self.DIFF_EQUAL, precontext))\n\n        while (len(bigpatch.diffs) != 0 and\n               patch.length1 < patch_size - self.Patch_Margin):\n          (diff_type, diff_text) = bigpatch.diffs[0]\n          if diff_type == self.DIFF_INSERT:\n            # Insertions are harmless.\n            patch.length2 += len(diff_text)\n            start2 += len(diff_text)\n            patch.diffs.append(bigpatch.diffs.pop(0))\n            empty = False\n          elif (diff_type == self.DIFF_DELETE and len(patch.diffs) == 1 and\n              patch.diffs[0][0] == self.DIFF_EQUAL and\n              len(diff_text) > 2 * patch_size):\n            # This is a large deletion.  Let it pass in one chunk.\n            patch.length1 += len(diff_text)\n            start1 += len(diff_text)\n            empty = False\n            patch.diffs.append((diff_type, diff_text))\n            del bigpatch.diffs[0]\n          else:\n            # Deletion or equality.  Only take as much as we can stomach.\n            diff_text = diff_text[:patch_size - patch.length1 -\n                                  self.Patch_Margin]\n            patch.length1 += len(diff_text)\n            start1 += len(diff_text)\n            if diff_type == self.DIFF_EQUAL:\n              patch.length2 += len(diff_text)\n              start2 += len(diff_text)\n            else:\n              empty = False\n\n            patch.diffs.append((diff_type, diff_text))\n            if diff_text == bigpatch.diffs[0][1]:\n              del bigpatch.diffs[0]\n            else:\n              bigpatch.diffs[0] = (bigpatch.diffs[0][0],\n                                   bigpatch.diffs[0][1][len(diff_text):])\n\n        # Compute the head context for the next patch.\n        precontext = self.diff_text2(patch.diffs)\n        precontext = precontext[-self.Patch_Margin:]\n        # Append the end context for this patch.\n        postcontext = self.diff_text1(bigpatch.diffs)[:self.Patch_Margin]\n        if postcontext:\n          patch.length1 += len(postcontext)\n          patch.length2 += len(postcontext)\n          if len(patch.diffs) != 0 and patch.diffs[-1][0] == self.DIFF_EQUAL:\n            patch.diffs[-1] = (self.DIFF_EQUAL, patch.diffs[-1][1] +\n                               postcontext)\n          else:\n            patch.diffs.append((self.DIFF_EQUAL, postcontext))\n\n        if not empty:\n          x += 1\n          patches.insert(x, patch)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of patches and return a textual representation.", "response": "def patch_toText(self, patches):\n    \"\"\"Take a list of patches and return a textual representation.\n\n    Args:\n      patches: Array of Patch objects.\n\n    Returns:\n      Text representation of patches.\n    \"\"\"\n    text = []\n    for patch in patches:\n      text.append(str(patch))\n    return \"\".join(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncrushing the diff into an encoded string which describes the operations that need to be applied to the text1 into text2.", "response": "def diff_toDelta(self, diffs):\n    \"\"\"Crush the diff into an encoded string which describes the operations\n    required to transform text1 into text2.\n    E.g. =3\\t-2\\t+ing  -> Keep 3 chars, delete 2 chars, insert 'ing'.\n    Operations are tab-separated.  Inserted text is escaped using %xx notation.\n\n    Args:\n      diffs: Array of diff tuples.\n\n    Returns:\n      Delta text.\n    \"\"\"\n    text = []\n    for (op, data) in diffs:\n      if op == self.DIFF_INSERT:\n        # High ascii will raise UnicodeDecodeError.  Use Unicode instead.\n        data = data.encode(\"utf-8\")\n        text.append(\"+\" + urllib.quote(data, \"!~*'();/?:@&=+$,# \"))\n      elif op == self.DIFF_DELETE:\n        text.append(\"-%d\" % len(data))\n      elif op == self.DIFF_EQUAL:\n        text.append(\"=%d\" % len(data))\n    return \"\\t\".join(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_alphabet(self, pattern):\n    s = {}\n    for char in pattern:\n      s[char] = 0\n    for i in xrange(len(pattern)):\n      s[pattern[i]] |= 1 << (len(pattern) - i - 1)\n    return s", "response": "Initialise the alphabet for the Bitap algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patch_fromText(self, textline):\n    if type(textline) == unicode:\n      # Patches should be composed of a subset of ascii chars, Unicode not\n      # required.  If this encode raises UnicodeEncodeError, patch is invalid.\n      textline = textline.encode(\"ascii\")\n    patches = []\n    if not textline:\n      return patches\n    text = textline.split('\\n')\n    while len(text) != 0:\n      m = re.match(\"^@@ -(\\d+),?(\\d*) \\+(\\d+),?(\\d*) @@$\", text[0])\n      if not m:\n        raise ValueError(\"Invalid patch string: \" + text[0])\n      patch = patch_obj()\n      patches.append(patch)\n      patch.start1 = int(m.group(1))\n      if m.group(2) == '':\n        patch.start1 -= 1\n        patch.length1 = 1\n      elif m.group(2) == '0':\n        patch.length1 = 0\n      else:\n        patch.start1 -= 1\n        patch.length1 = int(m.group(2))\n\n      patch.start2 = int(m.group(3))\n      if m.group(4) == '':\n        patch.start2 -= 1\n        patch.length2 = 1\n      elif m.group(4) == '0':\n        patch.length2 = 0\n      else:\n        patch.start2 -= 1\n        patch.length2 = int(m.group(4))\n\n      del text[0]\n\n      while len(text) != 0:\n        if text[0]:\n          sign = text[0][0]\n        else:\n          sign = ''\n        line = urllib.unquote(text[0][1:])\n        line = line.decode(\"utf-8\")\n        if sign == '+':\n          # Insertion.\n          patch.diffs.append((self.DIFF_INSERT, line))\n        elif sign == '-':\n          # Deletion.\n          patch.diffs.append((self.DIFF_DELETE, line))\n        elif sign == ' ':\n          # Minor equality.\n          patch.diffs.append((self.DIFF_EQUAL, line))\n        elif sign == '@':\n          # Start of next patch.\n          break\n        elif sign == '':\n          # Blank line?  Whatever.\n          pass\n        else:\n          # WTF?\n          raise ValueError(\"Invalid patch mode: '%s'\\n%s\" % (sign, line))\n        del text[0]\n    return patches", "response": "Parse a textual representation of patches and return a list of Patch objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking two lxml root elements or element trees", "response": "def diff_trees(left, right, diff_options=None, formatter=None):\n    \"\"\"Takes two lxml root elements or element trees\"\"\"\n    if formatter is not None:\n        formatter.prepare(left, right)\n    if diff_options is None:\n        diff_options = {}\n    differ = diff.Differ(**diff_options)\n    diffs = differ.diff(left, right)\n\n    if formatter is None:\n        return list(diffs)\n\n    return formatter.format(diffs, left)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diff_texts(left, right, diff_options=None, formatter=None):\n    return _diff(etree.fromstring, left, right,\n                 diff_options=diff_options, formatter=formatter)", "response": "Takes two Unicode strings containing XML"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking two filenames or streams and diffs the XML in those files", "response": "def diff_files(left, right, diff_options=None, formatter=None):\n    \"\"\"Takes two filenames or streams, and diffs the XML in those files\"\"\"\n    return _diff(etree.parse, left, right,\n                 diff_options=diff_options, formatter=formatter)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch_tree(actions, tree):\n    patcher = patch.Patcher()\n    return patcher.patch(actions, tree)", "response": "Takes an lxml root element or element tree and a list of actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch_text(actions, tree):\n    tree = etree.fromstring(tree)\n    actions = patch.DiffParser().parse(actions)\n    tree = patch_tree(actions, tree)\n    return etree.tounicode(tree)", "response": "Takes a string with XML and a string with actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes two filenames or streams one with XML the other a diff", "response": "def patch_file(actions, tree):\n    \"\"\"Takes two filenames or streams, one with XML the other a diff\"\"\"\n    tree = etree.parse(tree)\n\n    if isinstance(actions, six.string_types):\n        # It's a string, so it's a filename\n        with open(actions) as f:\n            actions = f.read()\n    else:\n        # We assume it's a stream\n        actions = actions.read()\n\n    actions = patch.DiffParser().parse(actions)\n    tree = patch_tree(actions, tree)\n    return etree.tounicode(tree)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare(self, left_tree, right_tree):\n        # We don't want to diff comments:\n        self._remove_comments(left_tree)\n        self._remove_comments(right_tree)\n\n        self.placeholderer.do_tree(left_tree)\n        self.placeholderer.do_tree(right_tree)", "response": "This is the main entry point for the diffing process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = None\r\n        for result_tuple in self.__feature_generator.generate():\r\n            observed_arr = result_tuple[0]\r\n            break\r\n\r\n        observed_arr = observed_arr.astype(float)\r\n        if self.__norm_mode == \"z_score\":\r\n            if observed_arr.std() != 0:\r\n                observed_arr = (observed_arr - observed_arr.mean()) / observed_arr.std()\r\n        elif self.__norm_mode == \"min_max\":\r\n            if (observed_arr.max() - observed_arr.min()) != 0:\r\n                observed_arr = (observed_arr - observed_arr.min()) / (observed_arr.max() - observed_arr.min())\r\n        elif self.__norm_mode == \"tanh\":\r\n            observed_arr = np.tanh(observed_arr)\r\n\r\n        return observed_arr", "response": "Returns the samples from the true distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate(self):\r\n        '''\r\n        Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        sampled_arr = np.zeros((self.__batch_size, self.__channel, self.__seq_len, self.__dim))\r\n\r\n        for batch in range(self.__batch_size):\r\n            for i in range(len(self.__program_list)):\r\n                program_key = self.__program_list[i]\r\n                key = np.random.randint(low=0, high=len(self.__midi_df_list))\r\n                midi_df = self.__midi_df_list[key]\r\n                midi_df = midi_df[midi_df.program == program_key]\r\n                if midi_df.shape[0] < self.__seq_len:\r\n                    continue\r\n\r\n                row = np.random.uniform(\r\n                    low=midi_df.start.min(), \r\n                    high=midi_df.end.max() - (self.__seq_len * self.__time_fraction)\r\n                )\r\n                for seq in range(self.__seq_len):\r\n                    start = row + (seq * self.__time_fraction)\r\n                    end = row + ((seq+1) * self.__time_fraction)\r\n                    df = midi_df[(start <= midi_df.start) & (midi_df.start <= end)]\r\n                    sampled_arr[batch, i, seq] = self.__convert_into_feature(df)\r\n\r\n        return sampled_arr", "response": "Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate samples from the true distribution.", "response": "def generate(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = None\r\n        for result_tuple in self.__feature_generator.generate():\r\n            observed_arr = result_tuple[0]\r\n            break\r\n\r\n        if self.noise_sampler is not None:\r\n            self.noise_sampler.output_shape = observed_arr.shape\r\n            observed_arr += self.noise_sampler.generate()\r\n\r\n        observed_arr = observed_arr.astype(float)\r\n        if self.__norm_mode == \"z_score\":\r\n            if observed_arr.std() != 0:\r\n                observed_arr = (observed_arr - observed_arr.mean()) / observed_arr.std()\r\n        elif self.__norm_mode == \"min_max\":\r\n            if (observed_arr.max() - observed_arr.min()) != 0:\r\n                observed_arr = (observed_arr - observed_arr.min()) / (observed_arr.max() - observed_arr.min())\r\n        elif self.__norm_mode == \"tanh\":\r\n            observed_arr = np.tanh(observed_arr)\r\n        \r\n        return observed_arr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute(self, x_arr, y_arr):\r\n        '''\r\n        Compute distance.\r\n\r\n        Args:\r\n            x_arr:      `np.ndarray` of vectors.\r\n            y_arr:      `np.ndarray` of vectors.\r\n\r\n        Retruns:\r\n            `np.ndarray` of distances.\r\n        '''\r\n        y_arr += 1e-08\r\n        return np.sum(x_arr * np.log(x_arr / y_arr), axis=-1)", "response": "Compute distance between two sets of entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_ngram_data_set(self, token_list, n=2):\r\n        '''\r\n        Generate the N-gram's pair.\r\n\r\n        Args:\r\n            token_list:     The list of tokens.\r\n            n               N\r\n\r\n        Returns:\r\n            zip of Tuple(Training N-gram data, Target N-gram data)\r\n        '''\r\n        n_gram_tuple_zip = self.generate_tuple_zip(token_list, n)\r\n        n_gram_tuple_list = [n_gram_tuple for n_gram_tuple in n_gram_tuple_zip]\r\n        n_gram_data_set = self.generate_tuple_zip(n_gram_tuple_list, 2)\r\n        return n_gram_data_set", "response": "Generate the N - gram s pair."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the Skip -gram s pair.", "response": "def generate_skip_gram_data_set(self, token_list):\r\n        '''\r\n        Generate the Skip-gram's pair.\r\n\r\n        Args:\r\n            token_list:     The list of tokens.\r\n\r\n        Returns:\r\n            zip of Tuple(Training N-gram data, Target N-gram data)\r\n        '''\r\n        n_gram_tuple_zip = self.generate_tuple_zip(token_list, 3)\r\n        skip_gram_list = []\r\n        for pre, point, post in n_gram_tuple_zip:\r\n            skip_gram_list.append((point, pre))\r\n            skip_gram_list.append((point, post))\r\n        return zip(skip_gram_list)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the N - grams. is a generator function that returns a tuple of n - grams.", "response": "def generate_tuple_zip(self, token_list, n=2):\r\n        '''\r\n        Generate the N-gram.\r\n\r\n        Args:\r\n            token_list:     The list of tokens.\r\n            n               N\r\n\r\n        Returns:\r\n            zip of Tuple(N-gram)\r\n        '''\r\n        return zip(*[token_list[i:] for i in range(n)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        sampled_arr = np.empty((self.__batch_size, self.__seq_len, self.__dim))\r\n\r\n        for batch in range(self.__batch_size):\r\n            key = np.random.randint(low=0, high=len(self.__midi_df_list))\r\n            midi_df = self.__midi_df_list[key]\r\n            program_arr = midi_df.program.drop_duplicates().values\r\n            key = np.random.randint(low=0, high=program_arr.shape[0])\r\n            program_key = program_arr[key]\r\n            midi_df = midi_df[midi_df.program == program_key]\r\n            if midi_df.shape[0] < self.__seq_len:\r\n                raise ValueError(\"The length of musical performance (program: \" + str(program_key) + \" is short.\")\r\n\r\n            row = np.random.uniform(\r\n                low=midi_df.start.min(), \r\n                high=midi_df.end.max() - (self.__seq_len * self.__time_fraction)\r\n            )\r\n            for seq in range(self.__seq_len):\r\n                start = row + (seq * self.__time_fraction)\r\n                end = row + ((seq+1) * self.__time_fraction)\r\n                df = midi_df[(start <= midi_df.start) & (midi_df.start <= end)]\r\n                sampled_arr[batch, seq] = self.__convert_into_feature(df)\r\n\r\n        return sampled_arr", "response": "Returns a numpy array of samples from the true distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url_to_text(self, url):\n        '''\n        Download PDF file and transform its document to string.\n\n        Args:\n            url:   PDF url.\n\n        Returns:\n            string.\n\n        '''\n        path, headers = urllib.request.urlretrieve(url)\n        return self.path_to_text(path)", "response": "Download PDF file and transform its document to string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef path_to_text(self, path):\n        '''\n        Transform local PDF file to string.\n\n        Args:\n            path:   path to PDF file.\n\n        Returns:\n            string.\n\n        '''\n        rsrcmgr = PDFResourceManager()\n        retstr = StringIO()\n        codec = 'utf-8'\n        laparams = LAParams()\n        device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n        fp = open(path, 'rb')\n        interpreter = PDFPageInterpreter(rsrcmgr, device)\n        password = \"\"\n        maxpages = 0\n        caching = True\n        pagenos = set()\n\n        pages_data = PDFPage.get_pages(\n            fp,\n            pagenos,\n            maxpages=maxpages,\n            password=password,\n            caching=caching,\n            check_extractable=True\n        )\n\n        for page in pages_data:\n            interpreter.process_page(page)\n\n        text = retstr.getvalue()\n        text = text.replace(\"\\n\", \"\")\n\n        fp.close()\n        device.close()\n        retstr.close()\n        return text", "response": "Transform local PDF file to text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndivide string into sentence list.", "response": "def listup_sentence(self, data, counter=0):\n        '''\n        Divide string into sentence list.\n\n        Args:\n            data:               string.\n            counter:            recursive counter.\n\n        Returns:\n            List of sentences.\n\n        '''\n        delimiter = self.delimiter_list[counter]\n        sentence_list = []\n        [sentence_list.append(sentence + delimiter) for sentence in data.split(delimiter) if sentence != \"\"]\n        if counter + 1 < len(self.delimiter_list):\n            sentence_list_r = []\n            [sentence_list_r.extend(self.listup_sentence(sentence, counter+1)) for sentence in sentence_list]\n            sentence_list = sentence_list_r\n\n        return sentence_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes likelihood of the current state of the current state.", "response": "def likelihood(self):\n        '''\n        Compute likelihood.\n\n        Returns:\n            likelihood.\n        '''\n        try:\n            likelihood = self.__success / (self.__success + self.__failure)\n        except ZeroDivisionError:\n            likelihood = 0.0\n        return likelihood"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expected_value(self):\n        '''\n        Compute expected value.\n\n        Returns:\n            Expected value.\n        '''\n        alpha = self.__success + self.__default_alpha\n        beta = self.__failure + self.__default_beta\n\n        try:\n            expected_value = alpha / (alpha + beta)\n        except ZeroDivisionError:\n            expected_value = 0.0\n        return expected_value", "response": "Compute expected value.\n\n        Returns:\n            Expected value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing variance. variance returns 0. 0", "response": "def variance(self):\n        '''\n        Compute variance.\n\n        Returns:\n            variance.\n        '''\n        alpha = self.__success + self.__default_alpha\n        beta = self.__failure + self.__default_beta\n\n        try:\n            variance = alpha * beta / ((alpha + beta) ** 2) * (alpha + beta + 1)\n        except ZeroDivisionError:\n            variance = 0.0\n        return variance"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the reward value.", "response": "def observe_reward_value(self, state_key, action_key):\n        '''\n        Compute the reward value.\n        \n        Args:\n            state_key:              The key of state.\n            action_key:             The key of action.\n        \n        Returns:\n            Reward value.\n\n        '''\n        reward_value = 0.0\n        if state_key in self.__state_action_list_dict:\n            if action_key in self.__state_action_list_dict[state_key]:\n                reward_value = 1.0\n\n        return reward_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_tokens_into_matrix(self, token_list):\r\n        '''\r\n        Create matrix of sentences.\r\n\r\n        Args:\r\n            token_list:     The list of tokens.\r\n        \r\n        Returns:\r\n            2-D `np.ndarray` of sentences.\r\n            Each row means one hot vectors of one sentence.\r\n        '''\r\n        return np.array(self.vectorize(token_list)).astype(np.float32)", "response": "Converts a list of tokens into a 2 - D matrix of sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves in the feature map.", "response": "def __move(self, current_pos):\n        '''\n        Move in the feature map.\n\n        Args:\n            current_pos:    The now position.\n\n        Returns:\n            The next position.\n        '''\n        if self.__move_range is not None:\n            next_pos = np.random.randint(current_pos - self.__move_range, current_pos + self.__move_range)\n            if next_pos < 0:\n                next_pos = 0\n            elif next_pos >= self.var_arr.shape[0] - 1:\n                next_pos = self.var_arr.shape[0] - 1\n            return next_pos\n        else:\n            next_pos = np.random.randint(self.var_arr.shape[0] - 1)\n            return next_pos"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nanneal method for the annealing of the current set of modules.", "response": "def annealing(self):\n        '''\n        Annealing.\n        '''\n        shape_list = list(self.var_arr.shape)\n        shape_list[0] = self.__cycles_num + 1\n        self.var_log_arr = np.zeros(tuple(shape_list))\n\n        current_pos = self.__start_pos\n        current_var_arr = self.var_arr[current_pos, :]\n        current_cost_arr = self.__cost_functionable.compute(self.var_arr[current_pos, :])\n\n        self.computed_cost_arr = np.zeros(self.__cycles_num + 1)\n        self.computed_cost_arr[0] = current_cost_arr\n\n        t = self.__init_temp\n        delta_e_avg = 0.0\n        pos_log_list = [current_pos]\n        predicted_log_list = []\n        for i in range(self.__cycles_num):\n            if isinstance(self.__tolerance_diff_e, float) and len(predicted_log_list) > 1:\n                diff = abs(predicted_log_list[-1][2] - predicted_log_list[-2][2])\n                if diff < self.__tolerance_diff_e:\n                    break\n\n            for j in range(self.__trials_per_cycle):\n                current_pos = self.__move(current_pos)\n                pos_log_list.append(current_pos)\n                self.__now_dist_mat_arr = self.var_arr[current_pos, :]\n                cost_arr = self.__cost_functionable.compute(self.__now_dist_mat_arr)\n                delta_e = np.abs(cost_arr - current_cost_arr)\n                \n                if (cost_arr > current_cost_arr):\n                    if (i == 0 and j == 0):\n                        delta_e_avg = delta_e\n                    try:\n                        p = np.exp(-delta_e/(delta_e_avg * t))\n                    except ZeroDivisionError:\n                        p = 0.0\n\n                    if (np.random.random() < p):\n                        accept = True\n                    else:\n                        accept = False\n                else:\n                    accept = True\n                    p = 0.0\n\n                if accept is True:\n                    current_var_arr = self.__now_dist_mat_arr\n                    current_cost_arr = cost_arr\n                    self.__accepted_sol_num = self.__accepted_sol_num + 1.0\n                    delta_e_avg = (delta_e_avg * (self.__accepted_sol_num - 1.0) +  delta_e) / self.__accepted_sol_num\n                predicted_log_list.append((cost_arr , delta_e, delta_e_avg, p, int(accept)))\n\n            self.var_log_arr[i + 1] = current_var_arr\n            self.computed_cost_arr[i + 1] = current_cost_arr\n            t = t * self.__fractional_reduction\n\n        self.predicted_log_arr = np.array(predicted_log_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw(self):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = self.noise_sampler.generate()\r\n        _ = self.inference(observed_arr)\r\n        feature_arr = self.__convolutional_auto_encoder.extract_feature_points_arr()\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            try:\r\n                feature_arr = self.__deconvolution_layer_list[i].forward_propagate(feature_arr)\r\n            except:\r\n                self.__logger.debug(\"Error raised in Deconvolution layer \" + str(i + 1))\r\n                raise\r\n\r\n        return feature_arr", "response": "Returns a numpy array of samples drawn from the fake distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef learn(self, grad_arr):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:   `np.ndarray` of gradients.\r\n\r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n\r\n        '''\r\n        deconvolution_layer_list = self.__deconvolution_layer_list[::-1]\r\n        for i in range(len(deconvolution_layer_list)):\r\n            try:\r\n                grad_arr = deconvolution_layer_list[i].back_propagate(grad_arr)\r\n            except:\r\n                self.__logger.debug(\"Error raised in Convolution layer \" + str(i + 1))\r\n                raise\r\n\r\n        self.__optimize_deconvolution_layer(self.__learning_rate, 1)\r\n\r\n        layerable_cnn_list = self.__convolutional_auto_encoder.layerable_cnn_list[::-1]\r\n        for i in range(len(layerable_cnn_list)):\r\n            try:\r\n                grad_arr = layerable_cnn_list[i].back_propagate(grad_arr)\r\n            except:\r\n                self.__logger.debug(\r\n                    \"Delta computation raised an error in CNN layer \" + str(len(layerable_cnn_list) - i)\r\n                )\r\n                raise\r\n\r\n        self.__convolutional_auto_encoder.optimize(self.__learning_rate, 1)\r\n\r\n        return grad_arr", "response": "Updates this Discriminator by ascending its stochastic gradient."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbacks propagation for Deconvolution layer. Args: learning_rate: Learning rate. epoch: Now epoch.", "response": "def __optimize_deconvolution_layer(self, learning_rate, epoch):\r\n        '''\r\n        Back propagation for Deconvolution layer.\r\n        \r\n        Args:\r\n            learning_rate:  Learning rate.\r\n            epoch:          Now epoch.\r\n            \r\n        '''\r\n        params_list = []\r\n        grads_list = []\r\n\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            if self.__deconvolution_layer_list[i].delta_weight_arr.shape[0] > 0:\r\n                params_list.append(self.__deconvolution_layer_list[i].graph.weight_arr)\r\n                grads_list.append(self.__deconvolution_layer_list[i].delta_weight_arr)\r\n\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            if self.__deconvolution_layer_list[i].delta_bias_arr.shape[0] > 0:\r\n                params_list.append(self.__deconvolution_layer_list[i].graph.bias_arr)\r\n                grads_list.append(self.__deconvolution_layer_list[i].delta_bias_arr)\r\n\r\n        params_list = self.__opt_params.optimize(\r\n            params_list,\r\n            grads_list,\r\n            learning_rate\r\n        )\r\n\r\n        i = 0\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            if self.__deconvolution_layer_list[i].delta_weight_arr.shape[0] > 0:\r\n                self.__deconvolution_layer_list[i].graph.weight_arr = params_list.pop(0)\r\n                if ((epoch + 1) % self.__attenuate_epoch == 0):\r\n                    self.__deconvolution_layer_list[i].graph.weight_arr = self.__opt_params.constrain_weight(\r\n                        self.__deconvolution_layer_list[i].graph.weight_arr\r\n                    )\r\n\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            if self.__deconvolution_layer_list[i].delta_bias_arr.shape[0] > 0:\r\n                self.__deconvolution_layer_list[i].graph.bias_arr = params_list.pop(0)\r\n\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            if self.__deconvolution_layer_list[i].delta_weight_arr.shape[0] > 0:\r\n                if self.__deconvolution_layer_list[i].delta_bias_arr.shape[0] > 0:\r\n                    self.__deconvolution_layer_list[i].reset_delta()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the encoder and decoder to minimize the reconstruction error of the inputs.", "response": "def update(self):\r\n        '''\r\n        Update the encoder and the decoder\r\n        to minimize the reconstruction error of the inputs.\r\n\r\n        Returns:\r\n            `np.ndarray` of the reconstruction errors.\r\n        '''\r\n        observed_arr = self.noise_sampler.generate()\r\n        inferenced_arr = self.inference(observed_arr)\r\n\r\n        error_arr = self.__convolutional_auto_encoder.computable_loss.compute_loss(\r\n            observed_arr,\r\n            inferenced_arr\r\n        )\r\n\r\n        delta_arr = self.__convolutional_auto_encoder.computable_loss.compute_delta(\r\n            observed_arr,\r\n            inferenced_arr\r\n        )\r\n\r\n        delta_arr = self.__convolutional_auto_encoder.back_propagation(delta_arr)\r\n        self.__convolutional_auto_encoder.optimize(self.__learning_rate, 1)\r\n\r\n        return error_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_readable_web_pdf(self):\n        ''' getter '''\n        if isinstance(self.__readable_web_pdf, ReadableWebPDF) is False and self.__readable_web_pdf is not None:\n            raise TypeError(\"The type of __readable_web_pdf must be ReadableWebPDF.\")\n        return self.__readable_web_pdf", "response": "getter Returns the readable web pdf object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scrape(self, url):\n        '''\n        Execute Web-Scraping.\n        The target dom objects are in self.__dom_object_list.\n\n        Args:\n            url:    Web site url.\n\n        Returns:\n            The result. this is a string.\n\n        @TODO(chimera0): check URLs format.\n        '''\n        if isinstance(url, str) is False:\n            raise TypeError(\"The type of url must be str.\")\n\n        if self.readable_web_pdf is not None and self.readable_web_pdf.is_pdf_url(url) is True:\n            web_data = self.readable_web_pdf.url_to_text(url)\n        else:\n            web_data = \"\"\n            req = urllib.request.Request(url=url)\n            with urllib.request.urlopen(req) as f:\n                web = f.read().decode('utf-8')\n                dom = pq(web)\n                [dom(remove_object).remove() for remove_object in self.__remove_object_list]\n\n                for dom_object in self.__dom_object_list:\n                    web_data += dom(dom_object).text()\n\n        sleep(1)\n        return web_data", "response": "Execute Web - Scraping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract MIDI data from a file.", "response": "def extract(self, file_path, is_drum=False):\n        '''\n        Extract MIDI file.\n        \n        Args:\n            file_path:    File path of MIDI.\n            is_drum:      Extract drum data or not.\n            \n        Returns:\n            pd.DataFrame(columns=[\"program\", \"start\", \"end\", \"pitch\", \"velocity\", \"duration\"])\n        '''\n        midi_data = pretty_midi.PrettyMIDI(file_path)\n        note_tuple_list = []\n        for instrument in midi_data.instruments:\n            if (is_drum is False and instrument.is_drum is False) or (is_drum is True and instrument.is_drum is True):\n                for note in instrument.notes:\n                    note_tuple_list.append((instrument.program, note.start, note.end, note.pitch, note.velocity))\n        note_df = pd.DataFrame(note_tuple_list, columns=[\"program\", \"start\", \"end\", \"pitch\", \"velocity\"])\n        note_df = note_df.sort_values(by=[\"program\", \"start\", \"end\"])\n        note_df[\"duration\"] = note_df.end - note_df.start\n\n        return note_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, file_path, note_df):\n        '''\n        Save MIDI file.\n        \n        Args:\n            file_path:    File path of MIDI.\n            note_df:      `pd.DataFrame` of note data.\n            \n        '''\n\n        chord = pretty_midi.PrettyMIDI()\n        for program in note_df.program.drop_duplicates().values.tolist():\n            df = note_df[note_df.program == program]\n            midi_obj = pretty_midi.Instrument(program=program)\n            for i in range(df.shape[0]):\n                note = pretty_midi.Note(\n                    velocity=int(df.iloc[i, :][\"velocity\"]),\n                    pitch=int(df.iloc[i, :][\"pitch\"]),\n                    start=float(df.iloc[i, :][\"start\"]), \n                    end=float(df.iloc[i, :][\"end\"])\n                )\n                # Add it to our cello instrument\n                midi_obj.notes.append(note)\n            # Add the cello instrument to the PrettyMIDI object\n            chord.instruments.append(midi_obj)\n        # Write out the MIDI data\n        chord.write(file_path)", "response": "Save MIDI file.\n        \n        Args:\n            file_path:    File path of MIDI.\n            note_df:      `pd.DataFrame` of note data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute(self, x):\n        '''\n        Compute cost.\n        \n        Args:\n            x:    `np.ndarray` of explanatory variables.\n        \n        Returns:\n            cost\n        '''\n        q_learning = copy(self.__greedy_q_learning)\n        q_learning.epsilon_greedy_rate = x[0]\n        q_learning.alpha_value = x[1]\n        q_learning.gamma_value = x[2]\n        if self.__init_state_key is not None:\n            q_learning.learn(state_key=self.__init_state_key, limit=int(x[3]))\n        else:\n            q_learning.learn(limit=x[3])\n        q_sum = q_learning.q_df.q_value.sum()\n        if q_sum != 0:\n            cost = q_learning.q_df.shape[0] / q_sum\n        else:\n            cost = q_learning.q_df.shape[0] / 1e-4\n\n        return cost", "response": "Compute cost.\n        \n        Args:\n            x:    `np.ndarray` of explanatory variables.\n        \n        Returns:\n            cost"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_top_sentences(self, value):\n        ''' setter '''\n        if isinstance(value, int) is False:\n            raise TypeError(\"The type of __top_sentences must be int.\")\n        self.__top_sentences = value", "response": "setter for __top_sentences field"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute summarization. Args: document: The target document. Abstractor: The object of AbstractableDoc. similarity_filter The object of SimilarityFilter. Returns: dict data. - \"summarize_result\": The list of summarized sentences., - \"scoring_data\": The list of scores.", "response": "def summarize(self, document, Abstractor, similarity_filter=None):\n        '''\n        Execute summarization.\n\n        Args:\n            document:           The target document.\n            Abstractor:         The object of AbstractableDoc.\n            similarity_filter   The object of SimilarityFilter.\n\n        Returns:\n            dict data.\n            - \"summarize_result\": The list of summarized sentences., \n            - \"scoring_data\":     The list of scores.\n        '''\n        if isinstance(document, str) is False:\n            raise TypeError(\"The type of document must be str.\")\n\n        if isinstance(Abstractor, AbstractableDoc) is False:\n            raise TypeError(\"The type of Abstractor must be AbstractableDoc.\")\n\n        if isinstance(similarity_filter, SimilarityFilter) is False and similarity_filter is not None:\n            raise TypeError(\"The type of similarity_filter must be SimilarityFilter.\")\n\n        normalized_sentences = self.listup_sentence(document)\n\n        # for filtering similar sentences.\n        if similarity_filter is not None:\n            normalized_sentences = similarity_filter.similar_filter_r(normalized_sentences)\n\n        self.tokenize(document)\n        words = self.token\n\n        fdist = nltk.FreqDist(words)\n        top_n_words = [w[0] for w in fdist.items()][:self.target_n]\n        scored_list = self.__closely_associated_score(normalized_sentences, top_n_words)\n        filtered_list = Abstractor.filter(scored_list)\n        result_list = [normalized_sentences[idx] for (idx, score) in filtered_list]\n        result_dict = {\n            \"summarize_result\": result_list,\n            \"scoring_data\": filtered_list\n        }\n        return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __closely_associated_score(self, normalized_sentences, top_n_words):\n        '''\n        Scoring the sentence with closely associations.\n\n        Args:\n            normalized_sentences:   The list of sentences.\n            top_n_words:            Important sentences.\n\n        Returns:\n            The list of scores.\n        '''\n        scores_list = []\n        sentence_idx = -1\n\n        for sentence in normalized_sentences:\n            self.tokenize(sentence)\n            sentence = self.token\n\n            sentence_idx += 1\n            word_idx = []\n\n            for w in top_n_words:\n                try:\n                    word_idx.append(sentence.index(w))\n                except ValueError:\n                    pass\n\n            word_idx.sort()\n\n            if len(word_idx) == 0:\n                continue\n\n            clusters = []\n            cluster = [word_idx[0]]\n            i = 1\n            while i < len(word_idx):\n                if word_idx[i] - word_idx[i - 1] < self.cluster_threshold:\n                    cluster.append(word_idx[i])\n                else:\n                    clusters.append(cluster[:])\n                    cluster = [word_idx[i]]\n                i += 1\n            clusters.append(cluster)\n\n            max_cluster_score = 0\n            for c in clusters:\n                significant_words_in_cluster = len(c)\n                total_words_in_cluster = c[-1] - c[0] + 1\n                score = 1.0 * significant_words_in_cluster \\\n                    * significant_words_in_cluster / total_words_in_cluster\n\n                if score > max_cluster_score:\n                    max_cluster_score = score\n\n            scores_list.append((sentence_idx, score))\n\n        return scores_list", "response": "Scoring the sentence with closely associations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a random array of samples from the true distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        return np.random.normal(loc=self.__mu, scale=self.__sigma, size=self.__output_shape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlearning the state of the multi - agent.", "response": "def learn(self, initial_state_key, limit=1000, game_n=1):\n        '''\n        Multi-Agent Learning.\n\n        Override.\n        \n        Args:\n            initial_state_key:  Initial state.\n            limit:              Limit of the number of learning.\n            game_n:             The number of games.\n            \n        '''\n        end_flag = False\n        state_key_list = [None] * len(self.q_learning_list)\n        action_key_list = [None] * len(self.q_learning_list)\n        next_action_key_list = [None] * len(self.q_learning_list)\n        for game in range(game_n):\n            state_key = initial_state_key\n            self.t = 1\n            while self.t <= limit:\n                for i in range(len(self.q_learning_list)):\n                    state_key_list[i] = state_key\n                    if game + 1 == game_n:\n                        self.state_key_list.append(tuple(i, state_key_list))\n                    self.q_learning_list[i].t = self.t\n                    next_action_list = self.q_learning_list[i].extract_possible_actions(tuple(i, state_key_list))\n                    if len(next_action_list):\n                        action_key = self.q_learning_list[i].select_action(\n                            state_key=tuple(i, state_key_list),\n                            next_action_list=next_action_list\n                        )\n                        action_key_list[i] = action_key\n                        reward_value = self.q_learning_list[i].observe_reward_value(\n                            tuple(i, state_key_list), \n                            tuple(i, action_key_list)\n                        )\n\n                        # Check.\n                        if self.q_learning_list[i].check_the_end_flag(tuple(i, state_key_list)) is True:\n                            end_flag = True\n\n                        # Max-Q-Value in next action time.\n                        next_next_action_list = self.q_learning_list[i].extract_possible_actions(\n                            tuple(i, action_key_list)\n                        )\n                        if len(next_next_action_list):\n                            next_action_key = self.q_learning_list[i].predict_next_action(\n                                tuple(i, action_key_list), \n                                next_next_action_list\n                            )\n                            next_action_key_list[i] = next_action_key\n                            next_max_q = self.q_learning_list[i].extract_q_df(\n                                tuple(i, action_key_list), \n                                next_action_key\n                            )\n\n                            # Update Q-Value.\n                            self.q_learning_list[i].update_q(\n                                state_key=tuple(i, state_key_list),\n                                action_key=tuple(i, action_key_list),\n                                reward_value=reward_value,\n                                next_max_q=next_max_q\n                            )\n\n                            # Update State.\n                            state_key = self.q_learning_list[i].update_state(\n                                state_key=tuple(i, state_key_list),\n                                action_key=tuple(i, action_key_list)\n                            )\n                            state_key_list[i] = state_key\n\n                    # Epsode.\n                    self.t += 1\n                    self.q_learning_list[i].t = self.t\n                    if end_flag is True:\n                        break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new numpy array containing the samples drawn from the fake distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = self.extract_conditions()\r\n        conv_arr = self.inference(observed_arr)\r\n\r\n        if self.__conditon_noise_sampler is not None:\r\n            self.__conditon_noise_sampler.output_shape = conv_arr.shape\r\n            noise_arr = self.__conditon_noise_sampler.generate()\r\n            conv_arr += noise_arr\r\n\r\n        deconv_arr = self.__deconvolution_model.inference(conv_arr)\r\n        return np.concatenate((deconv_arr, observed_arr), axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\r\n        channel = grad_arr.shape[1] // 2\r\n        grad_arr = self.__deconvolution_model.learn(grad_arr[:, :channel], fix_opt_flag=fix_opt_flag)\r\n        delta_arr = self.__cnn.back_propagation(grad_arr)\r\n        if fix_opt_flag is False:\r\n            self.__cnn.optimize(self.__learning_rate, 1)\r\n        return delta_arr", "response": "Updates this Discriminator by ascending its stochastic gradient."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw samples from the fake distribution.", "response": "def inference(self, observed_arr):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Args:\r\n            observed_arr:     `np.ndarray` of observed data points.\r\n        \r\n        Returns:\r\n            `np.ndarray` of inferenced.\r\n        '''\r\n        for i in range(len(self.__deconvolution_layer_list)):\r\n            try:\r\n                observed_arr = self.__deconvolution_layer_list[i].forward_propagate(observed_arr)\r\n            except:\r\n                self.__logger.debug(\"Error raised in Deconvolution layer \" + str(i + 1))\r\n                raise\r\n\r\n        return observed_arr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating this Discriminator by ascending its stochastic gradient.", "response": "def learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\r\n        deconvolution_layer_list = self.__deconvolution_layer_list[::-1]\r\n        for i in range(len(deconvolution_layer_list)):\r\n            try:\r\n                grad_arr = deconvolution_layer_list[i].back_propagate(grad_arr)\r\n            except:\r\n                self.__logger.debug(\"Error raised in Convolution layer \" + str(i + 1))\r\n                raise\r\n\r\n        if fix_opt_flag is False:\r\n            self.__optimize(self.__learning_rate, 1)\r\n        \r\n        return grad_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inference(self, observed_arr):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n\r\n        Args:\r\n            observed_arr:     `np.ndarray` of observed data points.\r\n        \r\n        Returns:\r\n            `np.ndarray` of inferenced.\r\n        '''\r\n        self.__pred_arr = self.__lstm_model.inference(observed_arr)\r\n        return self.__pred_arr", "response": "Draw samples from the true distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\r\n        if grad_arr.ndim > 3:\r\n            grad_arr = grad_arr.reshape((\r\n                grad_arr.shape[0],\r\n                grad_arr.shape[1],\r\n                -1\r\n            ))\r\n\r\n        delta_arr, grads_list = self.__lstm_model.back_propagation(self.__pred_arr, grad_arr)\r\n\r\n        if fix_opt_flag is False:\r\n            self.__lstm_model.optimize(\r\n                grads_list,\r\n                self.__learning_rate,\r\n                1\r\n            )\r\n\r\n        return delta_arr", "response": "Learn the current discriminator by ascending its stochastic gradient."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a random array of samples from the true distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        return np.random.uniform(loc=self.__low, scale=self.__high, size=self.__output_shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nlp_base(self):\n        ''' getter '''\n        if isinstance(self.__nlp_base, NlpBase) is False:\n            raise TypeError(\"The type of self.__nlp_base must be NlpBase.\")\n\n        return self.__nlp_base", "response": "getter Returns the NlpBase object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_nlp_base(self, value):\n        ''' setter '''\n        if isinstance(value, NlpBase) is False:\n            raise TypeError(\"The type of value must be NlpBase.\")\n        \n        self.__nlp_base = value", "response": "setter This method will set the nlp base of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_similarity_limit(self):\n        ''' getter '''\n        if isinstance(self.__similarity_limit, float) is False:\n            raise TypeError(\"__similarity_limit must be float.\")\n        return self.__similarity_limit", "response": "getter Returns the current similarity limit value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving duplicated elements. Args: token_list_x: [token, token, token, ...] token_list_y: [token, token, token, ...] Returns: Tuple(token_list_x, token_list_y)", "response": "def unique(self, token_list_x, token_list_y):\n        '''\n        Remove duplicated elements.\n        \n        Args:\n            token_list_x:    [token, token, token, ...]\n            token_list_y:    [token, token, token, ...]\n\n        Returns:\n            Tuple(token_list_x, token_list_y)\n        '''\n        x = set(list(token_list_x))\n        y = set(list(token_list_y))\n        return (x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount the number of tokens in token_list.", "response": "def count(self, token_list):\n        '''\n        Count the number of tokens in `token_list`.\n        \n        Args:\n            token_list:    The list of tokens.\n\n        Returns:\n            {token: the numbers}\n        '''\n        token_dict = {}\n        for token in token_list:\n            if token in token_dict:\n                token_dict[token] += 1\n            else:\n                token_dict[token] = 1\n        return token_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter mutually similar sentences.", "response": "def similar_filter_r(self, sentence_list):\n        '''\n        Filter mutually similar sentences.\n        \n        Args:\n            sentence_list:    The list of sentences.\n\n        Returns:\n            The list of filtered sentences.\n        '''\n        result_list = []\n        recursive_list = []\n\n        try:\n            self.nlp_base.tokenize(sentence_list[0])\n            subject_token = self.nlp_base.token\n            result_list.append(sentence_list[0])\n            if len(sentence_list) > 1:\n                for i in range(len(sentence_list)):\n                    if i > 0:\n                        self.nlp_base.tokenize(sentence_list[i])\n                        object_token = self.nlp_base.token\n                        similarity = self.calculate(subject_token, object_token)\n                        if similarity <= self.similarity_limit:\n                            recursive_list.append(sentence_list[i])\n\n            if len(recursive_list) > 0:\n                result_list.extend(self.similar_filter_r(recursive_list))\n        except IndexError:\n            result_list = sentence_list\n\n        return result_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the similarity of two lists of tokens with the Jaccard coefficient.", "response": "def calculate(self, token_list_x, token_list_y):\r\n        '''\r\n        Calculate similarity with the Jaccard coefficient.\r\n        \r\n        Concrete method.\r\n        \r\n        Args:\r\n            token_list_x:    [token, token, token, ...]\r\n            token_list_y:    [token, token, token, ...]\r\n        \r\n        Returns:\r\n            Similarity.\r\n        '''\r\n\r\n        x, y = self.unique(token_list_x, token_list_y)\r\n        try:\r\n            result = len(x & y) / len(x | y)\r\n        except ZeroDivisionError:\r\n            result = 0.0\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef learn_q(self, predicted_q_arr, real_q_arr):\n        '''\n        Infernce Q-Value.\n        \n        Args:\n            predicted_q_arr:    `np.ndarray` of predicted Q-Values.\n            real_q_arr:         `np.ndarray` of real Q-Values.\n        '''\n        \"\"\"\n        if self.__q_shape is None:\n            raise ValueError(\"Before learning, You should execute `__inference_q`.\")\n        \"\"\"\n\n        loss = self.__computable_loss.compute_loss(predicted_q_arr, real_q_arr)\n        delta_arr = self.__computable_loss.compute_delta(predicted_q_arr, real_q_arr)\n        delta_arr = self.__cnn.back_propagation(delta_arr)\n        self.__cnn.optimize(self.__learning_rate, 1)\n        self.__loss_list.append(loss)", "response": "Learn the Infernce Q - Value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_model(self):\n        '''\n        `object` of model as a function approximator,\n        which has `cnn` whose type is \n        `pydbm.cnn.pydbm.cnn.convolutional_neural_network.ConvolutionalNeuralNetwork`.\n        '''\n        class Model(object):\n            def __init__(self, cnn):\n                self.cnn = cnn\n\n        return Model(self.__cnn)", "response": "object of model as a function approximator cnn which has cnn whose type is \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef play_beat(\r\n        self,\r\n        frequencys,\r\n        play_time,\r\n        sample_rate=44100,\r\n        volume=0.01\r\n    ):\r\n        '''\r\n        \u5f15\u6570\u3067\u6307\u5b9a\u3057\u305f\u6761\u4ef6\u3067\u30d3\u30fc\u30c8\u3092\u9cf4\u3089\u3059\r\n\r\n        Args:\r\n            frequencys:     (\u5de6\u306e\u5468\u6ce2\u6570(Hz), \u53f3\u306e\u5468\u6ce2\u6570(Hz))\u306etuple\r\n            play_time:      \u518d\u751f\u6642\u9593\uff08\u79d2\uff09\r\n            sample_rate:    \u30b5\u30f3\u30d7\u30eb\u30ec\u30fc\u30c8\r\n            volume:         \u97f3\u91cf\r\n\r\n        Returns:\r\n            void\r\n        '''\r\n\r\n        # \u4f9d\u5b58\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u57fa\u5e95\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\r\n        audio = pyaudio.PyAudio()\r\n        # \u30b9\u30c8\u30ea\u30fc\u30e0\r\n        stream = audio.open(\r\n            format=pyaudio.paFloat32,\r\n            channels=2,\r\n            rate=sample_rate,\r\n            output=1\r\n        )\r\n        left_frequency, right_frequency = frequencys\r\n        left_chunk = self.__create_chunk(left_frequency, play_time, sample_rate)\r\n        right_chunk = self.__create_chunk(right_frequency, play_time, sample_rate)\r\n        self.write_stream(stream, left_chunk, right_chunk, volume)\r\n        stream.stop_stream()\r\n        stream.close()\r\n        audio.terminate()", "response": "Play a new audio file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a new beat file.", "response": "def save_beat(\r\n        self,\r\n        output_file_name,\r\n        frequencys,\r\n        play_time,\r\n        sample_rate=44100,\r\n        volume=0.01\r\n    ):\r\n        '''\r\n        \u5f15\u6570\u3067\u6307\u5b9a\u3057\u305f\u6761\u4ef6\u3067\u30d3\u30fc\u30c8\u3092\u9cf4\u3089\u3059\r\n\r\n        Args:\r\n            frequencys:     (\u5de6\u306e\u5468\u6ce2\u6570(Hz), \u53f3\u306e\u5468\u6ce2\u6570(Hz))\u306etuple\r\n            play_time:      \u518d\u751f\u6642\u9593\uff08\u79d2\uff09\r\n            sample_rate:    \u30b5\u30f3\u30d7\u30eb\u30ec\u30fc\u30c8\r\n            volume:         \u97f3\u91cf\r\n\r\n        Returns:\r\n            void\r\n        '''\r\n        left_frequency, right_frequency = frequencys\r\n        left_chunk = self.__create_chunk(left_frequency, play_time, sample_rate)\r\n        right_chunk = self.__create_chunk(right_frequency, play_time, sample_rate)\r\n        frame_list = self.read_stream(left_chunk, right_chunk, volume)\r\n\r\n        wf = wave.open(output_file_name, 'wb')\r\n        wf.setparams((2, 2, sample_rate, 0, 'NONE', 'not compressed'))\r\n        wf.writeframes(b''.join(frame_list))\r\n        wf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __create_chunk(self, frequency, play_time, sample_rate):\r\n        '''\r\n        \u30c1\u30e3\u30f3\u30af\u3092\u751f\u6210\u3059\u308b\r\n\r\n        Args:\r\n            frequency:      \u5468\u6ce2\u6570\r\n            play_time:      \u518d\u751f\u6642\u9593\uff08\u79d2\uff09\r\n            sample_rate:    \u30b5\u30f3\u30d7\u30eb\u30ec\u30fc\u30c8\r\n\r\n        Returns:\r\n            \u30c1\u30e3\u30f3\u30af\u306enumpy\u914d\u5217\r\n        '''\r\n        chunks = []\r\n        wave_form = self.wave_form.create(frequency, play_time, sample_rate)\r\n        chunks.append(wave_form)\r\n        chunk = numpy.concatenate(chunks)\r\n        return chunk", "response": "Create a new chunk of the same size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a numpy array of samples drawn from the true distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = self.__image_true_sampler.draw()\r\n        observed_arr = self.add_condition(observed_arr)\r\n        return observed_arr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_condition(self, observed_arr):\r\n        '''\r\n        Add condtion.\r\n\r\n        Args:\r\n            observed_arr:       `np.ndarray` of samples.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        condition_arr = self.__image_true_sampler.draw()\r\n        return np.concatenate((observed_arr, condition_arr), axis=1)", "response": "Adds condition to the observed_arr."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance of the class with the given frequency play_time and sample_rate.", "response": "def create(self, frequency, play_time, sample_rate):\r\n        '''\r\n        \u97f3\u306e\u6ce2\u5f62\u3092\u751f\u6210\u3059\u308b\r\n\r\n        Args:\r\n            frequency:      \u5468\u6ce2\u6570\r\n            play_time:      \u518d\u751f\u6642\u9593\r\n            sample_rate:    \u30b5\u30f3\u30d7\u30eb\u30ec\u30fc\u30c8\r\n\r\n        Returns:\r\n            \u6ce2\u5f62\u8981\u7d20\u3092\u683c\u7d0d\u3057\u305f\u914d\u5217\r\n        '''\r\n        length = int(play_time * sample_rate)\r\n        factor = float(frequency) * (math.pi * 2) / sample_rate\r\n        return numpy.sin(numpy.arange(length) * factor)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inference(self, state_arr, limit=1000):\n        '''\n        Infernce.\n        \n        Args:\n            state_arr:    `np.ndarray` of state.\n            limit:        The number of inferencing.\n        \n        Returns:\n            `list of `np.ndarray` of an optimal route.\n        '''\n        agent_x, agent_y = np.where(state_arr[0] == 1)\n        agent_x, agent_y = agent_x[0], agent_y[0]\n        result_list = [(agent_x, agent_y, 0.0)]\n        self.t = 1\n        while self.t <= limit:\n            next_action_arr = self.extract_possible_actions(state_arr)\n            next_q_arr = self.function_approximator.inference_q(next_action_arr)\n            action_arr, q = self.select_action(next_action_arr, next_q_arr)\n\n            agent_x, agent_y = np.where(action_arr[0] == 1)\n            agent_x, agent_y = agent_x[0], agent_y[0]\n            result_list.append((agent_x, agent_y, q[0]))\n\n            # Update State.\n            state_arr = self.update_state(state_arr, action_arr)\n\n            # Epsode.\n            self.t += 1\n            # Check.\n            end_flag = self.check_the_end_flag(state_arr)\n            if end_flag is True:\n                break\n\n        return result_list", "response": "Infer the state of a single class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef observe_reward_value(self, state_arr, action_arr):\n        '''\n        Compute the reward value.\n        \n        Args:\n            state_arr:              `np.ndarray` of state.\n            action_arr:             `np.ndarray` of action.\n        \n        Returns:\n            Reward value.\n        '''\n        if self.__check_goal_flag(action_arr) is True:\n            return 1.0\n        else:\n            x, y = np.where(action_arr[-1] == 1)\n            x, y = x[0], y[0]\n            goal_x, goal_y = self.__goal_pos\n            \n            if x == goal_x and y == goal_y:\n                distance = 0.0\n            else:\n                distance = np.sqrt(((x - goal_x) ** 2) + (y - goal_y) ** 2)\n\n            if (x, y) in self.__route_long_memory_list:\n                repeating_penalty = self.__repeating_penalty\n            else:\n                repeating_penalty = 0.0\n            return 1.0 - distance - repeating_penalty", "response": "Compute the reward value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract now map state. Returns np. ndarray of state.", "response": "def extract_now_state(self):\n        '''\n        Extract now map state.\n        \n        Returns:\n            `np.ndarray` of state.\n        '''\n        x, y = self.__agent_pos\n        state_arr = np.zeros(self.__map_arr.shape)\n        state_arr[x, y] = 1\n        return np.expand_dims(state_arr, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate state. Override. Args: state_arr: `np.ndarray` of state in `self.t`. action_arr: `np.ndarray` of action in `self.t`. Returns: `np.ndarray` of state in `self.t+1`.", "response": "def update_state(self, state_arr, action_arr):\n        '''\n        Update state.\n        \n        Override.\n\n        Args:\n            state_arr:    `np.ndarray` of state in `self.t`.\n            action_arr:   `np.ndarray` of action in `self.t`.\n        \n        Returns:\n            `np.ndarray` of state in `self.t+1`.\n        '''\n        x, y = np.where(action_arr[-1] == 1)\n        self.__agent_pos = (x[0], y[0])\n        self.__route_memory_list.append((x[0], y[0]))\n        self.__route_long_memory_list.append((x[0], y[0]))\n        self.__route_long_memory_list = list(set(self.__route_long_memory_list))\n        while len(self.__route_memory_list) > self.__memory_num:\n            self.__route_memory_list = self.__route_memory_list[1:]\n\n        return self.extract_now_state()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a map of the specified size.", "response": "def __create_map(self, map_size):\n        '''\n        Create map.\n        \n        References:\n            - https://qiita.com/kusano_t/items/487eec15d42aace7d685\n        '''\n        import random\n        import numpy as np\n        from itertools import product\n\n        news = ['n', 'e', 'w', 's']\n\n        m, n = map_size\n\n        SPACE = self.SPACE\n        WALL = self.WALL\n        START = self.START\n        GOAL = self.GOAL\n\n        memo = np.array([i for i in range(n * m)])\n        memo = memo.reshape(m, n)\n\n        # \u8ff7\u8def\u3092\u521d\u671f\u5316\n        maze = [[SPACE for _ in range(2 * n + 1)] for _ in range(2 * m + 1)]\n        maze[self.START_POS[0]][self.START_POS[1]] = START\n        \n        self.__goal_pos = (2 * m - 1, 2 * n - 1)\n\n        maze[2 * m - 1][2 * n - 1] = GOAL\n        for i, j in product(range(2 * m + 1), range(2 * n + 1)):\n            if i % 2 == 0 or j % 2 == 0:\n                maze[i][j] = WALL\n\n        while (memo != 0).any():\n            x1 = random.choice(range(m))\n            y1 = random.choice(range(n))\n            direction = random.choice(news)\n\n            if direction == 'e':\n                x2, y2 = x1, y1 + 1\n            elif direction == 'w':\n                x2, y2 = x1, y1 - 1\n            elif direction == 'n':\n                x2, y2 = x1 - 1, y1\n            elif direction == 's':\n                x2, y2 = x1 + 1, y1\n\n            # \u7bc4\u56f2\u5916\u306e\u5834\u5408\u306fcontinue\n            if (x2 < 0) or (x2 >= m) or (y2 < 0) or (y2 >= n):\n                continue\n\n            if memo[x1, y1] != memo[x2, y2]:\n                tmp_min = min(memo[x1, y1], memo[x2, y2])\n                tmp_max = max(memo[x1, y1], memo[x2, y2])\n\n                # \u30e1\u30e2\u306e\u66f4\u65b0\n                memo[memo == tmp_max] = tmp_min\n\n                # \u58c1\u3092\u58ca\u3059\n                maze[x1 + x2 + 1][y1 + y2 + 1] = SPACE\n\n        maze_arr = np.array(maze)\n        return maze_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vectorize(self, sentence_list):\n        '''\n        Args:\n            sentence_list:   The list of tokenized sentences.\n                             [[`token`, `token`, `token`, ...],\n                             [`token`, `token`, `token`, ...],\n                             [`token`, `token`, `token`, ...]]\n        \n        Returns:\n            `np.ndarray` of tokens.\n            [vector of token, vector of token, vector of token]\n        '''\n        test_observed_arr = self.__setup_dataset(sentence_list, self.__token_master_list, self.__seq_len)\n\n        inferenced_arr = self.__rbm.inference(\n            test_observed_arr,\n            training_count=1, \n            r_batch_size=-1\n        )\n\n        return inferenced_arr", "response": "Vectorizes the list of tokenized sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef learn(\n        self,\n        sentence_list,\n        token_master_list,\n        hidden_neuron_count=1000,\n        training_count=1,\n        batch_size=100,\n        learning_rate=1e-03,\n        seq_len=5\n    ):\n        '''\n        Init.\n        \n        Args:\n            sentence_list:                  The `list` of sentences.\n            token_master_list:              Unique `list` of tokens.\n            hidden_neuron_count:            The number of units in hidden layer.\n            training_count:                 The number of training.\n            bath_size:                      Batch size of Mini-batch.\n            learning_rate:                  Learning rate.\n            seq_len:                        The length of one sequence.\n        '''\n        observed_arr = self.__setup_dataset(sentence_list, token_master_list, seq_len)\n\n        visible_num = observed_arr.shape[-1]\n\n        # `Builder` in `Builder Pattern` for LSTM-RTRBM.\n        rnnrbm_builder = LSTMRTRBMSimpleBuilder()\n        # Learning rate.\n        rnnrbm_builder.learning_rate = learning_rate\n        # Set units in visible layer.\n        rnnrbm_builder.visible_neuron_part(LogisticFunction(), visible_num)\n        # Set units in hidden layer.\n        rnnrbm_builder.hidden_neuron_part(LogisticFunction(), hidden_neuron_count) \n        # Set units in RNN layer.\n        rnnrbm_builder.rnn_neuron_part(TanhFunction())\n        # Set graph and approximation function, delegating `SGD` which is-a `OptParams`.\n        rnnrbm_builder.graph_part(LSTMRTRBMCD(opt_params=SGD()))\n        # Building.\n        rbm = rnnrbm_builder.get_result()\n        \n        # Learning.\n        rbm.learn(\n            # The `np.ndarray` of observed data points.\n            observed_arr,\n            # Training count.\n            training_count=training_count, \n            # Batch size.\n            batch_size=batch_size\n        )\n        \n        self.__rbm = rbm\n        self.__token_master_list = token_master_list\n        self.__seq_len = seq_len", "response": "This function initializes the related class of LSTM - RTRBM and learns the related class of the related class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw samples from the true distribution.", "response": "def inference(self, observed_arr):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n\r\n        Args:\r\n            observed_arr:     `np.ndarray` of observed data points.\r\n        \r\n        Returns:\r\n            `np.ndarray` of inferenced.\r\n        '''\r\n        if observed_arr.ndim < 4:\r\n            # Add rank for channel.\r\n            observed_arr = np.expand_dims(observed_arr, axis=1)\r\n            self.__add_channel_flag = True\r\n        else:\r\n            self.__add_channel_flag = False\r\n\r\n        return super().inference(observed_arr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate this Discriminator by ascending its stochastic gradient.", "response": "def learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\r\n        delta_arr = super().learn(grad_arr, fix_opt_flag)\r\n        if self.__add_channel_flag is True:\r\n            return delta_arr[:, 0]\r\n        else:\r\n            return delta_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the similarity with the so - called Cosine similarity of Tf - Idf vectors.", "response": "def calculate(self, token_list_x, token_list_y):\n        '''\n        Calculate similarity with the so-called Cosine similarity of Tf-Idf vectors.\n        \n        Concrete method.\n        \n        Args:\n            token_list_x:    [token, token, token, ...]\n            token_list_y:    [token, token, token, ...]\n        \n        Returns:\n            Similarity.\n        '''\n        if len(token_list_x) == 0 or len(token_list_y) == 0:\n            return 0.0\n\n        document_list = token_list_x.copy()\n        [document_list.append(v) for v in token_list_y]\n        document_list = list(set(document_list))\n\n        tfidf_vectorizer = TfidfVectorizer(document_list)\n\n        vector_list_x = tfidf_vectorizer.vectorize(token_list_x)\n        vector_list_y = tfidf_vectorizer.vectorize(token_list_y)\n        \n        if len(vector_list_x) > len(vector_list_y):\n            [vector_list_y.append(0.0) for _ in range(len(vector_list_x) - len(vector_list_y))]\n        elif len(vector_list_y) > len(vector_list_x):\n            [vector_list_x.append(0.0) for _ in range(len(vector_list_y) - len(vector_list_x))]\n\n        dot_prod = np.dot(vector_list_x, vector_list_y)\n        norm_x = np.linalg.norm(vector_list_x)\n        norm_y = np.linalg.norm(vector_list_y)\n        try:\n            result = dot_prod / (norm_x * norm_y)\n            if np.isnan(result) is True:\n                return 0.0\n            else:\n                return result\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes distance. Args: x: Data point. y: Data point. Returns: Distance.", "response": "def compute(self, x, y):\n        '''\n        Compute distance.\n        \n        Args:\n            x:    Data point.\n            y:    Data point.\n        \n        Returns:\n            Distance.\n        '''\n        if x in self.__memo_dict:\n            x_v = self.__memo_dict[x]\n        else:\n            x_v = self.__cost_functionable.compute(self.__params_arr[x, :])\n            self.__memo_dict.setdefault(x, x_v)\n        if y in self.__memo_dict:\n            y_v = self.__memo_dict[y]\n        else:\n            y_v = self.__cost_functionable.compute(self.__params_arr[y, :])\n            self.__memo_dict.setdefault(y, y_v)\n\n        return abs(x_v - y_v)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the reward value.", "response": "def observe_reward_value(self, state_key, action_key):\n        '''\n        Compute the reward value.\n        \n        Args:\n            state_key:              The key of state.\n            action_key:             The key of action.\n        \n        Returns:\n            Reward value.\n\n        '''\n        x, y = state_key\n\n        if self.__map_arr[y][x] == self.__end_point_label:\n            return 100.0\n        elif self.__map_arr[y][x] == self.__start_point_label:\n            return 0.0\n        elif self.__map_arr[y][x] == self.__wall_label:\n            raise ValueError(\"It is the wall. (x, y)=(%d, %d)\" % (x, y))\n        else:\n            reward_value = float(self.__map_arr[y][x])\n            self.save_r_df(state_key, reward_value)\n            return reward_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the end flag.", "response": "def check_the_end_flag(self, state_key):\n        '''\n        Check the end flag.\n        \n        If this return value is `True`, the learning is end.\n\n        Args:\n            state_key:    The key of state in `self.t`.\n\n        Returns:\n            bool\n        '''\n        # As a rule, the learning can not be stopped.\n        x, y = state_key\n        end_point_tuple = np.where(self.__map_arr == self.__end_point_label)\n        end_point_x_arr, end_point_y_arr = end_point_tuple\n        if x == end_point_x_arr[0] and y == end_point_y_arr[0]:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_q_value(self):\n        '''\n        Normalize q-value.\n        \n        Override.\n        \n        This method is called in each learning steps.\n        \n        For example:\n            self.q_df.q_value = self.q_df.q_value / self.q_df.q_value.sum()\n        '''\n        if self.q_df is not None and self.q_df.shape[0]:\n            # min-max normalization\n            self.q_df.q_value = (self.q_df.q_value - self.q_df.q_value.min()) / (self.q_df.q_value.max() - self.q_df.q_value.min())", "response": "Normalize q - value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_r_value(self):\n        '''\n        Normalize r-value.\n\n        Override.\n\n        This method is called in each learning steps.\n\n        For example:\n            self.r_df = self.r_df.r_value / self.r_df.r_value.sum()\n        '''\n        if self.r_df is not None and self.r_df.shape[0]:\n            # z-score normalization.\n            self.r_df.r_value = (self.r_df.r_value - self.r_df.r_value.mean()) / self.r_df.r_value.std()", "response": "Normalize r - value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw(self):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = self.noise_sampler.generate()\r\n        _ = self.__encoder_decoder_controller.encoder.inference(observed_arr)\r\n        arr = self.__encoder_decoder_controller.encoder.get_feature_points()\r\n        return arr", "response": "Returns a numpy array of samples drawn from the fake distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlearn this Discriminator by ascending its stochastic gradient.", "response": "def learn(self, grad_arr):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:   `np.ndarray` of gradients.\r\n\r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n\r\n        '''\r\n        encoder_delta_arr, _, encoder_grads_list = self.__encoder_decoder_controller.encoder.hidden_back_propagate(\r\n            grad_arr[:, -1]\r\n        )\r\n        encoder_grads_list.insert(0, None)\r\n        encoder_grads_list.insert(0, None)\r\n\r\n        self.__encoder_decoder_controller.encoder.optimize(\r\n            encoder_grads_list, \r\n            self.__learning_rate,\r\n            1\r\n        )\r\n\r\n        return encoder_delta_arr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the encoder and decoder controller to minimize the reconstruction error of the inputs.", "response": "def update(self):\r\n        '''\r\n        Update the encoder and the decoder\r\n        to minimize the reconstruction error of the inputs.\r\n\r\n        Returns:\r\n            `np.ndarray` of the reconstruction errors.\r\n        '''\r\n        observed_arr = self.noise_sampler.generate()\r\n        inferenced_arr = self.inference(observed_arr)\r\n\r\n        error_arr = self.__encoder_decoder_controller.computable_loss.compute_loss(\r\n            observed_arr,\r\n            inferenced_arr\r\n        )\r\n        delta_arr = self.__encoder_decoder_controller.computable_loss.compute_delta(\r\n            observed_arr,\r\n            inferenced_arr\r\n        )\r\n        decoder_grads_list, encoder_delta_arr, encoder_grads_list = self.__encoder_decoder_controller.back_propagation(\r\n            delta_arr\r\n        )\r\n        self.__encoder_decoder_controller.optimize(\r\n            decoder_grads_list,\r\n            encoder_grads_list,\r\n            self.__learning_rate, \r\n            1\r\n        )\r\n\r\n        return error_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef learn(self, state_arr, limit=1000):\n        '''\n        Learning and searching the optimal solution.\n        \n        Args:\n            state_arr:      `np.ndarray` of initial state.\n            limit:          The maximum number of iterative updates based on value iteration algorithms.\n        '''\n        while self.t <= limit:\n            # Draw samples of next possible actions from any distribution.\n            next_action_arr = self.extract_possible_actions(state_arr)\n            # Inference Q-Values.\n            predicted_q_arr = self.__function_approximator.inference_q(next_action_arr)\n            # Set `np.ndarray` of rewards and next Q-Values.\n            reward_value_arr = np.empty((next_action_arr.shape[0], 1))\n            next_max_q_arr = np.empty((next_action_arr.shape[0], 1))\n            for i in range(reward_value_arr.shape[0]):\n                # Observe reward values.\n                reward_value_arr[i] = self.observe_reward_value(state_arr, next_action_arr[i])\n                # Inference the Max-Q-Value in next action time.\n                next_next_action_arr = self.extract_possible_actions(next_action_arr[i])\n                next_max_q_arr[i] = self.__function_approximator.inference_q(next_next_action_arr).max()\n\n            # Select action.\n            action_arr, predicted_q = self.select_action(next_action_arr, predicted_q_arr)\n            # Update real Q-Values.\n            real_q_arr = self.update_q(\n                predicted_q_arr,\n                reward_value_arr,\n                next_max_q_arr\n            )\n\n            # Maximum of predicted and real Q-Values.\n            real_q = real_q_arr[np.where(predicted_q_arr == predicted_q)[0][0]]\n            if self.__q_logs_arr.shape[0] > 0:\n                self.__q_logs_arr = np.r_[\n                    self.__q_logs_arr,\n                    np.array([predicted_q, real_q]).reshape(1, 2)\n                ]\n            else:\n                self.__q_logs_arr = np.array([predicted_q, real_q]).reshape(1, 2)\n\n            # Learn Q-Values.\n            self.learn_q(predicted_q_arr, real_q_arr)\n            # Update State.\n            state_arr = self.update_state(state_arr, action_arr)\n            # Epsode.\n            self.t += 1\n            # Check.\n            end_flag = self.check_the_end_flag(state_arr)\n            if end_flag is True:\n                break", "response": "Learning and searching the optimal solution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate Q. Args: predicted_q_arr: `np.ndarray` of predicted Q-Values. reward_value_arr: `np.ndarray` of reward values. next_max_q_arr: `np.ndarray` of maximum Q-Values in next time step. Returns: `np.ndarray` of real Q-Values.", "response": "def update_q(self, predicted_q_arr, reward_value_arr, next_max_q_arr):\n        '''\n        Update Q.\n        \n        Args:\n            predicted_q_arr:    `np.ndarray` of predicted Q-Values.\n            reward_value_arr:   `np.ndarray` of reward values.\n            next_max_q_arr:     `np.ndarray` of maximum Q-Values in next time step.\n        \n        Returns:\n            `np.ndarray` of real Q-Values.\n        '''\n        # Update Q-Value.\n        return predicted_q_arr + (self.alpha_value * (reward_value_arr + (self.gamma_value * next_max_q_arr) - predicted_q_arr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets alpha value of the log entry", "response": "def get_alpha_value(self):\n        '''\n        getter\n        Learning rate.\n        '''\n        if isinstance(self.__alpha_value, float) is False:\n            raise TypeError(\"The type of __alpha_value must be float.\")\n        return self.__alpha_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute distance. Args: x_arr: `np.ndarray` of vectors. y_arr: `np.ndarray` of vectors. Retruns: `np.ndarray` of distances.", "response": "def compute(self, x_arr, y_arr):\r\n        '''\r\n        Compute distance.\r\n\r\n        Args:\r\n            x_arr:      `np.ndarray` of vectors.\r\n            y_arr:      `np.ndarray` of vectors.\r\n\r\n        Retruns:\r\n            `np.ndarray` of distances.\r\n        '''\r\n        x_arr = x_arr / np.linalg.norm(x_arr, ord=1)\r\n        y_arr = y_arr / np.linalg.norm(y_arr, ord=1)\r\n        mixture_arr = 0.5 * (x_arr + y_arr)\r\n        return 0.5 * (super().compute(x_arr, mixture_arr) + super().compute(y_arr, mixture_arr))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter with top - n ranking.", "response": "def filter(self, scored_list):\n        '''\n        Filtering with top-n ranking.\n\n        Args:\n            scored_list:    The list of scoring.\n\n        Retruns:\n            The list of filtered result.\n\n        '''\n        top_n_key = -1 * self.top_n\n        top_n_list = sorted(scored_list, key=lambda x: x[1])[top_n_key:]\n        result_list = sorted(top_n_list, key=lambda x: x[0])\n        return result_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_n_gram(self, value):\n        ''' setter '''\n        if isinstance(value, Ngram):\n            self.__n_gram = value\n        else:\n            raise TypeError(\"The type of n_gram must be Ngram.\")", "response": "setter for n_gram field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_q_df(self):\n        '''\n        getter\n        '''\n        if isinstance(self.__q_df, pd.DataFrame) is False and self.__q_df is not None:\n            raise TypeError(\"The type of `__q_df` must be `pd.DataFrame`.\")\n        return self.__q_df", "response": "get_q_df is a method that returns a pandas. DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_q_df(self, state_key, action_key):\n        '''\n        Extract Q-Value from `self.q_df`.\n\n        Args:\n            state_key:      The key of state.\n            action_key:     The key of action.\n\n        Returns:\n            Q-Value.\n\n        '''\n        q = 0.0\n        if self.q_df is None:\n            self.save_q_df(state_key, action_key, q)\n            return q\n\n        q_df = self.q_df[self.q_df.state_key == state_key]\n        q_df = q_df[q_df.action_key == action_key]\n\n        if q_df.shape[0]:\n            q = float(q_df[\"q_value\"])\n        else:\n            self.save_q_df(state_key, action_key, q)\n        return q", "response": "Extracts Q - Value from self. q_df."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_q_df(self, state_key, action_key, q_value):\n        '''\n        Insert or update Q-Value in `self.q_df`.\n\n        Args:\n            state_key:      State.\n            action_key:     Action.\n            q_value:        Q-Value.\n\n        Exceptions:\n            TypeError:      If the type of `q_value` is not float.\n\n        '''\n        if isinstance(q_value, float) is False:\n            raise TypeError(\"The type of q_value must be float.\")\n\n        new_q_df = pd.DataFrame([(state_key, action_key, q_value)], columns=[\"state_key\", \"action_key\", \"q_value\"])\n        if self.q_df is not None:\n            self.q_df = pd.concat([new_q_df, self.q_df])\n            self.q_df = self.q_df.drop_duplicates([\"state_key\", \"action_key\"])\n        else:\n            self.q_df = new_q_df", "response": "Inserts or updates the Q - Value in self. q_df."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_r_df(self, value):\n        ''' setter '''\n        if isinstance(value, pd.DataFrame) is False and self.__r_df is not None:\n            raise TypeError(\"The type of `__r_df` must be `pd.DataFrame`.\")\n        self.__r_df = value", "response": "setter for r_df property"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting or updates the R - Value of a specific resource in the state.", "response": "def extract_r_df(self, state_key, r_value, action_key=None):\n        '''\n        Insert or update R-Value in `self.r_df`.\n\n        Args:\n            state_key:     The key of state.\n            r_value:       R-Value(Reward).\n            action_key:    The key of action if it is nesesary for the parametar of value function.\n\n        Exceptions:\n            TypeError:      If the type of `r_value` is not float.\n        '''\n        if isinstance(r_value, float) is False:\n            raise TypeError(\"The type of r_value must be float.\")\n\n        r = 0.0\n        if self.r_df is None:\n            self.save_r_df(state_key, r, action_key)\n            return r\n\n        r_df = self.r_df[self.r_df.state_key == state_key]\n        if action_key is not None:\n            r_df = r_df[r_df.action_key == action_key]\n        if r_df.shape[0]:\n            r = float(r_df[\"r_value\"])\n        else:\n            self.save_r_df(state_key, r, action_key)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_r_df(self, state_key, r_value, action_key=None):\n        '''\n        Insert or update R-Value in `self.r_df`.\n\n        Args:\n            state_key:     The key of state.\n            r_value:       R-Value(Reward).\n            action_key:    The key of action if it is nesesary for the parametar of value function.\n\n        Exceptions:\n            TypeError:      If the type of `r_value` is not float.\n        '''\n        if action_key is not None:\n            add_r_df = pd.DataFrame([(state_key, action_key, r_value)], columns=[\"state_key\", \"action_key\", \"r_value\"])\n        else:\n            add_r_df = pd.DataFrame([(state_key, r_value)], columns=[\"state_key\", \"r_value\"])\n\n        if self.r_df is not None:\n            self.r_df = pd.concat([add_r_df, self.r_df])\n            if action_key is not None:\n                self.r_df = self.r_df.drop_duplicates([\"state_key\", \"action_key\"])\n            else:\n                self.r_df = self.r_df.drop_duplicates([\"state_key\"])\n        else:\n            self.r_df = add_r_df", "response": "Insert or update R - Value in self. r_df."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlearn and searching the optimal solution.", "response": "def learn(self, state_key, limit=1000):\n        '''\n        Learning and searching the optimal solution.\n        \n        Args:\n            state_key:      Initial state.\n            limit:          The maximum number of iterative updates based on value iteration algorithms.\n        '''\n        self.t = 1\n        while self.t <= limit:\n            next_action_list = self.extract_possible_actions(state_key)\n            if len(next_action_list):\n                action_key = self.select_action(\n                    state_key=state_key,\n                    next_action_list=next_action_list\n                )\n                reward_value = self.observe_reward_value(state_key, action_key)\n\n            if len(next_action_list):\n                # Max-Q-Value in next action time.\n                next_state_key = self.update_state(\n                    state_key=state_key,\n                    action_key=action_key\n                )\n\n                next_next_action_list = self.extract_possible_actions(next_state_key)\n                next_action_key = self.predict_next_action(next_state_key, next_next_action_list)\n                next_max_q = self.extract_q_df(next_state_key, next_action_key)\n\n                # Update Q-Value.\n                self.update_q(\n                    state_key=state_key,\n                    action_key=action_key,\n                    reward_value=reward_value,\n                    next_max_q=next_max_q\n                )\n                # Update State.\n                state_key = next_state_key\n\n            # Normalize.\n            self.normalize_q_value()\n            self.normalize_r_value()\n\n            # Vis.\n            self.visualize_learning_result(state_key)\n            # Check.\n            if self.check_the_end_flag(state_key) is True:\n                break\n\n            # Epsode.\n            self.t += 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating Q - Value.", "response": "def update_q(self, state_key, action_key, reward_value, next_max_q):\n        '''\n        Update Q-Value.\n\n        Args:\n            state_key:              The key of state.\n            action_key:             The key of action.\n            reward_value:           R-Value(Reward).\n            next_max_q:             Maximum Q-Value.\n\n        '''\n        # Now Q-Value.\n        q = self.extract_q_df(state_key, action_key)\n        # Update Q-Value.\n        new_q = q + self.alpha_value * (reward_value + (self.gamma_value * next_max_q) - q)\n        # Save updated Q-Value.\n        self.save_q_df(state_key, action_key, new_q)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npredict next action by Q - Learning.", "response": "def predict_next_action(self, state_key, next_action_list):\n        '''\n        Predict next action by Q-Learning.\n\n        Args:\n            state_key:          The key of state in `self.t+1`.\n            next_action_list:   The possible action in `self.t+1`.\n\n        Returns:\n            The key of action.\n\n        '''\n        if self.q_df is not None:\n            next_action_q_df = self.q_df[self.q_df.state_key == state_key]\n            next_action_q_df = next_action_q_df[next_action_q_df.action_key.isin(next_action_list)]\n            if next_action_q_df.shape[0] == 0:\n                return random.choice(next_action_list)\n            else:\n                if next_action_q_df.shape[0] == 1:\n                    max_q_action = next_action_q_df[\"action_key\"].values[0]\n                else:\n                    next_action_q_df = next_action_q_df.sort_values(by=[\"q_value\"], ascending=False)\n                    max_q_action = next_action_q_df.iloc[0, :][\"action_key\"]\n                return max_q_action\n        else:\n            return random.choice(next_action_list)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npull arms. Args: arm_id: Arms master id. success: The number of success. failure: The number of failure.", "response": "def pull(self, arm_id, success, failure):\n        '''\n        Pull arms.\n\n        Args:\n            arm_id:     Arms master id.\n            success:    The number of success.\n            failure:    The number of failure.\n        '''\n        self.__beta_dist_dict[arm_id].observe(success, failure)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recommend(self, limit=10):\n        '''\n        Listup arms and expected value.\n\n        Args:\n            limit:      Length of the list.\n\n        Returns:\n            [Tuple(`Arms master id`, `expected value`)]\n        '''\n        expected_list = [(arm_id, beta_dist.expected_value()) for arm_id, beta_dist in self.__beta_dist_dict.items()]\n        expected_list = sorted(expected_list, key=lambda x: x[1], reverse=True)\n        return expected_list[:limit]", "response": "Return the list of arms and expected value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the similarity with the Tanimoto coefficient.", "response": "def calculate(self, token_list_x, token_list_y):\r\n        '''\r\n        Calculate similarity with the Tanimoto coefficient.\r\n        \r\n        Concrete method.\r\n        \r\n        Args:\r\n            token_list_x:    [token, token, token, ...]\r\n            token_list_y:    [token, token, token, ...]\r\n        \r\n        Returns:\r\n            Similarity.\r\n        '''\r\n        match_list = [tanimoto_value for tanimoto_value in token_list_x if tanimoto_value in token_list_y]\r\n        return float(len(match_list) / (len(token_list_x) + len(token_list_y) - len(match_list)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the distance between two data points.", "response": "def compute(self, x, y):\n        '''\n        Args:\n            x:    Data point.\n            y:    Data point.\n        \n        Returns:\n            Distance.\n        '''\n        return np.sqrt(np.sum((x-y)**2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the time rate of the entry", "response": "def get_time_rate(self):\n        '''\n        getter\n        Time rate.\n        '''\n        if isinstance(self.__time_rate, float) is False:\n            raise TypeError(\"The type of __time_rate must be float.\")\n\n        if self.__time_rate <= 0.0:\n            raise ValueError(\"The value of __time_rate must be greater than 0.0\")\n\n        return self.__time_rate"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_action(self, state_key, next_action_list):\n        '''\n        Select action by Q(state, action).\n        \n        Concreat method for boltzmann distribution.\n\n        Args:\n            state_key:              The key of state.\n            next_action_list:       The possible action in `self.t+1`.\n                                    If the length of this list is 0, all action should be possible.\n\n        Returns:\n            The key of action.\n\n        '''\n        if self.q_df is None or self.q_df.shape[0] == 0:\n            return random.choice(next_action_list)\n\n        next_action_b_df = self.__calculate_boltzmann_factor(state_key, next_action_list)\n\n        if next_action_b_df.shape[0] == 1:\n            return next_action_b_df[\"action_key\"].values[0]\n\n        prob = np.random.random()\n        next_action_b_df = next_action_b_df.sort_values(by=[\"boltzmann_factor\"])\n\n        i = 0\n        while prob > next_action_b_df.iloc[i, :][\"boltzmann_factor\"] + next_action_b_df.iloc[i + 1, :][\"boltzmann_factor\"]:\n            i += 1\n            if i + 1 >= next_action_b_df.shape[0]:\n                break\n\n        max_b_action_key = next_action_b_df.iloc[i, :][\"action_key\"]\n        return max_b_action_key", "response": "Select action by Q."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __calculate_sigmoid(self):\n        '''\n        Function of temperature.\n\n        Returns:\n            Sigmoid.\n\n        '''\n        sigmoid = 1 / np.log(self.t * self.time_rate + 1.1)\n        return sigmoid", "response": "Calculates the sigmoid of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __calculate_boltzmann_factor(self, state_key, next_action_list):\n        '''\n        Calculate boltzmann factor.\n\n        Args:\n            state_key:              The key of state.\n            next_action_list:       The possible action in `self.t+1`.\n                                    If the length of this list is 0, all action should be possible.\n\n        Returns:\n            [(`The key of action`, `boltzmann probability`)]\n        '''\n        sigmoid = self.__calculate_sigmoid()\n        q_df = self.q_df[self.q_df.state_key == state_key]\n        q_df = q_df[q_df.isin(next_action_list)]\n        q_df[\"boltzmann_factor\"] = q_df[\"q_value\"] / sigmoid\n        q_df[\"boltzmann_factor\"] = q_df[\"boltzmann_factor\"].apply(np.exp)\n        q_df[\"boltzmann_factor\"] = q_df[\"boltzmann_factor\"] / q_df[\"boltzmann_factor\"].sum()\n        \n        return q_df", "response": "Calculates the boltzmann factor of the action in the state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_epsilon_greedy_rate(self):\n        ''' getter '''\n        if isinstance(self.__epsilon_greedy_rate, float) is True:\n            return self.__epsilon_greedy_rate\n        else:\n            raise TypeError(\"The type of __epsilon_greedy_rate must be float.\")", "response": "getter Returns the epsilon greedy rate of the current user s user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_epsilon_greedy_rate(self, value):\n        ''' setter '''\n        if isinstance(value, float) is True:\n            self.__epsilon_greedy_rate = value\n        else:\n            raise TypeError(\"The type of __epsilon_greedy_rate must be float.\")", "response": "setter that sets the epsilon greedy rate of the resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect action by Q.", "response": "def select_action(self, state_key, next_action_list):\n        '''\n        Select action by Q(state, action).\n        \n        Concreat method.\n\n        \u03b5-greedy.\n\n        Args:\n            state_key:              The key of state.\n            next_action_list:       The possible action in `self.t+1`.\n                                    If the length of this list is 0, all action should be possible.\n\n        Returns:\n            The key of action.\n\n        '''\n        epsilon_greedy_flag = bool(np.random.binomial(n=1, p=self.epsilon_greedy_rate))\n        \n        if epsilon_greedy_flag is False:\n            action_key = random.choice(next_action_list)\n        else:\n            action_key = self.predict_next_action(state_key, next_action_list)\n        return action_key"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a numpy array of samples drawn from the fake distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = self.noise_sampler.generate()\r\n        arr = self.inference(observed_arr)\r\n        return arr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw samples from the fake distribution.", "response": "def inference(self, observed_arr):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Args:\r\n            observed_arr:     `np.ndarray` of observed data points.\r\n        \r\n        Returns:\r\n            `np.ndarray` of inferenced.\r\n        '''\r\n        _ = self.__lstm_model.inference(observed_arr)\r\n        return self.__lstm_model.get_feature_points()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef learn(self, grad_arr):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:   `np.ndarray` of gradients.\r\n\r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n\r\n        '''\r\n        if grad_arr.ndim > 3:\r\n            grad_arr = grad_arr.reshape((\r\n                grad_arr.shape[0],\r\n                grad_arr.shape[1],\r\n                -1\r\n            ))\r\n            grad_arr = grad_arr[:, -1]\r\n        elif grad_arr.ndim == 3:\r\n            grad_arr = grad_arr[:, -1]\r\n\r\n        delta_arr, _, grads_list = self.__lstm_model.hidden_back_propagate(grad_arr)\r\n        grads_list.insert(0, None)\r\n        grads_list.insert(0, None)\r\n\r\n        self.__lstm_model.optimize(\r\n            grads_list,\r\n            self.__learning_rate,\r\n            1\r\n        )\r\n\r\n        return delta_arr", "response": "Updates this Discriminator by ascending its stochastic gradient."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef learn_q(self, predicted_q_arr, real_q_arr):\n        '''\n        Infernce Q-Value.\n        \n        Args:\n            predicted_q_arr:    `np.ndarray` of predicted Q-Values.\n            real_q_arr:         `np.ndarray` of real Q-Values.\n        '''\n        self.__predicted_q_arr_list.append(predicted_q_arr)\n        while len(self.__predicted_q_arr_list) > self.__seq_len:\n            self.__predicted_q_arr_list = self.__predicted_q_arr_list[1:]\n        while len(self.__predicted_q_arr_list) < self.__seq_len:\n            self.__predicted_q_arr_list.append(self.__predicted_q_arr_list[-1])\n        predicted_q_arr = np.array(self.__predicted_q_arr_list)\n        predicted_q_arr = predicted_q_arr.transpose((1, 0, 2))\n\n        self.__real_q_arr_list.append(real_q_arr)\n        while len(self.__real_q_arr_list) > self.__seq_len:\n            self.__real_q_arr_list = self.__real_q_arr_list[1:]\n        while len(self.__real_q_arr_list) < self.__seq_len:\n            self.__real_q_arr_list.append(self.__real_q_arr_list[-1])\n        real_q_arr = np.array(self.__real_q_arr_list)\n        real_q_arr = real_q_arr.transpose((1, 0, 2))\n\n        loss = self.__computable_loss.compute_loss(predicted_q_arr, real_q_arr)\n        delta_arr = self.__computable_loss.compute_delta(predicted_q_arr, real_q_arr)\n\n        delta_arr, lstm_output_grads_list = self.__lstm_model.output_back_propagate(\n            predicted_q_arr,\n            delta_arr\n        )\n        delta_arr, _, lstm_hidden_grads_list = self.__lstm_model.hidden_back_propagate(\n            delta_arr[:, -1]\n        )\n        lstm_grads_list = lstm_output_grads_list\n        lstm_grads_list.extend(lstm_hidden_grads_list)\n        self.__lstm_model.optimize(lstm_grads_list, self.__learning_rate, 1)\n        self.__loss_list.append(loss)", "response": "This function learns the state of the Infernce Q - Value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inference_q(self, next_action_arr):\n        '''\n        Infernce Q-Value.\n        \n        Args:\n            next_action_arr:     `np.ndarray` of action.\n        \n        Returns:\n            `np.ndarray` of Q-Values.\n        '''\n        q_arr = next_action_arr.reshape((next_action_arr.shape[0], -1))\n        self.__q_arr_list.append(q_arr)\n        while len(self.__q_arr_list) > self.__seq_len:\n            self.__q_arr_list = self.__q_arr_list[1:]\n        while len(self.__q_arr_list) < self.__seq_len:\n            self.__q_arr_list.append(self.__q_arr_list[-1])\n\n        q_arr = np.array(self.__q_arr_list)\n        q_arr = q_arr.transpose((1, 0, 2))\n        q_arr = self.__lstm_model.inference(q_arr)\n        return q_arr[:, -1].reshape((q_arr.shape[0], 1))", "response": "Infernce Q - Value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate noise samples. Returns: np. ndarray of samples.", "response": "def generate(self):\r\n        '''\r\n        Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        generated_arr = np.random.uniform(\r\n            low=0.1,\r\n            high=0.9,\r\n            size=((self.__batch_size, self.__seq_len, self.__dim))\r\n        )\r\n\r\n        if self.noise_sampler is not None:\r\n            self.noise_sampler.output_shape = generated_arr.shape\r\n            generated_arr += self.noise_sampler.generate()\r\n\r\n        return generated_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute(self, x_arr, y_arr):\r\n        '''\r\n        Compute distance.\r\n\r\n        Args:\r\n            x_arr:      `np.ndarray` of vectors.\r\n            y_arr:      `np.ndarray` of vectors.\r\n\r\n        Retruns:\r\n            `np.ndarray` of distances.\r\n        '''\r\n        return np.linalg.norm(x_arr - y_arr, axis=-1)", "response": "Compute distance between two arrays of vectors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vectorize(self, token_list):\r\n        '''\r\n        Tokenize token list.\r\n        \r\n        Args:\r\n            token_list:   The list of tokens..\r\n        \r\n        Returns:\r\n            [vector of token, vector of token, vector of token, ...]\r\n        '''\r\n        sentence_list = [token_list]\r\n        test_observed_arr = self.__setup_dataset(sentence_list, self.__token_master_list)\r\n        pred_arr = self.__controller.inference(test_observed_arr)\r\n        return self.__controller.get_feature_points()", "response": "Tokenize token list.\r\n            Tokenize token list.\r\n            Returns a list of feature points."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new MIDI file from a given file.", "response": "def compose(self, file_path, velocity_mean=None, velocity_std=None):\r\n        '''\r\n        Compose by learned model.\r\n\r\n        Args:\r\n            file_path:      Path to generated MIDI file.\r\n\r\n            velocity_mean:  Mean of velocity.\r\n                            This class samples the velocity from a Gaussian distribution of \r\n                            `velocity_mean` and `velocity_std`.\r\n                            If `None`, the average velocity in MIDI files set to this parameter.\r\n\r\n            velocity_std:   Standard deviation(SD) of velocity.\r\n                            This class samples the velocity from a Gaussian distribution of \r\n                            `velocity_mean` and `velocity_std`.\r\n                            If `None`, the SD of velocity in MIDI files set to this parameter.\r\n        '''\r\n        generated_arr = self.__generative_model.draw()\r\n        channel = generated_arr.shape[1] // 2\r\n        generated_arr = generated_arr[:, :channel]\r\n\r\n        # @TODO(chimera0(RUM)): Fix the redundant processings.\r\n        if velocity_mean is None:\r\n            velocity_mean = np.array(\r\n                [self.__midi_df_list[i].velocity.mean() for i in range(len(self.__midi_df_list))]\r\n            ).mean()\r\n        if velocity_std is None:\r\n            velocity_std = np.array(\r\n                [self.__midi_df_list[i].velocity.std() for i in range(len(self.__midi_df_list))]\r\n            ).std()\r\n\r\n        generated_list = []\r\n        start = 0\r\n        end = self.__time_fraction\r\n        for batch in range(generated_arr.shape[0]):\r\n            for seq in range(generated_arr.shape[2]):\r\n                add_flag = False\r\n                for program_key in range(generated_arr.shape[1]):\r\n                    pitch_key = np.argmax(generated_arr[batch, program_key, seq])\r\n                    pitch_tuple = self.__bar_gram.pitch_tuple_list[pitch_key]\r\n                    for pitch in pitch_tuple:\r\n                        velocity = np.random.normal(\r\n                            loc=velocity_mean, \r\n                            scale=velocity_std\r\n                        )\r\n                        velocity = int(velocity)\r\n                        program = self.__noise_sampler.program_list[program_key]\r\n                        generated_list.append((program, start, end, pitch, velocity))\r\n                        add_flag = True\r\n\r\n                if add_flag is True:\r\n                    start += self.__time_fraction\r\n                    end += self.__time_fraction\r\n\r\n        generated_midi_df = pd.DataFrame(\r\n            generated_list, \r\n            columns=[\r\n                \"program\",\r\n                \"start\", \r\n                \"end\", \r\n                \"pitch\", \r\n                \"velocity\"\r\n            ]\r\n        )\r\n\r\n        pitch_arr = generated_midi_df.pitch.drop_duplicates()\r\n        df_list = []\r\n        for pitch in pitch_arr:\r\n            df = generated_midi_df[generated_midi_df.pitch == pitch]\r\n            df = df.sort_values(by=[\"start\", \"end\"])\r\n            df[\"next_start\"] = df.start.shift(-1)\r\n            df[\"next_end\"] = df.end.shift(-1)\r\n            df.loc[df.end == df.next_start, \"end\"] = df.loc[df.end == df.next_start, \"next_end\"]\r\n            df = df.drop_duplicates([\"end\"])\r\n            df_list.append(df)\r\n\r\n        generated_midi_df = pd.concat(df_list)\r\n        generated_midi_df = generated_midi_df.sort_values(by=[\"start\", \"end\"])\r\n\r\n        self.__midi_controller.save(\r\n            file_path=file_path, \r\n            note_df=generated_midi_df\r\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train_discriminator(\r\n        self,\r\n        k_step,\r\n        true_sampler,\r\n        generative_model,\r\n        discriminative_model,\r\n        d_logs_list\r\n    ):\r\n        '''\r\n        Train the discriminator.\r\n\r\n        Args:\r\n            k_step:                 The number of learning of the discriminative_model.\r\n            true_sampler:           Sampler which draws samples from the `true` distribution.\r\n            generative_model:       Generator which draws samples from the `fake` distribution.\r\n            discriminative_model:   Discriminator which discriminates `true` from `fake`.\r\n            d_logs_list:            `list` of probabilities inferenced by the `discriminator` (mean) in the `discriminator`'s update turn.\r\n\r\n        Returns:\r\n            Tuple data. The shape is...\r\n            - Discriminator which discriminates `true` from `fake`.\r\n            - `list` of probabilities inferenced by the `discriminator` (mean) in the `discriminator`'s update turn.\r\n\r\n        '''\r\n        for k in range(k_step):\r\n            true_arr = true_sampler.draw()\r\n            generated_arr = generative_model.draw()\r\n            true_posterior_arr = discriminative_model.inference(true_arr)\r\n            generated_posterior_arr = discriminative_model.inference(generated_arr)\r\n            grad_arr = self.__gans_value_function.compute_discriminator_reward(\r\n                true_posterior_arr,\r\n                generated_posterior_arr\r\n            )\r\n            discriminative_model.learn(grad_arr)\r\n\r\n            self.__logger.debug(\r\n                \"Probability inferenced by the `discriminator` (mean): \" + str(generated_posterior_arr.mean())\r\n            )\r\n            self.__logger.debug(\r\n                \"And update the `discriminator` by descending its stochastic gradient(means): \" + str(grad_arr.mean())\r\n            )\r\n\r\n            d_logs_list.append(generated_posterior_arr.mean())\r\n\r\n        return discriminative_model, d_logs_list", "response": "Train the discriminator.\r\n\r\n        Args:\r\n            k_step:                 The number of learning of the discriminative_model.\r\n            true_sampler:           Sampler which draws samples from the `true` distribution.\r\n            generative_model:       Generator which draws samples from the `fake` distribution.\r\n            discriminative_model:   Discriminator which discriminates `true` from `fake`.\r\n            d_logs_list:            `list` of probabilities inferenced by the `discriminator` (mean) in the `discriminator`'s update turn.\r\n\r\n        Returns:\r\n            Tuple data. The shape is...\r\n            - Discriminator which discriminates `true` from `fake`.\r\n            - `list` of probabilities inferenced by the `discriminator` (mean) in the `discriminator`'s update turn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain the generator. Args: generative_model: Generator which draws samples from the `fake` distribution. discriminative_model: Discriminator which discriminates `true` from `fake`. g_logs_list: `list` of Probabilities inferenced by the `discriminator` (mean) in the `generator`'s update turn. Returns: Tuple data. The shape is... - Generator which draws samples from the `fake` distribution. - `list` of probabilities inferenced by the `discriminator` (mean) in the `generator`'s update turn.", "response": "def train_generator(\r\n        self,\r\n        generative_model,\r\n        discriminative_model,\r\n        g_logs_list\r\n    ):\r\n        '''\r\n        Train the generator.\r\n\r\n        Args:\r\n            generative_model:       Generator which draws samples from the `fake` distribution.\r\n            discriminative_model:   Discriminator which discriminates `true` from `fake`.\r\n            g_logs_list:            `list` of Probabilities inferenced by the `discriminator` (mean) in the `generator`'s update turn.\r\n\r\n        Returns:\r\n            Tuple data. The shape is...\r\n            - Generator which draws samples from the `fake` distribution.\r\n            - `list` of probabilities inferenced by the `discriminator` (mean) in the `generator`'s update turn.\r\n\r\n        '''\r\n        generated_arr = generative_model.draw()\r\n        generated_posterior_arr = discriminative_model.inference(generated_arr)\r\n        grad_arr = self.__gans_value_function.compute_generator_reward(\r\n            generated_posterior_arr\r\n        )\r\n        grad_arr = discriminative_model.learn(grad_arr, fix_opt_flag=True)\r\n        grad_arr = grad_arr.reshape(generated_arr.shape)\r\n        generative_model.learn(grad_arr)\r\n\r\n        self.__logger.debug(\r\n            \"Probability inferenced by the `discriminator` (mean): \" + str(generated_posterior_arr.mean())\r\n        )\r\n        self.__logger.debug(\r\n            \"And update the `generator` by descending its stochastic gradient(means): \" + str(grad_arr.mean())\r\n        )\r\n        g_logs_list.append(generated_posterior_arr.mean())\r\n\r\n        return generative_model, g_logs_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate(self):\r\n        '''\r\n        Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        generated_arr = np.random.uniform(low=self.__low, high=self.__high, size=self.__output_shape)\r\n        if self.noise_sampler is not None:\r\n            self.noise_sampler.output_shape = generated_arr.shape\r\n            generated_arr += self.noise_sampler.generate()\r\n        return generated_arr", "response": "Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters with mean and std.", "response": "def filter(self, scored_list):\n        '''\n        Filtering with std.\n\n        Args:\n            scored_list:    The list of scoring.\n\n        Retruns:\n            The list of filtered result.\n\n        '''\n        if len(scored_list) > 0:\n            avg = np.mean([s[1] for s in scored_list])\n            std = np.std([s[1] for s in scored_list])\n        else:\n            avg = 0\n            std = 0\n        limiter = avg + 0.5 * std\n        mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_list if score > limiter]\n        return mean_scored"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inference(self, state_arr, limit=1000):\n        '''\n        Infernce.\n        \n        Args:\n            state_arr:    `np.ndarray` of state.\n            limit:        The number of inferencing.\n        \n        Returns:\n            `list of `np.ndarray` of an optimal route.\n        '''\n        self.__inferencing_flag = True\n\n        agent_x, agent_y = np.where(state_arr[0] == 1)\n        agent_x, agent_y = agent_x[0], agent_y[0]\n        self.__create_enemy(self.__map_arr)\n        result_list = [(agent_x, agent_y, 0.0)]\n        result_val_list = [agent_x, agent_y]\n        for e in range(self.__enemy_num):\n            result_val_list.append(self.__enemy_pos_list[e][0])\n            result_val_list.append(self.__enemy_pos_list[e][1])\n        result_val_list.append(0.0)\n        result_list.append(tuple(result_val_list))\n\n        self.t = 0\n        while self.t < limit:\n            next_action_arr = self.extract_possible_actions(state_arr)\n            next_q_arr = self.function_approximator.inference_q(next_action_arr)\n            action_arr, q = self.select_action(next_action_arr, next_q_arr)\n            self.__move_enemy(action_arr)\n\n            agent_x, agent_y = np.where(action_arr[0] == 1)\n            agent_x, agent_y = agent_x[0], agent_y[0]\n            \n            result_val_list = [agent_x, agent_y]\n            for e in range(self.__enemy_num):\n                result_val_list.append(self.__enemy_pos_list[e][0])\n                result_val_list.append(self.__enemy_pos_list[e][1])\n            try:\n                result_val_list.append(q[0])\n            except IndexError:\n                result_val_list.append(q)\n\n            result_list.append(tuple(result_val_list))\n\n            # Update State.\n            state_arr = self.update_state(state_arr, action_arr)\n\n            # Epsode.\n            self.t += 1\n            # Check.\n            end_flag = self.check_the_end_flag(state_arr)\n            if end_flag is True:\n                break\n\n        return result_list", "response": "Inferencing of the state_arr."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_possible_actions(self, state_arr):\n        '''\n        Extract possible actions.\n\n        Args:\n            state_arr:  `np.ndarray` of state.\n        \n        Returns:\n            `np.ndarray` of actions.\n            The shape is:(\n                `batch size corresponded to each action key`, \n                `channel that is 1`, \n                `feature points1`, \n                `feature points2`\n            )\n        '''\n        agent_x, agent_y = np.where(state_arr[-1] == 1)\n        agent_x, agent_y = agent_x[0], agent_y[0]\n\n        possible_action_arr = None\n        for x, y in [\n            (-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)\n        ]:\n            next_x = agent_x + x\n            if next_x < 0 or next_x >= state_arr[-1].shape[1]:\n                continue\n            next_y = agent_y + y\n            if next_y < 0 or next_y >= state_arr[-1].shape[0]:\n                continue\n\n            wall_flag = False\n            if x > 0:\n                for add_x in range(1, x):\n                    if self.__map_arr[agent_x + add_x, next_y] == self.WALL:\n                        wall_flag = True\n            elif x < 0:\n                for add_x in range(x, 0):\n                    if self.__map_arr[agent_x + add_x, next_y] == self.WALL:\n                        wall_flag = True\n                    \n            if wall_flag is True:\n                continue\n\n            if y > 0:\n                for add_y in range(1, y):\n                    if self.__map_arr[next_x, agent_y + add_y] == self.WALL:\n                        wall_flag = True\n            elif y < 0:\n                for add_y in range(y, 0):\n                    if self.__map_arr[next_x, agent_y + add_y] == self.WALL:\n                        wall_flag = True\n\n            if wall_flag is True:\n                continue\n\n            if self.__map_arr[next_x, next_y] == self.WALL:\n                continue\n\n            if (next_x, next_y) in self.__route_memory_list:\n                continue\n\n            next_action_arr = np.zeros((\n                 3 + self.__enemy_num,\n                 state_arr[-1].shape[0],\n                 state_arr[-1].shape[1]\n            ))\n            next_action_arr[0][agent_x, agent_y] = 1\n            next_action_arr[1] = self.__map_arr\n            next_action_arr[-1][next_x, next_y] = 1\n\n            for e in range(self.__enemy_num):\n                enemy_state_arr = np.zeros(state_arr[0].shape)\n                enemy_state_arr[self.__enemy_pos_list[e][0], self.__enemy_pos_list[e][1]] = 1\n                next_action_arr[2 + e] = enemy_state_arr\n\n            next_action_arr = np.expand_dims(next_action_arr, axis=0)\n            if possible_action_arr is None:\n                possible_action_arr = next_action_arr\n            else:\n                possible_action_arr = np.r_[possible_action_arr, next_action_arr]\n\n        if possible_action_arr is not None:\n            while possible_action_arr.shape[0] < self.__batch_size:\n                key = np.random.randint(low=0, high=possible_action_arr.shape[0])\n                possible_action_arr = np.r_[\n                    possible_action_arr,\n                    np.expand_dims(possible_action_arr[key], axis=0)\n                ]\n        else:\n            # Forget oldest memory and do recuresive executing.\n            self.__route_memory_list = self.__route_memory_list[1:]\n            possible_action_arr = self.extract_possible_actions(state_arr)\n\n        return possible_action_arr", "response": "Extracts possible actions from the state array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef observe_reward_value(self, state_arr, action_arr):\n        '''\n        Compute the reward value.\n        \n        Args:\n            state_arr:              `np.ndarray` of state.\n            action_arr:             `np.ndarray` of action.\n        \n        Returns:\n            Reward value.\n        '''\n        if self.__check_goal_flag(action_arr) is True:\n            return 1.0\n        else:\n            self.__move_enemy(action_arr)\n\n            x, y = np.where(action_arr[-1] == 1)\n            x, y = x[0], y[0]\n\n            e_dist_sum = 0.0\n            for e in range(self.__enemy_num):\n                e_dist = np.sqrt(\n                    ((x - self.__enemy_pos_list[e][0]) ** 2) + ((y - self.__enemy_pos_list[e][1]) ** 2)\n                )\n                e_dist_sum += e_dist\n\n            e_dist_penalty = e_dist_sum / self.__enemy_num\n            goal_x, goal_y = self.__goal_pos\n            \n            if x == goal_x and y == goal_y:\n                distance = 0.0\n            else:\n                distance = np.sqrt(((x - goal_x) ** 2) + (y - goal_y) ** 2)\n\n            if (x, y) in self.__route_long_memory_list:\n                repeating_penalty = self.__repeating_penalty\n            else:\n                repeating_penalty = 0.0\n\n            return 1.0 - distance - repeating_penalty + e_dist_penalty", "response": "Compute the reward value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_the_end_flag(self, state_arr):\n        '''\n        Check the end flag.\n        \n        If this return value is `True`, the learning is end.\n\n        As a rule, the learning can not be stopped.\n        This method should be overrided for concreate usecases.\n\n        Args:\n            state_arr:    `np.ndarray` of state in `self.t`.\n\n        Returns:\n            bool\n        '''\n        if self.__check_goal_flag(state_arr) is True or self.__check_crash_flag(state_arr):\n            return True\n        else:\n            return False", "response": "Check the end flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the similarity with the Dice coefficient.", "response": "def calculate(self, token_list_x, token_list_y):\r\n        '''\r\n        Calculate similarity with the Dice coefficient.\r\n        \r\n        Concrete method.\r\n        \r\n        Args:\r\n            token_list_x:    [token, token, token, ...]\r\n            token_list_y:    [token, token, token, ...]\r\n        \r\n        Returns:\r\n            Similarity.\r\n        '''\r\n\r\n        x, y = self.unique(token_list_x, token_list_y)\r\n        try:\r\n            result = 2 * len(x & y) / float(sum(map(len, (x, y))))\r\n        except ZeroDivisionError:\r\n            result = 0.0\r\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef summarize(self, test_arr, vectorizable_token, sentence_list, limit=5):\r\n        '''\r\n        Summarize input document.\r\n\r\n        Args:\r\n            test_arr:               `np.ndarray` of observed data points..\r\n            vectorizable_token:     is-a `VectorizableToken`.\r\n            sentence_list:          `list` of all sentences.\r\n            limit:                  The number of selected abstract sentence.\r\n        \r\n        Returns:\r\n            `np.ndarray` of scores.\r\n        '''\r\n        if isinstance(vectorizable_token, VectorizableToken) is False:\r\n            raise TypeError()\r\n\r\n        _ = self.inference(test_arr)\r\n        score_arr = self.__encoder_decoder_controller.get_reconstruction_error()\r\n        score_arr = score_arr.reshape((\r\n            score_arr.shape[0],\r\n            -1\r\n        )).mean(axis=1)\r\n\r\n        score_list = score_arr.tolist()\r\n\r\n        abstract_list = []\r\n        for i in range(limit):\r\n            if self.__normal_prior_flag is True:\r\n                key = score_arr.argmin()\r\n            else:\r\n                key = score_arr.argmax()\r\n\r\n            score = score_list.pop(key)\r\n            score_arr = np.array(score_list)\r\n\r\n            seq_arr = test_arr[key]\r\n            token_arr = vectorizable_token.tokenize(seq_arr.tolist())\r\n            s = \" \".join(token_arr.tolist())\r\n            _s = \"\".join(token_arr.tolist())\r\n\r\n            for sentence in sentence_list:\r\n                if s in sentence or _s in sentence:\r\n                    abstract_list.append(sentence)\r\n                    abstract_list = list(set(abstract_list))\r\n\r\n            if len(abstract_list) >= limit:\r\n                break\r\n\r\n        return abstract_list", "response": "Summarize the input document."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the inferenced data points from the fake distribution.", "response": "def inference(self, observed_arr):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Args:\r\n            observed_arr:     `np.ndarray` of observed data points.\r\n        \r\n        Returns:\r\n            `np.ndarray` of inferenced.\r\n        '''\r\n        if observed_arr.ndim != 2:\r\n            observed_arr = observed_arr.reshape((observed_arr.shape[0], -1))\r\n        \r\n        pred_arr = self.__nn.inference(observed_arr)\r\n        return pred_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\r\n        if grad_arr.ndim != 2:\r\n            grad_arr = grad_arr.reshape((grad_arr.shape[0], -1))\r\n        delta_arr = self.__nn.back_propagation(grad_arr)\r\n        if fix_opt_flag is False:\r\n            self.__nn.optimize(self.__learning_rate, 1)\r\n        \r\n        return delta_arr", "response": "Updates this Discriminator by ascending its stochastic gradient."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new numpy array containing the samples drawn from the true distribution.", "response": "def draw(self):\r\n        '''\r\n        Draws samples from the `true` distribution.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        if self.__conditional_flag is True:\r\n            return np.concatenate((self.__create_samples(), self.__create_samples()), axis=1)\r\n        else:\r\n            return self.__create_samples()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate(self):\r\n        '''\r\n        Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        generated_arr = np.random.normal(loc=self.__mu, scale=self.__sigma, size=self.__output_shape)\r\n        if self.noise_sampler is not None:\r\n            self.noise_sampler.output_shape = generated_arr.shape\r\n            generated_arr += self.noise_sampler.generate()\r\n        \r\n        return generated_arr", "response": "Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vectorize(self, sentence_list):\n        '''\n        Args:\n            sentence_list:   The list of tokenized sentences.\n                             [[`token`, `token`, `token`, ...],\n                             [`token`, `token`, `token`, ...],\n                             [`token`, `token`, `token`, ...]]\n        \n        Returns:\n            `np.ndarray` of tokens.\n            [vector of token, vector of token, vector of token]\n        '''\n        test_observed_arr, _ = self.__setup_dataset(sentence_list, self.__token_master_list, self.__sentence_mean_len)\n        pred_arr = self.__controller.inference(test_observed_arr)\n        return self.__controller.get_feature_points()", "response": "Vectorizes the list of tokenized sentences."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef adaptive_set(\n        self,\n        reannealing_per=50,\n        thermostat=0.9,\n        t_min=0.001,\n        t_default=1.0\n    ):\n        '''\n        Init for Adaptive Simulated Annealing.\n        \n        Args:\n            reannealing_per:    How often will this model reanneals there per cycles.\n            thermostat:         Thermostat.\n            t_min:              The minimum temperature.\n            t_default:          The default temperature.\n        '''\n        self.__reannealing_per = reannealing_per\n        self.__thermostat = thermostat\n        self.__t_min = t_min\n        self.__t_default = t_default", "response": "This method sets the attributes of the object to be used in the Aleph s internal state machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_t(self, t):\n        '''\n        Change temperature.\n        \n        Override.\n        \n        Args:\n            t:    Now temperature.\n        \n        Returns:\n            Next temperature.\n        '''\n        t = super().change_t(t)\n        self.__now_cycles += 1\n        if self.__now_cycles % self.__reannealing_per == 0:\n            t = t * self.__thermostat\n            \n            if t < self.__t_min:\n                t = self.__t_default\n        return t", "response": "Change the temperature t to viable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef learn(self, observed_arr, target_arr):\r\n        '''\r\n        Training the model.\r\n\r\n        Args:\r\n            observed_arr:       `np.ndarray` of observed data points.\r\n            target_arr:         `np.ndarray` of target labeled data.\r\n        '''\r\n        # Pre-learning.\r\n        if self.__pre_learning_epochs > 0:\r\n            self.__encoder_decoder_controller.learn(observed_arr, observed_arr)\r\n\r\n        learning_rate = self.__learning_rate\r\n        row_o = observed_arr.shape[0]\r\n        row_t = target_arr.shape[0]\r\n        if row_t != 0 and row_t != row_o:\r\n            raise ValueError(\"The row of `target_arr` must be equivalent to the row of `observed_arr`.\")\r\n\r\n        if row_t == 0:\r\n            target_arr = observed_arr.copy()\r\n        else:\r\n            if target_arr.ndim == 2:\r\n                target_arr = target_arr.reshape((target_arr.shape[0], 1, target_arr.shape[1]))\r\n\r\n        if self.__test_size_rate > 0:\r\n            train_index = np.random.choice(observed_arr.shape[0], round((1 - self.__test_size_rate) * observed_arr.shape[0]), replace=False)\r\n            test_index = np.array(list(set(range(observed_arr.shape[0])) - set(train_index)))\r\n            train_observed_arr = observed_arr[train_index]\r\n            test_observed_arr = observed_arr[test_index]\r\n            train_target_arr = target_arr[train_index]\r\n            test_target_arr = target_arr[test_index]\r\n        else:\r\n            train_observed_arr = observed_arr\r\n            train_target_arr = target_arr\r\n\r\n        encoder_best_params_list = []\r\n        decoder_best_params_list = []\r\n        re_encoder_best_params_list = []\r\n        try:\r\n            self.__change_inferencing_mode(False)\r\n            self.__memory_tuple_list = []\r\n            eary_stop_flag = False\r\n            loss_list = []\r\n            min_loss = None\r\n            for epoch in range(self.__epochs):\r\n                if ((epoch + 1) % self.__attenuate_epoch == 0):\r\n                    learning_rate = learning_rate / self.__learning_attenuate_rate\r\n\r\n                rand_index = np.random.choice(train_observed_arr.shape[0], size=self.__batch_size)\r\n                batch_observed_arr = train_observed_arr[rand_index]\r\n                batch_target_arr = train_target_arr[rand_index]\r\n                try:\r\n                    _ = self.inference(batch_observed_arr)\r\n                    delta_arr, _, loss = self.compute_retrospective_loss()\r\n\r\n                    remember_flag = False\r\n                    if len(loss_list) > 0:\r\n                        if abs(loss - (sum(loss_list)/len(loss_list))) > self.__tld:\r\n                            remember_flag = True\r\n\r\n                    if remember_flag is True:\r\n                        self.__remember_best_params(\r\n                            encoder_best_params_list,\r\n                            decoder_best_params_list,\r\n                            re_encoder_best_params_list\r\n                        )\r\n                        # Re-try.\r\n                        _ = self.inference(batch_observed_arr)\r\n                        delta_arr, _, loss = self.compute_retrospective_loss()\r\n\r\n                    re_encoder_grads_list, decoder_grads_list, encoder_delta_arr, encoder_grads_list = self.back_propagation(delta_arr)\r\n                    self.optimize(\r\n                        re_encoder_grads_list,\r\n                        decoder_grads_list, \r\n                        encoder_grads_list, \r\n                        learning_rate, \r\n                        epoch\r\n                    )\r\n\r\n                    if min_loss is None or min_loss > loss:\r\n                        min_loss = loss\r\n                        \r\n                        encoder_best_params_list = [\r\n                            self.__encoder_decoder_controller.encoder.graph.weights_lstm_hidden_arr,\r\n                            self.__encoder_decoder_controller.encoder.graph.weights_lstm_observed_arr,\r\n                            self.__encoder_decoder_controller.encoder.graph.lstm_bias_arr\r\n                        ]\r\n                        decoder_best_params_list = [\r\n                            self.__encoder_decoder_controller.decoder.graph.weights_lstm_hidden_arr,\r\n                            self.__encoder_decoder_controller.decoder.graph.weights_lstm_observed_arr,\r\n                            self.__encoder_decoder_controller.decoder.graph.lstm_bias_arr\r\n                        ]\r\n                        re_encoder_best_params_list = [\r\n                            self.__retrospective_encoder.graph.weights_lstm_hidden_arr,\r\n                            self.__retrospective_encoder.graph.weights_lstm_observed_arr,\r\n                            self.__retrospective_encoder.graph.lstm_bias_arr\r\n                        ]\r\n                        self.__logger.debug(\"Best params are updated.\")\r\n\r\n                    self.__encoder_decoder_controller.encoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.encoder.graph.cec_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.decoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.decoder.graph.cec_activity_arr = np.array([])\r\n                    self.__retrospective_encoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__retrospective_encoder.graph.cec_activity_arr = np.array([])\r\n\r\n                except FloatingPointError:\r\n                    if epoch > int(self.__epochs * 0.7):\r\n                        self.__logger.debug(\r\n                            \"Underflow occurred when the parameters are being updated. Because of early stopping, this error is catched and the parameter is not updated.\"\r\n                        )\r\n                        eary_stop_flag = True\r\n                        break\r\n                    else:\r\n                        self.__logger.debug(\r\n                            \"Underflow occurred when the parameters are being updated.\"\r\n                        )\r\n                        raise\r\n\r\n                if self.__test_size_rate > 0:\r\n                    rand_index = np.random.choice(test_observed_arr.shape[0], size=self.__batch_size)\r\n                    test_batch_observed_arr = test_observed_arr[rand_index]\r\n                    test_batch_target_arr = test_target_arr[rand_index]\r\n\r\n                    self.__change_inferencing_mode(True)\r\n                    _ = self.inference(test_batch_observed_arr)\r\n                    _, _, test_loss = self.compute_retrospective_loss()\r\n\r\n                    remember_flag = False\r\n                    if len(loss_list) > 0:\r\n                        if abs(test_loss - (sum(loss_list)/len(loss_list))) > self.__tld:\r\n                            remember_flag = True\r\n\r\n                    if remember_flag is True:\r\n                        self.__remember_best_params(\r\n                            encoder_best_params_list, \r\n                            decoder_best_params_list,\r\n                            re_encoder_best_params_list\r\n                        )\r\n                        # Re-try.\r\n                        _ = self.inference(test_batch_observed_arr)\r\n                        _, _, test_loss = self.compute_retrospective_loss()\r\n\r\n                    self.__change_inferencing_mode(False)\r\n\r\n                    self.__verificate_retrospective_loss(loss, test_loss)\r\n\r\n                self.__encoder_decoder_controller.encoder.graph.hidden_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.encoder.graph.cec_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.decoder.graph.hidden_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.decoder.graph.cec_activity_arr = np.array([])\r\n\r\n                if epoch > 1 and abs(loss - loss_list[-1]) < self.__tol:\r\n                    eary_stop_flag = True\r\n                    break\r\n                loss_list.append(loss)\r\n\r\n        except KeyboardInterrupt:\r\n            self.__logger.debug(\"Interrupt.\")\r\n\r\n        if eary_stop_flag is True:\r\n            self.__logger.debug(\"Early stopping.\")\r\n            eary_stop_flag = False\r\n        \r\n        self.__remember_best_params(\r\n            encoder_best_params_list, \r\n            decoder_best_params_list,\r\n            re_encoder_best_params_list\r\n        )\r\n        self.__change_inferencing_mode(True)\r\n        self.__logger.debug(\"end. \")", "response": "Training the model.\r\n\r\n        Args:\r\n            observed_arr:       `np.ndarray` of observed data points.\r\n            target_arr:         `np.ndarray` of target labeled data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef learn_generated(self, feature_generator):\r\n        '''\r\n        Learn features generated by `FeatureGenerator`.\r\n        \r\n        Args:\r\n            feature_generator:    is-a `FeatureGenerator`.\r\n\r\n        '''\r\n        if isinstance(feature_generator, FeatureGenerator) is False:\r\n            raise TypeError(\"The type of `feature_generator` must be `FeatureGenerator`.\")\r\n\r\n        # Pre-learning.\r\n        if self.__pre_learning_epochs > 0:\r\n            self.__encoder_decoder_controller.learn_generated(feature_generator)\r\n\r\n        learning_rate = self.__learning_rate\r\n\r\n        encoder_best_params_list = []\r\n        decoder_best_params_list = []\r\n        re_encoder_best_params_list = []\r\n        try:\r\n            self.__change_inferencing_mode(False)\r\n            self.__memory_tuple_list = []\r\n            eary_stop_flag = False\r\n            loss_list = []\r\n            min_loss = None\r\n            epoch = 0\r\n            for batch_observed_arr, batch_target_arr, test_batch_observed_arr, test_batch_target_arr in feature_generator.generate():\r\n                epoch += 1\r\n                if ((epoch + 1) % self.__attenuate_epoch == 0):\r\n                    learning_rate = learning_rate / self.__learning_attenuate_rate\r\n\r\n                try:\r\n                    _ = self.inference(batch_observed_arr)\r\n                    delta_arr, _, loss = self.compute_retrospective_loss()\r\n\r\n                    remember_flag = False\r\n                    if len(loss_list) > 0:\r\n                        if abs(loss - (sum(loss_list)/len(loss_list))) > self.__tld:\r\n                            remember_flag = True\r\n\r\n                    if remember_flag is True:\r\n                        self.__remember_best_params(\r\n                            encoder_best_params_list,\r\n                            decoder_best_params_list,\r\n                            re_encoder_best_params_list\r\n                        )\r\n                        # Re-try.\r\n                        _ = self.inference(batch_observed_arr)\r\n                        delta_arr, _, loss = self.compute_retrospective_loss()\r\n\r\n                    re_encoder_grads_list, decoder_grads_list, encoder_delta_arr, encoder_grads_list = self.back_propagation(delta_arr)\r\n                    self.optimize(\r\n                        re_encoder_grads_list,\r\n                        decoder_grads_list, \r\n                        encoder_grads_list, \r\n                        learning_rate, \r\n                        epoch\r\n                    )\r\n\r\n                    if min_loss is None or min_loss > loss:\r\n                        min_loss = loss\r\n                        \r\n                        encoder_best_params_list = [\r\n                            self.__encoder_decoder_controller.encoder.graph.weights_lstm_hidden_arr,\r\n                            self.__encoder_decoder_controller.encoder.graph.weights_lstm_observed_arr,\r\n                            self.__encoder_decoder_controller.encoder.graph.lstm_bias_arr\r\n                        ]\r\n                        decoder_best_params_list = [\r\n                            self.__encoder_decoder_controller.decoder.graph.weights_lstm_hidden_arr,\r\n                            self.__encoder_decoder_controller.decoder.graph.weights_lstm_observed_arr,\r\n                            self.__encoder_decoder_controller.decoder.graph.lstm_bias_arr\r\n                        ]\r\n                        re_encoder_best_params_list = [\r\n                            self.__retrospective_encoder.graph.weights_lstm_hidden_arr,\r\n                            self.__retrospective_encoder.graph.weights_lstm_observed_arr,\r\n                            self.__retrospective_encoder.graph.lstm_bias_arr\r\n                        ]\r\n                        self.__logger.debug(\"Best params are updated.\")\r\n\r\n                    self.__encoder_decoder_controller.encoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.encoder.graph.cec_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.decoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__encoder_decoder_controller.decoder.graph.cec_activity_arr = np.array([])\r\n                    self.__retrospective_encoder.graph.hidden_activity_arr = np.array([])\r\n                    self.__retrospective_encoder.graph.cec_activity_arr = np.array([])\r\n\r\n                except FloatingPointError:\r\n                    if epoch > int(self.__epochs * 0.7):\r\n                        self.__logger.debug(\r\n                            \"Underflow occurred when the parameters are being updated. Because of early stopping, this error is catched and the parameter is not updated.\"\r\n                        )\r\n                        eary_stop_flag = True\r\n                        break\r\n                    else:\r\n                        self.__logger.debug(\r\n                            \"Underflow occurred when the parameters are being updated.\"\r\n                        )\r\n                        raise\r\n\r\n                if self.__test_size_rate > 0:\r\n                    self.__change_inferencing_mode(True)\r\n                    _ = self.inference(test_batch_observed_arr)\r\n                    _, _, test_loss = self.compute_retrospective_loss()\r\n\r\n                    remember_flag = False\r\n                    if len(loss_list) > 0:\r\n                        if abs(test_loss - (sum(loss_list)/len(loss_list))) > self.__tld:\r\n                            remember_flag = True\r\n\r\n                    if remember_flag is True:\r\n                        self.__remember_best_params(\r\n                            encoder_best_params_list, \r\n                            decoder_best_params_list,\r\n                            re_encoder_best_params_list\r\n                        )\r\n                        # Re-try.\r\n                        _ = self.inference(test_batch_observed_arr)\r\n                        _, _, test_loss = self.compute_retrospective_loss()\r\n\r\n                    self.__change_inferencing_mode(False)\r\n                    self.__verificate_retrospective_loss(loss, test_loss)\r\n\r\n                self.__encoder_decoder_controller.encoder.graph.hidden_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.encoder.graph.cec_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.decoder.graph.hidden_activity_arr = np.array([])\r\n                self.__encoder_decoder_controller.decoder.graph.cec_activity_arr = np.array([])\r\n\r\n                if epoch > 1 and abs(loss - loss_list[-1]) < self.__tol:\r\n                    eary_stop_flag = True\r\n                    break\r\n                loss_list.append(loss)\r\n\r\n        except KeyboardInterrupt:\r\n            self.__logger.debug(\"Interrupt.\")\r\n\r\n        if eary_stop_flag is True:\r\n            self.__logger.debug(\"Early stopping.\")\r\n            eary_stop_flag = False\r\n        \r\n        self.__remember_best_params(\r\n            encoder_best_params_list, \r\n            decoder_best_params_list,\r\n            re_encoder_best_params_list\r\n        )\r\n        self.__change_inferencing_mode(True)\r\n        self.__logger.debug(\"end. \")", "response": "Learn features generated by a feature generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inference(self, observed_arr):\r\n        '''\r\n        Infernece by the model.\r\n\r\n        Args:\r\n            observed_arr:       `np.ndarray` of observed data points.\r\n\r\n        Returns:\r\n            `np.ndarray` of inferenced feature points.\r\n        '''\r\n        decoded_arr = self.__encoder_decoder_controller.inference(observed_arr)\r\n        encoded_arr = self.__encoder_decoder_controller.get_feature_points()\r\n        _ = self.__retrospective_encoder.inference(decoded_arr)\r\n        re_encoded_arr = self.__retrospective_encoder.get_feature_points()\r\n        self.__inferenced_tuple = (observed_arr, encoded_arr, decoded_arr, re_encoded_arr)\r\n        return re_encoded_arr", "response": "Infernece by the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsummarize the input document.", "response": "def summarize(self, test_arr, vectorizable_token, sentence_list, limit=5):\r\n        '''\r\n        Summarize input document.\r\n\r\n        Args:\r\n            test_arr:               `np.ndarray` of observed data points..\r\n            vectorizable_token:     is-a `VectorizableToken`.\r\n            sentence_list:          `list` of all sentences.\r\n            limit:                  The number of selected abstract sentence.\r\n        \r\n        Returns:\r\n            `list` of `str` of abstract sentences.\r\n        '''\r\n        if isinstance(vectorizable_token, VectorizableToken) is False:\r\n            raise TypeError()\r\n\r\n        _ = self.inference(test_arr)\r\n        _, loss_arr, _ = self.compute_retrospective_loss()\r\n\r\n        loss_list = loss_arr.tolist()\r\n\r\n        abstract_list = []\r\n        for i in range(limit):\r\n            key = loss_arr.argmin()\r\n            _ = loss_list.pop(key)\r\n            loss_arr = np.array(loss_list)\r\n\r\n            seq_arr = test_arr[key]\r\n            token_arr = vectorizable_token.tokenize(seq_arr.tolist())\r\n            s = \" \".join(token_arr.tolist())\r\n            _s = \"\".join(token_arr.tolist())\r\n\r\n            for sentence in sentence_list:\r\n                if s in sentence or _s in sentence:\r\n                    abstract_list.append(sentence)\r\n                    abstract_list = list(set(abstract_list))\r\n\r\n            if len(abstract_list) >= limit:\r\n                break\r\n\r\n        return abstract_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbacks propagation. Args: delta_output_arr: Delta. Returns: Tuple data. - decoder's `list` of gradations, - encoder's `np.ndarray` of Delta, - encoder's `list` of gradations.", "response": "def back_propagation(self, delta_arr):\r\n        '''\r\n        Back propagation.\r\n\r\n        Args:\r\n            delta_output_arr:    Delta.\r\n        \r\n        Returns:\r\n            Tuple data.\r\n            - decoder's `list` of gradations,\r\n            - encoder's `np.ndarray` of Delta, \r\n            - encoder's `list` of gradations.\r\n        '''\r\n        re_encoder_delta_arr, delta_hidden_arr, re_encoder_grads_list = self.__retrospective_encoder.hidden_back_propagate(\r\n            delta_arr[:, -1]\r\n        )\r\n        re_encoder_grads_list.insert(0, None)\r\n        re_encoder_grads_list.insert(0, None)\r\n\r\n        observed_arr, encoded_arr, decoded_arr, re_encoded_arr = self.__inferenced_tuple\r\n        delta_arr = self.__encoder_decoder_controller.computable_loss.compute_delta(\r\n            decoded_arr, \r\n            observed_arr\r\n        )\r\n        delta_arr[:, -1] += re_encoder_delta_arr[:, -1]\r\n\r\n        decoder_grads_list, encoder_delta_arr, encoder_grads_list = self.__encoder_decoder_controller.back_propagation(\r\n            delta_arr\r\n        )\r\n        return re_encoder_grads_list, decoder_grads_list, encoder_delta_arr, encoder_grads_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_retrospective_loss(self):\r\n        '''\r\n        Compute retrospective loss.\r\n\r\n        Returns:\r\n            The tuple data.\r\n            - `np.ndarray` of delta.\r\n            - `np.ndarray` of losses of each batch.\r\n            - float of loss of all batch.\r\n\r\n        '''\r\n        observed_arr, encoded_arr, decoded_arr, re_encoded_arr = self.__inferenced_tuple\r\n        batch_size = observed_arr.shape[0]\r\n        if self.__input_neuron_count == self.__hidden_neuron_count:\r\n            target_arr = encoded_arr - np.expand_dims(observed_arr.mean(axis=2), axis=2)\r\n            summary_delta_arr = np.sqrt(np.power(decoded_arr - target_arr, 2))\r\n        else:\r\n            # For each batch, draw a samples from the Uniform distribution.\r\n            if self.__input_neuron_count > self.__hidden_neuron_count:\r\n                all_dim_arr = np.arange(self.__input_neuron_count)\r\n                np.random.shuffle(all_dim_arr)\r\n                choiced_dim_arr = all_dim_arr[:self.__hidden_neuron_count]\r\n                target_arr = encoded_arr - np.expand_dims(observed_arr[:, :, choiced_dim_arr].mean(axis=2), axis=2)\r\n                summary_delta_arr = np.sqrt(np.power(decoded_arr[:, :, choiced_dim_arr] - target_arr, 2))\r\n            else:\r\n                all_dim_arr = np.arange(self.__hidden_neuron_count)\r\n                np.random.shuffle(all_dim_arr)\r\n                choiced_dim_arr = all_dim_arr[:self.__input_neuron_count]\r\n                target_arr = encoded_arr[:, :, choiced_dim_arr] - np.expand_dims(observed_arr.mean(axis=2), axis=2)\r\n                summary_delta_arr = np.sqrt(np.power(decoded_arr - target_arr, 2))\r\n\r\n        summary_delta_arr = np.nan_to_num(summary_delta_arr)\r\n        summary_delta_arr = (summary_delta_arr - summary_delta_arr.mean()) / (summary_delta_arr.std() + 1e-08)\r\n\r\n        match_delta_arr = np.sqrt(np.power(encoded_arr[:, -1] - re_encoded_arr[:, -1], 2))\r\n        match_delta_arr = np.nan_to_num(match_delta_arr)\r\n        match_delta_arr = (match_delta_arr - match_delta_arr.mean()) / (match_delta_arr.std() + 1e-08)\r\n\r\n        other_encoded_delta_arr = np.nansum(\r\n            np.sqrt(\r\n                np.power(\r\n                    np.maximum(\r\n                        0,\r\n                        encoded_arr[:, :-1] - re_encoded_arr[:, -1].reshape(\r\n                            re_encoded_arr[:, -1].shape[0], \r\n                            1, \r\n                            re_encoded_arr[:, -1].shape[1]\r\n                        )\r\n                    ),\r\n                    2\r\n                )\r\n            ) + self.__margin_param,\r\n            axis=1\r\n        )\r\n        other_encoded_delta_arr = np.nan_to_num(other_encoded_delta_arr)\r\n        other_encoded_delta_arr = (other_encoded_delta_arr - other_encoded_delta_arr.mean()) / (other_encoded_delta_arr.std() + 1e-08)\r\n\r\n        other_re_encoded_delta_arr = np.nansum(\r\n            np.sqrt(\r\n                np.power(\r\n                    np.maximum(\r\n                        0, \r\n                        encoded_arr[:, -1].reshape(\r\n                            encoded_arr[:, -1].shape[0],\r\n                            1,\r\n                            encoded_arr[:, -1].shape[1]\r\n                        ) - re_encoded_arr[:, :-1], \r\n                    ),\r\n                    2\r\n                )\r\n            ) + self.__margin_param,\r\n            axis=1\r\n        )\r\n        other_encoded_delta_arr = np.nan_to_num(other_encoded_delta_arr)\r\n        other_re_encoded_delta_arr = (other_re_encoded_delta_arr - other_re_encoded_delta_arr.mean()) / (other_re_encoded_delta_arr.std() + 1e-08)\r\n\r\n        mismatch_delta_arr = (match_delta_arr - other_encoded_delta_arr) + (match_delta_arr - other_re_encoded_delta_arr)\r\n\r\n        delta_arr = summary_delta_arr + np.expand_dims(self.__retrospective_lambda * match_delta_arr, axis=1) + np.expand_dims(self.__retrospective_eta * mismatch_delta_arr, axis=1)\r\n\r\n        v = np.linalg.norm(delta_arr)\r\n        if v > self.__grad_clip_threshold:\r\n            delta_arr = delta_arr * self.__grad_clip_threshold / v\r\n\r\n        loss = np.square(delta_arr).mean()\r\n        loss_arr = np.square(delta_arr).sum(axis=1).mean(axis=1)\r\n        return delta_arr, loss_arr, loss", "response": "Returns the retrospective loss of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __change_inferencing_mode(self, inferencing_mode):\r\n        '''\r\n        Change dropout rate in Encoder/Decoder.\r\n        \r\n        Args:\r\n            dropout_rate:   The probalibity of dropout.\r\n        '''\r\n        self.__encoder_decoder_controller.decoder.opt_params.inferencing_mode = inferencing_mode\r\n        self.__encoder_decoder_controller.encoder.opt_params.inferencing_mode = inferencing_mode\r\n        self.__retrospective_encoder.opt_params.inferencing_mode = inferencing_mode", "response": "Change the inferencing mode of the encoder and decoder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremembering best parameters. Args: encoder_best_params_list: `list` of encoder's parameters. decoder_best_params_list: `list` of decoder's parameters. re_encoder_best_params_list: `list` of re-decoder's parameters.", "response": "def __remember_best_params(self, encoder_best_params_list, decoder_best_params_list, re_encoder_best_params_list):\r\n        '''\r\n        Remember best parameters.\r\n        \r\n        Args:\r\n            encoder_best_params_list:    `list` of encoder's parameters.\r\n            decoder_best_params_list:    `list` of decoder's parameters.\r\n            re_encoder_best_params_list: `list` of re-decoder's parameters.\r\n\r\n        '''\r\n        if len(encoder_best_params_list) > 0 and len(decoder_best_params_list) > 0:\r\n            self.__encoder_decoder_controller.encoder.graph.weights_lstm_hidden_arr = encoder_best_params_list[0]\r\n            self.__encoder_decoder_controller.encoder.graph.weights_lstm_observed_arr = encoder_best_params_list[1]\r\n            self.__encoder_decoder_controller.encoder.graph.lstm_bias_arr = encoder_best_params_list[2]\r\n\r\n            self.__encoder_decoder_controller.decoder.graph.weights_lstm_hidden_arr = decoder_best_params_list[0]\r\n            self.__encoder_decoder_controller.decoder.graph.weights_lstm_observed_arr = decoder_best_params_list[1]\r\n            self.__encoder_decoder_controller.decoder.graph.lstm_bias_arr = decoder_best_params_list[2]\r\n\r\n            self.__retrospective_encoder.graph.weights_lstm_hidden_arr = re_encoder_best_params_list[0]\r\n            self.__retrospective_encoder.graph.weights_lstm_observed_arr = re_encoder_best_params_list[1]\r\n            self.__retrospective_encoder.graph.lstm_bias_arr = re_encoder_best_params_list[2]\r\n\r\n            self.__logger.debug(\"Best params are saved.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate(self):\r\n        '''\r\n        Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\r\n        observed_arr = None\r\n        for row in range(self.__batch_size):\r\n            arr = None\r\n            for d in range(self.__dim):\r\n                _arr = self.__generate_sin(\r\n                    amp=self.__amp,\r\n                    sampling_freq=self.__sampling_freq,\r\n                    freq=self.__freq,\r\n                    sec=self.__sec,\r\n                    seq_len=self.__seq_len\r\n                )\r\n                _arr = np.expand_dims(_arr, axis=0)\r\n                if arr is None:\r\n                    arr = _arr\r\n                else:\r\n                    arr = np.r_[arr, _arr]\r\n\r\n            arr = np.expand_dims(arr, axis=0)\r\n\r\n            if observed_arr is None:\r\n                observed_arr = arr\r\n            else:\r\n                observed_arr = np.r_[observed_arr, arr]\r\n\r\n        observed_arr = observed_arr.transpose((0, 2, 1))\r\n        gauss_noise = np.random.normal(loc=self.__mu, scale=self.__sigma, size=observed_arr.shape)\r\n        observed_arr = observed_arr + gauss_noise\r\n        if self.noise_sampler is not None:\r\n            self.noise_sampler.output_shape = observed_arr.shape\r\n            observed_arr += self.noise_sampler.generate()\r\n\r\n        if self.__norm_mode == \"z_score\":\r\n            if observed_arr.std() != 0:\r\n                observed_arr = (observed_arr - observed_arr.mean()) / observed_arr.std()\r\n        elif self.__norm_mode == \"min_max\":\r\n            if (observed_arr.max() - observed_arr.min()) != 0:\r\n                observed_arr = (observed_arr - observed_arr.min()) / (observed_arr.max() - observed_arr.min())\r\n        elif self.__norm_mode == \"tanh\":\r\n            observed_arr = np.tanh(observed_arr)\r\n\r\n        return observed_arr", "response": "Generate noise samples.\r\n        \r\n        Returns:\r\n            `np.ndarray` of samples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_var_arr(self):\n        ''' getter '''\n        if isinstance(self.__var_arr, np.ndarray):\n            return self.__var_arr\n        else:\n            raise TypeError()", "response": "getter Returns array of all the data variables in the object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_var_log_arr(self):\n        ''' getter '''\n        if isinstance(self.__var_log_arr, np.ndarray):\n            return self.__var_log_arr\n        else:\n            raise TypeError()", "response": "getter Returns the array of the variance log of the user s income"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a chunk of data to a stream.", "response": "def write_stream(self, stream, left_chunk, right_chunk, volume):\r\n        '''\r\n        \u5177\u8c61\u30e1\u30bd\u30c3\u30c9\r\n        \u30e2\u30ce\u30e9\u30eb\u30d3\u30fc\u30c8\u3092\u751f\u6210\u3059\u308b\r\n\r\n        Args:\r\n            stream:         PyAudio\u306e\u30b9\u30c8\u30ea\u30fc\u30e0\r\n            left_chunk:     \u5de6\u97f3\u6e90\u306b\u5bfe\u5fdc\u3059\u308b\u30c1\u30e3\u30f3\u30af\r\n            right_chunk:    \u53f3\u97f3\u6e90\u306b\u5bfe\u5fdc\u3059\u308b\u30c1\u30e3\u30f3\u30af\r\n            volume:         \u97f3\u91cf\r\n\r\n        Returns:\r\n            void\r\n        '''\r\n        if len(left_chunk) != len(right_chunk):\r\n            raise ValueError()\r\n\r\n        for i in range(len(left_chunk)):\r\n            chunk = (left_chunk[i] + right_chunk[i]) * volume\r\n            data = struct.pack(\"2f\", chunk, chunk)\r\n            stream.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_stream(self, left_chunk, right_chunk, volume, bit16=32767.0):\r\n        '''\r\n        \u5177\u8c61\u30e1\u30bd\u30c3\u30c9\r\n        wav\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u30e2\u30ce\u30e9\u30eb\u30d3\u30fc\u30c8\u3092\u8aad\u307f\u8fbc\u3080\r\n\r\n        Args:\r\n            left_chunk:     \u5de6\u97f3\u6e90\u306b\u5bfe\u5fdc\u3059\u308b\u30c1\u30e3\u30f3\u30af\r\n            right_chunk:    \u53f3\u97f3\u6e90\u306b\u5bfe\u5fdc\u3059\u308b\u30c1\u30e3\u30f3\u30af\r\n            volume:         \u97f3\u91cf\r\n            bit16:          \u6574\u6570\u5316\u306e\u6761\u4ef6\r\n\r\n        Returns:\r\n            \u30d5\u30ec\u30fc\u30e0\u306elist\r\n        '''\r\n        if len(left_chunk) != len(right_chunk):\r\n            raise ValueError()\r\n\r\n        frame_list = []\r\n        for i in range(len(left_chunk)):\r\n            chunk = int((left_chunk[i] + right_chunk[i]) * bit16 * volume)\r\n            data = struct.pack(\"2h\", chunk, chunk)\r\n            frame_list.append(data)\r\n\r\n        return frame_list", "response": "Reads a list of bytes from the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraining the generative model as the Auto - Encoder.", "response": "def train_auto_encoder(self, generative_model, a_logs_list):\r\n        '''\r\n        Train the generative model as the Auto-Encoder.\r\n\r\n        Args:\r\n            generative_model:   Generator which draws samples from the `fake` distribution.\r\n            a_logs_list:        `list` of the reconstruction errors.\r\n        \r\n        Returns:\r\n            The tuple data. The shape is...\r\n            - Generator which draws samples from the `fake` distribution.\r\n            - `list` of the reconstruction errors.\r\n        '''\r\n        error_arr = generative_model.update()\r\n        if error_arr.ndim > 1:\r\n            error_arr = error_arr.mean()\r\n        a_logs_list.append(error_arr)\r\n\r\n        self.__logger.debug(\"The reconstruction error (mean): \" + str(error_arr))\r\n\r\n        return generative_model, a_logs_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_discriminator_reward(\r\n        self,\r\n        true_posterior_arr,\r\n        generated_posterior_arr\r\n    ):\r\n        '''\r\n        Compute discriminator's reward.\r\n\r\n        Args:\r\n            true_posterior_arr:         `np.ndarray` of `true` posterior inferenced by the discriminator.\r\n            generated_posterior_arr:    `np.ndarray` of `fake` posterior inferenced by the discriminator.\r\n        \r\n        Returns:\r\n            `np.ndarray` of Gradients.\r\n        '''\r\n        grad_arr = np.log(true_posterior_arr + 1e-08) + np.log(1 - generated_posterior_arr + 1e-08)\r\n        return grad_arr", "response": "Compute the discriminator s reward."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects action by Q - Value.", "response": "def select_action(self, next_action_arr, next_q_arr):\n        '''\n        Select action by Q(state, action).\n\n        Args:\n            next_action_arr:        `np.ndarray` of actions.\n            next_q_arr:             `np.ndarray` of Q-Values.\n\n        Retruns:\n            Tuple(`np.ndarray` of action., Q-Value)\n        '''\n        key_arr = self.select_action_key(next_action_arr, next_q_arr)\n        return next_action_arr[key_arr], next_q_arr[key_arr]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_action_key(self, next_action_arr, next_q_arr):\n        '''\n        Select action by Q(state, action).\n\n        Args:\n            next_action_arr:        `np.ndarray` of actions.\n            next_q_arr:             `np.ndarray` of Q-Values.\n\n        Retruns:\n            `np.ndarray` of keys.\n        '''\n        epsilon_greedy_flag = bool(np.random.binomial(n=1, p=self.epsilon_greedy_rate))\n        if epsilon_greedy_flag is False:\n            key = np.random.randint(low=0, high=next_action_arr.shape[0])\n        else:\n            key = next_q_arr.argmax()\n\n        return key", "response": "Select action by Q(state, action).\n\n        Args:\n            next_action_arr:        `np.ndarray` of actions.\n            next_q_arr:             `np.ndarray` of Q-Values.\n\n        Retruns:\n            `np.ndarray` of keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, query, fields=None, page=1, max_records=None, flatten=True):\n        if fields is None:\n            fields = []\n        page = int(page)\n        pages = float('inf')\n        data = {\n            \"query\": query,\n            \"page\": page,\n            \"fields\": fields,\n            \"flatten\": flatten\n        }\n\n        count = 0\n        while page <= pages:\n            payload = self._post(self.search_path, data=data)\n            pages = payload['metadata']['pages']\n            page += 1\n            data[\"page\"] = page\n\n            for result in payload[\"results\"]:\n                yield result\n                count += 1\n                if max_records and count >= max_records:\n                    return", "response": "returns iterator over all records that match the given query"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadjusts the colors of the object based on the mode.", "response": "def adjustColors(self, mode='dark'):\n        \"\"\"\n        Change a few colors depending on the mode to use. The default mode\n        doesn't assume anything and avoid using white & black colors. The dark\n        mode use white and avoid dark blue while the light mode use black and\n        avoid yellow, to give a few examples.\n        \"\"\"\n        rp = Game.__color_modes.get(mode, {})\n        for k, color in self.__colors.items():\n            self.__colors[k] = rp.get(color, color)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loadBestScore(self):\n        try:\n            with open(self.scores_file, 'r') as f:\n                self.best_score = int(f.readline(), 10)\n        except:\n            return False\n        return True", "response": "load local best score from the default file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef saveBestScore(self):\n        if self.score > self.best_score:\n            self.best_score = self.score\n        try:\n            with open(self.scores_file, 'w') as f:\n                f.write(str(self.best_score))\n        except:\n            return False\n        return True", "response": "save current best score in the default file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef incScore(self, pts):\n        self.score += pts\n        if self.score > self.best_score:\n            self.best_score = self.score", "response": "update the score by adding the specified number of points to the current score"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the current game session s score and data for further use", "response": "def store(self):\n        \"\"\"\n        save the current game session's score and data for further use\n        \"\"\"\n        size = self.board.SIZE\n        cells = []\n\n        for i in range(size):\n            for j in range(size):\n                cells.append(str(self.board.getCell(j, i)))\n\n        score_str = \"%s\\n%d\" % (' '.join(cells), self.score)\n\n        try:\n            with open(self.store_file, 'w') as f:\n                f.write(score_str)\n        except:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestore the saved game score and data", "response": "def restore(self):\n        \"\"\"\n        restore the saved game score and data\n        \"\"\"\n\n        size = self.board.SIZE\n\n        try:\n            with open(self.store_file, 'r') as f:\n                lines = f.readlines()\n                score_str = lines[0]\n                self.score = int(lines[1])\n        except:\n            return False\n\n        score_str_list = score_str.split(' ')\n        count = 0\n\n        for i in range(size):\n            for j in range(size):\n                value = score_str_list[count]\n                self.board.setCell(j, i, int(value))\n                count += 1\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getCellStr(self, x, y):  # TODO: refactor regarding issue #11\n        c = self.board.getCell(x, y)\n\n        if c == 0:\n            return '.' if self.__azmode else '  .'\n\n        elif self.__azmode:\n            az = {}\n            for i in range(1, int(math.log(self.board.goal(), 2))):\n                az[2 ** i] = chr(i + 96)\n\n            if c not in az:\n                return '?'\n            s = az[c]\n        elif c == 1024:\n            s = ' 1k'\n        elif c == 2048:\n            s = ' 2k'\n        else:\n            s = '%3d' % c\n\n        return self.__colors.get(c, Fore.RESET) + s + Style.RESET_ALL", "response": "return a string representation of the cell located at x y."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of the current board.", "response": "def boardToString(self, margins=None):\n        \"\"\"\n        return a string representation of the current board.\n        \"\"\"\n        if margins is None:\n            margins = {}\n\n        b = self.board\n        rg = range(b.size())\n        left = ' '*margins.get('left', 0)\n        s = '\\n'.join(\n            [left + ' '.join([self.getCellStr(x, y) for x in rg]) for y in rg])\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntest if a move is possible", "response": "def canMove(self):\n        \"\"\"\n        test if a move is possible\n        \"\"\"\n        if not self.filled():\n            return True\n\n        for y in self.__size_range:\n            for x in self.__size_range:\n                c = self.getCell(x, y)\n                if (x < self.__size-1 and c == self.getCell(x+1, y)) \\\n                   or (y < self.__size-1 and c == self.getCell(x, y+1)):\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addTile(self, value=None, choices=None):\n        if choices is None:\n            choices = [2] * 9 + [4]\n\n        if value:\n            choices = [value]\n\n        v = random.choice(choices)\n        empty = self.getEmptyCells()\n        if empty:\n            x, y = random.choice(empty)\n            self.setCell(x, y, v)", "response": "add a random tile in an empty cell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value at x y", "response": "def setCell(self, x, y, v):\n        \"\"\"set the cell value at x,y\"\"\"\n        self.cells[y][x] = v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getCol(self, x):\n        return [self.getCell(x, i) for i in self.__size_range]", "response": "return the x - th column starting at 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setCol(self, x, l):\n        for i in xrange(0, self.__size):\n            self.setCell(x, i, l[i])", "response": "set the x - th column starting at 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getEmptyCells(self):\n        return [(x, y)\n                for x in self.__size_range\n                for y in self.__size_range if self.getCell(x, y) == 0]", "response": "return a list of tuples for each empty cell"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __collapseLineOrCol(self, line, d):\n        if (d == Board.LEFT or d == Board.UP):\n            inc = 1\n            rg = xrange(0, self.__size-1, inc)\n        else:\n            inc = -1\n            rg = xrange(self.__size-1, 0, inc)\n\n        pts = 0\n        for i in rg:\n            if line[i] == 0:\n                continue\n            if line[i] == line[i+inc]:\n                v = line[i]*2\n                if v == self.__goal:\n                    self.__won = True\n\n                line[i] = v\n                line[i+inc] = 0\n                pts += v\n\n        return (line, pts)", "response": "Merge tiles in a line or column according to a direction and return a tuple with the new line and the score for the move on this line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __moveLineOrCol(self, line, d):\n        nl = [c for c in line if c != 0]\n        if d == Board.UP or d == Board.LEFT:\n            return nl + [0] * (self.__size - len(nl))\n        return [0] * (self.__size - len(nl)) + nl", "response": "Move a line or column to a given direction"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove and return the move score", "response": "def move(self, d, add_tile=True):\n        \"\"\"\n        move and return the move score\n        \"\"\"\n        if d == Board.LEFT or d == Board.RIGHT:\n            chg, get = self.setLine, self.getLine\n        elif d == Board.UP or d == Board.DOWN:\n            chg, get = self.setCol, self.getCol\n        else:\n            return 0\n\n        moved = False\n        score = 0\n\n        for i in self.__size_range:\n            # save the original line/col\n            origin = get(i)\n            # move it\n            line = self.__moveLineOrCol(origin, d)\n            # merge adjacent tiles\n            collapsed, pts = self.__collapseLineOrCol(line, d)\n            # move it again (for when tiles are merged, because empty cells are\n            # inserted in the middle of the line/col)\n            new = self.__moveLineOrCol(collapsed, d)\n            # set it back in the board\n            chg(i, new)\n            # did it change?\n            if origin != new:\n                moved = True\n            score += pts\n\n        # don't add a new tile if nothing changed\n        if moved and add_tile:\n            self.addTile()\n\n        return score"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses args from the CLI and return a dict", "response": "def parse_cli_args():\n    \"\"\"parse args from the CLI and return a dict\"\"\"\n    parser = argparse.ArgumentParser(description='2048 in your terminal')\n    parser.add_argument('--mode', dest='mode', type=str,\n                        default=None, help='colors mode (dark or light)')\n    parser.add_argument('--az', dest='azmode', action='store_true',\n                        help='Use the letters a-z instead of numbers')\n    parser.add_argument('--resume', dest='resume', action='store_true',\n                        help='restart the game from where you left')\n    parser.add_argument('-v', '--version', action='store_true')\n    parser.add_argument('-r', '--rules', action='store_true')\n    return vars(parser.parse_args())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts a new game.", "response": "def start_game(debug=False):\n    \"\"\"\n    Start a new game. If ``debug`` is set to ``True``, the game object is\n    returned and the game loop isn't fired.\n    \"\"\"\n    args = parse_cli_args()\n\n    if args['version']:\n        print_version_and_exit()\n\n    if args['rules']:\n        print_rules_and_exit()\n\n    game = Game(**args)\n    if args['resume']:\n        game.restore()\n\n    if debug:\n        return game\n\n    return game.loop()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_message(self, message):\n        message = ObjectDict(escape.json_decode(message))\n        if message.command == 'hello':\n            handshake = {\n                'command': 'hello',\n                'protocols': [\n                    'http://livereload.com/protocols/official-7',\n                ],\n                'serverName': 'livereload-tornado',\n            }\n            self.send_message(handshake)\n\n        if message.command == 'info' and 'url' in message:\n            logger.info('Browser Connected: %s' % message.url)\n            LiveReloadHandler.waiters.add(self)", "response": "Handle messages from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_content_modified_time(cls, abspath):\n        stat_result = os.stat(abspath)\n        modified = datetime.datetime.utcfromtimestamp(\n            stat_result[stat.ST_MTIME])\n        return modified", "response": "Returns the time that the given file was last modified."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_content_version(cls, abspath):\n        data = cls.get_content(abspath)\n        hasher = hashlib.md5()\n\n        mtime_data = format(cls.get_content_modified_time(abspath), \"%Y-%m-%d %H:%M:%S\")\n\n        hasher.update(mtime_data.encode())\n\n        if isinstance(data, bytes):\n            hasher.update(data)\n        else:\n            for chunk in data:\n                hasher.update(chunk)\n        return hasher.hexdigest()", "response": "Returns a version string for the resource at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nignores a given filename or not.", "response": "def ignore(self, filename):\n        \"\"\"Ignore a given filename or not.\"\"\"\n        _, ext = os.path.splitext(filename)\n        return ext in ['.pyc', '.pyo', '.o', '.swp']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch(self, path, func=None, delay=0, ignore=None):\n        self._tasks[path] = {\n            'func': func,\n            'delay': delay,\n            'ignore': ignore,\n        }", "response": "Add a task to the watcher."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef examine(self):\n        if self._changes:\n            return self._changes.pop()\n\n        # clean filepath\n        self.filepath = None\n        delays = set()\n        for path in self._tasks:\n            item = self._tasks[path]\n            if self.is_changed(path, item['ignore']):\n                func = item['func']\n                delay = item['delay']\n                if delay and isinstance(delay, float):\n                    delays.add(delay)\n                if func:\n                    name = getattr(func, 'name', None)\n                    if not name:\n                        name = getattr(func, '__name__', 'anonymous')\n                    logger.info(\n                        \"Running task: {} (delay: {})\".format(name, delay))\n                    func()\n\n        if delays:\n            delay = max(delays)\n        else:\n            delay = None\n        return self.filepath, delay", "response": "Check if there are changes run the given task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shell(cmd, output=None, mode='w', cwd=None, shell=False):\n    if not output:\n        output = os.devnull\n    else:\n        folder = os.path.dirname(output)\n        if folder and not os.path.isdir(folder):\n            os.makedirs(folder)\n\n    if not isinstance(cmd, (list, tuple)) and not shell:\n        cmd = shlex.split(cmd)\n\n    def run_shell():\n        try:\n            p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd,\n                      shell=shell)\n        except OSError as e:\n            logger.error(e)\n            if e.errno == os.errno.ENOENT:  # file (command) not found\n                logger.error(\"maybe you haven't installed %s\", cmd[0])\n            return e\n        stdout, stderr = p.communicate()\n        if stderr:\n            logger.error(stderr)\n            return stderr\n        #: stdout is bytes, decode for python3\n        if PY3:\n            stdout = stdout.decode()\n        with open(output, mode) as f:\n            f.write(stdout)\n\n    return run_shell", "response": "Execute a shell command."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the given filepath for watcher list.", "response": "def watch(self, filepath, func=None, delay=None, ignore=None):\n        \"\"\"Add the given filepath for watcher list.\n\n        Once you have intialized a server, watch file changes before\n        serve the server::\n\n            server.watch('static/*.stylus', 'make static')\n            def alert():\n                print('foo')\n            server.watch('foo.txt', alert)\n            server.serve()\n\n        :param filepath: files to be watched, it can be a filepath,\n                         a directory, or a glob pattern\n        :param func: the function to be called, it can be a string of\n                     shell command, or any callable object without\n                     parameters\n        :param delay: Delay sending the reload message. Use 'forever' to\n                      not send it. This is useful to compile sass files to\n                      css, but reload on changed css files then only.\n        :param ignore: A function return True to ignore a certain pattern of\n                       filepath.\n        \"\"\"\n        if isinstance(func, string_types):\n            cmd = func\n            func = shell(func)\n            func.name = \"shell: {}\".format(cmd)\n\n        self.watcher.watch(filepath, func, delay, ignore=ignore)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart serving the application with the given port.", "response": "def serve(self, port=5500, liveport=None, host=None, root=None, debug=None,\n              open_url=False, restart_delay=2, open_url_delay=None,\n              live_css=True):\n        \"\"\"Start serve the server with the given port.\n\n        :param port: serve on this port, default is 5500\n        :param liveport: live reload on this port\n        :param host: serve on this hostname, default is 127.0.0.1\n        :param root: serve static on this root directory\n        :param debug: set debug mode, which autoreloads the app on code changes\n                      via Tornado (and causes polling). Defaults to True when\n                      ``self.app`` is set, otherwise False.\n        :param open_url_delay: open webbrowser after the delay seconds\n        :param live_css: whether to use live css or force reload on css.\n                         Defaults to True\n        \"\"\"\n        host = host or '127.0.0.1'\n        if root is not None:\n            self.root = root\n\n        self._setup_logging()\n        logger.info('Serving on http://%s:%s' % (host, port))\n\n        self.application(\n            port, host, liveport=liveport, debug=debug, live_css=live_css)\n\n        # Async open web browser after 5 sec timeout\n        if open_url or open_url_delay:\n            if open_url:\n                logger.warn('Use `open_url_delay` instead of `open_url`')\n            sleep = open_url_delay or 5\n\n            def opener():\n                time.sleep(sleep)\n                webbrowser.open('http://%s:%s' % (host, port))\n            threading.Thread(target=opener).start()\n\n        try:\n            self.watcher._changes.append(('__livereload__', restart_delay))\n            LiveReloadHandler.start_tasks()\n            add_reload_hook(lambda: IOLoop.instance().close(all_fds=True))\n            IOLoop.instance().start()\n        except KeyboardInterrupt:\n            logger.info('Shutting down...')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntesting used to see if we have coordinates or address", "response": "def already_coords(self, address):\n        \"\"\"test used to see if we have coordinates or address\"\"\"\n\n        m = re.search(self.COORD_MATCH, address)\n        return (m != None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef coords_string_parser(self, coords):\n\n        lat, lon = coords.split(',')\n        return {\"lat\": lat.strip(), \"lon\": lon.strip(), \"bounds\": {}}", "response": "Pareses the address string into coordinates to match address_to_coords return object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an address to coordinates", "response": "def address_to_coords(self, address):\n        \"\"\"Convert address to coordinates\"\"\"\n\n        base_coords = self.BASE_COORDS[self.region]\n        get_cord = self.COORD_SERVERS[self.region]\n        url_options = {\n            \"q\": address,\n            \"lang\": \"eng\",\n            \"origin\": \"livemap\",\n            \"lat\": base_coords[\"lat\"],\n            \"lon\": base_coords[\"lon\"]\n        }\n\n        response = requests.get(self.WAZE_URL + get_cord, params=url_options, headers=self.HEADERS)\n        for response_json in response.json():\n            if response_json.get('city'):\n                lat = response_json['location']['lat']\n                lon = response_json['location']['lon']\n                bounds = response_json['bounds']  # sometimes the coords don't match up\n                if bounds is not None:\n                    bounds['top'], bounds['bottom'] = max(bounds['top'], bounds['bottom']), min(bounds['top'], bounds['bottom'])\n                    bounds['left'], bounds['right'] = min(bounds['left'], bounds['right']), max(bounds['left'], bounds['right'])\n                else:\n                    bounds = {}\n                return {\"lat\": lat, \"lon\": lon, \"bounds\": bounds}\n        raise WRCError(\"Cannot get coords for %s\" % address)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_route(self, npaths=1, time_delta=0):\n\n        routing_server = self.ROUTING_SERVERS[self.region]\n\n        url_options = {\n            \"from\": \"x:%s y:%s\" % (self.start_coords[\"lon\"], self.start_coords[\"lat\"]),\n            \"to\": \"x:%s y:%s\" % (self.end_coords[\"lon\"], self.end_coords[\"lat\"]),\n            \"at\": time_delta,\n            \"returnJSON\": \"true\",\n            \"returnGeometries\": \"true\",\n            \"returnInstructions\": \"true\",\n            \"timeout\": 60000,\n            \"nPaths\": npaths,\n            \"options\": \"AVOID_TRAILS:t\",\n        }\n        if self.vehicle_type:\n            url_options[\"vehicleType\"] = self.vehicle_type\n\n        response = requests.get(self.WAZE_URL + routing_server, params=url_options, headers=self.HEADERS)\n        response.encoding = 'utf-8'\n        response_json = self._check_response(response)\n        if response_json:\n            if 'error' in response_json:\n                raise WRCError(response_json.get(\"error\"))\n            else:\n                if response_json.get(\"alternatives\"):\n                    return [alt['response'] for alt in response_json['alternatives']]\n                if npaths > 1:\n                    return [response_json['response']]\n                return response_json['response']\n        else:\n            raise WRCError(\"empty response\")", "response": "Get route data from waze"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_up_route(self, results, real_time=True, stop_at_bounds=False):\n\n        start_bounds = self.start_coords['bounds']\n        end_bounds = self.end_coords['bounds']\n\n        def between(target, min, max):\n            return target > min and target < max\n\n        time = 0\n        distance = 0\n        for segment in results:\n            if stop_at_bounds and segment.get('path'):\n                x = segment['path']['x']\n                y = segment['path']['y']\n                if (\n                    between(x, start_bounds.get('left', 0), start_bounds.get('right', 0)) or\n                    between(x, end_bounds.get('left', 0), end_bounds.get('right', 0))\n                ) and (\n                    between(y, start_bounds.get('bottom', 0), start_bounds.get('top', 0)) or\n                    between(y, end_bounds.get('bottom', 0), end_bounds.get('top', 0))\n                ):\n                    continue\n            time += segment['crossTime' if real_time else 'crossTimeWithoutRealTime']\n            distance += segment['length']\n        route_time = time / 60.0\n        route_distance = distance / 1000.0\n        return route_time, route_distance", "response": "Calculate route time and distance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calc_route_info(self, real_time=True, stop_at_bounds=False, time_delta=0):\n\n        route = self.get_route(1, time_delta)\n        results = route['results']\n        route_time, route_distance = self._add_up_route(results, real_time=real_time, stop_at_bounds=stop_at_bounds)\n        self.log.info('Time %.2f minutes, distance %.2f km.', route_time, route_distance)\n        return route_time, route_distance", "response": "Calculate best route info."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_all_routes_info(self, npaths=3, real_time=True, stop_at_bounds=False, time_delta=0):\n\n        routes = self.get_route(npaths, time_delta)\n        results = {route['routeName']: self._add_up_route(route['results'], real_time=real_time, stop_at_bounds=stop_at_bounds) for route in routes}\n        route_time = [route[0] for route in results.values()]\n        route_distance = [route[1] for route in results.values()]\n        self.log.info('Time %.2f - %.2f minutes, distance %.2f - %.2f km.', min(route_time), max(route_time), min(route_distance), max(route_distance))\n        return results", "response": "Calculate all route infos."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize(self):\n        self._key_prefix = self._config.get('redis', 'key_prefix')\n        self._job_expire_interval = int(\n            self._config.get('sharq', 'job_expire_interval')\n        )\n        self._default_job_requeue_limit = int(\n            self._config.get('sharq', 'default_job_requeue_limit')\n        )\n\n        # initalize redis\n        redis_connection_type = self._config.get('redis', 'conn_type')\n        db = self._config.get('redis', 'db')\n        if redis_connection_type == 'unix_sock':\n            self._r = redis.StrictRedis(\n                db=db,\n                unix_socket_path=self._config.get('redis', 'unix_socket_path')\n            )\n        elif redis_connection_type == 'tcp_sock':\n            self._r = redis.StrictRedis(\n                db=db,\n                host=self._config.get('redis', 'host'),\n                port=self._config.get('redis', 'port')\n            )\n\n        self._load_lua_scripts()", "response": "Read the SharQ configuration and set appropriate\n        variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the configuration file and load it into memory.", "response": "def _load_config(self):\n        \"\"\"Read the configuration file and load it into memory.\"\"\"\n        self._config = ConfigParser.SafeConfigParser()\n        self._config.read(self.config_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_lua_scripts(self):\n        # load lua scripts\n        lua_script_path = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)),\n            'scripts/lua'\n        )\n        with open(os.path.join(\n                lua_script_path,\n                'enqueue.lua'), 'r') as enqueue_file:\n            self._lua_enqueue_script = enqueue_file.read()\n            self._lua_enqueue = self._r.register_script(\n                self._lua_enqueue_script)\n\n        with open(os.path.join(\n                lua_script_path,\n                'dequeue.lua'), 'r') as dequeue_file:\n            self._lua_dequeue_script = dequeue_file.read()\n            self._lua_dequeue = self._r.register_script(\n                self._lua_dequeue_script)\n\n        with open(os.path.join(\n                lua_script_path,\n                'finish.lua'), 'r') as finish_file:\n            self._lua_finish_script = finish_file.read()\n            self._lua_finish = self._r.register_script(self._lua_finish_script)\n\n        with open(os.path.join(\n                lua_script_path,\n                'interval.lua'), 'r') as interval_file:\n            self._lua_interval_script = interval_file.read()\n            self._lua_interval = self._r.register_script(\n                self._lua_interval_script)\n\n        with open(os.path.join(\n                lua_script_path,\n                'requeue.lua'), 'r') as requeue_file:\n            self._lua_requeue_script = requeue_file.read()\n            self._lua_requeue = self._r.register_script(\n                self._lua_requeue_script)\n\n        with open(os.path.join(\n                lua_script_path,\n                'metrics.lua'), 'r') as metrics_file:\n            self._lua_metrics_script = metrics_file.read()\n            self._lua_metrics = self._r.register_script(\n                self._lua_metrics_script)", "response": "Loads all lua scripts required by SharQ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dequeue(self, queue_type='default'):\n        if not is_valid_identifier(queue_type):\n            raise BadArgumentException('`queue_type` has an invalid value.')\n\n        timestamp = str(generate_epoch())\n\n        keys = [\n            self._key_prefix,\n            queue_type\n        ]\n        args = [\n            timestamp,\n            self._job_expire_interval\n        ]\n\n        dequeue_response = self._lua_dequeue(keys=keys, args=args)\n\n        if len(dequeue_response) < 4:\n            response = {\n                'status': 'failure'\n            }\n            return response\n\n        queue_id, job_id, payload, requeues_remaining = dequeue_response\n        payload = deserialize_payload(payload[1:-1])\n\n        response = {\n            'status': 'success',\n            'queue_id': queue_id,\n            'job_id': job_id,\n            'payload': payload,\n            'requeues_remaining': int(requeues_remaining)\n        }\n\n        return response", "response": "Dequeues a job from any of the ready queues\n            returns a success status."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmarks any dequeued job as completed successfully.", "response": "def finish(self, job_id, queue_id, queue_type='default'):\n        \"\"\"Marks any dequeued job as *completed successfully*.\n        Any job which gets a finish will be treated as complete\n        and will be removed from the SharQ.\n        \"\"\"\n        if not is_valid_identifier(job_id):\n            raise BadArgumentException('`job_id` has an invalid value.')\n\n        if not is_valid_identifier(queue_id):\n            raise BadArgumentException('`queue_id` has an invalid value.')\n\n        if not is_valid_identifier(queue_type):\n            raise BadArgumentException('`queue_type` has an invalid value.')\n\n        keys = [\n            self._key_prefix,\n            queue_type\n        ]\n\n        args = [\n            queue_id,\n            job_id\n        ]\n\n        response = {\n            'status': 'success'\n        }\n\n        finish_response = self._lua_finish(keys=keys, args=args)\n        if finish_response == 0:\n            # the finish failed.\n            response.update({\n                'status': 'failure'\n            })\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interval(self, interval, queue_id, queue_type='default'):\n        # validate all the input\n        if not is_valid_interval(interval):\n            raise BadArgumentException('`interval` has an invalid value.')\n\n        if not is_valid_identifier(queue_id):\n            raise BadArgumentException('`queue_id` has an invalid value.')\n\n        if not is_valid_identifier(queue_type):\n            raise BadArgumentException('`queue_type` has an invalid value.')\n\n        # generate the interval key\n        interval_hmap_key = '%s:interval' % self._key_prefix\n        interval_queue_key = '%s:%s' % (queue_type, queue_id)\n        keys = [\n            interval_hmap_key,\n            interval_queue_key\n        ]\n\n        args = [\n            interval\n        ]\n        interval_response = self._lua_interval(keys=keys, args=args)\n        if interval_response == 0:\n            # the queue with the id and type does not exist.\n            response = {\n                'status': 'failure'\n            }\n        else:\n            response = {\n                'status': 'success'\n            }\n\n        return response", "response": "Updates the interval for a specific queue_id and queue_type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef requeue(self):\n        timestamp = str(generate_epoch())\n        # get all queue_types and requeue one by one.\n        # not recommended to do this entire process\n        # in lua as it might take long and block other\n        # enqueues and dequeues.\n        active_queue_type_list = self._r.smembers(\n            '%s:active:queue_type' % self._key_prefix)\n        for queue_type in active_queue_type_list:\n            # requeue all expired jobs in all queue types.\n            keys = [\n                self._key_prefix,\n                queue_type\n            ]\n\n            args = [\n                timestamp\n            ]\n            job_discard_list = self._lua_requeue(keys=keys, args=args)\n            # discard the jobs if any\n            for job in job_discard_list:\n                queue_id, job_id = job.split(':')\n                # explicitly finishing a job\n                # is nothing but discard.\n                self.finish(\n                    job_id=job_id,\n                    queue_id=queue_id,\n                    queue_type=queue_type\n                )", "response": "Requeues any expired jobs back into their respective queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef metrics(self, queue_type=None, queue_id=None):\n        if queue_id is not None and not is_valid_identifier(queue_id):\n            raise BadArgumentException('`queue_id` has an invalid value.')\n\n        if queue_type is not None and not is_valid_identifier(queue_type):\n            raise BadArgumentException('`queue_type` has an invalid value.')\n\n        response = {\n            'status': 'failure'\n        }\n        if not queue_type and not queue_id:\n            # return global stats.\n            # list of active queue types (ready + active)\n            active_queue_types = self._r.smembers(\n                '%s:active:queue_type' % self._key_prefix)\n            ready_queue_types = self._r.smembers(\n                '%s:ready:queue_type' % self._key_prefix)\n            all_queue_types = active_queue_types | ready_queue_types\n            # global rates for past 10 minutes\n            timestamp = str(generate_epoch())\n            keys = [\n                self._key_prefix\n            ]\n            args = [\n                timestamp\n            ]\n\n            enqueue_details, dequeue_details = self._lua_metrics(\n                keys=keys, args=args)\n\n            enqueue_counts = {}\n            dequeue_counts = {}\n            # the length of enqueue & dequeue details are always same.\n            for i in xrange(0, len(enqueue_details), 2):\n                enqueue_counts[str(enqueue_details[i])] = int(\n                    enqueue_details[i + 1] or 0)\n                dequeue_counts[str(dequeue_details[i])] = int(\n                    dequeue_details[i + 1] or 0)\n\n            response.update({\n                'status': 'success',\n                'queue_types': list(all_queue_types),\n                'enqueue_counts': enqueue_counts,\n                'dequeue_counts': dequeue_counts\n            })\n            return response\n        elif queue_type and not queue_id:\n            # return list of queue_ids.\n            # get data from two sorted sets in a transaction\n            pipe = self._r.pipeline()\n            pipe.zrange('%s:%s' % (self._key_prefix, queue_type), 0, -1)\n            pipe.zrange('%s:%s:active' % (self._key_prefix, queue_type), 0, -1)\n            ready_queues, active_queues = pipe.execute()\n            # extract the queue_ids from the queue_id:job_id string\n            active_queues = [i.split(':')[0] for i in active_queues]\n            all_queue_set = set(ready_queues) | set(active_queues)\n            response.update({\n                'status': 'success',\n                'queue_ids': list(all_queue_set)\n            })\n            return response\n        elif queue_type and queue_id:\n            # return specific details.\n            active_queue_types = self._r.smembers(\n                '%s:active:queue_type' % self._key_prefix)\n            ready_queue_types = self._r.smembers(\n                '%s:ready:queue_type' % self._key_prefix)\n            all_queue_types = active_queue_types | ready_queue_types\n            # queue specific rates for past 10 minutes\n            timestamp = str(generate_epoch())\n            keys = [\n                '%s:%s:%s' % (self._key_prefix, queue_type, queue_id)\n            ]\n            args = [\n                timestamp\n            ]\n\n            enqueue_details, dequeue_details = self._lua_metrics(\n                keys=keys, args=args)\n\n            enqueue_counts = {}\n            dequeue_counts = {}\n            # the length of enqueue & dequeue details are always same.\n            for i in xrange(0, len(enqueue_details), 2):\n                enqueue_counts[str(enqueue_details[i])] = int(\n                    enqueue_details[i + 1] or 0)\n                dequeue_counts[str(dequeue_details[i])] = int(\n                    dequeue_details[i + 1] or 0)\n\n            # get the queue length for the job queue\n            queue_length = self._r.llen('%s:%s:%s' % (\n                self._key_prefix, queue_type, queue_id))\n\n            response.update({\n                'status': 'success',\n                'queue_length': int(queue_length),\n                'enqueue_counts': enqueue_counts,\n                'dequeue_counts': dequeue_counts\n            })\n            return response\n        elif not queue_type and queue_id:\n            raise BadArgumentException(\n                '`queue_id` should be accompanied by `queue_type`.')\n\n        return response", "response": "Returns a dictionary of statistics about the specified queue type and id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_queue(self, queue_type=None, queue_id=None, purge_all=False):\n        if queue_id is None or not is_valid_identifier(queue_id):\n            raise BadArgumentException('`queue_id` has an invalid value.')\n\n        if queue_type is None or not is_valid_identifier(queue_type):\n            raise BadArgumentException('`queue_type` has an invalid value.')\n\n        response = {\n            'status': 'Failure',\n            'message': 'No queued calls found'\n            }\n        # remove from the primary sorted set\n        primary_set = '{}:{}'.format(self._key_prefix, queue_type)\n        queued_status = self._r.zrem(primary_set, queue_id)\n        if queued_status:\n            response.update({'status': 'Success', \n                    'message': 'Successfully removed all queued calls'})\n        # do a full cleanup of reources\n        # although this is not necessary as we don't remove resources \n        # while dequeue operation\n        job_queue_list = '{}:{}:{}'.format(self._key_prefix, queue_type, queue_id)\n        if queued_status and purge_all:\n            job_list = self._r.lrange(job_queue_list, 0, -1)\n            pipe = self._r.pipeline()\n            # clear the payload data for job_uuid\n            for job_uuid in job_list:\n                if job_uuid is None:\n                    continue\n                payload_set = '{}:payload'.format(self._key_prefix)\n                job_payload_key = '{}:{}:{}'.format(queue_type, queue_id, job_uuid)\n                pipe.hdel(payload_set, job_payload_key)\n            # clear jobrequest interval\n            interval_set = '{}:interval'.format(self._key_prefix)\n            job_interval_key = '{}:{}'.format(queue_type, queue_id)\n            pipe.hdel(interval_set, job_interval_key)\n            # clear job_queue_list\n            pipe.delete(job_queue_list)\n            pipe.execute()\n            response.update({'status': 'Success', \n                    'message': 'Successfully removed all queued calls and purged related resources'})\n        else:\n            # always delete the job queue list\n            self._r.delete(job_queue_list)\n        return response", "response": "This method deletes all entries in the queue with particular queue_id and queue_type. It will remove all queued entries from the redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_identifier(identifier):\n    if not isinstance(identifier, basestring):\n        return False\n\n    if len(identifier) > 100 or len(identifier) < 1:\n        return False\n\n    condensed_form = set(list(identifier.lower()))\n    return condensed_form.issubset(VALID_IDENTIFIER_SET)", "response": "Checks if the given identifier is valid or not."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the given requeue limit is valid.", "response": "def is_valid_requeue_limit(requeue_limit):\n    \"\"\"Checks if the given requeue limit is valid.\n    A valid requeue limit is always greater than\n    or equal to -1.\n    \"\"\"\n    if not isinstance(requeue_limit, (int, long)):\n        return False\n\n    if requeue_limit <= -2:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of values to search on when we are looking for a package with the given name.", "response": "def get_search_names(name):\n    \"\"\"Return a list of values to search on when we are looking for a package\n    with the given name.\n\n    This is required to search on both pyramid_debugtoolbar and\n    pyramid-debugtoolbar.\n\n    \"\"\"\n    parts = re.split('[-_.]', name)\n    if len(parts) == 1:\n        return parts\n\n    result = set()\n    for i in range(len(parts) - 1, 0, -1):\n        for s1 in '-_.':\n            prefix = s1.join(parts[:i])\n            for s2 in '-_.':\n                suffix = s2.join(parts[i:])\n                for s3 in '-_.':\n                    result.add(s3.join([prefix, suffix]))\n    return list(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nalter the request body for compatibility with older distutils clients", "response": "def alter_old_distutils_request(request: WSGIRequest):\n    \"\"\"Alter the request body for compatibility with older distutils clients\n\n    Due to a bug in the Python distutils library, the request post is sent\n    using \\n as a separator instead of the \\r\\n that the HTTP spec demands.\n    This breaks the Django form parser and therefore we have to write a\n    custom parser.\n\n    This bug was fixed in the Python 2.7.4 and 3.4:\n\n    http://bugs.python.org/issue10510\n    \"\"\"\n    # We first need to retrieve the body before accessing POST or FILES since\n    # it can only be read once.\n    body = request.body\n    if request.POST or request.FILES:\n        return\n\n    new_body = BytesIO()\n\n    # Split the response in the various parts based on the boundary string\n    content_type, opts = parse_header(request.META['CONTENT_TYPE'].encode('ascii'))\n    parts = body.split(b'\\n--' + opts['boundary'] + b'\\n')\n    for part in parts:\n        if b'\\n\\n' not in part:\n            continue\n\n        headers, content = part.split(b'\\n\\n', 1)\n        if not headers:\n            continue\n\n        new_body.write(b'--' + opts['boundary'] + b'\\r\\n')\n        new_body.write(headers.replace(b'\\n', b'\\r\\n'))\n        new_body.write(b'\\r\\n\\r\\n')\n        new_body.write(content)\n        new_body.write(b'\\r\\n')\n    new_body.write(b'--' + opts['boundary'] + b'--\\r\\n')\n\n    request._body = new_body.getvalue()\n    request.META['CONTENT_LENGTH'] = len(request._body)\n\n    # Clear out _files and _post so that the request object re-parses the body\n    if hasattr(request, '_files'):\n        delattr(request, '_files')\n    if hasattr(request, '_post'):\n        delattr(request, '_post')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_files(sender, **kwargs):\n    instance = kwargs['instance']\n\n    if not hasattr(instance.distribution, 'path'):\n        return\n\n    if not os.path.exists(instance.distribution.path):\n        return\n\n    # Check if there are other instances which reference this fle\n    is_referenced = (\n        instance.__class__.objects\n        .filter(distribution=instance.distribution)\n        .exclude(pk=instance._get_pk_val())\n        .exists())\n    if is_referenced:\n        return\n\n    try:\n        instance.distribution.storage.delete(instance.distribution.path)\n    except Exception:\n        logger.exception(\n            'Error when trying to delete file %s of package %s:' % (\n                instance.pk, instance.distribution.path))", "response": "Signal callback for deleting old files when database item is deleted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the md5 hash of the given file - object", "response": "def md5_hash_file(fh):\n    \"\"\"Return the md5 hash of the given file-object\"\"\"\n    md5 = hashlib.md5()\n    while True:\n        data = fh.read(8192)\n        if not data:\n            break\n        md5.update(data)\n    return md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_versio_versioning_scheme(full_class_path):\n    module_path = '.'.join(full_class_path.split('.')[0:-1])\n    class_name = full_class_path.split('.')[-1]\n    try:\n        module = importlib.import_module(module_path)\n    except ImportError:\n        raise RuntimeError('Invalid specified Versio schema {}'.format(full_class_path))\n\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise RuntimeError(\n            'Could not find Versio schema class {!r} inside {!r} module.'.format(\n                class_name, module_path))", "response": "Return a class based on it s full path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements xmlrpc search command.", "response": "def search(spec, operator='and'):\n    \"\"\"Implement xmlrpc search command.\n\n    This only searches through the mirrored and private packages\n\n    \"\"\"\n    field_map = {\n        'name': 'name__icontains',\n        'summary': 'releases__summary__icontains',\n    }\n\n    query_filter = None\n    for field, values in spec.items():\n        for value in values:\n            if field not in field_map:\n                continue\n\n            field_filter = Q(**{field_map[field]: value})\n            if not query_filter:\n                query_filter = field_filter\n                continue\n\n            if operator == 'and':\n                query_filter &= field_filter\n            else:\n                query_filter |= field_filter\n\n    result = []\n    packages = models.Package.objects.filter(query_filter).all()[:20]\n    for package in packages:\n        release = package.releases.all()[0]\n        result.append({\n            'name': package.name,\n            'summary': release.summary,\n            'version': release.version,\n            '_pypi_ordering': 0,\n        })\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef credentials_required(view_func):\n    @wraps(view_func, assigned=available_attrs(view_func))\n    def decorator(request, *args, **kwargs):\n        if settings.LOCALSHOP_USE_PROXIED_IP:\n            try:\n                ip_addr = request.META['HTTP_X_FORWARDED_FOR']\n            except KeyError:\n                return HttpResponseForbidden('No permission')\n            else:\n                # HTTP_X_FORWARDED_FOR can be a comma-separated list of IPs.\n                # The client's IP will be the first one.\n                ip_addr = ip_addr.split(\",\")[0].strip()\n        else:\n            ip_addr = request.META['REMOTE_ADDR']\n\n        if CIDR.objects.has_access(ip_addr, with_credentials=False):\n            return view_func(request, *args, **kwargs)\n\n        if not CIDR.objects.has_access(ip_addr, with_credentials=True):\n            return HttpResponseForbidden('No permission')\n\n        # Just return the original view because already logged in\n        if request.user.is_authenticated():\n            return view_func(request, *args, **kwargs)\n\n        user = authenticate_user(request)\n        if user is not None:\n            login(request, user)\n            return view_func(request, *args, **kwargs)\n\n        return HttpResponseUnauthorized(content='Authorization Required')\n    return decorator", "response": "This decorator is used to decorate views that need simple authentication against Django s authentication framework."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef no_duplicates(function, *args, **kwargs):\n    @wraps(function)\n    def wrapper(self, *args, **kwargs):\n        key = generate_key(function, *args, **kwargs)\n        try:\n            function(self, *args, **kwargs)\n        finally:\n            logging.info('Removing key %s', key)\n            cache.delete(key)\n\n    return wrapper", "response": "Decorator that removes duplicate entries from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload the file with the given pk and save it to the local cache.", "response": "def download_file(pk):\n    \"\"\"Download the file reference in `models.ReleaseFile` with the given pk.\n\n    \"\"\"\n    release_file = models.ReleaseFile.objects.get(pk=pk)\n    logging.info(\"Downloading %s\", release_file.url)\n\n    proxies = None\n    if settings.LOCALSHOP_HTTP_PROXY:\n        proxies = settings.LOCALSHOP_HTTP_PROXY\n    response = requests.get(release_file.url, stream=True, proxies=proxies)\n\n    # Write the file to the django file field\n    filename = os.path.basename(release_file.url)\n\n    # Setting the size manually since Django can't figure it our from\n    # the raw HTTPResponse\n    if 'content-length' in response.headers:\n        size = int(response.headers['content-length'])\n    else:\n        size = len(response.content)\n\n    # Setting the content type by first looking at the response header\n    # and falling back to guessing it from the filename\n    default_content_type = 'application/octet-stream'\n    content_type = response.headers.get('content-type')\n    if content_type is None or content_type == default_content_type:\n        content_type = mimetypes.guess_type(filename)[0] or default_content_type\n\n    # Using Django's temporary file upload system to not risk memory\n    # overflows\n    with TemporaryUploadedFile(name=filename, size=size, charset='utf-8',\n                               content_type=content_type) as temp_file:\n        temp_file.write(response.content)\n        temp_file.seek(0)\n\n        # Validate the md5 hash of the downloaded file\n        md5_hash = md5_hash_file(temp_file)\n        if md5_hash != release_file.md5_digest:\n            logging.error(\"MD5 hash mismatch: %s (expected: %s)\" % (\n                md5_hash, release_file.md5_digest))\n            return\n\n        release_file.distribution.save(filename, temp_file)\n        release_file.save()\n    logging.info(\"Complete\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_register_or_upload(post_data, files, user, repository):\n    name = post_data.get('name')\n    version = post_data.get('version')\n\n    if settings.LOCALSHOP_VERSIONING_TYPE:\n        scheme = get_versio_versioning_scheme(settings.LOCALSHOP_VERSIONING_TYPE)\n        try:\n            Version(version, scheme=scheme)\n        except AttributeError:\n            response = HttpResponseBadRequest(\n                reason=\"Invalid version supplied '{!s}' for '{!s}' scheme.\".format(\n                    version, settings.LOCALSHOP_VERSIONING_TYPE))\n            return response\n\n    if not name or not version:\n        logger.info(\"Missing name or version for package\")\n        return HttpResponseBadRequest('No name or version given')\n\n    try:\n        condition = Q()\n        for search_name in get_search_names(name):\n            condition |= Q(name__iexact=search_name)\n\n        package = repository.packages.get(condition)\n\n        # Error out when we try to override a mirror'ed package for now\n        # not sure what the best thing is\n        if not package.is_local:\n            return HttpResponseBadRequest(\n                '%s is a pypi package!' % package.name)\n\n        try:\n            release = package.releases.get(version=version)\n        except ObjectDoesNotExist:\n            release = None\n    except ObjectDoesNotExist:\n        package = None\n        release = None\n\n    # Validate the data\n    form = forms.ReleaseForm(post_data, instance=release)\n    if not form.is_valid():\n        return HttpResponseBadRequest(reason=form.errors.values()[0][0])\n\n    if not package:\n        pkg_form = forms.PackageForm(post_data, repository=repository)\n        if not pkg_form.is_valid():\n            return HttpResponseBadRequest(\n                reason=six.next(six.itervalues(pkg_form.errors))[0])\n        package = pkg_form.save()\n\n    release = form.save(commit=False)\n    release.package = package\n    release.save()\n\n    # If this is an upload action then process the uploaded file\n    if files:\n        files = {\n            'distribution': files['content']\n        }\n        filename = files['distribution']._name\n        try:\n            release_file = release.files.get(filename=filename)\n            if settings.LOCALSHOP_RELEASE_OVERWRITE is False:\n                message = 'That it already released, please bump version.'\n                return HttpResponseBadRequest(message)\n        except ObjectDoesNotExist:\n            release_file = models.ReleaseFile(\n                release=release, filename=filename)\n\n        form_file = forms.ReleaseFileForm(\n            post_data, files, instance=release_file)\n        if not form_file.is_valid():\n            return HttpResponseBadRequest('ERRORS %s' % form_file.errors)\n        release_file = form_file.save(commit=False)\n        release_file.save()\n\n    return HttpResponse()", "response": "Handle a register or upload comment issued via distutils."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(self):\n        from .tasks import download_file\n        if not settings.LOCALSHOP_ISOLATED:\n            download_file.delay(pk=self.pk)\n        else:\n            download_file(pk=self.pk)", "response": "Start a celery task to download the release file from pypi."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dispatch_queue(loader):\n    # type: (DataLoader) -> None\n    \"\"\"\n    Given the current state of a Loader instance, perform a batch load\n    from its current queue.\n    \"\"\"\n    # Take the current loader queue, replacing it with an empty queue.\n    queue = loader._queue\n    loader._queue = []\n\n    # If a maxBatchSize was provided and the queue is longer, then segment the\n    # queue into multiple batches, otherwise treat the queue as a single batch.\n    max_batch_size = loader.max_batch_size\n\n    if max_batch_size and max_batch_size < len(queue):\n        chunks = get_chunks(queue, max_batch_size)\n        for chunk in chunks:\n            dispatch_queue_batch(loader, chunk)\n    else:\n        dispatch_queue_batch(loader, queue)", "response": "Given the current state of a Loader instance perform a batch load\n    from its current queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef failed_dispatch(loader, queue, error):\n    # type: (DataLoader, Iterable[Loader], Exception) -> None\n    \"\"\"\n    Do not cache individual loads if the entire batch dispatch fails,\n    but still reject each request so they do not hang.\n    \"\"\"\n    for l in queue:\n        loader.clear(l.key)\n        l.reject(error)", "response": "Cache individual loads if the entire batch dispatch fails."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self, key=None):\n        # type: (Hashable) -> Promise\n        \"\"\"\n        Loads a key, returning a `Promise` for the value represented by that key.\n        \"\"\"\n        if key is None:\n            raise TypeError(\n                (\n                    \"The loader.load() function must be called with a value,\"\n                    + \"but got: {}.\"\n                ).format(key)\n            )\n\n        cache_key = self.get_cache_key(key)\n\n        # If caching and there is a cache-hit, return cached Promise.\n        if self.cache:\n            cached_promise = self._promise_cache.get(cache_key)\n            if cached_promise:\n                return cached_promise\n\n        # Otherwise, produce a new Promise for this value.\n\n        promise = Promise(partial(self.do_resolve_reject, key))  # type: ignore\n\n        # If caching, cache this promise.\n        if self.cache:\n            self._promise_cache[cache_key] = promise\n\n        return promise", "response": "Loads a key returning a Promise that resolves with the value represented by that key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading multiple keys promising an array of values", "response": "def load_many(self, keys):\n        # type: (Iterable[Hashable]) -> Promise\n        \"\"\"\n        Loads multiple keys, promising an array of values\n\n        >>> a, b = await my_loader.load_many([ 'a', 'b' ])\n\n        This is equivalent to the more verbose:\n\n        >>> a, b = await Promise.all([\n        >>>    my_loader.load('a'),\n        >>>    my_loader.load('b')\n        >>> ])\n        \"\"\"\n        if not isinstance(keys, Iterable):\n            raise TypeError(\n                (\n                    \"The loader.loadMany() function must be called with Array<key> \"\n                    + \"but got: {}.\"\n                ).format(keys)\n            )\n\n        return Promise.all([self.load(key) for key in keys])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear(self, key):\n        # type: (Hashable) -> DataLoader\n        \"\"\"\n        Clears the value at `key` from the cache, if it exists. Returns itself for\n        method chaining.\n        \"\"\"\n        cache_key = self.get_cache_key(key)\n        self._promise_cache.pop(cache_key, None)\n        return self", "response": "Clears the value at key from the cache. Returns itself for\n        method chaining."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the provied key and value to the cache.", "response": "def prime(self, key, value):\n        # type: (Hashable, Any) -> DataLoader\n        \"\"\"\n        Adds the provied key and value to the cache. If the key already exists, no\n        change is made. Returns itself for method chaining.\n        \"\"\"\n        cache_key = self.get_cache_key(key)\n\n        # Only add the key if it does not already exist.\n        if cache_key not in self._promise_cache:\n            # Cache a rejected promise if the value is an Error, in order to match\n            # the behavior of load(key).\n            if isinstance(value, Exception):\n                promise = Promise.reject(value)\n            else:\n                promise = Promise.resolve(value)\n\n            self._promise_cache[cache_key] = promise\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tuple of the promise version.", "response": "def get_complete_version(version=None):\n    \"\"\"Returns a tuple of the promise version. If version argument is non-empty,\n    then checks for correctness of the tuple provided.\n    \"\"\"\n    if version is None:\n        from promise import VERSION\n\n        return VERSION\n    else:\n        assert len(version) == 5\n        assert version[3] in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a promise that resolves with the dictionary of keys and values.", "response": "def for_dict(cls, m):\n        # type: (Dict[Hashable, Promise[S]]) -> Promise[Dict[Hashable, S]]\n        \"\"\"\n        A special function that takes a dictionary of promises\n        and turns them into a promise for a dictionary of values.\n        In other words, this turns an dictionary of promises for values\n        into a promise for a dictionary of values.\n        \"\"\"\n        dict_type = type(m)  # type: Type[Dict]\n\n        if not m:\n            return cls.resolve(dict_type())\n\n        def handle_success(resolved_values):\n            # type: (List[S]) -> Dict[Hashable, S]\n            return dict_type(zip(m.keys(), resolved_values))\n\n        return cls.all(m.values()).then(handle_success)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_thenable(cls, obj):\n        # type: (Any) -> bool\n        \"\"\"\n        A utility function to determine if the specified\n        object is a promise using \"duck typing\".\n        \"\"\"\n        _type = obj.__class__\n        if obj is None or _type in BASE_TYPES:\n            return False\n\n        return (\n            issubclass(_type, Promise)\n            or iscoroutine(obj)  # type: ignore\n            or is_future_like(_type)\n        )", "response": "A utility function to determine if the specified object is a promise using duck typing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _channel_loop(detection, template, min_cc, detection_id, interpolate, i,\n                  pre_lag_ccsum=None, detect_chans=0,\n                  horizontal_chans=['E', 'N', '1', '2'], vertical_chans=['Z'],\n                  debug=0):\n    \"\"\"\n    Inner loop for correlating and assigning picks.\n\n    Utility function to take a stream of data for the detected event and write\n    maximum correlation to absolute time as picks in an obspy.core.event.Event\n    object.\n    Only outputs picks for picks above min_cc.\n\n    :type detection: obspy.core.stream.Stream\n    :param detection:\n        Stream of data for the slave event detected using template.\n    :type template: obspy.core.stream.Stream\n    :param template: Stream of data as the template for the detection.\n    :type min_cc: float\n    :param min_cc: Minimum cross-correlation value to allow a pick to be made.\n    :type detection_id: str\n    :param detection_id: Detection ID to associate the event with.\n    :type interpolate: bool\n    :param interpolate:\n        Interpolate the correlation function to achieve sub-sample precision.\n    :type i: int\n    :param i:\n        Used to track which process has occurred when running in parallel.\n    :type pre_lag_ccsum: float\n    :param pre_lag_ccsum:\n        Cross-correlation sum before lag-calc, will check that the\n        cross-correlation sum is increased by lag-calc (using all channels,\n        ignoring min_cc)\n    :type detect_chans: int\n    :param detect_chans:\n        Number of channels originally used in detections, must match the number\n        used here to allow for cccsum checking.\n    :type horizontal_chans: list\n    :param horizontal_chans:\n        List of channel endings for horizontal-channels, on which S-picks will\n        be made.\n    :type vertical_chans: list\n    :param vertical_chans:\n        List of channel endings for vertical-channels, on which P-picks will\n        be made.\n    :type debug: int\n    :param debug: Debug output level 0-5.\n\n    :returns:\n        Event object containing network, station, channel and pick information.\n    :rtype: :class:`obspy.core.event.Event`\n    \"\"\"\n    from eqcorrscan.core.match_filter import normxcorr2\n    import math\n    event = Event()\n    s_stachans = {}\n    cccsum = 0\n    checksum = 0\n    used_chans = 0\n    for tr in template:\n        temp_net = tr.stats.network\n        temp_sta = tr.stats.station\n        temp_chan = tr.stats.channel\n        debug_print('Working on: %s.%s.%s' % (temp_net, temp_sta, temp_chan),\n                    3, debug)\n        image = detection.select(station=temp_sta, channel=temp_chan)\n        if len(image) == 0 or sum(image[0].data) == 0:\n            print('No match in image.')\n            continue\n        if interpolate:\n            try:\n                ccc = normxcorr2(tr.data, image[0].data)\n            except Exception:\n                print('Could not calculate cc')\n                print('Image is %i long' % len(image[0].data))\n                print('Template is %i long' % len(tr.data))\n                continue\n            try:\n                shift, cc_max = _xcorr_interp(ccc=ccc, dt=image[0].stats.delta)\n            except IndexError:\n                print('Could not interpolate ccc, not smooth')\n                ccc = normxcorr2(tr.data, image[0].data)\n                cc_max = np.amax(ccc)\n                shift = np.argmax(ccc) * image[0].stats.delta\n            # Convert the maximum cross-correlation time to an actual time\n            if math.isnan(cc_max):\n                print('Problematic trace, no cross correlation possible')\n                continue\n            else:\n                picktime = image[0].stats.starttime + shift\n        else:\n            # Convert the maximum cross-correlation time to an actual time\n            try:\n                ccc = normxcorr2(tr.data, image[0].data)\n            except Exception:\n                print('Could not calculate cc')\n                print('Image is %i long' % len(image[0].data))\n                print('Template is %i long' % len(tr.data))\n                continue\n            cc_max = np.amax(ccc)\n            if math.isnan(cc_max):\n                print('Problematic trace, no cross correlation possible')\n                continue\n            else:\n                picktime = image[0].stats.starttime + (\n                    np.argmax(ccc) * image[0].stats.delta)\n        debug_print('Maximum cross-corr=%s' % cc_max, 3, debug)\n        checksum += cc_max\n        used_chans += 1\n        if cc_max < min_cc:\n            debug_print('Correlation below threshold, not used', 3, debug)\n            continue\n        cccsum += cc_max\n        # Perhaps weight each pick by the cc val or cc val^2?\n        # weight = np.amax(ccc) ** 2\n        if temp_chan[-1] in vertical_chans:\n            phase = 'P'\n        # Only take the S-pick with the best correlation\n        elif temp_chan[-1] in horizontal_chans:\n            phase = 'S'\n            debug_print('Making S-pick on: %s.%s.%s' %\n                        (temp_net, temp_sta, temp_chan), 4, debug)\n            if temp_sta not in s_stachans.keys():\n                s_stachans[temp_sta] = ((temp_chan, np.amax(ccc),\n                                         picktime))\n            elif temp_sta in s_stachans.keys():\n                if np.amax(ccc) > s_stachans[temp_sta][1]:\n                    picktime = picktime\n                else:\n                    continue\n        else:\n            phase = None\n        _waveform_id = WaveformStreamID(\n            network_code=temp_net, station_code=temp_sta,\n            channel_code=temp_chan)\n        event.picks.append(Pick(\n            waveform_id=_waveform_id, time=picktime,\n            method_id=ResourceIdentifier('EQcorrscan'), phase_hint=phase,\n            creation_info='eqcorrscan.core.lag_calc',\n            evaluation_mode='automatic',\n            comments=[Comment(text='cc_max=%s' % cc_max)]))\n        event.resource_id = detection_id\n    ccc_str = (\"detect_val=%s\" % cccsum)\n    event.comments.append(Comment(text=ccc_str))\n    if used_chans == detect_chans:\n        if pre_lag_ccsum is not None and\\\n           checksum - pre_lag_ccsum < -(0.3 * pre_lag_ccsum):\n            msg = ('lag-calc has decreased cccsum from %f to %f - '\n                   % (pre_lag_ccsum, checksum))\n            raise LagCalcError(msg)\n    else:\n        warnings.warn('Cannot check if cccsum is better, used %i channels '\n                      'for detection, but %i are used here'\n                      % (detect_chans, used_chans))\n    return i, event", "response": "This function is used to create a new object for each channel in the sequence of picks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to loop through multiple detections for one template. Designed to run for the same day of data for I/O simplicity, but as you are passing stream objects it could run for all the detections ever, as long as you have the RAM! :type detection_streams: list :param detection_streams: List of all the detections for this template that you want to compute the optimum pick for. Individual things in list should be of :class:`obspy.core.stream.Stream` type. :type template: obspy.core.stream.Stream :param template: The original template used to detect the detections passed :type min_cc: float :param min_cc: Minimum cross-correlation value to be allowed for a pick. :type detections: list :param detections: List of detections to associate events with an input detection. :type horizontal_chans: list :param horizontal_chans: List of channel endings for horizontal-channels, on which S-picks will be made. :type vertical_chans: list :param vertical_chans: List of channel endings for vertical-channels, on which P-picks will be made. :type interpolate: bool :param interpolate: Interpolate the correlation function to achieve sub-sample precision. :type debug: int :param debug: debug output level 0-5. :returns: Catalog object containing Event objects for each detection created by this template. :rtype: :class:`obspy.core.event.Catalog`", "response": "def _day_loop(detection_streams, template, min_cc, detections,\n              horizontal_chans, vertical_chans, interpolate, cores, parallel,\n              debug=0):\n    \"\"\"\n    Function to loop through multiple detections for one template.\n\n    Designed to run for the same day of data for I/O simplicity, but as you\n    are passing stream objects it could run for all the detections ever, as\n    long as you have the RAM!\n\n    :type detection_streams: list\n    :param detection_streams:\n        List of all the detections for this template that you want to compute\n        the optimum pick for. Individual things in list should be of\n        :class:`obspy.core.stream.Stream` type.\n    :type template: obspy.core.stream.Stream\n    :param template: The original template used to detect the detections passed\n    :type min_cc: float\n    :param min_cc: Minimum cross-correlation value to be allowed for a pick.\n    :type detections: list\n    :param detections:\n        List of detections to associate events with an input detection.\n    :type horizontal_chans: list\n    :param horizontal_chans:\n        List of channel endings for horizontal-channels, on which S-picks will\n        be made.\n    :type vertical_chans: list\n    :param vertical_chans:\n        List of channel endings for vertical-channels, on which P-picks will\n        be made.\n    :type interpolate: bool\n    :param interpolate:\n        Interpolate the correlation function to achieve sub-sample precision.\n    :type debug: int\n    :param debug: debug output level 0-5.\n\n    :returns:\n        Catalog object containing Event objects for each detection created by\n        this template.\n    :rtype: :class:`obspy.core.event.Catalog`\n    \"\"\"\n    if len(detection_streams) == 0:\n        return Catalog()\n    if not cores:\n        num_cores = cpu_count()\n    else:\n        num_cores = cores\n    if num_cores > len(detection_streams):\n        num_cores = len(detection_streams)\n    if parallel:\n        pool = Pool(processes=num_cores)\n        debug_print('Made pool of %i workers' % num_cores, 4, debug)\n        # Parallel generation of events for each detection:\n        # results will be a list of (i, event class)\n        results = [pool.apply_async(\n            _channel_loop, (detection_streams[i], ),\n            {'template': template, 'min_cc': min_cc,\n             'detection_id': detections[i].id, 'interpolate': interpolate,\n             'i': i, 'pre_lag_ccsum': detections[i].detect_val,\n             'detect_chans': detections[i].no_chans,\n             'horizontal_chans': horizontal_chans,\n             'vertical_chans': vertical_chans})\n                   for i in range(len(detection_streams))]\n        pool.close()\n        try:\n            events_list = [p.get() for p in results]\n        except KeyboardInterrupt as e:  # pragma: no cover\n            pool.terminate()\n            raise e\n        pool.join()\n        events_list.sort(key=lambda tup: tup[0])  # Sort based on index.\n    else:\n        events_list = []\n        for i in range(len(detection_streams)):\n            events_list.append(_channel_loop(\n                detection=detection_streams[i], template=template,\n                min_cc=min_cc, detection_id=detections[i].id,\n                interpolate=interpolate, i=i,\n                pre_lag_ccsum=detections[i].detect_val,\n                detect_chans=detections[i].no_chans,\n                horizontal_chans=horizontal_chans,\n                vertical_chans=vertical_chans, debug=debug))\n    temp_catalog = Catalog()\n    temp_catalog.events = [event_tup[1] for event_tup in events_list]\n    return temp_catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares data for lag_calc - reduce memory here.", "response": "def _prepare_data(detect_data, detections, template, delays,\n                  shift_len, plot):\n    \"\"\"\n    Prepare data for lag_calc - reduce memory here.\n\n    :type detect_data: obspy.core.stream.Stream\n    :param detect_data: Stream to extract detection streams from.\n    :type detections: list\n    :param detections:\n        List of :class:`eqcorrscan.core.match_filter.Detection` to get\n        data for.\n    :type template: tuple\n    :param template: tuple of (template_name, template)\n    :type delays: list\n    :param delays:\n        Dictionary of delay times in seconds keyed by sta.channel.\n    :type shift_len: float\n    :param shift_len: Shift length in seconds allowed for picking.\n    :type plot: bool\n    :param plot:\n        Whether to plot the data extracted or not, used for debugging.\n\n    :returns: List of detect_streams to be worked on\n    :rtype: list\n    \"\"\"\n    detect_streams = []\n    for detection in detections:\n        if detection.template_name != template[0]:\n            continue\n        # Stream to be saved for new detection\n        detect_stream = []\n        max_delay = 0\n        for tr in detect_data:\n            template_tr = template[1].select(\n                station=tr.stats.station, channel=tr.stats.channel)\n            if len(template_tr) >= 1:\n                # Save template trace length in seconds\n                template_len = (\n                    len(template_tr[0]) / template_tr[0].stats.sampling_rate)\n            else:\n                continue\n                # If there is no template-data match then skip the rest\n                # of the trace loop.\n            # Grab the delays for the desired template: [(sta, chan, delay)]\n            # Now grab the delay for the desired trace for this template\n            delay = delays[tr.stats.station + '.' + tr.stats.channel]\n            if delay > max_delay:\n                max_delay = delay\n            detect_stream.append(tr.slice(\n                starttime=detection.detect_time - shift_len + delay,\n                endtime=detection.detect_time + delay + shift_len +\n                template_len).copy())\n        for tr in detect_stream:\n            if len(tr.data) == 0:\n                msg = ('No data in %s.%s for detection at time %s' %\n                       (tr.stats.station, tr.stats.channel,\n                        detection.detect_time))\n                warnings.warn(msg)\n                detect_stream.remove(tr)\n            elif tr.stats.endtime - tr.stats.starttime < (\n                        2 * shift_len) + template_len:\n                msg = (\"Insufficient data for %s.%s will not use.\"\n                       % (tr.stats.station, tr.stats.channel))\n                warnings.warn(msg)\n                detect_stream.remove(tr)\n            elif np.ma.is_masked(tr.data):\n                msg = (\"Masked data found for %s.%s, will not use.\"\n                       % (tr.stats.station, tr.stats.channel))\n                warnings.warn(msg)\n                detect_stream.remove(tr)\n        # Check for duplicate traces\n        stachans = [(tr.stats.station, tr.stats.channel)\n                    for tr in detect_stream]\n        c_stachans = Counter(stachans)\n        for key in c_stachans.keys():\n            if c_stachans[key] > 1:\n                msg = ('Multiple channels for %s.%s, likely a data issue'\n                       % (key[0], key[1]))\n                raise LagCalcError(msg)\n        if plot:\n            background = detect_data.slice(\n                starttime=detection.detect_time - (shift_len + 5),\n                endtime=detection.detect_time +\n                shift_len + max_delay + 7).copy()\n            for tr in background:\n                if len(tr.data) == 0:\n                    background.remove(tr)\n            detection_multiplot(\n                stream=background, template=Stream(detect_stream),\n                times=[detection.detect_time - shift_len],\n                title='Detection Extracted')\n        if not len(detect_stream) == 0:\n            detect_stream = Stream(detect_stream).split()\n            # Make sure there are no masks left over.\n            # Create tuple of (template name, data stream)\n            detect_streams.append((detection.template_name,\n                                   Stream(detect_stream)))\n    return detect_streams"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lag_calc(detections, detect_data, template_names, templates,\n             shift_len=0.2, min_cc=0.4, horizontal_chans=['E', 'N', '1', '2'],\n             vertical_chans=['Z'], cores=1, interpolate=False,\n             plot=False, parallel=True, debug=0):\n    \"\"\"\n    Main lag-calculation function for detections of specific events.\n\n    Overseer function to take a list of detection objects, cut the data for\n    them to lengths of the same length of the template + shift_len on\n    either side. This will output a :class:`obspy.core.event.Catalog` of\n    picked events. Pick times are based on the lag-times found at the maximum\n    correlation, providing that correlation is above the min_cc.\n\n    :type detections: list\n    :param detections:\n        List of :class:`eqcorrscan.core.match_filter.Detection` objects.\n    :type detect_data: obspy.core.stream.Stream\n    :param detect_data:\n        All the data needed to cut from - can be a gappy Stream.\n    :type template_names: list\n    :param template_names:\n        List of the template names, used to help identify families of events.\n        Must be in the same order as templates.\n    :type templates: list\n    :param templates:\n        List of the templates, templates must be a list of\n         :class:`obspy.core.stream.Stream` objects.\n    :type shift_len: float\n    :param shift_len:\n        Shift length allowed for the pick in seconds, will be plus/minus this\n        amount - default=0.2\n    :type min_cc: float\n    :param min_cc:\n        Minimum cross-correlation value to be considered a pick, default=0.4.\n    :type horizontal_chans: list\n    :param horizontal_chans:\n        List of channel endings for horizontal-channels, on which S-picks will\n        be made.\n    :type vertical_chans: list\n    :param vertical_chans:\n        List of channel endings for vertical-channels, on which P-picks will\n        be made.\n    :type cores: int\n    :param cores:\n        Number of cores to use in parallel processing, defaults to one.\n    :type interpolate: bool\n    :param interpolate:\n        Interpolate the correlation function to achieve sub-sample precision.\n    :type plot: bool\n    :param plot:\n        To generate a plot for every detection or not, defaults to False\n    :type parallel: bool\n    :param parallel: Turn parallel processing on or off.\n    :type debug: int\n    :param debug: Debug output level, 0-5 with 5 being the most output.\n\n\n    :returns:\n        Catalog of events with picks.  No origin information is included.\n        These events can then be written out via\n        :func:`obspy.core.event.Catalog.write`, or to Nordic Sfiles using\n        :func:`eqcorrscan.utils.sfile_util.eventtosfile` and located\n        externally.\n    :rtype: obspy.core.event.Catalog\n\n    .. note::\n        Picks output in catalog are generated relative to the template\n        start-time.  For example, if you generated your template with a\n        pre_pick time of 0.2 seconds, you should expect picks generated by\n        lag_calc to occur 0.2 seconds before the true phase-pick.  This\n        is because we do not currently store template meta-data alongside the\n        templates.\n\n    .. warning::\n        Because of the above note, origin times will be consistently\n        shifted by the static pre_pick applied to the templates.\n\n    .. warning::\n        This routine requires only one template per channel (e.g. you should\n        not use templates with a P and S template on a single channel).  If\n        this does occur an error will be raised.\n\n    .. note::\n        S-picks will be made on horizontal channels, and P picks made on\n        vertical channels - the default is that horizontal channels end in\n        one of: 'E', 'N', '1' or '2', and that vertical channels end in 'Z'.\n        The options vertical_chans and horizontal_chans can be changed to suit\n        your dataset.\n\n    .. note::\n        Individual channel cross-correlations are stored as a\n        :class:`obspy.core.event.Comment` for each pick, and the summed\n        cross-correlation value resulting from these is stored as a\n        :class:`obspy.core.event.Comment` in the main\n        :class:`obspy.core.event.Event` object.\n\n    .. note::\n        The order of events is preserved (e.g. detections[n] == output[n]),\n        providing picks have been made for that event.  If no picks have\n        been made for an event, it will not be included in the output.\n        However, as each detection has an ID associated with it, these can\n        be mapped to the output resource_id for each Event in the output\n        Catalog. e.g.\n\n            detections[n].id == output[m].resource_id\n\n        if the output[m] is for the same event as detections[n].\n    \"\"\"\n    if debug > 2 and plot:\n        prep_plot = True\n    else:\n        prep_plot = False\n    # First check that sample rates are equal for everything\n    for tr in detect_data:\n        if tr.stats.sampling_rate != detect_data[0].stats.sampling_rate:\n            raise LagCalcError('Sampling rates are not equal')\n    for template in templates:\n        for tr in template:\n            if tr.stats.sampling_rate != detect_data[0].stats.sampling_rate:\n                raise LagCalcError('Sampling rates are not equal')\n    # Work out the delays for each template\n    delays = []  # List of tuples of (tempname, (sta, chan, delay))\n    zipped_templates = list(zip(template_names, templates))\n    detect_stachans = [(tr.stats.station, tr.stats.channel)\n                       for tr in detect_data]\n    for template in zipped_templates:\n        temp_delays = {}\n        # Remove channels not present in continuous data\n        _template = template[1].copy()\n        for tr in _template:\n            if (tr.stats.station, tr.stats.channel) not in detect_stachans:\n                _template.remove(tr)\n        for tr in _template:\n            temp_delays.update(\n                {tr.stats.station + '.' + tr.stats.channel:\n                 tr.stats.starttime -\n                 _template.sort(['starttime'])[0].stats.starttime})\n        delays.append((template[0], temp_delays))\n        del _template\n    # Segregate detections by template, then feed to day_loop\n    initial_cat = Catalog()\n    for template in zipped_templates:\n        print('Running lag-calc for template %s' % template[0])\n        template_detections = [detection for detection in detections\n                               if detection.template_name == template[0]]\n        t_delays = [d for d in delays if d[0] == template[0]][0][1]\n        # Check template-channels against triggered detection-channels. If the\n        # detection was made without template-channels that would have trig-\n        # gered earlier, then adjust the detection by that delay/earliness.\n        delaylist = list(t_delays.items())\n        delaylist.sort(key=lambda tup: tup[1])\n        for detection in template_detections:\n            # Find the channel with smallest delay on which the detection\n            # triggered. Use that delay to reduce the detection-time.\n            detection_stachans = list()\n            for stachan in detection.chans:\n                detection_stachans.append(stachan[0] + '.' + stachan[1])\n            # Find the earliest template-channel that triggered at detection\n            earlier = 0\n            for delay in delaylist:\n                delay_stachan = delay[0]\n                if delay_stachan in detection_stachans:\n                    earlier = delay[1]\n                    break\n            detection.detect_time = detection.detect_time - earlier\n            if earlier > 0:\n                print('Adjusting ' + detection.id + ' by ' + str(earlier))\n        debug_print(\n            'There are %i detections' % len(template_detections), 2, debug)\n        detect_streams = _prepare_data(\n            detect_data=detect_data, detections=template_detections,\n            template=template, delays=t_delays, shift_len=shift_len,\n            plot=prep_plot)\n        detect_streams = [detect_stream[1] for detect_stream in detect_streams]\n        if len(template_detections) > 0:\n            template_cat = _day_loop(\n                detection_streams=detect_streams, template=template[1],\n                min_cc=min_cc, detections=template_detections,\n                horizontal_chans=horizontal_chans,\n                vertical_chans=vertical_chans, interpolate=interpolate,\n                cores=cores, parallel=parallel, debug=debug)\n            initial_cat += template_cat\n            if plot:\n                for i, event in enumerate(template_cat):\n                    if len(event.picks) == 0:\n                        continue\n                    plot_stream = detect_streams[i].copy()\n                    template_plot = template[1].copy()\n                    pick_stachans = [(pick.waveform_id.station_code,\n                                      pick.waveform_id.channel_code)\n                                     for pick in event.picks]\n                    for tr in plot_stream:\n                        if (tr.stats.station, tr.stats.channel) \\\n                                not in pick_stachans:\n                            plot_stream.remove(tr)\n                    for tr in template_plot:\n                        if (tr.stats.station, tr.stats.channel) \\\n                                not in pick_stachans:\n                            template_plot.remove(tr)\n                    plot_repicked(template=template_plot, picks=event.picks,\n                                  det_stream=plot_stream)\n    # Order the catalogue to match the input\n    output_cat = Catalog()\n    for det in detections:\n        event = [e for e in initial_cat if str(e.resource_id) == str(det.id)]\n        if len(event) == 1:\n            output_cat.append(event[0])\n        elif len(event) == 0:\n            print('No picks made for detection: \\n%s' % det.__str__())\n        else:\n            raise NotImplementedError('Multiple events with same id,'\n                                      ' should not happen')\n    return output_cat", "response": "This function is used to calculate the lag - times of a specific event system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_tutorial(min_magnitude=2, shift_len=0.2, num_cores=4, min_cc=0.5):\n    if num_cores > cpu_count():\n        num_cores = cpu_count()\n    client = Client('NCEDC')\n    t1 = UTCDateTime(2004, 9, 28)\n    t2 = t1 + 86400\n    print('Downloading catalog')\n    catalog = client.get_events(\n        starttime=t1, endtime=t2, minmagnitude=min_magnitude,\n        minlatitude=35.7, maxlatitude=36.1, minlongitude=-120.6,\n        maxlongitude=-120.2, includearrivals=True)\n    # We don't need all the picks, lets take the information from the\n    # five most used stations - note that this is done to reduce computational\n    # costs.\n    catalog = catalog_utils.filter_picks(\n        catalog, channels=['EHZ'], top_n_picks=5)\n    # There is a duplicate pick in event 3 in the catalog - this has the effect\n    # of reducing our detections - check it yourself.\n    for pick in catalog[3].picks:\n        if pick.waveform_id.station_code == 'PHOB' and \\\n                        pick.onset == 'emergent':\n            catalog[3].picks.remove(pick)\n    print('Generating templates')\n    templates = template_gen.from_client(\n        catalog=catalog, client_id='NCEDC', lowcut=2.0, highcut=9.0,\n        samp_rate=50.0, filt_order=4, length=3.0, prepick=0.15,\n        swin='all', process_len=3600)\n    # In this section we generate a series of chunks of data.\n    start_time = UTCDateTime(2004, 9, 28, 17)\n    end_time = UTCDateTime(2004, 9, 28, 20)\n    process_len = 3600\n    chunks = []\n    chunk_start = start_time\n    while chunk_start < end_time:\n        chunk_end = chunk_start + process_len\n        if chunk_end > end_time:\n            chunk_end = end_time\n        chunks.append((chunk_start, chunk_end))\n        chunk_start += process_len\n\n    all_detections = []\n    picked_catalog = Catalog()\n    template_names = [str(template[0].stats.starttime)\n                      for template in templates]\n    for t1, t2 in chunks:\n        print('Downloading and processing for start-time: %s' % t1)\n        # Download and process the data\n        bulk_info = [(tr.stats.network, tr.stats.station, '*',\n                      tr.stats.channel, t1, t2) for tr in templates[0]]\n        # Just downloading a chunk of data\n        st = client.get_waveforms_bulk(bulk_info)\n        st.merge(fill_value='interpolate')\n        st = pre_processing.shortproc(\n            st, lowcut=2.0, highcut=9.0, filt_order=4, samp_rate=50.0,\n            debug=0, num_cores=num_cores)\n        detections = match_filter.match_filter(\n            template_names=template_names, template_list=templates, st=st,\n            threshold=8.0, threshold_type='MAD', trig_int=6.0, plotvar=False,\n            plotdir='.', cores=num_cores)\n        # Extract unique detections from set.\n        unique_detections = []\n        for master in detections:\n            keep = True\n            for slave in detections:\n                if not master == slave and\\\n                   abs(master.detect_time - slave.detect_time) <= 1.0:\n                    # If the events are within 1s of each other then test which\n                    # was the 'best' match, strongest detection\n                    if not master.detect_val > slave.detect_val:\n                        keep = False\n                        break\n            if keep:\n                unique_detections.append(master)\n        all_detections += unique_detections\n\n        picked_catalog += lag_calc.lag_calc(\n            detections=unique_detections, detect_data=st,\n            template_names=template_names, templates=templates,\n            shift_len=shift_len, min_cc=min_cc, interpolate=False, plot=False,\n            parallel=True, debug=3)\n    # Return all of this so that we can use this function for testing.\n    return all_detections, picked_catalog, templates, template_names", "response": "Functional tested example script for running the lag - calc tutorial."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_trigger_parameters(filename):\n    parameters = []\n    f = open(filename, 'r')\n    print('Reading parameters with the following header:')\n    for line in f:\n        if line[0] == '#':\n            print(line.rstrip('\\n').lstrip('\\n'))\n        else:\n            parameter_dict = ast.literal_eval(line)\n            # convert the dictionary to the class\n            trig_par = TriggerParameters(parameter_dict)\n            parameters.append(trig_par)\n    f.close()\n    return parameters", "response": "Read the trigger parameters into a list of trigger_parameter classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _channel_loop(tr, parameters, max_trigger_length=60,\n                  despike=False, debug=0):\n    \"\"\"\n    Internal loop for parellel processing.\n\n    :type tr: obspy.core.trace\n    :param tr: Trace to look for triggers in.\n    :type parameters: list\n    :param parameters: List of TriggerParameter class for trace.\n    :type max_trigger_length: float\n    :type despike: bool\n    :type debug: int\n\n    :return: trigger\n    :rtype: list\n    \"\"\"\n    for par in parameters:\n        if par['station'] == tr.stats.station and \\\n           par['channel'] == tr.stats.channel:\n            parameter = par\n            break\n    else:\n        msg = 'No parameters set for station ' + str(tr.stats.station)\n        warnings.warn(msg)\n        return []\n\n    triggers = []\n    if debug > 0:\n        print(tr)\n    tr.detrend('simple')\n    if despike:\n        median_filter(tr)\n    if parameter['lowcut'] and parameter['highcut']:\n        tr.filter('bandpass', freqmin=parameter['lowcut'],\n                  freqmax=parameter['highcut'])\n    elif parameter['lowcut']:\n        tr.filter('highpass', freq=parameter['lowcut'])\n    elif parameter['highcut']:\n        tr.filter('lowpass', freq=parameter['highcut'])\n    # find triggers for each channel using recursive_sta_lta\n    df = tr.stats.sampling_rate\n    cft = recursive_sta_lta(tr.data, int(parameter['sta_len'] * df),\n                            int(parameter['lta_len'] * df))\n    if max_trigger_length:\n        trig_args = {'max_len_delete': True}\n        trig_args['max_len'] = int(max_trigger_length *\n                                   df + 0.5)\n    if debug > 3:\n        plot_trigger(tr, cft, parameter['thr_on'], parameter['thr_off'])\n    tmp_trigs = trigger_onset(cft, float(parameter['thr_on']),\n                              float(parameter['thr_off']),\n                              **trig_args)\n    for on, off in tmp_trigs:\n        cft_peak = tr.data[on:off].max()\n        cft_std = tr.data[on:off].std()\n        on = tr.stats.starttime + \\\n            float(on) / tr.stats.sampling_rate\n        off = tr.stats.starttime + \\\n            float(off) / tr.stats.sampling_rate\n        triggers.append((on.timestamp, off.timestamp,\n                         tr.id, cft_peak,\n                         cft_std))\n    return triggers", "response": "Internal loop for the channel loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef network_trigger(st, parameters, thr_coincidence_sum, moveout,\n                    max_trigger_length=60, despike=True, debug=0):\n    \"\"\"\n    Main function to compute triggers for a network of stations.\n    Computes single-channel characteristic functions using given parameters,\n    then combines these to find network triggers on a number of stations\n    within a set moveout window.\n\n    :type st: obspy.core.stream.Stream\n    :param st: Stream to compute detections within\n    :type parameters: list\n    :param parameters: List of parameter class\n    :type thr_coincidence_sum: int\n    :param thr_coincidence_sum:\n        Minimum number of stations required to raise a network trigger.\n    :type moveout: float\n    :param moveout: Window to find triggers within in the network detection \\\n        stage.\n    :type max_trigger_length: float\n    :param max_trigger_length: Maximum trigger length in seconds, used to \\\n        remove long triggers - can set to False to not use.\n    :type despike: bool\n    :param despike: Whether to apply simple despiking routine or not\n    :type debug: int\n    :param debug: Debug output level, higher is more output.\n\n    :returns: List of triggers\n    :rtype: list\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> from eqcorrscan.utils.trigger import TriggerParameters, network_trigger\n    >>> st = read(\"https://examples.obspy.org/\" +\n    ...           \"example_local_earthquake_3stations.mseed\")\n    >>> parameters = []\n    >>> for tr in st:\n    ...    parameters.append(TriggerParameters({'station': tr.stats.station,\n    ...                                          'channel': tr.stats.channel,\n    ...                                          'sta_len': 0.5,\n    ...                                          'lta_len': 10.0,\n    ...                                          'thr_on': 10.0,\n    ...                                          'thr_off': 3.0,\n    ...                                          'lowcut': 2.0,\n    ...                                          'highcut': 15.0}))\n    >>> triggers = network_trigger(st=st, parameters=parameters,\n    ...                            thr_coincidence_sum=5, moveout=30,\n    ...                            max_trigger_length=60, despike=False)\n    Looking for coincidence triggers ...\n    Found 1 Coincidence triggers\n    \"\"\"\n    triggers = []\n    trace_ids = [tr.id for tr in st]\n    trace_ids = dict.fromkeys(trace_ids, 1)\n    if debug > 3:\n        print('Not running in parallel')\n        # Don't run in parallel\n        for tr in st:\n            triggers += _channel_loop(tr=tr, parameters=parameters,\n                                      max_trigger_length=max_trigger_length,\n                                      despike=despike, debug=debug)\n    else:\n        # Needs to be pickleable\n        parameters = [par.__dict__ for par in parameters]\n        pool = Pool(processes=cpu_count())\n        results = [pool.apply_async(_channel_loop,\n                                    args=(tr, parameters, max_trigger_length,\n                                          despike, debug))\n                   for tr in st]\n        pool.close()\n        triggers = [p.get() for p in results]\n        pool.join()\n        triggers = [item for sublist in triggers for item in sublist]\n    triggers.sort()\n\n    if debug > 0:\n        details = True\n    else:\n        details = False\n\n    print('Looking for coincidence triggers ...')\n    # the coincidence triggering and coincidence sum computation\n    coincidence_triggers = []\n    last_off_time = 0.0\n    while triggers:\n        # remove first trigger from list and look for overlaps\n        on, off, tr_id, cft_peak, cft_std = triggers.pop(0)\n        event = {}\n        event['time'] = UTCDateTime(on)\n        event['stations'] = [tr_id.split(\".\")[1]]\n        event['trace_ids'] = [tr_id]\n        event['coincidence_sum'] = trace_ids[tr_id]\n        if details:\n            event['cft_peaks'] = [cft_peak]\n            event['cft_stds'] = [cft_std]\n        # compile the list of stations that overlap with the current trigger\n        for trigger in triggers:\n            tmp_on, tmp_off, tmp_tr_id, tmp_cft_peak, tmp_cft_std = trigger\n            # skip retriggering of already present station in current\n            # coincidence trigger\n            if tmp_tr_id in event['trace_ids']:\n                continue\n            # check for overlapping trigger\n            if tmp_on <= off + moveout:\n                event['stations'].append(tmp_tr_id.split(\".\")[1])\n                event['trace_ids'].append(tmp_tr_id)\n                event['coincidence_sum'] += trace_ids[tmp_tr_id]\n                if details:\n                    event['cft_peaks'].append(tmp_cft_peak)\n                    event['cft_stds'].append(tmp_cft_std)\n                # allow sets of triggers that overlap only on subsets of all\n                # stations (e.g. A overlaps with B and B overlaps w/ C => ABC)\n                off = max(off, tmp_off)\n            # break if there is a gap in between the two triggers\n            else:\n                break\n        # skip if coincidence sum threshold is not met\n        if event['coincidence_sum'] < thr_coincidence_sum:\n            continue\n        # skip coincidence trigger if it is just a subset of the previous\n        # (determined by a shared off-time, this is a bit sloppy)\n        if off == last_off_time:\n            continue\n        event['duration'] = off - on\n        if details:\n            weights = np.array([trace_ids[i] for i in event['trace_ids']])\n            weighted_values = np.array(event['cft_peaks']) * weights\n            event['cft_peak_wmean'] = weighted_values.sum() / weights.sum()\n            weighted_values = np.array(event['cft_stds']) * weights\n            event['cft_std_wmean'] = \\\n                (np.array(event['cft_stds']) * weights).sum() / weights.sum()\n        coincidence_triggers.append(event)\n        last_off_time = off\n\n    if debug > 1:\n        print('Coincidence triggers :')\n        pprint(coincidence_triggers)\n\n    print('Found %s Coincidence triggers' % len(coincidence_triggers))\n    return coincidence_triggers", "response": "This function is used to compute the network triggers for a given set of stations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, filename, append=True):\n        header = ' '.join(['# User:', getpass.getuser(),\n                           '\\n# Creation date:', str(UTCDateTime()),\n                           '\\n# EQcorrscan version:',\n                           str(eqcorrscan.__version__),\n                           '\\n\\n\\n'])\n        if append:\n            f = open(filename, 'a')\n        else:\n            f = open(filename, 'w')\n            f.write(header)\n        parameters = self.__dict__\n        f.write(str(parameters))\n        f.write('\\n')\n        f.close()\n        return", "response": "Write the parameters to a file as a human - readable series of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_cdll(name):\n    # our custom defined part of the extension file name\n    libname = _get_lib_name(name)\n    libdir = os.path.join(os.path.dirname(__file__), 'lib')\n    libpath = os.path.join(libdir, libname)\n    static_fftw = os.path.join(libdir, 'libfftw3-3.dll')\n    static_fftwf = os.path.join(libdir, 'libfftw3f-3.dll')\n    try:\n        fftw_lib = ctypes.CDLL(str(static_fftw))  # noqa: F841\n        fftwf_lib = ctypes.CDLL(str(static_fftwf))  # noqa: F841\n    except:\n        pass\n    try:\n        cdll = ctypes.CDLL(str(libpath))\n    except Exception as e:\n        msg = 'Could not load shared library \"%s\".\\n\\n %s' % (libname, str(e))\n        raise ImportError(msg)\n    return cdll", "response": "Helper function to load a shared library built during installation\n    with ctypes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cross_net(stream, env=False, debug=0, master=False):\n    event = Event()\n    event.origins.append(Origin())\n    event.creation_info = CreationInfo(author='EQcorrscan',\n                                       creation_time=UTCDateTime())\n    event.comments.append(Comment(text='cross_net'))\n    samp_rate = stream[0].stats.sampling_rate\n    if not env:\n        if debug > 2:\n            print('Using the raw data')\n        st = stream.copy()\n        st.resample(samp_rate)\n    else:\n        st = stream.copy()\n        if debug > 2:\n            print('Computing envelope')\n        for tr in st:\n            tr.resample(samp_rate)\n            tr.data = envelope(tr.data)\n    if not master:\n        master = st[0]\n    else:\n        master = master\n    master.data = np.nan_to_num(master.data)\n    for i, tr in enumerate(st):\n        tr.data = np.nan_to_num(tr.data)\n        if debug > 2:\n            msg = ' '.join(['Comparing', tr.stats.station, tr.stats.channel,\n                            'with the master'])\n            print(msg)\n        shift_len = int(0.3 * len(tr))\n        if debug > 2:\n            print('Shift length is set to ' + str(shift_len) + ' samples')\n        index, cc = xcorr(master, tr, shift_len)\n        wav_id = WaveformStreamID(station_code=tr.stats.station,\n                                  channel_code=tr.stats.channel,\n                                  network_code=tr.stats.network)\n        event.picks.append(Pick(time=tr.stats.starttime +\n                                (index / tr.stats.sampling_rate),\n                                waveform_id=wav_id,\n                                phase_hint='S',\n                                onset='emergent'))\n        if debug > 2:\n            print(event.picks[i])\n    event.origins[0].time = min([pick.time for pick in event.picks]) - 1\n    # event.origins[0].latitude = float('nan')\n    # event.origins[0].longitude = float('nan')\n    # Set arbitrary origin time\n    del st\n    return event", "response": "Generates a picks based on a simple envelope cross - correlation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stalta_pick(stream, stalen, ltalen, trig_on, trig_off, freqmin=False,\n                freqmax=False, debug=0, show=False):\n    \"\"\"\n    Basic sta/lta picker, suggest using alternative in obspy.\n\n    Simple sta/lta (short-term average/long-term average) picker, using\n    obspy's :func:`obspy.signal.trigger.classic_sta_lta` routine to generate\n    the characteristic function.\n\n    Currently very basic quick wrapper, there are many other (better) options\n    in obspy in the :mod:`obspy.signal.trigger` module.\n\n    :type stream: obspy.core.stream.Stream\n    :param stream: The stream to pick on, can be any number of channels.\n    :type stalen: float\n    :param stalen: Length of the short-term average window in seconds.\n    :type ltalen: float\n    :param ltalen: Length of the long-term average window in seconds.\n    :type trig_on: float\n    :param trig_on: sta/lta ratio to trigger a detection/pick\n    :type trig_off: float\n    :param trig_off: sta/lta ratio to turn the trigger off - no further picks\\\n        will be made between exceeding trig_on until trig_off is reached.\n    :type freqmin: float\n    :param freqmin: Low-cut frequency in Hz for bandpass filter\n    :type freqmax: float\n    :param freqmax: High-cut frequency in Hz for bandpass filter\n    :type debug: int\n    :param debug: Debug output level from 0-5.\n    :type show: bool\n    :param show: Show picks on waveform.\n\n    :returns: :class:`obspy.core.event.event.Event`\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> from eqcorrscan.utils.picker import stalta_pick\n    >>> st = read()\n    >>> event = stalta_pick(st, stalen=0.2, ltalen=4, trig_on=10,\n    ...             trig_off=1, freqmin=3.0, freqmax=20.0)\n    >>> print(event.creation_info.author)\n    EQcorrscan\n\n    .. warning::\n        This function is not designed for accurate picking, rather it can give\n        a first idea of whether picks may be possible.  Proceed with caution.\n    \"\"\"\n    event = Event()\n    event.origins.append(Origin())\n    event.creation_info = CreationInfo(author='EQcorrscan',\n                                       creation_time=UTCDateTime())\n    event.comments.append(Comment(text='stalta'))\n    picks = []\n    for tr in stream:\n        # We are going to assume, for now, that if the pick is made on the\n        # horizontal channel then it is an S, otherwise we will assume it is\n        # a P-phase: obviously a bad assumption...\n        if tr.stats.channel[-1] == 'Z':\n            phase = 'P'\n        else:\n            phase = 'S'\n        if freqmin and freqmax:\n            tr.detrend('simple')\n            tr.filter('bandpass', freqmin=freqmin, freqmax=freqmax,\n                      corners=3, zerophase=True)\n        df = tr.stats.sampling_rate\n        cft = classic_sta_lta(tr.data, int(stalen * df), int(ltalen * df))\n        if debug > 3:\n            plot_trigger(tr, cft, trig_on, trig_off)\n        triggers = trigger_onset(cft, trig_on, trig_off)\n        for trigger in triggers:\n            on = tr.stats.starttime + (trigger[0] / df)\n            # off = tr.stats.starttime + (trigger[1] / df)\n            wav_id = WaveformStreamID(station_code=tr.stats.station,\n                                      channel_code=tr.stats.channel,\n                                      network_code=tr.stats.network)\n            p = Pick(waveform_id=wav_id, phase_hint=phase, time=on)\n            if debug > 2:\n                print('Pick made:')\n                print(p)\n            picks.append(p)\n    # QC picks\n    pick_stations = list(set([pick.waveform_id.station_code\n                              for pick in picks]))\n    for pick_station in pick_stations:\n        station_picks = [pick for pick in picks if\n                         pick.waveform_id.station_code == pick_station]\n        # If P-pick is after S-picks, remove it.\n        p_time = [pick.time for pick in station_picks\n                  if pick.phase_hint == 'P']\n        s_time = [pick.time for pick in station_picks\n                  if pick.phase_hint == 'S']\n        if p_time > s_time:\n            p_pick = [pick for pick in station_picks if pick.phase_hint == 'P']\n            for pick in p_pick:\n                print('P pick after S pick, removing P pick')\n                picks.remove(pick)\n    if show:\n        plotting.pretty_template_plot(stream, picks=picks, title='Autopicks',\n                                      size=(8, 9))\n    event.picks = picks\n    if len(event.picks) > 0:\n        event.origins[0].time = min([pick.time for pick in event.picks]) - 1\n        # event.origins[0].latitude = float('nan')\n        # event.origins[0].longitude = float('nan')\n    # Set arbitrary origin time\n    return event", "response": "This function generates a new object for the given stream and picks the next time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cross_chan_coherence(st1, st2, allow_shift=False, shift_len=0.2, i=0,\n                         xcorr_func='time_domain'):\n    \"\"\"\n    Calculate cross-channel coherency.\n\n    Determine the cross-channel coherency between two streams of multichannel\n    seismic data.\n\n    :type st1: obspy.core.stream.Stream\n    :param st1: Stream one\n    :type st2: obspy.core.stream.Stream\n    :param st2: Stream two\n    :type allow_shift: bool\n    :param allow_shift:\n        Whether to allow the optimum alignment to be found for coherence,\n        defaults to `False` for strict coherence\n    :type shift_len: float\n    :param shift_len: Seconds to shift, only used if `allow_shift=True`\n    :type i: int\n    :param i: index used for parallel async processing, returned unaltered\n    :type xcorr_func: str, callable\n    :param xcorr_func:\n        The method for performing correlations. Accepts either a string or\n         callabe. See :func:`eqcorrscan.utils.correlate.register_array_xcorr`\n         for more details\n\n    :returns:\n        cross channel coherence, float - normalized by number of channels,\n        and i, where i is int, as input.\n    :rtype: tuple\n    \"\"\"\n    cccoh = 0.0\n    kchan = 0\n    array_xcorr = get_array_xcorr(xcorr_func)\n    for tr in st1:\n        tr2 = st2.select(station=tr.stats.station,\n                         channel=tr.stats.channel)\n        if len(tr2) > 0 and tr.stats.sampling_rate != \\\n                tr2[0].stats.sampling_rate:\n            warnings.warn('Sampling rates do not match, not using: %s.%s'\n                          % (tr.stats.station, tr.stats.channel))\n        if len(tr2) > 0 and allow_shift:\n            index, corval = xcorr(tr, tr2[0],\n                                  int(shift_len * tr.stats.sampling_rate))\n            cccoh += corval\n            kchan += 1\n        elif len(tr2) > 0:\n            min_len = min(len(tr.data), len(tr2[0].data))\n            cccoh += array_xcorr(\n                np.array([tr.data[0:min_len]]), tr2[0].data[0:min_len],\n                [0])[0][0][0]\n            kchan += 1\n    if kchan:\n        cccoh /= kchan\n        return np.round(cccoh, 6), i\n    else:\n        warnings.warn('No matching channels')\n        return 0, i", "response": "Calculate the cross - channel coherence between two streams."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the distance matrix for all waveforms in a list of templates.", "response": "def distance_matrix(stream_list, allow_shift=False, shift_len=0, cores=1):\n    \"\"\"\n    Compute distance matrix for waveforms based on cross-correlations.\n\n    Function to compute the distance matrix for all templates - will give\n    distance as 1-abs(cccoh), e.g. a well correlated pair of templates will\n    have small distances, and an equally well correlated reverse image will\n    have the same distance as a positively correlated image - this is an issue.\n\n    :type stream_list: list\n    :param stream_list:\n        List of the :class:`obspy.core.stream.Stream` to compute the distance\n        matrix for\n    :type allow_shift: bool\n    :param allow_shift: To allow templates to shift or not?\n    :type shift_len: float\n    :param shift_len: How many seconds for templates to shift\n    :type cores: int\n    :param cores: Number of cores to parallel process using, defaults to 1.\n\n    :returns: distance matrix\n    :rtype: :class:`numpy.ndarray`\n\n    .. warning::\n        Because distance is given as :math:`1-abs(coherence)`, negatively\n        correlated and positively correlated objects are given the same\n        distance.\n    \"\"\"\n    # Initialize square matrix\n    dist_mat = np.array([np.array([0.0] * len(stream_list))] *\n                        len(stream_list))\n    for i, master in enumerate(stream_list):\n        # Start a parallel processing pool\n        pool = Pool(processes=cores)\n        # Parallel processing\n        results = [pool.apply_async(cross_chan_coherence,\n                                    args=(master, stream_list[j], allow_shift,\n                                          shift_len, j))\n                   for j in range(len(stream_list))]\n        pool.close()\n        # Extract the results when they are done\n        dist_list = [p.get() for p in results]\n        # Close and join all the processes back to the master process\n        pool.join()\n        # Sort the results by the input j\n        dist_list.sort(key=lambda tup: tup[1])\n        # Sort the list into the dist_mat structure\n        for j in range(i, len(stream_list)):\n            if i == j:\n                dist_mat[i, j] = 0.0\n            else:\n                dist_mat[i, j] = 1 - dist_list[j][0]\n    # Reshape the distance matrix\n    for i in range(1, len(stream_list)):\n        for j in range(i):\n            dist_mat[i, j] = dist_mat.T[i, j]\n    return dist_mat"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclusters a list of template waveforms.", "response": "def cluster(template_list, show=True, corr_thresh=0.3, allow_shift=False,\n            shift_len=0, save_corrmat=False, cores='all', debug=1):\n    \"\"\"\n    Cluster template waveforms based on average correlations.\n\n    Function to take a set of templates and cluster them, will return groups\n    as lists of streams.  Clustering is done by computing the cross-channel\n    correlation sum of each stream in stream_list with every other stream in\n    the list.  :mod:`scipy.cluster.hierarchy` functions are then used to\n    compute the complete distance matrix, where distance is 1 minus the\n    normalised cross-correlation sum such that larger distances are less\n    similar events.  Groups are then created by clustering the distance matrix\n    at distances less than 1 - corr_thresh.\n\n    Will compute the distance matrix in parallel, using all available cores\n\n    :type template_list: list\n    :param template_list:\n        List of tuples of the template (:class:`obspy.core.stream.Stream`)\n        and the template id to compute clustering for\n    :type show: bool\n    :param show: plot linkage on screen if True, defaults to True\n    :type corr_thresh: float\n    :param corr_thresh: Cross-channel correlation threshold for grouping\n    :type allow_shift: bool\n    :param allow_shift:\n        Whether to allow the templates to shift when correlating\n    :type shift_len: float\n    :param shift_len: How many seconds to allow the templates to shift\n    :type save_corrmat: bool\n    :param save_corrmat:\n        If True will save the distance matrix to dist_mat.npy in the local\n        directory.\n    :type cores: int\n    :param cores:\n        number of cores to use when computing the distance matrix, defaults to\n        'all' which will work out how many cpus are available and hog them.\n    :type debug: int\n    :param debug:\n        Level of debugging from 1-5, higher is more output,\n        currently only level 1 implemented.\n\n    :returns:\n        List of groups. Each group is a list of\n        :class:`obspy.core.stream.Stream` making up that group.\n    \"\"\"\n    if cores == 'all':\n        num_cores = cpu_count()\n    else:\n        num_cores = cores\n    # Extract only the Streams from stream_list\n    stream_list = [x[0] for x in template_list]\n    # Compute the distance matrix\n    if debug >= 1:\n        print('Computing the distance matrix using %i cores' % num_cores)\n    dist_mat = distance_matrix(stream_list, allow_shift, shift_len,\n                               cores=num_cores)\n    if save_corrmat:\n        np.save('dist_mat.npy', dist_mat)\n        if debug >= 1:\n            print('Saved the distance matrix as dist_mat.npy')\n    dist_vec = squareform(dist_mat)\n    if debug >= 1:\n        print('Computing linkage')\n    Z = linkage(dist_vec)\n    if show:\n        if debug >= 1:\n            print('Plotting the dendrogram')\n        dendrogram(Z, color_threshold=1 - corr_thresh,\n                   distance_sort='ascending')\n        plt.show()\n    # Get the indices of the groups\n    if debug >= 1:\n        print('Clustering')\n    indices = fcluster(Z, t=1 - corr_thresh, criterion='distance')\n    # Indices start at 1...\n    group_ids = list(set(indices))  # Unique list of group ids\n    if debug >= 1:\n        msg = ' '.join(['Found', str(len(group_ids)), 'groups'])\n        print(msg)\n    # Convert to tuple of (group id, stream id)\n    indices = [(indices[i], i) for i in range(len(indices))]\n    # Sort by group id\n    indices.sort(key=lambda tup: tup[0])\n    groups = []\n    if debug >= 1:\n        print('Extracting and grouping')\n    for group_id in group_ids:\n        group = []\n        for ind in indices:\n            if ind[0] == group_id:\n                group.append(template_list[ind[1]])\n            elif ind[0] > group_id:\n                # Because we have sorted by group id, when the index is greater\n                # than the group_id we can break the inner loop.\n                # Patch applied by CJC 05/11/2015\n                groups.append(group)\n                break\n    # Catch the final group\n    groups.append(group)\n    return groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group_delays(stream_list):\n    groups = []\n    group_delays = []\n    group_chans = []\n    # Sort templates by number of channels\n    stream_list = [(st, len(st)) for st in stream_list]\n    stream_list.sort(key=lambda tup: tup[1])\n    stream_list = [st[0] for st in stream_list]\n    for i, st in enumerate(stream_list):\n        msg = ' '.join(['Working on waveform', str(i), 'of',\n                        str(len(stream_list))])\n        print(msg)\n        # Calculate the delays\n        starttimes = []\n        chans = []\n        for tr in st:\n            starttimes.append(tr.stats.starttime)\n            chans.append((tr.stats.station, tr.stats.channel))\n        # This delays calculation will be an issue if we\n        # have changes in channels\n        delays = [starttimes[m] - min(starttimes)\n                  for m in range(len(starttimes))]\n        delays = [round(d, 2) for d in delays]\n        if len(groups) == 0:\n            groups.append([stream_list[i]])\n            group_delays.append(delays)\n            group_chans.append(chans)\n        else:\n            j = 0\n            match = False\n            while not match:\n                kmatch = 0\n                # Find the set of shared stations and channels\n                shared_chans = []\n                shared_delays_slave = []\n                shared_delays_master = []\n                for k, chan in enumerate(chans):\n                    if chan in group_chans[j]:\n                        shared_chans.append(chan)\n                        shared_delays_slave.append(delays[k])\n                        shared_delays_master. \\\n                            append(group_delays[j][group_chans[j].index(chan)])\n                # Normalize master and slave delay times\n                shared_delays_slave = [delay - min(shared_delays_slave)\n                                       for delay in shared_delays_slave]\n                shared_delays_master = [delay - min(shared_delays_master)\n                                        for delay in shared_delays_master]\n                for k in range(len(shared_chans)):\n                    # Check if the channel and delay match another group\n                    if shared_delays_slave[k] == shared_delays_master[k]:\n                        kmatch += 1  # increase the match index\n                if kmatch == len(shared_chans):\n                    # If all the channels match, add it to the group\n                    groups[j].append(stream_list[i])\n                    match = True\n                elif j < len(groups) - 1:\n                    j += 1\n                else:\n                    # Create a new group and break the loop\n                    groups.append([stream_list[i]])\n                    group_delays.append(delays)\n                    group_chans.append(chans)\n                    match = True  # Use this to break the loop\n    return groups", "response": "Group waveforms according to their arrival times."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the SVD of a number of templates.", "response": "def svd(stream_list, full=False):\n    \"\"\"\n    Compute the SVD of a number of templates.\n\n    Returns the singular vectors and singular values of the templates.\n\n    :type stream_list: List of :class: obspy.Stream\n    :param stream_list: List of the templates to be analysed\n    :type full: bool\n    :param full: Whether to compute the full input vector matrix or not.\n\n    :return: SValues(list) for each channel, SVectors(list of ndarray),  \\\n        UVectors(list of ndarray) for each channel, \\\n        stachans, List of String (station.channel)\n\n    .. note:: We recommend that you align the data before computing the \\\n        SVD, e.g., the P-arrival on all templates for the same channel \\\n        should appear at the same time in the trace.  See the \\\n        stacking.align_traces function for a way to do this.\n\n    .. note:: Uses the numpy.linalg.svd function, their U, s and V are mapped \\\n        to UVectors, SValues and SVectors respectively.  Their V (and ours) \\\n        corresponds to V.H.\n    \"\"\"\n    # Convert templates into ndarrays for each channel\n    # First find all unique channels:\n    stachans = list(set([(tr.stats.station, tr.stats.channel)\n                         for st in stream_list for tr in st]))\n    stachans.sort()\n    # Initialize a list for the output matrices, one matrix per-channel\n    svalues = []\n    svectors = []\n    uvectors = []\n    for stachan in stachans:\n        lengths = []\n        for st in stream_list:\n            tr = st.select(station=stachan[0],\n                           channel=stachan[1])\n            if len(tr) > 0:\n                tr = tr[0]\n            else:\n                warnings.warn('Stream does not contain %s'\n                              % '.'.join(list(stachan)))\n                continue\n            lengths.append(len(tr.data))\n        min_length = min(lengths)\n        for stream in stream_list:\n            chan = stream.select(station=stachan[0],\n                                 channel=stachan[1])\n            if chan:\n                if len(chan[0].data) > min_length:\n                    if abs(len(chan[0].data) - min_length) > 0.1 * \\\n                            chan[0].stats.sampling_rate:\n                        raise IndexError('More than 0.1 s length '\n                                         'difference, align and fix')\n                    warnings.warn('Channels are not equal length, trimming')\n                    chan[0].data = chan[0].data[0:min_length]\n                if 'chan_mat' not in locals():\n                    chan_mat = chan[0].data\n                else:\n                    chan_mat = np.vstack((chan_mat, chan[0].data))\n        if not len(chan_mat.shape) > 1:\n            warnings.warn('Matrix of traces is less than 2D for %s'\n                          % '.'.join(list(stachan)))\n            continue\n        # Be sure to transpose chan_mat as waveforms must define columns\n        chan_mat = np.asarray(chan_mat)\n        u, s, v = np.linalg.svd(chan_mat.T, full_matrices=full)\n        svalues.append(s)\n        svectors.append(v)\n        uvectors.append(u)\n        del (chan_mat)\n    return uvectors, svalues, svectors, stachans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef empirical_svd(stream_list, linear=True):\n    # Run a check to ensure all traces are the same length\n    stachans = list(set([(tr.stats.station, tr.stats.channel)\n                         for st in stream_list for tr in st]))\n    for stachan in stachans:\n        lengths = []\n        for st in stream_list:\n            lengths.append(len(st.select(station=stachan[0],\n                                         channel=stachan[1])[0]))\n        min_length = min(lengths)\n        for st in stream_list:\n            tr = st.select(station=stachan[0],\n                           channel=stachan[1])[0]\n            if len(tr.data) > min_length:\n                sr = tr.stats.sampling_rate\n                if abs(len(tr.data) - min_length) > (0.1 * sr):\n                    msg = 'More than 0.1 s length difference, align and fix'\n                    raise IndexError(msg)\n                msg = ' is not the same length as others, trimming the end'\n                warnings.warn(str(tr) + msg)\n                tr.data = tr.data[0:min_length]\n    if linear:\n        first_subspace = stacking.linstack(stream_list)\n    else:\n        first_subspace = stacking.PWS_stack(streams=stream_list)\n    second_subspace = first_subspace.copy()\n    for i in range(len(second_subspace)):\n        second_subspace[i].data = np.diff(second_subspace[i].data)\n        delta = second_subspace[i].stats.delta\n        second_subspace[i].stats.starttime += 0.5 * delta\n\n    return [first_subspace, second_subspace]", "response": "This function generates the subspace detectors from a list of templates and computes the differential of this as the first order\n    subspace detector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef svd_to_stream(uvectors, stachans, k, sampling_rate):\n    svstreams = []\n    for i in range(k):\n        svstream = []\n        for j, stachan in enumerate(stachans):\n            if len(uvectors[j]) <= k:\n                warnings.warn('Too few traces at %s for a %02d dimensional '\n                              'subspace. Detector streams will not include '\n                              'this channel.' % ('.'.join(stachan[0],\n                                                          stachan[1]), k))\n            else:\n                svstream.append(Trace(uvectors[j][i],\n                                      header={'station': stachan[0],\n                                              'channel': stachan[1],\n                                              'sampling_rate': sampling_rate}))\n        svstreams.append(Stream(svstream))\n    return svstreams", "response": "Convert the singular vectors output by SVD to streams."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef corr_cluster(trace_list, thresh=0.9):\n    stack = stacking.linstack([Stream(tr) for tr in trace_list])[0]\n    output = np.array([False] * len(trace_list))\n    group1 = []\n    array_xcorr = get_array_xcorr()\n    for i, tr in enumerate(trace_list):\n        if array_xcorr(\n                np.array([tr.data]), stack.data, [0])[0][0][0] > 0.6:\n            output[i] = True\n            group1.append(tr)\n    if not group1:\n        warnings.warn('Nothing made it past the first 0.6 threshold')\n        return output\n    stack = stacking.linstack([Stream(tr) for tr in group1])[0]\n    group2 = []\n    for i, tr in enumerate(trace_list):\n        if array_xcorr(\n                np.array([tr.data]), stack.data, [0])[0][0][0] > thresh:\n            group2.append(tr)\n            output[i] = True\n        else:\n            output[i] = False\n    return output", "response": "Compute similarity between two lists of traces."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting waveforms associated with detections from a list of templates and archive.", "response": "def extract_detections(detections, templates, archive, arc_type,\n                       extract_len=90.0, outdir=None, extract_Z=True,\n                       additional_stations=[]):\n    \"\"\"\n    Extract waveforms associated with detections\n\n    Takes a list of detections for the template, template.  Waveforms will be\n    returned as a list of :class:`obspy.core.stream.Stream` containing\n    segments of extract_len.  They will also be saved if outdir is set.\n    The default is unset.  The  default extract_len is 90 seconds per channel.\n\n    :type detections: list\n    :param detections: List of :class:`eqcorrscan.core.match_filter.Detection`.\n    :type templates: list\n    :param templates:\n        A list of tuples of the template name and the template Stream used\n        to detect detections.\n    :type archive: str\n    :param archive:\n        Either name of archive or path to continuous data, see\n        :func:`eqcorrscan.utils.archive_read` for details\n    :type arc_type: str\n    :param arc_type: Type of archive, either seishub, FDSN, day_vols\n    :type extract_len: float\n    :param extract_len:\n        Length to extract around the detection (will be equally cut around\n        the detection time) in seconds.  Default is 90.0.\n    :type outdir: str\n    :param outdir:\n        Default is None, with None set, no files will be saved,\n        if set each detection will be saved into this directory with files\n        named according to the detection time, NOT than the waveform\n        start time. Detections will be saved into template subdirectories.\n        Files written will be multiplexed miniseed files, the encoding will\n        be chosen automatically and will likely be float.\n    :type extract_Z: bool\n    :param extract_Z:\n        Set to True to also extract Z channels for detections delays will be\n        the same as horizontal channels, only applies if only horizontal\n        channels were used in the template.\n    :type additional_stations: list\n    :param additional_stations:\n        List of tuples of (station, channel) to also extract data\n        for using an average delay.\n\n    :returns: list of :class:`obspy.core.streams.Stream`\n    :rtype: list\n\n    .. rubric: Example\n\n    >>> from eqcorrscan.utils.clustering import extract_detections\n    >>> from eqcorrscan.core.match_filter import Detection\n    >>> from obspy import read, UTCDateTime\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> # Use some dummy detections, you would use real one\n    >>> detections = [Detection(\n    ...     template_name='temp1', detect_time=UTCDateTime(2012, 3, 26, 9, 15),\n    ...     no_chans=2, chans=['WHYM', 'EORO'], detect_val=2, threshold=1.2,\n    ...     typeofdet='corr', threshold_type='MAD', threshold_input=8.0),\n    ...               Detection(\n    ...     template_name='temp2', detect_time=UTCDateTime(2012, 3, 26, 18, 5),\n    ...     no_chans=2, chans=['WHYM', 'EORO'], detect_val=2, threshold=1.2,\n    ...     typeofdet='corr', threshold_type='MAD', threshold_input=8.0)]\n    >>> archive = os.path.join(TEST_PATH, 'day_vols')\n    >>> template_files = [os.path.join(TEST_PATH, 'temp1.ms'),\n    ...                   os.path.join(TEST_PATH, 'temp2.ms')]\n    >>> templates = [('temp' + str(i), read(filename))\n    ...              for i, filename in enumerate(template_files)]\n    >>> extracted = extract_detections(detections, templates,\n    ...                                archive=archive, arc_type='day_vols')\n    Working on detections for day: 2012-03-26T00:00:00.000000Z\n    Cutting for detections at: 2012/03/26 09:15:00\n    Cutting for detections at: 2012/03/26 18:05:00\n    >>> print(extracted[0].sort())\n    2 Trace(s) in Stream:\n    AF.EORO..SHZ | 2012-03-26T09:14:15.000000Z - 2012-03-26T09:15:45.000000Z |\\\n 1.0 Hz, 91 samples\n    AF.WHYM..SHZ | 2012-03-26T09:14:15.000000Z - 2012-03-26T09:15:45.000000Z |\\\n 1.0 Hz, 91 samples\n    >>> print(extracted[1].sort())\n    2 Trace(s) in Stream:\n    AF.EORO..SHZ | 2012-03-26T18:04:15.000000Z - 2012-03-26T18:05:45.000000Z |\\\n 1.0 Hz, 91 samples\n    AF.WHYM..SHZ | 2012-03-26T18:04:15.000000Z - 2012-03-26T18:05:45.000000Z |\\\n 1.0 Hz, 91 samples\n    >>> # Extract from stations not included in the detections\n    >>> extracted = extract_detections(\n    ...    detections, templates, archive=archive, arc_type='day_vols',\n    ...    additional_stations=[('GOVA', 'SHZ')])\n    Adding additional stations\n    Added station GOVA.SHZ\n    Added station GOVA.SHZ\n    Working on detections for day: 2012-03-26T00:00:00.000000Z\n    Cutting for detections at: 2012/03/26 09:15:00\n    Cutting for detections at: 2012/03/26 18:05:00\n    >>> print(extracted[0].sort())\n    3 Trace(s) in Stream:\n    AF.EORO..SHZ | 2012-03-26T09:14:15.000000Z - 2012-03-26T09:15:45.000000Z |\\\n 1.0 Hz, 91 samples\n    AF.GOVA..SHZ | 2012-03-26T09:14:15.000000Z - 2012-03-26T09:15:45.000000Z |\\\n 1.0 Hz, 91 samples\n    AF.WHYM..SHZ | 2012-03-26T09:14:15.000000Z - 2012-03-26T09:15:45.000000Z |\\\n 1.0 Hz, 91 samples\n    >>> # The detections can be saved to a file:\n    >>> extract_detections(detections, templates, archive=archive,\n    ...                    arc_type='day_vols',\n    ...                    additional_stations=[('GOVA', 'SHZ')], outdir='.')\n    Adding additional stations\n    Added station GOVA.SHZ\n    Added station GOVA.SHZ\n    Working on detections for day: 2012-03-26T00:00:00.000000Z\n    Cutting for detections at: 2012/03/26 09:15:00\n    Written file: ./temp1/2012-03-26_09-15-00.ms\n    Cutting for detections at: 2012/03/26 18:05:00\n    Written file: ./temp2/2012-03-26_18-05-00.ms\n    \"\"\"\n    # Sort the template according to start-times, needed so that stachan[i]\n    # corresponds to delays[i]\n    all_delays = []  # List of tuples of template name, delays\n    all_stachans = []\n    for template in templates:\n        templatestream = template[1].sort(['starttime'])\n        stachans = [(tr.stats.station, tr.stats.channel)\n                    for tr in templatestream]\n        mintime = templatestream[0].stats.starttime\n        delays = [tr.stats.starttime - mintime for tr in templatestream]\n        all_delays.append((template[0], delays))\n        all_stachans.append((template[0], stachans))\n    # Sort the detections and group by day\n    detections.sort(key=lambda d: d.detect_time)\n    detection_days = [detection.detect_time.date\n                      for detection in detections]\n    detection_days = list(set(detection_days))\n    detection_days.sort()\n    detection_days = [UTCDateTime(d) for d in detection_days]\n\n    # Initialize output list\n    detection_wavefiles = []\n\n    # Also include Z channels when extracting detections\n    if extract_Z:\n        new_all_stachans = []\n        new_all_delays = []\n        for t, template in enumerate(all_stachans):\n            stachans = template[1]\n            delays = all_delays[t][1]\n            new_stachans = []\n            new_delays = []\n            j = 0\n            for i, stachan in enumerate(stachans):\n                if j == 1:\n                    new_stachans.append((stachan[0], stachan[1][0] + 'Z'))\n                    new_delays.append(delays[i])\n                    new_stachans.append(stachan)\n                    new_delays.append(delays[i])\n                    j = 0\n                else:\n                    new_stachans.append(stachan)\n                    new_delays.append(delays[i])\n                    j += 1\n            new_all_stachans.append((template[0], new_stachans))\n            new_all_delays.append((template[0], new_delays))\n        all_delays = new_all_delays\n        all_stachans = new_all_stachans\n    if not len(additional_stations) == 0:\n        print('Adding additional stations')\n        for t, template in enumerate(all_stachans):\n            av_delay = np.mean(all_delays[t][1])\n            for sta in additional_stations:\n                if sta not in template[1]:\n                    print('Added station ' + '.'.join(sta))\n                    template[1].append(sta)\n                    all_delays[t][1].append(av_delay)\n    del stachans\n    # Loop through the days\n    for detection_day in detection_days:\n        print('Working on detections for day: ' + str(detection_day))\n        stachans = list(set([stachans[1] for stachans in all_stachans][0]))\n        # List of all unique stachans - read in all data\n        st = read_data(archive=archive, arc_type=arc_type, day=detection_day,\n                       stachans=stachans)\n        st.merge(fill_value='interpolate')\n        day_detections = [detection for detection in detections\n                          if UTCDateTime(detection.detect_time.date) ==\n                          detection_day]\n        del stachans, delays\n        for detection in day_detections:\n            print('Cutting for detections at: ' +\n                  detection.detect_time.strftime('%Y/%m/%d %H:%M:%S'))\n            detect_wav = st.copy()\n            for tr in detect_wav:\n                t1 = UTCDateTime(detection.detect_time) - extract_len / 2\n                t2 = UTCDateTime(detection.detect_time) + extract_len / 2\n                tr.trim(starttime=t1, endtime=t2)\n            if outdir:\n                if not os.path.isdir(os.path.join(outdir,\n                                                  detection.template_name)):\n                    os.makedirs(os.path.join(outdir, detection.template_name))\n                detect_wav.write(os.path.join(outdir, detection.template_name,\n                                              detection.detect_time.\n                                              strftime('%Y-%m-%d_%H-%M-%S') +\n                                              '.ms'),\n                                 format='MSEED')\n                print('Written file: %s' %\n                      '/'.join([outdir, detection.template_name,\n                                detection.detect_time.\n                               strftime('%Y-%m-%d_%H-%M-%S') + '.ms']))\n            if not outdir:\n                detection_wavefiles.append(detect_wav)\n            del detect_wav\n        del st\n        if outdir:\n            detection_wavefiles = []\n    if not outdir:\n        return detection_wavefiles\n    else:\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dist_mat_km(catalog):\n    # Initialize square matrix\n    dist_mat = np.array([np.array([0.0] * len(catalog))] *\n                        len(catalog))\n    # Calculate distance vector for each event\n    for i, master in enumerate(catalog):\n        mast_list = []\n        if master.preferred_origin():\n            master_ori = master.preferred_origin()\n        else:\n            master_ori = master.origins[-1]\n        master_tup = (master_ori.latitude,\n                      master_ori.longitude,\n                      master_ori.depth // 1000)\n        for slave in catalog:\n            if slave.preferred_origin():\n                slave_ori = slave.preferred_origin()\n            else:\n                slave_ori = slave.origins[-1]\n            slave_tup = (slave_ori.latitude,\n                         slave_ori.longitude,\n                         slave_ori.depth // 1000)\n            mast_list.append(dist_calc(master_tup, slave_tup))\n        # Sort the list into the dist_mat structure\n        for j in range(i, len(catalog)):\n            dist_mat[i, j] = mast_list[j]\n    # Reshape the distance matrix\n    for i in range(1, len(catalog)):\n        for j in range(i):\n            dist_mat[i, j] = dist_mat.T[i, j]\n    return dist_mat", "response": "Compute the distance matrix for all events in a catalog using epicentral separation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef space_cluster(catalog, d_thresh, show=True):\n    # Compute the distance matrix and linkage\n    dist_mat = dist_mat_km(catalog)\n    dist_vec = squareform(dist_mat)\n    Z = linkage(dist_vec, method='average')\n\n    # Cluster the linkage using the given threshold as the cutoff\n    indices = fcluster(Z, t=d_thresh, criterion='distance')\n    group_ids = list(set(indices))\n    indices = [(indices[i], i) for i in range(len(indices))]\n\n    if show:\n        # Plot the dendrogram...if it's not way too huge\n        dendrogram(Z, color_threshold=d_thresh,\n                   distance_sort='ascending')\n        plt.show()\n\n    # Sort by group id\n    indices.sort(key=lambda tup: tup[0])\n    groups = []\n    for group_id in group_ids:\n        group = Catalog()\n        for ind in indices:\n            if ind[0] == group_id:\n                group.append(catalog[ind[1]])\n            elif ind[0] > group_id:\n                # Because we have sorted by group id, when the index is greater\n                # than the group_id we can break the inner loop.\n                # Patch applied by CJC 05/11/2015\n                groups.append(group)\n                break\n    groups.append(group)\n    return groups", "response": "Cluster a catalog by distance only."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclusters detections in space and time.", "response": "def space_time_cluster(catalog, t_thresh, d_thresh):\n    \"\"\"\n    Cluster detections in space and time.\n\n    Use to separate repeaters from other events.  Clusters by distance\n    first, then removes events in those groups that are at different times.\n\n    :type catalog: obspy.core.event.Catalog\n    :param catalog: Catalog of events to clustered\n    :type t_thresh: float\n    :param t_thresh: Maximum inter-event time threshold in seconds\n    :type d_thresh: float\n    :param d_thresh: Maximum inter-event distance in km\n\n    :returns: list of :class:`obspy.core.event.Catalog` objects\n    :rtype: list\n\n    >>> from eqcorrscan.utils.clustering import space_time_cluster\n    >>> from obspy.clients.fdsn import Client\n    >>> from obspy import UTCDateTime\n    >>> client = Client(\"https://earthquake.usgs.gov\")\n    >>> starttime = UTCDateTime(\"2002-01-01\")\n    >>> endtime = UTCDateTime(\"2002-02-01\")\n    >>> cat = client.get_events(starttime=starttime, endtime=endtime,\n    ...                         minmagnitude=6)\n    >>> groups = space_time_cluster(catalog=cat, t_thresh=86400, d_thresh=1000)\n    \"\"\"\n    initial_spatial_groups = space_cluster(catalog=catalog, d_thresh=d_thresh,\n                                           show=False)\n    # Need initial_spatial_groups to be lists at the moment\n    initial_spatial_lists = []\n    for group in initial_spatial_groups:\n        initial_spatial_lists.append(list(group))\n    # Check within these groups and throw them out if they are not close in\n    # time.\n    groups = []\n    for group in initial_spatial_lists:\n        for master in group:\n            for event in group:\n                if abs(event.preferred_origin().time -\n                       master.preferred_origin().time) > t_thresh:\n                    # If greater then just put event in on it's own\n                    groups.append([event])\n                    group.remove(event)\n        groups.append(group)\n    return [Catalog(group) for group in groups]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves detections by changing the threshold. Can only be done to remove detection by increasing threshold, threshold lowering will have no effect. :type path: str :param path: Path to the .csv detection file :type old_thresh: float :param old_thresh: Old threshold MAD multiplier :type new_thresh: float :param new_thresh: New threshold MAD multiplier :type chan_thresh: int :param chan_thresh: Minimum number of channels for a detection :returns: List of detections :rtype: list .. rubric:: Example >>> from eqcorrscan.utils.clustering import re_thresh_csv >>> # Get the path to the test data >>> import eqcorrscan >>> import os >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data' >>> det_file = os.path.join(TEST_PATH, 'expected_tutorial_detections.txt') >>> detections = re_thresh_csv(path=det_file, old_thresh=8, new_thresh=10, ... chan_thresh=3) Read in 22 detections Left with 17 detections .. Note:: This is a legacy function, and will read detections from all versions. .. Warning:: Only works if thresholding was done by MAD.", "response": "def re_thresh_csv(path, old_thresh, new_thresh, chan_thresh):\n    \"\"\"\n    Remove detections by changing the threshold.\n\n    Can only be done to remove detection by increasing threshold,\n    threshold lowering will have no effect.\n\n    :type path: str\n    :param path: Path to the .csv detection file\n    :type old_thresh: float\n    :param old_thresh: Old threshold MAD multiplier\n    :type new_thresh: float\n    :param new_thresh: New threshold MAD multiplier\n    :type chan_thresh: int\n    :param chan_thresh: Minimum number of channels for a detection\n\n    :returns: List of detections\n    :rtype: list\n\n    .. rubric:: Example\n\n    >>> from eqcorrscan.utils.clustering import re_thresh_csv\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> det_file = os.path.join(TEST_PATH, 'expected_tutorial_detections.txt')\n    >>> detections = re_thresh_csv(path=det_file, old_thresh=8, new_thresh=10,\n    ...                            chan_thresh=3)\n    Read in 22 detections\n    Left with 17 detections\n\n    .. Note::\n        This is a legacy function, and will read detections from all versions.\n\n    .. Warning:: Only works if thresholding was done by MAD.\n    \"\"\"\n    from eqcorrscan.core.match_filter import read_detections\n    warnings.warn('Legacy function, please use '\n                  'eqcorrscan.core.match_filter.Party.rethreshold.')\n    old_detections = read_detections(path)\n    old_thresh = float(old_thresh)\n    new_thresh = float(new_thresh)\n    # Be nice, ensure that the thresholds are float\n    detections = []\n    detections_in = 0\n    detections_out = 0\n    for detection in old_detections:\n        detections_in += 1\n        con1 = (new_thresh / old_thresh) * detection.threshold\n        con2 = detection.no_chans >= chan_thresh\n        requirted_thresh = (new_thresh / old_thresh) * detection.threshold\n        con3 = abs(detection.detect_val) >= requirted_thresh\n        if all([con1, con2, con3]):\n            detections_out += 1\n            detections.append(detection)\n    print('Read in %i detections' % detections_in)\n    print('Left with %i detections' % detections_out)\n    return detections"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the general multithreading function using func", "response": "def _general_multithread(func):\n    \"\"\" return the general multithreading function using func \"\"\"\n\n    def multithread(templates, stream, *args, **kwargs):\n        with pool_boy(ThreadPool, len(stream), **kwargs) as pool:\n            return _pool_normxcorr(templates, stream, pool=pool, func=func)\n\n    return multithread"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_registerd_func(name_or_func):\n    # get the function or register callable\n    if callable(name_or_func):\n        func = register_array_xcorr(name_or_func)\n    else:\n        func = XCOR_FUNCS[name_or_func or 'default']\n    assert callable(func), 'func is not callable'\n    # ensure func has the added methods\n    if not hasattr(func, 'registered'):\n        func = register_array_xcorr(func)\n    return func", "response": "get a xcorr function from a str or callable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef numpy_normxcorr(templates, stream, pads, *args, **kwargs):\n    import bottleneck\n    from scipy.signal.signaltools import _centered\n\n    # Generate a template mask\n    used_chans = ~np.isnan(templates).any(axis=1)\n    # Currently have to use float64 as bottleneck runs into issues with other\n    # types: https://github.com/kwgoodman/bottleneck/issues/164\n    stream = stream.astype(np.float64)\n    templates = templates.astype(np.float64)\n    template_length = templates.shape[1]\n    stream_length = len(stream)\n    fftshape = next_fast_len(template_length + stream_length - 1)\n    # Set up normalizers\n    stream_mean_array = bottleneck.move_mean(\n        stream, template_length)[template_length - 1:]\n    stream_std_array = bottleneck.move_std(\n        stream, template_length)[template_length - 1:]\n    # because stream_std_array is in denominator or res, nan all 0s\n    stream_std_array[stream_std_array == 0] = np.nan\n    # Normalize and flip the templates\n    norm = ((templates - templates.mean(axis=-1, keepdims=True)) / (\n        templates.std(axis=-1, keepdims=True) * template_length))\n    norm_sum = norm.sum(axis=-1, keepdims=True)\n    stream_fft = np.fft.rfft(stream, fftshape)\n    template_fft = np.fft.rfft(np.flip(norm, axis=-1), fftshape, axis=-1)\n    res = np.fft.irfft(template_fft * stream_fft,\n                       fftshape)[:, 0:template_length + stream_length - 1]\n    res = ((_centered(res, stream_length - template_length + 1)) -\n           norm_sum * stream_mean_array) / stream_std_array\n    res[np.isnan(res)] = 0.0\n    # res[np.isinf(res)] = 0.0\n    for i, pad in enumerate(pads):  # range(len(pads)):\n        res[i] = np.append(res[i], np.zeros(pad))[pad:]\n    return res.astype(np.float32), used_chans", "response": "Compute the normalized cross - correlation using numpy and bottleneck."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time_multi_normxcorr(templates, stream, pads, threaded=False, *args,\n                         **kwargs):\n    \"\"\"\n    Compute cross-correlations in the time-domain using C routine.\n\n    :param templates: 2D Array of templates\n    :type templates: np.ndarray\n    :param stream: 1D array of continuous data\n    :type stream: np.ndarray\n    :param pads: List of ints of pad lengths in the same order as templates\n    :type pads: list\n    :param threaded: Whether to use the threaded routine or not\n    :type threaded: bool\n\n    :return: np.ndarray of cross-correlations\n    :return: np.ndarray channels used\n    \"\"\"\n    used_chans = ~np.isnan(templates).any(axis=1)\n\n    utilslib = _load_cdll('libutils')\n\n    argtypes = [\n        np.ctypeslib.ndpointer(dtype=np.float32, ndim=1,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_int, ctypes.c_int,\n        np.ctypeslib.ndpointer(dtype=np.float32, ndim=1,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_int,\n        np.ctypeslib.ndpointer(dtype=np.float32, ndim=1,\n                               flags=native_str('C_CONTIGUOUS'))]\n    restype = ctypes.c_int\n    if threaded:\n        func = utilslib.multi_normxcorr_time_threaded\n        argtypes.append(ctypes.c_int)\n    else:\n        func = utilslib.multi_normxcorr_time\n    func.argtypes = argtypes\n    func.restype = restype\n    # Need to de-mean everything\n    templates_means = templates.mean(axis=1).astype(np.float32)[:, np.newaxis]\n    stream_mean = stream.mean().astype(np.float32)\n    templates = templates.astype(np.float32) - templates_means\n    stream = stream.astype(np.float32) - stream_mean\n    template_len = templates.shape[1]\n    n_templates = templates.shape[0]\n    image_len = stream.shape[0]\n    ccc = np.ascontiguousarray(\n        np.empty((image_len - template_len + 1) * n_templates), np.float32)\n    t_array = np.ascontiguousarray(templates.flatten(), np.float32)\n    time_args = [t_array, template_len, n_templates,\n                 np.ascontiguousarray(stream, np.float32), image_len, ccc]\n    if threaded:\n        time_args.append(kwargs.get('cores', cpu_count()))\n    func(*time_args)\n    ccc[np.isnan(ccc)] = 0.0\n    ccc = ccc.reshape((n_templates, image_len - template_len + 1))\n    for i in range(len(pads)):\n        ccc[i] = np.append(ccc[i], np.zeros(pads[i]))[pads[i]:]\n    templates += templates_means\n    stream += stream_mean\n    return ccc, used_chans", "response": "Compute cross - correlations in the time - domain using C routine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalising cross-correlation using the fftw library. Internally this function used double precision numbers, which is definitely required for seismic data. Cross-correlations are computed as the inverse fft of the dot product of the ffts of the stream and the reversed, normalised, templates. The cross-correlation is then normalised using the running mean and standard deviation (not using the N-1 correction) of the stream and the sums of the normalised templates. This python function wraps the C-library written by C. Chamberlain for this purpose. :param templates: 2D Array of templates :type templates: np.ndarray :param stream: 1D array of continuous data :type stream: np.ndarray :param pads: List of ints of pad lengths in the same order as templates :type pads: list :param threaded: Whether to use the threaded routine or not - note openMP and python multiprocessing don't seem to play nice for this. :type threaded: bool :return: np.ndarray of cross-correlations :return: np.ndarray channels used", "response": "def fftw_normxcorr(templates, stream, pads, threaded=False, *args, **kwargs):\n    \"\"\"\n    Normalised cross-correlation using the fftw library.\n\n    Internally this function used double precision numbers, which is definitely\n    required for seismic data. Cross-correlations are computed as the\n    inverse fft of the dot product of the ffts of the stream and the reversed,\n    normalised, templates.  The cross-correlation is then normalised using the\n    running mean and standard deviation (not using the N-1 correction) of the\n    stream and the sums of the normalised templates.\n\n    This python function wraps the C-library written by C. Chamberlain for this\n    purpose.\n\n    :param templates: 2D Array of templates\n    :type templates: np.ndarray\n    :param stream: 1D array of continuous data\n    :type stream: np.ndarray\n    :param pads: List of ints of pad lengths in the same order as templates\n    :type pads: list\n    :param threaded:\n        Whether to use the threaded routine or not - note openMP and python\n        multiprocessing don't seem to play nice for this.\n    :type threaded: bool\n\n    :return: np.ndarray of cross-correlations\n    :return: np.ndarray channels used\n    \"\"\"\n    utilslib = _load_cdll('libutils')\n\n    argtypes = [\n        np.ctypeslib.ndpointer(dtype=np.float32, ndim=1,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long, ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.float32, ndim=1,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.float32,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS')),\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS')),\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS'))]\n    restype = ctypes.c_int\n\n    if threaded:\n        func = utilslib.normxcorr_fftw_threaded\n    else:\n        func = utilslib.normxcorr_fftw\n\n    func.argtypes = argtypes\n    func.restype = restype\n\n    # Generate a template mask\n    used_chans = ~np.isnan(templates).any(axis=1)\n    template_length = templates.shape[1]\n    stream_length = len(stream)\n    n_templates = templates.shape[0]\n    fftshape = next_fast_len(template_length + stream_length - 1)\n\n    # # Normalize and flip the templates\n    norm = ((templates - templates.mean(axis=-1, keepdims=True)) / (\n        templates.std(axis=-1, keepdims=True) * template_length))\n\n    norm = np.nan_to_num(norm)\n    ccc = np.zeros((n_templates, stream_length - template_length + 1),\n                   np.float32)\n    used_chans_np = np.ascontiguousarray(used_chans, dtype=np.intc)\n    pads_np = np.ascontiguousarray(pads, dtype=np.intc)\n    variance_warning = np.ascontiguousarray([0], dtype=np.intc)\n\n    # Check that stream is non-zero and above variance threshold\n    if not np.all(stream == 0) and np.var(stream) < 1e-8:\n        # Apply gain\n        stream *= 1e8\n        warnings.warn(\"Low variance found for, applying gain \"\n                      \"to stabilise correlations\")\n    ret = func(\n        np.ascontiguousarray(norm.flatten(order='C'), np.float32),\n        template_length, n_templates,\n        np.ascontiguousarray(stream, np.float32), stream_length,\n        np.ascontiguousarray(ccc, np.float32), fftshape,\n        used_chans_np, pads_np, variance_warning)\n    if ret < 0:\n        raise MemoryError()\n    elif ret not in [0, 999]:\n        print('Error in C code (possible normalisation error)')\n        print('Maximum ccc %f at %i' % (ccc.max(), ccc.argmax()))\n        print('Minimum ccc %f at %i' % (ccc.min(), ccc.argmin()))\n        raise CorrelationError(\"Internal correlation error\")\n    elif ret == 999:\n        warnings.warn(\"Some correlations not computed, are there \"\n                      \"zeros in data? If not, consider increasing gain.\")\n    if variance_warning[0] and variance_warning[0] > template_length:\n        warnings.warn(\n            \"Low variance found in {0} positions, check result.\".format(\n                variance_warning[0]))\n\n    return ccc, used_chans"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _time_threaded_normxcorr(templates, stream, *args, **kwargs):\n    no_chans = np.zeros(len(templates))\n    chans = [[] for _ in range(len(templates))]\n    array_dict_tuple = _get_array_dicts(templates, stream)\n    stream_dict, template_dict, pad_dict, seed_ids = array_dict_tuple\n    cccsums = np.zeros([len(templates),\n                        len(stream[0]) - len(templates[0][0]) + 1])\n    for seed_id in seed_ids:\n        tr_cc, tr_chans = time_multi_normxcorr(\n            template_dict[seed_id], stream_dict[seed_id], pad_dict[seed_id],\n            True)\n        cccsums = np.sum([cccsums, tr_cc], axis=0)\n        no_chans += tr_chans.astype(np.int)\n        for chan, state in zip(chans, tr_chans):\n            if state:\n                chan.append((seed_id.split('.')[1],\n                             seed_id.split('.')[-1].split('_')[0]))\n    return cccsums, no_chans, chans", "response": "This function is used to compute the time - domain for each template and stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply fftw normxcorr routine concurrently.", "response": "def _fftw_stream_xcorr(templates, stream, *args, **kwargs):\n    \"\"\"\n    Apply fftw normxcorr routine concurrently.\n\n    :type templates: list\n    :param templates:\n        A list of templates, where each one should be an obspy.Stream object\n        containing multiple traces of seismic data and the relevant header\n        information.\n    :type stream: obspy.core.stream.Stream\n    :param stream:\n        A single Stream object to be correlated with the templates.\n\n    :returns:\n        New list of :class:`numpy.ndarray` objects.  These will contain\n        the correlation sums for each template for this day of data.\n    :rtype: list\n    :returns:\n        list of ints as number of channels used for each cross-correlation.\n    :rtype: list\n    :returns:\n        list of list of tuples of station, channel for all cross-correlations.\n    :rtype: list\n    \"\"\"\n    # number of threads:\n    #   default to using inner threads\n    #   if `cores` or `cores_outer` passed in then use that\n    #   else if OMP_NUM_THREADS set use that\n    #   otherwise use all available\n    num_cores_inner = kwargs.get('cores')\n    num_cores_outer = kwargs.get('cores_outer')\n    if num_cores_inner is None and num_cores_outer is None:\n        num_cores_inner = int(os.getenv(\"OMP_NUM_THREADS\", cpu_count()))\n        num_cores_outer = 1\n    elif num_cores_inner is not None and num_cores_outer is None:\n        num_cores_outer = 1\n    elif num_cores_outer is not None and num_cores_inner is None:\n        num_cores_inner = 1\n\n    chans = [[] for _i in range(len(templates))]\n    array_dict_tuple = _get_array_dicts(templates, stream)\n    stream_dict, template_dict, pad_dict, seed_ids = array_dict_tuple\n    assert set(seed_ids)\n    cccsums, tr_chans = fftw_multi_normxcorr(\n        template_array=template_dict, stream_array=stream_dict,\n        pad_array=pad_dict, seed_ids=seed_ids, cores_inner=num_cores_inner,\n        cores_outer=num_cores_outer)\n    no_chans = np.sum(np.array(tr_chans).astype(np.int), axis=0)\n    for seed_id, tr_chan in zip(seed_ids, tr_chans):\n        for chan, state in zip(chans, tr_chan):\n            if state:\n                chan.append((seed_id.split('.')[1],\n                             seed_id.split('.')[-1].split('_')[0]))\n    return cccsums, no_chans, chans"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse a C loop rather than a Python loop - in some cases this will be fast. :type template_array: dict :param template_array: :type stream_array: dict :param stream_array: :type pad_array: dict :param pad_array: :type seed_ids: list :param seed_ids: rtype: np.ndarray, list :return: 3D Array of cross-correlations and list of used channels.", "response": "def fftw_multi_normxcorr(template_array, stream_array, pad_array, seed_ids,\n                         cores_inner, cores_outer):\n    \"\"\"\n    Use a C loop rather than a Python loop - in some cases this will be fast.\n\n    :type template_array: dict\n    :param template_array:\n    :type stream_array: dict\n    :param stream_array:\n    :type pad_array: dict\n    :param pad_array:\n    :type seed_ids: list\n    :param seed_ids:\n\n    rtype: np.ndarray, list\n    :return: 3D Array of cross-correlations and list of used channels.\n    \"\"\"\n    utilslib = _load_cdll('libutils')\n\n    utilslib.multi_normxcorr_fftw.argtypes = [\n        np.ctypeslib.ndpointer(dtype=np.float32,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long, ctypes.c_long, ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.float32,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.float32,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_long,\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS')),\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_int, ctypes.c_int,\n        np.ctypeslib.ndpointer(dtype=np.intc,\n                               flags=native_str('C_CONTIGUOUS'))]\n    utilslib.multi_normxcorr_fftw.restype = ctypes.c_int\n    '''\n    Arguments are:\n        templates (stacked [ch_1-t_1, ch_1-t_2, ..., ch_2-t_1, ch_2-t_2, ...])\n        number of templates\n        template length\n        number of channels\n        image (stacked [ch_1, ch_2, ..., ch_n])\n        image length\n        cross-correlations (stacked as per image)\n        fft-length\n        used channels (stacked as per templates)\n        pad array (stacked as per templates)\n    '''\n\n    # pre processing\n    used_chans = []\n    template_len = template_array[seed_ids[0]].shape[1]\n    for seed_id in seed_ids:\n        used_chans.append(~np.isnan(template_array[seed_id]).any(axis=1))\n        template_array[seed_id] = (\n            (template_array[seed_id] -\n             template_array[seed_id].mean(axis=-1, keepdims=True)) / (\n                template_array[seed_id].std(axis=-1, keepdims=True) *\n                template_len))\n        template_array[seed_id] = np.nan_to_num(template_array[seed_id])\n    n_channels = len(seed_ids)\n    n_templates = template_array[seed_ids[0]].shape[0]\n    image_len = stream_array[seed_ids[0]].shape[0]\n    fft_len = next_fast_len(template_len + image_len - 1)\n    template_array = np.ascontiguousarray([template_array[x]\n                                           for x in seed_ids],\n                                          dtype=np.float32)\n    for x in seed_ids:\n        # Check that stream is non-zero and above variance threshold\n        if not np.all(stream_array[x] == 0) and np.var(stream_array[x]) < 1e-8:\n            # Apply gain\n            stream_array *= 1e8\n            warnings.warn(\"Low variance found for {0}, applying gain \"\n                          \"to stabilise correlations\".format(x))\n    stream_array = np.ascontiguousarray([stream_array[x] for x in seed_ids],\n                                        dtype=np.float32)\n    cccs = np.zeros((n_templates, image_len - template_len + 1),\n                    np.float32)\n    used_chans_np = np.ascontiguousarray(used_chans, dtype=np.intc)\n    pad_array_np = np.ascontiguousarray([pad_array[seed_id]\n                                         for seed_id in seed_ids],\n                                        dtype=np.intc)\n    variance_warnings = np.ascontiguousarray(\n        np.zeros(n_channels), dtype=np.intc)\n\n    # call C function\n    ret = utilslib.multi_normxcorr_fftw(\n        template_array, n_templates, template_len, n_channels, stream_array,\n        image_len, cccs, fft_len, used_chans_np, pad_array_np, cores_outer,\n        cores_inner, variance_warnings)\n    if ret < 0:\n        raise MemoryError(\"Memory allocation failed in correlation C-code\")\n    elif ret not in [0, 999]:\n        print('Error in C code (possible normalisation error)')\n        print('Maximum cccs %f at %s' %\n              (cccs.max(), np.unravel_index(cccs.argmax(), cccs.shape)))\n        print('Minimum cccs %f at %s' %\n              (cccs.min(), np.unravel_index(cccs.argmin(), cccs.shape)))\n        raise CorrelationError(\"Internal correlation error\")\n    elif ret == 999:\n        warnings.warn(\"Some correlations not computed, are there \"\n                      \"zeros in data? If not, consider increasing gain.\")\n    for i, variance_warning in enumerate(variance_warnings):\n        if variance_warning and variance_warning > template_len:\n            warnings.warn(\"Low variance found in {0} places for {1},\"\n                          \" check result.\".format(variance_warning,\n                                                  seed_ids[i]))\n\n    return cccs, used_chans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stream_xcorr(name_or_func=None, concurrency=None):\n    func = _get_registerd_func(name_or_func)\n\n    concur = concurrency or 'stream_xcorr'\n    if not hasattr(func, concur):\n        msg = '%s does not support concurrency %s' % (func.__name__, concur)\n        raise ValueError(msg)\n    return getattr(func, concur)", "response": "Returns a function that performs normalized cross correlation on lists of streams."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares templates and stream return dicts", "response": "def _get_array_dicts(templates, stream, copy_streams=True):\n    \"\"\" prepare templates and stream, return dicts \"\"\"\n    # Do some reshaping\n    # init empty structures for data storage\n    template_dict = {}\n    stream_dict = {}\n    pad_dict = {}\n    t_starts = []\n\n    stream.sort(['network', 'station', 'location', 'channel'])\n    for template in templates:\n        template.sort(['network', 'station', 'location', 'channel'])\n        t_starts.append(min([tr.stats.starttime for tr in template]))\n    # get seed ids, make sure these are collected on sorted streams\n    seed_ids = [tr.id + '_' + str(i) for i, tr in enumerate(templates[0])]\n    # pull common channels out of streams and templates and put in dicts\n    for i, seed_id in enumerate(seed_ids):\n        temps_with_seed = [template[i].data for template in templates]\n        t_ar = np.array(temps_with_seed).astype(np.float32)\n        template_dict.update({seed_id: t_ar})\n        stream_dict.update(\n            {seed_id: stream.select(\n                id=seed_id.split('_')[0])[0].data.astype(np.float32)})\n        pad_list = [\n            int(round(template[i].stats.sampling_rate *\n                      (template[i].stats.starttime - t_starts[j])))\n            for j, template in zip(range(len(templates)), templates)]\n        pad_dict.update({seed_id: pad_list})\n\n    return stream_dict, template_dict, pad_dict, seed_ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef median_filter(tr, multiplier=10, windowlength=0.5,\n                  interp_len=0.05, debug=0):\n    \"\"\"\n    Filter out spikes in data above a multiple of MAD of the data.\n\n    Currently only has the ability to replaces spikes with linear\n    interpolation.  In the future we would aim to fill the gap with something\n    more appropriate.  Works in-place on data.\n\n    :type tr: obspy.core.trace.Trace\n    :param tr: trace to despike\n    :type multiplier: float\n    :param multiplier:\n        median absolute deviation multiplier to find spikes above.\n    :type windowlength: float\n    :param windowlength: Length of window to look for spikes in in seconds.\n    :type interp_len: float\n    :param interp_len: Length in seconds to interpolate around spikes.\n    :type debug: int\n    :param debug: Debug output level between 0 and 5, higher is more output.\n\n    :returns: :class:`obspy.core.trace.Trace`\n\n    .. warning::\n        Not particularly effective, and may remove earthquake signals, use with\n        caution.\n    \"\"\"\n    num_cores = cpu_count()\n    if debug >= 1:\n        data_in = tr.copy()\n    # Note - might be worth finding spikes in filtered data\n    filt = tr.copy()\n    filt.detrend('linear')\n    try:\n        filt.filter('bandpass', freqmin=10.0,\n                    freqmax=(tr.stats.sampling_rate / 2) - 1)\n    except Exception as e:\n        print(\"Could not filter due to error: {0}\".format(e))\n    data = filt.data\n    del filt\n    # Loop through windows\n    _windowlength = int(windowlength * tr.stats.sampling_rate)\n    _interp_len = int(interp_len * tr.stats.sampling_rate)\n    peaks = []\n    with Timer() as t:\n        pool = Pool(processes=num_cores)\n        results = [pool.apply_async(_median_window,\n                                    args=(data[chunk * _windowlength:\n                                               (chunk + 1) * _windowlength],\n                                          chunk * _windowlength, multiplier,\n                                          tr.stats.starttime + windowlength,\n                                          tr.stats.sampling_rate,\n                                          debug))\n                   for chunk in range(int(len(data) / _windowlength))]\n        pool.close()\n        for p in results:\n            peaks += p.get()\n        pool.join()\n        for peak in peaks:\n            tr.data = _interp_gap(tr.data, peak[1], _interp_len)\n    print(\"Despiking took: %s s\" % t.secs)\n    if debug >= 1:\n        plt.plot(data_in.data, 'r', label='raw')\n        plt.plot(tr.data, 'k', label='despiked')\n        plt.legend()\n        plt.show()\n    return tr", "response": "Filter out spikes in data above a multiple of MAD of the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _median_window(window, window_start, multiplier, starttime, sampling_rate,\n                   debug=0):\n    \"\"\"\n    Internal function to aid parallel processing\n\n    :type window: numpy.ndarry\n    :param window: Data to look for peaks in.\n    :type window_start: int\n    :param window_start: Index of window start point in larger array, used \\\n        for peak indexing.\n    :type multiplier: float\n    :param multiplier: Multiple of MAD to use as threshold\n    :type starttime: obspy.core.utcdatetime.UTCDateTime\n    :param starttime: Starttime of window, used in debug plotting.\n    :type sampling_rate: float\n    :param sampling_rate in Hz, used for debug plotting\n    :type debug: int\n    :param debug: debug level, if want plots, >= 4.\n\n    :returns: peaks\n    :rtype: list\n    \"\"\"\n    MAD = np.median(np.abs(window))\n    thresh = multiplier * MAD\n    if debug >= 2:\n        print('Threshold for window is: ' + str(thresh) +\n              '\\nMedian is: ' + str(MAD) +\n              '\\nMax is: ' + str(np.max(window)))\n    peaks = find_peaks2_short(arr=window,\n                              thresh=thresh, trig_int=5, debug=0)\n    if debug >= 4 and peaks:\n        peaks_plot(window, starttime, sampling_rate,\n                   save=False, peaks=peaks)\n    if peaks:\n        peaks = [(peak[0], peak[1] + window_start) for peak in peaks]\n    else:\n        peaks = []\n    return peaks", "response": "Internal function to aid parallel processing of a window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving spikes from a trace and returns the original trace.", "response": "def template_remove(tr, template, cc_thresh, windowlength,\n                    interp_len, debug=0):\n    \"\"\"\n    Looks for instances of template in the trace and removes the matches.\n\n    :type tr: obspy.core.trace.Trace\n    :param tr: Trace to remove spikes from.\n    :type template: osbpy.core.trace.Trace\n    :param template: Spike template to look for in data.\n    :type cc_thresh: float\n    :param cc_thresh: Cross-correlation threshold (-1 - 1).\n    :type windowlength: float\n    :param windowlength: Length of window to look for spikes in in seconds.\n    :type interp_len: float\n    :param interp_len: Window length to remove and fill in seconds.\n    :type debug: int\n    :param debug: Debug level.\n\n    :returns: tr, works in place.\n    :rtype: :class:`obspy.core.trace.Trace`\n    \"\"\"\n    data_in = tr.copy()\n    _interp_len = int(tr.stats.sampling_rate * interp_len)\n    if _interp_len < len(template.data):\n        warnings.warn('Interp_len is less than the length of the template,'\n                      'will used the length of the template!')\n        _interp_len = len(template.data)\n    if isinstance(template, Trace):\n        template = template.data\n    with Timer() as t:\n        cc = normxcorr2(image=tr.data.astype(np.float32),\n                        template=template.astype(np.float32))\n        if debug > 3:\n            plt.plot(cc.flatten(), 'k', label='cross-correlation')\n            plt.legend()\n            plt.show()\n        peaks = find_peaks2_short(arr=cc.flatten(), thresh=cc_thresh,\n                                  trig_int=windowlength * tr.stats.\n                                  sampling_rate)\n        for peak in peaks:\n            tr.data = _interp_gap(data=tr.data,\n                                  peak_loc=peak[1] + int(0.5 * _interp_len),\n                                  interp_len=_interp_len)\n    print(\"Despiking took: %s s\" % t.secs)\n    if debug > 2:\n        plt.plot(data_in.data, 'r', label='raw')\n        plt.plot(tr.data, 'k', label='despiked')\n        plt.legend()\n        plt.show()\n    return tr"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sactoevent(st, debug=0):\n    # Check the version\n    _version_check()\n    # Set the default SAC nan values\n    float_nan = -12345.0\n\n    if not isinstance(st, Stream):\n        msg = ('st must be a stream object, if you have just read in ' +\n               'multiple SAC files you may have a list of streams, convert ' +\n               'using: st = Stream([tr[0] for tr in st])')\n        raise ValueError(msg)\n        # Note, don't do this internally as we need to ensure that we are not\n        # taking traces from other events, the user should check this.\n    for tr in st:\n        if not tr.stats._format == 'SAC':\n            msg = ('%s.%s is not SAC formatted.' % (tr.stats.station,\n                                                    tr.stats.channel))\n            raise ValueError(msg)\n    # Now we need to create an event!\n    event = Event()\n    event.origins.append(Origin())\n    # print(st[0].stats.sac.keys())\n    event.origins[0].time = UTCDateTime(year=st[0].stats.sac.nzyear,\n                                        julday=st[0].stats.sac.nzjday,\n                                        hour=st[0].stats.sac.nzhour,\n                                        minute=st[0].stats.sac.nzmin,\n                                        second=st[0].stats.sac.nzsec,\n                                        microsecond=st[0].stats.\n                                        sac.nzmsec * 1000)\n    try:\n        event.origins[0].latitude = st[0].stats.sac.evla\n        event.origins[0].longitude = st[0].stats.sac.evlo\n        event.origins[0].depth = st[0].stats.sac.evdp\n        # Catch filled with 12345.0 as nan\n        if event.origins[0].latitude == float_nan:\n            event.origins[0].latitude = None\n        if event.origins[0].longitude == float_nan:\n            event.origins[0].longitude = None\n        if event.origins[0].depth == float_nan:\n            event.origins[0].depth = None\n    except KeyError:\n        event.origins[0].latitude = None\n        event.origins[0].longitude = None\n        event.origins[0].depth = None\n    except AttributeError:\n        event.origins[0].latitude = None\n        event.origins[0].longitude = None\n        event.origins[0].depth = None\n\n    # Add in the picks\n    for tr in st:\n        reference_time = UTCDateTime(year=tr.stats.sac.nzyear,\n                                     julday=tr.stats.sac.nzjday,\n                                     hour=tr.stats.sac.nzhour,\n                                     minute=tr.stats.sac.nzmin,\n                                     second=tr.stats.sac.nzsec,\n                                     microsecond=tr.stats.sac.nzmsec * 1000)\n        # Possible pick locations are in the t[0-9] slot\n        for pick_number in range(10):\n            pick_key = 't' + str(pick_number)\n            phase_key = 'kt' + str(pick_number)\n            try:\n                if tr.stats.sac[pick_key] == float_nan:\n                    # in version 0.10.2 and before. rather than not include\n                    # the keys, the variables are filled with SAC nans.\n                    if debug > 1:\n                        msg = 'No pick in position ' + pick_key + \\\n                            ' for trace: ' + tr.stats.station + '.' + \\\n                            tr.stats.channel\n                        warnings.warn(msg)\n                    continue\n                pick_time = reference_time + tr.stats.sac[pick_key]\n                phase_hint = tr.stats.sac[phase_key].split()[0]\n            except KeyError:\n                if debug > 1:\n                    msg = 'No pick in position ' + pick_key + ' for trace: ' +\\\n                        tr.stats.station + '.' + tr.stats.channel\n                    warnings.warn(msg)\n                continue\n            if debug > 0:\n                msg = 'Found pick in position ' + pick_key + ' for trace: ' +\\\n                    tr.stats.station + '.' + tr.stats.channel\n                print(msg)\n            waveform_id = WaveformStreamID(station_code=tr.stats.station,\n                                           network_code=tr.stats.network,\n                                           channel_code=tr.stats.channel)\n            pick = Pick(waveform_id=waveform_id,\n                        phase_hint=phase_hint,\n                        time=pick_time)\n            event.picks.append(pick)\n        # Also check header slots 'a' and 'ka'\n        try:\n            if tr.stats.sac['a'] == float_nan:\n                if debug > 1:\n                    msg = 'No pick in position ' + pick_key + \\\n                        ' for trace: ' + tr.stats.station + '.' + \\\n                        tr.stats.channel\n                    warnings.warn(msg)\n                continue\n            pick_time = reference_time + tr.stats.sac['a']\n            phase_hint = tr.stats.sac['ka'].split()[0]\n        except KeyError:\n            if debug > 1:\n                msg = 'No pick in position ' + pick_key + ' for trace: ' +\\\n                    tr.stats.station + '.' + tr.stats.channel\n                warnings.warn(msg)\n            continue\n        if debug > 0:\n            msg = 'Found pick in position a for trace: ' +\\\n                tr.stats.station + '.' + tr.stats.channel\n            print(msg)\n        waveform_id = WaveformStreamID(station_code=tr.stats.station,\n                                       network_code=tr.stats.network,\n                                       channel_code=tr.stats.channel)\n        pick = Pick(waveform_id=waveform_id,\n                    phase_hint=phase_hint,\n                    time=pick_time)\n        event.picks.append(pick)\n\n    return event", "response": "Convert SAC headers to obspy event class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to read the appropriate data from an archive for a given day.", "response": "def read_data(archive, arc_type, day, stachans, length=86400):\n    \"\"\"\n    Function to read the appropriate data from an archive for a day.\n\n    :type archive: str\n    :param archive:\n        The archive source - if arc_type is seishub, this should be a url,\n        if the arc_type is FDSN then this can be either a url or a known obspy\n        client.  If arc_type is day_vols, then this is the path to the top\n        directory.\n    :type arc_type: str\n    :param arc_type: The type of archive, can be: seishub, FDSN, day_volumes\n    :type day: datetime.date\n    :param day: Date to retrieve data for\n    :type stachans: list\n    :param stachans: List of tuples of Stations and channels to try and get,\n        will not fail if stations are not available, but will warn.\n    :type length: float\n    :param length: Data length to extract in seconds, defaults to 1 day.\n\n    :returns: Stream of data\n    :rtype: obspy.core.stream.Stream\n\n    .. note:: A note on arc_types, if arc_type is day_vols, then this will \\\n        look for directories labelled in the IRIS DMC conventions of \\\n        Yyyyy/Rjjj.01/... where yyyy is the year and jjj is the julian day. \\\n        Data within these files directories should be stored as day-long, \\\n        single-channel files.  This is not implemented in the fasted way \\\n        possible to allow for a more general situation.  If you require more \\\n        speed you will need to re-write this.\n\n    .. rubric:: Example\n\n    >>> from obspy import UTCDateTime\n    >>> t1 = UTCDateTime(2012, 3, 26)\n    >>> stachans = [('JCNB', 'SP1')]\n    >>> st = read_data('NCEDC', 'FDSN', t1, stachans)\n    >>> print(st)\n    1 Trace(s) in Stream:\n    BP.JCNB.40.SP1 | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.\\\n950000Z | 20.0 Hz, 1728000 samples\n\n    .. rubric:: Example, missing data\n\n    >>> t1 = UTCDateTime(2012, 3, 26)\n    >>> stachans = [('JCNB', 'SP1'), ('GCSZ', 'HHZ')]\n    >>> st = read_data('NCEDC', 'FDSN', t1, stachans)\n    >>> print(st)\n    1 Trace(s) in Stream:\n    BP.JCNB.40.SP1 | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.\\\n950000Z | 20.0 Hz, 1728000 samples\n\n\n    .. rubric:: Example, local day-volumes\n\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> t1 = UTCDateTime(2012, 3, 26)\n    >>> stachans = [('WHYM', 'SHZ'), ('EORO', 'SHZ')]\n    >>> st = read_data(TEST_PATH + '/day_vols', 'day_vols',\n    ...                t1, stachans)\n    >>> print(st)\n    2 Trace(s) in Stream:\n    AF.WHYM..SHZ | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.000000Z \\\n| 1.0 Hz, 86400 samples\n    AF.EORO..SHZ | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.000000Z \\\n| 1.0 Hz, 86400 samples\n    \"\"\"\n    st = []\n    available_stations = _check_available_data(archive, arc_type, day)\n    for station in stachans:\n        if len(station[1]) == 2:\n            # Cope with two char channel naming in seisan\n            station_map = (station[0], station[1][0] + '*' + station[1][1])\n            available_stations_map = [(sta[0], sta[1][0] + '*' + sta[1][-1])\n                                      for sta in available_stations]\n        else:\n            station_map = station\n            available_stations_map = available_stations\n        if station_map not in available_stations_map:\n            msg = ' '.join([station[0], station_map[1], 'is not available for',\n                            day.strftime('%Y/%m/%d')])\n            warnings.warn(msg)\n            continue\n        if arc_type.lower() == 'seishub':\n            client = SeishubClient(archive)\n            st += client.get_waveforms(\n                    network='*', station=station_map[0], location='*',\n                    channel=station_map[1], starttime=UTCDateTime(day),\n                    endtime=UTCDateTime(day) + length)\n        elif arc_type.upper() == \"FDSN\":\n            client = FDSNClient(archive)\n            try:\n                st += client.get_waveforms(\n                    network='*', station=station_map[0], location='*',\n                    channel=station_map[1], starttime=UTCDateTime(day),\n                    endtime=UTCDateTime(day) + length)\n            except FDSNException:\n                warnings.warn('No data on server despite station being ' +\n                              'available...')\n                continue\n        elif arc_type.lower() == 'day_vols':\n            wavfiles = _get_station_file(os.path.join(\n                archive, day.strftime('Y%Y' + os.sep + 'R%j.01')),\n                station_map[0], station_map[1])\n            for wavfile in wavfiles:\n                st += read(wavfile, starttime=day, endtime=day + length)\n    st = Stream(st)\n    return st"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_station_file(path_name, station, channel, debug=0):\n    wavfiles = glob.glob(path_name + os.sep + '*')\n\n    out_files = [_check_data(wavfile, station, channel, debug=debug)\n                 for wavfile in wavfiles]\n    out_files = list(set(out_files))\n    return out_files", "response": "Helper function to find the correct file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_available_data(archive, arc_type, day):\n    available_stations = []\n    if arc_type.lower() == 'day_vols':\n        wavefiles = glob.glob(os.path.join(archive, day.strftime('Y%Y'),\n                                           day.strftime('R%j.01'), '*'))\n        for wavefile in wavefiles:\n            header = read(wavefile, headonly=True)\n            available_stations.append((header[0].stats.station,\n                                       header[0].stats.channel))\n    elif arc_type.lower() == 'seishub':\n        client = SeishubClient(archive)\n        st = client.get_previews(starttime=UTCDateTime(day),\n                                 endtime=UTCDateTime(day) + 86400)\n        for tr in st:\n            available_stations.append((tr.stats.station, tr.stats.channel))\n    elif arc_type.lower() == 'fdsn':\n        client = FDSNClient(archive)\n        inventory = client.get_stations(starttime=UTCDateTime(day),\n                                        endtime=UTCDateTime(day) + 86400,\n                                        level='channel')\n        for network in inventory:\n            for station in network:\n                for channel in station:\n                    available_stations.append((station.code,\n                                               channel.code))\n    return available_stations", "response": "Function to check what stations are available in the archive for a given day."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rt_time_log(logfile, startdate):\n    if os.name == 'nt':\n        f = io.open(logfile, 'rb')\n    else:\n        f = io.open(logfile, 'rb')\n    phase_err = []\n    lock = []\n    # Extract all the phase errors\n    for line_binary in f:\n        try:\n            line = line_binary.decode(\"utf8\", \"ignore\")\n        except UnicodeDecodeError:\n            warnings.warn('Cannot decode line, skipping')\n            continue\n        if re.search(\"INTERNAL CLOCK PHASE ERROR\", line):\n            match = re.search(\"INTERNAL CLOCK PHASE ERROR\", line)\n            d_start = match.start() - 13\n            phase_err.append((dt.datetime.strptime(str(startdate.year) +\n                                                   ':' +\n                                                   line[d_start:d_start + 12],\n                                                   '%Y:%j:%H:%M:%S'),\n                             float(line.rstrip().split()[-2]) *\n                             0.000001))\n        elif re.search(\"EXTERNAL CLOCK POWER IS TURNED OFF\", line):\n            match = re.search(\"EXTERNAL CLOCK POWER IS TURNED OFF\", line)\n            d_start = match.start() - 13\n            lock.append((dt.datetime.strptime(str(startdate.year) +\n                                              ':' + line[d_start:d_start + 12],\n                                              '%Y:%j:%H:%M:%S'),\n                        999))\n    if len(phase_err) == 0 and len(lock) > 0:\n        phase_err = lock\n    f.close()\n    return phase_err", "response": "Function to read a reftek raw log - file and return a list of tuples of time stamps and phase error."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to read a specific RefTek RT130 log - file and extract all location information from a specific RefTek raw log - file.", "response": "def rt_location_log(logfile):\n    \"\"\"\n    Extract location information from a RefTek raw log-file.\n\n    Function to read a specific RefTek RT130 log-file and find all location\n    information.\n\n    :type logfile: str\n    :param logfile: The logfile to look in\n\n    :returns: list of tuples of lat, lon, elevation in decimal degrees and km.\n    :rtype: list\n    \"\"\"\n    if os.name == 'nt':\n        f = open(logfile, 'rb')\n    else:\n        f = open(logfile, 'rb')\n    locations = []\n    for line_binary in f:\n        try:\n            line = line_binary.decode(\"utf8\", \"ignore\")\n        except UnicodeDecodeError:\n            warnings.warn('Cannot decode line, skipping')\n            print(line_binary)\n            continue\n        match = re.search(\"GPS: POSITION:\", line)\n        if match:\n            # Line is of form:\n            # jjj:hh:mm:ss GPS: POSITION: xDD:MM:SS.SS xDDD:MM:SS.SS xMMMMMMM\n            loc = line[match.end() + 1:].rstrip().split(' ')\n            lat_sign = loc[0][0]\n            lat = loc[0][1:].split(':')\n            lat = int(lat[0]) + (int(lat[1]) / 60.0) + (float(lat[2]) / 3600.0)\n            if lat_sign == 'S':\n                lat *= -1\n            lon_sign = loc[1][0]\n            lon = loc[1][1:].split(':')\n            lon = int(lon[0]) + (int(lon[1]) / 60.0) + (float(lon[2]) / 3600.0)\n            if lon_sign == 'W':\n                lon *= -1\n            elev_sign = loc[2][0]\n            elev_unit = loc[2][-1]\n            if not elev_unit == 'M':\n                raise NotImplementedError('Elevation is not in M: unit=' +\n                                          elev_unit)\n            elev = int(loc[2][1:-1])\n            if elev_sign == '-':\n                elev *= -1\n            # Convert to km\n            elev /= 1000\n            locations.append((lat, lon, elev))\n    f.close()\n    return locations"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of large time errors in a node where the time of the node is greater than a threshold.", "response": "def flag_time_err(phase_err, time_thresh=0.02):\n    \"\"\"\n    Find large time errors in list.\n\n    Scan through a list of tuples of time stamps and phase errors\n    and return a list of time stamps with timing errors above a threshold.\n\n    .. note::\n        This becomes important for networks cross-correlations, where\n        if timing information is uncertain at one site, the relative arrival\n        time (lag) will be incorrect, which will degrade the cross-correlation\n        sum.\n\n    :type phase_err: list\n    :param phase_err: List of Tuple of float, datetime.datetime\n    :type time_thresh: float\n    :param time_thresh: Threshold to declare a timing error for\n\n    :returns: List of :class:`datetime.datetime` when timing is questionable.\n    \"\"\"\n    time_err = []\n    for stamp in phase_err:\n        if abs(stamp[1]) > time_thresh:\n            time_err.append(stamp[0])\n    return time_err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking all the log files in a directory tree for timing errors.", "response": "def check_all_logs(directory, time_thresh):\n    \"\"\"\n    Check all the log-files in a directory tree for timing errors.\n\n    :type directory: str\n    :param directory: Directory to search within\n    :type time_thresh: float\n    :param time_thresh: Time threshold in seconds\n\n    :returns: List of :class:`datetime.datetime` for which error timing is\n        above threshold, e.g. times when data are questionable.\n    :rtype: list\n    \"\"\"\n    log_files = glob.glob(directory + '/*/0/000000000_00000000')\n    print('I have ' + str(len(log_files)) + ' log files to scan')\n    total_phase_errs = []\n    for i, log_file in enumerate(log_files):\n        startdate = dt.datetime.strptime(log_file.split('/')[-4][0:7],\n                                         '%Y%j').date()\n        total_phase_errs += rt_time_log(log_file, startdate)\n        sys.stdout.write(\"\\r\" + str(float(i) / len(log_files) * 100) +\n                         \"% \\r\")\n        sys.stdout.flush()\n    time_errs = flag_time_err(total_phase_errs, time_thresh)\n    time_errs.sort()\n    return time_errs, total_phase_errs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readSTATION0(path, stations):\n    stalist = []\n    f = open(path + '/STATION0.HYP', 'r')\n    for line in f:\n        if line[1:6].strip() in stations:\n            station = line[1:6].strip()\n            lat = line[6:14]  # Format is either ddmm.mmS/N or ddmm(.)mmmS/N\n            if lat[-1] == 'S':\n                NS = -1\n            else:\n                NS = 1\n            if lat[4] == '.':\n                lat = (int(lat[0:2]) + float(lat[2:-1]) / 60) * NS\n            else:\n                lat = (int(lat[0:2]) + float(lat[2:4] + '.' + lat[4:-1]) /\n                       60) * NS\n            lon = line[14:23]\n            if lon[-1] == 'W':\n                EW = -1\n            else:\n                EW = 1\n            if lon[5] == '.':\n                lon = (int(lon[0:3]) + float(lon[3:-1]) / 60) * EW\n            else:\n                lon = (int(lon[0:3]) + float(lon[3:5] + '.' + lon[5:-1]) /\n                       60) * EW\n            elev = float(line[23:-1].strip())\n            # Note, negative altitude can be indicated in 1st column\n            if line[0] == '-':\n                elev *= -1\n            stalist.append((station, lat, lon, elev))\n    f.close()\n    f = open('station.dat', 'w')\n    for sta in stalist:\n        line = ''.join([sta[0].ljust(5), _cc_round(sta[1], 4).ljust(10),\n                        _cc_round(sta[2], 4).ljust(10),\n                        _cc_round(sta[3] / 1000, 4).rjust(7), '\\n'])\n        f.write(line)\n    f.close()\n    return stalist", "response": "Read a Seisan STATION0. HYP file on the path given and outputs the information and writes to station. dat file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sfiles_to_event(sfile_list):\n    event_list = []\n    sort_list = [(readheader(sfile).origins[0].time, sfile)\n                 for sfile in sfile_list]\n    sort_list.sort(key=lambda tup: tup[0])\n    sfile_list = [sfile[1] for sfile in sort_list]\n    catalog = Catalog()\n    for i, sfile in enumerate(sfile_list):\n        event_list.append((i, sfile))\n        catalog.append(readheader(sfile))\n    # Hand off to sister function\n    write_event(catalog)\n    return event_list", "response": "Write an event. dat file from a list of Seisan events\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_event(catalog):\n    f = open('event.dat', 'w')\n    for i, event in enumerate(catalog):\n        try:\n            evinfo = event.origins[0]\n        except IndexError:\n            raise IOError('No origin')\n        try:\n            Mag_1 = event.magnitudes[0].mag\n        except IndexError:\n            Mag_1 = 0.0\n        try:\n            t_RMS = event.origins[0].quality['standard_error']\n\n        except AttributeError:\n            print('No time residual in header')\n            t_RMS = 0.0\n        f.write(str(evinfo.time.year) + str(evinfo.time.month).zfill(2) +\n                str(evinfo.time.day).zfill(2) + '  ' +\n                str(evinfo.time.hour).rjust(2) +\n                str(evinfo.time.minute).zfill(2) +\n                str(evinfo.time.second).zfill(2) +\n                str(evinfo.time.microsecond)[0:2].zfill(2) + '  ' +\n                str(evinfo.latitude).ljust(8, str('0')) + '   ' +\n                str(evinfo.longitude).ljust(8, str('0')) + '  ' +\n                str(evinfo.depth / 1000).rjust(7).ljust(9, str('0')) + '   ' +\n                str(Mag_1) + '    0.00    0.00   ' +\n                str(t_RMS).ljust(4, str('0')) +\n                str(i).rjust(11) + '\\n')\n    f.close()\n    return", "response": "Writes a list of events to a hypoDD format event. dat file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_catalog(event_list, max_sep=8, min_link=8, debug=0):\n    # Cope with possibly being passed a zip in python 3.x\n    event_list = list(event_list)\n    f = open('dt.ct', 'w')\n    f2 = open('dt.ct2', 'w')\n    fphase = open('phase.dat', 'w')\n    stations = []\n    evcount = 0\n    for i, master in enumerate(event_list):\n        master_sfile = master[1]\n        master_event_id = master[0]\n        master_event = read_nordic(master_sfile)[0]\n        master_ori_time = master_event.origins[0].time\n        master_location = (master_event.origins[0].latitude,\n                           master_event.origins[0].longitude,\n                           master_event.origins[0].depth / 1000)\n        if len(master_event.magnitudes) > 0:\n            master_magnitude = master_event.magnitudes[0].mag or ' '\n        else:\n            master_magnitude = ' '\n        header = '# ' + \\\n            master_ori_time.strftime('%Y  %m  %d  %H  %M  %S.%f') +\\\n            ' ' + str(master_location[0]).ljust(8) + ' ' +\\\n            str(master_location[1]).ljust(8) + ' ' +\\\n            str(master_location[2]).ljust(4) + ' ' +\\\n            str(master_magnitude).ljust(4) + ' 0.0 0.0 0.0' +\\\n            str(master_event_id).rjust(4)\n        fphase.write(header + '\\n')\n        for pick in master_event.picks:\n            if not hasattr(pick, 'phase_hint') or len(pick.phase_hint) == 0:\n                warnings.warn('No phase-hint for pick:')\n                print(pick)\n                continue\n            if pick.phase_hint[0].upper() in ['P', 'S']:\n                weight = [arrival.time_weight\n                          for arrival in master_event.origins[0].arrivals\n                          if arrival.pick_id == pick.resource_id][0]\n                # Convert seisan weight to hypoDD 0-1 weights\n                if weight == 0:\n                    weight = 1.0\n                elif weight == 9:\n                    weight = 0.0\n                else:\n                    weight = 1 - weight / 4.0\n                fphase.write(pick.waveform_id.station_code + '  ' +\n                             _cc_round(pick.time -\n                                       master_ori_time, 3).rjust(6) +\n                             '   ' + str(weight).ljust(5) +\n                             pick.phase_hint + '\\n')\n        for j in range(i + 1, len(event_list)):\n            # Use this tactic to only output unique event pairings\n            slave_sfile = event_list[j][1]\n            slave_event_id = event_list[j][0]\n            # Write out the header line\n            event_text = '#' + str(master_event_id).rjust(10) +\\\n                str(slave_event_id).rjust(10) + '\\n'\n            event_text2 = '#' + str(master_event_id).rjust(10) +\\\n                str(slave_event_id).rjust(10) + '\\n'\n            slave_event = read_nordic(slave_sfile)[0]\n            slave_ori_time = slave_event.origins[0].time\n            slave_location = (slave_event.origins[0].latitude,\n                              slave_event.origins[0].longitude,\n                              slave_event.origins[0].depth / 1000)\n            if dist_calc(master_location, slave_location) > max_sep:\n                continue\n            links = 0  # Count the number of linkages\n            for pick in master_event.picks:\n                if not hasattr(pick, 'phase_hint') or\\\n                                len(pick.phase_hint) == 0:\n                    continue\n                if pick.phase_hint[0].upper() not in ['P', 'S']:\n                    continue\n                    # Only use P and S picks, not amplitude or 'other'\n                # Added by Carolin\n                slave_matches = [p for p in slave_event.picks\n                                 if hasattr(p, 'phase_hint') and\n                                 p.phase_hint == pick.phase_hint and\n                                 p.waveform_id.station_code.upper() ==\n                                 pick.waveform_id.station_code.upper()]\n                # Loop through the matches\n                for slave_pick in slave_matches:\n                    links += 1\n                    master_weight = [arrival.time_weight\n                                     for arrival in master_event.\n                                     origins[0].arrivals\n                                     if arrival.pick_id == pick.resource_id][0]\n                    slave_weight = [arrival.time_weight\n                                    for arrival in slave_event.\n                                    origins[0].arrivals\n                                    if arrival.pick_id ==\n                                    slave_pick.resource_id][0]\n                    master_weight = str(int(master_weight))\n                    slave_weight = str(int(slave_weight))\n                    event_text += pick.waveform_id.station_code.ljust(5) +\\\n                        _cc_round(pick.time - master_ori_time, 3).rjust(11) +\\\n                        _cc_round(slave_pick.time -\n                                  slave_ori_time, 3).rjust(8) +\\\n                        _av_weight(master_weight, slave_weight).rjust(7) +\\\n                        ' ' + pick.phase_hint + '\\n'\n                    # Added by Carolin\n                    event_text2 += pick.waveform_id.station_code.ljust(5) +\\\n                        _cc_round(pick.time - master_ori_time, 3).rjust(11) +\\\n                        _cc_round(slave_pick.time -\n                                  slave_ori_time, 3).rjust(8) +\\\n                        _av_weight(master_weight, slave_weight).rjust(7) +\\\n                        ' ' + pick.phase_hint + '\\n'\n                    stations.append(pick.waveform_id.station_code)\n            if links >= min_link:\n                f.write(event_text)\n                f2.write(event_text2)\n                evcount += 1\n    print('You have ' + str(evcount) + ' links')\n    # f.write('\\n')\n    f.close()\n    f2.close()\n    fphase.close()\n    return list(set(stations))", "response": "This function generates a dt. ct2 for a series of events."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a dt.cc file for hypoDD input for a given list of events. Takes an input list of events and computes pick refinements by correlation. Outputs two files, dt.cc and dt.cc2, each provides a different weight, dt.cc uses weights of the cross-correlation, and dt.cc2 provides weights as the square of the cross-correlation. :type event_list: list :param event_list: List of tuples of event_id (int) and sfile (String) :type wavbase: str :param wavbase: Path to the seisan wave directory that the wavefiles in the S-files are stored :type extract_len: float :param extract_len: Length in seconds to extract around the pick :type pre_pick: float :param pre_pick: Time before the pick to start the correlation window :type shift_len: float :param shift_len: Time to allow pick to vary :type lowcut: float :param lowcut: Lowcut in Hz - default=1.0 :type highcut: float :param highcut: Highcut in Hz - default=10.0 :type max_sep: float :param max_sep: Maximum separation between event pairs in km :type min_link: int :param min_link: Minimum links for an event to be paired :type cc_thresh: float :param cc_thresh: Threshold to include cross-correlation results. :type plotvar: bool :param plotvar: To show the pick-correction plots, defualts to False. :type debug: int :param debug: Variable debug levels from 0-5, higher=more output. .. warning:: This is not a fast routine! .. warning:: In contrast to seisan's corr routine, but in accordance with the hypoDD manual, this outputs corrected differential time. .. note:: Currently we have not implemented a method for taking unassociated event objects and wavefiles. As such if you have events \\ with associated wavefiles you are advised to generate Sfiles for each \\ event using the sfile_util module prior to this step. .. note:: There is no provision to taper waveforms within these functions, if you desire this functionality, you should apply the taper before calling this. Note the :func:`obspy.Trace.taper` functions.", "response": "def write_correlations(event_list, wavbase, extract_len, pre_pick, shift_len,\n                       lowcut=1.0, highcut=10.0, max_sep=8, min_link=8,\n                       cc_thresh=0.0, plotvar=False, debug=0):\n    \"\"\"\n    Write a dt.cc file for hypoDD input for a given list of events.\n\n    Takes an input list of events and computes pick refinements by correlation.\n    Outputs two files, dt.cc and dt.cc2, each provides a different weight,\n    dt.cc uses weights of the cross-correlation, and dt.cc2 provides weights\n    as the square of the cross-correlation.\n\n    :type event_list: list\n    :param event_list: List of tuples of event_id (int) and sfile (String)\n    :type wavbase: str\n    :param wavbase: Path to the seisan wave directory that the wavefiles in the\n                    S-files are stored\n    :type extract_len: float\n    :param extract_len: Length in seconds to extract around the pick\n    :type pre_pick: float\n    :param pre_pick: Time before the pick to start the correlation window\n    :type shift_len: float\n    :param shift_len: Time to allow pick to vary\n    :type lowcut: float\n    :param lowcut: Lowcut in Hz - default=1.0\n    :type highcut: float\n    :param highcut: Highcut in Hz - default=10.0\n    :type max_sep: float\n    :param max_sep: Maximum separation between event pairs in km\n    :type min_link: int\n    :param min_link: Minimum links for an event to be paired\n    :type cc_thresh: float\n    :param cc_thresh: Threshold to include cross-correlation results.\n    :type plotvar: bool\n    :param plotvar: To show the pick-correction plots, defualts to False.\n    :type debug: int\n    :param debug: Variable debug levels from 0-5, higher=more output.\n\n    .. warning:: This is not a fast routine!\n\n    .. warning::\n        In contrast to seisan's corr routine, but in accordance with the\n        hypoDD manual, this outputs corrected differential time.\n\n    .. note::\n        Currently we have not implemented a method for taking\n        unassociated event objects and wavefiles.  As such if you have events \\\n        with associated wavefiles you are advised to generate Sfiles for each \\\n        event using the sfile_util module prior to this step.\n\n    .. note::\n        There is no provision to taper waveforms within these functions, if you\n        desire this functionality, you should apply the taper before calling\n        this.  Note the :func:`obspy.Trace.taper` functions.\n    \"\"\"\n    warnings.filterwarnings(action=\"ignore\",\n                            message=\"Maximum of cross correlation \" +\n                                    \"lower than 0.8: *\")\n    corr_list = []\n    f = open('dt.cc', 'w')\n    f2 = open('dt.cc2', 'w')\n    k_events = len(list(event_list))\n    for i, master in enumerate(event_list):\n        master_sfile = master[1]\n        if debug > 1:\n            print('Computing correlations for master: %s' % master_sfile)\n        master_event_id = master[0]\n        master_event = read_nordic(master_sfile)[0]\n        master_picks = master_event.picks\n        master_ori_time = master_event.origins[0].time\n        master_location = (master_event.origins[0].latitude,\n                           master_event.origins[0].longitude,\n                           master_event.origins[0].depth / 1000.0)\n        master_wavefiles = readwavename(master_sfile)\n        masterpath = glob.glob(wavbase + os.sep + master_wavefiles[0])\n        if masterpath:\n            masterstream = read(masterpath[0])\n        if len(master_wavefiles) > 1:\n            for wavefile in master_wavefiles:\n                try:\n                    masterstream += read(os.join(wavbase, wavefile))\n                except:\n                    raise IOError(\"Couldn't find wavefile\")\n                    continue\n        for j in range(i + 1, k_events):\n            # Use this tactic to only output unique event pairings\n            slave_sfile = event_list[j][1]\n            if debug > 2:\n                print('Comparing to event: %s' % slave_sfile)\n            slave_event_id = event_list[j][0]\n            slave_wavefiles = readwavename(slave_sfile)\n            try:\n                slavestream = read(wavbase + os.sep + slave_wavefiles[0])\n            except:\n                raise IOError('No wavefile found: ' + slave_wavefiles[0] +\n                              ' ' + slave_sfile)\n            if len(slave_wavefiles) > 1:\n                for wavefile in slave_wavefiles:\n                    try:\n                        slavestream += read(wavbase + os.sep + wavefile)\n                    except IOError:\n                        print('No waveform found: %s' %\n                              (wavbase + os.sep + wavefile))\n                        continue\n            # Write out the header line\n            event_text = '#' + str(master_event_id).rjust(10) +\\\n                str(slave_event_id).rjust(10) + ' 0.0   \\n'\n            event_text2 = '#' + str(master_event_id).rjust(10) +\\\n                str(slave_event_id).rjust(10) + ' 0.0   \\n'\n            slave_event = read_nordic(slave_sfile)[0]\n            slave_picks = slave_event.picks\n            slave_ori_time = slave_event.origins[0].time\n            slave_location = (slave_event.origins[0].latitude,\n                              slave_event.origins[0].longitude,\n                              slave_event.origins[0].depth / 1000.0)\n            if dist_calc(master_location, slave_location) > max_sep:\n                if debug > 0:\n                    print('Seperation exceeds max_sep: %s' %\n                          (dist_calc(master_location, slave_location)))\n                continue\n            links = 0\n            phases = 0\n            for pick in master_picks:\n                if not hasattr(pick, 'phase_hint') or \\\n                                len(pick.phase_hint) == 0:\n                    warnings.warn('No phase-hint for pick:')\n                    print(pick)\n                    continue\n                if pick.phase_hint[0].upper() not in ['P', 'S']:\n                    warnings.warn('Will only use P or S phase picks')\n                    print(pick)\n                    continue\n                    # Only use P and S picks, not amplitude or 'other'\n                # Find station, phase pairs\n                # Added by Carolin\n                slave_matches = [p for p in slave_picks\n                                 if hasattr(p, 'phase_hint') and\n                                 p.phase_hint == pick.phase_hint and\n                                 p.waveform_id.station_code ==\n                                 pick.waveform_id.station_code]\n\n                if masterstream.select(station=pick.waveform_id.station_code,\n                                       channel='*' +\n                                       pick.waveform_id.channel_code[-1]):\n                    mastertr = masterstream.\\\n                        select(station=pick.waveform_id.station_code,\n                               channel='*' +\n                               pick.waveform_id.channel_code[-1])[0]\n                elif debug > 1:\n                    print('No waveform data for ' +\n                          pick.waveform_id.station_code + '.' +\n                          pick.waveform_id.channel_code)\n                    print(pick.waveform_id.station_code +\n                          '.' + pick.waveform_id.channel_code +\n                          ' ' + slave_sfile + ' ' + master_sfile)\n                    break\n                # Loop through the matches\n                for slave_pick in slave_matches:\n                    if slavestream.select(station=slave_pick.waveform_id.\n                                          station_code,\n                                          channel='*' + slave_pick.waveform_id.\n                                          channel_code[-1]):\n                        slavetr = slavestream.\\\n                            select(station=slave_pick.waveform_id.station_code,\n                                   channel='*' + slave_pick.waveform_id.\n                                   channel_code[-1])[0]\n                    else:\n                        print('No slave data for ' +\n                              slave_pick.waveform_id.station_code + '.' +\n                              slave_pick.waveform_id.channel_code)\n                        print(pick.waveform_id.station_code +\n                              '.' + pick.waveform_id.channel_code +\n                              ' ' + slave_sfile + ' ' + master_sfile)\n                        break\n                    # Correct the picks\n                    try:\n                        correction, cc =\\\n                            xcorr_pick_correction(\n                                pick.time, mastertr, slave_pick.time,\n                                slavetr, pre_pick, extract_len - pre_pick,\n                                shift_len, filter=\"bandpass\",\n                                filter_options={'freqmin': lowcut,\n                                                'freqmax': highcut},\n                                plot=plotvar)\n                        # Get the differential travel time using the\n                        # corrected time.\n                        # Check that the correction is within the allowed shift\n                        # This can occur in the obspy routine when the\n                        # correlation function is increasing at the end of the\n                        # window.\n                        if abs(correction) > shift_len:\n                            warnings.warn('Shift correction too large, ' +\n                                          'will not use')\n                            continue\n                        correction = (pick.time - master_ori_time) -\\\n                            (slave_pick.time + correction - slave_ori_time)\n                        links += 1\n                        if cc >= cc_thresh:\n                            weight = cc\n                            phases += 1\n                            # added by Caro\n                            event_text += pick.waveform_id.station_code.\\\n                                ljust(5) + _cc_round(correction, 3).\\\n                                rjust(11) + _cc_round(weight, 3).rjust(8) +\\\n                                ' ' + pick.phase_hint + '\\n'\n                            event_text2 += pick.waveform_id.station_code\\\n                                .ljust(5) + _cc_round(correction, 3).\\\n                                rjust(11) +\\\n                                _cc_round(weight * weight, 3).rjust(8) +\\\n                                ' ' + pick.phase_hint + '\\n'\n                            if debug > 3:\n                                print(event_text)\n                        else:\n                            print('cc too low: %s' % cc)\n                        corr_list.append(cc * cc)\n                    except:\n                        msg = \"Couldn't compute correlation correction\"\n                        warnings.warn(msg)\n                        continue\n            if links >= min_link and phases > 0:\n                f.write(event_text)\n                f2.write(event_text2)\n    if plotvar:\n        plt.hist(corr_list, 150)\n        plt.show()\n    # f.write('\\n')\n    f.close()\n    f2.close()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_phase(ph_file):\n    ph_catalog = Catalog()\n    f = open(ph_file, 'r')\n    # Topline of each event is marked by # in position 0\n    for line in f:\n        if line[0] == '#':\n            if 'event_text' not in locals():\n                event_text = {'header': line.rstrip(),\n                              'picks': []}\n            else:\n                ph_catalog.append(_phase_to_event(event_text))\n                event_text = {'header': line.rstrip(),\n                              'picks': []}\n        else:\n            event_text['picks'].append(line.rstrip())\n    ph_catalog.append(_phase_to_event(event_text))\n    return ph_catalog", "response": "Reads hypoDD phase files into Obspy catalog class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _phase_to_event(event_text):\n    ph_event = Event()\n    # Extract info from header line\n    # YR, MO, DY, HR, MN, SC, LAT, LON, DEP, MAG, EH, EZ, RMS, ID\n    header = event_text['header'].split()\n    ph_event.origins.append(Origin())\n    ph_event.origins[0].time =\\\n        UTCDateTime(year=int(header[1]), month=int(header[2]),\n                    day=int(header[3]), hour=int(header[4]),\n                    minute=int(header[5]), second=int(header[6].split('.')[0]),\n                    microsecond=int(float(('0.' + header[6].split('.')[1])) *\n                                    1000000))\n    ph_event.origins[0].latitude = float(header[7])\n    ph_event.origins[0].longitude = float(header[8])\n    ph_event.origins[0].depth = float(header[9]) * 1000\n    ph_event.origins[0].quality = OriginQuality(\n        standard_error=float(header[13]))\n    ph_event.magnitudes.append(Magnitude())\n    ph_event.magnitudes[0].mag = float(header[10])\n    ph_event.magnitudes[0].magnitude_type = 'M'\n    # Extract arrival info from picks!\n    for i, pick_line in enumerate(event_text['picks']):\n        pick = pick_line.split()\n        _waveform_id = WaveformStreamID(station_code=pick[0])\n        pick_time = ph_event.origins[0].time + float(pick[1])\n        ph_event.picks.append(Pick(waveform_id=_waveform_id,\n                                   phase_hint=pick[3],\n                                   time=pick_time))\n        ph_event.origins[0].arrivals.append(Arrival(phase=ph_event.picks[i],\n                                                    pick_id=ph_event.picks[i].\n                                                    resource_id))\n        ph_event.origins[0].arrivals[i].time_weight = float(pick[2])\n    return ph_event", "response": "Function to convert the text for one event in hypoDD phase format to obspy. core. event. Event object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a list of waveforms for use as templates.", "response": "def template_gen(method, lowcut, highcut, samp_rate, filt_order,\n                 length, prepick, swin, process_len=86400,\n                 all_horiz=False, delayed=True, plot=False, debug=0,\n                 return_event=False, min_snr=None, parallel=False,\n                 num_cores=False, save_progress=False, **kwargs):\n    \"\"\"\n    Generate processed and cut waveforms for use as templates.\n\n    :type method: str\n    :param method:\n        Template generation method, must be one of ('from_client',\n        'from_seishub', 'from_sac', 'from_meta_file'). - Each method requires\n        associated arguments, see note below.\n    :type lowcut: float\n    :param lowcut: Low cut (Hz), if set to None will not apply a lowcut.\n    :type highcut: float\n    :param highcut: High cut (Hz), if set to None will not apply a highcut.\n    :type samp_rate: float\n    :param samp_rate: New sampling rate in Hz.\n    :type filt_order: int\n    :param filt_order: Filter level (number of corners).\n    :type length: float\n    :param length: Extract length in seconds.\n    :type prepick: float\n    :param prepick: Pre-pick time in seconds\n    :type swin: str\n    :param swin:\n        P, S, P_all, S_all or all, defaults to all: see note in\n        :func:`eqcorrscan.core.template_gen.template_gen`\n    :type process_len: int\n    :param process_len: Length of data in seconds to download and process.\n    :type all_horiz: bool\n    :param all_horiz: To use both horizontal channels even if there is only \\\n        a pick on one of them.  Defaults to False.\n    :type delayed: bool\n    :param delayed: If True, each channel will begin relative to it's own \\\n        pick-time, if set to False, each channel will begin at the same time.\n    :type plot: bool\n    :param plot: Plot templates or not.\n    :type debug: int\n    :param debug: Level of debugging output, higher=more\n    :type return_event: bool\n    :param return_event: Whether to return the event and process length or not.\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio for a channel to be included in the\n        template, where signal-to-noise ratio is calculated as the ratio of\n        the maximum amplitude in the template window to the rms amplitude in\n        the whole window given.\n    :type parallel: bool\n    :param parallel: Whether to process data in parallel or not.\n    :type num_cores: int\n    :param num_cores:\n        Number of cores to try and use, if False and parallel=True, will use\n        either all your cores, or as many traces as in the data (whichever is\n        smaller).\n    :type save_progress: bool\n    :param save_progress:\n        Whether to save the resulting party at every data step or not.\n        Useful for long-running processes.\n\n    :returns: List of :class:`obspy.core.stream.Stream` Templates\n    :rtype: list\n\n    .. note::\n        *Method specific arguments:*\n\n        - `from_client` requires:\n            :param str client_id: string passable by obspy to generate Client\n            :param `obspy.core.event.Catalog` catalog:\n                Catalog of events to generate template for\n            :param float data_pad: Pad length for data-downloads in seconds\n        - `from_seishub` requires:\n            :param str url: url to seishub database\n            :param `obspy.core.event.Catalog` catalog:\n                Catalog of events to generate template for\n            :param float data_pad: Pad length for data-downloads in seconds\n        - `from_sac` requires:\n            :param list sac_files:\n                osbpy.core.stream.Stream of sac waveforms, or list of paths to\n                sac waveforms.\n        - `from_meta_file` requires:\n            :param str meta_file: Path to obspy-readable event file.\n            :param `obspy.core.stream.Stream` st:\n                Stream containing waveform data for template. Note that this\n                should be the same length of stream as you will use for the\n                continuous detection, e.g. if you detect in day-long files,\n                give this a day-long file!\n            :param bool process:\n                Whether to process the data or not, defaults to True.\n\n    .. note::\n        process_len should be set to the same length as used when computing\n        detections using match_filter.match_filter, e.g. if you read\n        in day-long data for match_filter, process_len should be 86400.\n\n        .. rubric:: Example\n\n    >>> from obspy.clients.fdsn import Client\n    >>> from eqcorrscan.core.template_gen import template_gen\n    >>> client = Client('NCEDC')\n    >>> catalog = client.get_events(eventid='72572665', includearrivals=True)\n    >>> # We are only taking two picks for this example to speed up the\n    >>> # example, note that you don't have to!\n    >>> catalog[0].picks = catalog[0].picks[0:2]\n    >>> templates = template_gen(\n    ...    method='from_client', catalog=catalog, client_id='NCEDC',\n    ...    lowcut=2.0, highcut=9.0, samp_rate=20.0, filt_order=4, length=3.0,\n    ...    prepick=0.15, swin='all', process_len=300, all_horiz=True)\n    >>> templates[0].plot(equal_scale=False, size=(800,600)) # doctest: +SKIP\n\n    .. figure:: ../../plots/template_gen.from_client.png\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> from eqcorrscan.core.template_gen import template_gen\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> st = read(TEST_PATH + '/WAV/TEST_/' +\n    ...           '2013-09-01-0410-35.DFDPC_024_00')\n    >>> quakeml = TEST_PATH + '/20130901T041115.xml'\n    >>> templates = template_gen(\n    ...    method='from_meta_file', meta_file=quakeml, st=st, lowcut=2.0,\n    ...    highcut=9.0, samp_rate=20.0, filt_order=3, length=2, prepick=0.1,\n    ...    swin='S', all_horiz=True)\n    >>> print(len(templates[0]))\n    10\n    >>> templates = template_gen(\n    ...    method='from_meta_file', meta_file=quakeml, st=st, lowcut=2.0,\n    ...    highcut=9.0, samp_rate=20.0, filt_order=3, length=2, prepick=0.1,\n    ...    swin='S_all', all_horiz=True)\n    >>> print(len(templates[0]))\n    15\n\n    .. rubric:: Example\n\n    >>> from eqcorrscan.core.template_gen import template_gen\n    >>> import glob\n    >>> # Get all the SAC-files associated with one event.\n    >>> sac_files = glob.glob(TEST_PATH + '/SAC/2014p611252/*')\n    >>> templates = template_gen(\n    ...    method='from_sac', sac_files=sac_files, lowcut=2.0, highcut=10.0,\n    ...    samp_rate=25.0, filt_order=4, length=2.0, swin='all', prepick=0.1,\n    ...    all_horiz=True)\n    >>> print(templates[0][0].stats.sampling_rate)\n    25.0\n    >>> print(len(templates[0]))\n    15\n    \"\"\"\n    client_map = {'from_client': 'fdsn', 'from_seishub': 'seishub'}\n    assert method in ('from_client', 'from_seishub', 'from_meta_file',\n                      'from_sac')\n    if not isinstance(swin, list):\n        swin = [swin]\n    process = True\n    if method in ['from_client', 'from_seishub']:\n        catalog = kwargs.get('catalog', Catalog())\n        data_pad = kwargs.get('data_pad', 90)\n        # Group catalog into days and only download the data once per day\n        sub_catalogs = _group_events(\n            catalog=catalog, process_len=process_len, template_length=length,\n            data_pad=data_pad)\n        if method == 'from_client':\n            client = FDSNClient(kwargs.get('client_id', None))\n            available_stations = []\n        else:\n            client = SeisHubClient(kwargs.get('url', None), timeout=10)\n            available_stations = client.waveform.get_station_ids()\n    elif method == 'from_meta_file':\n        if isinstance(kwargs.get('meta_file'), Catalog):\n            catalog = kwargs.get('meta_file')\n        elif kwargs.get('meta_file'):\n            catalog = read_events(kwargs.get('meta_file'))\n        elif kwargs.get('catalog'):\n            catalog = kwargs.get('catalog')\n        sub_catalogs = [catalog]\n        st = kwargs.get('st', Stream())\n        process = kwargs.get('process', True)\n    elif method == 'from_sac':\n        sac_files = kwargs.get('sac_files')\n        if isinstance(sac_files, list):\n            if isinstance(sac_files[0], (Stream, Trace)):\n                # This is a list of streams...\n                st = Stream(sac_files[0])\n                for sac_file in sac_files[1:]:\n                    st += sac_file\n            else:\n                sac_files = [read(sac_file)[0] for sac_file in sac_files]\n                st = Stream(sac_files)\n        else:\n            st = sac_files\n        # Make an event object...\n        catalog = Catalog([sactoevent(st, debug=debug)])\n        sub_catalogs = [catalog]\n\n    temp_list = []\n    process_lengths = []\n\n    if \"P_all\" in swin or \"S_all\" in swin or all_horiz:\n        all_channels = True\n    else:\n        all_channels = False\n    for sub_catalog in sub_catalogs:\n        if method in ['from_seishub', 'from_client']:\n            debug_print(\"Downloading data\", 1, debug)\n            st = _download_from_client(\n                client=client, client_type=client_map[method],\n                catalog=sub_catalog, data_pad=data_pad,\n                process_len=process_len, available_stations=available_stations,\n                all_channels=all_channels, debug=debug)\n        debug_print('Pre-processing data', 0, debug)\n        st.merge()\n        if process:\n            data_len = max([len(tr.data) / tr.stats.sampling_rate\n                            for tr in st])\n            if 80000 < data_len < 90000:\n                daylong = True\n                starttime = min([tr.stats.starttime for tr in st])\n                min_delta = min([tr.stats.delta for tr in st])\n                # Cope with the common starttime less than 1 sample before the\n                #  start of day.\n                if (starttime + min_delta).date > starttime.date:\n                    starttime = (starttime + min_delta)\n                # Check if this is stupid:\n                if abs(starttime - UTCDateTime(starttime.date)) > 600:\n                    print(abs(starttime - UTCDateTime(starttime.date)))\n                    daylong = False\n                starttime = starttime.date\n            else:\n                daylong = False\n            if daylong:\n                st = pre_processing.dayproc(\n                    st=st, lowcut=lowcut, highcut=highcut,\n                    filt_order=filt_order, samp_rate=samp_rate, debug=debug,\n                    parallel=parallel, starttime=UTCDateTime(starttime),\n                    num_cores=num_cores)\n            else:\n                st = pre_processing.shortproc(\n                    st=st, lowcut=lowcut, highcut=highcut,\n                    filt_order=filt_order, parallel=parallel,\n                    samp_rate=samp_rate, debug=debug, num_cores=num_cores)\n        data_start = min([tr.stats.starttime for tr in st])\n        data_end = max([tr.stats.endtime for tr in st])\n\n        for event in sub_catalog:\n            stations, channels, st_stachans = ([], [], [])\n            if len(event.picks) == 0:\n                debug_print('No picks for event {0}'.format(event.resource_id),\n                            2, debug)\n                continue\n            use_event = True\n            # Check that the event is within the data\n            for pick in event.picks:\n                if not data_start < pick.time < data_end:\n                    debug_print(\"Pick outside of data span:\\nPick time %s\\n\"\n                                \"Start time %s\\nEnd time: %s\" %\n                                (str(pick.time), str(data_start),\n                                 str(data_end)), 0, debug)\n                    use_event = False\n            if not use_event:\n                debug_print('Event is not within data time-span', 2, debug)\n                continue\n            # Read in pick info\n            debug_print(\"I have found the following picks\", 0, debug)\n            for pick in event.picks:\n                if not pick.waveform_id:\n                    debug_print(\n                        'Pick not associated with waveforms, will not use:'\n                        ' {0}'.format(pick), 1, debug)\n                    continue\n                debug_print(pick, 0, debug)\n                stations.append(pick.waveform_id.station_code)\n                channels.append(pick.waveform_id.channel_code)\n            # Check to see if all picks have a corresponding waveform\n            for tr in st:\n                st_stachans.append('.'.join([tr.stats.station,\n                                             tr.stats.channel]))\n            # Cut and extract the templates\n            template = _template_gen(\n                event.picks, st, length, swin, prepick=prepick, plot=plot,\n                debug=debug, all_horiz=all_horiz, delayed=delayed,\n                min_snr=min_snr)\n            process_lengths.append(len(st[0].data) / samp_rate)\n            temp_list.append(template)\n        if save_progress:\n            if not os.path.isdir(\"eqcorrscan_temporary_templates\"):\n                os.makedirs(\"eqcorrscan_temporary_templates\")\n            for template in temp_list:\n                template.write(\n                    \"eqcorrscan_temporary_templates{0}{1}.ms\".format(\n                        os.path.sep, template[0].stats.starttime),\n                    format=\"MSEED\")\n        del st\n    if return_event:\n        return temp_list, catalog, process_lengths\n    return temp_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_from_stack(stack, template, length, pre_pick, pre_pad,\n                       Z_include=False, pre_processed=True, samp_rate=None,\n                       lowcut=None, highcut=None, filt_order=3):\n    \"\"\"\n    Extract a multiplexed template from a stack of detections.\n\n    Function to extract a new template from a stack of previous detections.\n    Requires the stack, the template used to make the detections for the \\\n    stack, and we need to know if the stack has been pre-processed.\n\n    :type stack: obspy.core.stream.Stream\n    :param stack: Waveform stack from detections.  Can be of any length and \\\n        can have delays already included, or not.\n    :type template: obspy.core.stream.Stream\n    :param template: Template used to make the detections in the stack. Will \\\n        use the delays of this for the new template.\n    :type length: float\n    :param length: Length of new template in seconds\n    :type pre_pick: float\n    :param pre_pick: Extract additional data before the detection, seconds\n    :type pre_pad: float\n    :param pre_pad: Pad used in seconds when extracting the data, e.g. the \\\n        time before the detection extracted.  If using \\\n        clustering.extract_detections this half the length of the extracted \\\n        waveform.\n    :type Z_include: bool\n    :param Z_include: If True will include any Z-channels even if there is \\\n        no template for this channel, as long as there is a template for this \\\n        station at a different channel.  If this is False and Z channels are \\\n        included in the template Z channels will be included in the \\\n        new_template anyway.\n    :type pre_processed: bool\n    :param pre_processed: Have the data been pre-processed, if True (default) \\\n        then we will only cut the data here.\n    :type samp_rate: float\n    :param samp_rate: If pre_processed=False then this is required, desired \\\n        sampling rate in Hz, defaults to False.\n    :type lowcut: float\n    :param lowcut: If pre_processed=False then this is required, lowcut in \\\n        Hz, defaults to False.\n    :type highcut: float\n    :param highcut: If pre_processed=False then this is required, highcut in \\\n        Hz, defaults to False\n    :type filt_order: int\n    :param filt_order: If pre_processed=False then this is required, filter \\\n        order, defaults to False\n\n    :returns: Newly cut template.\n    :rtype: :class:`obspy.core.stream.Stream`\n    \"\"\"\n    new_template = stack.copy()\n    # Copy the data before we trim it to keep the stack safe\n    # Get the earliest time in the template as this is when the detection is\n    # taken.\n    mintime = min([tr.stats.starttime for tr in template])\n    # Generate a list of tuples of (station, channel, delay) with delay in\n    # seconds\n    delays = [(tr.stats.station, tr.stats.channel[-1],\n               tr.stats.starttime - mintime) for tr in template]\n\n    #  Process the data if necessary\n    if not pre_processed:\n        new_template = pre_processing.shortproc(\n            st=new_template, lowcut=lowcut, highcut=highcut,\n            filt_order=filt_order, samp_rate=samp_rate, debug=0)\n    # Loop through the stack and trim!\n    out = Stream()\n    for tr in new_template:\n        # Find the matching delay\n        delay = [d[2] for d in delays if d[0] == tr.stats.station and\n                 d[1] == tr.stats.channel[-1]]\n        if Z_include and len(delay) == 0:\n            delay = [d[2] for d in delays if d[0] == tr.stats.station]\n        if len(delay) == 0:\n            debug_print(\"No matching template channel found for stack channel\"\n                        \" {0}.{1}\".format(tr.stats.station, tr.stats.channel),\n                        2, 3)\n            new_template.remove(tr)\n        else:\n            for d in delay:\n                out += tr.copy().trim(\n                    starttime=tr.stats.starttime + d + pre_pad - pre_pick,\n                    endtime=tr.stats.starttime + d + pre_pad + length -\n                    pre_pick)\n    return out", "response": "This function extracts a new template from a stack of detections."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _template_gen(picks, st, length, swin='all', prepick=0.05,\n                  all_horiz=False, delayed=True, plot=False, min_snr=None,\n                  debug=0):\n    \"\"\"\n    Master function to generate a multiplexed template for a single event.\n\n    Function to generate a cut template as :class:`obspy.core.stream.Stream`\n    from a given set of picks and data.  Should be given pre-processed\n    data (downsampled and filtered).\n\n    :type picks: list\n    :param picks: Picks to extract data around, where each pick in the \\\n        list is an obspy.core.event.origin.Pick object.\n    :type st: obspy.core.stream.Stream\n    :param st: Stream to extract templates from\n    :type length: float\n    :param length: Length of template in seconds\n    :type swin: str\n    :param swin:\n        P, S, P_all, S_all or all, defaults to all: see note in\n        :func:`eqcorrscan.core.template_gen.template_gen`\n    :type prepick: float\n    :param prepick:\n        Length in seconds to extract before the pick time default is 0.05\n        seconds.\n    :type all_horiz: bool\n    :param all_horiz:\n        To use both horizontal channels even if there is only a pick on one\n        of them.  Defaults to False.\n    :type delayed: bool\n    :param delayed:\n        If True, each channel will begin relative to it's own pick-time, if\n        set to False, each channel will begin at the same time.\n    :type plot: bool\n    :param plot:\n        To plot the template or not, default is False. Plots are saved as\n        `template-starttime_template.png` and `template-starttime_noise.png`,\n        where `template-starttime` is the start-time of the template\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio for a channel to be included in the\n        template, where signal-to-noise ratio is calculated as the ratio of\n        the maximum amplitude in the template window to the rms amplitude in\n        the whole window given.\n    :type debug: int\n    :param debug: Debug output level from 0-5.\n\n    :returns: Newly cut template.\n    :rtype: :class:`obspy.core.stream.Stream`\n\n    .. note:: By convention templates are generated with P-phases on the \\\n        vertical channel and S-phases on the horizontal channels, normal \\\n        seismograph naming conventions are assumed, where Z denotes vertical \\\n        and N, E, R, T, 1 and 2 denote horizontal channels, either oriented \\\n        or not.  To this end we will **only** use Z channels if they have a \\\n        P-pick, and will use one or other horizontal channels **only** if \\\n        there is an S-pick on it.\n\n    .. note:: swin argument: Setting to `P` will return only data for channels\n        with P picks, starting at the pick time (minus the prepick).\n        Setting to `S` will return only data for channels with\n        S picks, starting at the S-pick time (minus the prepick)\n        (except if `all_horiz=True` when all horizontal channels will\n        be returned if there is an S pick on one of them). Setting to `all`\n        will return channels with either a P or S pick (including both\n        horizontals if `all_horiz=True`) - with this option vertical channels\n        will start at the P-pick (minus the prepick) and horizontal channels\n        will start at the S-pick time (minus the prepick).\n        `P_all` will return cut traces starting at the P-pick time for all\n        channels. `S_all` will return cut traces starting at the S-pick\n        time for all channels.\n\n    .. warning:: If there is no phase_hint included in picks, and swin=all, \\\n        all channels with picks will be used.\n    \"\"\"\n    from eqcorrscan.utils.debug_log import debug_print\n    from eqcorrscan.utils.plotting import pretty_template_plot as tplot\n    from eqcorrscan.utils.plotting import noise_plot\n    from eqcorrscan.core.bright_lights import _rms\n    picks_copy = copy.deepcopy(picks)  # Work on a copy of the picks and leave\n    # the users picks intact.\n    if not isinstance(swin, list):\n        swin = [swin]\n    for _swin in swin:\n        assert _swin in ['P', 'all', 'S', 'P_all', 'S_all']\n    for pick in picks_copy:\n        if not pick.waveform_id:\n            debug_print(\n                \"Pick not associated with waveform, will not use it: \"\n                \"{0}\".format(pick), 1, debug)\n            picks_copy.remove(pick)\n            continue\n        if not pick.waveform_id.station_code or not \\\n                pick.waveform_id.channel_code:\n            debug_print(\n                \"Pick not associated with a channel, will not use it:\"\n                \" {0}\".format(pick), 1, debug)\n            picks_copy.remove(pick)\n            continue\n    for tr in st:\n        # Check that the data can be represented by float16, and check they\n        # are not all zeros\n        if np.all(tr.data.astype(np.float16) == 0):\n            debug_print(\"Trace is all zeros at float16 level, either gain or \"\n                        \"check. Not using in template: {0}\".format(tr), 4,\n                        debug)\n            st.remove(tr)\n    # Get the earliest pick-time and use that if we are not using delayed.\n    picks_copy.sort(key=lambda p: p.time)\n    first_pick = picks_copy[0]\n    if plot:\n        stplot = st.slice(first_pick.time - 20,\n                          first_pick.time + length + 90).copy()\n        noise = stplot.copy()\n    # Work out starttimes\n    starttimes = []\n    for _swin in swin:\n        for tr in st:\n            starttime = {'station': tr.stats.station,\n                         'channel': tr.stats.channel, 'picks': []}\n            station_picks = [pick for pick in picks_copy\n                             if pick.waveform_id.station_code ==\n                             tr.stats.station]\n            if _swin == 'P_all':\n                p_pick = [pick for pick in station_picks\n                          if pick.phase_hint.upper()[0] == 'P']\n                if len(p_pick) == 0:\n                    continue\n                starttime.update({'picks': p_pick})\n            elif _swin == 'S_all':\n                s_pick = [pick for pick in station_picks\n                          if pick.phase_hint.upper()[0] == 'S']\n                if len(s_pick) == 0:\n                    continue\n                starttime.update({'picks': s_pick})\n            elif _swin == 'all':\n                if all_horiz and tr.stats.channel[-1] in ['1', '2', '3',\n                                                          'N', 'E']:\n                    # Get all picks on horizontal channels\n                    channel_pick = [\n                        pick for pick in station_picks\n                        if pick.waveform_id.channel_code[-1] in\n                        ['1', '2', '3', 'N', 'E']]\n                else:\n                    channel_pick = [\n                        pick for pick in station_picks\n                        if pick.waveform_id.channel_code == tr.stats.channel]\n                if len(channel_pick) == 0:\n                    continue\n                starttime.update({'picks': channel_pick})\n            elif _swin == 'P':\n                p_pick = [pick for pick in station_picks\n                          if pick.phase_hint.upper()[0] == 'P' and\n                          pick.waveform_id.channel_code == tr.stats.channel]\n                if len(p_pick) == 0:\n                    continue\n                starttime.update({'picks': p_pick})\n            elif _swin == 'S':\n                if tr.stats.channel[-1] in ['Z', 'U']:\n                    continue\n                s_pick = [pick for pick in station_picks\n                          if pick.phase_hint.upper()[0] == 'S']\n                if not all_horiz:\n                    s_pick = [pick for pick in s_pick\n                              if pick.waveform_id.channel_code ==\n                              tr.stats.channel]\n                starttime.update({'picks': s_pick})\n                if len(starttime['picks']) == 0:\n                    continue\n            if not delayed:\n                starttime.update({'picks': [first_pick]})\n            starttimes.append(starttime)\n    # Cut the data\n    st1 = Stream()\n    for _starttime in starttimes:\n        debug_print(\"Working on channel %s.%s\" %\n                    (_starttime['station'], _starttime['channel']),\n                    debug_level=0, print_level=debug)\n        tr = st.select(\n            station=_starttime['station'], channel=_starttime['channel'])[0]\n        debug_print(\"Found Trace %s\" % tr.__str__(), debug_level=0,\n                    print_level=debug)\n        noise_amp = _rms(tr.data)\n        used_tr = False\n        for pick in _starttime['picks']:\n            if not pick.phase_hint:\n                warnings.warn(\n                    \"Pick for {0}.{1} has no phase hint given, you should not \"\n                    \"use this template for cross-correlation\"\n                    \" re-picking!\".format(\n                        pick.waveform_id.station_code,\n                        pick.waveform_id.channel_code))\n            starttime = pick.time - prepick\n            debug_print(\n                \"Cutting \" + tr.stats.station + '.' + tr.stats.channel, 0,\n                debug)\n            tr_cut = tr.slice(\n                starttime=starttime, endtime=starttime + length,\n                nearest_sample=False).copy()\n            if plot:\n                noise.select(\n                    station=_starttime['station'],\n                    channel=_starttime['channel']).trim(\n                        noise[0].stats.starttime, starttime)\n            if len(tr_cut.data) == 0:\n                debug_print(\n                    \"No data provided for {0}.{1} starting at {2}\".format(\n                        tr.stats.station, tr.stats.channel, starttime), 3,\n                    debug)\n                continue\n            # Ensure that the template is the correct length\n            if len(tr_cut.data) == (tr_cut.stats.sampling_rate *\n                                    length) + 1:\n                tr_cut.data = tr_cut.data[0:-1]\n            debug_print(\n                'Cut starttime = %s\\nCut endtime %s' %\n                (str(tr_cut.stats.starttime), str(tr_cut.stats.endtime)), 0,\n                debug)\n            if min_snr is not None and \\\n               max(tr_cut.data) / noise_amp < min_snr:\n                debug_print(\n                    \"Signal-to-noise ratio below threshold for {0}.{1}\".format(\n                        tr_cut.stats.station, tr_cut.stats.channel), 3, debug)\n                continue\n            st1 += tr_cut\n            used_tr = True\n        if not used_tr:\n            debug_print('No pick for ' + tr.stats.station + '.' +\n                        tr.stats.channel, 0, debug)\n    if plot:\n        fig1 = tplot(st1, background=stplot, picks=picks_copy,\n                     title='Template for ' + str(st1[0].stats.starttime),\n                     show=False, return_figure=True)\n        fig2 = noise_plot(\n            signal=st1, noise=noise, show=False, return_figure=True)\n        fig1.savefig(\"{0}_template.png\".format(st1[0].stats.starttime))\n        fig2.savefig(\"{0}_noise.png\".format(st1[0].stats.starttime))\n        del(stplot, fig1, fig2)\n    return st1", "response": "This function is used to generate a multiplexed template for a single event. It is used to generate multiple templates for a single event."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates multiple templates from one stream of data. Thin wrapper around _template_gen to generate multiple templates from one stream of continuous data. Takes processed (filtered and resampled) seismic data! :type catalog: obspy.core.event.Catalog :param catalog: Events to extract templates for :type st: obspy.core.stream.Stream :param st: Processed stream to extract from, e.g. filtered and re-sampled to what you want using pre_processing.dayproc. :type length: float :param length: Length of template in seconds :type swin: string :param swin: P, S, P_all, S_all or all, defaults to all: see note in :func:`eqcorrscan.core.template_gen.template_gen` :type prepick: float :param prepick: Length in seconds to extract before the pick time default is 0.05 seconds. :type all_horiz: bool :param all_horiz: To use both horizontal channels even if there is only a pick on one of them. Defaults to False. :type delayed: bool :param delayed: If True, each channel will begin relative to it's own pick-time, if set to False, each channel will begin at the same time. :type plot: bool :param plot: To plot the template or not, default is True :type debug: int :param debug: Debug output level from 0-5. :type return_event: bool :param return_event: Whether to return the event and process length or not. :type min_snr: float :param min_snr: Minimum signal-to-noise ratio for a channel to be included in the template, where signal-to-noise ratio is calculated as the ratio of the maximum amplitude in the template window to the rms amplitude in the whole window given. :returns: List of :class:`obspy.core.stream.Stream` templates. :rtype: list .. warning:: Data must be processed before using this function - highcut, lowcut and filt_order are only used to generate the meta-data for the templates. .. note:: By convention templates are generated with P-phases on the \\ vertical channel and S-phases on the horizontal channels, normal \\ seismograph naming conventions are assumed, where Z denotes vertical \\ and N, E, R, T, 1 and 2 denote horizontal channels, either oriented \\ or not. To this end we will **only** use Z channels if they have a \\ P-pick, and will use one or other horizontal channels **only** if \\ there is an S-pick on it. .. warning:: If there is no phase_hint included in picks, and swin=all, \\ all channels with picks will be used.", "response": "def multi_template_gen(catalog, st, length, swin='all', prepick=0.05,\n                       all_horiz=False, delayed=True, plot=False, debug=0,\n                       return_event=False, min_snr=None):\n    \"\"\"\n    Generate multiple templates from one stream of data.\n\n    Thin wrapper around _template_gen to generate multiple templates from\n    one stream of continuous data.  Takes processed (filtered and resampled)\n    seismic data!\n\n    :type catalog: obspy.core.event.Catalog\n    :param catalog: Events to extract templates for\n    :type st: obspy.core.stream.Stream\n    :param st:\n        Processed stream to extract from, e.g. filtered and re-sampled to what\n        you want using pre_processing.dayproc.\n    :type length: float\n    :param length: Length of template in seconds\n    :type swin: string\n    :param swin:\n        P, S, P_all, S_all or all, defaults to all: see note in\n        :func:`eqcorrscan.core.template_gen.template_gen`\n    :type prepick: float\n    :param prepick:\n        Length in seconds to extract before the pick time default is\n        0.05 seconds.\n    :type all_horiz: bool\n    :param all_horiz:\n        To use both horizontal channels even if there is only a pick on one of\n        them.  Defaults to False.\n    :type delayed: bool\n    :param delayed:\n        If True, each channel will begin relative to it's own pick-time, if set\n         to False, each channel will begin at the same time.\n    :type plot: bool\n    :param plot: To plot the template or not, default is True\n    :type debug: int\n    :param debug: Debug output level from 0-5.\n    :type return_event: bool\n    :param return_event: Whether to return the event and process length or not.\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio for a channel to be included in the\n        template, where signal-to-noise ratio is calculated as the ratio of\n        the maximum amplitude in the template window to the rms amplitude in\n        the whole window given.\n\n    :returns: List of :class:`obspy.core.stream.Stream` templates.\n    :rtype: list\n\n    .. warning::\n        Data must be processed before using this function - highcut, lowcut and\n        filt_order are only used to generate the meta-data for the templates.\n\n    .. note:: By convention templates are generated with P-phases on the \\\n        vertical channel and S-phases on the horizontal channels, normal \\\n        seismograph naming conventions are assumed, where Z denotes vertical \\\n        and N, E, R, T, 1 and 2 denote horizontal channels, either oriented \\\n        or not.  To this end we will **only** use Z channels if they have a \\\n        P-pick, and will use one or other horizontal channels **only** if \\\n        there is an S-pick on it.\n\n    .. warning:: If there is no phase_hint included in picks, and swin=all, \\\n        all channels with picks will be used.\n    \"\"\"\n    EQcorrscanDeprecationWarning(\n        \"Function is depreciated and will be removed soon. Use \"\n        \"template_gen.template_gen instead.\")\n    temp_list = template_gen(\n        method=\"from_meta_file\", process=False, meta_file=catalog, st=st,\n        lowcut=None, highcut=None, samp_rate=st[0].stats.sampling_rate,\n        filt_order=None, length=length, prepick=prepick,\n        swin=swin, all_horiz=all_horiz, delayed=delayed, plot=plot,\n        debug=debug, return_event=return_event, min_snr=min_snr,\n        parallel=False)\n    return temp_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_client(catalog, client_id, lowcut, highcut, samp_rate, filt_order,\n                length, prepick, swin, process_len=86400, data_pad=90,\n                all_horiz=False, delayed=True, plot=False, debug=0,\n                return_event=False, min_snr=None):\n    \"\"\"\n    Generate multiplexed template from FDSN client.\n\n    Function to generate templates from an FDSN client. Must be given \\\n    an obspy.Catalog class and the client_id as input. The function returns \\\n    a list of obspy.Stream classes containing steams for each desired \\\n    template.\n\n    :type catalog: obspy.core.event.Catalog\n    :param catalog: Catalog class containing desired template events\n    :type client_id: str\n    :param client_id: Name of the client, either url, or Obspy \\\n        mappable (see the :mod:`obspy.clients.fdsn` documentation).\n    :type lowcut: float\n    :param lowcut: Low cut (Hz), if set to None will not apply a lowcut.\n    :type highcut: float\n    :param highcut: High cut (Hz), if set to None will not apply a highcut.\n    :type samp_rate: float\n    :param samp_rate: New sampling rate in Hz.\n    :type filt_order: int\n    :param filt_order: Filter level (number of corners).\n    :type length: float\n    :param length: Extract length in seconds.\n    :type prepick: float\n    :param prepick: Pre-pick time in seconds\n    :type swin: str\n    :param swin:\n        P, S, P_all, S_all or all, defaults to all: see note in\n        :func:`eqcorrscan.core.template_gen.template_gen`\n    :type process_len: int\n    :param process_len: Length of data in seconds to download and process.\n    :param data_pad: Length of data (in seconds) required before and after \\\n        any event for processing, use to reduce edge-effects of filtering on \\\n        the templates.\n    :type data_pad: int\n    :type all_horiz: bool\n    :param all_horiz: To use both horizontal channels even if there is only \\\n        a pick on one of them.  Defaults to False.\n    :type delayed: bool\n    :param delayed: If True, each channel will begin relative to it's own \\\n        pick-time, if set to False, each channel will begin at the same time.\n    :type plot: bool\n    :param plot: Plot templates or not.\n    :type debug: int\n    :param debug: Level of debugging output, higher=more\n    :type return_event: bool\n    :param return_event: Whether to return the event and process length or not.\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio for a channel to be included in the\n        template, where signal-to-noise ratio is calculated as the ratio of\n        the maximum amplitude in the template window to the rms amplitude in\n        the whole window given.\n\n    :returns: List of :class:`obspy.core.stream.Stream` Templates\n    :rtype: list\n\n    .. warning::\n        This function is depreciated and will be removed in a forthcoming\n        release. Please use `template_gen` instead.\n\n    .. note::\n        process_len should be set to the same length as used when computing\n        detections using match_filter.match_filter, e.g. if you read\n        in day-long data for match_filter, process_len should be 86400.\n\n    .. rubric:: Example\n\n    >>> from obspy.clients.fdsn import Client\n    >>> from eqcorrscan.core.template_gen import from_client\n    >>> client = Client('NCEDC')\n    >>> catalog = client.get_events(eventid='72572665', includearrivals=True)\n    >>> # We are only taking two picks for this example to speed up the\n    >>> # example, note that you don't have to!\n    >>> catalog[0].picks = catalog[0].picks[0:2]\n    >>> templates = from_client(catalog=catalog, client_id='NCEDC',\n    ...                         lowcut=2.0, highcut=9.0, samp_rate=20.0,\n    ...                         filt_order=4, length=3.0, prepick=0.15,\n    ...                         swin='all', process_len=300,\n    ...                         all_horiz=True)\n    >>> templates[0].plot(equal_scale=False, size=(800,600)) # doctest: +SKIP\n\n    .. figure:: ../../plots/template_gen.from_client.png\n    \"\"\"\n    EQcorrscanDeprecationWarning(\n        \"Function is depreciated and will be removed soon. Use \"\n        \"template_gen.template_gen instead.\")\n    temp_list = template_gen(\n        method=\"from_client\", catalog=catalog, client_id=client_id,\n        lowcut=lowcut, highcut=highcut, samp_rate=samp_rate,\n        filt_order=filt_order, length=length, prepick=prepick,\n        swin=swin, process_len=process_len, data_pad=data_pad,\n        all_horiz=all_horiz, delayed=delayed, plot=plot, debug=debug,\n        return_event=return_event, min_snr=min_snr)\n    return temp_list", "response": "This function generates multiplexed templates from a FDSN client."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a multiplexed template from a list of SAC files.", "response": "def from_sac(sac_files, lowcut, highcut, samp_rate, filt_order, length, swin,\n             prepick, all_horiz=False, delayed=True, plot=False, debug=0,\n             return_event=False, min_snr=None):\n    \"\"\"\n    Generate a multiplexed template from a list of SAC files.\n\n    Function to read picks and waveforms from SAC data, and generate a \\\n    template from these. Usually sac_files is a list of all single-channel \\\n    SAC files for a given event, a single, multi-channel template will be \\\n    created from these traces.\n\n    **All files listed in sac_files should be associated with a single event.**\n\n    :type sac_files: list\n    :param sac_files: osbpy.core.stream.Stream of sac waveforms, or\n        list of paths to sac waveforms.\n    :type lowcut: float\n    :param lowcut: Low cut (Hz), if set to None will not apply a lowcut.\n    :type highcut: float\n    :param highcut: High cut (Hz), if set to None will not apply a highcut.\n    :type samp_rate: float\n    :param samp_rate: New sampling rate in Hz.\n    :type filt_order: int\n    :param filt_order: Filter level.\n    :type length: float\n    :param length: Extract length in seconds.\n    :type swin: str\n    :param swin:\n        P, S, P_all, S_all or all, defaults to all: see note in\n        :func:`eqcorrscan.core.template_gen.template_gen`\n    :type prepick: float\n    :param prepick: Length to extract prior to the pick in seconds.\n    :type all_horiz: bool\n    :param all_horiz: To use both horizontal channels even if there is only \\\n        a pick on one of them.  Defaults to False.\n    :type delayed: bool\n    :param delayed: If True, each channel will begin relative to it's own \\\n        pick-time, if set to False, each channel will begin at the same time.\n    :type plot: bool\n    :param plot: Turns template plotting on or off.\n    :type debug: int\n    :param debug: Debug level, higher number=more output.\n    :type return_event: bool\n    :param return_event: Whether to return the event and process length or not.\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio for a channel to be included in the\n        template, where signal-to-noise ratio is calculated as the ratio of\n        the maximum amplitude in the template window to the rms amplitude in\n        the whole window given.\n\n    :returns: Newly cut template.\n    :rtype: :class:`obspy.core.stream.Stream`\n\n    .. note:: This functionality is not supported for obspy versions below \\\n        1.0.0 as references times are not read in by SACIO, which are needed \\\n        for defining pick times.\n\n    .. rubric:: Example\n\n    >>> from eqcorrscan.core.template_gen import from_sac\n    >>> import glob\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> # Get all the SAC-files associated with one event.\n    >>> sac_files = glob.glob(TEST_PATH + '/SAC/2014p611252/*')\n    >>> templates = from_sac(sac_files=sac_files, lowcut=2.0, highcut=10.0,\n    ...                      samp_rate=25.0, filt_order=4, length=2.0,\n    ...                      swin='all', prepick=0.1, all_horiz=True)\n    >>> print(templates[0][0].stats.sampling_rate)\n    25.0\n    >>> print(len(templates[0]))\n    15\n    \"\"\"\n    EQcorrscanDeprecationWarning(\n        \"Function is depreciated and will be removed soon. Use \"\n        \"template_gen.template_gen instead.\")\n    temp_list = template_gen(\n        method=\"from_sac\", sac_files=sac_files,\n        lowcut=lowcut, highcut=highcut, samp_rate=samp_rate,\n        filt_order=filt_order, length=length, prepick=prepick,\n        swin=swin, all_horiz=all_horiz, delayed=delayed, plot=plot,\n        debug=debug, return_event=return_event, min_snr=min_snr,\n        parallel=False)\n    return temp_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntiming a function with args and kwargs print name of func and how long it took.", "response": "def time_func(func, name, *args, **kwargs):\n    \"\"\" call a func with args and kwargs, print name of func and how\n    long it took. \"\"\"\n    tic = time.time()\n    out = func(*args, **kwargs)\n    toc = time.time()\n    print('%s took %0.2f seconds' % (name, toc - tic))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_tutorial(plot=False, multiplex=True, return_streams=False, cores=4,\n                 verbose=False):\n    \"\"\"\n    Run the tutorial.\n\n    :return: detections\n    \"\"\"\n    client = Client(\"GEONET\", debug=verbose)\n    cat = client.get_events(\n        minlatitude=-40.98, maxlatitude=-40.85, minlongitude=175.4,\n        maxlongitude=175.5, starttime=UTCDateTime(2016, 5, 1),\n        endtime=UTCDateTime(2016, 5, 20))\n    print(\"Downloaded a catalog of %i events\" % len(cat))\n    # This gives us a catalog of events - it takes a while to download all\n    # the information, so give it a bit!\n    # We will generate a five station, multi-channel detector.\n    cat = filter_picks(catalog=cat, top_n_picks=5)\n    stachans = list(set(\n        [(pick.waveform_id.station_code, pick.waveform_id.channel_code)\n         for event in cat for pick in event.picks]))\n    # In this tutorial we will only work on one cluster, defined spatially.\n    # You can work on multiple clusters, or try to whole set.\n    clusters = space_cluster(catalog=cat, d_thresh=2, show=False)\n    # We will work on the largest cluster\n    cluster = sorted(clusters, key=lambda c: len(c))[-1]\n    # This cluster contains 32 events, we will now download and trim the\n    # waveforms.  Note that each chanel must start at the same time and be the\n    # same length for multiplexing.  If not multiplexing EQcorrscan will\n    # maintain the individual differences in time between channels and delay\n    # the detection statistics by that amount before stacking and detection.\n    client = Client('GEONET')\n    design_set = []\n    st = Stream()\n    for event in cluster:\n        print(\"Downloading for event {0}\".format(event.resource_id.id))\n        bulk_info = []\n        t1 = event.origins[0].time\n        t2 = t1 + 25.1  # Have to download extra data, otherwise GeoNet will\n        # trim wherever suits.\n        t1 -= 0.1\n        for station, channel in stachans:\n            bulk_info.append(('NZ', station, '*', channel[0:2] + '?', t1, t2))\n        st += client.get_waveforms_bulk(bulk=bulk_info)\n    print(\"Downloaded %i channels\" % len(st))\n    for event in cluster:\n        t1 = event.origins[0].time\n        t2 = t1 + 25\n        design_set.append(st.copy().trim(t1, t2))\n    # Construction of the detector will process the traces, then align them,\n    # before multiplexing.\n    print(\"Making detector\")\n    detector = subspace.Detector()\n    detector.construct(\n        streams=design_set, lowcut=2.0, highcut=9.0, filt_order=4,\n        sampling_rate=20, multiplex=multiplex, name='Wairarapa1', align=True,\n        reject=0.2, shift_len=6, plot=plot).partition(9)\n    print(\"Constructed Detector\")\n    if plot:\n        detector.plot()\n    # We also want the continuous stream to detect in.\n    t1 = UTCDateTime(2016, 5, 11, 19)\n    t2 = UTCDateTime(2016, 5, 11, 20)\n    # We are going to look in a single hour just to minimize cost, but you can\n    # run for much longer.\n    bulk_info = [('NZ', stachan[0], '*',\n                  stachan[1][0] + '?' + stachan[1][-1],\n                  t1, t2) for stachan in detector.stachans]\n    print(\"Downloading continuous data\")\n    st = client.get_waveforms_bulk(bulk_info)\n    st.merge().detrend('simple').trim(starttime=t1, endtime=t2)\n    # We set a very low threshold because the detector is not that great, we\n    # haven't aligned it particularly well - however, at this threshold we make\n    # two real detections.\n    print(\"Computing detections\")\n    detections, det_streams = detector.detect(\n        st=st, threshold=0.3, trig_int=2, extract_detections=True,\n        cores=cores)\n    if return_streams:\n        return detections, det_streams\n    else:\n        return detections", "response": "Run the tutorial.\n\n    :return: detections"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a simulated seismogram from a given S - P time.", "response": "def seis_sim(sp, amp_ratio=1.5, flength=False, phaseout='all'):\n    \"\"\"\n    Generate a simulated seismogram from a given S-P time.\n\n    Will generate spikes separated by a given S-P time, which are then\n    convolved with a decaying sine function.  The P-phase is simulated by a\n    positive spike of value 1, the S-arrival is simulated by a decaying\n    boxcar of maximum amplitude 1.5.  These amplitude ratios can be altered by\n    changing the amp_ratio, which is the ratio S amplitude:P amplitude.\n\n    .. note::\n        In testing this can achieve 0.3 or greater cross-correlations with\n        data.\n\n    :type sp: int\n    :param sp: S-P time in samples\n    :type amp_ratio: float\n    :param amp_ratio: S:P amplitude ratio\n    :type flength: int\n    :param flength: Fixed length in samples, defaults to False\n    :type phaseout: str\n    :param phaseout:\n        Either 'P', 'S' or 'all', controls which phases to cut around, defaults\n        to 'all'. Can only be used with 'P' or 'S' options if flength\n        is set.\n\n    :returns: Simulated data.\n    :rtype: :class:`numpy.ndarray`\n    \"\"\"\n    if flength and 2.5 * sp < flength and 100 < flength:\n        additional_length = flength\n    elif 2.5 * sp < 100.0:\n        additional_length = 100\n    else:\n        additional_length = 2.5 * sp\n    synth = np.zeros(int(sp + 10 + additional_length))\n    # Make the array begin 10 samples before the P\n    # and at least 2.5 times the S-P samples after the S arrival\n    synth[10] = 1.0  # P-spike fixed at 10 samples from start of window\n    # The length of the decaying S-phase should depend on the SP time,\\\n    # Some basic estimations suggest this should be atleast 10 samples\\\n    # and that the coda should be about 1/10 of the SP time\n    S_length = 10 + int(sp // 3)\n    S_spikes = np.arange(amp_ratio, 0, -(amp_ratio / S_length))\n    # What we actually want, or what appears better is to have a series of\\\n    # individual spikes, of alternating polarity...\n    for i in range(len(S_spikes)):\n        if i in np.arange(1, len(S_spikes), 2):\n            S_spikes[i] = 0\n        if i in np.arange(2, len(S_spikes), 4):\n            S_spikes[i] *= -1\n    # Put these spikes into the synthetic\n    synth[10 + sp:10 + sp + len(S_spikes)] = S_spikes\n    # Generate a rough damped sine wave to convolve with the model spikes\n    sine_x = np.arange(0, 10.0, 0.5)\n    damped_sine = np.exp(-sine_x) * np.sin(2 * np.pi * sine_x)\n    # Convolve the spike model with the damped sine!\n    synth = np.convolve(synth, damped_sine)\n    # Normalize snyth\n    synth = synth / np.max(np.abs(synth))\n    if not flength:\n        return synth\n    else:\n        if phaseout in ['all', 'P']:\n            synth = synth[0:flength]\n        elif phaseout == 'S':\n            synth = synth[sp:]\n            if len(synth) < flength:\n                # If this is too short, pad\n                synth = np.append(synth, np.zeros(flength - len(synth)))\n            else:\n                synth = synth[0:flength]\n        return synth"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SVD_sim(sp, lowcut, highcut, samp_rate,\n            amp_range=np.arange(-10, 10, 0.01)):\n    \"\"\"\n    Generate basis vectors of a set of simulated seismograms.\n\n    Inputs should have a range of S-P amplitude ratios, in theory to simulate \\\n    a range of focal mechanisms.\n\n    :type sp: int\n    :param sp: S-P time in seconds - will be converted to samples according \\\n        to samp_rate.\n    :type lowcut: float\n    :param lowcut: Low-cut for bandpass filter in Hz\n    :type highcut: float\n    :param highcut: High-cut for bandpass filter in Hz\n    :type samp_rate: float\n    :param samp_rate: Sampling rate in Hz\n    :type amp_range: numpy.ndarray\n    :param amp_range: Amplitude ratio range to generate synthetics for.\n\n    :returns: set of output basis vectors\n    :rtype: :class:`numpy.ndarray`\n    \"\"\"\n    # Convert SP to samples\n    sp = int(sp * samp_rate)\n    # Scan through a range of amplitude ratios\n    synthetics = [Stream(Trace(seis_sim(sp, a))) for a in amp_range]\n    for st in synthetics:\n        for tr in st:\n            tr.stats.station = 'SYNTH'\n            tr.stats.channel = 'SH1'\n            tr.stats.sampling_rate = samp_rate\n            tr.filter('bandpass', freqmin=lowcut, freqmax=highcut)\n    # We have a list of obspy Trace objects, we can pass this to EQcorrscan's\n    # SVD functions\n    U, s, V, stachans = clustering.svd(synthetics)\n    return U, s, V, stachans", "response": "Generate a set of simulated seismograms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating synthetic seismograms for a given set of stations and nodes.", "response": "def template_grid(stations, nodes, travel_times, phase, PS_ratio=1.68,\n                  samp_rate=100, flength=False, phaseout='all'):\n    \"\"\"\n    Generate a group of synthetic seismograms for a grid of sources.\n\n    Used to simulate phase arrivals from a grid of known sources in a\n    three-dimensional model.  Lags must be known and supplied, these can be\n    generated from the bright_lights function: read_tt, and resampled to fit\n    the desired grid dimensions and spacing using other functions therein.\n    These synthetic seismograms are very simple models of seismograms using\n    the seis_sim function herein. These approximate body-wave P and S first\n    arrivals as spikes convolved with damped sine waves.\n\n    :type stations: list\n    :param stations: List of the station names\n    :type nodes: list\n    :param nodes: List of node locations in (lon,lat,depth)\n    :type travel_times: numpy.ndarray\n    :param travel_times: Array of travel times where travel_times[i][:] \\\n        refers to the travel times for station=stations[i], and \\\n        travel_times[i][j] refers to stations[i] for nodes[j]\n    :type phase: str\n    :param phase: Can be either 'P' or 'S'\n    :type PS_ratio: float\n    :param PS_ratio: P/S velocity ratio, defaults to 1.68\n    :type samp_rate: float\n    :param samp_rate: Desired sample rate in Hz, defaults to 100.0\n    :type flength: int\n    :param flength: Length of template in samples, defaults to False\n    :type phaseout: str\n    :param phaseout: Either 'S', 'P', 'all' or 'both', determines which \\\n        phases to clip around.  'all' Encompasses both phases in one channel, \\\n        but will return nothing if the flength is not long enough, 'both' \\\n        will return two channels for each stations, one SYN_Z with the \\\n        synthetic P-phase, and one SYN_H with the synthetic S-phase.\n\n    :returns: List of :class:`obspy.core.stream.Stream`\n    \"\"\"\n    if phase not in ['S', 'P']:\n        raise IOError('Phase is neither P nor S')\n    # Initialize empty list for templates\n    templates = []\n    # Loop through the nodes, for every node generate a template!\n    for i, node in enumerate(nodes):\n        st = []  # Empty list to be filled with synthetics\n        # Loop through stations\n        for j, station in enumerate(stations):\n            tr = Trace()\n            tr.stats.sampling_rate = samp_rate\n            tr.stats.station = station\n            tr.stats.channel = 'SYN'\n            tt = travel_times[j][i]\n            if phase == 'P':\n                # If the input travel-time is the P-wave travel-time\n                SP_time = (tt * PS_ratio) - tt\n                if phaseout == 'S':\n                    tr.stats.starttime += tt + SP_time\n                else:\n                    tr.stats.starttime += tt\n            elif phase == 'S':\n                # If the input travel-time is the S-wave travel-time\n                SP_time = tt - (tt / PS_ratio)\n                if phaseout == 'S':\n                    tr.stats.starttime += tt\n                else:\n                    tr.stats.starttime += tt - SP_time\n            # Set start-time of trace to be travel-time for P-wave\n            # Check that the template length is long enough to include the SP\n            if flength and SP_time * samp_rate < flength - 11 \\\n               and phaseout == 'all':\n                tr.data = seis_sim(sp=int(SP_time * samp_rate), amp_ratio=1.5,\n                                   flength=flength, phaseout=phaseout)\n                st.append(tr)\n            elif flength and phaseout == 'all':\n                warnings.warn('Cannot make a bulk synthetic with this fixed ' +\n                              'length for station ' + station)\n            elif phaseout == 'all':\n                tr.data = seis_sim(sp=int(SP_time * samp_rate), amp_ratio=1.5,\n                                   flength=flength, phaseout=phaseout)\n                st.append(tr)\n            elif phaseout in ['P', 'S']:\n                tr.data = seis_sim(sp=int(SP_time * samp_rate), amp_ratio=1.5,\n                                   flength=flength, phaseout=phaseout)\n                st.append(tr)\n            elif phaseout == 'both':\n                for _phaseout in ['P', 'S']:\n                    _tr = tr.copy()\n                    _tr.data = seis_sim(sp=int(SP_time * samp_rate),\n                                        amp_ratio=1.5, flength=flength,\n                                        phaseout=_phaseout)\n                    if _phaseout == 'P':\n                        _tr.stats.channel = 'SYN_Z'\n                        # starttime defaults to S-time\n                        _tr.stats.starttime = _tr.stats.starttime - SP_time\n                    elif _phaseout == 'S':\n                        _tr.stats.channel = 'SYN_H'\n                    st.append(_tr)\n        templates.append(Stream(st))\n        # Stream(st).plot(size=(800,600))\n    return templates"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating synthetic data for a given number of stations and template.", "response": "def generate_synth_data(nsta, ntemplates, nseeds, samp_rate, t_length,\n                        max_amp, max_lag, debug=0):\n    \"\"\"\n    Generate a synthetic dataset to be used for testing.\n\n    This will generate both templates and data to scan through.\n    Templates will be generated using the utils.synth_seis functions.\n    The day of data will be random noise, with random signal-to-noise\n    ratio copies of the templates randomly seeded throughout the day.\n    It also returns the seed times and signal-to-noise ratios used.\n\n    :type nsta: int\n    :param nsta: Number of stations to generate data for < 15.\n    :type ntemplates: int\n    :param ntemplates: Number of templates to generate, will be generated \\\n        with random arrival times.\n    :type nseeds: int\n    :param nseeds: Number of copies of the template to seed within the \\\n        day of noisy data for each template.\n    :type samp_rate: float\n    :param samp_rate: Sampling rate to use in Hz\n    :type t_length: float\n    :param t_length: Length of templates in seconds.\n    :type max_amp: float\n    :param max_amp: Maximum signal-to-noise ratio of seeds.\n    :param max_lag: Maximum lag time in seconds (randomised).\n    :type max_lag: float\n    :type debug: int\n    :param debug: Debug level, bigger the number, the more plotting/output.\n\n    :returns: Templates: List of :class:`obspy.core.stream.Stream`\n    :rtype: list\n    :returns: Data: :class:`obspy.core.stream.Stream` of seeded noisy data\n    :rtype: :class:`obspy.core.stream.Stream`\n    :returns: Seeds: dictionary of seed SNR and time with time in samples.\n    :rtype: dict\n    \"\"\"\n    # Generate random arrival times\n    t_times = np.abs(np.random.random([nsta, ntemplates])) * max_lag\n    # Generate random node locations - these do not matter as they are only\n    # used for naming\n    lats = np.random.random(ntemplates) * 90.0\n    lons = np.random.random(ntemplates) * 90.0\n    depths = np.abs(np.random.random(ntemplates) * 40.0)\n    nodes = zip(lats, lons, depths)\n    # Generating a 5x3 array to make 3 templates\n    stations = ['ALPH', 'BETA', 'GAMM', 'KAPP', 'ZETA', 'BOB', 'MAGG',\n                'ALF', 'WALR', 'ALBA', 'PENG', 'BANA', 'WIGG', 'SAUS',\n                'MALC']\n    if debug > 1:\n        print(nodes)\n        print(t_times)\n        print(stations[0:nsta])\n    templates = template_grid(stations=stations[0:nsta], nodes=nodes,\n                              travel_times=t_times, phase='S',\n                              samp_rate=samp_rate,\n                              flength=int(t_length * samp_rate))\n    if debug > 2:\n        for template in templates:\n            print(template)\n    # Now we want to create a day of synthetic data\n    seeds = []\n    data = templates[0].copy()  # Copy a template to get the correct length\n    # and stats for data, we will overwrite the data on this copy\n    for tr in data:\n        tr.data = np.zeros(86400 * int(samp_rate))\n        # Set all the traces to have a day of zeros\n        tr.stats.starttime = UTCDateTime(0)\n    for i, template in enumerate(templates):\n        impulses = np.zeros(86400 * int(samp_rate))\n        # Generate a series of impulses for seeding\n        # Need three seperate impulse traces for each of the three templates,\n        # all will be convolved within the data though.\n        impulse_times = np.random.randint(86400 * int(samp_rate),\n                                          size=nseeds)\n        impulse_amplitudes = np.random.randn(nseeds) * max_amp\n        # Generate amplitudes up to maximum amplitude in a normal distribution\n        seeds.append({'SNR': impulse_amplitudes,\n                      'time': impulse_times})\n        for j in range(nseeds):\n            impulses[impulse_times[j]] = impulse_amplitudes[j]\n        # We now have one vector of impulses, we need nsta numbers of them,\n        # shifted with the appropriate lags\n        mintime = min([template_tr.stats.starttime\n                       for template_tr in template])\n        for j, template_tr in enumerate(template):\n            offset = int((template_tr.stats.starttime - mintime) * samp_rate)\n            pad = np.zeros(offset)\n            tr_impulses = np.append(pad, impulses)[0:len(impulses)]\n            # Convolve this with the template trace to give the daylong seeds\n            data[j].data += np.convolve(tr_impulses,\n                                        template_tr.data)[0:len(impulses)]\n    # Add the noise\n    for tr in data:\n        noise = np.random.randn(86400 * int(samp_rate))\n        tr.data += noise / max(noise)\n    return templates, data, seeds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the linear stack of a series of seismic streams of \\ multiplexed data.", "response": "def linstack(streams, normalize=True):\n    \"\"\"\n    Compute the linear stack of a series of seismic streams of \\\n    multiplexed data.\n\n    :type streams: list\n    :param streams: List of streams to stack\n    :type normalize: bool\n    :param normalize: Normalize traces before stacking, normalizes by the RMS \\\n        amplitude.\n\n    :returns: stacked data\n    :rtype: :class:`obspy.core.stream.Stream`\n    \"\"\"\n    stack = streams[np.argmax([len(stream) for stream in streams])].copy()\n    if normalize:\n        for tr in stack:\n            tr.data = tr.data / np.sqrt(np.mean(np.square(tr.data)))\n            tr.data = np.nan_to_num(tr.data)\n    for i in range(1, len(streams)):\n        for tr in stack:\n            matchtr = streams[i].select(station=tr.stats.station,\n                                        channel=tr.stats.channel)\n            if matchtr:\n                # Normalize the data before stacking\n                if normalize:\n                    norm = matchtr[0].data /\\\n                        np.sqrt(np.mean(np.square(matchtr[0].data)))\n                    norm = np.nan_to_num(norm)\n                else:\n                    norm = matchtr[0].data\n                tr.data = np.sum((norm, tr.data), axis=0)\n    return stack"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the phase weighted stack of a series of streams.", "response": "def PWS_stack(streams, weight=2, normalize=True):\n    \"\"\"\n    Compute the phase weighted stack of a series of streams.\n\n    .. note:: It is recommended to align the traces before stacking.\n\n    :type streams: list\n    :param streams: List of :class:`obspy.core.stream.Stream` to stack.\n    :type weight: float\n    :param weight: Exponent to the phase stack used for weighting.\n    :type normalize: bool\n    :param normalize: Normalize traces before stacking.\n\n    :return: Stacked stream.\n    :rtype: :class:`obspy.core.stream.Stream`\n    \"\"\"\n    # First get the linear stack which we will weight by the phase stack\n    Linstack = linstack(streams)\n    # Compute the instantaneous phase\n    instaphases = []\n    print(\"Computing instantaneous phase\")\n    for stream in streams:\n        instaphase = stream.copy()\n        for tr in instaphase:\n            analytic = hilbert(tr.data)\n            envelope = np.sqrt(np.sum((np.square(analytic),\n                                       np.square(tr.data)), axis=0))\n            tr.data = analytic / envelope\n        instaphases.append(instaphase)\n    # Compute the phase stack\n    print(\"Computing the phase stack\")\n    Phasestack = linstack(instaphases, normalize=normalize)\n    # Compute the phase-weighted stack\n    for tr in Phasestack:\n        tr.data = Linstack.select(station=tr.stats.station)[0].data *\\\n            np.abs(tr.data ** weight)\n    return Phasestack"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef align_traces(trace_list, shift_len, master=False, positive=False,\n                 plot=False):\n    \"\"\"\n    Align traces relative to each other based on their cross-correlation value.\n\n    Uses the :func:`eqcorrscan.core.match_filter.normxcorr2` function to find\n    the optimum shift to align traces relative to a master event.  Either uses\n    a given master to align traces, or uses the trace with the highest MAD\n    amplitude.\n\n    :type trace_list: list\n    :param trace_list: List of traces to align\n    :type shift_len: int\n    :param shift_len: Length to allow shifting within in samples\n    :type master: obspy.core.trace.Trace\n    :param master: Master trace to align to, if set to False will align to \\\n        the largest amplitude trace (default)\n    :type positive: bool\n    :param positive: Return the maximum positive cross-correlation, or the \\\n        absolute maximum, defaults to False (absolute maximum).\n    :type plot: bool\n    :param plot: If true, will plot each trace aligned with the master.\n\n    :returns: list of shifts and correlations for best alignment in seconds.\n    :rtype: list\n    \"\"\"\n    from eqcorrscan.core.match_filter import normxcorr2\n    from eqcorrscan.utils.plotting import xcorr_plot\n    traces = deepcopy(trace_list)\n    if not master:\n        # Use trace with largest MAD amplitude as master\n        master = traces[0]\n        MAD_master = np.median(np.abs(master.data))\n        for i in range(1, len(traces)):\n            if np.median(np.abs(traces[i].data)) > MAD_master:\n                master = traces[i]\n                MAD_master = np.median(np.abs(master.data))\n    else:\n        print('Using master given by user')\n    shifts = []\n    ccs = []\n    for i in range(len(traces)):\n        if not master.stats.sampling_rate == traces[i].stats.sampling_rate:\n            raise ValueError('Sampling rates not the same')\n        cc_vec = normxcorr2(template=traces[i].data.\n                            astype(np.float32)[shift_len:-shift_len],\n                            image=master.data.astype(np.float32))\n        cc_vec = cc_vec[0]\n        shift = np.abs(cc_vec).argmax()\n        cc = cc_vec[shift]\n        if plot:\n            xcorr_plot(template=traces[i].data.\n                       astype(np.float32)[shift_len:-shift_len],\n                       image=master.data.astype(np.float32), shift=shift,\n                       cc=cc)\n        shift -= shift_len\n        if cc < 0 and positive:\n            cc = cc_vec.max()\n            shift = cc_vec.argmax() - shift_len\n        shifts.append(shift / master.stats.sampling_rate)\n        ccs.append(cc)\n    return shifts, ccs", "response": "Align traces relative to each other based on their cross - correlation value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef temporary_directory():\n    dir_name = tempfile.mkdtemp()\n    yield dir_name\n    if os.path.exists(dir_name):\n        shutil.rmtree(dir_name)", "response": "create a temporary directory and cleanup on exit"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _total_microsec(t1, t2):\n    td = t1 - t2\n    return (td.seconds + td.days * 24 * 3600) * 10 ** 6 + td.microseconds", "response": "Calculate the total microseconds between two datetime stamps in microseconds."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if a tribe matches a family file path.", "response": "def _templates_match(t, family_file):\n    \"\"\"\n    Return True if a tribe matches a family file path.\n\n    :type t: Tribe\n    :type family_file: str\n    :return: bool\n    \"\"\"\n    return t.name == family_file.split(os.sep)[-1].split('_detections.csv')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_detect(templates, stream, threshold, threshold_type, trig_int,\n                  plotvar, group_size=None, pre_processed=False, daylong=False,\n                  parallel_process=True, xcorr_func=None, concurrency=None,\n                  cores=None, ignore_length=False, overlap=\"calculate\",\n                  debug=0, full_peaks=False, process_cores=None, **kwargs):\n    \"\"\"\n    Pre-process and compute detections for a group of templates.\n\n    Will process the stream object, so if running in a loop, you will want\n    to copy the stream before passing it to this function.\n\n    :type templates: list\n    :param templates: List of :class:`eqcorrscan.core.match_filter.Template`\n    :type stream: `obspy.core.stream.Stream`\n    :param stream: Continuous data to detect within using the Template.\n    :type threshold: float\n    :param threshold:\n        Threshold level, if using `threshold_type='MAD'` then this will be\n        the multiple of the median absolute deviation.\n    :type threshold_type: str\n    :param threshold_type:\n        The type of threshold to be used, can be MAD, absolute or\n        av_chan_corr.  See Note on thresholding below.\n    :type trig_int: float\n    :param trig_int:\n        Minimum gap between detections in seconds. If multiple detections\n        occur within trig_int of one-another, the one with the highest\n        cross-correlation sum will be selected.\n    :type plotvar: bool\n    :param plotvar:\n        Turn plotting on or off, see warning about plotting below.\n    :type group_size: int\n    :param group_size:\n        Maximum number of templates to run at once, use to reduce memory\n        consumption, if unset will use all templates.\n    :type pre_processed: bool\n    :param pre_processed:\n        Set to True if `stream` has already undergone processing, in this\n        case eqcorrscan will only check that the sampling rate is correct.\n        Defaults to False, which will use the\n        :mod:`eqcorrscan.utils.pre_processing` routines to resample and\n        filter the continuous data.\n    :type daylong: bool\n    :param daylong:\n        Set to True to use the\n        :func:`eqcorrscan.utils.pre_processing.dayproc` routine, which\n        preforms additional checks and is more efficient for day-long data\n        over other methods.\n    :type parallel_process: bool\n    :param parallel_process:\n    :type xcorr_func: str or callable\n    :param xcorr_func:\n        A str of a registered xcorr function or a callable for implementing\n        a custom xcorr function. For more details see:\n        :func:`eqcorrscan.utils.correlate.register_array_xcorr`\n    :type concurrency: str\n    :param concurrency:\n        The type of concurrency to apply to the xcorr function. Options are\n        'multithread', 'multiprocess', 'concurrent'. For more details see\n        :func:`eqcorrscan.utils.correlate.get_stream_xcorr`\n    :type cores: int\n    :param cores: Number of workers for processing and correlation.\n    :type ignore_length: bool\n    :param ignore_length:\n        If using daylong=True, then dayproc will try check that the data\n        are there for at least 80% of the day, if you don't want this check\n        (which will raise an error if too much data are missing) then set\n        ignore_length=True.  This is not recommended!\n    :type overlap: float\n    :param overlap:\n        Either None, \"calculate\" or a float of number of seconds to\n        overlap detection streams by.  This is to counter the effects of\n        the delay-and-stack in calculating cross-correlation sums. Setting\n        overlap = \"calculate\" will work out the appropriate overlap based\n        on the maximum lags within templates.\n    :type debug: int\n    :param debug:\n        Debug level from 0-5 where five is more output, for debug levels\n        4 and 5, detections will not be computed in parallel.\n    :type full_peaks: bool\n    :param full_peaks: See `eqcorrscan.utils.findpeaks.find_peaks2_short`\n    :type process_cores: int\n    :param process_cores:\n        Number of processes to use for pre-processing (if different to\n        `cores`).\n\n    :return:\n        :class:`eqcorrscan.core.match_filter.Party` of families of detections.\n    \"\"\"\n    master = templates[0]\n    # Check that they are all processed the same.\n    lap = 0.0\n    for template in templates:\n        starts = [t.stats.starttime for t in template.st.sort(['starttime'])]\n        if starts[-1] - starts[0] > lap:\n            lap = starts[-1] - starts[0]\n        if not template.same_processing(master):\n            raise MatchFilterError('Templates must be processed the same.')\n    if overlap is None:\n        overlap = 0.0\n    elif not isinstance(overlap, float) and str(overlap) == str(\"calculate\"):\n        overlap = lap\n    elif not isinstance(overlap, float):\n        raise NotImplementedError(\n            \"%s is not a recognised overlap type\" % str(overlap))\n    if not pre_processed:\n        if process_cores is None:\n            process_cores = cores\n        streams = _group_process(\n            template_group=templates, parallel=parallel_process, debug=debug,\n            cores=process_cores, stream=stream, daylong=daylong,\n            ignore_length=ignore_length, overlap=overlap)\n    else:\n        warnings.warn('Not performing any processing on the continuous data.')\n        streams = [stream]\n    detections = []\n    party = Party()\n    if group_size is not None:\n        n_groups = int(len(templates) / group_size)\n        if n_groups * group_size < len(templates):\n            n_groups += 1\n    else:\n        n_groups = 1\n    for st_chunk in streams:\n        debug_print(\n            'Computing detections between %s and %s' %\n            (st_chunk[0].stats.starttime, st_chunk[0].stats.endtime), 0, debug)\n        st_chunk.trim(starttime=st_chunk[0].stats.starttime,\n                      endtime=st_chunk[0].stats.endtime)\n        for tr in st_chunk:\n            if len(tr) > len(st_chunk[0]):\n                tr.data = tr.data[0:len(st_chunk[0])]\n        for i in range(n_groups):\n            if group_size is not None:\n                end_group = (i + 1) * group_size\n                start_group = i * group_size\n                if i == n_groups:\n                    end_group = len(templates)\n            else:\n                end_group = len(templates)\n                start_group = 0\n            template_group = [t for t in templates[start_group: end_group]]\n            detections += match_filter(\n                template_names=[t.name for t in template_group],\n                template_list=[t.st for t in template_group], st=st_chunk,\n                xcorr_func=xcorr_func, concurrency=concurrency,\n                threshold=threshold, threshold_type=threshold_type,\n                trig_int=trig_int, plotvar=plotvar, debug=debug, cores=cores,\n                full_peaks=full_peaks, peak_cores=process_cores, **kwargs)\n            for template in template_group:\n                family = Family(template=template, detections=[])\n                for detection in detections:\n                    if detection.template_name == template.name:\n                        family.detections.append(detection)\n                party += family\n    return party", "response": "Compute detections for a group of templates within a given threshold."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess data into chunks based on template processing length.", "response": "def _group_process(template_group, parallel, debug, cores, stream, daylong,\n                   ignore_length, overlap):\n    \"\"\"\n    Process data into chunks based on template processing length.\n\n    Templates in template_group must all have the same processing parameters.\n\n    :type template_group: list\n    :param template_group: List of Templates.\n    :type parallel: bool\n    :param parallel: Whether to use parallel processing or not\n    :type debug: int\n    :param debug: Debug level from 0-5\n    :type cores: int\n    :param cores: Number of cores to use, can be False to use all available.\n    :type stream: :class:`obspy.core.stream.Stream`\n    :param stream: Stream to process, will be left intact.\n    :type daylong: bool\n    :param daylong: Whether to enforce day-length files or not.\n    :type ignore_length: bool\n    :param ignore_length:\n        If using daylong=True, then dayproc will try check that the data\n        are there for at least 80% of the day, if you don't want this check\n        (which will raise an error if too much data are missing) then set\n        ignore_length=True.  This is not recommended!\n    :type overlap: float\n    :param overlap: Number of seconds to overlap chunks by.\n\n    :return: list of processed streams.\n    \"\"\"\n    master = template_group[0]\n    processed_streams = []\n    kwargs = {\n        'filt_order': master.filt_order,\n        'highcut': master.highcut, 'lowcut': master.lowcut,\n        'samp_rate': master.samp_rate, 'debug': debug,\n        'parallel': parallel, 'num_cores': cores}\n    # Processing always needs to be run to account for gaps - pre-process will\n    # check whether filtering and resampling needs to be done.\n    if daylong:\n        if not master.process_length == 86400:\n            warnings.warn(\n                'Processing day-long data, but template was cut from %i s long'\n                ' data, will reduce correlations' % master.process_length)\n        func = dayproc\n        kwargs.update({'ignore_length': ignore_length})\n        # Check that data all start on the same day, otherwise strange\n        # things will happen...\n        starttimes = [tr.stats.starttime.date for tr in stream]\n        if not len(list(set(starttimes))) == 1:\n            warnings.warn('Data start on different days, setting to last day')\n            starttime = UTCDateTime(\n                stream.sort(['starttime'])[-1].stats.starttime.date)\n        else:\n            starttime = stream.sort(['starttime'])[0].stats.starttime\n    else:\n        # We want to use shortproc to allow overlaps\n        func = shortproc\n        starttime = stream.sort(['starttime'])[0].stats.starttime\n    endtime = stream.sort(['endtime'])[-1].stats.endtime\n    data_len_samps = round((endtime - starttime) * master.samp_rate) + 1\n    chunk_len_samps = (master.process_length - overlap) * master.samp_rate\n    n_chunks = int(data_len_samps / chunk_len_samps)\n    if n_chunks == 0:\n        print('Data must be process_length or longer, not computing')\n    for i in range(n_chunks):\n        kwargs.update(\n            {'starttime': starttime + (i * (master.process_length - overlap))})\n        if not daylong:\n            kwargs.update(\n                {'endtime': kwargs['starttime'] + master.process_length})\n            chunk_stream = stream.slice(starttime=kwargs['starttime'],\n                                        endtime=kwargs['endtime']).copy()\n        else:\n            chunk_stream = stream.copy()\n        for tr in chunk_stream:\n            tr.data = tr.data[0:int(\n                master.process_length * tr.stats.sampling_rate)]\n        processed_streams.append(func(st=chunk_stream, **kwargs))\n    return processed_streams"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _par_read(dirname, compressed=True):\n    templates = []\n    if compressed:\n        arc = tarfile.open(dirname, \"r:*\")\n        members = arc.getmembers()\n        _parfile = [member for member in members\n                    if member.name.split(os.sep)[-1] ==\n                    'template_parameters.csv']\n        if len(_parfile) == 0:\n            arc.close()\n            raise MatchFilterError(\n                'No template parameter file in archive')\n        parfile = arc.extractfile(_parfile[0])\n    else:\n        parfile = open(dirname + '/' + 'template_parameters.csv', 'r')\n    for line in parfile:\n        t_in = Template()\n        for key_pair in line.rstrip().split(','):\n            if key_pair.split(':')[0].strip() == 'name':\n                t_in.__dict__[key_pair.split(':')[0].strip()] = \\\n                    key_pair.split(':')[-1].strip()\n            elif key_pair.split(':')[0].strip() == 'filt_order':\n                try:\n                    t_in.__dict__[key_pair.split(':')[0].strip()] = \\\n                        int(key_pair.split(':')[-1])\n                except ValueError:\n                    pass\n            else:\n                try:\n                    t_in.__dict__[key_pair.split(':')[0].strip()] = \\\n                        float(key_pair.split(':')[-1])\n                except ValueError:\n                    pass\n        templates.append(t_in)\n    parfile.close()\n    if compressed:\n        arc.close()\n    return templates", "response": "Internal function to read a formatted parameter file from a directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _badpath(path, base):\n    return not _resolved(os.path.join(base, path)).startswith(base)", "response": "Check if path is relative and not in base."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a badlink string for the link info.", "response": "def _badlink(info, base):\n    \"\"\"\n    Links are interpreted relative to the directory containing the link\n    \"\"\"\n    tip = _resolved(os.path.join(base, os.path.dirname(info.name)))\n    return _badpath(info.linkname, base=tip)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _safemembers(members):\n    base = _resolved(\".\")\n\n    for finfo in members:\n        if _badpath(finfo.name, base):\n            print(finfo.name, \"is blocked (illegal path)\")\n        elif finfo.issym() and _badlink(finfo, base):\n            print(finfo.name, \"is blocked: Hard link to\", finfo.linkname)\n        elif finfo.islnk() and _badlink(finfo, base):\n            print(finfo.name, \"is blocked: Symlink to\", finfo.linkname)\n        else:\n            yield finfo", "response": "Check that the members of a tar archive are safe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_family(family, filename):\n    with open(filename, 'w') as f:\n        for detection in family.detections:\n            det_str = ''\n            for key in detection.__dict__.keys():\n                if key == 'event' and detection.__dict__[key] is not None:\n                    value = str(detection.event.resource_id)\n                elif key in ['threshold', 'detect_val', 'threshold_input']:\n                    value = format(detection.__dict__[key], '.32f').rstrip('0')\n                else:\n                    value = str(detection.__dict__[key])\n                det_str += key + ': ' + value + '; '\n            f.write(det_str + '\\n')\n    return", "response": "Write a family to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_party(fname=None, read_detection_catalog=True):\n    party = Party()\n    party.read(filename=fname, read_detection_catalog=read_detection_catalog)\n    return party", "response": "Reads a single party from a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a file to a list of Detection objects.", "response": "def read_detections(fname):\n    \"\"\"\n    Read detections from a file to a list of Detection objects.\n\n    :type fname: str\n    :param fname: File to read from, must be a file written to by \\\n        Detection.write.\n\n    :returns: list of :class:`eqcorrscan.core.match_filter.Detection`\n    :rtype: list\n\n    .. note::\n        :class:`eqcorrscan.core.match_filter.Detection`'s returned do not\n        contain Detection.event\n    \"\"\"\n    f = open(fname, 'r')\n    detections = []\n    for index, line in enumerate(f):\n        if index == 0:\n            continue  # Skip header\n        if line.rstrip().split('; ')[0] == 'Template name':\n            continue  # Skip any repeated headers\n        detection = line.rstrip().split('; ')\n        detection[1] = UTCDateTime(detection[1])\n        detection[2] = int(float(detection[2]))\n        detection[3] = ast.literal_eval(detection[3])\n        detection[4] = float(detection[4])\n        detection[5] = float(detection[5])\n        if len(detection) < 9:\n            detection.extend(['Unset', float('NaN')])\n        else:\n            detection[7] = float(detection[7])\n        detections.append(Detection(\n            template_name=detection[0], detect_time=detection[1],\n            no_chans=detection[2], detect_val=detection[4],\n            threshold=detection[5], threshold_type=detection[6],\n            threshold_input=detection[7], typeofdet=detection[8],\n            chans=detection[3]))\n    f.close()\n    return detections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting events contained within detections to a catalog file.", "response": "def write_catalog(detections, fname, format=\"QUAKEML\"):\n    \"\"\"Write events contained within detections to a catalog file.\n\n    :type detections: list\n    :param detections: list of eqcorrscan.core.match_filter.Detection\n    :type fname: str\n    :param fname: Name of the file to write to\n    :type format: str\n    :param format: File format to use, see obspy.core.event.Catalog.write \\\n        for supported formats.\n    \"\"\"\n    catalog = get_catalog(detections)\n    catalog.write(filename=fname, format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates an event. Catalog from list of Detections.", "response": "def get_catalog(detections):\n    \"\"\"\n    Generate an :class:`obspy.core.event.Catalog` from list of \\\n    :class:`Detection`'s.\n\n    :type detections: list\n    :param detections: list of :class:`eqcorrscan.core.match_filter.Detection`\n\n    :returns: Catalog of detected events.\n    :rtype: :class:`obspy.core.event.Catalog`\n\n    .. warning::\n        Will only work if the detections have an event associated with them.\n        This will not be the case if detections have been written to csv\n        format using :func:`eqcorrscan.core.match_filter.Detection.write`\n        and read back in.\n    \"\"\"\n    catalog = Catalog()\n    for detection in detections:\n        if detection.event:\n            catalog.append(detection.event)\n    return catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts waveforms for a list of detections from a stream.", "response": "def extract_from_stream(stream, detections, pad=5.0, length=30.0):\n    \"\"\"\n    Extract waveforms for a list of detections from a stream.\n\n    :type stream: obspy.core.stream.Stream\n    :param stream: Stream containing the detections.\n    :type detections: list\n    :param detections: list of eqcorrscan.core.match_filter.detection\n    :type pad: float\n    :param pad: Pre-detection extract time in seconds.\n    :type length: float\n    :param length: Total extracted length in seconds.\n\n    :returns:\n        list of :class:`obspy.core.stream.Stream`, one for each detection.\n    :type: list\n    \"\"\"\n    streams = []\n    for detection in detections:\n        cut_stream = Stream()\n        for pick in detection.event.picks:\n            tr = stream.select(station=pick.waveform_id.station_code,\n                               channel=pick.waveform_id.channel_code)\n            if len(tr) == 0:\n                print('No data in stream for pick:')\n                print(pick)\n                continue\n            cut_stream += tr.slice(\n                starttime=pick.time - pad,\n                endtime=pick.time - pad + length).copy()\n        streams.append(cut_stream)\n    return streams"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normxcorr2(template, image):\n    array_xcorr = get_array_xcorr()\n    # Check that we have been passed numpy arrays\n    if type(template) != np.ndarray or type(image) != np.ndarray:\n        print('You have not provided numpy arrays, I will not convert them')\n        return 'NaN'\n    if len(template) > len(image):\n        ccc = array_xcorr(\n            templates=np.array([image]).astype(np.float32),\n            stream=template.astype(np.float32), pads=[0],\n            threaded=False)[0][0]\n    else:\n        ccc = array_xcorr(\n            templates=np.array([template]).astype(np.float32),\n            stream=image.astype(np.float32), pads=[0], threaded=False)[0][0]\n    ccc = ccc.reshape((1, len(ccc)))\n    return ccc", "response": "This function is used to normalize the correlation of a template and an image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_filter(template_names, template_list, st, threshold,\n                 threshold_type, trig_int, plotvar, plotdir='.',\n                 xcorr_func=None, concurrency=None, cores=None,\n                 debug=0, plot_format='png', output_cat=False,\n                 output_event=True, extract_detections=False,\n                 arg_check=True, full_peaks=False, peak_cores=None, **kwargs):\n    \"\"\"\n    Main matched-filter detection function.\n\n    Over-arching code to run the correlations of given templates with a\n    day of seismic data and output the detections based on a given threshold.\n    For a functional example see the tutorials.\n\n    :type template_names: list\n    :param template_names:\n        List of template names in the same order as template_list\n    :type template_list: list\n    :param template_list:\n        A list of templates of which each template is a\n        :class:`obspy.core.stream.Stream` of obspy traces containing seismic\n        data and header information.\n    :type st: :class:`obspy.core.stream.Stream`\n    :param st:\n        A Stream object containing all the data available and\n        required for the correlations with templates given.  For efficiency\n        this should contain no excess traces which are not in one or more of\n        the templates.  This will now remove excess traces internally, but\n        will copy the stream and work on the copy, leaving your input stream\n        untouched.\n    :type threshold: float\n    :param threshold: A threshold value set based on the threshold_type\n    :type threshold_type: str\n    :param threshold_type:\n        The type of threshold to be used, can be MAD, absolute or av_chan_corr.\n        See Note on thresholding below.\n    :type trig_int: float\n    :param trig_int: Minimum gap between detections in seconds.\n    :type plotvar: bool\n    :param plotvar: Turn plotting on or off\n    :type plotdir: str\n    :param plotdir:\n        Path to plotting folder, plots will be output here, defaults to run\n        location.\n    :type xcorr_func: str or callable\n    :param xcorr_func:\n        A str of a registered xcorr function or a callable for implementing\n        a custom xcorr function. For more information see:\n        :func:`eqcorrscan.utils.correlate.register_array_xcorr`\n    :type concurrency: str\n    :param concurrency:\n        The type of concurrency to apply to the xcorr function. Options are\n        'multithread', 'multiprocess', 'concurrent'. For more details see\n        :func:`eqcorrscan.utils.correlate.get_stream_xcorr`\n    :type cores: int\n    :param cores: Number of cores to use\n    :type debug: int\n    :param debug:\n        Debug output level, the bigger the number, the more the output.\n    :type plot_format: str\n    :param plot_format: Specify format of output plots if saved\n    :type output_cat: bool\n    :param output_cat:\n        Specifies if matched_filter will output an obspy.Catalog class\n        containing events for each detection. Default is False, in which case\n        matched_filter will output a list of detection classes, as normal.\n    :type output_event: bool\n    :param output_event:\n        Whether to include events in the Detection objects, defaults to True,\n        but for large cases you may want to turn this off as Event objects\n        can be quite memory intensive.\n    :type extract_detections: bool\n    :param extract_detections:\n        Specifies whether or not to return a list of streams, one stream per\n        detection.\n    :type arg_check: bool\n    :param arg_check:\n        Check arguments, defaults to True, but if running in bulk, and you are\n        certain of your arguments, then set to False.\n    :type full_peaks: bool\n    :param full_peaks: See `eqcorrscan.core.findpeaks.find_peaks2_short`.\n    :type peak_cores: int\n    :param peak_cores:\n        Number of processes to use for parallel peak-finding (if different to\n        `cores`).\n\n    .. note::\n        **Returns:**\n\n        If neither `output_cat` or `extract_detections` are set to `True`,\n        then only the list of :class:`eqcorrscan.core.match_filter.Detection`'s\n        will be output:\n\n        :return:\n            :class:`eqcorrscan.core.match_filter.Detection` detections for each\n            detection made.\n        :rtype: list\n\n        If `output_cat` is set to `True`, then the\n        :class:`obspy.core.event.Catalog` will also be output:\n\n        :return: Catalog containing events for each detection, see above.\n        :rtype: :class:`obspy.core.event.Catalog`\n\n        If `extract_detections` is set to `True` then the list of\n        :class:`obspy.core.stream.Stream`'s will also be output.\n\n        :return:\n            list of :class:`obspy.core.stream.Stream`'s for each detection, see\n            above.\n        :rtype: list\n\n    .. note::\n        If your data contain gaps these must be padded with zeros before\n        using this function. The `eqcorrscan.utils.pre_processing` functions\n        will provide gap-filled data in the appropriate format.  Note that if\n        you pad your data with zeros before filtering or resampling the gaps\n        will not be all zeros after filtering. This will result in the\n        calculation of spurious correlations in the gaps.\n\n    .. Note::\n        Detections are not corrected for `pre-pick`, the\n        detection.detect_time corresponds to the beginning of the earliest\n        template channel at detection.\n\n    .. note::\n        **Data overlap:**\n\n        Internally this routine shifts and trims the data according to the\n        offsets in the template (e.g. if trace 2 starts 2 seconds after trace 1\n        in the template then the continuous data will be shifted by 2 seconds\n        to align peak correlations prior to summing).  Because of this,\n        detections at the start and end of continuous data streams\n        **may be missed**.  The maximum time-period that might be missing\n        detections is the maximum offset in the template.\n\n        To work around this, if you are conducting matched-filter detections\n        through long-duration continuous data, we suggest using some overlap\n        (a few seconds, on the order of the maximum offset in the templates)\n        in the continous data.  You will then need to post-process the\n        detections (which should be done anyway to remove duplicates).\n\n    .. note::\n        **Thresholding:**\n\n        **MAD** threshold is calculated as the:\n\n        .. math::\n\n            threshold {\\\\times} (median(abs(cccsum)))\n\n        where :math:`cccsum` is the cross-correlation sum for a given template.\n\n        **absolute** threshold is a true absolute threshold based on the\n        cccsum value.\n\n        **av_chan_corr** is based on the mean values of single-channel\n        cross-correlations assuming all data are present as required for the\n        template, e.g:\n\n        .. math::\n\n            av\\_chan\\_corr\\_thresh=threshold \\\\times (cccsum\\ /\\ len(template))\n\n        where :math:`template` is a single template from the input and the\n        length is the number of channels within this template.\n\n    .. note::\n        The output_cat flag will create an :class:`obspy.core.event.Catalog`\n        containing one event for each\n        :class:`eqcorrscan.core.match_filter.Detection`'s generated by\n        match_filter. Each event will contain a number of comments dealing\n        with correlation values and channels used for the detection. Each\n        channel used for the detection will have a corresponding\n        :class:`obspy.core.event.Pick` which will contain time and\n        waveform information. **HOWEVER**, the user should note that\n        the pick times do not account for the prepick times inherent in\n        each template. For example, if a template trace starts 0.1 seconds\n        before the actual arrival of that phase, then the pick time generated\n        by match_filter for that phase will be 0.1 seconds early.\n\n    .. Note::\n        xcorr_func can be used as follows:\n\n        .. rubric::xcorr_func argument example\n\n        >>> import obspy\n        >>> import numpy as np\n        >>> from eqcorrscan.core.match_filter import match_filter\n        >>> from eqcorrscan.utils.correlate import time_multi_normxcorr\n        >>> # define a custom xcorr function\n        >>> def custom_normxcorr(templates, stream, pads, *args, **kwargs):\n        ...     # Just to keep example short call other xcorr function\n        ...     # in practice you would define your own function here\n        ...     print('calling custom xcorr function')\n        ...     return time_multi_normxcorr(templates, stream, pads)\n        >>> # generate some toy templates and stream\n        >>> random = np.random.RandomState(42)\n        >>> template = obspy.read()\n        >>> stream = obspy.read()\n        >>> for num, tr in enumerate(stream):  # iter st and embed templates\n        ...     data = tr.data\n        ...     tr.data = random.randn(6000) * 5\n        ...     tr.data[100: 100 + len(data)] = data\n        >>> # call match_filter ane ensure the custom function is used\n        >>> detections = match_filter(\n        ...     template_names=['1'], template_list=[template], st=stream,\n        ...     threshold=.5, threshold_type='absolute', trig_int=1,\n        ...     plotvar=False,\n        ...     xcorr_func=custom_normxcorr)  # doctest:+ELLIPSIS\n        calling custom xcorr function...\n    \"\"\"\n    from eqcorrscan.utils.plotting import _match_filter_plot\n    if arg_check:\n        # Check the arguments to be nice - if arguments wrong type the parallel\n        # output for the error won't be useful\n        if not isinstance(template_names, list):\n            raise MatchFilterError('template_names must be of type: list')\n        if not isinstance(template_list, list):\n            raise MatchFilterError('templates must be of type: list')\n        if not len(template_list) == len(template_names):\n            raise MatchFilterError('Not the same number of templates as names')\n        for template in template_list:\n            if not isinstance(template, Stream):\n                msg = 'template in template_list must be of type: ' + \\\n                      'obspy.core.stream.Stream'\n                raise MatchFilterError(msg)\n        if not isinstance(st, Stream):\n            msg = 'st must be of type: obspy.core.stream.Stream'\n            raise MatchFilterError(msg)\n        if str(threshold_type) not in [str('MAD'), str('absolute'),\n                                       str('av_chan_corr')]:\n            msg = 'threshold_type must be one of: MAD, absolute, av_chan_corr'\n            raise MatchFilterError(msg)\n        for tr in st:\n            if not tr.stats.sampling_rate == st[0].stats.sampling_rate:\n                raise MatchFilterError('Sampling rates are not equal %f: %f' %\n                                       (tr.stats.sampling_rate,\n                                        st[0].stats.sampling_rate))\n        for template in template_list:\n            for tr in template:\n                if not tr.stats.sampling_rate == st[0].stats.sampling_rate:\n                    raise MatchFilterError(\n                        'Template sampling rate does not '\n                        'match continuous data')\n    _spike_test(st)\n    if cores is not None:\n        parallel = True\n    else:\n        parallel = False\n    # Copy the stream here because we will muck about with it\n    stream = st.copy()\n    templates = copy.deepcopy(template_list)\n    _template_names = copy.deepcopy(template_names)\n    # Debug option to confirm that the channel names match those in the\n    # templates\n    if debug >= 2:\n        template_stachan = []\n        data_stachan = []\n        for template in templates:\n            for tr in template:\n                if isinstance(tr.data, np.ma.core.MaskedArray):\n                    raise MatchFilterError('Template contains masked array,'\n                                           ' split first')\n                template_stachan.append(tr.stats.station + '.' +\n                                        tr.stats.channel)\n        for tr in stream:\n            data_stachan.append(tr.stats.station + '.' + tr.stats.channel)\n        template_stachan = list(set(template_stachan))\n        data_stachan = list(set(data_stachan))\n        debug_print('I have template info for these stations:\\n' +\n                    template_stachan.__str__() +\n                    '\\nI have daylong data for these stations:\\n' +\n                    data_stachan.__str__(), 3, debug)\n    # Perform a check that the continuous data are all the same length\n    min_start_time = min([tr.stats.starttime for tr in stream])\n    max_end_time = max([tr.stats.endtime for tr in stream])\n    longest_trace_length = stream[0].stats.sampling_rate * (max_end_time -\n                                                            min_start_time)\n    longest_trace_length += 1\n    for tr in stream:\n        if not tr.stats.npts == longest_trace_length:\n            msg = 'Data are not equal length, padding short traces'\n            warnings.warn(msg)\n            start_pad = np.zeros(int(tr.stats.sampling_rate *\n                                     (tr.stats.starttime - min_start_time)))\n            end_pad = np.zeros(int(tr.stats.sampling_rate *\n                                   (max_end_time - tr.stats.endtime)))\n            # In some cases there will be one sample missing when sampling\n            # time-stamps are not set consistently between channels, this\n            # results in start_pad and end_pad being len==0\n            if len(start_pad) == 0 and len(end_pad) == 0:\n                debug_print(\"start and end pad are both zero, padding at one \"\n                            \"end\", 2, debug)\n                if (tr.stats.starttime - min_start_time) > (\n                   max_end_time - tr.stats.endtime):\n                    start_pad = np.zeros(\n                        int(longest_trace_length - tr.stats.npts))\n                else:\n                    end_pad = np.zeros(\n                        int(longest_trace_length - tr.stats.npts))\n            tr.data = np.concatenate([start_pad, tr.data, end_pad])\n    # Perform check that all template lengths are internally consistent\n    for i, temp in enumerate(template_list):\n        if len(set([tr.stats.npts for tr in temp])) > 1:\n            msg = ('Template %s contains traces of differing length, this is '\n                   'not currently supported' % _template_names[i])\n            raise MatchFilterError(msg)\n    outtic = time.clock()\n    debug_print('Ensuring all template channels have matches in'\n                ' continuous data', 2, debug)\n    template_stachan = {}\n    # Work out what station-channel pairs are in the templates, including\n    # duplicate station-channel pairs.  We will use this information to fill\n    # all templates with the same station-channel pairs as required by\n    # _template_loop.\n    for template in templates:\n        stachans_in_template = []\n        for tr in template:\n            stachans_in_template.append((tr.stats.network, tr.stats.station,\n                                         tr.stats.location, tr.stats.channel))\n        stachans_in_template = dict(Counter(stachans_in_template))\n        for stachan in stachans_in_template.keys():\n            stachans = stachans_in_template[stachan]\n            if stachan not in template_stachan.keys():\n                template_stachan.update({stachan: stachans})\n            elif stachans_in_template[stachan] > template_stachan[stachan]:\n                template_stachan.update({stachan: stachans})\n    # Remove un-matched channels from templates.\n    _template_stachan = copy.deepcopy(template_stachan)\n    for stachan in template_stachan.keys():\n        if not stream.select(network=stachan[0], station=stachan[1],\n                             location=stachan[2], channel=stachan[3]):\n            # Remove stachan from list of dictionary of template_stachans\n            _template_stachan.pop(stachan)\n            # Remove template traces rather than adding NaN data\n            for template in templates:\n                if template.select(network=stachan[0], station=stachan[1],\n                                   location=stachan[2], channel=stachan[3]):\n                    for tr in template.select(\n                            network=stachan[0], station=stachan[1],\n                            location=stachan[2], channel=stachan[3]):\n                        template.remove(tr)\n                        print('Removing template channel %s.%s.%s.%s due to'\n                              ' no matches in continuous data' %\n                              (stachan[0], stachan[1], stachan[2], stachan[3]))\n    template_stachan = _template_stachan\n    # Remove un-needed channels from continuous data.\n    for tr in stream:\n        if not (tr.stats.network, tr.stats.station,\n                tr.stats.location, tr.stats.channel) in \\\n                template_stachan.keys():\n            print('Removing channel in continuous data for %s.%s.%s.%s:'\n                  ' no match in template' %\n                  (tr.stats.network, tr.stats.station, tr.stats.location,\n                   tr.stats.channel))\n            stream.remove(tr)\n    # Check for duplicate channels\n    stachans = [(tr.stats.network, tr.stats.station,\n                 tr.stats.location, tr.stats.channel) for tr in stream]\n    c_stachans = Counter(stachans)\n    for key in c_stachans.keys():\n        if c_stachans[key] > 1:\n            msg = ('Multiple channels for %s.%s.%s.%s, likely a data issue'\n                   % (key[0], key[1], key[2], key[3]))\n            raise MatchFilterError(msg)\n    # Pad out templates to have all channels\n    _templates = []\n    used_template_names = []\n    for template, template_name in zip(templates, _template_names):\n        if len(template) == 0:\n            msg = ('No channels matching in continuous data for ' +\n                   'template' + template_name)\n            warnings.warn(msg)\n            continue\n        for stachan in template_stachan.keys():\n            number_of_channels = len(template.select(\n                network=stachan[0], station=stachan[1], location=stachan[2],\n                channel=stachan[3]))\n            if number_of_channels < template_stachan[stachan]:\n                missed_channels = template_stachan[stachan] - \\\n                                  number_of_channels\n                nulltrace = Trace()\n                nulltrace.stats.update(\n                    {'network': stachan[0], 'station': stachan[1],\n                     'location': stachan[2], 'channel': stachan[3],\n                     'sampling_rate': template[0].stats.sampling_rate,\n                     'starttime': template[0].stats.starttime,\n                     'not_in_original': True})\n                nulltrace.data = np.array([np.NaN] * len(template[0].data),\n                                          dtype=np.float32)\n                for dummy in range(missed_channels):\n                    template += nulltrace\n        template.sort()\n        _templates.append(template)\n        used_template_names.append(template_name)\n        # Quick check that this has all worked\n        if len(template) != max([len(t) for t in templates]):\n            raise MatchFilterError('Internal error forcing same template '\n                                   'lengths, report this error.')\n    templates = _templates\n    _template_names = used_template_names\n    debug_print('Starting the correlation run for these data', 2, debug)\n    for template in templates:\n        debug_print(template.__str__(), 3, debug)\n    debug_print(stream.__str__(), 3, debug)\n    multichannel_normxcorr = get_stream_xcorr(xcorr_func, concurrency)\n    [cccsums, no_chans, chans] = multichannel_normxcorr(\n        templates=templates, stream=stream, cores=cores, **kwargs)\n    if len(cccsums[0]) == 0:\n        raise MatchFilterError('Correlation has not run, zero length cccsum')\n    outtoc = time.clock()\n    debug_print(' '.join(['Looping over templates and streams took:',\n                          str(outtoc - outtic), 's']), 0, debug)\n    debug_print('The shape of the returned cccsums is: %s\\n'\n                'This is from %i templates\\nCorrelated with %i channels of '\n                'data' % (cccsums.shape, len(templates), len(stream)), 2,\n                debug)\n    detections = []\n    if output_cat:\n        det_cat = Catalog()\n    if str(threshold_type) == str(\"absolute\"):\n        thresholds = [threshold for _ in range(len(cccsums))]\n    elif str(threshold_type) == str('MAD'):\n        thresholds = [threshold * np.median(np.abs(cccsum))\n                      for cccsum in cccsums]\n    else:\n        thresholds = [threshold * no_chans[i] for i in range(len(cccsums))]\n    if peak_cores is None:\n        peak_cores = cores\n    all_peaks = multi_find_peaks(\n        arr=cccsums, thresh=thresholds, debug=debug, parallel=parallel,\n        trig_int=int(trig_int * stream[0].stats.sampling_rate),\n        full_peaks=full_peaks, cores=peak_cores)\n    for i, cccsum in enumerate(cccsums):\n        if np.abs(np.mean(cccsum)) > 0.05:\n            warnings.warn('Mean is not zero!  Check this!')\n        # Set up a trace object for the cccsum as this is easier to plot and\n        # maintains timing\n        if plotvar:\n            _match_filter_plot(\n                stream=stream, cccsum=cccsum, template_names=_template_names,\n                rawthresh=thresholds[i], plotdir=plotdir,\n                plot_format=plot_format, i=i)\n        if debug >= 4:\n            np.save(_template_names[i] +\n                    stream[0].stats.starttime.datetime.strftime('%Y%j'),\n                    cccsum)\n        debug_print(\n            ' '.join(['Saved the cccsum to:', _template_names[i],\n                      stream[0].stats.starttime.datetime.strftime('%Y%j')]),\n            4, debug)\n        if all_peaks[i]:\n            for peak in all_peaks[i]:\n                detecttime = (\n                        stream[0].stats.starttime +\n                        peak[1] / stream[0].stats.sampling_rate)\n                detection = Detection(\n                    template_name=_template_names[i], detect_time=detecttime,\n                    no_chans=no_chans[i], detect_val=peak[0],\n                    threshold=thresholds[i], typeofdet='corr', chans=chans[i],\n                    threshold_type=threshold_type, threshold_input=threshold)\n                if output_cat or output_event:\n                    detection._calculate_event(template_st=templates[i])\n                detections.append(detection)\n                if output_cat:\n                    det_cat.append(detection.event)\n        if extract_detections:\n            detection_streams = extract_from_stream(stream, detections)\n    del stream, templates\n    if output_cat and not extract_detections:\n        return detections, det_cat\n    elif not extract_detections:\n        return detections\n    elif extract_detections and not output_cat:\n        return detections, detection_streams\n    else:\n        return detections, det_cat, detection_streams", "response": "This function will run the correlations of a given list of templates and output the detections based on a given threshold."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self, template_name):\n        return [fam for fam in self.families\n                if fam.template.name == template_name][0]", "response": "Select a specific family from the party."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort(self):\n        self.families.sort(key=lambda x: x.template.name)\n        return self", "response": "Sort the keys of the log entries by template name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(self, dates=None, min_dets=1):\n        if dates is None:\n            raise MatchFilterError('Need a list defining a date range')\n        new_party = Party()\n        for fam in self.families:\n            new_fam = Family(\n                template=fam.template,\n                detections=[det for det in fam if\n                            dates[0] < det.detect_time < dates[1]])\n            if len(new_fam) >= min_dets:\n                new_party.families.append(new_fam)\n        return new_party", "response": "Return a new Party with only detections within a date range and only families with a minimum number of detections."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the cumulative detections for all templates and the given date range.", "response": "def plot(self, plot_grouped=False, dates=None, min_dets=1, rate=False,\n             **kwargs):\n        \"\"\"\n        Plot the cumulative detections in time.\n\n        :type plot_grouped: bool\n        :param plot_grouped:\n            Whether to plot all families together (plot_grouped=True), or each\n            as a separate line.\n        :type dates: list\n        :param dates: list of obspy.core.UTCDateTime objects bounding the\n            plot. The first should be the start date, the last the end date.\n        :type min_dets: int\n        :param min_dets: Plot only families with this number of detections\n            or more.\n        :type rate: bool\n        :param rate: Whether or not to plot the daily rate of detection as\n            opposed to cumulative number. Only works with plot_grouped=True.\n        :param \\**kwargs: Any other arguments accepted by\n            :func:`eqcorrscan.utils.plotting.cumulative_detections`\n\n        .. rubric:: Examples\n\n        Plot cumulative detections for all templates individually:\n\n        >>> Party().read().plot()  # doctest: +SKIP\n\n        Plot cumulative detections for all templates grouped together:\n\n        >>> Party().read().plot(plot_grouped=True) # doctest: +SKIP\n\n        Plot the rate of detection for all templates grouped together:\n\n        >>> Party().read().plot(plot_grouped=True, rate=True) # doctest: +SKIP\n\n        Plot cumulative detections for all templates with more than five\n        detections between June 1st, 2012 and July 31st, 2012:\n\n        >>> from obspy import UTCDateTime\n        >>> Party().read().plot(dates=[UTCDateTime(2012, 6, 1),\n        ...                            UTCDateTime(2012, 7, 31)],\n        ...                     min_dets=5) # doctest: +SKIP\n\n        \"\"\"\n        all_dets = []\n        if dates:\n            new_party = self.filter(dates=dates, min_dets=min_dets)\n            for fam in new_party.families:\n                all_dets.extend(fam.detections)\n        else:\n            for fam in self.families:\n                all_dets.extend(fam.detections)\n        fig = cumulative_detections(detections=all_dets,\n                                    plot_grouped=plot_grouped,\n                                    rate=rate, **kwargs)\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove detections from the Party that are below a new threshold. .. Note:: threshold can only be set higher. .. Warning:: Works in place on Party. :type new_threshold: float :param new_threshold: New threshold level :type new_threshold_type: str :param new_threshold_type: Either 'MAD', 'absolute' or 'av_chan_corr' .. rubric:: Examples Using the MAD threshold on detections made using the MAD threshold: >>> party = Party().read() >>> len(party) 4 >>> party = party.rethreshold(10.0) >>> len(party) 4 >>> # Note that all detections are self detections Using the absolute thresholding method on the same Party: >>> party = Party().read().rethreshold(6.0, 'absolute') >>> len(party) 1 Using the av_chan_corr method on the same Party: >>> party = Party().read().rethreshold(0.9, 'av_chan_corr') >>> len(party) 4", "response": "def rethreshold(self, new_threshold, new_threshold_type='MAD'):\n        \"\"\"\n        Remove detections from the Party that are below a new threshold.\n\n        .. Note:: threshold can only be set higher.\n\n        .. Warning::\n            Works in place on Party.\n\n        :type new_threshold: float\n        :param new_threshold: New threshold level\n        :type new_threshold_type: str\n        :param new_threshold_type: Either 'MAD', 'absolute' or 'av_chan_corr'\n\n        .. rubric:: Examples\n\n        Using the MAD threshold on detections made using the MAD threshold:\n\n        >>> party = Party().read()\n        >>> len(party)\n        4\n        >>> party = party.rethreshold(10.0)\n        >>> len(party)\n        4\n        >>> # Note that all detections are self detections\n\n\n        Using the absolute thresholding method on the same Party:\n\n        >>> party = Party().read().rethreshold(6.0, 'absolute')\n        >>> len(party)\n        1\n\n\n        Using the av_chan_corr method on the same Party:\n\n        >>> party = Party().read().rethreshold(0.9, 'av_chan_corr')\n        >>> len(party)\n        4\n        \"\"\"\n        for family in self.families:\n            rethresh_detections = []\n            for d in family.detections:\n                if new_threshold_type == 'MAD' and d.threshold_type == 'MAD':\n                    new_thresh = (d.threshold /\n                                  d.threshold_input) * new_threshold\n                elif new_threshold_type == 'MAD' and d.threshold_type != 'MAD':\n                    raise MatchFilterError(\n                        'Cannot recalculate MAD level, '\n                        'use another threshold type')\n                elif new_threshold_type == 'absolute':\n                    new_thresh = new_threshold\n                elif new_threshold_type == 'av_chan_corr':\n                    new_thresh = new_threshold * d.no_chans\n                else:\n                    raise MatchFilterError(\n                        'new_threshold_type %s is not recognised' %\n                        str(new_threshold_type))\n                if d.detect_val >= new_thresh:\n                    d.threshold = new_thresh\n                    d.threshold_input = new_threshold\n                    d.threshold_type = new_threshold_type\n                    rethresh_detections.append(d)\n            family.detections = rethresh_detections\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decluster(self, trig_int, timing='detect', metric='avg_cor'):\n        all_detections = []\n        for fam in self.families:\n            all_detections.extend(fam.detections)\n        if timing == 'detect':\n            if metric == 'avg_cor':\n                detect_info = [(d.detect_time, d.detect_val / d.no_chans)\n                               for d in all_detections]\n            elif metric == 'cor_sum':\n                detect_info = [(d.detect_time, d.detect_val)\n                               for d in all_detections]\n            else:\n                raise MatchFilterError('metric is not cor_sum or avg_cor')\n        elif timing == 'origin':\n            if metric == 'avg_cor':\n                detect_info = [(_get_origin(d.event).time,\n                                d.detect_val / d.no_chans)\n                               for d in all_detections]\n            elif metric == 'cor_sum':\n                detect_info = [(_get_origin(d.event).time, d.detect_val)\n                               for d in all_detections]\n            else:\n                raise MatchFilterError('metric is not cor_sum or avg_cor')\n        else:\n            raise MatchFilterError('timing is not detect or origin')\n        min_det = sorted([d[0] for d in detect_info])[0]\n        detect_vals = np.array([d[1] for d in detect_info])\n        detect_times = np.array([\n            _total_microsec(d[0].datetime, min_det.datetime)\n            for d in detect_info])\n        # Trig_int must be converted from seconds to micro-seconds\n        peaks_out = decluster(\n            peaks=detect_vals, index=detect_times, trig_int=trig_int * 10 ** 6)\n        # Need to match both the time and the detection value\n        declustered_detections = []\n        for ind in peaks_out:\n            matching_time_indeces = np.where(detect_times == ind[-1])[0]\n            matches = matching_time_indeces[\n                np.where(detect_vals[matching_time_indeces] == ind[0])[0][0]]\n            declustered_detections.append(all_detections[matches])\n        # Convert this list into families\n        template_names = list(set([d.template_name\n                                   for d in declustered_detections]))\n        new_families = []\n        for template_name in template_names:\n            template = [fam.template for fam in self.families\n                        if fam.template.name == template_name][0]\n            new_families.append(Family(\n                template=template,\n                detections=[d for d in declustered_detections\n                            if d.template_name == template_name]))\n        self.families = new_families\n        return self", "response": "This method declares a Party of detections by enforcing a detection separation in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite out the information of the current is to a file.", "response": "def write(self, filename, format='tar', write_detection_catalog=True,\n              catalog_format=\"QUAKEML\", debug=0):\n        \"\"\"\n        Write Family out, select output format.\n\n        :type format: str\n        :param format:\n            One of either 'tar', 'csv', or any obspy supported\n            catalog output. See note below on formats\n        :type filename: str\n        :param filename: Path to write file to.\n        :type debug: int\n        :param debug: Whether to output progress or not.\n        :type write_detection_catalog: bool\n        :param write_detection_catalog:\n            Whether to write the detection catalog object or not - writing\n            large catalog files can be slow, and catalogs can be reconstructed\n            from the Tribe.\n        :type catalog_format: str\n        :param catalog_format:\n            What format to write the detection-catalog with. Only Nordic,\n            SC3ML, QUAKEML are supported. Note that not all information is\n            written for all formats (QUAKEML is the most complete, but is\n            slow for IO).\n\n        .. NOTE::\n            csv format will write out detection objects, all other\n            outputs will write the catalog.  These cannot be rebuilt into\n            a Family object.  The only format that can be read back into\n            Family objects is the 'tar' type.\n\n        .. NOTE::\n            We recommend writing to the 'tar' format, which will write out\n            all the template information (wavefiles as miniseed and metadata)\n            alongside the detections and store these in a tar archive. This\n            is readable by other programs and maintains all information\n            required for further study.\n\n        .. rubric:: Example\n\n        >>> party = Party().read()\n        >>> party.write('test_tar_write', format='tar', debug=1)\n        Writing family 0\n        Writing family 1\n        Writing family 2\n        Writing family 3\n        Party of 4 Families.\n        >>> party.write('test_csv_write.csv', format='csv')\n        Party of 4 Families.\n        >>> party.write('test_quakeml.xml', format='quakeml')\n        Party of 4 Families.\n        \"\"\"\n        if catalog_format not in CAT_EXT_MAP.keys():\n            raise TypeError(\"{0} is not supported\".format(catalog_format))\n        if format.lower() == 'csv':\n            if os.path.isfile(filename):\n                raise MatchFilterError(\n                    'Will not overwrite existing file: %s' % filename)\n            for family in self.families:\n                for detection in family.detections:\n                    detection.write(fname=filename, append=True)\n        elif format.lower() == 'tar':\n            if os.path.exists(filename):\n                raise IOError('Will not overwrite existing file: %s'\n                              % filename)\n            # os.makedirs(filename)\n            with temporary_directory() as temp_dir:\n                Tribe([f.template for f in self.families]).write(\n                    filename=temp_dir, compress=False,\n                    catalog_format=catalog_format)\n                if write_detection_catalog:\n                    all_cat = Catalog()\n                    for family in self.families:\n                        all_cat += family.catalog\n                    if not len(all_cat) == 0:\n                        all_cat.write(\n                            join(temp_dir, 'catalog{0}'.format(\n                                CAT_EXT_MAP[catalog_format])),\n                            format=catalog_format)\n                for i, family in enumerate(self.families):\n                    debug_print('Writing family %i' % i, 0, debug)\n                    name = family.template.name + '_detections.csv'\n                    name_to_write = join(temp_dir, name)\n                    _write_family(family=family, filename=name_to_write)\n                with tarfile.open(filename + '.tgz', \"w:gz\") as tar:\n                    tar.add(temp_dir, arcname=os.path.basename(filename))\n        else:\n            warnings.warn('Writing only the catalog component, metadata '\n                          'will not be preserved')\n            self.get_catalog().write(filename=filename, format=format)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a Party from a file.", "response": "def read(self, filename=None, read_detection_catalog=True):\n        \"\"\"\n        Read a Party from a file.\n\n        :type filename: str\n        :param filename:\n            File to read from - can be a list of files, and can contain\n            wildcards.\n        :type read_detection_catalog: bool\n        :param read_detection_catalog:\n            Whether to read the detection catalog or not, if False, catalog\n            will be regenerated - for large catalogs this can be faster.\n\n        .. rubric:: Example\n\n        >>> Party().read()\n        Party of 4 Families.\n        \"\"\"\n        tribe = Tribe()\n        families = []\n        if filename is None:\n            # If there is no filename given, then read the example.\n            filename = os.path.join(os.path.dirname(__file__),\n                                    '..', 'tests', 'test_data',\n                                    'test_party.tgz')\n        if isinstance(filename, list):\n            filenames = []\n            for _filename in filename:\n                # Expand wildcards\n                filenames.extend(glob.glob(_filename))\n        else:\n            # Expand wildcards\n            filenames = glob.glob(filename)\n        for _filename in filenames:\n            with tarfile.open(_filename, \"r:*\") as arc:\n                temp_dir = tempfile.mkdtemp()\n                arc.extractall(path=temp_dir, members=_safemembers(arc))\n            # Read in the detections first, this way, if we read from multiple\n            # files then we can just read in extra templates as needed.\n            # Read in families here!\n            party_dir = glob.glob(temp_dir + os.sep + '*')[0]\n            tribe._read_from_folder(dirname=party_dir)\n            det_cat_file = glob.glob(os.path.join(party_dir, \"catalog.*\"))\n            if len(det_cat_file) != 0 and read_detection_catalog:\n                try:\n                    all_cat = read_events(det_cat_file[0])\n                except TypeError as e:\n                    print(e)\n                    pass\n            else:\n                all_cat = Catalog()\n            for family_file in glob.glob(join(party_dir, '*_detections.csv')):\n                template = [\n                    t for t in tribe if _templates_match(t, family_file)]\n                family = Family(template=template[0] or Template())\n                new_family = True\n                if family.template.name in [f.template.name for f in families]:\n                    family = [\n                        f for f in families if\n                        f.template.name == family.template.name][0]\n                    new_family = False\n                family.detections = _read_family(\n                    fname=family_file, all_cat=all_cat, template=template[0])\n                if new_family:\n                    families.append(family)\n            shutil.rmtree(temp_dir)\n        self.families = families\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the lag of the given sequence of events for a given stream.", "response": "def lag_calc(self, stream, pre_processed, shift_len=0.2, min_cc=0.4,\n                 horizontal_chans=['E', 'N', '1', '2'], vertical_chans=['Z'],\n                 cores=1, interpolate=False, plot=False, parallel=True,\n                 overlap='calculate', process_cores=None, debug=0):\n        \"\"\"\n        Compute picks based on cross-correlation alignment.\n\n        :type stream: obspy.core.stream.Stream\n        :param stream:\n            All the data needed to cut from - can be a gappy Stream.\n        :type pre_processed: bool\n        :param pre_processed:\n            Whether the stream has been pre-processed or not to match the\n            templates. See note below.\n        :type shift_len: float\n        :param shift_len:\n            Shift length allowed for the pick in seconds, will be plus/minus\n            this amount - default=0.2\n        :type min_cc: float\n        :param min_cc:\n            Minimum cross-correlation value to be considered a pick,\n            default=0.4.\n        :type horizontal_chans: list\n        :param horizontal_chans:\n            List of channel endings for horizontal-channels, on which S-picks\n            will be made.\n        :type vertical_chans: list\n        :param vertical_chans:\n            List of channel endings for vertical-channels, on which P-picks\n            will be made.\n        :type cores: int\n        :param cores:\n            Number of cores to use in parallel processing, defaults to one.\n        :type interpolate: bool\n        :param interpolate:\n            Interpolate the correlation function to achieve sub-sample\n            precision.\n        :type plot: bool\n        :param plot:\n            To generate a plot for every detection or not, defaults to False\n        :type parallel: bool\n        :param parallel: Turn parallel processing on or off.\n        :type overlap: float\n        :param overlap:\n            Either None, \"calculate\" or a float of number of seconds to\n            overlap detection streams by.  This is to counter the effects of\n            the delay-and-stack in calculating cross-correlation sums. Setting\n            overlap = \"calculate\" will work out the appropriate overlap based\n            on the maximum lags within templates.\n        :type process_cores: int\n        :param process_cores:\n            Number of processes to use for pre-processing (if different to\n            `cores`).\n        :type debug: int\n        :param debug: Debug output level, 0-5 with 5 being the most output.\n\n        :returns:\n            Catalog of events with picks.  No origin information is included.\n            These events can then be written out via\n            :func:`obspy.core.event.Catalog.write`, or to Nordic Sfiles using\n            :func:`eqcorrscan.utils.sfile_util.eventtosfile` and located\n            externally.\n        :rtype: obspy.core.event.Catalog\n\n        .. Note::\n            Note on pre-processing: You can provide a pre-processed stream,\n            which may be beneficial for detections over large time periods\n            (the stream can have gaps, which reduces memory usage).  However,\n            in this case the processing steps are not checked, so you must\n            ensure that all the template in the Party have the same sampling\n            rate and filtering as the stream.\n            If pre-processing has not be done then the data will be processed\n            according to the parameters in the templates, in this case\n            templates will be grouped by processing parameters and run with\n            similarly processed data.  In this case, all templates do not have\n            to have the same processing parameters.\n\n        .. Note::\n            Picks are corrected for the template pre-pick time.\n        \"\"\"\n        catalog = Catalog()\n        template_groups = [[]]\n        detection_groups = [[]]\n        for master in self.families:\n            master_chans = [(tr.stats.station,\n                             tr.stats.channel) for tr in master.template.st]\n            if len(master_chans) > len(set(master_chans)):\n                warnings.warn(master.template.name +\n                              ' has duplicate channels, will not use this '\n                              'template for lag-calc as this is not coded')\n                continue\n            for group in template_groups:\n                if master.template in group:\n                    break\n            else:\n                new_group = [master.template.copy()]\n                new_det_group = copy.deepcopy(master.detections)\n                for slave in self.families:\n                    if master.template.same_processing(slave.template) and \\\n                                    master.template != slave.template:\n                        slave_chans = [\n                            (tr.stats.station,\n                             tr.stats.channel) for tr in slave.template.st]\n                        if len(slave_chans) > len(set(slave_chans)):\n                            continue\n                        else:\n                            new_group.append(slave.template.copy())\n                            new_det_group.extend(\n                                copy.deepcopy(slave.detections))\n                template_groups.append(new_group)\n                detection_groups.append(new_det_group)\n        # template_groups will contain an empty first list\n        for group, det_group in zip(template_groups, detection_groups):\n            if len(group) == 0:\n                template_groups.remove(group)\n                detection_groups.remove(det_group)\n        # Process the data for each group and time-chunk\n        for group, det_group in zip(template_groups, detection_groups):\n            lap = 0.0\n            for template in group:\n                starts = [t.stats.starttime for t in\n                          template.st.sort(['starttime'])]\n                if starts[-1] - starts[0] > lap:\n                    lap = starts[-1] - starts[0]\n            if overlap is None:\n                lap = 0.0\n            elif isinstance(overlap, float):\n                lap = overlap\n            if not pre_processed:\n                if process_cores is None:\n                    process_cores = cores\n                processed_streams = _group_process(\n                    template_group=group, cores=process_cores,\n                    parallel=parallel, stream=stream.copy(), debug=debug,\n                    daylong=False,  ignore_length=False, overlap=lap)\n                processed_stream = Stream()\n                for p in processed_streams:\n                    processed_stream += p\n                processed_stream.merge(method=1)\n                print(processed_stream)\n            else:\n                processed_stream = stream\n            temp_cat = lag_calc(\n                detections=det_group, detect_data=processed_stream,\n                template_names=[t.name for t in group],\n                templates=[t.st for t in group], shift_len=shift_len,\n                min_cc=min_cc, horizontal_chans=horizontal_chans,\n                vertical_chans=vertical_chans, cores=cores,\n                interpolate=interpolate, plot=plot, parallel=parallel,\n                debug=debug)\n            for event in temp_cat:\n                det = [d for d in det_group\n                       if str(d.id) == str(event.resource_id)][0]\n                pre_pick = [t for t in group\n                            if t.name == det.template_name][0].prepick\n                for pick in event.picks:\n                    pick.time += pre_pick\n            catalog += temp_cat\n        return catalog"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_catalog(self):\n        catalog = Catalog()\n        for fam in self.families:\n            if len(fam.catalog) != 0:\n                catalog.events.extend(fam.catalog.events)\n        return catalog", "response": "Get an obspy catalog object from the party."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef min_chans(self, min_chans):\n        declustered = Party()\n        for family in self.families:\n            fam = Family(family.template)\n            for d in family.detections:\n                if d.no_chans > min_chans:\n                    fam.detections.append(d)\n            declustered.families.append(fam)\n        self.families = declustered.families\n        return self", "response": "Removes detections with fewer channels used than min_chans."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _uniq(self):\n        _detections = []\n        [_detections.append(d) for d in self.detections\n         if not _detections.count(d)]\n        self.detections = _detections\n        return self", "response": "Get list of unique detections."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort(self):\n        self.detections = sorted(self.detections, key=lambda d: d.detect_time)\n        return self", "response": "Sort by detection time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, plot_grouped=False):\n        cumulative_detections(\n            detections=self.detections, plot_grouped=plot_grouped)", "response": "Plot the cumulative number of detections in time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites out the current object to a file.", "response": "def write(self, filename, format='tar'):\n        \"\"\"\n        Write Family out, select output format.\n\n        :type format: str\n        :param format:\n            One of either 'tar', 'csv', or any obspy supported\n            catalog output.\n        :type filename: str\n        :param filename: Path to write file to.\n\n        .. Note:: csv format will write out detection objects, all other\n            outputs will write the catalog.  These cannot be rebuilt into\n            a Family object.  The only format that can be read back into\n            Family objects is the 'tar' type.\n\n        .. Note:: csv format will append detections to filename, all others\n            will overwrite any existing files.\n\n        .. rubric:: Example\n\n        >>> family = Family(\n        ...     template=Template(name='a', st=read()), detections=[\n        ...     Detection(template_name='a', detect_time=UTCDateTime(0) + 200,\n        ...               no_chans=8, detect_val=4.2, threshold=1.2,\n        ...               typeofdet='corr', threshold_type='MAD',\n        ...               threshold_input=8.0),\n        ...     Detection(template_name='a', detect_time=UTCDateTime(0),\n        ...               no_chans=8, detect_val=4.5, threshold=1.2,\n        ...               typeofdet='corr', threshold_type='MAD',\n        ...               threshold_input=8.0),\n        ...     Detection(template_name='a', detect_time=UTCDateTime(0) + 10,\n        ...               no_chans=8, detect_val=4.5, threshold=1.2,\n        ...               typeofdet='corr', threshold_type='MAD',\n        ...               threshold_input=8.0)])\n        >>> family.write('test_family')\n        \"\"\"\n        Party(families=[self]).write(filename=filename, format=format)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the lag of the current state of the entry.", "response": "def lag_calc(self, stream, pre_processed, shift_len=0.2, min_cc=0.4,\n                 horizontal_chans=['E', 'N', '1', '2'], vertical_chans=['Z'],\n                 cores=1, interpolate=False, plot=False, parallel=True,\n                 process_cores=None, debug=0):\n        \"\"\"\n        Compute picks based on cross-correlation alignment.\n\n        :type stream: obspy.core.stream.Stream\n        :param stream:\n            All the data needed to cut from - can be a gappy Stream.\n        :type pre_processed: bool\n        :param pre_processed:\n            Whether the stream has been pre-processed or not to match the\n            templates. See note below.\n        :type shift_len: float\n        :param shift_len:\n            Shift length allowed for the pick in seconds, will be\n            plus/minus this amount - default=0.2\n        :type min_cc: float\n        :param min_cc:\n            Minimum cross-correlation value to be considered a pick,\n            default=0.4.\n        :type horizontal_chans: list\n        :param horizontal_chans:\n            List of channel endings for horizontal-channels, on which\n            S-picks will be made.\n        :type vertical_chans: list\n        :param vertical_chans:\n            List of channel endings for vertical-channels, on which P-picks\n            will be made.\n        :type cores: int\n        :param cores:\n            Number of cores to use in parallel processing, defaults to one.\n        :type interpolate: bool\n        :param interpolate:\n            Interpolate the correlation function to achieve sub-sample\n            precision.\n        :type plot: bool\n        :param plot:\n            To generate a plot for every detection or not, defaults to False\n        :type parallel: bool\n        :param parallel: Turn parallel processing on or off.\n        :type process_cores: int\n        :param process_cores:\n            Number of processes to use for pre-processing (if different to\n            `cores`).\n        :type debug: int\n        :param debug: Debug output level, 0-5 with 5 being the most output.\n\n        :returns:\n            Catalog of events with picks.  No origin information is included.\n            These events can then be written out via\n            :func:`obspy.core.event.Catalog.write`, or to Nordic Sfiles using\n            :func:`eqcorrscan.utils.sfile_util.eventtosfile` and located\n            externally.\n        :rtype: obspy.core.event.Catalog\n\n        .. Note::\n            Note on pre-processing: You can provide a pre-processed stream,\n            which may be beneficial for detections over large time periods\n            (the stream can have gaps, which reduces memory usage).  However,\n            in this case the processing steps are not checked, so you must\n            ensure that all the template in the Party have the same sampling\n            rate and filtering as the stream.\n            If pre-processing has not be done then the data will be processed\n            according to the parameters in the templates, in this case\n            templates will be grouped by processing parameters and run with\n            similarly processed data.  In this case, all templates do not have\n            to have the same processing parameters.\n\n        .. Note::\n            Picks are corrected for the template pre-pick time.\n        \"\"\"\n        return Party(families=[self]).lag_calc(\n            stream=stream, pre_processed=pre_processed, shift_len=shift_len,\n            min_cc=min_cc, horizontal_chans=horizontal_chans,\n            vertical_chans=vertical_chans, cores=cores,\n            interpolate=interpolate, plot=plot, parallel=parallel,\n            process_cores=process_cores, debug=debug)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the templates are processed the same.", "response": "def same_processing(self, other):\n        \"\"\"\n        Check is the templates are processed the same.\n\n        .. rubric:: Example\n\n        >>> template_a = Template(\n        ...     name='a', st=read(), lowcut=2.0, highcut=8.0, samp_rate=100,\n        ...     filt_order=4, process_length=3600, prepick=0.5)\n        >>> template_b = template_a.copy()\n        >>> template_a.same_processing(template_b)\n        True\n        >>> template_b.lowcut = 5.0\n        >>> template_a.same_processing(template_b)\n        False\n        \"\"\"\n        for key in self.__dict__.keys():\n            if key in ['name', 'st', 'prepick', 'event', 'template_info']:\n                continue\n            if not self.__dict__[key] == other.__dict__[key]:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the current object to a file.", "response": "def write(self, filename, format='tar'):\n        \"\"\"\n        Write template.\n\n        :type filename: str\n        :param filename:\n            Filename to write to, if it already exists it will be opened and\n            appended to, otherwise it will be created.\n        :type format: str\n        :param format:\n            Format to write to, either 'tar' (to retain metadata), or any obspy\n            supported waveform format to just extract the waveform.\n\n        .. rubric:: Example\n\n        >>> template_a = Template(\n        ...     name='a', st=read(), lowcut=2.0, highcut=8.0, samp_rate=100,\n        ...     filt_order=4, process_length=3600, prepick=0.5)\n        >>> template_a.write('test_template') # doctest: +NORMALIZE_WHITESPACE\n        Template a:\n         3 channels;\n         lowcut: 2.0 Hz;\n         highcut: 8.0 Hz;\n         sampling rate 100 Hz;\n         filter order: 4;\n         process length: 3600 s\n        >>> template_a.write('test_waveform.ms',\n        ...                  format='MSEED') # doctest: +NORMALIZE_WHITESPACE\n        Template a:\n         3 channels;\n         lowcut: 2.0 Hz;\n         highcut: 8.0 Hz;\n         sampling rate 100 Hz;\n         filter order: 4;\n         process length: 3600 s\n        \"\"\"\n        if format == 'tar':\n            Tribe(templates=[self]).write(filename=filename)\n        else:\n            self.st.write(filename, format=format)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, filename):\n        tribe = Tribe()\n        tribe.read(filename=filename)\n        if len(tribe) > 1:\n            raise IOError('Multiple templates in file')\n        for key in self.__dict__.keys():\n            self.__dict__[key] = tribe[0].__dict__[key]\n        return self", "response": "Read a new template from a tar format with metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting a single template within a continuous stream.", "response": "def detect(self, stream, threshold, threshold_type, trig_int, plotvar,\n               pre_processed=False, daylong=False, parallel_process=True,\n               xcorr_func=None, concurrency=None, cores=None,\n               ignore_length=False, overlap=\"calculate\", debug=0,\n               full_peaks=False):\n        \"\"\"\n        Detect using a single template within a continuous stream.\n\n        :type stream: `obspy.core.stream.Stream`\n        :param stream: Continuous data to detect within using the Template.\n        :type threshold: float\n        :param threshold:\n            Threshold level, if using `threshold_type='MAD'` then this will be\n            the multiple of the median absolute deviation.\n        :type threshold_type: str\n        :param threshold_type:\n            The type of threshold to be used, can be MAD, absolute or\n            av_chan_corr.  See Note on thresholding below.\n        :type trig_int: float\n        :param trig_int:\n            Minimum gap between detections in seconds. If multiple detections\n            occur within trig_int of one-another, the one with the highest\n            cross-correlation sum will be selected.\n        :type plotvar: bool\n        :param plotvar:\n            Turn plotting on or off, see warning about plotting below\n        :type pre_processed: bool\n        :param pre_processed:\n            Set to True if `stream` has already undergone processing, in this\n            case eqcorrscan will only check that the sampling rate is correct.\n            Defaults to False, which will use the\n            :mod:`eqcorrscan.utils.pre_processing` routines to resample and\n            filter the continuous data.\n        :type daylong: bool\n        :param daylong:\n            Set to True to use the\n            :func:`eqcorrscan.utils.pre_processing.dayproc` routine, which\n            preforms additional checks and is more efficient for day-long data\n            over other methods.\n        :type parallel_process: bool\n        :param parallel_process:\n        :type xcorr_func: str or callable\n        :param xcorr_func:\n            A str of a registered xcorr function or a callable for implementing\n            a custom xcorr function. For more details see\n            :func:`eqcorrscan.utils.correlate.register_array_xcorr`.\n        :type concurrency: str\n        :param concurrency:\n            The type of concurrency to apply to the xcorr function. Options are\n            'multithread', 'multiprocess', 'concurrent'. For more details see\n            :func:`eqcorrscan.utils.correlate.get_stream_xcorr`.\n        :type cores: int\n        :param cores: Number of workers for processing and detection.\n        :type ignore_length: bool\n        :param ignore_length:\n            If using daylong=True, then dayproc will try check that the data\n            are there for at least 80% of the day, if you don't want this check\n            (which will raise an error if too much data are missing) then set\n            ignore_length=True.  This is not recommended!\n        :type overlap: float\n        :param overlap:\n            Either None, \"calculate\" or a float of number of seconds to\n            overlap detection streams by.  This is to counter the effects of\n            the delay-and-stack in calcualting cross-correlation sums. Setting\n            overlap = \"calculate\" will work out the appropriate overlap based\n            on the maximum lags within templates.\n        :type debug: int\n        :param debug:\n            Debug level from 0-5 where five is more output, for debug levels\n            4 and 5, detections will not be computed in parallel.\n        :type full_peaks:\n        :param full_peaks: See `eqcorrscan.utils.findpeaks.find_peaks2_short`\n\n        :returns: Family of detections.\n\n        .. Note::\n            `stream` must not be pre-processed. If your data contain gaps\n            you should *NOT* fill those gaps before using this method.\n            The pre-process functions (called within) will fill the gaps\n            internally prior to processing, process the data, then re-fill\n            the gaps with zeros to ensure correlations are not incorrectly\n            calculated within gaps. If your data have gaps you should pass a\n            merged stream without the `fill_value` argument\n            (e.g.: `stream = stream.merge()`).\n\n        .. Note::\n            Detections are not corrected for `pre-pick`, the\n            detection.detect_time corresponds to the beginning of the earliest\n            template channel at detection.\n\n        .. note::\n            **Data overlap:**\n\n            Internally this routine shifts and trims the data according to\n            the offsets in the template (e.g. if trace 2 starts 2 seconds\n            after trace 1 in the template then the continuous data will be\n            shifted by 2 seconds to align peak correlations prior to summing).\n            Because of this, detections at the start and end of continuous data\n            streams **may be missed**.  The maximum time-period that might be\n            missing detections is the maximum offset in the template.\n\n            To work around this, if you are conducting matched-filter\n            detections through long-duration continuous data, we suggest using\n            some overlap (a few seconds, on the order of the maximum offset\n            in the templates) in the continous data.  You will then need to\n            post-process the detections (which should be done anyway to remove\n            duplicates).\n\n        .. note::\n            **Thresholding:**\n\n            **MAD** threshold is calculated as the:\n\n            .. math::\n\n                threshold {\\\\times} (median(abs(cccsum)))\n\n            where :math:`cccsum` is the cross-correlation sum for a\n            given template.\n\n            **absolute** threshold is a true absolute threshold based on the\n            cccsum value.\n\n            **av_chan_corr** is based on the mean values of single-channel\n            cross-correlations assuming all data are present as required\n            for the template, e.g:\n\n            .. math::\n\n                av\\_chan\\_corr\\_thresh=threshold \\\\times (cccsum /\n                len(template))\n\n            where :math:`template` is a single template from the input and the\n            length is the number of channels within this template.\n\n        .. Note::\n            See tutorials for example.\n        \"\"\"\n        party = _group_detect(\n            templates=[self], stream=stream.copy(), threshold=threshold,\n            threshold_type=threshold_type, trig_int=trig_int,\n            plotvar=plotvar, pre_processed=pre_processed, daylong=daylong,\n            parallel_process=parallel_process, xcorr_func=xcorr_func,\n            concurrency=concurrency, cores=cores, ignore_length=ignore_length,\n            overlap=overlap, debug=debug, full_peaks=full_peaks)\n        return party[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a new template using a given method.", "response": "def construct(self, method, name, lowcut, highcut, samp_rate, filt_order,\n                  prepick, **kwargs):\n        \"\"\"\n        Construct a template using a given method.\n\n        :param method:\n            Method to make the template,\n            see :mod:`eqcorrscan.core.template_gen` for possible methods.\n        :type method: str\n        :type name: str\n        :param name: Name for the template\n        :type lowcut: float\n        :param lowcut:\n            Low cut (Hz), if set to None will not apply a lowcut\n        :type highcut: float\n        :param highcut:\n            High cut (Hz), if set to None will not apply a highcut.\n        :type samp_rate: float\n        :param samp_rate:\n            New sampling rate in Hz.\n        :type filt_order: int\n        :param filt_order:\n            Filter level (number of corners).\n        :type prepick: float\n        :param prepick: Pre-pick time in seconds\n\n        .. Note::\n            methods `from_meta_file`, `from_seishub`, `from_client` and\n            `multi_template_gen` are not accommodated in this function and must\n            be called from Tribe.construct as these generate multiple\n            templates.\n\n        .. Note::\n            Calls functions from `eqcorrscan.core.template_gen`, see these\n            functions for details on what further arguments are required.\n\n        .. rubric:: Example\n\n        >>> # Get the path to the test data\n        >>> import eqcorrscan\n        >>> import os\n        >>> TEST_PATH = (\n        ...     os.path.dirname(eqcorrscan.__file__) + '/tests/test_data')\n        >>> sac_files = glob.glob(TEST_PATH + '/SAC/2014p611252/*')\n        >>> template = Template().construct(\n        ...     method='from_sac', name='test', lowcut=2.0, highcut=8.0,\n        ...     samp_rate=20.0, filt_order=4, prepick=0.1, swin='all',\n        ...     length=2.0, sac_files=sac_files)\n        >>> print(template) # doctest: +NORMALIZE_WHITESPACE\n        Template test:\n         12 channels;\n         lowcut: 2.0 Hz;\n         highcut: 8.0 Hz;\n         sampling rate 20.0 Hz;\n         filter order: 4;\n         process length: 300.0 s\n\n\n        This will raise an error if the method is unsupported:\n\n        >>> template = Template().construct(\n        ...     method='from_meta_file', name='test', lowcut=2.0, highcut=8.0,\n        ...     samp_rate=20.0, filt_order=4, prepick=0.1, swin='all',\n        ...     length=2.0) # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        NotImplementedError: Method is not supported, use \\\n        Tribe.construct instead.\n\n        \"\"\"\n        if method in ['from_meta_file', 'from_seishub', 'from_client',\n                      'multi_template_gen']:\n            raise NotImplementedError('Method is not supported, '\n                                      'use Tribe.construct instead.')\n        streams, events, process_lengths = template_gen.template_gen(\n            method=method, lowcut=lowcut, highcut=highcut,\n            filt_order=filt_order, samp_rate=samp_rate, prepick=prepick,\n            return_event=True, **kwargs)\n        self.name = name\n        st = streams[0]\n        event = events[0]\n        process_length = process_lengths[0]\n        for tr in st:\n            if not np.any(tr.data.astype(np.float16)):\n                warnings.warn('Data are zero in float16, missing data,'\n                              ' will not use: %s' % tr.id)\n                st.remove(tr)\n        self.st = st\n        self.lowcut = lowcut\n        self.highcut = highcut\n        self.filt_order = filt_order\n        self.samp_rate = samp_rate\n        self.process_length = process_length\n        self.prepick = prepick\n        self.event = event\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the tribe by template name.", "response": "def sort(self):\n        \"\"\"\n        Sort the tribe, sorts by template name.\n\n        .. rubric:: Example\n\n        >>> tribe = Tribe(templates=[Template(name='c'), Template(name='b'),\n        ...                          Template(name='a')])\n        >>> tribe.sort()\n        Tribe of 3 templates\n        >>> tribe[0] # doctest: +NORMALIZE_WHITESPACE\n        Template a:\n         0 channels;\n         lowcut: None Hz;\n         highcut: None Hz;\n         sampling rate None Hz;\n         filter order: None;\n         process length: None s\n        \"\"\"\n        self.templates = sorted(self.templates, key=lambda x: x.name)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select(self, template_name):\n        return [t for t in self.templates if t.name == template_name][0]", "response": "Select a particular template from the tribe."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a template from the tribe.", "response": "def remove(self, template):\n        \"\"\"\n        Remove a template from the tribe.\n\n        :type template: :class:`eqcorrscan.core.match_filter.Template`\n        :param template: Template to remove from tribe\n\n        .. rubric:: Example\n\n        >>> tribe = Tribe(templates=[Template(name='c'), Template(name='b'),\n        ...                          Template(name='a')])\n        >>> tribe.remove(tribe.templates[0])\n        Tribe of 2 templates\n        \"\"\"\n        self.templates = [t for t in self.templates if t != template]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, filename, compress=True, catalog_format=\"QUAKEML\"):\n        if catalog_format not in CAT_EXT_MAP.keys():\n            raise TypeError(\"{0} is not supported\".format(catalog_format))\n        if not os.path.isdir(filename):\n            os.makedirs(filename)\n        self._par_write(filename)\n        tribe_cat = Catalog()\n        for t in self.templates:\n            if t.event is not None:\n                tribe_cat.append(t.event)\n        if len(tribe_cat) > 0:\n            tribe_cat.write(\n                os.path.join(filename, 'tribe_cat.{0}'.format(\n                    CAT_EXT_MAP[catalog_format])), format=catalog_format)\n        for template in self.templates:\n            template.st.write(filename + '/' + template.name + '.ms',\n                              format='MSEED')\n        if compress:\n            with tarfile.open(filename + '.tgz', \"w:gz\") as tar:\n                tar.add(filename, arcname=os.path.basename(filename))\n            shutil.rmtree(filename)\n        return self", "response": "Writes the tribe to a tar archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a tribe of templates from a tar formatted file.", "response": "def read(self, filename):\n        \"\"\"\n        Read a tribe of templates from a tar formatted file.\n\n        :type filename: str\n        :param filename: File to read templates from.\n\n        .. rubric:: Example\n\n        >>> tribe = Tribe(templates=[Template(name='c', st=read())])\n        >>> tribe.write('test_tribe')\n        Tribe of 1 templates\n        >>> tribe_back = Tribe().read('test_tribe.tgz')\n        >>> tribe_back == tribe\n        True\n        \"\"\"\n        with tarfile.open(filename, \"r:*\") as arc:\n            temp_dir = tempfile.mkdtemp()\n            arc.extractall(path=temp_dir, members=_safemembers(arc))\n            tribe_dir = glob.glob(temp_dir + os.sep + '*')[0]\n            self._read_from_folder(dirname=tribe_dir)\n        shutil.rmtree(temp_dir)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncluster the tribe. Cluster templates within a tribe: returns multiple tribes each of which could be stacked. :type method: str :param method: Method of stacking, see :mod:`eqcorrscan.utils.clustering` :return: List of tribes. .. rubric:: Example", "response": "def cluster(self, method, **kwargs):\n        \"\"\"\n        Cluster the tribe.\n\n        Cluster templates within a tribe: returns multiple tribes each of\n        which could be stacked.\n\n        :type method: str\n        :param method:\n            Method of stacking, see :mod:`eqcorrscan.utils.clustering`\n\n        :return: List of tribes.\n\n        .. rubric:: Example\n\n\n        \"\"\"\n        from eqcorrscan.utils import clustering\n        tribes = []\n        func = getattr(clustering, method)\n        if method in ['space_cluster', 'space_time_cluster']:\n            cat = Catalog([t.event for t in self.templates])\n            groups = func(cat, **kwargs)\n            for group in groups:\n                new_tribe = Tribe()\n                for event in group:\n                    new_tribe.templates.extend([t for t in self.templates\n                                                if t.event == event])\n                tribes.append(new_tribe)\n        return tribes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect the most recent data within a continuous stream.", "response": "def detect(self, stream, threshold, threshold_type, trig_int, plotvar,\n               daylong=False, parallel_process=True, xcorr_func=None,\n               concurrency=None, cores=None, ignore_length=False,\n               group_size=None, overlap=\"calculate\", debug=0,\n               full_peaks=False, save_progress=False,\n               process_cores=None, **kwargs):\n        \"\"\"\n        Detect using a Tribe of templates within a continuous stream.\n\n        :type stream: `obspy.core.stream.Stream`\n        :param stream: Continuous data to detect within using the Template.\n        :type threshold: float\n        :param threshold:\n            Threshold level, if using `threshold_type='MAD'` then this will be\n            the multiple of the median absolute deviation.\n        :type threshold_type: str\n        :param threshold_type:\n            The type of threshold to be used, can be MAD, absolute or\n            av_chan_corr.  See Note on thresholding below.\n        :type trig_int: float\n        :param trig_int:\n            Minimum gap between detections in seconds. If multiple detections\n            occur within trig_int of one-another, the one with the highest\n            cross-correlation sum will be selected.\n        :type plotvar: bool\n        :param plotvar:\n            Turn plotting on or off, see warning about plotting below\n        :type daylong: bool\n        :param daylong:\n            Set to True to use the\n            :func:`eqcorrscan.utils.pre_processing.dayproc` routine, which\n            preforms additional checks and is more efficient for day-long data\n            over other methods.\n        :type parallel_process: bool\n        :param parallel_process:\n        :type xcorr_func: str or callable\n        :param xcorr_func:\n            A str of a registered xcorr function or a callable for implementing\n            a custom xcorr function. For more information see:\n            :func:`eqcorrscan.utils.correlate.register_array_xcorr`\n        :type concurrency: str\n        :param concurrency:\n            The type of concurrency to apply to the xcorr function. Options are\n            'multithread', 'multiprocess', 'concurrent'. For more details see\n            :func:`eqcorrscan.utils.correlate.get_stream_xcorr`\n        :type cores: int\n        :param cores: Number of workers for procesisng and detection.\n        :type ignore_length: bool\n        :param ignore_length:\n            If using daylong=True, then dayproc will try check that the data\n            are there for at least 80% of the day, if you don't want this check\n            (which will raise an error if too much data are missing) then set\n            ignore_length=True.  This is not recommended!\n        :type group_size: int\n        :param group_size:\n            Maximum number of templates to run at once, use to reduce memory\n            consumption, if unset will use all templates.\n        :type overlap: float\n        :param overlap:\n            Either None, \"calculate\" or a float of number of seconds to\n            overlap detection streams by.  This is to counter the effects of\n            the delay-and-stack in calculating cross-correlation sums. Setting\n            overlap = \"calculate\" will work out the appropriate overlap based\n            on the maximum lags within templates.\n        :type debug: int\n        :param debug:\n            Debug level from 0-5 where five is more output, for debug levels\n            4 and 5, detections will not be computed in parallel.\n        :type full_peaks: bool\n        :param full_peaks: See `eqcorrscan.utils.findpeak.find_peaks2_short`\n        :type save_progress: bool\n        :param save_progress:\n            Whether to save the resulting party at every data step or not.\n            Useful for long-running processes.\n        :type process_cores: int\n        :param process_cores:\n            Number of processes to use for pre-processing (if different to\n            `cores`).\n\n        :return:\n            :class:`eqcorrscan.core.match_filter.Party` of Families of\n            detections.\n\n        .. Note::\n            `stream` must not be pre-processed. If your data contain gaps\n            you should *NOT* fill those gaps before using this method.\n            The pre-process functions (called within) will fill the gaps\n            internally prior to processing, process the data, then re-fill\n            the gaps with zeros to ensure correlations are not incorrectly\n            calculated within gaps. If your data have gaps you should pass a\n            merged stream without the `fill_value` argument\n            (e.g.: `stream = stream.merge()`).\n\n        .. Note::\n            Detections are not corrected for `pre-pick`, the\n            detection.detect_time corresponds to the beginning of the earliest\n            template channel at detection.\n\n        .. warning::\n            Picks included in the output Party.get_catalog() will not be\n            corrected for pre-picks in the template.\n\n        .. note::\n            **Data overlap:**\n\n            Internally this routine shifts and trims the data according to the\n            offsets in the template (e.g. if trace 2 starts 2 seconds after\n            trace 1 in the template then the continuous data will be shifted\n            by 2 seconds to align peak correlations prior to summing).\n            Because of this, detections at the start and end of continuous\n            data streams **may be missed**.  The maximum time-period that\n            might be missing detections is the maximum offset in the template.\n\n            To work around this, if you are conducting matched-filter\n            detections through long-duration continuous data, we suggest\n            using some overlap (a few seconds, on the order of the maximum\n            offset in the templates) in the continuous data.  You will then\n            need to post-process the detections (which should be done anyway\n            to remove duplicates).  See below note for how `overlap` argument\n            affects data internally if `stream` is longer than the processing\n            length.\n\n        .. Note::\n            If `stream` is longer than processing length, this routine will\n            ensure that data overlap between loops, which will lead to no\n            missed detections at data start-stop points (see above note).\n            This will result in end-time not being strictly\n            honoured, so detections may occur after the end-time set.  This is\n            because data must be run in the correct process-length.\n\n        .. note::\n            **Thresholding:**\n\n            **MAD** threshold is calculated as the:\n\n            .. math::\n\n                threshold {\\\\times} (median(abs(cccsum)))\n\n            where :math:`cccsum` is the cross-correlation sum for a given\n            template.\n\n            **absolute** threshold is a true absolute threshold based on the\n            cccsum value.\n\n            **av_chan_corr** is based on the mean values of single-channel\n            cross-correlations assuming all data are present as required for\n            the template, e.g:\n\n            .. math::\n\n                av\\_chan\\_corr\\_thresh=threshold \\\\times (cccsum /\n                len(template))\n\n            where :math:`template` is a single template from the input and the\n            length is the number of channels within this template.\n        \"\"\"\n        party = Party()\n        template_groups = []\n        for master in self.templates:\n            for group in template_groups:\n                if master in group:\n                    break\n            else:\n                new_group = [master]\n                for slave in self.templates:\n                    if master.same_processing(slave) and master != slave:\n                        new_group.append(slave)\n                template_groups.append(new_group)\n        # template_groups will contain an empty first list\n        for group in template_groups:\n            if len(group) == 0:\n                template_groups.remove(group)\n        # now we can compute the detections for each group\n        for group in template_groups:\n            group_party = _group_detect(\n                templates=group, stream=stream.copy(), threshold=threshold,\n                threshold_type=threshold_type, trig_int=trig_int,\n                plotvar=plotvar, group_size=group_size, pre_processed=False,\n                daylong=daylong, parallel_process=parallel_process,\n                xcorr_func=xcorr_func, concurrency=concurrency, cores=cores,\n                ignore_length=ignore_length, overlap=overlap, debug=debug,\n                full_peaks=full_peaks, process_cores=process_cores, **kwargs)\n            party += group_party\n            if save_progress:\n                party.write(\"eqcorrscan_temporary_party\")\n        if len(party) > 0:\n            for family in party:\n                if family is not None:\n                    family.detections = family._uniq().detections\n        return party"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting a set of known - in - flight data for a given client.", "response": "def client_detect(self, client, starttime, endtime, threshold,\n                      threshold_type, trig_int, plotvar, min_gap=None,\n                      daylong=False, parallel_process=True, xcorr_func=None,\n                      concurrency=None, cores=None, ignore_length=False,\n                      group_size=None, debug=0, return_stream=False,\n                      full_peaks=False, save_progress=False,\n                      process_cores=None, retries=3, **kwargs):\n        \"\"\"\n        Detect using a Tribe of templates within a continuous stream.\n\n        :type client: `obspy.clients.*.Client`\n        :param client: Any obspy client with a dataselect service.\n        :type starttime: :class:`obspy.core.UTCDateTime`\n        :param starttime: Start-time for detections.\n        :type endtime: :class:`obspy.core.UTCDateTime`\n        :param endtime: End-time for detections\n        :type threshold: float\n        :param threshold:\n            Threshold level, if using `threshold_type='MAD'` then this will be\n            the multiple of the median absolute deviation.\n        :type threshold_type: str\n        :param threshold_type:\n            The type of threshold to be used, can be MAD, absolute or\n            av_chan_corr.  See Note on thresholding below.\n        :type trig_int: float\n        :param trig_int:\n            Minimum gap between detections in seconds. If multiple detections\n            occur within trig_int of one-another, the one with the highest\n            cross-correlation sum will be selected.\n        :type plotvar: bool\n        :param plotvar:\n            Turn plotting on or off, see warning about plotting below\n        :type min_gap: float\n        :param min_gap:\n            Minimum gap allowed in data - use to remove traces with known\n            issues\n        :type daylong: bool\n        :param daylong:\n            Set to True to use the\n            :func:`eqcorrscan.utils.pre_processing.dayproc` routine, which\n            preforms additional checks and is more efficient for day-long data\n            over other methods.\n        :type parallel_process: bool\n        :param parallel_process:\n        :type xcorr_func: str or callable\n        :param xcorr_func:\n            A str of a registered xcorr function or a callable for implementing\n            a custom xcorr function. For more information see:\n            :func:`eqcorrscan.utils.correlate.register_array_xcorr`\n        :type concurrency: str\n        :param concurrency:\n            The type of concurrency to apply to the xcorr function. Options are\n            'multithread', 'multiprocess', 'concurrent'. For more details see\n            :func:`eqcorrscan.utils.correlate.get_stream_xcorr`\n        :type cores: int\n        :param cores: Number of workers for processing and detection.\n        :type ignore_length: bool\n        :param ignore_length:\n            If using daylong=True, then dayproc will try check that the data\n            are there for at least 80% of the day, if you don't want this check\n            (which will raise an error if too much data are missing) then set\n            ignore_length=True.  This is not recommended!\n        :type group_size: int\n        :param group_size:\n            Maximum number of templates to run at once, use to reduce memory\n            consumption, if unset will use all templates.\n        :type full_peaks: bool\n        :param full_peaks: See `eqcorrscan.utils.findpeaks.find_peaks2_short`\n        :type save_progress: bool\n        :param save_progress:\n            Whether to save the resulting party at every data step or not.\n            Useful for long-running processes.\n        :type process_cores: int\n        :param process_cores:\n            Number of processes to use for pre-processing (if different to\n            `cores`).\n        :type debug: int\n        :param debug:\n            Debug level from 0-5 where five is more output, for debug levels\n            4 and 5, detections will not be computed in parallel.\n        :type return_stream: bool\n        :param return_stream:\n            Whether to also output the stream downloaded, useful if you plan\n            to use the stream for something else, e.g. lag_calc.\n        :type retries: int\n        :param retries:\n            Number of attempts allowed for downloading - allows for transient\n            server issues.\n\n        :return:\n            :class:`eqcorrscan.core.match_filter.Party` of Families of\n            detections.\n\n        .. Note::\n            Detections are not corrected for `pre-pick`, the\n            detection.detect_time corresponds to the beginning of the earliest\n            template channel at detection.\n\n        .. warning::\n            Picks included in the output Party.get_catalog() will not be\n            corrected for pre-picks in the template.\n\n        .. Note::\n            Ensures that data overlap between loops, which will lead to no\n            missed detections at data start-stop points (see note for\n            :meth:`eqcorrscan.core.match_filter.Tribe.detect` method).\n            This will result in end-time not being strictly\n            honoured, so detections may occur after the end-time set.  This is\n            because data must be run in the correct process-length.\n\n        .. warning::\n            Plotting within the match-filter routine uses the Agg backend\n            with interactive plotting turned off.  This is because the function\n            is designed to work in bulk.  If you wish to turn interactive\n            plotting on you must import matplotlib in your script first,\n            when you then import match_filter you will get the warning that\n            this call to matplotlib has no effect, which will mean that\n            match_filter has not changed the plotting behaviour.\n\n        .. note::\n            **Thresholding:**\n\n            **MAD** threshold is calculated as the:\n\n            .. math::\n\n                threshold {\\\\times} (median(abs(cccsum)))\n\n            where :math:`cccsum` is the cross-correlation sum for a given\n            template.\n\n            **absolute** threshold is a true absolute threshold based on the\n            cccsum value.\n\n            **av_chan_corr** is based on the mean values of single-channel\n            cross-correlations assuming all data are present as required for\n            the template, e.g:\n\n            .. math::\n\n                av\\_chan\\_corr\\_thresh=threshold \\\\times (cccsum /\n                len(template))\n\n            where :math:`template` is a single template from the input and the\n            length is the number of channels within this template.\n        \"\"\"\n        party = Party()\n        buff = 300\n        # Apply a buffer, often data downloaded is not the correct length\n        data_length = max([t.process_length for t in self.templates])\n        pad = 0\n        for template in self.templates:\n            max_delay = (template.st.sort(['starttime'])[-1].stats.starttime -\n                         template.st.sort(['starttime'])[0].stats.starttime)\n            if max_delay > pad:\n                pad = max_delay\n        download_groups = int(endtime - starttime) / data_length\n        template_channel_ids = []\n        for template in self.templates:\n            for tr in template.st:\n                if tr.stats.network not in [None, '']:\n                    chan_id = (tr.stats.network,)\n                else:\n                    chan_id = ('*',)\n                if tr.stats.station not in [None, '']:\n                    chan_id += (tr.stats.station,)\n                else:\n                    chan_id += ('*',)\n                if tr.stats.location not in [None, '']:\n                    chan_id += (tr.stats.location,)\n                else:\n                    chan_id += ('*',)\n                if tr.stats.channel not in [None, '']:\n                    if len(tr.stats.channel) == 2:\n                        chan_id += (tr.stats.channel[0] + '?' +\n                                    tr.stats.channel[-1],)\n                    else:\n                        chan_id += (tr.stats.channel,)\n                else:\n                    chan_id += ('*',)\n                template_channel_ids.append(chan_id)\n        template_channel_ids = list(set(template_channel_ids))\n        if return_stream:\n            stream = Stream()\n        if int(download_groups) < download_groups:\n            download_groups = int(download_groups) + 1\n        else:\n            download_groups = int(download_groups)\n        for i in range(download_groups):\n            bulk_info = []\n            for chan_id in template_channel_ids:\n                bulk_info.append((\n                    chan_id[0], chan_id[1], chan_id[2], chan_id[3],\n                    starttime + (i * data_length) - (pad + buff),\n                    starttime + ((i + 1) * data_length) + (pad + buff)))\n            for retry_attempt in range(retries):\n                try:\n                    st = client.get_waveforms_bulk(bulk_info)\n                    break\n                except Exception as e:\n                    print(e)\n                    continue\n            else:\n                raise MatchFilterError(\n                    \"Could not download data after {0} attempts\".format(\n                        retries))\n            # Get gaps and remove traces as necessary\n            if min_gap:\n                gaps = st.get_gaps(min_gap=min_gap)\n                if len(gaps) > 0:\n                    print(\"Large gaps in downloaded data\")\n                    st.merge()\n                    gappy_channels = list(\n                        set([(gap[0], gap[1], gap[2], gap[3])\n                             for gap in gaps]))\n                    _st = Stream()\n                    for tr in st:\n                        tr_stats = (tr.stats.network, tr.stats.station,\n                                    tr.stats.location, tr.stats.channel)\n                        if tr_stats in gappy_channels:\n                            print(\"Removing gappy channel: %s\" % str(tr))\n                        else:\n                            _st += tr\n                    st = _st\n                    st.split()\n            st.merge()\n            st.trim(starttime=starttime + (i * data_length) - pad,\n                    endtime=starttime + ((i + 1) * data_length) + pad)\n            for tr in st:\n                if not _check_daylong(tr):\n                    st.remove(tr)\n                    print(\"{0} contains more zeros than non-zero, \"\n                          \"removed\".format(tr.id))\n            for tr in st:\n                if tr.stats.endtime - tr.stats.starttime < \\\n                   0.8 * data_length:\n                    st.remove(tr)\n                    print(\"{0} is less than 80% of the required length\"\n                          \", removed\".format(tr.id))\n            if return_stream:\n                stream += st\n            try:\n                party += self.detect(\n                    stream=st, threshold=threshold,\n                    threshold_type=threshold_type, trig_int=trig_int,\n                    plotvar=plotvar, daylong=daylong,\n                    parallel_process=parallel_process, xcorr_func=xcorr_func,\n                    concurrency=concurrency, cores=cores,\n                    ignore_length=ignore_length, group_size=group_size,\n                    overlap=None, debug=debug, full_peaks=full_peaks,\n                    process_cores=process_cores, **kwargs)\n                if save_progress:\n                    party.write(\"eqcorrscan_temporary_party\")\n            except Exception as e:\n                print('Error, routine incomplete, returning incomplete Party')\n                print('Error: %s' % str(e))\n                if return_stream:\n                    return party, stream\n                else:\n                    return party\n        for family in party:\n            if family is not None:\n                family.detections = family._uniq().detections\n        if return_stream:\n            return party, stream\n        else:\n            return party"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef construct(self, method, lowcut, highcut, samp_rate, filt_order,\n                  prepick, save_progress=False, **kwargs):\n        \"\"\"\n        Generate a Tribe of Templates.\n\n        See :mod:`eqcorrscan.core.template_gen` for available methods.\n\n        :param method: Method of Tribe generation.\n        :param kwargs: Arguments for the given method.\n        :type lowcut: float\n        :param lowcut:\n            Low cut (Hz), if set to None will not apply a lowcut\n        :type highcut: float\n        :param highcut:\n            High cut (Hz), if set to None will not apply a highcut.\n        :type samp_rate: float\n        :param samp_rate:\n            New sampling rate in Hz.\n        :type filt_order: int\n        :param filt_order:\n            Filter level (number of corners).\n        :type prepick: float\n        :param prepick: Pre-pick time in seconds\n        :type save_progress: bool\n        :param save_progress:\n            Whether to save the resulting party at every data step or not.\n            Useful for long-running processes.\n\n        .. Note::\n            Methods: `from_contbase`, `from_sfile` and `from_sac` are not\n            supported by Tribe.construct and must use Template.construct.\n\n        .. Note::\n            The Method `multi_template_gen` is not supported because the\n            processing parameters for the stream are not known. Use\n            `from_meta_file` instead.\n\n        .. Note:: Templates will be named according to their start-time.\n        \"\"\"\n        templates, catalog, process_lengths = template_gen.template_gen(\n            method=method, lowcut=lowcut, highcut=highcut,\n            filt_order=filt_order, samp_rate=samp_rate, prepick=prepick,\n            return_event=True, save_progress=save_progress, **kwargs)\n        for template, event, process_len in zip(templates, catalog,\n                                                process_lengths):\n            t = Template()\n            for tr in template:\n                if not np.any(tr.data.astype(np.float16)):\n                    warnings.warn('Data are zero in float16, missing data,'\n                                  ' will not use: %s' % tr.id)\n                    template.remove(tr)\n            if len(template) == 0:\n                print('Empty Template')\n                continue\n            t.st = template\n            t.name = template.sort(['starttime'])[0]. \\\n                stats.starttime.strftime('%Y_%m_%dt%H_%M_%S')\n            t.lowcut = lowcut\n            t.highcut = highcut\n            t.filt_order = filt_order\n            t.samp_rate = samp_rate\n            t.process_length = process_len\n            t.prepick = prepick\n            event.comments.append(Comment(\n                text=\"eqcorrscan_template_\" + t.name,\n                creation_info=CreationInfo(agency='eqcorrscan',\n                                           author=getpass.getuser())))\n            t.event = event\n            self.templates.append(t)\n        return self", "response": "Generate a Tribe of Templates."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the detection to a CSV formatted file.", "response": "def write(self, fname, append=True):\n        \"\"\"\n        Write detection to csv formatted file.\n\n        Will append if append==True and file exists\n\n        :type fname: str\n        :param fname: Full path to file to open and write to.\n        :type append: bool\n        :param append: Set to true to append to an existing file, if True \\\n            and file doesn't exist, will create new file and warn.  If False\n            will overwrite old files.\n        \"\"\"\n        mode = 'w'\n        if append and os.path.isfile(fname):\n            mode = 'a'\n        header = '; '.join(['Template name', 'Detection time (UTC)',\n                            'Number of channels', 'Channel list',\n                            'Detection value', 'Threshold',\n                            'Threshold type', 'Input threshold',\n                            'Detection type'])\n        print_str = \"{0}; {1}; {2}; {3}; {4}; {5}; {6}; {7}; {8}\\n\".format(\n            self.template_name, self.detect_time, self.no_chans,\n            self.chans, self.detect_val, self.threshold,\n            self.threshold_type, self.threshold_input, self.typeofdet)\n        with open(fname, mode) as _f:\n            _f.write(header + '\\n')  # Write a header for the file\n            _f.write(print_str)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculate_event(self, template=None, template_st=None):\n        if template is not None and template.name != self.template_name:\n            print(\"Template names do not match: {0}: {1}\".format(\n                template.name, self.template_name))\n            return\n        # Detect time must be valid QuakeML uri within resource_id.\n        # This will write a formatted string which is still\n        # readable by UTCDateTime\n        det_time = str(self.detect_time.strftime('%Y%m%dT%H%M%S.%f'))\n        ev = Event(resource_id=ResourceIdentifier(\n            id=self.template_name + '_' + det_time,\n            prefix='smi:local'))\n        ev.creation_info = CreationInfo(\n            author='EQcorrscan', creation_time=UTCDateTime())\n        ev.comments.append(\n            Comment(text='threshold={0}'.format(self.threshold)))\n        ev.comments.append(\n            Comment(text='detect_val={0}'.format(self.detect_val)))\n        if self.chans is not None:\n            ev.comments.append(\n                Comment(text='channels used: {0}'.format(\n                    ' '.join([str(pair) for pair in self.chans]))))\n        if template is not None:\n            template_st = template.st\n        min_template_tm = min(\n            [tr.stats.starttime for tr in template_st])\n        for tr in template_st:\n            if (tr.stats.station, tr.stats.channel) \\\n                    not in self.chans:\n                continue\n            elif tr.stats.__contains__(\"not_in_original\"):\n                continue\n            else:\n                pick_time = self.detect_time + (\n                        tr.stats.starttime - min_template_tm)\n                ev.picks.append(Pick(\n                    time=pick_time, waveform_id=WaveformStreamID(\n                        network_code=tr.stats.network,\n                        station_code=tr.stats.station,\n                        channel_code=tr.stats.channel,\n                        location_code=tr.stats.location)))\n        self.event = ev\n        return", "response": "Calculate an event for this detection using a given template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread in. csv files of slowness generated from Grid2Time.", "response": "def _read_tt(path, stations, phase, phaseout='S', ps_ratio=1.68,\n             lags_switch=True):\n    \"\"\"\n    Read in .csv files of slowness generated from Grid2Time.\n\n    Converts these data to a useful format here.\n\n    It should be noted that this can read either P or S travel-time grids, not\n    both at the moment.\n\n    :type path: str\n    :param path: The path to the .csv Grid2Time outputs\n    :type stations: list\n    :param stations: List of station names to read slowness files for.\n    :type phase: str\n    :param phase: Input phase type.\n    :type phaseout: str\n    :param phaseout: What phase to return the lagtimes in.\n    :type ps_ratio: float\n    :param ps_ratio: p to s ratio for conversion\n    :type lags_switch: bool\n    :param lags_switch:\n        Return lags or raw travel-times, if set to true will return lags.\n\n    :returns: Stations\n    :rtype: list\n    :returns: List of lists of tuples of node locations\n    :rtype: list\n    :returns: Array of lags.\n    :rtype: :class:`numpy.ndarray`\n\n    .. note::\n        **Output:**\n\n        station[1] refers to nodes[1] and lags[1] nodes[1][1] refers\n        to station[1] and lags[1][1] nodes[n][n] is a tuple of latitude,\n        longitude and depth.\n\n    .. note::\n        This function currently needs comma separated grid files in\n        NonLinLoc format.  Only certain versions of NonLinLoc write these csv\n        files, however it should be possible to read the binary files directly.\n        If you find you need this capability let us know and we can try and\n        implement it.\n    \"\"\"\n    # Locate the slowness file information\n    gridfiles = []\n    stations_out = []\n    for station in stations:\n        gridfiles += (glob.glob(path + '*.' + phase + '.' + station +\n                      '.time.csv'))\n        if glob.glob(path + '*.' + phase + '.' + station + '*.csv'):\n            stations_out += [station]\n    # Read the files\n    allnodes = []\n    for gridfile in gridfiles:\n        print('     Reading slowness from: ' + gridfile)\n        f = open(gridfile, 'r')\n        grid = csv.reader(f, delimiter=str(' '))\n        traveltime = []\n        nodes = []\n        for row in grid:\n            nodes.append((float(row[0]), float(row[1]), float(row[2])))\n            traveltime.append(float(row[3]))\n        traveltime = np.array(traveltime)\n        if not phase == phaseout:\n            if phase == 'S':\n                traveltime = traveltime / ps_ratio\n            else:\n                traveltime = traveltime * ps_ratio\n        if lags_switch:\n            lags = traveltime - min(traveltime)\n        else:\n            lags = traveltime\n        if 'alllags' not in locals():\n            alllags = [lags]\n        else:\n            alllags = np.concatenate((alllags, [lags]), axis=0)\n        allnodes = nodes\n        # each element of allnodes should be the same as the\n        # other one, e.g. for each station the grid must be the\n        # same, hence allnodes=nodes\n        f.close()\n    alllags = np.array(alllags)\n    return stations_out, allnodes, alllags"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resample_grid(stations, nodes, lags, mindepth, maxdepth, corners):\n    resamp_nodes = []\n    resamp_lags = []\n    # Cut the volume\n    for i, node in enumerate(nodes):\n        # If the node is within the volume range, keep it\n        if mindepth < float(node[2]) < maxdepth and\\\n           corners.contains_point(node[0:2]):\n                resamp_nodes.append(node)\n                resamp_lags.append([lags[:, i]])\n    # Reshape the lags\n    print(np.shape(resamp_lags))\n    resamp_lags = np.reshape(resamp_lags, (len(resamp_lags), len(stations))).T\n    # Resample the nodes - they are sorted in order of size with largest long\n    # then largest lat, then depth.\n    print(' '.join(['Grid now has ', str(len(resamp_nodes)), 'nodes']))\n    return stations, resamp_nodes, resamp_lags", "response": "Resample the lagtime grid to a given volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove nodes that have a very similar network moveout to another node. This function will, for each node, calculate the difference in lagtime at each station at every node, then sum these for each node to get a cumulative difference in network moveout. This will result in an array of arrays with zeros on the diagonal. :type stations: list :param stations: List of station names from in the form where stations[i] refers to nodes[i][:] and lags[i][:] :type nodes: list :param nodes: List of node points where nodes[i] referes to stations[i] and nodes[:][:][0] is latitude in degrees, nodes[:][:][1] is longitude in degrees, nodes[:][:][2] is depth in km. :type lags: numpy.ndarray :param lags: Array of arrays where lags[i][:] refers to stations[i]. lags[i][j] should be the delay to the nodes[i][j] for stations[i] in seconds. :type threshold: float :param threshold: Threshold for removal in seconds :returns: Stations :rtype: list :returns: List of lists of tuples of node locations :rtype: list :returns: Array of lags. :rtype: :class:`numpy.ndarray` .. note:: **Output:** station[1] refers to nodes[1] and lags[1] nodes[1][1] refers to station[1] and lags[1][1] nodes[n][n] is a tuple of latitude, longitude and depth.", "response": "def _rm_similarlags(stations, nodes, lags, threshold):\n    \"\"\"\n    Remove nodes that have a very similar network moveout to another node.\n\n    This function will, for each node, calculate the difference in lagtime\n    at each station at every node, then sum these for each node to get a\n    cumulative difference in network moveout.  This will result in an\n    array of arrays with zeros on the diagonal.\n\n    :type stations: list\n    :param stations:\n        List of station names from in the form where stations[i] refers to\n        nodes[i][:] and lags[i][:]\n    :type nodes: list\n    :param nodes:\n        List of node points where nodes[i] referes to stations[i] and\n        nodes[:][:][0] is latitude in degrees, nodes[:][:][1] is longitude in\n        degrees, nodes[:][:][2] is depth in km.\n    :type lags: numpy.ndarray\n    :param lags:\n        Array of arrays where lags[i][:] refers to stations[i]. lags[i][j]\n        should be the delay to the nodes[i][j] for stations[i] in seconds.\n    :type threshold: float\n    :param threshold: Threshold for removal in seconds\n\n    :returns: Stations\n    :rtype: list\n    :returns: List of lists of tuples of node locations\n    :rtype: list\n    :returns: Array of lags.\n    :rtype: :class:`numpy.ndarray`\n\n    .. note::\n        **Output:**\n\n        station[1] refers to nodes[1] and lags[1] nodes[1][1] refers\n        to station[1] and lags[1][1] nodes[n][n] is a tuple of latitude,\n        longitude and depth.\n    \"\"\"\n    netdif = abs((lags.T - lags.T[0]).sum(axis=1).reshape(1, len(nodes))) \\\n        > threshold\n    for i in range(len(nodes)):\n        _netdif = abs((lags.T -\n                       lags.T[i]).sum(axis=1).reshape(1, len(nodes)))\\\n            > threshold\n        netdif = np.concatenate((netdif, _netdif), axis=0)\n        sys.stdout.write(\"\\r\" + str(float(i) // len(nodes) * 100) + \"% \\r\")\n        sys.stdout.flush()\n    nodes_out = [nodes[0]]\n    node_indices = [0]\n    print(\"\\n\")\n    print(len(nodes))\n    for i in range(1, len(nodes)):\n        if np.all(netdif[i][node_indices]):\n            node_indices.append(i)\n            nodes_out.append(nodes[i])\n    lags_out = lags.T[node_indices].T\n    print(\"Removed \" + str(len(nodes) - len(nodes_out)) + \" duplicate nodes\")\n    return stations, nodes_out, lags_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _node_loop(stations, lags, stream, clip_level,\n               i=0, mem_issue=False, instance=0, plot=False):\n    \"\"\"\n    Internal function to allow for brightness to be paralleled.\n\n    :type stations: list\n    :param stations: List of stations to use.\n    :type lags: numpy.ndarray\n    :param lags: List of lags where lags[i[:]] are the lags for stations[i].\n    :type stream: obspy.core.stream.Stream\n    :param stream: Data stream to find the brightness for.\n    :type clip_level: float\n    :param clip_level: Upper limit for energy as a multiplier to the mean \\\n        energy.\n    :type i: int\n    :param i: Index of loop for parallelisation.\n    :type mem_issue: bool\n    :param mem_issue: If True will write to disk rather than storing data in \\\n        RAM.\n    :type instance: int\n    :param instance: instance for bulk parallelisation, only used if \\\n        mem_issue=true.\n    :type plot: bool\n    :param plot: Turn plotting on or off, defaults to False.\n\n    :returns: index\n    :rtype: int\n    :returns: network response\n    :rtype: numpy.ndarray\n    \"\"\"\n    import matplotlib.pyplot as plt\n    # Set up some overhead for plotting\n    energy_stream = Stream()  # Using a stream as a handy container for\n    # plotting\n\n    for l, tr in enumerate(stream):\n        j = [k for k in range(len(stations))\n             if stations[k] == tr.stats.station]\n        # Check that there is only one matching station\n        if len(j) > 1:\n            warnings.warn('Too many stations')\n            j = [j[0]]\n        if len(j) == 0:\n            warnings.warn('No station match')\n            continue\n        lag = lags[j[0]]\n        lagged_energy = np.square(\n            np.concatenate((tr.data,\n                            np.zeros(\n                                int(round(lag * tr.stats.sampling_rate)))))\n        )[int(round(lag * tr.stats.sampling_rate)):]\n        # Clip energy\n        lagged_energy = np.clip(\n            lagged_energy, 0, clip_level * np.mean(lagged_energy))\n        if 'energy' not in locals():\n            energy = (lagged_energy /\n                      _rms(lagged_energy)).reshape(1, len(lagged_energy))\n            # Cope with zeros encountered\n            energy = np.nan_to_num(energy)\n            # This is now an array of floats - we can convert this to int16\n            # normalize to have max at max of int16 range\n            if not max(energy[0]) == 0.0:\n                energy = (500 * (energy *\n                                 (1 / max(energy[0])))).astype(np.int16)\n            else:\n                energy = energy.astype(np.int16)\n        else:\n            norm_energy = (lagged_energy /\n                           _rms(lagged_energy)).reshape(1, len(lagged_energy))\n            norm_energy = np.nan_to_num(norm_energy)\n            # Convert to int16\n            if not max(norm_energy[0]) == 0.0:\n                norm_energy = (500 * (norm_energy *\n                                      (1 / max(norm_energy[0])))).\\\n                    astype(np.int16)\n            else:\n                norm_energy = norm_energy.astype(np.int16)\n            # Apply lag to data and add it to energy - normalize the data here\n            energy = np.concatenate((energy, norm_energy), axis=0)\n        energy_stream += Trace(\n            data=lagged_energy,\n            header=Stats({'station': tr.stats.station,\n                          'sampling_rate': tr.stats.sampling_rate}))\n    energy = np.sum(energy, axis=0).reshape(1, len(lagged_energy))\n    energy = energy.astype(np.uint16)\n    # Convert any nans to zeros\n    energy = np.nan_to_num(energy)\n    if plot:\n        fig, axes = plt.subplots(len(stream) + 1, 1, sharex=True)\n        axes = axes.ravel()\n        for lagged_energy, tr, axis in zip(energy_stream, stream, axes):\n            axis.plot(lagged_energy * 200, 'r')\n            axis.plot(tr.data, 'k')\n        axes[-1].plot(energy[0])\n        plt.subplots_adjust(hspace=0)\n        plt.show()\n    if not mem_issue:\n        return i, energy\n    else:\n        np.save('tmp' + str(instance) + '/node_' + str(i), energy)\n        return i, str('tmp' + str(instance) + '/node_' + str(i))", "response": "Internal function to perform node - loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the cumulative network response by reading saved energy. npy files.", "response": "def _cum_net_resp(node_lis, instance=0):\n    \"\"\"\n    Compute the cumulative network response by reading saved energy .npy files.\n\n    :type node_lis: numpy.ndarray\n    :param node_lis: List of nodes (ints) to read from\n    :type instance: int\n    :param instance: Instance flag for parallel workflows, defaults to 0.\n\n    :returns: cumulative network response\n    :rtype: numpy.ndarray\n    :returns: node indices for each sample of the cumulative network response.\n    :rtype: list\n    \"\"\"\n    cum_net_resp = np.load('tmp' + str(instance) +\n                           '/node_' + str(node_lis[0]) + '.npy')[0]\n    os.remove('tmp' + str(instance) + '/node_' + str(node_lis[0]) + '.npy')\n    indices = np.ones(len(cum_net_resp)) * node_lis[0]\n    for i in node_lis[1:]:\n        node_energy = np.load('tmp' + str(instance) + '/node_' +\n                              str(i) + '.npy')[0]\n        updated_indices = np.argmax([cum_net_resp, node_energy], axis=0)\n        temp = np.array([cum_net_resp, node_energy])\n        cum_net_resp = np.array([temp[updated_indices[j]][j]\n                                 for j in range(len(updated_indices))])\n        del temp, node_energy\n        updated_indices[updated_indices == 1] = i\n        indices = updated_indices\n        os.remove('tmp' + str(instance) + '/node_' + str(i) + '.npy')\n    return cum_net_resp, indices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding detections within the cumulative network response. :type cum_net_resp: numpy.ndarray :param cum_net_resp: Array of cumulative network response for nodes :type nodes: list :param nodes: Nodes associated with the source of energy in the \\ cum_net_resp :type threshold: float :param threshold: Threshold value :type thresh_type: str :param thresh_type: Either MAD (Median Absolute Deviation) or abs \\ (absolute) or RMS (Root Mean Squared) :type samp_rate: float :param samp_rate: Sampling rate in Hz :type realstations: list :param realstations: List of stations used to make the cumulative network response, will be reported in the :class:`eqcorrscan.core.match_filter.Detection` :type length: float :param length: Maximum length of peak to look for in seconds :returns: Detections as :class:`eqcorrscan.core.match_filter.Detection` objects. :rtype: list", "response": "def _find_detections(cum_net_resp, nodes, threshold, thresh_type,\n                     samp_rate, realstations, length):\n    \"\"\"\n    Find detections within the cumulative network response.\n\n    :type cum_net_resp: numpy.ndarray\n    :param cum_net_resp: Array of cumulative network response for nodes\n    :type nodes: list\n    :param nodes: Nodes associated with the source of energy in the \\\n        cum_net_resp\n    :type threshold: float\n    :param threshold: Threshold value\n    :type thresh_type: str\n    :param thresh_type: Either MAD (Median Absolute Deviation) or abs \\\n        (absolute) or RMS (Root Mean Squared)\n    :type samp_rate: float\n    :param samp_rate: Sampling rate in Hz\n    :type realstations: list\n    :param realstations:\n        List of stations used to make the cumulative network response, will be\n        reported in the :class:`eqcorrscan.core.match_filter.Detection`\n    :type length: float\n    :param length: Maximum length of peak to look for in seconds\n\n    :returns:\n        Detections as :class:`eqcorrscan.core.match_filter.Detection` objects.\n    :rtype: list\n    \"\"\"\n    cum_net_resp = np.nan_to_num(cum_net_resp)  # Force no NaNs\n    if np.isnan(cum_net_resp).any():\n        raise ValueError(\"Nans present\")\n    print('Mean of data is: ' + str(np.median(cum_net_resp)))\n    print('RMS of data is: ' + str(np.sqrt(np.mean(np.square(cum_net_resp)))))\n    print('MAD of data is: ' + str(np.median(np.abs(cum_net_resp))))\n    if thresh_type == 'MAD':\n        thresh = (np.median(np.abs(cum_net_resp)) * threshold)\n    elif thresh_type == 'abs':\n        thresh = threshold\n    elif thresh_type == 'RMS':\n        thresh = _rms(cum_net_resp) * threshold\n    print('Threshold is set to: ' + str(thresh))\n    print('Max of data is: ' + str(max(cum_net_resp)))\n    peaks = findpeaks.find_peaks2_short(cum_net_resp, thresh,\n                                        length * samp_rate, debug=0)\n    detections = []\n    if peaks:\n        for peak in peaks:\n            node = nodes[peak[1]]\n            detections.append(\n                Detection(template_name=str(node[0]) + '_' +\n                          str(node[1]) + '_' + str(node[2]),\n                          detect_time=peak[1] / samp_rate,\n                          no_chans=len(realstations), detect_val=peak[0],\n                          threshold=thresh, typeofdet='brightness',\n                          chans=realstations, id=str(node[0]) + '_' +\n                          str(node[1]) + '_' + str(node[2]) +\n                          str(peak[1] / samp_rate),\n                          threshold_type=thresh_type,\n                          threshold_input=threshold))\n    else:\n        detections = []\n    print('I have found ' + str(len(peaks)) + ' possible detections')\n    return detections"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coherence(stream_in, stations=['all'], clip=False):\n    stream = stream_in.copy()  # Copy the data before we remove stations\n    # First check that all channels in stream have data of the same length\n    maxlen = np.max([len(tr.data) for tr in stream])\n    if maxlen == 0:\n        warnings.warn('template without data')\n        return 0.0, len(stream)\n    if not stations[0] == 'all':\n        for tr in stream:\n            if tr.stats.station not in stations:\n                stream.remove(tr)  # Remove stations we don't want to use\n    for tr in stream:\n        if not len(tr.data) == maxlen and not len(tr.data) == 0:\n            warnings.warn(tr.stats.station + '.' + tr.stats.channel +\n                          ' is not the same length, padding \\n' +\n                          'Length is ' + str(len(tr.data)) + ' samples')\n            pad = np.zeros(maxlen - len(tr.data))\n            if tr.stats.starttime.hour == 0:\n                tr.data = np.concatenate((pad, tr.data), axis=0)\n            else:\n                tr.data = np.concatenate((tr.data, pad), axis=0)\n        elif len(tr.data) == 0:\n            tr.data = np.zeros(maxlen)\n    # Clip the data to the set length\n    if clip:\n        for tr in stream:\n            tr.trim(tr.stats.starttime + clip[0], tr.stats.starttime + clip[1])\n    _coherence = 0.0\n    # Loop through channels and generate a correlation value for each\n    # unique cross-channel pairing\n    for i in range(len(stream)):\n        for j in range(i + 1, len(stream)):\n            _coherence += np.abs(normxcorr2(stream[i].data,\n                                            stream[j].data))[0][0]\n    _coherence = 2 * _coherence / (len(stream) * (len(stream) - 1))\n    return _coherence, len(stream)", "response": "Calculates the average network coherence of a given template or detection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the brightness function for a single day of data.", "response": "def brightness(stations, nodes, lags, stream, threshold, thresh_type,\n               template_length, template_saveloc, coherence_thresh,\n               coherence_stations=['all'], coherence_clip=False,\n               gap=2.0, clip_level=100, instance=0, pre_pick=0.2,\n               plotvar=False, plotsave=True, cores=1, debug=0,\n               mem_issue=False):\n    \"\"\"\n    Calculate the brightness function for a single day.\n\n    Written to calculate the brightness function for a single day of data,\n    using moveouts from a 3D travel-time grid.\n\n    .. Note::\n        Data in stream must be all of the same length and have the same\n        sampling rates, see :func:`eqcorrscan.utils.pre_processing.dayproc`\n\n    :type stations: list\n    :param stations:\n        List of station names from in the form where stations[i] refers to\n        nodes[i][:] and lags[i][:]\n    :type nodes: list\n    :param nodes:\n        List of node points where nodes[i] refers to stations[i] and\n        nodes[:][:][0] is latitude in degrees, nodes[:][:][1] is longitude in\n        degrees, nodes[:][:][2] is depth in km.\n    :type lags: numpy.ndarray\n    :param lags:\n        Array of arrays where lags[i][:] refers to stations[i]. lags[i][j]\n        should be the delay to the nodes[i][j] for stations[i] in seconds.\n    :type stream: obspy.core.stream.Stream\n    :param stream: Data through which to look for detections.\n    :type threshold: float\n    :param threshold:\n        Threshold value for detection of template within the brightness\n        function.\n    :type thresh_type: str\n    :param thresh_type:\n        Either MAD or abs where MAD is the Median Absolute Deviation and abs\n        is an absolute brightness.\n    :type template_length: float\n    :param template_length: Length of template to extract in seconds\n    :type template_saveloc: str\n    :param template_saveloc: Path of where to save the templates.\n    :type coherence_thresh: tuple\n    :param coherence_thresh:\n            Threshold for removing incoherent peaks in the network response,\n            those below this will not be used as templates. Must be in the\n            form of (a,b) where the coherence is given by: :math:`a-kchan/b`\n            where kchan is the number of channels used to compute the\n            coherence.\n    :type coherence_stations: list\n    :param coherence_stations:\n        List of stations to use in the coherence thresholding - defaults to\n        `all` which uses all the stations.\n    :type coherence_clip: tuple\n    :param coherence_clip:\n        Start and end in seconds of data to window around, defaults to False,\n        which uses all the data given.\n    :type gap: float\n    :param gap: Minimum inter-event time in seconds for detections.\n    :type clip_level: float\n    :param clip_level:\n        Multiplier applied to the mean deviation of the energy as an upper\n        limit, used to remove spikes (earthquakes, lightning, electrical\n        spikes) from the energy stack.\n    :type instance: int\n    :param instance:\n        Optional, used for tracking when using a distributed computing system.\n    :type pre_pick: float\n    :param pre_pick: Seconds before the detection time to include in template\n    :type plotvar: bool\n    :param plotvar: Turn plotting on or off\n    :type plotsave: bool\n    :param plotsave:\n        Save or show plots, if `False` will try and show the plots on screen -\n        as this is designed for bulk use this is set to `True` to save any\n        plots rather than show them if you create them - changes the backend\n        of matplotlib, so if is set to `False` you will see NO PLOTS!\n    :type cores: int\n    :param cores: Number of cores to use, defaults to 1.\n    :type debug: int\n    :param debug: Debug level from 0-5, higher is more output.\n    :type mem_issue: bool\n    :param mem_issue:\n        Set to True to write temporary variables to disk rather than store in\n        memory, slow.\n\n    :return: list of templates as :class:`obspy.core.stream.Stream` objects\n    :rtype: list\n    \"\"\"\n    if plotsave:\n        import matplotlib\n        matplotlib.use('Agg')\n        import matplotlib.pyplot as plt\n        plt.ioff()\n    from eqcorrscan.utils import plotting\n    from eqcorrscan.utils.debug_log import debug_print\n    # Check that we actually have the correct stations\n    realstations = []\n    for station in stations:\n        st = stream.select(station=station)\n        if st:\n            realstations += station\n    del st\n    stream_copy = stream.copy()\n    # Force convert to int16\n    for tr in stream_copy:\n        # int16 max range is +/- 32767\n        if max(abs(tr.data)) > 32767:\n            tr.data = 32767 * (tr.data / max(abs(tr.data)))\n            # Make sure that the data aren't clipped it they are high gain\n            # scale the data\n        tr.data = tr.data.astype(np.int16)\n    # The internal _node_loop converts energy to int16 too to conserve memory,\n    # to do this it forces the maximum of a single energy trace to be 500 and\n    # normalises to this level - this only works for fewer than 65 channels of\n    # data\n    if len(stream_copy) > 130:\n        raise BrightnessError(\n            'Too many streams, either re-code and cope with either more memory'\n            ' usage, or less precision, or reduce data volume')\n    # Loop through each node in the input\n    # Linear run\n    print('Computing the energy stacks')\n    # Parallel run\n    num_cores = cores\n    if num_cores > len(nodes):\n        num_cores = len(nodes)\n    if num_cores > cpu_count():\n        num_cores = cpu_count()\n    if mem_issue and not os.path.isdir('tmp' + str(instance)):\n        os.makedirs('tmp' + str(instance))\n    pool = Pool(processes=num_cores)\n    results = [pool.apply_async(_node_loop, (stations,),\n                                {'lags': lags[:, i], 'stream': stream, 'i': i,\n                                 'clip_level': clip_level,\n                                 'mem_issue': mem_issue, 'instance': instance})\n               for i in range(len(nodes))]\n    pool.close()\n    if not mem_issue:\n        print('Computing the cumulative network response from memory')\n        try:\n            energy = [p.get() for p in results]\n        except KeyboardInterrupt as e:  # pragma: no cover\n            pool.terminate()\n            raise e\n        pool.join()\n        energy.sort(key=lambda tup: tup[0])\n        energy = [node[1] for node in energy]\n        energy = np.concatenate(energy, axis=0)\n        print(energy.shape)\n    else:\n        pool.join()\n        del results\n    # Now compute the cumulative network response and then detect possible\n    # events\n    if not mem_issue:\n        print(energy.shape)\n        indices = np.argmax(energy, axis=0)  # Indices of maximum energy\n        print(indices.shape)\n        cum_net_resp = np.array([np.nan] * len(indices))\n        cum_net_resp[0] = energy[indices[0]][0]\n        peak_nodes = [nodes[indices[0]]]\n        for i in range(1, len(indices)):\n            cum_net_resp[i] = energy[indices[i]][i]\n            peak_nodes.append(nodes[indices[i]])\n        del energy, indices\n    else:\n        print('Reading the temp files and computing network response')\n        node_splits = int(len(nodes) // num_cores)\n        print(node_splits)\n        indices = []\n        for i in range(num_cores):\n            indices.append(list(np.arange(node_splits * i,\n                                          node_splits * (i + 1))))\n        indices[-1] += list(np.arange(node_splits * (i + 1), len(nodes)))\n        # results = [_cum_net_resp(node_lis=indices[i], instance=instance)\n        #            for i in range(num_cores)]\n        pool = Pool(processes=num_cores)\n        results = [pool.apply_async(_cum_net_resp, args=(indices[i], instance))\n                   for i in range(num_cores)]\n        pool.close()\n        try:\n            results = [p.get() for p in results]\n        except KeyboardInterrupt as e:  # pragma: no cover\n            pool.terminate()\n            raise e\n        pool.join()\n        responses = [result[0] for result in results]\n        print(np.shape(responses))\n        node_indices = [result[1] for result in results]\n        cum_net_resp = np.array(responses)\n        indices = np.argmax(cum_net_resp, axis=0)\n        print(indices.shape)\n        print(cum_net_resp.shape)\n        cum_net_resp = np.array([cum_net_resp[indices[i]][i]\n                                 for i in range(len(indices))])\n        peak_nodes = [nodes[node_indices[indices[i]][i]]\n                      for i in range(len(indices))]\n        del indices, node_indices\n    if plotvar:\n        cum_net_trace = Stream(Trace(\n            data=cum_net_resp, header=Stats(\n                {'station': 'NR', 'channel': '', 'network': 'Z',\n                 'location': '', 'starttime': stream[0].stats.starttime,\n                 'sampling_rate': stream[0].stats.sampling_rate})))\n        cum_net_trace += stream.select(channel='*N')\n        cum_net_trace += stream.select(channel='*1')\n        cum_net_trace.sort(['network', 'station', 'channel'])\n\n    # Find detection within this network response\n    print('Finding detections in the cumulative network response')\n    detections = _find_detections(cum_net_resp, peak_nodes, threshold,\n                                  thresh_type, stream[0].stats.sampling_rate,\n                                  realstations, gap)\n    del cum_net_resp\n    templates = []\n    nodesout = []\n    good_detections = []\n    if detections:\n        print('Converting detections into templates')\n        # Generate a catalog of detections\n        # detections_cat = Catalog()\n        for j, detection in enumerate(detections):\n            debug_print('Converting for detection %i of %i'\n                        % (j, len(detections)), 3, debug)\n            # Create an event for each detection\n            event = Event()\n            # Set up some header info for the event\n            event.event_descriptions.append(EventDescription())\n            event.event_descriptions[0].text = 'Brightness detection'\n            event.creation_info = CreationInfo(agency_id='EQcorrscan')\n            copy_of_stream = deepcopy(stream_copy)\n            # Convert detections to obspy.core.event type -\n            # name of detection template is the node.\n            node = (detection.template_name.split('_')[0],\n                    detection.template_name.split('_')[1],\n                    detection.template_name.split('_')[2])\n            # Look up node in nodes and find the associated lags\n            index = nodes.index((float(node[0]), float(node[1]),\n                                 float(node[2])))\n            detect_lags = lags[:, index]\n            ksta = Comment(text='Number of stations=' + str(len(detect_lags)))\n            event.origins.append(Origin())\n            event.origins[0].comments.append(ksta)\n            event.origins[0].time = copy_of_stream[0].stats.starttime +\\\n                detect_lags[0] + detection.detect_time\n            event.origins[0].latitude = node[0]\n            event.origins[0].longitude = node[1]\n            event.origins[0].depth = node[2]\n            for i, detect_lag in enumerate(detect_lags):\n                station = stations[i]\n                st = copy_of_stream.select(station=station)\n                if len(st) != 0:\n                    for tr in st:\n                        _waveform_id = WaveformStreamID(\n                            station_code=tr.stats.station,\n                            channel_code=tr.stats.channel,\n                            network_code=tr.stats.network)\n                        event.picks.append(Pick(\n                            waveform_id=_waveform_id,\n                            time=tr.stats.starttime + detect_lag +\n                            detection.detect_time + pre_pick,\n                            onset='emergent', evalutation_mode='automatic'))\n            debug_print('Generating template for detection: %i' % j, 0, debug)\n            template = _template_gen(\n                picks=event.picks, st=copy_of_stream, length=template_length,\n                swin='all')\n            template_name = template_saveloc + '/' +\\\n                str(template[0].stats.starttime) + '.ms'\n            # In the interests of RAM conservation we write then read\n            # Check coherency here!\n            temp_coher, kchan = coherence(template, coherence_stations,\n                                          coherence_clip)\n            coh_thresh = float(coherence_thresh[0]) - kchan / \\\n                float(coherence_thresh[1])\n            coherent = False\n            if temp_coher > coh_thresh:\n                template.write(template_name, format=\"MSEED\")\n                print('Written template as: ' + template_name)\n                print('---------------------------------coherence LEVEL: ' +\n                      str(temp_coher))\n                coherent = True\n                debug_print('Template was incoherent, coherence level: ' +\n                            str(temp_coher), 0, debug)\n                coherent = False\n            del copy_of_stream, tr, template\n            if coherent:\n                templates.append(obsread(template_name))\n                nodesout += [node]\n                good_detections.append(detection)\n            debug_print('No template for you', 0, debug)\n            # detections_cat += event\n    if plotvar:\n        good_detections = [(cum_net_trace[-1].stats.starttime +\n                            detection.detect_time).datetime\n                           for detection in good_detections]\n        if not plotsave:\n            plotting.NR_plot(cum_net_trace[0:-1],\n                             Stream(cum_net_trace[-1]),\n                             detections=good_detections,\n                             size=(18.5, 10),\n                             title='Network response')\n            # cum_net_trace.plot(size=(800,600), equal_scale=False)\n        else:\n            savefile = 'plots/' +\\\n                cum_net_trace[0].stats.starttime.datetime.strftime('%Y%m%d') +\\\n                '_NR_timeseries.pdf'\n            plotting.NR_plot(cum_net_trace[0:-1],\n                             Stream(cum_net_trace[-1]),\n                             detections=good_detections,\n                             size=(18.5, 10), save=True, savefile=savefile,\n                             title='Network response')\n    nodesout = list(set(nodesout))\n    return templates, nodesout"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect within continuous data using the detector.", "response": "def _detect(detector, st, threshold, trig_int, moveout=0, min_trig=0,\n            process=True, extract_detections=False, cores=1, debug=0):\n    \"\"\"\n    Detect within continuous data using the subspace method.\n\n    Not to be called directly, use the detector.detect method.\n\n    :type detector: eqcorrscan.core.subspace.Detector\n    :param detector: Detector to use.\n    :type st: obspy.core.stream.Stream\n    :param st: Un-processed stream to detect within using the subspace \\\n        detector\n    :type threshold: float\n    :param threshold: Threshold value for detections between 0-1\n    :type trig_int: float\n    :param trig_int: Minimum trigger interval in seconds.\n    :type moveout: float\n    :param moveout: Maximum allowable moveout window for non-multiplexed,\n        network detection.  See note.\n    :type min_trig: int\n    :param min_trig: Minimum number of stations exceeding threshold for \\\n        non-multiplexed, network detection. See note.\n    :type process: bool\n    :param process: Whether or not to process the stream according to the \\\n        parameters defined by the detector.  Default is to process the \\\n        data (True).\n    :type extract_detections: bool\n    :param extract_detections: Whether to extract waveforms for each \\\n        detection or not, if true will return detections and streams.\n    :type debug: int\n    :param debug: Debug output level from 0-5.\n\n    :return: list of detections\n    :rtype: list of eqcorrscan.core.match_filter.Detection\n    \"\"\"\n    detections = []\n    # First process the stream\n    if process:\n        debug_print('Processing Stream', 0, debug)\n        stream, stachans = _subspace_process(\n            streams=[st.copy()], lowcut=detector.lowcut,\n            highcut=detector.highcut, filt_order=detector.filt_order,\n            sampling_rate=detector.sampling_rate, multiplex=detector.multiplex,\n            stachans=detector.stachans, parallel=True, align=False,\n            shift_len=None, reject=False, cores=cores)\n    else:\n        # Check the sampling rate at the very least\n        for tr in st:\n            if not tr.stats.sampling_rate == detector.sampling_rate:\n                raise ValueError('Sampling rates do not match.')\n        stream = [st]\n        stachans = detector.stachans\n    outtic = time.clock()\n    # If multiplexed, how many samples do we increment by?\n    if detector.multiplex:\n        Nc = len(detector.stachans)\n    else:\n        Nc = 1\n    # Here do all ffts\n    fft_vars = _do_ffts(detector, stream, Nc)\n    debug_print('Computing detection statistics', 0, debug)\n    debug_print('Preallocating stats matrix', 0, debug)\n    stats = np.zeros((len(stream[0]),\n                      (len(stream[0][0]) // Nc) - (fft_vars[4] // Nc) + 1))\n    for det_freq, data_freq_sq, data_freq, i in zip(fft_vars[0], fft_vars[1],\n                                                    fft_vars[2],\n                                                    np.arange(len(stream[0]))):\n        # Calculate det_statistic in frequency domain\n        stats[i] = _det_stat_freq(det_freq, data_freq_sq, data_freq,\n                                  fft_vars[3], Nc, fft_vars[4], fft_vars[5])\n        debug_print('Stats matrix is shape %s' % str(stats[i].shape), 0, debug)\n        if debug >= 3:\n            fig, ax = plt.subplots()\n            t = np.arange(len(stats[i]))\n            ax.plot(t, stats[i], color='k')\n            ax.axis('tight')\n            ax.set_ylim([0, 1])\n            ax.plot([min(t), max(t)], [threshold, threshold], color='r', lw=1,\n                    label='Threshold')\n            ax.legend()\n            plt.title('%s' % str(stream[0][i].stats.station))\n            plt.show()\n    trig_int_samples = detector.sampling_rate * trig_int\n    debug_print('Finding peaks', 0, debug)\n    peaks = []\n    for i in range(len(stream[0])):\n        peaks.append(findpeaks.find_peaks2_short(\n            arr=stats[i], thresh=threshold, trig_int=trig_int_samples,\n            debug=debug))\n    if not detector.multiplex:\n        # Conduct network coincidence triggering\n        peaks = findpeaks.coin_trig(\n            peaks=peaks, samp_rate=detector.sampling_rate, moveout=moveout,\n            min_trig=min_trig, stachans=stachans, trig_int=trig_int)\n    else:\n        peaks = peaks[0]\n    if len(peaks) > 0:\n        for peak in peaks:\n            detecttime = st[0].stats.starttime + \\\n                (peak[1] / detector.sampling_rate)\n            rid = ResourceIdentifier(\n                id=detector.name + '_' + str(detecttime), prefix='smi:local')\n            ev = Event(resource_id=rid)\n            cr_i = CreationInfo(\n                author='EQcorrscan', creation_time=UTCDateTime())\n            ev.creation_info = cr_i\n            # All detection info in Comments for lack of a better idea\n            thresh_str = 'threshold=' + str(threshold)\n            ccc_str = 'detect_val=' + str(peak[0])\n            used_chans = 'channels used: ' +\\\n                ' '.join([str(pair) for pair in detector.stachans])\n            ev.comments.append(Comment(text=thresh_str))\n            ev.comments.append(Comment(text=ccc_str))\n            ev.comments.append(Comment(text=used_chans))\n            for stachan in detector.stachans:\n                tr = st.select(station=stachan[0], channel=stachan[1])\n                if tr:\n                    net_code = tr[0].stats.network\n                else:\n                    net_code = ''\n                pick_tm = detecttime\n                wv_id = WaveformStreamID(\n                    network_code=net_code, station_code=stachan[0],\n                    channel_code=stachan[1])\n                ev.picks.append(Pick(time=pick_tm, waveform_id=wv_id))\n            detections.append(\n                Detection(template_name=detector.name, detect_time=detecttime,\n                          no_chans=len(detector.stachans), detect_val=peak[0],\n                          threshold=threshold, typeofdet='subspace',\n                          threshold_type='abs', threshold_input=threshold,\n                          chans=detector.stachans, event=ev))\n    outtoc = time.clock()\n    print('Detection took %s seconds' % str(outtoc - outtic))\n    if extract_detections:\n        detection_streams = extract_from_stream(st, detections)\n        return detections, detection_streams\n    return detections"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_ffts(detector, stream, Nc):\n    min_fftlen = int(stream[0][0].data.shape[0] +\n                     detector.data[0].shape[0] - Nc)\n    fftlen = scipy.fftpack.next_fast_len(min_fftlen)\n    mplen = stream[0][0].data.shape[0]\n    ulen = detector.data[0].shape[0]\n    num_st_fd = [np.fft.rfft(tr.data, n=fftlen)\n                 for tr in stream[0]]\n    denom_st_fd = [np.fft.rfft(np.square(tr.data), n=fftlen)\n                   for tr in stream[0]]\n    # Frequency domain of boxcar\n    w = np.fft.rfft(np.ones(detector.data[0].shape[0]),\n                    n=fftlen)\n    # This should go into the detector object as in Detex\n    detector_fd = []\n    for dat_mat in detector.data:\n        detector_fd.append(np.array([np.fft.rfft(col[::-1], n=fftlen)\n                                     for col in dat_mat.T]))\n    return detector_fd, denom_st_fd, num_st_fd, w, ulen, mplen", "response": "Perform ffts on data detector and denominator boxcar and return the resulting list of time - reversed detectors and data stream in frequency domain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _det_stat_freq(det_freq, data_freq_sq, data_freq, w, Nc, ulen, mplen):\n    num_cor = np.multiply(det_freq, data_freq)  # Numerator convolution\n    den_cor = np.multiply(w, data_freq_sq)  # Denominator convolution\n    # Do inverse fft\n    # First and last Nt - 1 samples are invalid; clip them off\n    num_ifft = np.real(np.fft.irfft(num_cor))[:, ulen-1:mplen:Nc]\n    denominator = np.real(np.fft.irfft(den_cor))[ulen-1:mplen:Nc]\n    # Ratio of projected to envelope energy = det_stat across all channels\n    result = np.sum(np.square(num_ifft), axis=0) / denominator\n    return result", "response": "Compute detection statistic in the frequency domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the data for a single subspace detector.", "response": "def _subspace_process(streams, lowcut, highcut, filt_order, sampling_rate,\n                      multiplex, align, shift_len, reject, no_missed=True,\n                      stachans=None, parallel=False, plot=False, cores=1):\n    \"\"\"\n    Process stream data, internal function.\n\n    :type streams: list\n    :param streams: List of obspy.core.stream.Stream to be used to \\\n        generate the subspace detector.  These should be pre-clustered \\\n        and aligned.\n    :type lowcut: float\n    :param lowcut: Lowcut in Hz, can be None to not apply filter\n    :type highcut: float\n    :param highcut: Highcut in Hz, can be None to not apply filter\n    :type filt_order: int\n    :param filt_order: Number of corners for filter.\n    :type sampling_rate: float\n    :param sampling_rate: Desired sampling rate in Hz\n    :type multiplex: bool\n    :param multiplex: Whether to multiplex the data or not.  Data are \\\n        multiplexed according to the method of Harris, see the multi \\\n        function for details.\n    :type stachans: list of tuple\n    :param stachans: list of tuples of (station, channel) to use.\n    :type align: bool\n    :param align: Whether to align the data or not - needs to be done \\\n        at some point\n    :type shift_len: float\n    :param shift_len: Maximum shift allowed for alignment in seconds.\n    :type reject: float\n    :param reject: Minimum correlation for traces, only used if align=True.\n    :type no_missed: bool\n    :param: no_missed: Reject streams with missed traces, defaults to True. \\\n        A missing trace from lots of events will reduce the quality of the \\\n        subspace detector if multiplexed.  Only used when multi is set to True.\n    :type plot: bool\n    :param plot: Passed down to align traces - used to check alignment process.\n\n    :return: Processed streams\n    :rtype: list\n    :return: Station, channel pairs in order\n    :rtype: list of tuple\n    :return: List of delays\n    :rtype: list\n    \"\"\"\n    from multiprocessing import Pool, cpu_count\n    processed_streams = []\n    if not stachans:\n        input_stachans = list(set([(tr.stats.station, tr.stats.channel)\n                                   for st in streams for tr in st.sort()]))\n    else:\n        input_stachans = stachans\n    input_stachans.sort()  # Make sure stations and channels are in order\n    # Check that all channels are the same length in seconds\n    first_length = len(streams[0][0].data) /\\\n        streams[0][0].stats.sampling_rate\n    for st in streams:\n        for tr in st:\n            if not len(tr) / tr.stats.sampling_rate == first_length:\n                msg = 'All channels of all streams must be the same length'\n                raise IOError(msg)\n    for st in streams:\n        if not parallel:\n            processed_stream = Stream()\n            for stachan in input_stachans:\n                dummy, tr = _internal_process(\n                    st=st, lowcut=lowcut, highcut=highcut,\n                    filt_order=filt_order, sampling_rate=sampling_rate,\n                    first_length=first_length, stachan=stachan, debug=0)\n                processed_stream += tr\n            processed_streams.append(processed_stream)\n        else:\n            pool = Pool(processes=min(cores, cpu_count()))\n            results = [pool.apply_async(\n                _internal_process, (st,),\n                {'lowcut': lowcut, 'highcut': highcut,\n                 'filt_order': filt_order, 'sampling_rate': sampling_rate,\n                 'first_length': first_length, 'stachan': stachan, 'debug': 0,\n                 'i': i}) for i, stachan in enumerate(input_stachans)]\n            pool.close()\n            try:\n                processed_stream = [p.get() for p in results]\n            except KeyboardInterrupt as e:  # pragma: no cover\n                pool.terminate()\n                raise e\n            pool.join()\n            processed_stream.sort(key=lambda tup: tup[0])\n            processed_stream = Stream([p[1] for p in processed_stream])\n            processed_streams.append(processed_stream)\n        if no_missed and multiplex:\n            for tr in processed_stream:\n                if np.count_nonzero(tr.data) == 0:\n                    processed_streams.remove(processed_stream)\n                    print('Removed stream with empty trace')\n                    break\n    if align:\n        processed_streams = align_design(\n            design_set=processed_streams, shift_len=shift_len, reject=reject,\n            multiplex=multiplex, plot=plot, no_missed=no_missed)\n    output_streams = []\n    for processed_stream in processed_streams:\n        if len(processed_stream) == 0:\n            # If we have removed all of the traces then onwards!\n            continue\n        # Need to order the stream according to input_stachans\n        _st = Stream()\n        for stachan in input_stachans:\n            tr = processed_stream.select(\n                station=stachan[0], channel=stachan[1])\n            if len(tr) >= 1:\n                _st += tr[0]\n            elif multiplex and len(tr) == 0:\n                raise IndexError(\n                    'Missing data for %s.%s' % (stachan[0], stachan[1]))\n        if multiplex:\n            st = multi(stream=_st)\n            st = Stream(Trace(st))\n            st[0].stats.station = 'Multi'\n            st[0].stats.sampling_rate = sampling_rate\n        else:\n            st = _st\n        for tr in st:\n            # Normalize the data\n            norm = np.linalg.norm(tr.data)\n            if not norm == 0:\n                tr.data /= norm\n        output_streams.append(st)\n    return output_streams, input_stachans"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multi(stream):\n    stack = stream[0].data\n    for tr in stream[1:]:\n        stack = np.dstack(np.array([stack, tr.data]))\n    multiplex = stack.reshape(stack.size, )\n    return multiplex", "response": "Returns a multiplexed stream of seismic data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef align_design(design_set, shift_len, reject, multiplex, no_missed=True,\n                 plot=False):\n    \"\"\"\n    Align individual traces within streams of the design set.\n\n    Perform before Detector.construct to align traces before computing the \\\n    singular value decomposition.\n\n    :type design_set: list\n    :param design_set: List of obspy.core.stream.Stream to be aligned\n    :type shift_len: float\n    :param shift_len: Maximum shift (plus/minus) in seconds.\n    :type reject: float\n    :param reject: Minimum correlation for traces, only used if align=True.\n    :type multiplex: bool\n    :param multiplex: If you are going to multiplex the data, then there has \\\n        to be data for all channels, so we will pad with zeros, otherwise \\\n        there is no need.\n    :type no_missed: bool\n    :param: no_missed: Reject streams with missed traces, defaults to True. \\\n        A missing trace from lots of events will reduce the quality of the \\\n        subspace detector if multiplexed.  Only used when multi is set to True.\n    :type plot: bool\n    :param plot: Whether to plot the aligned traces as we go or not.\n\n    :rtype: list\n    :return: List of obspy.core.stream.Stream of aligned streams\n\n    .. Note:: Assumes only one trace for each channel for each stream in the \\\n        design_set. If more are present will only use the first one.\n\n    .. Note:: Will cut all traces to be the same length as required for the \\\n        svd, this length will be the shortest trace length - 2 * shift_len\n    \"\"\"\n    trace_lengths = [tr.stats.endtime - tr.stats.starttime for st in design_set\n                     for tr in st]\n    clip_len = min(trace_lengths) - (2 * shift_len)\n    stachans = list(set([(tr.stats.station, tr.stats.channel)\n                         for st in design_set for tr in st]))\n    remove_set = []\n    for stachan in stachans:\n        trace_list = []\n        trace_ids = []\n        for i, st in enumerate(design_set):\n            tr = st.select(station=stachan[0], channel=stachan[1])\n            if len(tr) > 0:\n                trace_list.append(tr[0])\n                trace_ids.append(i)\n            if len(tr) > 1:\n                warnings.warn('Too many matches for %s %s' % (stachan[0],\n                                                              stachan[1]))\n        shift_len_samples = int(shift_len * trace_list[0].stats.sampling_rate)\n        shifts, cccs = stacking.align_traces(\n            trace_list=trace_list, shift_len=shift_len_samples, positive=True)\n        for i, shift in enumerate(shifts):\n            st = design_set[trace_ids[i]]\n            start_t = st.select(\n                station=stachan[0], channel=stachan[1])[0].stats.starttime\n            start_t += shift_len\n            start_t -= shift\n            st.select(\n                station=stachan[0], channel=stachan[1])[0].trim(\n                start_t, start_t + clip_len)\n            if cccs[i] < reject:\n                if multiplex and not no_missed:\n                    st.select(station=stachan[0],\n                              channel=stachan[1])[0].data = np.zeros(\n                        int(clip_len * (st.select(\n                            station=stachan[0],\n                            channel=stachan[1])[0].stats.sampling_rate) + 1))\n                    warnings.warn('Padding stream with zero trace for ' +\n                                  'station ' + stachan[0] + '.' + stachan[1])\n                    print('zero padding')\n                elif multiplex and no_missed:\n                    remove_set.append(st)\n                    warnings.warn('Will remove stream due to low-correlation')\n                    continue\n                else:\n                    st.remove(st.select(station=stachan[0],\n                              channel=stachan[1])[0])\n                    print('Removed channel with correlation at %s' % cccs[i])\n                    continue\n    if no_missed:\n        for st in remove_set:\n            if st in design_set:\n                design_set.remove(st)\n    if plot:\n        for stachan in stachans:\n            trace_list = []\n            for st in design_set:\n                tr = st.select(station=stachan[0], channel=stachan[1])\n                if len(tr) > 0:\n                    trace_list.append(tr[0])\n            if len(trace_list) > 1:\n                plotting.multi_trace_plot(traces=trace_list, corr=True,\n                                          stack=None, title='.'.join(stachan))\n            else:\n                print('No plot for you, only one trace left after rejection')\n    return design_set", "response": "Aligns the given design set with the given length."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconducts subspace detection with chosen detectors. :type detectors: list :param detectors: list of :class:`eqcorrscan.core.subspace.Detector` to be used for detection. :type stream: obspy.core.stream.Stream :param stream: Stream to detect within. :type threshold: float :param threshold: Threshold between 0 and 1 for detection, see :func:`Detector.detect` :type trig_int: float :param trig_int: Minimum trigger interval in seconds. :type moveout: float :param moveout: Maximum allowable moveout window for non-multiplexed, network detection. See note. :type min_trig: int :param min_trig: Minimum number of stations exceeding threshold for non-multiplexed, network detection. See note in :func:`Detector.detect`. :type parallel: bool :param parallel: Whether to run detectors in parallel in groups. :type num_cores: int :param num_cores: How many cpu cores to use if parallel==True. If set to None (default), will use all available cores. :rtype: list :return: List of :class:`eqcorrscan.core.match_filter.Detection` detections. .. Note:: This will loop through your detectors using their detect method. If the detectors are multiplexed it will run groups of detectors with the same channels at the same time.", "response": "def subspace_detect(detectors, stream, threshold, trig_int, moveout=0,\n                    min_trig=1, parallel=True, num_cores=None):\n    \"\"\"\n    Conduct subspace detection with chosen detectors.\n\n    :type detectors: list\n    :param detectors:\n        list of :class:`eqcorrscan.core.subspace.Detector` to be used\n        for detection.\n    :type stream: obspy.core.stream.Stream\n    :param stream: Stream to detect within.\n    :type threshold: float\n    :param threshold:\n        Threshold between 0 and 1 for detection, see :func:`Detector.detect`\n    :type trig_int: float\n    :param trig_int: Minimum trigger interval in seconds.\n    :type moveout: float\n    :param moveout:\n        Maximum allowable moveout window for non-multiplexed, network\n        detection.  See note.\n    :type min_trig: int\n    :param min_trig:\n        Minimum number of stations exceeding threshold for non-multiplexed,\n        network detection. See note in :func:`Detector.detect`.\n    :type parallel: bool\n    :param parallel: Whether to run detectors in parallel in groups.\n    :type num_cores: int\n    :param num_cores:\n        How many cpu cores to use if parallel==True. If set to None (default),\n        will use all available cores.\n\n    :rtype: list\n    :return:\n        List of :class:`eqcorrscan.core.match_filter.Detection` detections.\n\n    .. Note::\n        This will loop through your detectors using their detect method.\n        If the detectors are multiplexed it will run groups of detectors with\n        the same channels at the same time.\n    \"\"\"\n    from multiprocessing import Pool, cpu_count\n    # First check that detector parameters are the same\n    parameters = []\n    detections = []\n    for detector in detectors:\n        parameter = (detector.lowcut, detector.highcut,\n                     detector.filt_order, detector.sampling_rate,\n                     detector.multiplex, detector.stachans)\n        if parameter not in parameters:\n            parameters.append(parameter)\n    for parameter_set in parameters:\n        parameter_detectors = []\n        for detector in detectors:\n            det_par = (detector.lowcut, detector.highcut, detector.filt_order,\n                       detector.sampling_rate, detector.multiplex,\n                       detector.stachans)\n            if det_par == parameter_set:\n                parameter_detectors.append(detector)\n        stream, stachans = \\\n            _subspace_process(\n                streams=[stream.copy()], lowcut=parameter_set[0],\n                highcut=parameter_set[1], filt_order=parameter_set[2],\n                sampling_rate=parameter_set[3], multiplex=parameter_set[4],\n                stachans=parameter_set[5], parallel=True, align=False,\n                shift_len=None, reject=False)\n        if not parallel:\n            for detector in parameter_detectors:\n                detections += _detect(\n                    detector=detector, st=stream[0], threshold=threshold,\n                    trig_int=trig_int, moveout=moveout, min_trig=min_trig,\n                    process=False, extract_detections=False, debug=0)\n        else:\n            if num_cores:\n                ncores = num_cores\n            else:\n                ncores = cpu_count()\n            pool = Pool(processes=ncores)\n            results = [pool.apply_async(\n                _detect, args=(detector, stream[0], threshold, trig_int,\n                               moveout, min_trig, False, False, 0))\n                       for detector in parameter_detectors]\n            pool.close()\n            try:\n                _detections = [p.get() for p in results]\n            except KeyboardInterrupt as e:  # pragma: no cover\n                pool.terminate()\n                raise e\n            pool.join()\n            for d in _detections:\n                if isinstance(d, list):\n                    detections += d\n                else:\n                    detections.append(d)\n    return detections"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef construct(self, streams, lowcut, highcut, filt_order,\n                  sampling_rate, multiplex, name, align, shift_len=0,\n                  reject=0.3, no_missed=True, plot=False):\n        \"\"\"\n        Construct a subspace detector from a list of streams, full rank.\n\n        Subspace detector will be full-rank, further functions can be used \\\n        to select the desired dimensions.\n\n        :type streams: list\n        :param streams:\n            List of :class:`obspy.core.stream.Stream` to be used to generate\n            the subspace detector.  These should be pre-clustered and aligned.\n        :type lowcut: float\n        :param lowcut: Lowcut in Hz, can be None to not apply filter\n        :type highcut: float\n        :param highcut: Highcut in Hz, can be None to not apply filter\n        :type filt_order: int\n        :param filt_order: Number of corners for filter.\n        :type sampling_rate: float\n        :param sampling_rate: Desired sampling rate in Hz\n        :type multiplex: bool\n        :param multiplex:\n            Whether to multiplex the data or not.  Data are multiplexed\n            according to the method of Harris, see the multi function for\n            details.\n        :type name: str\n        :param name: Name of the detector, used for book-keeping.\n        :type align: bool\n        :param align:\n            Whether to align the data or not - needs to be done at some point\n        :type shift_len: float\n        :param shift_len: Maximum shift allowed for alignment in seconds.\n        :type reject: float\n        :param reject:\n            Minimum correlation to include traces - only used if align=True.\n        :type no_missed: bool\n        :param no_missed:\n            Reject streams with missed traces, defaults to True. A missing\n            trace from lots of events will reduce the quality of the subspace\n            detector if multiplexed.  Only used when multi is set to True.\n        :type plot: bool\n        :param plot: Whether to plot the alignment stage or not.\n\n        .. note::\n            The detector will be normalized such that the data, before\n            computing the singular-value decomposition, will have unit energy.\n            e.g. We divide the amplitudes of the data by the L1 norm of the\n            data.\n\n        .. warning::\n            EQcorrscan's alignment will attempt to align over the whole data\n            window given.  For long (more than 2s) chunks of data this can give\n            poor results and you might be better off using the\n            :func:`eqcorrscan.utils.stacking.align_traces` function externally,\n            focusing on a smaller window of data.  To do this you would align\n            the data prior to running construct.\n        \"\"\"\n        self.lowcut = lowcut\n        self.highcut = highcut\n        self.filt_order = filt_order\n        self.sampling_rate = sampling_rate\n        self.name = name\n        self.multiplex = multiplex\n        # Pre-process data\n        p_streams, stachans = _subspace_process(\n            streams=copy.deepcopy(streams), lowcut=lowcut, highcut=highcut,\n            filt_order=filt_order, sampling_rate=sampling_rate,\n            multiplex=multiplex, align=align, shift_len=shift_len,\n            reject=reject, plot=plot, no_missed=no_missed)\n        # Compute the SVD, use the cluster.SVD function\n        u, sigma, v, svd_stachans = svd(stream_list=p_streams, full=True)\n        self.stachans = stachans\n        # self.delays = delays\n        self.u = u\n        self.v = v\n        self.sigma = sigma\n        self.data = copy.deepcopy(u)  # Set the data matrix to be full rank U.\n        self.dimension = np.inf\n        return self", "response": "Constructs a subspace detector from a list of streams lowcut highcut and filter_rate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npartitioning subspace into desired dimension.", "response": "def partition(self, dimension):\n        \"\"\"\n        Partition subspace into desired dimension.\n\n        :type dimension: int\n        :param dimension: Maximum dimension to use.\n        \"\"\"\n        # Take leftmost 'dimension' input basis vectors\n        for i, channel in enumerate(self.u):\n            if self.v[i].shape[1] < dimension:\n                raise IndexError('Channel is max dimension %s'\n                                 % self.v[i].shape[1])\n            self.data[i] = channel[:, 0:dimension]\n        self.dimension = dimension\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef energy_capture(self, stachans='all', size=(10, 7), show=False):\n        if show:\n            return subspace_fc_plot(detector=self, stachans=stachans,\n                                    size=size, show=show)\n        percent_capture = 0\n        if np.isinf(self.dimension):\n            return 100\n        for channel in self.sigma:\n            fc = np.sum(channel[0:self.dimension]) / np.sum(channel)\n            percent_capture += fc\n        else:\n            return 100 * (percent_capture / len(self.sigma))", "response": "Calculates the average percentage energy capture for this subspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detect(self, st, threshold, trig_int, moveout=0, min_trig=0,\n               process=True, extract_detections=False, cores=1, debug=0):\n        \"\"\"\n        Detect within continuous data using the subspace method.\n\n        :type st: obspy.core.stream.Stream\n        :param st:\n            Un-processed stream to detect within using the subspace detector.\n        :type threshold: float\n        :param threshold: Threshold value for detections between 0-1\n        :type trig_int: float\n        :param trig_int: Minimum trigger interval in seconds.\n        :type moveout: float\n        :param moveout:\n            Maximum allowable moveout window for non-multiplexed, network\n            detection.  See note.\n        :type min_trig: int\n        :param min_trig:\n            Minimum number of stations exceeding threshold for non-multiplexed,\n            network detection. See note.\n        :type process: bool\n        :param process:\n            Whether or not to process the stream according to the parameters\n            defined by the detector.  Default is True, which will process the\n            data.\n        :type extract_detections: bool\n        :param extract_detections:\n            Whether to extract waveforms for each detection or not, if True\n            will return detections and streams.\n        :type cores: int\n        :param cores: Number of threads to process data with.\n        :type debug: int\n        :param debug: Debug output level from 0-5.\n\n        :return: list of :class:`eqcorrscan.core.match_filter.Detection`\n        :rtype: list\n\n        .. warning::\n            Subspace is currently in beta, see note in the subspace tutorial\n            for information.\n\n        .. note::\n            If running in bulk with detectors that all have the same\n            parameters then you can pre-process the data and set process to\n            False.  This will speed up this detect function dramatically.\n\n        .. warning::\n            If the detector and stream are multiplexed then they must\n            contain the same channels and multiplexed in the same order. This\n            is handled internally when process=True, but if running in bulk\n            you must take care.\n\n        .. note::\n            Non-multiplexed, network detection.  When the detector is\n            not multiplexed, but there are multiple channels within the\n            detector, we do not stack the single-channel detection statistics\n            because we do not have a one-size-fits-all solution for computing\n            delays for a subspace detector (if you want to implement one, then\n            please contribute it!).  Therefore, these parameters provide a\n            means for declaring a network coincidence trigger using\n            single-channel detection statistics, in a similar fashion to the\n            commonly used network-coincidence trigger with energy detection\n            statistics.\n        \"\"\"\n        return _detect(detector=self, st=st, threshold=threshold,\n                       trig_int=trig_int, moveout=moveout, min_trig=min_trig,\n                       process=process, extract_detections=extract_detections,\n                       debug=debug, cores=cores)", "response": "Detects within continuous data using the subspace detector."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, filename):\n        f = h5py.File(filename, \"w\")\n        # Must store eqcorrscan version number, username would be useful too.\n        data_group = f.create_group(name=\"data\")\n        for i, data in enumerate(self.data):\n            dset = data_group.create_dataset(name=\"data_\" + str(i),\n                                             shape=data.shape,\n                                             dtype=data.dtype)\n            dset[...] = data\n        data_group.attrs['length'] = len(self.data)\n        data_group.attrs['name'] = self.name.encode(\"ascii\", \"ignore\")\n        data_group.attrs['sampling_rate'] = self.sampling_rate\n        data_group.attrs['multiplex'] = self.multiplex\n        data_group.attrs['lowcut'] = self.lowcut\n        data_group.attrs['highcut'] = self.highcut\n        data_group.attrs['filt_order'] = self.filt_order\n        data_group.attrs['dimension'] = self.dimension\n        data_group.attrs['user'] = getpass.getuser()\n        data_group.attrs['eqcorrscan_version'] = str(eqcorrscan.__version__)\n        # Convert station-channel list to something writable\n        ascii_stachans = ['.'.join(stachan).encode(\"ascii\", \"ignore\")\n                          for stachan in self.stachans]\n        stachans = f.create_dataset(name=\"stachans\",\n                                    shape=(len(ascii_stachans),),\n                                    dtype='S10')\n        stachans[...] = ascii_stachans\n        u_group = f.create_group(\"u\")\n        for i, u in enumerate(self.u):\n            uset = u_group.create_dataset(name=\"u_\" + str(i),\n                                          shape=u.shape, dtype=u.dtype)\n            uset[...] = u\n        u_group.attrs['length'] = len(self.u)\n        sigma_group = f.create_group(\"sigma\")\n        for i, sigma in enumerate(self.sigma):\n            sigmaset = sigma_group.create_dataset(name=\"sigma_\" + str(i),\n                                                  shape=sigma.shape,\n                                                  dtype=sigma.dtype)\n            sigmaset[...] = sigma\n        sigma_group.attrs['length'] = len(self.sigma)\n        v_group = f.create_group(\"v\")\n        for i, v in enumerate(self.v):\n            vset = v_group.create_dataset(name=\"v_\" + str(i),\n                                          shape=v.shape, dtype=v.dtype)\n            vset[...] = v\n        v_group.attrs['length'] = len(self.v)\n        f.flush()\n        f.close()\n        return self", "response": "Write the detector to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a detector from a file.", "response": "def read(self, filename):\n        \"\"\"\n        Read detector from a file, must be HDF5 format.\n\n        Reads a Detector object from an HDF5 file, usually created by \\\n        eqcorrscan.\n\n        :type filename: str\n        :param filename: Filename to save the detector to.\n        \"\"\"\n        f = h5py.File(filename, \"r\")\n        self.data = []\n        for i in range(f['data'].attrs['length']):\n            self.data.append(f['data']['data_' + str(i)].value)\n        self.u = []\n        for i in range(f['u'].attrs['length']):\n            self.u.append(f['u']['u_' + str(i)].value)\n        self.sigma = []\n        for i in range(f['sigma'].attrs['length']):\n            self.sigma.append(f['sigma']['sigma_' + str(i)].value)\n        self.v = []\n        for i in range(f['v'].attrs['length']):\n            self.v.append(f['v']['v_' + str(i)].value)\n        self.stachans = [tuple(stachan.decode('ascii').split('.'))\n                         for stachan in f['stachans'].value]\n        self.dimension = f['data'].attrs['dimension']\n        self.filt_order = f['data'].attrs['filt_order']\n        self.highcut = f['data'].attrs['highcut']\n        self.lowcut = f['data'].attrs['lowcut']\n        self.multiplex = bool(f['data'].attrs['multiplex'])\n        self.sampling_rate = f['data'].attrs['sampling_rate']\n        if isinstance(f['data'].attrs['name'], str):\n            self.name = f['data'].attrs['name']\n        else:\n            self.name = f['data'].attrs['name'].decode('ascii')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(self, stachans='all', size=(10, 7), show=True):\n        return subspace_detector_plot(detector=self, stachans=stachans,\n                                      size=size, show=show)", "response": "Plot the basis vectors for the detector at the given dimension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all symbols in a file", "response": "def export_symbols(*path):\n    \"\"\"\n    Required for windows systems - functions defined in libutils.def.\n    \"\"\"\n    lines = open(os.path.join(*path), 'r').readlines()[2:]\n    return [s.strip() for s in lines if s.strip() != '']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dist_calc(loc1, loc2):\n    R = 6371.009  # Radius of the Earth in km\n    dlat = np.radians(abs(loc1[0] - loc2[0]))\n    dlong = np.radians(abs(loc1[1] - loc2[1]))\n    ddepth = abs(loc1[2] - loc2[2])\n    mean_lat = np.radians((loc1[0] + loc2[0]) / 2)\n    dist = R * np.sqrt(dlat ** 2 + (np.cos(mean_lat) * dlong) ** 2)\n    dist = np.sqrt(dist ** 2 + ddepth ** 2)\n    return dist", "response": "Function to calculate the distance between two points in km."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the maximum curvature for the given list of magnitudes.", "response": "def calc_max_curv(magnitudes, plotvar=False):\n    \"\"\"\n    Calculate the magnitude of completeness using the maximum curvature method.\n\n    :type magnitudes: list\n    :param magnitudes:\n        List of magnitudes from which to compute the maximum curvature which\n        will give an estimate of the magnitude of completeness given the\n        assumption of a power-law scaling.\n    :type plotvar: bool\n    :param plotvar: Turn plotting on and off\n\n    :rtype: float\n    :return: Magnitude at maximum curvature\n\n    .. Note:: Should be used as a guide, often under-estimates Mc.\n\n    .. rubric:: Example\n\n    >>> import numpy as np\n    >>> mags = []\n    >>> for mag in np.arange(2.5,3, 0.1):\n    ...     mags.extend([mag] * int(20000 - 10 * mag))\n    >>> for mag in np.arange(3,7, 0.1):\n    ...     mags.extend([mag] * int(10 ** (7 - 1 * mag)))\n    >>> calc_max_curv(mags, plotvar=False)\n    3.0\n    \"\"\"\n    counts = Counter(magnitudes)\n    df = np.zeros(len(counts))\n    mag_steps = np.zeros(len(counts))\n    grad = np.zeros(len(counts) - 1)\n    grad_points = grad.copy()\n    for i, magnitude in enumerate(sorted(counts.keys(), reverse=True)):\n        mag_steps[i] = magnitude\n        if i > 0:\n            df[i] = counts[magnitude] + df[i - 1]\n        else:\n            df[i] = counts[magnitude]\n    for i, val in enumerate(df):\n        if i > 0:\n            grad[i - 1] = (val - df[i - 1]) / (mag_steps[i] - mag_steps[i - 1])\n            grad_points[i - 1] = mag_steps[i] - ((mag_steps[i] -\n                                                  mag_steps[i - 1]) / 2.0)\n    # Need to find the second order derivative\n    curvature = np.zeros(len(grad) - 1)\n    curvature_points = curvature.copy()\n    for i, _grad in enumerate(grad):\n        if i > 0:\n            curvature[i - 1] = (_grad - grad[i - 1]) / (grad_points[i] -\n                                                        grad_points[i - 1])\n            curvature_points[i - 1] = grad_points[i] - ((grad_points[i] -\n                                                         grad_points[i - 1]) /\n                                                        2.0)\n    if plotvar:\n        plt.scatter(mag_steps, df, c='k', label='Magnitude function')\n        plt.plot(mag_steps, df, c='k')\n        plt.scatter(grad_points, grad, c='r', label='Gradient')\n        plt.plot(grad_points, grad, c='r')\n        plt.scatter(curvature_points, curvature, c='g', label='Curvature')\n        plt.plot(curvature_points, curvature, c='g')\n        plt.legend()\n        plt.show()\n    return curvature_points[np.argmax(abs(curvature))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the b - value for a given set of magnitudes.", "response": "def calc_b_value(magnitudes, completeness, max_mag=None, plotvar=True):\n    \"\"\"\n    Calculate the b-value for a range of completeness magnitudes.\n\n    Calculates a power-law fit to given magnitudes for each completeness\n    magnitude.  Plots the b-values and residuals for the fitted catalogue\n    against the completeness values. Computes fits using numpy.polyfit,\n    which uses a least-squares technique.\n\n    :type magnitudes: list\n    :param magnitudes: Magnitudes to compute the b-value for.\n    :type completeness: list\n    :param completeness: list of completeness values to compute b-values for.\n    :type max_mag: float\n    :param max_mag: Maximum magnitude to attempt to fit in magnitudes.\n    :type plotvar: bool\n    :param plotvar: Turn plotting on or off.\n\n    :rtype: list\n    :return: List of tuples of (completeness, b-value, residual,\\\n        number of magnitudes used)\n\n    .. rubric:: Example\n\n    >>> from obspy.clients.fdsn import Client\n    >>> from obspy import UTCDateTime\n    >>> from eqcorrscan.utils.mag_calc import calc_b_value\n    >>> client = Client('IRIS')\n    >>> t1 = UTCDateTime('2012-03-26T00:00:00')\n    >>> t2 = t1 + (3 * 86400)\n    >>> catalog = client.get_events(starttime=t1, endtime=t2, minmagnitude=3)\n    >>> magnitudes = [event.magnitudes[0].mag for event in catalog]\n    >>> b_values = calc_b_value(magnitudes, completeness=np.arange(3, 7, 0.2),\n    ...                         plotvar=False)\n    >>> round(b_values[4][1])\n    1.0\n    >>> # We can set a maximum magnitude:\n    >>> b_values = calc_b_value(magnitudes, completeness=np.arange(3, 7, 0.2),\n    ...                         plotvar=False, max_mag=5)\n    >>> round(b_values[4][1])\n    1.0\n    \"\"\"\n    b_values = []\n    # Calculate the cdf for all magnitudes\n    counts = Counter(magnitudes)\n    cdf = np.zeros(len(counts))\n    mag_steps = np.zeros(len(counts))\n    for i, magnitude in enumerate(sorted(counts.keys(), reverse=True)):\n        mag_steps[i] = magnitude\n        if i > 0:\n            cdf[i] = cdf[i - 1] + counts[magnitude]\n        else:\n            cdf[i] = counts[magnitude]\n\n    if not max_mag:\n        max_mag = max(magnitudes)\n    for m_c in completeness:\n        if m_c >= max_mag or m_c >= max(magnitudes):\n            warnings.warn('Not computing completeness at %s, above max_mag' %\n                          str(m_c))\n            break\n        complete_mags = []\n        complete_freq = []\n        for i, mag in enumerate(mag_steps):\n            if mag >= m_c <= max_mag:\n                complete_mags.append(mag)\n                complete_freq.append(np.log10(cdf[i]))\n        if len(complete_mags) < 4:\n            warnings.warn('Not computing completeness above ' + str(m_c) +\n                          ', fewer than 4 events')\n            break\n        fit = np.polyfit(complete_mags, complete_freq, 1, full=True)\n        # Calculate the residuals according to the Wiemer & Wys 2000 definition\n        predicted_freqs = [fit[0][1] - abs(fit[0][0] * M)\n                           for M in complete_mags]\n        r = 100 - ((np.sum([abs(complete_freq[i] - predicted_freqs[i])\n                           for i in range(len(complete_freq))]) * 100) /\n                   np.sum(complete_freq))\n        b_values.append((m_c, abs(fit[0][0]), r, str(len(complete_mags))))\n    if plotvar:\n        fig, ax1 = plt.subplots()\n        b_vals = ax1.scatter(list(zip(*b_values))[0], list(zip(*b_values))[1],\n                             c='k')\n        resid = ax1.scatter(list(zip(*b_values))[0],\n                            [100 - b for b in list(zip(*b_values))[2]], c='r')\n        ax1.set_ylabel('b-value and residual')\n        plt.xlabel('Completeness magnitude')\n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Number of events used in fit')\n        n_ev = ax2.scatter(list(zip(*b_values))[0], list(zip(*b_values))[3],\n                           c='g')\n        fig.legend((b_vals, resid, n_ev),\n                   ('b-values', 'residuals', 'number of events'),\n                   'lower right')\n        ax1.set_title('Possible completeness values')\n        plt.show()\n    return b_values"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsimulating a Wood - Anderson trace.", "response": "def _sim_WA(trace, PAZ, seedresp, water_level, velocity=False):\n    \"\"\"\n    Remove the instrument response from a trace and simulate a Wood-Anderson.\n\n    Returns a de-meaned, de-trended, Wood Anderson simulated trace in\n    its place.\n\n    Works in-place on data and will destroy your original data, copy the\n    trace before giving it to this function!\n\n    :type trace: obspy.core.trace.Trace\n    :param trace:\n        A standard obspy trace, generally should be given without\n        pre-filtering, if given with pre-filtering for use with\n        amplitude determination for magnitudes you will need to\n        worry about how you cope with the response of this filter\n        yourself.\n    :type PAZ: dict\n    :param PAZ:\n        Dictionary containing lists of poles and zeros, the gain and\n        the sensitivity. If unset will expect seedresp.\n    :type seedresp: dict\n    :param seedresp: Seed response information - if unset will expect PAZ.\n    :type water_level: int\n    :param water_level: Water level for the simulation.\n    :type velocity: bool\n    :param velocity:\n        Whether to return a velocity trace or not - velocity is non-standard\n        for Wood-Anderson instruments, but institutes that use seiscomp3 or\n        Antelope require picks in velocity.\n\n    :returns: Trace of Wood-Anderson simulated data\n    :rtype: :class:`obspy.core.trace.Trace`\n    \"\"\"\n    # Note Wood anderson sensitivity is 2080 as per Uhrhammer & Collins 1990\n    PAZ_WA = {'poles': [-6.283 + 4.7124j, -6.283 - 4.7124j],\n              'zeros': [0 + 0j], 'gain': 1.0, 'sensitivity': 2080}\n    if velocity:\n        PAZ_WA['zeros'] = [0 + 0j, 0 + 0j]\n    # De-trend data\n    trace.detrend('simple')\n    # Simulate Wood Anderson\n    if PAZ:\n        trace.data = seis_sim(trace.data, trace.stats.sampling_rate,\n                              paz_remove=PAZ, paz_simulate=PAZ_WA,\n                              water_level=water_level,\n                              remove_sensitivity=True)\n    elif seedresp:\n        trace.data = seis_sim(trace.data, trace.stats.sampling_rate,\n                              paz_remove=None, paz_simulate=PAZ_WA,\n                              water_level=water_level, seedresp=seedresp)\n    else:\n        UserWarning('No response given to remove, will just simulate WA')\n        trace.data = seis_sim(trace.data, trace.stats.sampling_rate,\n                              paz_remove=None, paz_simulate=PAZ_WA,\n                              water_level=water_level)\n    return trace"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _max_p2t(data, delta):\n    turning_points = []  # A list of tuples of (amplitude, sample)\n    for i in range(1, len(data) - 1):\n        if (data[i] < data[i - 1] and data[i] < data[i + 1]) or\\\n           (data[i] > data[i - 1] and data[i] > data[i + 1]):\n            turning_points.append((data[i], i))\n    if len(turning_points) >= 1:\n        amplitudes = np.empty([len(turning_points) - 1],)\n        half_periods = np.empty([len(turning_points) - 1],)\n    else:\n        print('Turning points has length: ' + str(len(turning_points)) +\n              ' data have length: ' + str(len(data)))\n        return 0.0, 0.0, 0.0\n    for i in range(1, len(turning_points)):\n        half_periods[i - 1] = (delta * (turning_points[i][1] -\n                                        turning_points[i - 1][1]))\n        amplitudes[i - 1] = np.abs(turning_points[i][0] -\n                                   turning_points[i - 1][0])\n    amplitude = np.max(amplitudes)\n    period = 2 * half_periods[np.argmax(amplitudes)]\n    return amplitude, period, delta * turning_points[np.argmax(amplitudes)][1]", "response": "This function calculates the maximum peak - to - trough amplitude and period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _GSE2_PAZ_read(gsefile):\n    with open(gsefile, 'r') as f:\n        # First line should start with CAL2\n        header = f.readline()\n        if not header[0:4] == 'CAL2':\n            raise IOError('Unknown format for GSE file, only coded for CAL2')\n        station = header.split()[1]\n        channel = header.split()[2]\n        sensor = header.split()[3]\n        date = dt.datetime.strptime(header.split()[7], '%Y/%m/%d')\n        header = f.readline()\n        if not header[0:4] == 'PAZ2':\n            raise IOError('Unknown format for GSE file, only coded for PAZ2')\n        gain = float(header.split()[3])  # Measured in nm/counts\n        kpoles = int(header.split()[4])\n        kzeros = int(header.split()[5])\n        poles = []\n        for i in range(kpoles):\n            pole = f.readline()\n            poles.append(complex(float(pole.split()[0]),\n                                 float(pole.split()[1])))\n        zeros = []\n        for i in range(kzeros):\n            zero = f.readline()\n            zeros.append(complex(float(zero.split()[0]),\n                                 float(zero.split()[1])))\n        # Have Poles and Zeros, but need Gain and Sensitivity\n        # Gain should be in the DIG2 line:\n        for line in f:\n            if line[0:4] == 'DIG2':\n                sensitivity = float(line.split()[2])\n                # measured in counts/muVolt\n    PAZ = {'poles': poles, 'zeros': zeros, 'gain': gain,\n           'sensitivity': sensitivity}\n    return PAZ, date, station, channel, sensor", "response": "Reads the instrument response information from a GSE2 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping on itertools. tee for SVD_magnitude. Wrapper on itertools. tee for SVD_magnitude.", "response": "def _pairwise(iterable):\n    \"\"\"\n    Wrapper on itertools for SVD_magnitude.\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    if sys.version_info.major == 2:\n        return itertools.izip(a, b)\n    else:\n        return zip(a, b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npicks amplitudes for local magnitude for a single event. Looks for maximum peak-to-trough amplitude for a channel in a stream, and picks this amplitude and period. There are a few things it does internally to stabilise the result: 1. Applies a given filter to the data - very necessary for small magnitude earthquakes; 2. Keeps track of the poles and zeros of this filter and removes them from the picked amplitude; 3. Picks the peak-to-trough amplitude, but records half of this: the specification for the local magnitude is to use a peak amplitude on a horizontal, however, with modern digital seismometers, the peak amplitude often has an additional, DC-shift applied to it, to stabilise this, and to remove possible issues with de-meaning data recorded during the wave-train of an event (e.g. the mean may not be the same as it would be for longer durations), we use half the peak-to-trough amplitude; 4. Despite the original definition of local magnitude requiring the use of a horizontal channel, more recent work has shown that the vertical channels give more consistent magnitude estimations between stations, due to a reduction in site-amplification effects, we therefore use the vertical channels by default, but allow the user to chose which channels they deem appropriate; 5. We do not specify that the maximum amplitude should be the S-phase: The original definition holds that the maximum body-wave amplitude should be used - while this is often the S-phase, we do not discriminate against the P-phase. We do note that, unless the user takes care when assigning winlen and filters, they may end up with amplitude picks for surface waves; 6. We use a variable window-length by default that takes into account P-S times if available, this is in an effort to include only the body waves. When P-S times are not available we us the ps_multiplier variable, which defaults to 0.34 x hypocentral distance. :type event: obspy.core.event.event.Event :param event: Event to pick :type st: obspy.core.stream.Stream :param st: Stream associated with event :type respdir: str :param respdir: Path to the response information directory :type chans: list :param chans: List of the channels to pick on, defaults to ['Z'] - should just be the orientations, e.g. Z, 1, 2, N, E :type var_wintype: bool :param var_wintype: If True, the winlen will be multiplied by the P-S time if both P and S picks are available, otherwise it will be multiplied by the hypocentral distance*ps_multiplier, defaults to True :type winlen: float :param winlen: Length of window, see above parameter, if var_wintype is False then this will be in seconds, otherwise it is the multiplier to the p-s time, defaults to 0.9. :type pre_pick: float :param pre_pick: Time before the s-pick to start the cut window, defaults to 0.2. :type pre_filt: bool :param pre_filt: To apply a pre-filter or not, defaults to True :type lowcut: float :param lowcut: Lowcut in Hz for the pre-filter, defaults to 1.0 :type highcut: float :param highcut: Highcut in Hz for the pre-filter, defaults to 20.0 :type corners: int :param corners: Number of corners to use in the pre-filter :type min_snr: float :param min_snr: Minimum signal-to-noise ratio to allow a pick - see note below on signal-to-noise ratio calculation. :type plot: bool :param plot: Turn plotting on or off. :type remove_old: bool :param remove_old: If True, will remove old amplitude picks from event and overwrite with new picks. Defaults to False. :type ps_multiplier: float :param ps_multiplier: A p-s time multiplier of hypocentral distance - defaults to 0.34, based on p-s ratio of 1.68 and an S-velocity 0f 1.5km/s, deliberately chosen to be quite slow. :type velocity: bool :param velocity: Whether to make the pick in velocity space or not. Original definition of local magnitude used displacement of Wood-Anderson, MLv in seiscomp and Antelope uses a velocity measurement. :returns: Picked event :rtype: :class:`obspy.core.event.Event` .. Note:: Signal-to-noise ratio is calculated using the filtered data by dividing the maximum amplitude in the signal window (pick window) by the normalized noise amplitude (taken from the whole window supplied). .. Warning:: Works in place on data - will filter and remove response from data, you are recommended to give this function a copy of the data if you are using it in a loop.", "response": "def amp_pick_event(event, st, respdir, chans=['Z'], var_wintype=True,\n                   winlen=0.9, pre_pick=0.2, pre_filt=True, lowcut=1.0,\n                   highcut=20.0, corners=4, min_snr=1.0, plot=False,\n                   remove_old=False, ps_multiplier=0.34, velocity=False):\n    \"\"\"\n    Pick amplitudes for local magnitude for a single event.\n\n    Looks for maximum peak-to-trough amplitude for a channel in a stream, and\n    picks this amplitude and period.  There are a few things it does\n    internally to stabilise the result:\n\n        1. Applies a given filter to the data - very necessary for small\n        magnitude earthquakes;\n\n        2. Keeps track of the poles and zeros of this filter and removes them\n        from the picked amplitude;\n\n        3. Picks the peak-to-trough amplitude, but records half of this: the\n        specification for the local magnitude is to use a peak amplitude on\n        a horizontal, however, with modern digital seismometers, the peak\n        amplitude often has an additional, DC-shift applied to it, to\n        stabilise this, and to remove possible issues with de-meaning data\n        recorded during the wave-train of an event (e.g. the mean may not be\n        the same as it would be for longer durations), we use half the\n        peak-to-trough amplitude;\n\n        4. Despite the original definition of local magnitude requiring the\n        use of a horizontal channel, more recent work has shown that the\n        vertical channels give more consistent magnitude estimations between\n        stations, due to a reduction in site-amplification effects, we\n        therefore use the vertical channels by default, but allow the user\n        to chose which channels they deem appropriate;\n\n        5. We do not specify that the maximum amplitude should be the\n        S-phase: The original definition holds that the maximum body-wave\n        amplitude should be used - while this is often the S-phase, we do not\n        discriminate against the P-phase.  We do note that, unless the user\n        takes care when assigning winlen and filters, they may end up with\n        amplitude picks for surface waves;\n\n        6. We use a variable window-length by default that takes into account\n        P-S times if available, this is in an effort to include only the\n        body waves.  When P-S times are not available we us the ps_multiplier\n        variable, which defaults to 0.34 x hypocentral distance.\n\n    :type event: obspy.core.event.event.Event\n    :param event: Event to pick\n    :type st: obspy.core.stream.Stream\n    :param st: Stream associated with event\n    :type respdir: str\n    :param respdir: Path to the response information directory\n    :type chans: list\n    :param chans:\n        List of the channels to pick on, defaults to ['Z'] - should just be\n        the orientations, e.g. Z, 1, 2, N, E\n    :type var_wintype: bool\n    :param var_wintype:\n        If True, the winlen will be multiplied by the P-S time if both P and\n        S picks are available, otherwise it will be multiplied by the\n        hypocentral distance*ps_multiplier, defaults to True\n    :type winlen: float\n    :param winlen:\n        Length of window, see above parameter, if var_wintype is False then\n        this will be in seconds, otherwise it is the multiplier to the\n        p-s time, defaults to 0.9.\n    :type pre_pick: float\n    :param pre_pick:\n        Time before the s-pick to start the cut window, defaults to 0.2.\n    :type pre_filt: bool\n    :param pre_filt: To apply a pre-filter or not, defaults to True\n    :type lowcut: float\n    :param lowcut: Lowcut in Hz for the pre-filter, defaults to 1.0\n    :type highcut: float\n    :param highcut: Highcut in Hz for the pre-filter, defaults to 20.0\n    :type corners: int\n    :param corners: Number of corners to use in the pre-filter\n    :type min_snr: float\n    :param min_snr:\n        Minimum signal-to-noise ratio to allow a pick - see note below on\n        signal-to-noise ratio calculation.\n    :type plot: bool\n    :param plot: Turn plotting on or off.\n    :type remove_old: bool\n    :param remove_old:\n        If True, will remove old amplitude picks from event and overwrite\n        with new picks. Defaults to False.\n    :type ps_multiplier: float\n    :param ps_multiplier:\n        A p-s time multiplier of hypocentral distance - defaults to 0.34,\n        based on p-s ratio of 1.68 and an S-velocity 0f 1.5km/s, deliberately\n        chosen to be quite slow.\n    :type velocity: bool\n    :param velocity:\n        Whether to make the pick in velocity space or not. Original definition\n        of local magnitude used displacement of Wood-Anderson, MLv in seiscomp\n        and Antelope uses a velocity measurement.\n\n    :returns: Picked event\n    :rtype: :class:`obspy.core.event.Event`\n\n    .. Note::\n        Signal-to-noise ratio is calculated using the filtered data by\n        dividing the maximum amplitude in the signal window (pick window)\n        by the normalized noise amplitude (taken from the whole window\n        supplied).\n\n    .. Warning::\n        Works in place on data - will filter and remove response from data,\n        you are recommended to give this function a copy of the data if you\n        are using it in a loop.\n    \"\"\"\n    # Convert these picks into a lists\n    stations = []  # List of stations\n    channels = []  # List of channels\n    picktimes = []  # List of pick times\n    picktypes = []  # List of pick types\n    picks_out = []\n    try:\n        depth = _get_origin(event).depth\n    except MatchFilterError:\n        depth = 0\n    if remove_old and event.amplitudes:\n        for amp in event.amplitudes:\n            # Find the pick and remove it too\n            pick = [p for p in event.picks if p.resource_id == amp.pick_id][0]\n            event.picks.remove(pick)\n        event.amplitudes = []\n    for pick in event.picks:\n        if pick.phase_hint in ['P', 'S']:\n            picks_out.append(pick)  # Need to be able to remove this if there\n            # isn't data for a station!\n            stations.append(pick.waveform_id.station_code)\n            channels.append(pick.waveform_id.channel_code)\n            picktimes.append(pick.time)\n            picktypes.append(pick.phase_hint)\n    if len(picktypes) == 0:\n        warnings.warn('No P or S picks found')\n    st.merge()  # merge the data, just in case!\n    # For each station cut the window\n    uniq_stas = list(set(stations))\n    for sta in uniq_stas:\n        for chan in chans:\n            print('Working on ' + sta + ' ' + chan)\n            tr = st.select(station=sta, channel='*' + chan)\n            if not tr:\n                warnings.warn(\n                    'There is no station and channel match in the wavefile!')\n                continue\n            else:\n                tr = tr[0]\n            # Apply the pre-filter\n            if pre_filt:\n                try:\n                    tr.split().detrend('simple').merge(fill_value=0)\n                except:\n                    print('Some issue splitting this one')\n                    dummy = tr.split()\n                    dummy.detrend('simple')\n                    tr = dummy.merge(fill_value=0)\n                try:\n                    tr.filter('bandpass', freqmin=lowcut, freqmax=highcut,\n                              corners=corners)\n                except NotImplementedError:\n                    print('For some reason trace is not continuous:')\n                    print(tr)\n                    continue\n            # Find the response information\n            resp_info = _find_resp(\n                tr.stats.station, tr.stats.channel, tr.stats.network,\n                tr.stats.starttime, tr.stats.delta, respdir)\n            PAZ = []\n            seedresp = []\n            if resp_info and 'gain' in resp_info:\n                PAZ = resp_info\n            elif resp_info:\n                seedresp = resp_info\n            # Simulate a Wood Anderson Seismograph\n            if PAZ and len(tr.data) > 10:\n                # Set ten data points to be the minimum to pass\n                tr = _sim_WA(tr, PAZ, None, 10, velocity=velocity)\n            elif seedresp and len(tr.data) > 10:\n                tr = _sim_WA(tr, None, seedresp, 10, velocity=velocity)\n            elif len(tr.data) > 10:\n                warnings.warn('No PAZ for ' + tr.stats.station + ' ' +\n                              tr.stats.channel + ' at time: ' +\n                              str(tr.stats.starttime))\n                continue\n            sta_picks = [i for i in range(len(stations))\n                         if stations[i] == sta]\n            pick_id = event.picks[sta_picks[0]].resource_id\n            arrival = [arrival for arrival in event.origins[0].arrivals\n                       if arrival.pick_id == pick_id][0]\n            hypo_dist = np.sqrt(\n                np.square(degrees2kilometers(arrival.distance)) +\n                np.square(depth / 1000))\n            if var_wintype and hypo_dist:\n                if 'S' in [picktypes[i] for i in sta_picks] and\\\n                   'P' in [picktypes[i] for i in sta_picks]:\n                    # If there is an S-pick we can use this :D\n                    s_pick = [picktimes[i] for i in sta_picks\n                              if picktypes[i] == 'S']\n                    s_pick = min(s_pick)\n                    p_pick = [picktimes[i] for i in sta_picks\n                              if picktypes[i] == 'P']\n                    p_pick = min(p_pick)\n                    try:\n                        tr.trim(starttime=s_pick - pre_pick,\n                                endtime=s_pick + (s_pick - p_pick) * winlen)\n                    except ValueError:\n                        continue\n                elif 'S' in [picktypes[i] for i in sta_picks]:\n                    s_pick = [picktimes[i] for i in sta_picks\n                              if picktypes[i] == 'S']\n                    s_pick = min(s_pick)\n                    p_modelled = s_pick - (hypo_dist * ps_multiplier)\n                    try:\n                        tr.trim(starttime=s_pick - pre_pick,\n                                endtime=s_pick + (s_pick - p_modelled) *\n                                winlen)\n                    except ValueError:\n                        continue\n                else:\n                    # In this case we only have a P pick\n                    p_pick = [picktimes[i] for i in sta_picks\n                              if picktypes[i] == 'P']\n                    p_pick = min(p_pick)\n                    s_modelled = p_pick + (hypo_dist * ps_multiplier)\n                    print('P_pick=%s' % str(p_pick))\n                    print('hypo_dist: %s' % str(hypo_dist))\n                    print('S modelled=%s' % str(s_modelled))\n                    try:\n                        tr.trim(starttime=s_modelled - pre_pick,\n                                endtime=s_modelled + (s_modelled - p_pick) *\n                                winlen)\n                        print(tr)\n                    except ValueError:\n                        continue\n                # Work out the window length based on p-s time or distance\n            elif 'S' in [picktypes[i] for i in sta_picks]:\n                # If the window is fixed we still need to find the start time,\n                # which can be based either on the S-pick (this elif), or\n                # on the hypocentral distance and the P-pick\n\n                # Take the minimum S-pick time if more than one S-pick is\n                # available\n                s_pick = [picktimes[i] for i in sta_picks\n                          if picktypes[i] == 'S']\n                s_pick = min(s_pick)\n                try:\n                    tr.trim(starttime=s_pick - pre_pick,\n                            endtime=s_pick + winlen)\n                except ValueError:\n                    continue\n            else:\n                # In this case, there is no S-pick and the window length is\n                # fixed we need to calculate an expected S_pick based on the\n                # hypocentral distance, this will be quite hand-wavey as we\n                # are not using any kind of velocity model.\n                p_pick = [picktimes[i] for i in sta_picks\n                          if picktypes[i] == 'P']\n                print(picktimes)\n                p_pick = min(p_pick)\n                s_modelled = p_pick + hypo_dist * ps_multiplier\n                try:\n                    tr.trim(starttime=s_modelled - pre_pick,\n                            endtime=s_modelled + winlen)\n                except ValueError:\n                    continue\n            if len(tr.data) <= 10:\n                warnings.warn('No data found for: ' + tr.stats.station)\n                continue\n            # Get the amplitude\n            try:\n                amplitude, period, delay = _max_p2t(tr.data, tr.stats.delta)\n            except ValueError:\n                print('No amplitude picked for tr %s' % str(tr))\n                continue\n            # Calculate the normalized noise amplitude\n            noise_amplitude = np.sqrt(np.mean(np.square(tr.data)))\n            if amplitude == 0.0:\n                continue\n            if amplitude / noise_amplitude < min_snr:\n                print('Signal to noise ratio of %s is below threshold.' %\n                      (amplitude / noise_amplitude))\n                continue\n            if plot:\n                plt.plot(np.arange(len(tr.data)), tr.data, 'k')\n                plt.scatter(tr.stats.sampling_rate * delay, amplitude / 2)\n                plt.scatter(tr.stats.sampling_rate * (delay + period),\n                            -amplitude / 2)\n                plt.show()\n            print('Amplitude picked: ' + str(amplitude))\n            print('Signal-to-noise ratio is: %s' %\n                  (amplitude / noise_amplitude))\n            # Note, amplitude should be in meters at the moment!\n            # Remove the pre-filter response\n            if pre_filt:\n                # Generate poles and zeros for the filter we used earlier: this\n                # is how the filter is designed in the convenience methods of\n                # filtering in obspy.\n                z, p, k = iirfilter(\n                    corners, [lowcut / (0.5 * tr.stats.sampling_rate),\n                              highcut / (0.5 * tr.stats.sampling_rate)],\n                    btype='band', ftype='butter', output='zpk')\n                filt_paz = {'poles': list(p), 'zeros': list(z), 'gain': k,\n                            'sensitivity': 1.0}\n                amplitude /= (paz_2_amplitude_value_of_freq_resp(\n                    filt_paz, 1 / period) * filt_paz['sensitivity'])\n            if PAZ:\n                amplitude /= 1000\n            if seedresp:  # Seedresp method returns mm\n                amplitude *= 1000000\n            # Write out the half amplitude, approximately the peak amplitude as\n            # used directly in magnitude calculations\n            amplitude *= 0.5\n            # Append an amplitude reading to the event\n            _waveform_id = WaveformStreamID(\n                station_code=tr.stats.station, channel_code=tr.stats.channel,\n                network_code=tr.stats.network)\n            pick_ind = len(event.picks)\n            event.picks.append(Pick(\n                waveform_id=_waveform_id, phase_hint='IAML',\n                polarity='undecidable', time=tr.stats.starttime + delay,\n                evaluation_mode='automatic'))\n            if not velocity:\n                event.amplitudes.append(Amplitude(\n                    generic_amplitude=amplitude / 1e9, period=period,\n                    pick_id=event.picks[pick_ind].resource_id,\n                    waveform_id=event.picks[pick_ind].waveform_id, unit='m',\n                    magnitude_hint='ML', type='AML', category='point'))\n            else:\n                event.amplitudes.append(Amplitude(\n                    generic_amplitude=amplitude / 1e9, period=period,\n                    pick_id=event.picks[pick_ind].resource_id,\n                    waveform_id=event.picks[pick_ind].waveform_id, unit='m/s',\n                    magnitude_hint='ML', type='AML', category='point'))\n    return event"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef svd_moments(u, s, v, stachans, event_list, n_svs=2):\n    # Define maximum number of events, will be the width of K\n    K_width = max([max(ev_list) for ev_list in event_list]) + 1\n    # Sometimes the randomisation generates a singular matrix - rather than\n    # attempting to regulerize this matrix I propose undertaking the\n    # randomisation step a further time\n    if len(stachans) == 1:\n        print('Only provided data from one station-channel - '\n              'will not try to invert')\n        return u[0][:, 0], event_list[0]\n    for i, stachan in enumerate(stachans):\n        k = []  # Small kernel matrix for one station - channel\n        # Copy the relevant vectors so as not to destroy them\n        # Here we'll swap into the Rubinstein U and V matrices\n        U_working = copy.deepcopy(v[i].T)\n        V_working = copy.deepcopy(u[i])\n        s_working = copy.deepcopy(s[i].T)\n        ev_list = event_list[i]\n        if len(ev_list) > len(U_working):\n            print('U is : ' + str(U_working.shape))\n            print('ev_list is len %s' % str(len(ev_list)))\n            f_dump = open('mag_calc_U_working.pkl', 'wb')\n            pickle.dump(U_working, f_dump)\n            f_dump.close()\n            raise IOError('More events than represented in U')\n        # Set all non-important singular values to zero\n        s_working[n_svs:len(s_working)] = 0\n        s_working = np.diag(s_working)\n        # Convert to numpy matrices\n        U_working = np.matrix(U_working)\n        V_working = np.matrix(V_working)\n        s_working = np.matrix(s_working)\n        SVD_weights = U_working[:, 0]\n        # If all the weights are negative take the abs\n        if np.all(SVD_weights < 0):\n            warnings.warn('All weights are negative - flipping them')\n            SVD_weights = np.abs(SVD_weights)\n        SVD_weights = np.array(SVD_weights).reshape(-1).tolist()\n        # Shuffle the SVD_weights prior to pairing - will give one of multiple\n        # pairwise options - see p1956 of Rubinstein & Ellsworth 2010\n        # We need to keep the real indexes though, otherwise, if there are\n        # multiple events with the same weight we will end up with multiple\n        # -1 values\n        random_SVD_weights = np.copy(SVD_weights)\n        # Tack on the indexes\n        random_SVD_weights = random_SVD_weights.tolist()\n        random_SVD_weights = [(random_SVD_weights[_i], _i)\n                              for _i in range(len(random_SVD_weights))]\n        random.shuffle(random_SVD_weights)\n        # Add the first element to the end so all elements will be paired twice\n        random_SVD_weights.append(random_SVD_weights[0])\n        # Take pairs of all the SVD_weights (each weight appears in 2 pairs)\n        pairs = []\n        for pair in _pairwise(random_SVD_weights):\n            pairs.append(pair)\n        # Deciding values for each place in kernel matrix using the pairs\n        for pairsIndex in range(len(pairs)):\n            # We will normalize by the minimum weight\n            _weights = list(zip(*list(pairs[pairsIndex])))[0]\n            _indeces = list(zip(*list(pairs[pairsIndex])))[1]\n            min_weight = min(np.abs(_weights))\n            max_weight = max(np.abs(_weights))\n            min_index = _indeces[np.argmin(np.abs(_weights))]\n            max_index = _indeces[np.argmax(np.abs(_weights))]\n            row = []\n            # Working out values for each row of kernel matrix\n            for j in range(len(SVD_weights)):\n                if j == max_index:\n                    result = -1\n                elif j == min_index:\n                    normalised = max_weight / min_weight\n                    result = float(normalised)\n                else:\n                    result = 0\n                row.append(result)\n            # Add each row to the K matrix\n            k.append(row)\n        # k is now a square matrix, we need to flesh it out to be K_width\n        k_filled = np.zeros([len(k), K_width])\n        for j in range(len(k)):\n            for l, ev in enumerate(ev_list):\n                k_filled[j, ev] = k[j][l]\n        if 'K' not in locals():\n            K = k_filled\n        else:\n            K = np.concatenate([K, k_filled])\n    # Remove any empty rows\n    K_nonempty = []\n    events_out = []\n    for i in range(0, K_width):\n        if not np.all(K[:, i] == 0):\n            K_nonempty.append(K[:, i])\n            events_out.append(i)\n    K = np.array(K_nonempty).T\n    K = K.tolist()\n    K_width = len(K[0])\n    # Add an extra row to K, so average moment = 1\n    K.append(np.ones(K_width) * (1. / K_width))\n    print(\"Created Kernel matrix: \")\n    del row\n    print('\\n'.join([''.join([str(round(float(item), 3)).ljust(6)\n          for item in row]) for row in K]))\n    Krounded = np.around(K, decimals=4)\n    # Create a weighting matrix to put emphasis on the final row.\n    W = np.matrix(np.identity(len(K)))\n    # the final element of W = the number of stations*number of events\n    W[-1, -1] = len(K) - 1\n    # Make K into a matrix\n    K = np.matrix(K)\n    ############\n\n    # Solve using the weighted least squares equation, K.T is K transpose\n    Kinv = np.array(np.linalg.inv(K.T * W * K) * K.T * W)\n\n    # M are the relative moments of the events\n    M = Kinv[:, -1]\n    # XXX TODO This still needs an outlier removal step\n    return M, events_out", "response": "This function calculates the relative moments of a given set of events using singular - value decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_picks(catalog, stations=None, channels=None, networks=None,\n                 locations=None, top_n_picks=None, evaluation_mode='all'):\n    \"\"\"\n    Filter events in the catalog based on a number of parameters.\n\n    :param catalog: Catalog to filter.\n    :type catalog: obspy.core.event.Catalog\n    :param stations: List for stations to keep picks from.\n    :type stations: list\n    :param channels: List of channels to keep picks from.\n    :type channels: list\n    :param networks: List of networks to keep picks from.\n    :type networks: list\n    :param locations: List of location codes to use\n    :type locations: list\n    :param top_n_picks: Filter only the top N most used station-channel pairs.\n    :type top_n_picks: int\n    :param evaluation_mode:\n        To select only manual or automatic picks, or use all (default).\n    :type evaluation_mode: str\n\n\n    :return:\n        Filtered Catalog - if events are left with no picks, they are removed\n        from the catalog.\n    :rtype: obspy.core.event.Catalog\n\n    .. note::\n        Will filter first by station, then by channel, then by network, if\n        using top_n_picks, this will be done last, after the other filters\n        have been applied.\n\n    .. note::\n        Doesn't work in place on the catalog, your input catalog will be safe\n        unless you overwrite it.\n\n    .. note:: Doesn't expand wildcard characters.\n\n    .. rubric:: Example\n\n    >>> from obspy.clients.fdsn import Client\n    >>> from eqcorrscan.utils.catalog_utils import filter_picks\n    >>> from obspy import UTCDateTime\n    >>> client = Client('NCEDC')\n    >>> t1 = UTCDateTime(2004, 9, 28)\n    >>> t2 = t1 + 86400\n    >>> catalog = client.get_events(starttime=t1, endtime=t2, minmagnitude=3,\n    ...                             minlatitude=35.7, maxlatitude=36.1,\n    ...                             minlongitude=-120.6, maxlongitude=-120.2,\n    ...                             includearrivals=True)\n    >>> print(len(catalog))\n    12\n    >>> filtered_catalog = filter_picks(catalog, stations=['BMS', 'BAP',\n    ...                                                    'PAG', 'PAN',\n    ...                                                    'PBI', 'PKY',\n    ...                                                    'YEG', 'WOF'])\n    >>> print(len(filtered_catalog))\n    12\n    >>> stations = []\n    >>> for event in filtered_catalog:\n    ...     for pick in event.picks:\n    ...         stations.append(pick.waveform_id.station_code)\n    >>> print(sorted(list(set(stations))))\n    ['BAP', 'BMS', 'PAG', 'PAN', 'PBI', 'PKY', 'WOF', 'YEG']\n    \"\"\"\n    # Don't work in place on the catalog\n    filtered_catalog = catalog.copy()\n\n    if stations:\n        for event in filtered_catalog:\n            if len(event.picks) == 0:\n                continue\n            event.picks = [pick for pick in event.picks\n                           if pick.waveform_id.station_code in stations]\n    if channels:\n        for event in filtered_catalog:\n            if len(event.picks) == 0:\n                continue\n            event.picks = [pick for pick in event.picks\n                           if pick.waveform_id.channel_code in channels]\n    if networks:\n        for event in filtered_catalog:\n            if len(event.picks) == 0:\n                continue\n            event.picks = [pick for pick in event.picks\n                           if pick.waveform_id.network_code in networks]\n    if locations:\n        for event in filtered_catalog:\n            if len(event.picks) == 0:\n                continue\n            event.picks = [pick for pick in event.picks\n                           if pick.waveform_id.location_code in locations]\n    if evaluation_mode == 'manual':\n        for event in filtered_catalog:\n            event.picks = [pick for pick in event.picks\n                           if pick.evaluation_mode == 'manual']\n    elif evaluation_mode == 'automatic':\n        for event in filtered_catalog:\n            event.picks = [pick for pick in event.picks\n                           if pick.evaluation_mode == 'automatic']\n    elif evaluation_mode != 'all':\n        warnings.warn('Unrecognised evaluation_mode: %s, using all picks' %\n                      evaluation_mode)\n    if top_n_picks:\n        all_picks = []\n        for event in filtered_catalog:\n            all_picks += [(pick.waveform_id.station_code,\n                           pick.waveform_id.channel_code)\n                          for pick in event.picks]\n        counted = Counter(all_picks).most_common()\n        all_picks = []\n        # Hack around sorting the counter object: Py 2 does it differently to 3\n        for i in range(counted[0][1]):\n            highest = [item[0] for item in counted\n                       if item[1] >= counted[0][1] - i]\n            # Sort them by alphabetical order in station\n            highest = sorted(highest, key=lambda tup: tup[0])\n            for stachan in highest:\n                if stachan not in all_picks:\n                    all_picks.append(stachan)\n            if len(all_picks) > top_n_picks:\n                all_picks = all_picks[0:top_n_picks]\n                break\n        for event in filtered_catalog:\n            if len(event.picks) == 0:\n                continue\n            event.picks = [pick for pick in event.picks\n                           if (pick.waveform_id.station_code,\n                               pick.waveform_id.channel_code) in all_picks]\n    # Remove events without picks\n    tmp_catalog = Catalog()\n    for event in filtered_catalog:\n        if len(event.picks) > 0:\n            tmp_catalog.append(event)\n\n    return tmp_catalog", "response": "Filter events in a catalog based on a number of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spatial_clip(catalog, corners, mindepth=None, maxdepth=None):\n    cat_out = catalog.copy()\n    if mindepth is not None:\n        for event in cat_out:\n            try:\n                origin = _get_origin(event)\n            except IOError:\n                continue\n            if origin.depth < mindepth * 1000:\n                cat_out.events.remove(event)\n    if maxdepth is not None:\n        for event in cat_out:\n            try:\n                origin = _get_origin(event)\n            except IOError:\n                continue\n            if origin.depth > maxdepth * 1000:\n                cat_out.events.remove(event)\n    for event in cat_out:\n        try:\n            origin = _get_origin(event)\n        except IOError:\n            continue\n        if not corners.contains_point((origin.latitude, origin.longitude)):\n            cat_out.events.remove(event)\n    return cat_out", "response": "Clip the catalog to a spatial box."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the origin of an event.", "response": "def _get_origin(event):\n    \"\"\"\n    Get the origin of an event.\n\n    :type event: :class:`obspy.core.event.Event`\n    :param event: Event to get the origin of.\n    :return: :class:`obspy.core.event.Origin`\n    \"\"\"\n    if event.preferred_origin() is not None:\n        origin = event.preferred_origin()\n    elif len(event.origins) > 0:\n        origin = event.origins[0]\n    else:\n        raise IndexError('No origin set, cannot constrain')\n    return origin"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_tutorial(plot=False, process_len=3600, num_cores=cpu_count()):\n    # First we want to load our templates\n    template_names = glob.glob('tutorial_template_*.ms')\n\n    if len(template_names) == 0:\n        raise IOError('Template files not found, have you run the template ' +\n                      'creation tutorial?')\n\n    templates = [read(template_name) for template_name in template_names]\n\n    # Work out what stations we have and get the data for them\n    stations = []\n    for template in templates:\n        for tr in template:\n            stations.append((tr.stats.station, tr.stats.channel))\n    # Get a unique list of stations\n    stations = list(set(stations))\n\n    # We will loop through the data chunks at a time, these chunks can be any\n    # size, in general we have used 1 day as our standard, but this can be\n    # as short as five minutes (for MAD thresholds) or shorter for other\n    # threshold metrics. However the chunk size should be the same as your\n    # template process_len.\n\n    # You should test different parameters!!!\n    start_time = UTCDateTime(2016, 1, 4)\n    end_time = UTCDateTime(2016, 1, 5)\n    chunks = []\n    chunk_start = start_time\n    while chunk_start < end_time:\n        chunk_end = chunk_start + process_len\n        if chunk_end > end_time:\n            chunk_end = end_time\n        chunks.append((chunk_start, chunk_end))\n        chunk_start += process_len\n\n    unique_detections = []\n\n    # Set up a client to access the GeoNet database\n    client = Client(\"GEONET\")\n\n    # Note that these chunks do not rely on each other, and could be paralleled\n    # on multiple nodes of a distributed cluster, see the SLURM tutorial for\n    # an example of this.\n    for t1, t2 in chunks:\n        # Generate the bulk information to query the GeoNet database\n        bulk_info = []\n        for station in stations:\n            bulk_info.append(('NZ', station[0], '*',\n                              station[1][0] + 'H' + station[1][-1], t1, t2))\n\n        # Note this will take a little while.\n        print('Downloading seismic data, this may take a while')\n        st = client.get_waveforms_bulk(bulk_info)\n        # Merge the stream, it will be downloaded in chunks\n        st.merge(fill_value='interpolate')\n\n        # Pre-process the data to set frequency band and sampling rate\n        # Note that this is, and MUST BE the same as the parameters used for\n        # the template creation.\n        print('Processing the seismic data')\n        st = pre_processing.shortproc(\n            st, lowcut=2.0, highcut=9.0, filt_order=4, samp_rate=20.0,\n            debug=0, num_cores=num_cores, starttime=t1, endtime=t2)\n        # Convert from list to stream\n        st = Stream(st)\n\n        # Now we can conduct the matched-filter detection\n        detections = match_filter.match_filter(\n            template_names=template_names, template_list=templates,\n            st=st, threshold=8.0, threshold_type='MAD', trig_int=6.0,\n            plotvar=plot, plotdir='.', cores=num_cores, debug=0,\n            plot_format='png')\n\n        # Now lets try and work out how many unique events we have just to\n        # compare with the GeoNet catalog of 20 events on this day in this\n        # sequence\n        for master in detections:\n            keep = True\n            for slave in detections:\n                if not master == slave and abs(master.detect_time -\n                                               slave.detect_time) <= 1.0:\n                    # If the events are within 1s of each other then test which\n                    # was the 'best' match, strongest detection\n                    if not master.detect_val > slave.detect_val:\n                        keep = False\n                        print('Removed detection at %s with cccsum %s'\n                              % (master.detect_time, master.detect_val))\n                        print('Keeping detection at %s with cccsum %s'\n                              % (slave.detect_time, slave.detect_val))\n                        break\n            if keep:\n                unique_detections.append(master)\n                print('Detection at :' + str(master.detect_time) +\n                      ' for template ' + master.template_name +\n                      ' with a cross-correlation sum of: ' +\n                      str(master.detect_val))\n                # We can plot these too\n                if plot:\n                    stplot = st.copy()\n                    template = templates[template_names.index(\n                        master.template_name)]\n                    lags = sorted([tr.stats.starttime for tr in template])\n                    maxlag = lags[-1] - lags[0]\n                    stplot.trim(starttime=master.detect_time - 10,\n                                endtime=master.detect_time + maxlag + 10)\n                    plotting.detection_multiplot(\n                        stplot, template, [master.detect_time.datetime])\n    print('We made a total of ' + str(len(unique_detections)) + ' detections')\n    return unique_detections", "response": "Main function to run the tutorial dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_parameters(infile='../parameters/EQcorrscan_parameters.txt'):\n    try:\n        import ConfigParser\n    except ImportError:\n        import configparser as ConfigParser\n    import ast\n    f = open(infile, 'r')\n    print('Reading parameters with the following header:')\n    for line in f:\n        if line[0] == '#':\n            print(line.rstrip('\\n').lstrip('\\n'))\n    f.close()\n    config = ConfigParser.ConfigParser()\n    config.read(infile)\n    # Slightly tricky list reading\n    template_names = list(ast.literal_eval(config.get(\"eqcorrscan_pars\",\n                                                      \"template_names\")))\n    parameters = \\\n        EQcorrscanParameters(template_names=template_names,\n                             lowcut=config.get(\"eqcorrscan_pars\", \"lowcut\"),\n                             highcut=config.get(\"eqcorrscan_pars\", \"highcut\"),\n                             filt_order=config.get(\"eqcorrscan_pars\",\n                                                   \"filt_order\"),\n                             samp_rate=config.get(\"eqcorrscan_pars\",\n                                                  \"samp_rate\"),\n                             debug=config.get(\"eqcorrscan_pars\", \"debug\"),\n                             startdate=config.get(\"eqcorrscan_pars\",\n                                                  \"startdate\"),\n                             enddate=config.get(\"eqcorrscan_pars\", \"enddate\"),\n                             archive=config.get(\"eqcorrscan_pars\", \"archive\"),\n                             arc_type=config.get(\"eqcorrscan_pars\",\n                                                 \"arc_type\"),\n                             cores=config.get(\"eqcorrscan_pars\", \"cores\"),\n                             plotvar=config.getboolean(\"eqcorrscan_pars\",\n                                                       \"plotvar\"),\n                             plotdir=config.get(\"eqcorrscan_pars\", \"plotdir\"),\n                             plot_format=config.get(\"eqcorrscan_pars\",\n                                                    \"plot_format\"),\n                             tempdir=ast.literal_eval(config.\n                                                      get(\"eqcorrscan_pars\",\n                                                          \"tempdir\")),\n                             threshold=config.get(\"eqcorrscan_pars\",\n                                                  \"threshold\"),\n                             threshold_type=config.get(\"eqcorrscan_pars\",\n                                                       \"threshold_type\"),\n                             trigger_interval=config.get(\"eqcorrscan_pars\",\n                                                         \"trigger_interval\")\n                             )\n\n    return parameters", "response": "Read the default parameters from file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, outfile='../parameters/EQcorrscan_parameters.txt',\n              overwrite=False):\n        \"\"\"\n        Function to write the parameters to a file - user readable.\n\n        :type outfile: str\n        :param outfile: Full path to filename to store parameters in.\n        :type overwrite: bool\n        :param overwrite: Whether to overwrite the old file or not.\n        \"\"\"\n        outpath = os.sep.join(outfile.split(os.sep)[0:-1])\n        if len(outpath) > 0 and not os.path.isdir(outpath):\n            msg = ' '.join([os.path.join(outfile.split(os.sep)[0:-1]),\n                            'does not exist, check path.'])\n            raise IOError(msg)\n        # Make sure that the user wants to overwrite the old parameters\n        if os.path.isfile(outfile) and not overwrite:\n            responding = True\n            while responding:\n                print(' '.join([outfile, 'exists.  Overwrite? [y/N]']))\n                option = raw_input()\n                if option.upper() == 'N':\n                    raise IOError('File exists, will not overwrite')\n                elif option.upper() == 'Y':\n                    responding = False\n                else:\n                    print('Must respond with y or n')\n        f = open(outfile, 'w')\n        # Write creation info.\n        header = ' '.join(['# User:', getpass.getuser(),\n                           '\\n# Creation date:', str(UTCDateTime()),\n                           '\\n# EQcorrscan version:',\n                           str(eqcorrscan.__version__),\n                           '\\n\\n\\n'])\n        f.write(header)\n        # Write parameter info in a user readable, and parsable format.\n        parameters = self.__str__().split('\\n')[1:]\n        f.write('[eqcorrscan_pars]\\n')\n        for parameter in parameters:\n            f.write(parameter.lstrip() + '\\n')\n        f.close()\n        print('Written parameter file: ' + outfile)", "response": "Function to write the parameters to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the data quality of the daylong file.", "response": "def _check_daylong(tr):\n    \"\"\"\n    Check the data quality of the daylong file.\n\n    Check to see that the day isn't just zeros, with large steps, if it is\n    then the resampling will hate it.\n\n    :type tr: obspy.core.trace.Trace\n    :param tr: Trace to check if the data are daylong.\n\n    :return quality (simply good or bad)\n    :rtype: bool\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> from eqcorrscan.utils.pre_processing import _check_daylong\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>> st = read(TEST_PATH + '/WAV/TEST_/' +\n    ...           '2013-09-01-0410-35.DFDPC_024_00')\n    >>> _check_daylong(st[0])\n    True\n    \"\"\"\n    if len(np.nonzero(tr.data)[0]) < 0.5 * len(tr.data):\n        qual = False\n    else:\n        qual = True\n    return qual"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dayproc(st, lowcut, highcut, filt_order, samp_rate, starttime, debug=0,\n            parallel=True, num_cores=False, ignore_length=False,\n            seisan_chan_names=False, fill_gaps=True):\n    \"\"\"\n    Wrapper for dayproc to parallel multiple traces in a stream.\n\n    Works in place on data.  This is employed to ensure all parts of the data \\\n    are processed in the same way.\n\n    :type st: obspy.core.stream.Stream\n    :param st: Stream to process (can be trace).\n    :type lowcut: float\n    :param lowcut: Low cut in Hz for bandpass.\n    :type highcut: float\n    :param highcut: High cut in Hz for bandpass.\n    :type filt_order: int\n    :param filt_order: Corners for bandpass.\n    :type samp_rate: float\n    :param samp_rate: Desired sampling rate in Hz.\n    :type starttime: obspy.core.utcdatetime.UTCDateTime\n    :param starttime: Desired start-date of trace.\n    :type debug: int\n    :param debug: Debug output level from 0-5, higher numbers = more output.\n    :type parallel: bool\n    :param parallel:\n        Set to True to process traces in parallel, this is often faster than\n        serial processing of traces: defaults to True.\n    :type num_cores: int\n    :param num_cores:\n        Control the number of cores for parallel processing, if set to False\n        then this will use all the cores.\n    :type ignore_length: bool\n    :param ignore_length: See warning below.\n    :type seisan_chan_names: bool\n    :param seisan_chan_names:\n        Whether channels are named like seisan channels (which are two letters\n        rather than SEED convention of three) - defaults to True.\n    :type fill_gaps: bool\n    :param fill_gaps: Whether to pad any gaps found with zeros or not.\n\n    :return: Processed stream.\n    :rtype: :class:`obspy.core.stream.Stream`\n\n    .. note::\n        If your data contain gaps you should *NOT* fill those gaps before\n        using the pre-process functions. The pre-process functions will fill\n        the gaps internally prior to processing, process the data, then re-fill\n        the gaps with zeros to ensure correlations are not incorrectly\n        calculated within gaps. If your data have gaps you should pass a merged\n        stream without the `fill_value` argument (e.g.: `st = st.merge()`).\n\n    .. warning::\n        Will fail if data are less than 19.2 hours long - this number is\n        arbitrary and is chosen to alert the user to the dangers of padding\n        to day-long, if you don't care you can ignore this error by setting\n        `ignore_length=True`. Use this option at your own risk!  It will also\n        warn any-time it has to pad data - if you see strange artifacts in your\n        detections, check whether the data have gaps.\n\n    .. rubric:: Example\n\n    >>> import obspy\n    >>> if int(obspy.__version__.split('.')[0]) >= 1:\n    ...     from obspy.clients.fdsn import Client\n    ... else:\n    ...     from obspy.fdsn import Client\n    >>> from obspy import UTCDateTime\n    >>> from eqcorrscan.utils.pre_processing import dayproc\n    >>> client = Client('NCEDC')\n    >>> t1 = UTCDateTime(2012, 3, 26)\n    >>> t2 = t1 + 86400\n    >>> bulk_info = [('BP', 'JCNB', '40', 'SP1', t1, t2)]\n    >>> st = client.get_waveforms_bulk(bulk_info)\n    >>> st_keep = st.copy()  # Copy the stream for later examples\n    >>> # Example of bandpass filtering\n    >>> st = dayproc(st=st, lowcut=2, highcut=9, filt_order=3, samp_rate=20,\n    ...              starttime=t1, debug=0, parallel=True, num_cores=2)\n    >>> print(st[0])\n    BP.JCNB.40.SP1 | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.\\\n950000Z | 20.0 Hz, 1728000 samples\n    >>> # Example of lowpass filtering\n    >>> st = dayproc(st=st, lowcut=None, highcut=9, filt_order=3, samp_rate=20,\n    ...              starttime=t1, debug=0, parallel=True, num_cores=2)\n    >>> print(st[0])\n    BP.JCNB.40.SP1 | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.\\\n950000Z | 20.0 Hz, 1728000 samples\n    >>> # Example of highpass filtering\n    >>> st = dayproc(st=st, lowcut=2, highcut=None, filt_order=3, samp_rate=20,\n    ...              starttime=t1, debug=0, parallel=True, num_cores=2)\n    >>> print(st[0])\n    BP.JCNB.40.SP1 | 2012-03-26T00:00:00.000000Z - 2012-03-26T23:59:59.\\\n950000Z | 20.0 Hz, 1728000 samples\n    \"\"\"\n    # Add sanity check for filter\n    if isinstance(st, Trace):\n        st = Stream(st)\n        tracein = True\n    else:\n        tracein = False\n    if highcut and highcut >= 0.5 * samp_rate:\n        raise IOError('Highcut must be lower than the nyquist')\n    if debug > 4:\n        parallel = False\n    # Set the start-time to a day start - cope with\n    if starttime is None:\n        startdates = []\n        for tr in st:\n            if abs(tr.stats.starttime - (UTCDateTime(\n                    tr.stats.starttime.date) + 86400)) < tr.stats.delta:\n                # If the trace starts within 1 sample of the next day, use the\n                # next day as the startdate\n                startdates.append((tr.stats.starttime + 86400).date)\n                debug_print(\n                    '{0} starts within 1 sample of the next day, using this '\n                    'time {1}'.format(\n                        tr.id, (tr.stats.starttime + 86400).date), 2, debug)\n            else:\n                startdates.append(tr.stats.starttime.date)\n        # Check that all traces start on the same date...\n        if not len(set(startdates)) == 1:\n            raise NotImplementedError('Traces start on different days')\n        starttime = UTCDateTime(startdates[0])\n    if parallel:\n        if not num_cores:\n            num_cores = cpu_count()\n        if num_cores > len(st):\n            num_cores = len(st)\n        pool = Pool(processes=num_cores)\n        results = [pool.apply_async(process, (tr,), {\n            'lowcut': lowcut, 'highcut': highcut, 'filt_order': filt_order,\n            'samp_rate': samp_rate, 'debug': debug, 'starttime': starttime,\n            'clip': True, 'ignore_length': ignore_length, 'length': 86400,\n            'seisan_chan_names': seisan_chan_names, 'fill_gaps': fill_gaps})\n                   for tr in st]\n        pool.close()\n        try:\n            stream_list = [p.get() for p in results]\n        except KeyboardInterrupt as e:  # pragma: no cover\n            pool.terminate()\n            raise e\n        pool.join()\n        st = Stream(stream_list)\n    else:\n        for i, tr in enumerate(st):\n            st[i] = process(\n                tr=tr, lowcut=lowcut, highcut=highcut, filt_order=filt_order,\n                samp_rate=samp_rate, debug=debug, starttime=starttime,\n                clip=True, length=86400, ignore_length=ignore_length,\n                seisan_chan_names=seisan_chan_names, fill_gaps=fill_gaps)\n    for tr in st:\n        if len(tr.data) == 0:\n            st.remove(tr)\n    if tracein:\n        st.merge()\n        return st[0]\n    return st", "response": "This function processes a single segment of data in a stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(tr, lowcut, highcut, filt_order, samp_rate, debug,\n            starttime=False, clip=False, length=86400,\n            seisan_chan_names=False, ignore_length=False, fill_gaps=True):\n    \"\"\"\n    Basic function to process data, usually called by dayproc or shortproc.\n\n    Functionally, this will bandpass, downsample and check headers and length\n    of trace to ensure files start when they should and are the correct length.\n    This is a simple wrapper on obspy functions, we include it here to provide\n    a system to ensure all parts of the dataset are processed in the same way.\n\n    .. note:: Usually this function is called via dayproc or shortproc.\n\n    :type tr: obspy.core.trace.Trace\n    :param tr: Trace to process\n    :type lowcut: float\n    :param lowcut: Low cut in Hz, if set to None and highcut is set, will use \\\n        a lowpass filter.\n    :type highcut: float\n    :param highcut: High cut in Hz, if set to None and lowcut is set, will \\\n        use a highpass filter.\n    :type filt_order: int\n    :param filt_order: Number of corners for filter.\n    :type samp_rate: float\n    :param samp_rate: Desired sampling rate in Hz.\n    :type debug: int\n    :param debug: Debug output level from 0-5, higher numbers = more output.\n    :type starttime: obspy.core.utcdatetime.UTCDateTime\n    :param starttime: Desired start of trace\n    :type clip: bool\n    :param clip: Whether to expect, and enforce a set length of data or not.\n    :type length: float\n    :param length: Use to set a fixed length for data from the given starttime.\n    :type seisan_chan_names: bool\n    :param seisan_chan_names:\n        Whether channels are named like seisan channels (which are two letters\n        rather than SEED convention of three) - defaults to True.\n    :type ignore_length: bool\n    :param ignore_length: See warning in dayproc.\n    :type fill_gaps: bool\n    :param fill_gaps: Whether to pad any gaps found with zeros or not.\n\n    :return: Processed trace.\n    :type: :class:`obspy.core.stream.Trace`\n\n    .. note::\n        If your data contain gaps you should *NOT* fill those gaps before\n        using the pre-process functions. The pre-process functions will fill\n        the gaps internally prior to processing, process the data, then re-fill\n        the gaps with zeros to ensure correlations are not incorrectly\n        calculated within gaps. If your data have gaps you should pass a merged\n        stream without the `fill_value` argument (e.g.: `tr = tr.merge()`).\n    \"\"\"\n    # Add sanity check\n    if highcut and highcut >= 0.5 * samp_rate:\n        raise IOError('Highcut must be lower than the nyquist')\n\n    # Define the start-time\n    if starttime:\n        # Be nice and allow a datetime object.\n        if isinstance(starttime, dt.date) or isinstance(starttime,\n                                                        dt.datetime):\n            starttime = UTCDateTime(starttime)\n        day = starttime.date\n    else:\n        day = tr.stats.starttime.date\n\n    debug_print(\n        'Working on: ' + tr.stats.station + '.' + tr.stats.channel, 2, debug)\n    if debug >= 5:\n        tr.plot()\n    # Check if the trace is gappy and pad if it is.\n    gappy = False\n    if isinstance(tr.data, np.ma.MaskedArray):\n        gappy = True\n        gaps, tr = _fill_gaps(tr)\n    # Do a brute force quality check\n    qual = _check_daylong(tr)\n    if not qual:\n        msg = (\"Data have more zeros than actual data, please check the raw\",\n               \" data set-up and manually sort it: \" + tr.stats.station + \".\" +\n               tr.stats.channel)\n        raise ValueError(msg)\n    tr = tr.detrend('simple')\n    # Detrend data before filtering\n    debug_print('I have ' + str(len(tr.data)) + ' data points for ' +\n                tr.stats.station + '.' + tr.stats.channel +\n                ' before processing', 0, debug)\n\n    # Sanity check to ensure files are daylong\n    padded = False\n    if clip:\n        tr = tr.trim(starttime, starttime + length, nearest_sample=True)\n    if float(tr.stats.npts / tr.stats.sampling_rate) != length and clip:\n        debug_print('Data for ' + tr.stats.station + '.' + tr.stats.channel +\n                    ' are not of daylong length, will zero pad', 2, debug)\n        if tr.stats.endtime - tr.stats.starttime < 0.8 * length\\\n           and not ignore_length:\n            raise NotImplementedError(\n                \"Data for {0}.{1} is {2:.2f} seconds long, which is less than \"\n                \"80 percent of the desired length ({3} seconds), will not \"\n                \"pad\".format(\n                    tr.stats.station, tr.stats.channel,\n                    tr.stats.endtime - tr.stats.starttime, length))\n        # trim, then calculate length of any pads required\n        pre_pad_secs = tr.stats.starttime - starttime\n        post_pad_secs = (starttime + length) - tr.stats.endtime\n        if pre_pad_secs > 0 or post_pad_secs > 0:\n            padded = True\n            pre_pad = np.zeros(int(pre_pad_secs * tr.stats.sampling_rate))\n            post_pad = np.zeros(int(post_pad_secs * tr.stats.sampling_rate))\n            debug_print(str(tr), 2, debug)\n            debug_print(\n                \"Padding to day long with %f s before and %f s at end\" %\n                (pre_pad_secs, post_pad_secs), 1, debug)\n            tr.data = np.concatenate([pre_pad, tr.data, post_pad])\n            # Use this rather than the expected pad because of rounding samples\n            tr.stats.starttime -= len(pre_pad) * tr.stats.delta\n            debug_print(str(tr), 2, debug)\n        # If there is one sample too many after this remove the first one\n        # by convention\n        if len(tr.data) == (length * tr.stats.sampling_rate) + 1:\n            tr.data = tr.data[1:len(tr.data)]\n        if not tr.stats.sampling_rate * length == tr.stats.npts:\n                raise ValueError('Data are not daylong for ' +\n                                 tr.stats.station + '.' + tr.stats.channel)\n        debug_print('I now have %i data points after enforcing length'\n                    % len(tr.data), 0, debug)\n    # Check sampling rate and resample\n    if tr.stats.sampling_rate != samp_rate:\n        debug_print('Resampling', 1, debug)\n        tr.resample(samp_rate)\n    # Filtering section\n    tr = tr.detrend('simple')    # Detrend data again before filtering\n    if highcut and lowcut:\n        debug_print('Bandpassing', 1, debug)\n        tr.data = bandpass(tr.data, lowcut, highcut,\n                           tr.stats.sampling_rate, filt_order, True)\n    elif highcut:\n        debug_print('Lowpassing', 1, debug)\n        tr.data = lowpass(tr.data, highcut, tr.stats.sampling_rate,\n                          filt_order, True)\n    elif lowcut:\n        debug_print('Highpassing', 1, debug)\n        tr.data = highpass(tr.data, lowcut, tr.stats.sampling_rate,\n                           filt_order, True)\n    else:\n        debug_print('No filters applied', 2, debug)\n    # Account for two letter channel names in s-files and therefore templates\n    if seisan_chan_names:\n        tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n\n    # Sanity check the time header\n    if tr.stats.starttime.day != day and clip:\n        debug_print(\"Time headers do not match expected date: {0}\".format(\n            tr.stats.starttime), 2, debug)\n\n    if padded:\n        debug_print(\"Reapplying zero pads post processing\", 1, debug)\n        debug_print(str(tr), 2, debug)\n        pre_pad = np.zeros(int(pre_pad_secs * tr.stats.sampling_rate))\n        post_pad = np.zeros(int(post_pad_secs * tr.stats.sampling_rate))\n        pre_pad_len = len(pre_pad)\n        post_pad_len = len(post_pad)\n        debug_print(\"Taking only valid data between %i and %i samples\" %\n                    (pre_pad_len, len(tr.data) - post_pad_len), 1, debug)\n        # Re-apply the pads, taking only the data section that was valid\n        tr.data = np.concatenate(\n            [pre_pad, tr.data[pre_pad_len: len(tr.data) - post_pad_len],\n             post_pad])\n        debug_print(str(tr), 2, debug)\n    # Sanity check to ensure files are daylong\n    if float(tr.stats.npts / tr.stats.sampling_rate) != length and clip:\n        debug_print('Data for ' + tr.stats.station + '.' + tr.stats.channel +\n                    ' are not of daylong length, will zero pad', 1, debug)\n        # Use obspy's trim function with zero padding\n        tr = tr.trim(starttime, starttime + length, pad=True, fill_value=0,\n                     nearest_sample=True)\n        # If there is one sample too many after this remove the last one\n        # by convention\n        if len(tr.data) == (length * tr.stats.sampling_rate) + 1:\n            tr.data = tr.data[1:len(tr.data)]\n        if not tr.stats.sampling_rate * length == tr.stats.npts:\n                raise ValueError('Data are not daylong for ' +\n                                 tr.stats.station + '.' + tr.stats.channel)\n    # Replace the gaps with zeros\n    if gappy:\n        tr = _zero_pad_gaps(tr, gaps, fill_gaps=fill_gaps)\n    # Final visual check for debug\n    if debug > 4:\n        tr.plot()\n    return tr", "response": "This function processes the data in a single segment of a trace and returns a list of the data in the order that it should be processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new trace with padded parts of trace with zeros.", "response": "def _zero_pad_gaps(tr, gaps, fill_gaps=True):\n    \"\"\"\n    Replace padded parts of trace with zeros.\n\n    Will cut around gaps, detrend, then pad the gaps with zeros.\n\n    :type tr: :class:`osbpy.core.stream.Trace`\n    :param tr: A trace that has had the gaps padded\n    :param gaps: List of dict of start-time and end-time as UTCDateTime objects\n    :type gaps: list\n\n    :return: :class:`obspy.core.stream.Trace`\n    \"\"\"\n    start_in, end_in = (tr.stats.starttime, tr.stats.endtime)\n    for gap in gaps:\n        stream = Stream()\n        if gap['starttime'] > tr.stats.starttime:\n            stream += tr.slice(tr.stats.starttime, gap['starttime']).copy()\n        if gap['endtime'] < tr.stats.endtime:\n            # Note this can happen when gaps are calculated for a trace that\n            # is longer than `length`, e.g. gaps are calculated pre-trim.\n            stream += tr.slice(gap['endtime'], tr.stats.endtime).copy()\n        tr = stream.merge()[0]\n    if fill_gaps:\n        tr = tr.split()\n        tr = tr.detrend()\n        tr = tr.merge(fill_value=0)[0]\n        # Need to check length - if a gap happened overlapping the end or start\n        #  of the trace this will be lost.\n        if tr.stats.starttime != start_in:\n            # pad with zeros\n            tr.data = np.concatenate(\n                [np.zeros(int(tr.stats.starttime - start_in)), tr.data])\n            tr.stats.starttime = start_in\n        if tr.stats.endtime != end_in:\n            tr.data = np.concatenate(\n                [tr.data, np.zeros(int(end_in - tr.stats.endtime))])\n    return tr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fill_gaps(tr):\n    tr = tr.split()\n    gaps = tr.get_gaps()\n    tr = tr.detrend().merge(fill_value=0)[0]\n    gaps = [{'starttime': gap[4], 'endtime': gap[5]} for gap in gaps]\n    return gaps, tr", "response": "Interpolate through gaps and work - out where gaps are."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest primality of a number.", "response": "def is_prime(number):\n    \"\"\"\n    Function to test primality of a number. Function lifted from online\n    resource:\n        http://www.codeproject.com/Articles/691200/Primality-test-algorithms-Prime-test-The-fastest-w\n\n    This function is distributed under a separate licence:\n        This article, along with any associated source code and files, is \\\n        licensed under The Code Project Open License (CPOL)\n\n    :type number: int\n    :param number: Integer to test for primality\n\n    :returns: bool\n\n    >>> is_prime(4)\n    False\n    >>> is_prime(3)\n    True\n    \"\"\"\n    ''' if number != 1 '''\n    if number > 1:\n        ''' repeat the test few times '''\n        for time in range(3):\n            ''' Draw a RANDOM number in range of number ( Z_number )  '''\n            randomNumber = random.randint(2, number - 1)\n            ''' Test if a^(n-1) = 1 mod n '''\n            if pow(randomNumber, number - 1, number) != 1:\n                return False\n        return True\n    else:\n        ''' case number == 1 '''\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_peaks2_short(arr, thresh, trig_int, debug=0, starttime=False,\n                      samp_rate=1.0, full_peaks=False):\n    \"\"\"\n    Determine peaks in an array of data above a certain threshold.\n\n    Uses a mask to remove data below threshold and finds peaks in what is left.\n\n    :type arr: numpy.ndarray\n    :param arr: 1-D numpy array is required\n    :type thresh: float\n    :param thresh:\n        The threshold below which will be considered noise and peaks will\n        not be found in.\n    :type trig_int: int\n    :param trig_int:\n        The minimum difference in samples between triggers, if multiple\n        peaks within this window this code will find the highest.\n    :type debug: int\n    :param debug: Optional, debug level 0-5\n    :type starttime: obspy.core.utcdatetime.UTCDateTime\n    :param starttime: Starttime for plotting, only used if debug > 2.\n    :type samp_rate: float\n    :param samp_rate: Sampling rate in Hz, only used for plotting if debug > 2.\n    :type full_peaks: bool\n    :param full_peaks:\n        If True, will remove the issue eluded to below, by declustering within\n        data-sections above the threshold, rather than just taking the peak\n        within that section. This will take more time. This defaults to True\n        for match_filter.\n\n    :return: peaks: Lists of tuples of peak values and locations.\n    :rtype: list\n\n\n    >>> import numpy as np\n    >>> arr = np.random.randn(100)\n    >>> threshold = 10\n    >>> arr[40] = 20\n    >>> arr[60] = 100\n    >>> find_peaks2_short(arr, threshold, 3)\n    [(20.0, 40), (100.0, 60)]\n\n    .. note::\n        peak-finding is optimised for zero-mean cross-correlation data where\n        fluctuations are frequent.  Because of this, in certain cases some\n        peaks may be missed if the trig_int is short and the threshold is low.\n        Consider the following case:\n\n        >>> arr = np.array([1, .2, .2, .2, .2, 1, .2, .2, .2, .2, 1])\n        >>> find_peaks2_short(arr, thresh=.2, trig_int=3)\n        [(1.0, 0)]\n\n        Whereas you would expect the following:\n\n        >>> arr = np.array([1, .2, .2, .2, .2, 1, .2, .2, .2, .2, 1])\n        >>> find_peaks2_short(arr, thresh=.2, trig_int=3, full_peaks=True)\n        [(1.0, 0), (1.0, 5), (1.0, 10)]\n\n        This is rare and unlikely to happen for correlation cases, where\n        trigger intervals are usually large and thresholds high.\n\n    \"\"\"\n    if not starttime:\n        starttime = UTCDateTime(0)\n    # Set everything below the threshold to zero\n    image = np.copy(arr)\n    image = np.abs(image)\n    debug_print(\"Threshold: {0}\\tMax: {1}\".format(thresh, max(image)),\n                2, debug)\n    image[image < thresh] = 0\n    if len(image[image > thresh]) == 0:\n        debug_print(\"No values over threshold {0}\".format(thresh), 0, debug)\n        return []\n    debug_print('Found {0} samples above the threshold'.format(\n        len(image[image > thresh])), 0, debug)\n    initial_peaks = []\n    # Find the peaks\n    labeled_image, number_of_objects = ndimage.label(image)\n    peak_slices = ndimage.find_objects(labeled_image)\n    for peak_slice in peak_slices:\n        window = arr[peak_slice[0].start: peak_slice[0].stop]\n        if peak_slice[0].stop - peak_slice[0].start > trig_int and full_peaks:\n            peaks = decluster(\n                peaks=window, trig_int=trig_int,\n                index=np.arange(peak_slice[0].start, peak_slice[0].stop))\n        else:\n            peaks = [(window[np.argmax(abs(window))],\n                      int(peak_slice[0].start + np.argmax(abs(window))))]\n        initial_peaks.extend(peaks)\n    peaks = decluster(peaks=np.array(list(zip(*initial_peaks))[0]),\n                      index=np.array(list(zip(*initial_peaks))[1]),\n                      trig_int=trig_int)\n    if initial_peaks:\n        if debug >= 3:\n            from eqcorrscan.utils import plotting\n            _fname = ''.join([\n                'peaks_', starttime.datetime.strftime('%Y-%m-%d'), '.pdf'])\n            plotting.peaks_plot(\n                data=image, starttime=starttime, samp_rate=samp_rate,\n                save=True, peaks=peaks, savefile=_fname)\n        peaks = sorted(peaks, key=lambda time: time[1], reverse=False)\n        return peaks\n    else:\n        print('No peaks for you!')\n        return []", "response": "This function finds peaks in an array of data above a certain threshold."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap for find - peaks for multiple arrays.", "response": "def multi_find_peaks(arr, thresh, trig_int, debug=0, starttime=False,\n                     samp_rate=1.0, parallel=True, full_peaks=False,\n                     cores=None):\n    \"\"\"\n    Wrapper for find-peaks for multiple arrays.\n\n    :type arr: numpy.ndarray\n    :param arr: 2-D numpy array is required\n    :type thresh: list\n    :param thresh:\n        The threshold below which will be considered noise and peaks will not\n        be found in. One threshold per array.\n    :type trig_int: int\n    :param trig_int:\n        The minimum difference in samples between triggers, if multiple\n        peaks within this window this code will find the highest.\n    :type debug: int\n    :param debug: Optional, debug level 0-5\n    :type starttime: obspy.core.utcdatetime.UTCDateTime\n    :param starttime: Starttime for plotting, only used if debug > 2.\n    :type samp_rate: float\n    :param samp_rate: Sampling rate in Hz, only used for plotting if debug > 2.\n    :type parallel: bool\n    :param parallel:\n        Whether to compute in parallel or not - will use multiprocessing\n    :type full_peaks: bool\n    :param full_peaks: See `eqcorrscan.utils.findpeaks.find_peaks2_short`\n    :type cores: int\n    :param cores:\n        Maximum number of processes to spin up for parallel peak-finding\n\n    :returns:\n        List of list of tuples of (peak, index) in same order as input arrays\n    \"\"\"\n    peaks = []\n    if not parallel:\n        for sub_arr, arr_thresh in zip(arr, thresh):\n            peaks.append(find_peaks2_short(\n                arr=sub_arr, thresh=arr_thresh, trig_int=trig_int, debug=debug,\n                starttime=starttime, samp_rate=samp_rate,\n                full_peaks=full_peaks))\n    else:\n        if cores is None:\n            cores = arr.shape[0]\n        with pool_boy(Pool=Pool, traces=cores) as pool:\n            params = ((sub_arr, arr_thresh, trig_int, debug,\n                       False, 1.0, full_peaks)\n                      for sub_arr, arr_thresh in zip(arr, thresh))\n            results = [pool.apply_async(find_peaks2_short, param)\n                       for param in params]\n            peaks = [res.get() for res in results]\n    return peaks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decluster(peaks, index, trig_int):\n    utilslib = _load_cdll('libutils')\n\n    length = np.int32(len(peaks))\n    utilslib.find_peaks.argtypes = [\n        np.ctypeslib.ndpointer(dtype=np.float32, shape=(length,),\n                               flags=native_str('C_CONTIGUOUS')),\n        np.ctypeslib.ndpointer(dtype=np.float32, shape=(length,),\n                               flags=native_str('C_CONTIGUOUS')),\n        ctypes.c_int, ctypes.c_float, ctypes.c_float,\n        np.ctypeslib.ndpointer(dtype=np.uint32, shape=(length,),\n                               flags=native_str('C_CONTIGUOUS'))]\n    utilslib.find_peaks.restype = ctypes.c_int\n    peaks_sort = sorted(zip(peaks, index),\n                        key=lambda amplitude: abs(amplitude[0]),\n                        reverse=True)\n    arr, inds = zip(*peaks_sort)\n    arr = np.ascontiguousarray(arr, dtype=np.float32)\n    inds = np.array(inds, dtype=np.float32) / trig_int\n    inds = np.ascontiguousarray(inds, dtype=np.float32)\n    out = np.zeros(len(arr), dtype=np.uint32)\n    ret = utilslib.find_peaks(\n        arr, inds, length, 0, np.float32(1), out)\n    if ret != 0:\n        raise MemoryError(\"Issue with c-routine, returned %i\" % ret)\n    peaks_out = list(compress(peaks_sort, out))\n    return peaks_out", "response": "Return a list of peaks based on an enforced minimum separation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds network coincidence triggers within peaks of detection statistics. Useful for finding network detections from sets of detections on individual stations. :type peaks: list :param peaks: List of lists of tuples of (peak, index) for each \\ station-channel. Index should be in samples. :type stachans: list :param stachans: List of tuples of (station, channel) in the order of \\ peaks. :type samp_rate: float :param samp_rate: Sampling rate in Hz :type moveout: float :param moveout: Allowable network moveout in seconds. :type min_trig: int :param min_trig: Minimum station-channels required to declare a trigger. :type trig_int: float :param trig_int: Minimum allowable time between network triggers in seconds. :return: List of tuples of (peak, index), for the earliest detected station. :rtype: list >>> peaks = [[(0.5, 100), (0.3, 800)], [(0.4, 120), (0.7, 850)]] >>> triggers = coin_trig(peaks, [('a', 'Z'), ('b', 'Z')], 10, 3, 2, 1) >>> print(triggers) [(0.45, 100)]", "response": "def coin_trig(peaks, stachans, samp_rate, moveout, min_trig, trig_int):\n    \"\"\"\n    Find network coincidence triggers within peaks of detection statistics.\n\n    Useful for finding network detections from sets of detections on individual\n    stations.\n\n    :type peaks: list\n    :param peaks: List of lists of tuples of (peak, index) for each \\\n        station-channel.  Index should be in samples.\n    :type stachans: list\n    :param stachans: List of tuples of (station, channel) in the order of \\\n        peaks.\n    :type samp_rate: float\n    :param samp_rate: Sampling rate in Hz\n    :type moveout: float\n    :param moveout: Allowable network moveout in seconds.\n    :type min_trig: int\n    :param min_trig: Minimum station-channels required to declare a trigger.\n    :type trig_int: float\n    :param trig_int:\n        Minimum allowable time between network triggers in seconds.\n\n    :return:\n        List of tuples of (peak, index), for the earliest detected station.\n    :rtype: list\n\n    >>> peaks = [[(0.5, 100), (0.3, 800)], [(0.4, 120), (0.7, 850)]]\n    >>> triggers = coin_trig(peaks, [('a', 'Z'), ('b', 'Z')], 10, 3, 2, 1)\n    >>> print(triggers)\n    [(0.45, 100)]\n    \"\"\"\n    triggers = []\n    for stachan, _peaks in zip(stachans, peaks):\n        for peak in _peaks:\n            trigger = (peak[1], peak[0], '.'.join(stachan))\n            triggers.append(trigger)\n    coincidence_triggers = []\n    for i, master in enumerate(triggers):\n        slaves = triggers[i + 1:]\n        coincidence = 1\n        trig_time = master[0]\n        trig_val = master[1]\n        for slave in slaves:\n            if abs(slave[0] - master[0]) <= (moveout * samp_rate) and \\\n               slave[2] != master[2]:\n                coincidence += 1\n                if slave[0] < master[0]:\n                    trig_time = slave[0]\n                trig_val += slave[1]\n        if coincidence >= min_trig:\n            coincidence_triggers.append((trig_val / coincidence,\n                                         trig_time))\n    # Sort by trigger-value, largest to smallest - remove duplicate detections\n    if coincidence_triggers:\n        coincidence_triggers.sort(key=lambda tup: tup[0], reverse=True)\n        output = [coincidence_triggers[0]]\n        for coincidence_trigger in coincidence_triggers[1:]:\n            add = True\n            for peak in output:\n                # If the event occurs within the trig_int time then do not add\n                # it, and break out of the inner loop.\n                if abs(coincidence_trigger[1] - peak[1]) < (trig_int *\n                                                            samp_rate):\n                    add = False\n                    break\n            if add:\n                output.append((coincidence_trigger[0],\n                               coincidence_trigger[1]))\n        output.sort(key=lambda tup: tup[1])\n        return output\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _finalise_figure(fig, **kwargs):  # pragma: no cover\n    title = kwargs.get(\"title\") or None\n    show = kwargs.get(\"show\") or False\n    save = kwargs.get(\"save\") or False\n    savefile = kwargs.get(\"savefile\") or \"EQcorrscan_figure.png\"\n    return_fig = kwargs.get(\"return_figure\") or False\n    if title:\n        fig.suptitle(title)\n    if show:\n        fig.show()\n    if save:\n        fig.savefig(savefile)\n        print(\"Saved figure to {0}\".format(savefile))\n    if return_fig:\n        return fig\n    return None", "response": "Internal function to wrap up a figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xcorr_plot(template, image, shift=None, cc=None, cc_vec=None, **kwargs):\n    import matplotlib.pyplot as plt\n    if cc is None or shift is None:\n        if not isinstance(cc_vec, np.ndarray):\n            print('Given cc: %s and shift: %s' % (cc, shift))\n            raise IOError('Must provide either cc_vec, or cc and shift')\n        shift = np.abs(cc_vec).argmax()\n        cc = cc_vec[shift]\n    x = np.arange(len(image))\n    plt.plot(x, image / abs(image).max(), 'k', lw=1.3, label='Image')\n    x = np.arange(len(template)) + shift\n    plt.plot(x, template / abs(template).max(), 'r', lw=1.1, label='Template')\n    plt.title('Shift=%s, Correlation=%s' % (shift, cc))\n    fig = plt.gcf()\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plots a template overlying an image aligned by correlation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef triple_plot(cccsum, cccsum_hist, trace, threshold, **kwargs):\n    import matplotlib.pyplot as plt\n    if len(cccsum) != len(trace.data):\n        print('cccsum is: ' +\n              str(len(cccsum)) + ' trace is: ' + str(len(trace.data)))\n        msg = ' '.join(['cccsum and trace must have the',\n                        'same number of data points'])\n        raise ValueError(msg)\n    df = trace.stats.sampling_rate\n    npts = trace.stats.npts\n    t = np.arange(npts, dtype=np.float32) / (df * 3600)\n    # Generate the subplot for the seismic data\n    ax1 = plt.subplot2grid((2, 5), (0, 0), colspan=4)\n    ax1.plot(t, trace.data, 'k')\n    ax1.axis('tight')\n    ax1.set_ylim([-15 * np.mean(np.abs(trace.data)),\n                  15 * np.mean(np.abs(trace.data))])\n    # Generate the subplot for the correlation sum data\n    ax2 = plt.subplot2grid((2, 5), (1, 0), colspan=4, sharex=ax1)\n    # Plot the threshold values\n    ax2.plot([min(t), max(t)], [threshold, threshold], color='r', lw=1,\n             label=\"Threshold\")\n    ax2.plot([min(t), max(t)], [-threshold, -threshold], color='r', lw=1)\n    ax2.plot(t, cccsum, 'k')\n    ax2.axis('tight')\n    ax2.set_ylim([-1.7 * threshold, 1.7 * threshold])\n    ax2.set_xlabel(\"Time after %s [hr]\" % trace.stats.starttime.isoformat())\n    # ax2.legend()\n    # Generate a small subplot for the histogram of the cccsum data\n    ax3 = plt.subplot2grid((2, 5), (1, 4), sharey=ax2)\n    ax3.hist(cccsum_hist, 200, normed=1, histtype='stepfilled',\n             orientation='horizontal', color='black')\n    ax3.set_ylim([-5, 5])\n    fig = plt.gcf()\n    fig.suptitle(trace.id)\n    fig.canvas.draw()\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plots a seismogram cross - channel cross - correlation sum and histogram."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef peaks_plot(data, starttime, samp_rate, peaks=[(0, 0)], **kwargs):\n    import matplotlib.pyplot as plt\n    npts = len(data)\n    t = np.arange(npts, dtype=np.float32) / (samp_rate * 3600)\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax1.plot(t, data, 'k')\n    ax1.scatter(peaks[0][1] / (samp_rate * 3600), abs(peaks[0][0]),\n                color='r', label='Peaks')\n    for peak in peaks:\n        ax1.scatter(peak[1] / (samp_rate * 3600), abs(peak[0]), color='r')\n    ax1.legend()\n    ax1.set_xlabel(\"Time after %s [hr]\" % starttime.isoformat())\n    ax1.axis('tight')\n    fig.suptitle('Peaks')\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plot peaks to check that the peak finding routine is running correctly."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cumulative_detections(dates=None, template_names=None, detections=None,\n                          plot_grouped=False, group_name=None, rate=False,\n                          plot_legend=True, ax=None, **kwargs):\n    \"\"\"\n    Plot cumulative detections or detection rate in time.\n\n    Simple plotting function to take a list of either datetime objects or\n    :class:`eqcorrscan.core.match_filter.Detection` objects and plot\n    a cumulative detections list.  Can take dates as a list of lists and will\n    plot each list separately, e.g. if you have dates from more than one\n    template it will overlay them in different colours.\n\n    :type dates: list\n    :param dates: Must be a list of lists of datetime.datetime objects\n    :type template_names: list\n    :param template_names: List of the template names in order of the dates\n    :type detections: list\n    :param detections: List of :class:`eqcorrscan.core.match_filter.Detection`\n    :type plot_grouped: bool\n    :param plot_grouped:\n        Plot detections for each template individually, or group them all\n        together - set to False (plot template detections individually) by\n        default.\n    :type rate: bool\n    :param rate:\n        Whether or not to plot the rate of detection per day. Only works for\n        plot_grouped=True\n    :type plot_legend: bool\n    :param plot_legend:\n        Specify whether to plot legend of template names. Defaults to True.\n\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. note::\n        Can either take lists of\n        :class:`eqcorrscan.core.match_filter.Detection` objects directly, or\n        two lists of dates and template names - either/or, not both.\n\n    .. rubric:: Example\n\n    >>> import datetime as dt\n    >>> import numpy as np\n    >>> from eqcorrscan.utils.plotting import cumulative_detections\n    >>> dates = []\n    >>> for i in range(3):\n    ...     dates.append([dt.datetime(2012, 3, 26) + dt.timedelta(n)\n    ...                   for n in np.random.randn(100)])\n    >>> cumulative_detections(dates, ['a', 'b', 'c'],\n    ...                       show=True) # doctest: +SKIP\n\n    .. plot::\n\n        import datetime as dt\n        import numpy as np\n        from eqcorrscan.utils.plotting import cumulative_detections\n        dates = []\n        for i in range(3):\n            dates.append([dt.datetime(2012, 3, 26) + dt.timedelta(n)\n                          for n in np.random.randn(100)])\n        cumulative_detections(dates, ['a', 'b', 'c'], show=True)\n\n    .. rubric:: Example 2: Rate plotting\n\n    >>> import datetime as dt\n    >>> import numpy as np\n    >>> from eqcorrscan.utils.plotting import cumulative_detections\n    >>> dates = []\n    >>> for i in range(3):\n    ...     dates.append([dt.datetime(2012, 3, 26) + dt.timedelta(n)\n    ...                   for n in np.random.randn(100)])\n    >>> cumulative_detections(dates, ['a', 'b', 'c'], plot_grouped=True,\n    ...                       rate=True, show=True) # doctest: +SKIP\n\n    .. plot::\n\n        import datetime as dt\n        import numpy as np\n        from eqcorrscan.utils.plotting import cumulative_detections\n        dates = []\n        for i in range(3):\n            dates.append([dt.datetime(2012, 3, 26) + dt.timedelta(n)\n                          for n in np.random.randn(100)])\n        cumulative_detections(dates, ['a', 'b', 'c'], plot_grouped=True,\n                              rate=True, show=True)\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from eqcorrscan.core.match_filter import Detection\n    # Set up a default series of parameters for lines\n    colors = cycle(['red', 'green', 'blue', 'cyan', 'magenta', 'yellow',\n                    'black', 'firebrick', 'purple', 'darkgoldenrod', 'gray'])\n    linestyles = cycle(['-', '-.', '--', ':'])\n    # Check that dates is a list of lists\n    if not detections:\n        if type(dates[0]) != list:\n            dates = [dates]\n    else:\n        dates = []\n        template_names = []\n        for detection in detections:\n            if not type(detection) == Detection:\n                raise IOError(\n                    'detection not of type: eqcorrscan.core.match_filter'\n                    '.Detection')\n            dates.append(detection.detect_time.datetime)\n            template_names.append(detection.template_name)\n        _dates = []\n        _template_names = []\n        for template_name in list(set(template_names)):\n            _template_names.append(template_name)\n            _dates.append([date for i, date in enumerate(dates)\n                           if template_names[i] == template_name])\n        dates = _dates\n        template_names = _template_names\n    if plot_grouped:\n        _dates = []\n        for template_dates in dates:\n            _dates += template_dates\n        dates = [_dates]\n        if group_name:\n            template_names = group_name\n        else:\n            template_names = ['All templates']\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n    else:\n        fig = ax.figure()\n    # Make sure not to pad at edges\n    ax.margins(0, 0)\n    min_date = min([min(_d) for _d in dates])\n    max_date = max([max(_d) for _d in dates])\n    for k, template_dates in enumerate(dates):\n        template_dates.sort()\n        plot_dates = deepcopy(template_dates)\n        plot_dates.insert(0, min_date)\n        plot_dates.insert(-1, template_dates[-1])\n        color = next(colors)\n        if color == 'red':\n            linestyle = next(linestyles)\n        counts = np.arange(-1, len(template_dates) + 1)\n        if rate:\n            if not plot_grouped:\n                msg = 'Plotting rate only implemented for plot_grouped=True'\n                raise NotImplementedError(msg)\n            if 31 < (max_date - min_date).days < 365:\n                bins = (max_date - min_date).days\n                ax.set_ylabel('Detections per day')\n            elif (max_date - min_date).days <= 31:\n                bins = (max_date - min_date).days * 4\n                ax.set_ylabel('Detections per 6 hour bin')\n            else:\n                bins = (max_date - min_date).days // 7\n                ax.set_ylabel('Detections per week')\n            if len(plot_dates) <= 10:\n                bins = 1\n            ax.hist(mdates.date2num(plot_dates), bins=bins,\n                    label='Rate of detections', color='darkgrey',\n                    alpha=0.5)\n        else:\n            ax.plot(plot_dates, counts, linestyle,\n                    color=color, label=template_names[k],\n                    linewidth=2.0, drawstyle='steps')\n            ax.set_ylabel('Cumulative detections')\n    ax.set_xlabel('Date')\n    # Set formatters for x-labels\n    mins = mdates.MinuteLocator()\n    max_date = dates[0][0]\n    min_date = max_date\n    for date_list in dates:\n        if max(date_list) > max_date:\n            max_date = max(date_list)\n        if min(date_list) < min_date:\n            min_date = min(date_list)\n    timedif = max_date - min_date\n    if 10800 <= timedif.total_seconds() <= 25200:\n        hours = mdates.MinuteLocator(byminute=[0, 30])\n        mins = mdates.MinuteLocator(byminute=np.arange(0, 60, 10))\n    elif 7200 <= timedif.total_seconds() < 10800:\n        hours = mdates.MinuteLocator(byminute=[0, 15, 30, 45])\n        mins = mdates.MinuteLocator(byminute=np.arange(0, 60, 5))\n    elif timedif.total_seconds() <= 1200:\n        hours = mdates.MinuteLocator(byminute=np.arange(0, 60, 2))\n        mins = mdates.MinuteLocator(byminute=np.arange(0, 60, 0.5))\n    elif 25200 < timedif.total_seconds() <= 86400:\n        hours = mdates.HourLocator(byhour=np.arange(0, 24, 3))\n        mins = mdates.HourLocator(byhour=np.arange(0, 24, 1))\n    elif 86400 < timedif.total_seconds() <= 172800:\n        hours = mdates.HourLocator(byhour=np.arange(0, 24, 6))\n        mins = mdates.HourLocator(byhour=np.arange(0, 24, 1))\n    elif timedif.total_seconds() > 172800:\n        hours = mdates.AutoDateLocator()\n        mins = mdates.HourLocator(byhour=np.arange(0, 24, 3))\n    else:\n        hours = mdates.MinuteLocator(byminute=np.arange(0, 60, 5))\n    # Minor locator overruns maxticks for ~year-long datasets\n    if timedif.total_seconds() < 172800:\n        ax.xaxis.set_minor_locator(mins)\n        hrFMT = mdates.DateFormatter('%Y/%m/%d %H:%M:%S')\n    else:\n        hrFMT = mdates.DateFormatter('%Y/%m/%d')\n    ax.xaxis.set_major_locator(hours)\n    ax.xaxis.set_major_formatter(hrFMT)\n    fig.autofmt_xdate()\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=15)\n    if not rate:\n        ax.set_ylim([0, max([len(_d) for _d in dates])])\n    if plot_legend:\n        if ax.legend() is not None:\n            leg = ax.legend(loc=2, prop={'size': 8}, ncol=2)\n            leg.get_frame().set_alpha(0.5)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plots the cumulative detections of a list of template names and detections."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot in a series of grid points in 3D.", "response": "def threeD_gridplot(nodes, **kwargs):\n    \"\"\"Plot in a series of grid points in 3D.\n\n    :type nodes: list\n    :param nodes: List of tuples of the form (lat, long, depth)\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from eqcorrscan.utils.plotting import threeD_gridplot\n    >>> nodes = [(-43.5, 170.4, 4), (-43.3, 170.8, 12), (-43.4, 170.3, 8)]\n    >>> threeD_gridplot(nodes=nodes)  # doctest: +SKIP\n\n    .. plot::\n\n        from eqcorrscan.utils.plotting import threeD_gridplot\n        nodes = [(-43.5, 170.4, 4), (-43.3, 170.8, 12), (-43.4, 170.3, 8)]\n        threeD_gridplot(nodes=nodes)\n    \"\"\"\n    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n    import matplotlib.pyplot as plt\n    lats = []\n    longs = []\n    depths = []\n    for node in nodes:\n        lats.append(float(node[0]))\n        longs.append(float(node[1]))\n        depths.append(float(node[2]))\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(lats, longs, depths)\n    ax.set_ylabel(\"Latitude (deg)\")\n    ax.set_xlabel(\"Longitude (deg)\")\n    ax.set_zlabel(\"Depth(km)\")\n    ax.get_xaxis().get_major_formatter().set_scientific(False)\n    ax.get_yaxis().get_major_formatter().set_scientific(False)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots data from a single channel for multiple streams.", "response": "def multi_event_singlechan(streams, catalog, station, channel,\n                           clip=10.0, pre_pick=2.0,\n                           freqmin=False, freqmax=False, realign=False,\n                           cut=(-3.0, 5.0), PWS=False, **kwargs):\n    \"\"\"\n    Plot data from a single channel for multiple events.\n\n    Data will be aligned by their pick-time given in the appropriate picks.\n    Requires an individual stream for each event you want to plot,\n    events are stored in the :class:`obspy.core.event.Catalog` object, and\n    there must be picks present for the streams you wish to plot.  Events will\n    be aligned if `realign=True`, in this case the traces will be aligned\n    using the window defined by `cut`.\n\n    :type streams: list\n    :param streams:\n        List of the :class:`obspy.core.stream.Stream` objects to use, can\n        contain more traces than you plan on plotting (e.g. from more channels)\n        - must be in the same order as events in catalog.\n    :type catalog: obspy.core.event.Catalog\n    :param catalog: Catalog of events, one for each stream.\n    :type station: str\n    :param station: Station to plot.\n    :type channel: str\n    :param channel: Channel to plot.\n    :type clip: float\n    :param clip: Length in seconds to plot, defaults to 10.0\n    :type pre_pick: float\n    :param pre_pick: Length in seconds to extract and plot before the pick, \\\n        defaults to 2.0\n    :type freqmin: float\n    :param freqmin: Low cut for bandpass in Hz\n    :type freqmax: float\n    :param freqmax: High cut for bandpass in Hz\n    :type realign: bool\n    :param realign:\n        To compute best alignment based on correlation with the stack or not.\n    :type cut: tuple\n    :param cut: tuple of start and end times for cut in seconds from the \\\n        pick, used for alignment.  Will only use this window to align the \\\n        traces.\n    :type PWS: bool\n    :param PWS: compute Phase Weighted Stack, if False, will compute linear \\\n        stack for alignment.\n\n    :returns: Aligned and cut :class:`obspy.core.trace.Trace`\n    :rtype: list\n    :returns:\n        New picks in based on alignment (if alignment is performed, if not\n        will return the same as input)\n    :rtype: :class:`obspy.core.event.Catalog`\n    :returns: Figure object for further editing\n    :rtype: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from obspy import read, Catalog, read_events\n    >>> from obspy.io.nordic.core import readwavename\n    >>> from eqcorrscan.utils.plotting import multi_event_singlechan\n    >>> import glob\n    >>> sfiles = glob.glob('eqcorrscan/tests/test_data/REA/TEST_/*.S??????')\n    >>> catalog = Catalog()\n    >>> streams = []\n    >>> for sfile in sfiles:\n    ...     catalog += read_events(sfile)\n    ...     wavfile = readwavename(sfile)[0]\n    ...     stream_path = 'eqcorrscan/tests/test_data/WAV/TEST_/' + wavfile\n    ...     stream = read(stream_path)\n    ...     # Annoying coping with seisan 2 letter channels\n    ...     for tr in stream:\n    ...         tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n    ...     streams.append(stream)\n    >>> multi_event_singlechan(streams=streams, catalog=catalog,\n    ...                        station='GCSZ', channel='EZ') # doctest: +SKIP\n\n    .. image:: ../../plots/multi_event_singlechan.png\n    \"\"\"\n    import matplotlib.pyplot as plt\n    # Work out how many picks we should have...\n    short_cat = Catalog()\n    short_streams = []\n    for i, event in enumerate(catalog):\n        event_stachans = [(pick.waveform_id.station_code,\n                           pick.waveform_id.channel_code)\n                          for pick in event.picks]\n        if (station, channel) in event_stachans:\n            short_cat.append(event)\n            short_streams.append(streams[i])\n    if len(short_cat) == 0:\n        raise IOError('No picks for ' + station + ' ' + channel)\n    traces = []\n    al_traces = []\n    al_picks = []\n    if isinstance(short_streams, Stream):\n        short_streams = [short_streams]\n    st_list = deepcopy(short_streams)\n    print(short_cat)\n    for i, event in enumerate(short_cat):\n        # Extract the appropriate pick\n        _pick = [pick for pick in event.picks if\n                 pick.waveform_id.station_code == station and\n                 pick.waveform_id.channel_code == channel]\n        if len(_pick) == 0:\n            print('No pick for channel')\n            continue\n        else:\n            _pick = _pick[0]\n        if st_list[i].select(station=station, channel=channel):\n            tr = st_list[i].select(station=station, channel=channel)[0]\n        else:\n            print('No data for ' + _pick.waveform_id.station_code)\n            continue\n        tr.detrend('linear')\n        if freqmin:\n            tr.filter('bandpass', freqmin=freqmin, freqmax=freqmax)\n        if realign:\n            tr_cut = tr.copy()\n            tr_cut.trim(_pick.time + cut[0],\n                        _pick.time + cut[1],\n                        nearest_sample=False)\n            if len(tr_cut.data) <= (0.5 * (cut[1] - cut[0]) *\n                                    tr_cut.stats.sampling_rate):\n                msg = ''.join(['Not enough in the trace for ',\n                               tr.stats.station,\n                               '.', tr.stats.channel, '\\n',\n                               'Suggest removing pick from event at time ',\n                               str(_pick.time)])\n                warnings.warn(msg)\n            else:\n                al_traces.append(tr_cut)\n                al_picks.append(_pick)\n        else:\n            tr.trim(_pick.time - pre_pick,\n                    _pick.time + clip - pre_pick,\n                    nearest_sample=False)\n        if len(tr.data) == 0:\n            msg = ''.join(['No data in the trace for ', tr.stats.station,\n                           '.', tr.stats.channel, '\\n',\n                           'Suggest removing pick from event at time ',\n                           str(event.picks[0].time)])\n            warnings.warn(msg)\n            continue\n        traces.append(tr)\n    if realign:\n        shift_len = int(0.25 * (cut[1] - cut[0]) *\n                        al_traces[0].stats.sampling_rate)\n        shifts = align_traces(al_traces, shift_len)[0]\n        for i in range(len(shifts)):\n            print('Shifting by ' + str(shifts[i]) + ' seconds')\n            _pick.time -= shifts[i]\n            traces[i].trim(al_picks[i].time - pre_pick,\n                           al_picks[i].time + clip - pre_pick,\n                           nearest_sample=True)\n    # We now have a list of traces\n    if PWS:\n        stack = 'PWS'\n    else:\n        stack = 'linstack'\n    for tr in traces:\n        print(tr)\n    fig = multi_trace_plot(\n        traces=traces, corr=True, stack=stack, show=False, return_figure=True)\n    plt.subplots_adjust(hspace=0)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return traces, short_cat, fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots multiple traces on the same plot.", "response": "def multi_trace_plot(traces, corr=True, stack='linstack', size=(7, 12),\n                     **kwargs):\n    \"\"\"\n    Plot multiple traces (usually from the same station) on the same plot.\n\n    Differs somewhat to obspy's stream.plot in that only relative time within\n    traces is worried about, it will not merge traces together.\n\n    :type traces: list\n    :param traces: List of obspy.core.Trace\n    :type corr: bool\n    :param corr:\n        To calculate the correlation or not, if True, will add this to the\n        axes\n    :type stack: str\n    :param stack:\n        To plot the stack as the first trace or not, select type of\n         stack: 'linstack' or 'PWS', or None.\n    :type size: tuple\n    :param size: Size of figure.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from eqcorrscan.core.match_filter import normxcorr2\n    n_axes = len(traces)\n    if stack in ['linstack', 'PWS']:\n        n_axes += 1\n    fig, axes = plt.subplots(n_axes, 1, sharex=True, figsize=size)\n    if len(traces) > 1:\n        axes = axes.ravel()\n    traces = [(trace, trace.stats.starttime.datetime) for trace in traces]\n    traces.sort(key=lambda tup: tup[1])\n    traces = [trace[0] for trace in traces]\n    # Plot the traces\n    for i, tr in enumerate(traces):\n        y = tr.data\n        x = np.arange(len(y))\n        x = x / tr.stats.sampling_rate  # convert to seconds\n        if not stack:\n            ind = i\n        else:\n            ind = i + 1\n        axes[ind].plot(x, y, 'k', linewidth=1.1)\n        axes[ind].yaxis.set_ticks([])\n    traces = [Stream(trace) for trace in traces]\n    if stack == 'PWS':\n        stacked = PWS_stack(traces)\n    elif stack == 'linstack':\n        stacked = linstack(traces)\n    if stack in ['linstack', 'PWS']:\n        tr = stacked[0]\n        y = tr.data\n        x = np.arange(len(y))\n        x = x / tr.stats.sampling_rate\n        axes[0].plot(x, y, 'r', linewidth=2.0)\n        axes[0].set_ylabel('Stack', rotation=0)\n        axes[0].yaxis.set_ticks([])\n    for i, slave in enumerate(traces):\n        if corr:\n            cc = normxcorr2(tr.data, slave[0].data)\n        if not stack:\n            ind = i\n        else:\n            ind = i + 1\n        if corr:\n            axes[ind].set_ylabel('cc=' + str(round(np.max(cc), 2)), rotation=0)\n        axes[ind].text(0.9, 0.15, str(round(np.max(slave[0].data))),\n                       bbox=dict(facecolor='white', alpha=0.95),\n                       transform=axes[ind].transAxes)\n        axes[ind].text(0.7, 0.85, slave[0].stats.starttime.datetime.\n                       strftime('%Y/%m/%d %H:%M:%S'),\n                       bbox=dict(facecolor='white', alpha=0.95),\n                       transform=axes[ind].transAxes)\n    axes[-1].set_xlabel('Time (s)')\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting a single stream of data at a given time at a given template at a given number of times.", "response": "def detection_multiplot(stream, template, times, streamcolour='k',\n                        templatecolour='r', size=(10.5, 7.5), **kwargs):\n    \"\"\"\n    Plot a stream of data with a template on top of it at detection times.\n\n    :type stream: obspy.core.stream.Stream\n    :param stream: Stream of data to be plotted as the background.\n    :type template: obspy.core.stream.Stream\n    :param template: Template to be plotted on top of the base stream.\n    :type times: list\n    :param times: list of detection times, one for each event\n    :type streamcolour: str\n    :param streamcolour: String of matplotlib colour types for the stream\n    :type templatecolour: str\n    :param templatecolour: Colour to plot the template in.\n    :type size: tuple\n    :param size: Figure size.\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from obspy import read, read_events\n    >>> import os\n    >>> from eqcorrscan.core import template_gen\n    >>> from eqcorrscan.utils.plotting import detection_multiplot\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>>\n    >>> test_file = os.path.join(TEST_PATH, 'REA',\n    ...                          'TEST_', '01-0411-15L.S201309')\n    >>> test_wavefile = os.path.join(\n    ...     TEST_PATH, 'WAV', 'TEST_', '2013-09-01-0410-35.DFDPC_024_00')\n    >>> event = read_events(test_file)[0]\n    >>> st = read(test_wavefile)\n    >>> st = st.filter('bandpass', freqmin=2.0, freqmax=15.0)\n    >>> for tr in st:\n    ...     tr = tr.trim(tr.stats.starttime + 30, tr.stats.endtime - 30)\n    ...     # Hack around seisan 2-letter channel naming\n    ...     tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n    >>> template = template_gen._template_gen(event.picks, st, 2)\n    >>> times = [min([pk.time -0.05 for pk in event.picks])]\n    >>> detection_multiplot(stream=st, template=template,\n    ...                     times=times) # doctest: +SKIP\n\n    .. plot::\n\n        from obspy import read, read_events\n        import os\n        from eqcorrscan.core import template_gen\n        from eqcorrscan.utils.plotting import detection_multiplot\n        test_file = os.path.realpath('../../..') + \\\n            '/tests/test_data/REA/TEST_/01-0411-15L.S201309'\n        test_wavefile = os.path.realpath('../../..') +\\\n            '/tests/test_data/WAV/TEST_/' +\\\n            '2013-09-01-0410-35.DFDPC_024_00'\n        event = read_events(test_file)[0]\n        st = read(test_wavefile)\n        st.filter('bandpass', freqmin=2.0, freqmax=15.0)\n        for tr in st:\n            tr.trim(tr.stats.starttime + 30, tr.stats.endtime - 30)\n            tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n        template = template_gen._template_gen(event.picks, st, 2)\n        times = [min([pk.time -0.05 for pk in event.picks])]\n        detection_multiplot(stream=st, template=template,\n                            times=times)\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    # Only take traces that match in both accounting for streams shorter than\n    # templates\n    template_stachans = [(tr.stats.station, tr.stats.channel)\n                         for tr in template]\n    stream_stachans = [(tr.stats.station, tr.stats.channel)\n                       for tr in stream]\n    temp = Stream([tr for tr in template\n                   if (tr.stats.station,\n                       tr.stats.channel) in stream_stachans])\n    st = Stream([tr for tr in stream\n                 if (tr.stats.station,\n                     tr.stats.channel) in template_stachans])\n    ntraces = len(temp)\n    fig, axes = plt.subplots(ntraces, 1, sharex=True, figsize=size)\n    if len(temp) > 1:\n        axes = axes.ravel()\n    mintime = min([tr.stats.starttime for tr in temp])\n    temp.sort(keys=['starttime'])\n    for i, template_tr in enumerate(temp):\n        if len(temp) > 1:\n            axis = axes[i]\n        else:\n            axis = axes\n        image = st.select(station=template_tr.stats.station,\n                          channel='*' + template_tr.stats.channel[-1])\n        if not image:\n            msg = ' '.join(['No data for', template_tr.stats.station,\n                            template_tr.stats.channel])\n            print(msg)\n            continue\n        image = image.merge()[0]\n        # Downsample if needed\n        if image.stats.sampling_rate > 20 and image.stats.npts > 10000:\n            image.decimate(int(image.stats.sampling_rate // 20))\n            template_tr.decimate(int(template_tr.stats.sampling_rate // 20))\n        # Get a list of datetime objects\n        image_times = [image.stats.starttime.datetime +\n                       dt.timedelta((j * image.stats.delta) / 86400)\n                       for j in range(len(image.data))]\n        axis.plot(image_times, image.data / max(image.data),\n                  streamcolour, linewidth=1.2)\n        for time in times:\n            lagged_time = UTCDateTime(time) + (template_tr.stats.starttime -\n                                               mintime)\n            lagged_time = lagged_time.datetime\n            template_times = [lagged_time +\n                              dt.timedelta((j * template_tr.stats.delta) /\n                                           86400)\n                              for j in range(len(template_tr.data))]\n            # Normalize the template according to the data detected in\n            try:\n                normalizer = max(image.data[int((template_times[0] -\n                                                image_times[0]).\n                                                total_seconds() /\n                                                image.stats.delta):\n                                            int((template_times[-1] -\n                                                 image_times[0]).\n                                                total_seconds() /\n                                                image.stats.delta)] /\n                                 max(image.data))\n            except ValueError:\n                # Occurs when there is no data in the image at this time...\n                normalizer = max(image.data)\n            normalizer /= max(template_tr.data)\n            axis.plot(template_times,\n                      template_tr.data * normalizer,\n                      templatecolour, linewidth=1.2)\n        ylab = '.'.join([template_tr.stats.station,\n                         template_tr.stats.channel])\n        axis.set_ylabel(ylab, rotation=0,\n                        horizontalalignment='right')\n    if len(template) > 1:\n        axes[len(axes) - 1].set_xlabel('Time')\n    else:\n        axis.set_xlabel('Time')\n    plt.subplots_adjust(hspace=0, left=0.175, right=0.95, bottom=0.07)\n    plt.xticks(rotation=10)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interev_mag(times, mags, size=(10.5, 7.5), **kwargs):\n    import matplotlib.pyplot as plt\n    info = [(times[i], mags[i]) for i in range(len(times))]\n    info.sort(key=lambda tup: tup[0])\n    times = [x[0] for x in info]\n    mags = [x[1] for x in info]\n    # Make two subplots next to each other of time before and time after\n    fig, axes = plt.subplots(1, 2, sharey=True, figsize=size)\n    axes = axes.ravel()\n    pre_times = []\n    post_times = []\n    for i in range(len(times)):\n        if i > 0:\n            pre_times.append((times[i] - times[i - 1]) / 60)\n        if i < len(times) - 1:\n            post_times.append((times[i + 1] - times[i]) / 60)\n    axes[0].scatter(pre_times, mags[1:])\n    axes[0].set_title('Pre-event times')\n    axes[0].set_ylabel('Magnitude')\n    axes[0].set_xlabel('Time (Minutes)')\n    plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=30)\n    axes[1].scatter(pre_times, mags[:-1])\n    axes[1].set_title('Post-event times')\n    axes[1].set_xlabel('Time (Minutes)')\n    axes[0].autoscale(enable=True, tight=True)\n    axes[1].autoscale(enable=True, tight=True)\n    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=30)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plot inter - event times against magnitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obspy_3d_plot(inventory, catalog, size=(10.5, 7.5), **kwargs):\n    nodes = []\n    for ev in catalog:\n        nodes.append((ev.preferred_origin().latitude,\n                      ev.preferred_origin().longitude,\n                      ev.preferred_origin().depth / 1000))\n    # Will plot borehole instruments at elevation - depth if provided\n    all_stas = []\n    for net in inventory:\n        for sta in net:\n            if len(sta.channels) > 0:\n                all_stas.append((sta.latitude, sta.longitude,\n                                 sta.elevation / 1000 -\n                                 sta.channels[0].depth / 1000))\n            else:\n                warnings.warn('No channel information attached, '\n                              'setting elevation without depth')\n                all_stas.append((sta.latitude, sta.longitude,\n                                 sta.elevation / 1000))\n    fig = threeD_seismplot(\n        stations=all_stas, nodes=nodes, size=size, **kwargs)\n    return fig", "response": "Plots an obspy inventory and obspy catalog classes in three dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots seismicity and stations in a 3D movable zoomable space.", "response": "def threeD_seismplot(stations, nodes, size=(10.5, 7.5), **kwargs):\n    \"\"\"\n    Plot seismicity and stations in a 3D, movable, zoomable space.\n\n    Uses matplotlibs Axes3D package.\n\n    :type stations: list\n    :param stations: list of one tuple per station of (lat, long, elevation), \\\n        with up positive.\n    :type nodes: list\n    :param nodes: list of one tuple per event of (lat, long, depth) with down \\\n        positive.\n    :type size: tuple\n    :param size: Size of figure in inches.\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. Note::\n        See :func:`eqcorrscan.utils.plotting.obspy_3d_plot` for example output.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    stalats, stalongs, staelevs = zip(*stations)\n    evlats, evlongs, evdepths = zip(*nodes)\n    # Cope with +/-180 latitudes...\n    _evlongs = []\n    for evlong in evlongs:\n        if evlong < 0:\n            evlong = float(evlong)\n            evlong += 360\n        _evlongs.append(evlong)\n    evlongs = _evlongs\n    _stalongs = []\n    for stalong in stalongs:\n        if stalong < 0:\n            stalong = float(stalong)\n            stalong += 360\n        _stalongs.append(stalong)\n    stalongs = _stalongs\n    evdepths = [-1 * depth for depth in evdepths]\n    fig = plt.figure(figsize=size)\n    ax = Axes3D(fig)\n    ax.scatter(evlats, evlongs, evdepths, marker=\"x\", c=\"k\",\n               label='Hypocenters')\n    ax.scatter(stalats, stalongs, staelevs, marker=\"v\", c=\"r\",\n               label='Stations')\n    ax.set_ylabel(\"Longitude (deg)\")\n    ax.set_xlabel(\"Latitude (deg)\")\n    ax.set_zlabel(\"Elevation (km)\")\n    ax.get_xaxis().get_major_formatter().set_scientific(False)\n    ax.get_yaxis().get_major_formatter().set_scientific(False)\n    plt.legend()\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef noise_plot(signal, noise, normalise=False, **kwargs):\n    import matplotlib.pyplot as plt\n\n    # Work out how many traces we can plot\n    n_traces = 0\n    for tr in signal:\n        try:\n            noise.select(id=tr.id)[0]\n        except IndexError:  # pragma: no cover\n            continue\n        n_traces += 1\n\n    fig, axes = plt.subplots(n_traces, 2, sharex=True)\n    if len(signal) > 1:\n        axes = axes.ravel()\n    i = 0\n    lines = []\n    labels = []\n    for tr in signal:\n        try:\n            noise_tr = noise.select(id=tr.id)[0]\n        except IndexError:  # pragma: no cover\n            continue\n        ax1 = axes[i]\n        ax2 = axes[i + 1]\n        fft_len = fftpack.next_fast_len(\n            max(noise_tr.stats.npts, tr.stats.npts))\n        if not normalise:\n            signal_fft = fftpack.rfft(tr.data, fft_len)\n            noise_fft = fftpack.rfft(noise_tr.data, fft_len)\n        else:\n            signal_fft = fftpack.rfft(tr.data / max(tr.data), fft_len)\n            noise_fft = fftpack.rfft(\n                noise_tr.data / max(noise_tr.data), fft_len)\n        frequencies = np.linspace(0, 1 / (2 * tr.stats.delta), fft_len // 2)\n        noise_line, = ax1.semilogy(\n            frequencies, 2.0 / fft_len * np.abs(noise_fft[0: fft_len // 2]),\n            'k', label=\"noise\")\n        signal_line, = ax1.semilogy(\n            frequencies, 2.0 / fft_len * np.abs(signal_fft[0: fft_len // 2]),\n            'r', label=\"signal\")\n        if \"signal\" not in labels:\n            labels.append(\"signal\")\n            lines.append(signal_line)\n        if \"noise\" not in labels:\n            labels.append(\"noise\")\n            lines.append(noise_line)\n        ax1.set_ylabel(tr.id, rotation=0, horizontalalignment='right')\n        ax2.plot(\n            frequencies,\n            (2.0 / fft_len * np.abs(signal_fft[0: fft_len // 2])) -\n            (2.0 / fft_len * np.abs(noise_fft[0: fft_len // 2])), 'k')\n        ax2.yaxis.tick_right()\n        ax2.set_ylim(bottom=0)\n        i += 2\n    axes[-1].set_xlabel(\"Frequency (Hz)\")\n    axes[-2].set_xlabel(\"Frequency (Hz)\")\n    axes[0].set_title(\"Spectra\")\n    axes[1].set_title(\"Signal - noise\")\n    plt.figlegend(lines, labels, 'upper left')\n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plot signal and noise fourier transforms and the difference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pretty_template_plot(template, size=(10.5, 7.5), background=False,\n                         picks=False, **kwargs):\n    \"\"\"\n    Plot of a single template, possibly within background data.\n\n    :type template: obspy.core.stream.Stream\n    :param template: Template stream to plot\n    :type size: tuple\n    :param size: tuple of plot size\n    :type background: obspy.core.stream.stream\n    :param background: Stream to plot the template within.\n    :type picks: list\n    :param picks: List of :class:`obspy.core.event.origin.Pick` picks.\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from obspy import read, read_events\n    >>> import os\n    >>> from eqcorrscan.core import template_gen\n    >>> from eqcorrscan.utils.plotting import pretty_template_plot\n    >>> # Get the path to the test data\n    >>> import eqcorrscan\n    >>> import os\n    >>> TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n    >>>\n    >>> test_file = os.path.join(TEST_PATH, 'REA', 'TEST_',\n    ...                          '01-0411-15L.S201309')\n    >>> test_wavefile = os.path.join(\n    ...     TEST_PATH, 'WAV', 'TEST_', '2013-09-01-0410-35.DFDPC_024_00')\n    >>> event = read_events(test_file)[0]\n    >>> st = read(test_wavefile)\n    >>> st = st.filter('bandpass', freqmin=2.0, freqmax=15.0)\n    >>> for tr in st:\n    ...     tr = tr.trim(tr.stats.starttime + 30, tr.stats.endtime - 30)\n    ...     # Hack around seisan 2-letter channel naming\n    ...     tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n    >>> template = template_gen._template_gen(event.picks, st, 2)\n    >>> pretty_template_plot(template, background=st, # doctest +SKIP\n    ...                      picks=event.picks) # doctest: +SKIP\n\n    .. plot::\n\n        from obspy import read, read_events\n        from eqcorrscan.core import template_gen\n        from eqcorrscan.utils.plotting import pretty_template_plot\n        import os\n        # Get the path to the test data\n        import eqcorrscan\n        import os\n        TEST_PATH = os.path.dirname(eqcorrscan.__file__) + '/tests/test_data'\n        test_file = os.path.join(\n            TEST_PATH, 'REA', 'TEST_', '01-0411-15L.S201309'\n        test_wavefile = os.path.join(\n            TEST_PATH, 'WAV', 'TEST_', '2013-09-01-0410-35.DFDPC_024_00')\n        event = read_events(test_file)[0]\n        st = read(test_wavefile)\n        st.filter('bandpass', freqmin=2.0, freqmax=15.0)\n        for tr in st:\n            tr.trim(tr.stats.starttime + 30, tr.stats.endtime - 30)\n            tr.stats.channel = tr.stats.channel[0] + tr.stats.channel[-1]\n        template = template_gen._template_gen(event.picks, st, 2)\n        pretty_template_plot(template, background=st,\n                             picks=event.picks)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(len(template), 1, sharex=True, figsize=size)\n    if len(template) > 1:\n        axes = axes.ravel()\n    if not background:\n        mintime = template.sort(['starttime'])[0].stats.starttime\n    else:\n        mintime = background.sort(['starttime'])[0].stats.starttime\n    template.sort(['network', 'station', 'starttime'])\n    lengths = []\n    lines = []\n    labels = []\n    for i, tr in enumerate(template):\n        # Cope with a single channel template case.\n        if len(template) > 1:\n            axis = axes[i]\n        else:\n            axis = axes\n        delay = tr.stats.starttime - mintime\n        y = tr.data\n        x = np.linspace(0, (len(y) - 1) * tr.stats.delta, len(y))\n        x += delay\n        if background:\n            btr = background.select(station=tr.stats.station,\n                                    channel=tr.stats.channel)[0]\n            bdelay = btr.stats.starttime - mintime\n            by = btr.data\n            bx = np.linspace(0, (len(by) - 1) * btr.stats.delta, len(by))\n            bx += bdelay\n            axis.plot(bx, by, 'k', linewidth=1)\n            template_line, = axis.plot(x, y, 'r', linewidth=1.1,\n                                       label='Template')\n            if i == 0:\n                lines.append(template_line)\n                labels.append('Template')\n            lengths.append(max(bx[-1], x[-1]))\n        else:\n            template_line, = axis.plot(x, y, 'k', linewidth=1.1,\n                                       label='Template')\n            if i == 0:\n                lines.append(template_line)\n                labels.append('Template')\n            lengths.append(x[-1])\n        # print(' '.join([tr.stats.station, str(len(x)), str(len(y))]))\n        axis.set_ylabel('.'.join([tr.stats.station, tr.stats.channel]),\n                        rotation=0, horizontalalignment='right')\n        axis.yaxis.set_ticks([])\n        # Plot the picks if they are given\n        if picks:\n            tr_picks = [pick for pick in picks if\n                        pick.waveform_id.station_code == tr.stats.station and\n                        pick.waveform_id.channel_code[0] +\n                        pick.waveform_id.channel_code[-1] ==\n                        tr.stats.channel[0] + tr.stats.channel[-1]]\n            for pick in tr_picks:\n                if not pick.phase_hint:\n                    pcolor = 'k'\n                    label = 'Unknown pick'\n                elif 'P' in pick.phase_hint.upper():\n                    pcolor = 'red'\n                    label = 'P-pick'\n                elif 'S' in pick.phase_hint.upper():\n                    pcolor = 'blue'\n                    label = 'S-pick'\n                else:\n                    pcolor = 'k'\n                    label = 'Unknown pick'\n                pdelay = pick.time - mintime\n                # print(pdelay)\n                line = axis.axvline(x=pdelay, color=pcolor, linewidth=2,\n                                    linestyle='--', label=label)\n                if label not in labels:\n                    lines.append(line)\n                    labels.append(label)\n                # axes[i].plot([pdelay, pdelay], [])\n    axis.set_xlim([0, max(lengths)])\n    if len(template) > 1:\n        axis = axes[len(template) - 1]\n    else:\n        axis = axes\n    axis.set_xlabel('Time (s) from start of template')\n    plt.figlegend(lines, labels, 'upper right')\n    title = kwargs.get(\"title\") or None\n    if title:\n        if len(template) > 1:\n            axes[0].set_title(title)\n        else:\n            axes.set_title(title)\n        kwargs.pop(\"title\")  # Do not give title to _finalise_figure\n    else:\n        plt.subplots_adjust(top=0.98)\n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Pretty plot of a single template."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the network response of the given stream.", "response": "def NR_plot(stream, NR_stream, detections, false_detections=False,\n            size=(18.5, 10), **kwargs):\n    \"\"\"\n    Plot Network response alongside the stream used.\n\n    Highlights detection times in the network response.\n\n    :type stream: obspy.core.stream.Stream\n    :param stream: Stream to plot\n    :type NR_stream: obspy.core.stream.Stream\n    :param NR_stream: Stream for the network response\n    :type detections: list\n    :param detections: List of the detection time as :class:`datetime.datetime`\n    :type false_detections: list\n    :param false_detections:\n        Either False (default) or list of false detection times\n        (:class:`datetime.datetime`).\n    :type size: tuple\n    :param size: Size of figure, default is (18.5, 10)\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. Note::\n        Called by :mod:`eqcorrscan.core.bright_lights`, not a general use\n        plot (hence no example)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots(len(stream) + 1, 1, sharex=True, figsize=size)\n    if len(stream) > 1:\n        axes = axes.ravel()\n    else:\n        return\n    mintime = stream.sort(['starttime'])[0].stats.starttime\n    stream.sort(['network', 'station', 'starttime'])\n    for i, tr in enumerate(stream):\n        delay = tr.stats.starttime - mintime\n        delay *= tr.stats.sampling_rate\n        y = tr.data\n        x = [tr.stats.starttime.datetime + dt.timedelta(\n            seconds=s / tr.stats.sampling_rate) for s in range(len(y))]\n        x = mdates.date2num(x)\n        axes[i].plot(x, y, 'k', linewidth=1.1)\n        axes[i].set_ylabel('.'.join([tr.stats.station, tr.stats.channel]),\n                           rotation=0)\n        axes[i].yaxis.set_ticks([])\n        axes[i].set_xlim(x[0], x[-1])\n    # Plot the network response\n    tr = NR_stream[0]\n    delay = tr.stats.starttime - mintime\n    delay *= tr.stats.sampling_rate\n    y = tr.data\n    x = [tr.stats.starttime.datetime +\n         dt.timedelta(seconds=s / tr.stats.sampling_rate)\n         for s in range(len(y))]\n    x = mdates.date2num(x)\n    axes[-1].plot(x, y, 'k', linewidth=1.1)\n    axes[-1].set_ylabel('.'.join([tr.stats.station, tr.stats.channel]),\n                        rotation=0)\n    axes[-1].yaxis.set_ticks([])\n    axes[-1].set_xlabel('Time')\n    axes[-1].set_xlim(x[0], x[-1])\n    # Plot the detections!\n    ymin, ymax = axes[-1].get_ylim()\n    if false_detections:\n        for detection in false_detections:\n            xd = mdates.date2num(detection)\n            axes[-1].plot((xd, xd), (ymin, ymax), 'k--', linewidth=0.9,\n                          alpha=0.5)\n    for detection in detections:\n        xd = mdates.date2num(detection)\n        axes[-1].plot((xd, xd), (ymin, ymax), 'r--', linewidth=1.1)\n    # Set formatters for x-labels\n    mins = mdates.MinuteLocator()\n    timedif = tr.stats.endtime.datetime - tr.stats.starttime.datetime\n    if timedif.total_seconds() >= 10800 and timedif.total_seconds() <= 25200:\n        hours = mdates.MinuteLocator(byminute=[0, 15, 30, 45])\n    elif timedif.total_seconds() <= 1200:\n        hours = mdates.MinuteLocator(byminute=range(0, 60, 2))\n    elif timedif.total_seconds > 25200 and timedif.total_seconds() <= 172800:\n        hours = mdates.HourLocator(byhour=range(0, 24, 3))\n    elif timedif.total_seconds() > 172800:\n        hours = mdates.DayLocator()\n    else:\n        hours = mdates.MinuteLocator(byminute=range(0, 60, 5))\n    hrFMT = mdates.DateFormatter('%Y/%m/%d %H:%M:%S')\n    axes[-1].xaxis.set_major_locator(hours)\n    axes[-1].xaxis.set_major_formatter(hrFMT)\n    axes[-1].xaxis.set_minor_locator(mins)\n    plt.gcf().autofmt_xdate()\n    axes[-1].fmt_xdata = mdates.DateFormatter('%Y/%m/%d %H:%M:%S')\n    plt.subplots_adjust(hspace=0)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot singular vectors from the SVStreams and the list of stachans.", "response": "def svd_plot(svstreams, svalues, stachans, **kwargs):\n    \"\"\"\n    Plot singular vectors from the :mod:`eqcorrscan.utils.clustering` routines.\n\n    One plot for each channel.\n\n    :type svstreams: list\n    :param svstreams:\n        See :func:`eqcorrscan.utils.clustering.svd_to_stream` - these should be\n        ordered by power, e.g. first singular vector in the first stream.\n    :type svalues: list\n    :param svalues:\n        List of floats of the singular values corresponding to the SVStreams\n    :type stachans: list\n    :param stachans: List of station.channel\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> import glob\n    >>> from eqcorrscan.utils.plotting import svd_plot\n    >>> from eqcorrscan.utils.clustering import svd, svd_to_stream\n    >>> wavefiles = glob.glob('eqcorrscan/tests/test_data/WAV/TEST_/*')\n    >>> streams = [read(w) for w in wavefiles[1:10]]\n    >>> stream_list = []\n    >>> for st in streams:\n    ...     tr = st.select(station='GCSZ', channel='EHZ')\n    ...     tr = tr.detrend('simple').resample(100).filter(\n    ...        'bandpass', freqmin=2, freqmax=8)\n    ...     stream_list.append(tr)\n    >>> uvec, sval, svec, stachans = svd(stream_list=stream_list)\n    >>> svstreams = svd_to_stream(uvectors=uvec, stachans=stachans, k=3,\n    ...                           sampling_rate=100)\n    >>> svd_plot(svstreams=svstreams, svalues=sval,\n    ...          stachans=stachans) # doctest: +SKIP\n\n    .. plot::\n\n        from obspy import read\n        import glob, os\n        from eqcorrscan.utils.plotting import svd_plot\n        from eqcorrscan.utils.clustering import svd, svd_to_stream\n        wavefiles = glob.glob(os.path.realpath('../../..') +\n                             '/tests/test_data/WAV/TEST_/*')\n        streams = [read(w) for w in wavefiles[1:10]]\n        stream_list = []\n        for st in streams:\n            tr = st.select(station='GCSZ', channel='EHZ')\n            st.detrend('simple').resample(100).filter('bandpass', freqmin=5,\n                                                      freqmax=40)\n            stream_list.append(tr)\n        svec, sval, uvec, stachans = svd(stream_list=stream_list)\n        svstreams = svd_to_stream(uvectors=uvec, stachans=stachans, k=3,\n                                  sampling_rate=100)\n        svd_plot(svstreams=svstreams, svalues=sval,\n                 stachans=stachans)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    figures = []\n    for sval, stachan in zip(svalues, stachans):\n        print(stachan)\n        plot_traces = [SVStream.select(station=stachan[0],\n                                       channel=stachan[1])[0]\n                       for SVStream in svstreams]\n        fig, axes = plt.subplots(len(plot_traces), 1, sharex=True)\n        if len(plot_traces) > 1:\n            axes = axes.ravel()\n        for i, tr in enumerate(plot_traces):\n            y = tr.data\n            x = np.linspace(0, len(y) * tr.stats.delta, len(y))\n            axes[i].plot(x, y, 'k', linewidth=1.1)\n            ylab = 'SV %s = %s' % (i + 1, round(sval[i] / len(sval), 2))\n            axes[i].set_ylabel(ylab, rotation=0)\n            axes[i].yaxis.set_ticks([])\n            print(i)\n        axes[-1].set_xlabel('Time (s)')\n        plt.subplots_adjust(hspace=0)\n        fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n        figures.append(fig)\n    return figures"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_synth_real(real_template, synthetic, channels=False, size=(5, 10),\n                    **kwargs):\n    \"\"\"\n    Plot multiple channels of data for real data and synthetic.\n\n    :type real_template: obspy.core.stream.Stream\n    :param real_template: Stream of the real template\n    :type synthetic: obspy.core.stream.Stream\n    :param synthetic: Stream of synthetic template\n    :type channels: list\n    :param channels: List of tuples of (station, channel) to plot, default is \\\n            False, which plots all.\n    :type size: tuple\n    :param size: Plot size.\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    >>> from obspy import read, Stream, Trace\n    >>> from eqcorrscan.utils.synth_seis import seis_sim\n    >>> from eqcorrscan.utils.plotting import plot_synth_real\n    >>> real = read()\n    >>> synth = Stream(Trace(seis_sim(sp=100, flength=200)))\n    >>> synth[0].stats.station = 'RJOB'\n    >>> synth[0].stats.channel = 'EHZ'\n    >>> synth[0].stats.sampling_rate = 100\n    >>> synth = synth.filter('bandpass', freqmin=2, freqmax=8)\n    >>> real = real.select(\n    ...    station='RJOB', channel='EHZ').detrend('simple').filter(\n    ...       'bandpass', freqmin=2, freqmax=8)\n    >>> real = real.trim(\n    ...    starttime=real[0].stats.starttime + 4.9,\n    ...    endtime=real[0].stats.starttime + 6.9).detrend('simple')\n    >>> plot_synth_real(real_template=real, synthetic=synth,\n    ...                 size=(7, 4)) # doctest: +SKIP\n\n    .. plot::\n\n        from eqcorrscan.utils.plotting import plot_synth_real\n        from obspy import read, Stream, Trace\n        from eqcorrscan.utils.synth_seis import seis_sim\n        import os\n        real = read()\n        synth = Stream(Trace(seis_sim(sp=100, flength=200)))\n        synth[0].stats.station = 'RJOB'\n        synth[0].stats.channel = 'EHZ'\n        synth[0].stats.sampling_rate = 100\n        synth.filter('bandpass', freqmin=2, freqmax=8)\n        real = real.select(station='RJOB', channel='EHZ').detrend('simple').\\\n            filter('bandpass', freqmin=2, freqmax=8)\n        real.trim(starttime=real[0].stats.starttime + 4.9,\n                  endtime=real[0].stats.starttime + 6.9).detrend('simple')\n        plot_synth_real(real_template=real, synthetic=synth, size=(7, 4))\n    \"\"\"\n    import matplotlib.pyplot as plt\n    colours = ['k', 'r']\n    labels = ['Real', 'Synthetic']\n    if channels:\n        real = []\n        synth = []\n        for stachan in channels:\n            real.append(real_template.select(station=stachan[0],\n                                             channel=stachan[1]))\n            synth.append(synthetic.select(station=stachan[0],\n                                          channel=stachan[1]))\n        real_template = Stream(real)\n        synthetic = Stream(synth)\n\n    # Extract the station and channels\n    stachans = list(set([(tr.stats.station, tr.stats.channel)\n                         for tr in real_template]))\n    fig, axes = plt.subplots(len(stachans), 1, sharex=True, figsize=size)\n    if len(stachans) > 1:\n        axes = axes.ravel()\n    for i, stachan in enumerate(stachans):\n        if len(stachans) > 1:\n            axis = axes[i]\n        else:\n            axis = axes\n        real_tr = real_template.select(station=stachan[0],\n                                       channel=stachan[1])[0]\n        synth_tr = synthetic.select(station=stachan[0],\n                                    channel=stachan[1])[0]\n        shift, corr = xcorr(real_tr, synth_tr, 2)\n        print('Shifting by: ' + str(shift) + ' samples')\n        if corr < 0:\n            synth_tr.data = synth_tr.data * -1\n            corr = corr * -1\n        if shift < 0:\n            synth_tr.data = synth_tr.data[abs(shift):]\n            real_tr.data = real_tr.data[0:len(synth_tr.data)]\n        elif shift > 0:\n            real_tr.data = real_tr.data[abs(shift):]\n            synth_tr.data = synth_tr.data[0:len(real_tr.data)]\n        for j, tr in enumerate([real_tr, synth_tr]):\n            y = tr.data\n            y = y / float(max(abs(y)))\n            x = np.linspace(0, len(y) * tr.stats.delta, len(y))\n            axis.plot(x, y, colours[j], linewidth=2.0, label=labels[j])\n            axis.get_yaxis().set_ticks([])\n        ylab = stachan[0] + '.' + stachan[1] + ' cc=' + str(round(corr, 2))\n        axis.set_ylabel(ylab, rotation=0)\n    plt.subplots_adjust(hspace=0)\n    # axes[0].legend()\n    if len(stachans) > 1:\n        axes[-1].set_xlabel('Time (s)')\n    else:\n        axes.set_xlabel('Time (s)')\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plots multiple channels of data for real data and synthetic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef freq_mag(magnitudes, completeness, max_mag, binsize=0.2, **kwargs):\n    import matplotlib.pyplot as plt\n    # Ensure magnitudes are sorted\n    magnitudes.sort()\n    # Check that there are no nans or infs\n    if np.isnan(magnitudes).any():\n        warnings.warn('Found nan values, removing them')\n        magnitudes = [mag for mag in magnitudes if not np.isnan(mag)]\n    if np.isinf(magnitudes).any():\n        warnings.warn('Found inf values, removing them')\n        magnitudes = [mag for mag in magnitudes if not np.isinf(mag)]\n    fig, ax1 = plt.subplots()\n    # Set up the bins, the bin-size could be a variables\n    bins = np.arange(int(min(magnitudes) - 1), int(max(magnitudes) + 1),\n                     binsize)\n    n, bins, patches = ax1.hist(magnitudes, bins, facecolor='Black',\n                                alpha=0.5, label='Magnitudes')\n    ax1.set_ylabel('Frequency')\n    ax1.set_ylim([0, max(n) + 0.5 * max(n)])\n    plt.xlabel('Magnitude')\n    # Now make the cumulative density function\n    counts = Counter(magnitudes)\n    cdf = np.zeros(len(counts))\n    mag_steps = np.zeros(len(counts))\n    for i, magnitude in enumerate(sorted(counts.keys(), reverse=True)):\n        mag_steps[i] = magnitude\n        if i > 0:\n            cdf[i] = cdf[i - 1] + counts[magnitude]\n        else:\n            cdf[i] = counts[magnitude]\n    ax2 = ax1.twinx()\n    # ax2.scatter(magnitudes, np.log10(cdf), c='k', marker='+', s=20, lw=2,\n    ax2.scatter(mag_steps, np.log10(cdf), c='k', marker='+', s=20, lw=2,\n                label='Magnitude cumulative density')\n    # Now we want to calculate the b-value and plot the fit\n    x = []\n    y = []\n    for i, magnitude in enumerate(mag_steps):\n        if magnitude >= completeness <= max_mag:\n            x.append(magnitude)\n            y.append(cdf[i])\n    fit = np.polyfit(x, np.log10(y), 1)\n    fit_fn = np.poly1d(fit)\n    ax2.plot(magnitudes, fit_fn(magnitudes), '--k',\n             label='GR trend, b-value = ' + str(abs(fit[0]))[0:4] +\n             '\\n $M_C$ = ' + str(completeness))\n    ax2.set_ylabel('$Log_{10}$ of cumulative density')\n    plt.xlim([min(magnitudes) - 0.1, max(magnitudes) + 0.2])\n    plt.ylim([min(np.log10(cdf)) - 0.5, max(np.log10(cdf)) + 1.0])\n    plt.legend(loc=2)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "This function will plot a frequency - magnitude histogram and cumulative density plot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the traces in the spectra with the seismic data behind the current node.", "response": "def spec_trace(traces, cmap=None, wlen=0.4, log=False, trc='k', tralpha=0.9,\n               size=(10, 13), fig=None, **kwargs):\n    \"\"\"\n    Plots seismic data with spectrogram behind.\n\n    Takes a stream or list of traces and plots the trace with the spectra\n    beneath it.\n\n    :type traces: list\n    :param traces: Traces to be plotted, can be a single\n        :class:`obspy.core.stream.Stream`, or a list of\n        :class:`obspy.core.trace.Trace`.\n    :type cmap: str\n    :param cmap:\n        `Matplotlib colormap\n        <http://matplotlib.org/examples/color/colormaps_reference.html>`_.\n    :type wlen: float\n    :param wlen: Window length for fft in seconds\n    :type log: bool\n    :param log: Use a log frequency scale\n    :type trc: str\n    :param trc: Color for the trace.\n    :type tralpha: float\n    :param tralpha: Opacity level for the seismogram, from transparent (0.0) \\\n        to opaque (1.0).\n    :type size: tuple\n    :param size: Plot size, tuple of floats, inches\n    :type fig: matplotlib.figure.Figure\n    :param fig: Figure to plot onto, defaults to self generating.\n\n    :returns: :class:`matplotlib.figure.Figure`\n\n    .. rubric:: Example\n\n    >>> from obspy import read\n    >>> from eqcorrscan.utils.plotting import spec_trace\n    >>> st = read()\n    >>> spec_trace(st, trc='white') # doctest: +SKIP\n\n\n    .. plot::\n\n        from obspy import read\n        from eqcorrscan.utils.plotting import spec_trace\n        st = read()\n        spec_trace(st, trc='white')\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    if isinstance(traces, Stream):\n        traces.sort(['station', 'channel'])\n    if not fig:\n        fig = plt.figure()\n    for i, tr in enumerate(traces):\n        if i == 0:\n            ax = fig.add_subplot(len(traces), 1, i + 1)\n        else:\n            ax = fig.add_subplot(len(traces), 1, i + 1, sharex=ax)\n        ax1, ax2 = _spec_trace(tr, cmap=cmap, wlen=wlen, log=log, trc=trc,\n                               tralpha=tralpha, axes=ax)\n        ax.set_yticks([])\n        if i < len(traces) - 1:\n            plt.setp(ax1.get_xticklabels(), visible=False)\n        if isinstance(traces, list):\n            ax.text(0.005, 0.85, \"{0}::{1}\".format(tr.id, tr.stats.starttime),\n                    bbox=dict(facecolor='white', alpha=0.8),\n                    transform=ax2.transAxes)\n        elif isinstance(traces, Stream):\n            ax.text(0.005, 0.85, tr.id,\n                    bbox=dict(facecolor='white', alpha=0.8),\n                    transform=ax2.transAxes)\n        ax.text(0.005, 0.02, str(np.max(tr.data).round(1)),\n                bbox=dict(facecolor='white', alpha=0.95),\n                transform=ax2.transAxes)\n    ax.set_xlabel('Time (s)')\n    fig.subplots_adjust(hspace=0)\n    fig.set_size_inches(w=size[0], h=size[1], forward=True)\n    fig.text(0.04, 0.5, 'Frequency (Hz)', va='center', rotation='vertical')\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _spec_trace(trace, cmap=None, wlen=0.4, log=False, trc='k',\n                tralpha=0.9, size=(10, 2.5), axes=None, title=None):\n    \"\"\"\n    Function to plot a trace over that traces spectrogram.\n\n    Uses obspys spectrogram routine.\n\n    :type trace: obspy.core.trace.Trace\n    :param trace: trace to plot\n    :type cmap: str\n    :param cmap: [Matplotlib colormap](http://matplotlib.org/examples/color/\n        colormaps_reference.html)\n    :type wlen: float\n    :param wlen: Window length for fft in seconds\n    :type log: bool\n    :param log: Use a log frequency scale\n    :type trc: str\n    :param trc: Color for the trace.\n    :type tralpha: float\n    :param tralpha: Opacity level for the seismogram, from transparent (0.0) \\\n        to opaque (1.0).\n    :type size: tuple\n    :param size: Plot size, tuple of floats, inches\n    :type axes: matplotlib axes\n    :param axes: Axes to plot onto, defaults to self generating.\n    :type title: str\n    :param title: Title for the plot.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    if not axes:\n        fig = plt.figure(figsize=size)\n        ax1 = fig.add_subplot(111)\n    else:\n        ax1 = axes\n    trace.spectrogram(wlen=wlen, log=log, show=False, cmap=cmap, axes=ax1)\n    fig = plt.gcf()\n    ax2 = ax1.twinx()\n    y = trace.data\n    x = np.linspace(0, len(y) / trace.stats.sampling_rate, len(y))\n    ax2.plot(x, y, color=trc, linewidth=2.0, alpha=tralpha)\n    ax2.set_xlim(min(x), max(x))\n    ax2.set_ylim(min(y) * 2, max(y) * 2)\n    if title:\n        ax1.set_title(' '.join([trace.stats.station, trace.stats.channel,\n                                trace.stats.starttime.datetime.\n                                strftime('%Y/%m/%d %H:%M:%S')]))\n    if not axes:\n        fig.set_size_inches(size)\n        fig.show()\n    else:\n        return ax1, ax2", "response": "Plot a trace over that traces spectrogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the output basis vectors for the given detector.", "response": "def subspace_detector_plot(detector, stachans, size, **kwargs):\n    \"\"\"\n    Plotting for the subspace detector class.\n\n    Plot the output basis vectors for the detector at the given dimension.\n\n    Corresponds to the first n horizontal vectors of the V matrix.\n\n    :type detector: :class:`eqcorrscan.core.subspace.Detector`\n    :type stachans: list\n    :param stachans: list of tuples of station, channel pairs to plot.\n    :type stachans: list\n    :param stachans: List of tuples of (station, channel) to use.  Can set\\\n        to 'all' to use all the station-channel pairs available. If \\\n        detector is multiplexed, will just plot that.\n    :type size: tuple\n    :param size: Figure size.\n\n    :returns: Figure\n    :rtype: matplotlib.pyplot.Figure\n\n    .. rubric:: Example\n\n    >>> from eqcorrscan.core import subspace\n    >>> import os\n    >>> detector = subspace.Detector()\n    >>> detector.read(os.path.join(\n    ...    os.path.abspath(os.path.dirname(__file__)),\n    ...    '..', 'tests', 'test_data', 'subspace',\n    ...    'stat_test_detector.h5'))\n    Detector: Tester\n    >>> subspace_detector_plot(detector=detector, stachans='all', size=(10, 7),\n    ...                        show=True) # doctest: +SKIP\n\n    .. plot::\n\n        from eqcorrscan.core import subspace\n        from eqcorrscan.utils.plotting import subspace_detector_plot\n        import os\n        print('running subspace plot')\n        detector = subspace.Detector()\n        detector.read(os.path.join('..', '..', '..', 'tests', 'test_data',\n                                   'subspace', 'stat_test_detector.h5'))\n        subspace_detector_plot(detector=detector, stachans='all', size=(10, 7),\n                               show=True)\n    \"\"\"\n    import matplotlib.pyplot as plt\n    if stachans == 'all' and not detector.multiplex:\n        stachans = detector.stachans\n    elif detector.multiplex:\n        stachans = [('multi', ' ')]\n    if np.isinf(detector.dimension):\n        msg = ' '.join(['Infinite subspace dimension. Only plotting as many',\n                        'dimensions as events in design set'])\n        warnings.warn(msg)\n        nrows = detector.v[0].shape[1]\n    else:\n        nrows = detector.dimension\n    fig, axes = plt.subplots(nrows=nrows, ncols=len(stachans),\n                             sharex=True, sharey=True, figsize=size)\n    x = np.arange(len(detector.u[0]), dtype=np.float32)\n    if detector.multiplex:\n        x /= len(detector.stachans) * detector.sampling_rate\n    else:\n        x /= detector.sampling_rate\n    for column, stachan in enumerate(stachans):\n        channel = detector.u[column]\n        for row, vector in enumerate(channel.T[0:nrows]):\n            if len(stachans) == 1:\n                if nrows == 1:\n                    axis = axes\n                else:\n                    axis = axes[row]\n            else:\n                axis = axes[row, column]\n            if row == 0:\n                axis.set_title('.'.join(stachan))\n            axis.plot(x, vector, 'k', linewidth=1.1)\n            if column == 0:\n                axis.set_ylabel('Basis %s' % (row + 1), rotation=0)\n            if row == nrows - 1:\n                axis.set_xlabel('Time (s)')\n            axis.set_yticks([])\n    plt.subplots_adjust(hspace=0.05)\n    plt.subplots_adjust(wspace=0.05)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subspace_fc_plot(detector, stachans, size, **kwargs):\n    import matplotlib.pyplot as plt\n    if stachans == 'all' and not detector.multiplex:\n        stachans = detector.stachans\n    elif detector.multiplex:\n        stachans = [('multi', ' ')]\n    # Work out how many rows and columns are most 'square'\n    pfs = []\n    for x in range(1, len(stachans)):\n        if len(stachans) % x == 0:\n            pfs.append(x)\n    if stachans == [('multi', ' ')]:\n        ncols = 1\n    else:\n        ncols = min(pfs,\n                    key=lambda x: abs((np.floor(np.sqrt(len(stachans))) - x)))\n    nrows = len(stachans) // ncols\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=True,\n                             sharey=True, figsize=size, squeeze=False)\n    for column, axis in enumerate(axes.reshape(-1)):\n        axis.set_title('.'.join(stachans[column]))\n        sig = diagsvd(detector.sigma[column], detector.u[column].shape[0],\n                      detector.v[column].shape[0])\n        A = np.dot(sig, detector.v[column])  # v is v.H from scipy.svd\n        if detector.dimension > max(\n                detector.v[column].shape) or detector.dimension == np.inf:\n            dim = max(detector.v[column].shape) + 1\n        else:\n            dim = detector.dimension + 1\n        av_fc_dict = {i: [] for i in range(dim)}\n        for ai in A.T:\n            fcs = []\n            for j in range(dim):\n                av_fc_dict[j].append(float(np.dot(ai[:j].T, ai[:j])))\n                fcs.append(float(np.dot(ai[:j].T, ai[:j])))\n            axis.plot(fcs, color='grey')\n        avg = [np.average(_dim[1]) for _dim in av_fc_dict.items()]\n        axis.plot(avg, color='red', linewidth=3.)\n        if column % ncols == 0 or column == 0:\n            axis.set_ylabel('Frac. E Capture (Fc)')\n        if column + 1 > len(stachans) - ncols:\n            axis.set_xlabel('Subspace Dimension')\n    plt.subplots_adjust(hspace=0.2)\n    plt.subplots_adjust(wspace=0.2)\n    fig = _finalise_figure(fig=fig, **kwargs)  # pragma: no cover\n    return fig", "response": "Plot the fractional energy capture of the detector for all events in the specified list of stachans."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _match_filter_plot(stream, cccsum, template_names, rawthresh, plotdir,\n                       plot_format, i):  # pragma: no cover\n    \"\"\"\n    Plotting function for match_filter.\n\n    :param stream: Stream to plot\n    :param cccsum: Cross-correlation sum to plot\n    :param template_names: Template names used\n    :param rawthresh: Threshold level\n    :param plotdir: Location to save plots\n    :param plot_format: Output plot type (e.g. png, svg, eps, pdf...)\n    :param i: Template index name to plot.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    plt.ioff()\n    stream_plot = copy.deepcopy(stream[0])\n    # Downsample for plotting\n    stream_plot = _plotting_decimation(stream_plot, 10e5, 4)\n    cccsum_plot = Trace(cccsum)\n    cccsum_plot.stats.sampling_rate = stream[0].stats.sampling_rate\n    # Resample here to maintain shape better\n    cccsum_hist = cccsum_plot.copy()\n    cccsum_hist = cccsum_hist.decimate(int(stream[0].stats.\n                                           sampling_rate / 10)).data\n    cccsum_plot = chunk_data(cccsum_plot, 10, 'Maxabs').data\n    # Enforce same length\n    stream_plot.data = stream_plot.data[0:len(cccsum_plot)]\n    cccsum_plot = cccsum_plot[0:len(stream_plot.data)]\n    cccsum_hist = cccsum_hist[0:len(stream_plot.data)]\n    plot_name = (plotdir + os.sep + 'cccsum_plot_' + template_names[i] + '_' +\n                 stream[0].stats.starttime.datetime.strftime('%Y-%m-%d') +\n                 '.' + plot_format)\n    triple_plot(cccsum=cccsum_plot, cccsum_hist=cccsum_hist,\n                trace=stream_plot, threshold=rawthresh, save=True,\n                savefile=plot_name)", "response": "Plots the match filter for the given template."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecimating data until required length reached.", "response": "def _plotting_decimation(trace, max_len=10e5, decimation_step=4):\n    \"\"\"\n    Decimate data until required length reached.\n\n    :type trace: obspy.core.stream.Trace\n    :param trace: Trace to decimate\n    type max_len: int\n    :param max_len: Maximum length in samples\n    :type decimation_step: int\n    :param decimation_step: Decimation factor to use for each step.\n\n    :return: obspy.core.stream.Trace\n\n    .. rubric: Example\n\n    >>> from obspy import Trace\n    >>> import numpy as np\n    >>> trace = Trace(np.random.randn(1000))\n    >>> trace = _plotting_decimation(trace, max_len=100, decimation_step=2)\n    >>> print(trace.stats.npts)\n    63\n    \"\"\"\n    trace_len = trace.stats.npts\n    while trace_len > max_len:\n        trace.decimate(decimation_step)\n        trace_len = trace.stats.npts\n    return trace"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_images_responsive(app, doctree):\n\n    for fig in doctree.traverse(condition=nodes.figure):\n        if 'thumbnail' in fig['classes']:\n            continue\n\n        for img in fig.traverse(condition=nodes.image):\n            img['classes'].append('img-responsive')", "response": "Add Bootstrap img - responsive class to images."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _manual_overrides(_cache_date=None):\n    log = logging.getLogger('ciu')\n    request = requests.get(\"https://raw.githubusercontent.com/brettcannon/\"\n                           \"caniusepython3/master/caniusepython3/overrides.json\")\n    if request.status_code == 200:\n        log.info(\"Overrides loaded from GitHub and cached\")\n        overrides = request.json()\n    else:\n        log.info(\"Overrides loaded from included package data and cached\")\n        raw_bytes = pkgutil.get_data(__name__, 'overrides.json')\n        overrides = json.loads(raw_bytes.decode('utf-8'))\n    return frozenset(map(packaging.utils.canonicalize_name, overrides.keys()))", "response": "Read the overrides file and return a set of names that are used in the caniuse Python version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef supports_py3(project_name):\n    log = logging.getLogger(\"ciu\")\n    log.info(\"Checking {} ...\".format(project_name))\n    request = requests.get(\"https://pypi.org/pypi/{}/json\".format(project_name))\n    if request.status_code >= 400:\n        log = logging.getLogger(\"ciu\")\n        log.warning(\"problem fetching {}, assuming ported ({})\".format(\n                        project_name, request.status_code))\n        return True\n    response = request.json()\n    return any(c.startswith(\"Programming Language :: Python :: 3\")\n               for c in response[\"info\"][\"classifiers\"])", "response": "Check with PyPI if a project supports Python 3."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(requirements_paths=[], metadata=[], projects=[]):\n    dependencies = []\n    dependencies.extend(projects_.projects_from_requirements(requirements_paths))\n    dependencies.extend(projects_.projects_from_metadata(metadata))\n    dependencies.extend(projects)\n\n    manual_overrides = pypi.manual_overrides()\n\n    for dependency in dependencies:\n        if dependency in manual_overrides:\n            continue\n        elif not pypi.supports_py3(dependency):\n            return False\n    return True", "response": "Return True if all of the specified dependencies have been ported to Python 3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking arguments through the CLI can create a list of specified projects.", "response": "def projects_from_cli(args):\n    \"\"\"Take arguments through the CLI can create a list of specified projects.\"\"\"\n    description = ('Determine if a set of project dependencies will work with '\n                   'Python 3')\n    parser = argparse.ArgumentParser(description=description)\n    req_help = 'path(s) to a pip requirements file (e.g. requirements.txt)'\n    parser.add_argument('--requirements', '-r', nargs='+', default=(),\n                        help=req_help)\n    meta_help = 'path(s) to a PEP 426 metadata file (e.g. PKG-INFO, pydist.json)'\n    parser.add_argument('--metadata', '-m', nargs='+', default=(),\n                        help=meta_help)\n    parser.add_argument('--projects', '-p', nargs='+', default=(),\n                        help='name(s) of projects to test for Python 3 support')\n    parser.add_argument('--verbose', '-v', action='store_true',\n                        help='verbose output (e.g. list compatibility overrides)')\n    parsed = parser.parse_args(args)\n\n    if not (parsed.requirements or parsed.metadata or parsed.projects):\n        parser.error(\"Missing 'requirements', 'metadata', or 'projects'\")\n\n    projects = []\n    if parsed.verbose:\n        logging.getLogger('ciu').setLevel(logging.INFO)\n    projects.extend(projects_.projects_from_requirements(parsed.requirements))\n    metadata = []\n    for metadata_path in parsed.metadata:\n        with io.open(metadata_path) as file:\n            metadata.append(file.read())\n    projects.extend(projects_.projects_from_metadata(metadata))\n    projects.extend(map(packaging.utils.canonicalize_name, parsed.projects))\n\n    return projects"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a sequence of key messages based on what is blocking.", "response": "def message(blockers):\n    \"\"\"Create a sequence of key messages based on what is blocking.\"\"\"\n    if not blockers:\n        encoding = getattr(sys.stdout, 'encoding', '')\n        if encoding:\n            encoding = encoding.lower()\n        if encoding == 'utf-8':\n            # party hat\n            flair = \"\\U0001F389  \"\n        else:\n            flair = ''\n        return [flair +\n                'You have 0 projects blocking you from using Python 3!']\n    flattened_blockers = set()\n    for blocker_reasons in blockers:\n        for blocker in blocker_reasons:\n            flattened_blockers.add(blocker)\n    need = 'You need {0} project{1} to transition to Python 3.'\n    formatted_need = need.format(len(flattened_blockers),\n                      's' if len(flattened_blockers) != 1 else '')\n    can_port = ('Of {0} {1} project{2}, {3} {4} no direct dependencies '\n                'blocking {5} transition:')\n    formatted_can_port = can_port.format(\n            'those' if len(flattened_blockers) != 1 else 'that',\n            len(flattened_blockers),\n            's' if len(flattened_blockers) != 1 else '',\n            len(blockers),\n            'have' if len(blockers) != 1 else 'has',\n            'their' if len(blockers) != 1 else 'its')\n    return formatted_need, formatted_can_port"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pprint_blockers(blockers):\n    pprinted = []\n    for blocker in sorted(blockers, key=lambda x: tuple(reversed(x))):\n        buf = [blocker[0]]\n        if len(blocker) > 1:\n            buf.append(' (which is blocking ')\n            buf.append(', which is blocking '.join(blocker[1:]))\n            buf.append(')')\n        pprinted.append(''.join(buf))\n    return pprinted", "response": "Pretty print blockers into a sequence of strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(projects):\n    log = logging.getLogger('ciu')\n    log.info('{0} top-level projects to check'.format(len(projects)))\n    print('Finding and checking dependencies ...')\n    blockers = dependencies.blockers(projects)\n\n    print('')\n    for line in message(blockers):\n        print(line)\n\n    print('')\n    for line in pprint_blockers(blockers):\n        print(' ', line)\n\n    return len(blockers) == 0", "response": "Check the specified projects for Python 3 compatibility."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reasons_to_paths(reasons):\n    blockers = set(reasons.keys()) - set(reasons.values())\n    paths = set()\n    for blocker in blockers:\n        path = [blocker]\n        parent = reasons[blocker]\n        while parent:\n            if parent in path:\n                raise CircularDependencyError(dict(parent=parent,\n                                                   blocker=blocker,\n                                                   path=path))\n            path.append(parent)\n            parent = reasons.get(parent)\n        paths.add(tuple(path))\n    return paths", "response": "Calculate the dependency paths to the reasons of the blockers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dependencies(project_name):\n    log = logging.getLogger('ciu')\n    log.info('Locating dependencies for {}'.format(project_name))\n    located = distlib.locators.locate(project_name, prereleases=True)\n    if not located:\n        log.warning('{0} not found'.format(project_name))\n        return None\n    return {packaging.utils.canonicalize_name(pypi.just_name(dep))\n            for dep in located.run_requires}", "response": "Get the dependencies for a project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the project dependencies from a Requirements specification.", "response": "def projects_from_requirements(requirements):\n    \"\"\"Extract the project dependencies from a Requirements specification.\"\"\"\n    log = logging.getLogger('ciu')\n    valid_reqs = []\n    for requirements_path in requirements:\n        with io.open(requirements_path) as file:\n            requirements_text = file.read()\n        # Drop line continuations.\n        requirements_text = re.sub(r\"\\\\s*\", \"\", requirements_text)\n        # Drop comments.\n        requirements_text = re.sub(r\"#.*\", \"\", requirements_text)\n        reqs = []\n        for line in requirements_text.splitlines():\n            if not line:\n                continue\n            try:\n                reqs.append(packaging.requirements.Requirement(line))\n            except packaging.requirements.InvalidRequirement:\n                log.warning('Skipping {0!r}: could not parse requirement'.format(line))\n        for req in reqs:\n            if not req.name:\n                log.warning('A requirement lacks a name '\n                            '(e.g. no `#egg` on a `file:` path)')\n            elif req.url:\n                log.warning(\n                    'Skipping {0}: URL-specified projects unsupported'.format(req.name))\n            else:\n                valid_reqs.append(req.name)\n    return frozenset(map(packaging.utils.canonicalize_name, valid_reqs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef projects_from_metadata(metadata):\n    projects = []\n    for data in metadata:\n        meta = distlib.metadata.Metadata(fileobj=io.StringIO(data))\n        projects.extend(pypi.just_name(project) for project in meta.run_requires)\n    return frozenset(map(packaging.utils.canonicalize_name, projects))", "response": "Extract the project dependencies from a metadata spec."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding all matches in a given image and yield a new image with the same image.", "response": "def _locateAll_opencv(needleImage, haystackImage, grayscale=None, limit=10000, region=None, step=1,\n                      confidence=0.999):\n    \"\"\" faster but more memory-intensive than pure python\n        step 2 skips every other row and column = ~3x faster but prone to miss;\n            to compensate, the algorithm automatically reduces the confidence\n            threshold by 5% (which helps but will not avoid all misses).\n        limitations:\n          - OpenCV 3.x & python 3.x not tested\n          - RGBA images are treated as RBG (ignores alpha channel)\n    \"\"\"\n    if grayscale is None:\n        grayscale = GRAYSCALE_DEFAULT\n\n    confidence = float(confidence)\n\n    needleImage = _load_cv2(needleImage, grayscale)\n    needleHeight, needleWidth = needleImage.shape[:2]\n    haystackImage = _load_cv2(haystackImage, grayscale)\n\n    if region:\n        haystackImage = haystackImage[region[1]:region[1]+region[3],\n                                      region[0]:region[0]+region[2]]\n    else:\n        region = (0, 0)  # full image; these values used in the yield statement\n    if (haystackImage.shape[0] < needleImage.shape[0] or\n        haystackImage.shape[1] < needleImage.shape[1]):\n        # avoid semi-cryptic OpenCV error below if bad size\n        raise ValueError('needle dimension(s) exceed the haystack image or region dimensions')\n\n    if step == 2:\n        confidence *= 0.95\n        needleImage = needleImage[::step, ::step]\n        haystackImage = haystackImage[::step, ::step]\n    else:\n        step = 1\n\n    # get all matches at once, credit: https://stackoverflow.com/questions/7670112/finding-a-subimage-inside-a-numpy-image/9253805#9253805\n    result = cv2.matchTemplate(haystackImage, needleImage, cv2.TM_CCOEFF_NORMED)\n    match_indices = numpy.arange(result.size)[(result > confidence).flatten()]\n    matches = numpy.unravel_index(match_indices[:limit], result.shape)\n\n    if len(matches[0]) == 0:\n        if USE_IMAGE_NOT_FOUND_EXCEPTION:\n            raise ImageNotFoundException('Could not locate the image (highest confidence = %.3f)' % result.max())\n        else:\n            return None\n\n    # use a generator for API consistency:\n    matchx = matches[1] * step + region[0]  # vectorized\n    matchy = matches[0] * step + region[1]\n    for x, y in zip(matchx, matchy):\n        yield Box(x, y, needleWidth, needleHeight)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef locateOnScreen(image, minSearchTime=0, **kwargs):\n    start = time.time()\n    while True:\n        try:\n            screenshotIm = screenshot(region=None) # the locateAll() function must handle cropping to return accurate coordinates, so don't pass a region here.\n            retVal = locate(image, screenshotIm, **kwargs)\n            try:\n                screenshotIm.fp.close()\n            except AttributeError:\n                # Screenshots on Windows won't have an fp since they came from\n                # ImageGrab, not a file. Screenshots on Linux will have fp set\n                # to None since the file has been unlinked\n                pass\n            if retVal or time.time() - start > minSearchTime:\n                return retVal\n        except ImageNotFoundException:\n            if time.time() - start > minSearchTime:\n                if USE_IMAGE_NOT_FOUND_EXCEPTION:\n                    raise\n                else:\n                    return None", "response": "Locate a single image on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getDetails(self, ip_address=None):\n        raw_details = self._requestDetails(ip_address)\n        raw_details['country_name'] = self.countries.get(raw_details.get('country'))\n        raw_details['ip_address'] = ipaddress.ip_address(raw_details.get('ip'))\n        raw_details['latitude'], raw_details['longitude'] = self._read_coords(raw_details.get('loc'))\n        return Details(raw_details)", "response": "Get details for specified IP address as a Details object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _requestDetails(self, ip_address=None):\n        if ip_address not in self.cache:\n            url = self.API_URL\n            if ip_address:\n                url += '/' + ip_address\n\n            response = requests.get(url, headers=self._get_headers(), **self.request_options)\n            if response.status_code == 429:\n                raise RequestQuotaExceededError()\n            response.raise_for_status()\n            self.cache[ip_address] = response.json()\n\n        return self.cache[ip_address]", "response": "Get IP address data by sending request to IPinfo API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_headers(self):\n        headers = {\n            'user-agent': 'IPinfoClient/Python{version}/1.0'.format(version=sys.version_info[0]),\n            'accept': 'application/json'\n        }\n\n        if self.access_token:\n            headers['authorization'] = 'Bearer {}'.format(self.access_token)\n\n        return headers", "response": "Build the headers for the request to IPinfo API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_country_names(self, countries_file=None):\n        if not countries_file:\n            countries_file = os.path.join(os.path.dirname(__file__), self.COUNTRY_FILE_DEFAULT)\n        with open(countries_file) as f:\n            countries_json = f.read()\n\n        return json.loads(countries_json)", "response": "Read list of countries from specified country file or default file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_secure_option(self, section, option):\n        if not self.has_section(section):\n            return False\n        if not self.has_option(section, option):\n            return False\n        if ConfigParser.get(self, section, option) == self._secure_placeholder:\n            return True\n        return False", "response": "Test an option to see if it is secured or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef items(self, section):\n        items = []\n        for k, v in ConfigParser.items(self, section):\n            if self.is_secure_option(section, k):\n                v = self.get(section, k)\n            if v == '!!False!!':\n                v = False\n            items.append((k, v))\n        return items", "response": "Get all items for a section. Subclassed to ensure secure\n        items come back with the unencrypted data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike items but only return secure items.", "response": "def secure_items(self, section):\n        \"\"\"Like items() but only return secure items.\n\n        :param section: section id\n        :type section: string\n        \"\"\"\n        return [x\n                for x in self.items(section)\n                if self.is_secure_option(section, x[0])]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set(self, section, option, value):\n        if not value:\n            value = '!!False!!'\n        if self.is_secure_option(section, option):\n            self.set_secure(section, option, value)\n        else:\n            ConfigParser.set(self, section, option, value)", "response": "Set an option value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset an option and mark it as secure.", "response": "def set_secure(self, section, option, value):\n        \"\"\"Set an option and mark it as secure.\n\n        Any subsequent uses of 'set' or 'get' will also\n        now know that this option is secure as well.\n        \"\"\"\n        if self.keyring_available:\n            s_option = \"%s%s\" % (section, option)\n            self._unsaved[s_option] = ('set', value)\n            value = self._secure_placeholder\n        ConfigParser.set(self, section, option, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsees ConfigParser. write. Also writes secure items to keystore.", "response": "def write(self, *args):\n        \"\"\"See ConfigParser.write().  Also writes secure items to keystore.\"\"\"\n        ConfigParser.write(self, *args)\n        if self.keyring_available:\n            for key, thing in self._unsaved.items():\n                action = thing[0]\n                value = thing[1]\n                if action == 'set':\n                    keyring.set_password(self.keyring_name, key, value)\n                elif action == 'delete':\n                    try:\n                        keyring.delete_password(self.keyring_name, key)\n                    except:\n                        pass\n        self._unsaved = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef account(self, id):\n        if self.parser.has_section(id):\n            return self._section_to_account(id)\n        return None", "response": "Get the account object by section id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an account to the config", "response": "def add_account(self, account):\n        \"\"\"Add Account to config (does not save)\"\"\"\n        serialized = account.serialize()\n        section_items = flatten_dict(serialized)\n        section_id = section_items['local_id']\n\n        if not self.parser.has_section(section_id):\n            self.parser.add_section(section_id)\n\n        for key in sorted(section_items):\n            self.parser.set(section_id, key, section_items[key])\n\n        self.encrypt_account(id=section_id)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encrypt_account(self, id):\n        for key in self.secured_field_names:\n            value = self.parser.get(id, key)\n            self.parser.set_secure(id, key, value)\n        return self", "response": "Make sure that certain fields are encrypted."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_encrypted_account(self, id):\n        for key in self.secured_field_names:\n            if not self.parser.is_secure_option(id, key):\n                return False\n        return True", "response": "Is the account id encrypted?"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves an account from the config file.", "response": "def remove_account(self, id):\n        \"\"\"Add Account from config (does not save)\"\"\"\n        if self.parser.has_section(id):\n            self.parser.remove_section(id)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave changes to config file", "response": "def save(self):\n        \"\"\"Save changes to config file\"\"\"\n        with open(self.file_name, 'w') as fp:\n            self.parser.write(fp)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef client_args_for_bank(bank_info, ofx_version):\n    client_args = {'ofx_version': str(ofx_version)}\n    if 'ofx.discovercard.com' in bank_info['url']:\n        # Discover needs no User-Agent and no Accept headers\n        client_args['user_agent'] = False\n        client_args['accept'] = False\n    if 'www.accountonline.com' in bank_info['url']:\n        # Citi needs no User-Agent header\n        client_args['user_agent'] = False\n    return client_args", "response": "Return the client arguments to use for a particular Institution for a particular Bank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate(self, username=None, password=None):\n\n        u = self.username\n        p = self.password\n        if username and password:\n            u = username\n            p = password\n\n        client = self.client()\n        query = client.authenticated_query(username=u, password=p)\n        res = client.post(query)\n        ofx = BeautifulSoup(res, 'lxml')\n\n        sonrs = ofx.find('sonrs')\n        code = int(sonrs.find('code').contents[0].strip())\n\n        try:\n            status = sonrs.find('message').contents[0].strip()\n        except Exception:\n            status = ''\n\n        if code == 0:\n            return 1\n\n        raise ValueError(status)", "response": "Test the authentication credentials for the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accounts(self):\n        from ofxclient.account import Account\n        client = self.client()\n        query = client.account_list_query()\n        resp = client.post(query)\n        resp_handle = StringIO(resp)\n\n        if IS_PYTHON_2:\n            parsed = OfxParser.parse(resp_handle)\n        else:\n            parsed = OfxParser.parse(BytesIO(resp_handle.read().encode()))\n\n        return [Account.from_ofxparse(a, institution=self)\n                for a in parsed.accounts]", "response": "Ask the bank for the known accounts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self):\n        return {\n            'id': self.id,\n            'org': self.org,\n            'url': self.url,\n            'broker_id': self.broker_id,\n            'username': self.username,\n            'password': self.password,\n            'description': self.description,\n            'client_args': self.client().init_args,\n            'local_id': self.local_id()\n        }", "response": "Serialize predictably for use in configuration storage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload the data for the given time range.", "response": "def download(self, days=60):\n        \"\"\"Downloaded OFX response for the given time range\n\n        :param days: Number of days to look back at\n        :type days: integer\n        :rtype: :py:class:`StringIO`\n\n        \"\"\"\n        days_ago = datetime.datetime.now() - datetime.timedelta(days=days)\n        as_of = time.strftime(\"%Y%m%d\", days_ago.timetuple())\n        query = self._download_query(as_of=as_of)\n        response = self.institution.client().post(query)\n        return StringIO(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading OFX response parsed by OfxParser. parse", "response": "def download_parsed(self, days=60):\n        \"\"\"Downloaded OFX response parsed by :py:meth:`OfxParser.parse`\n\n        :param days: Number of days to look back at\n        :type days: integer\n        :rtype: :py:class:`ofxparser.Ofx`\n        \"\"\"\n        if IS_PYTHON_2:\n            return OfxParser.parse(\n                self.download(days=days)\n            )\n        else:\n            return OfxParser.parse(\n                BytesIO(self.download(days=days).read().encode())\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef statement(self, days=60):\n        parsed = self.download_parsed(days=days)\n        return parsed.account.statement", "response": "Download the : py : class : ofxparse. Statement given the time range\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing predictably for use in configuration storage.", "response": "def serialize(self):\n        \"\"\"Serialize predictably for use in configuration storage.\n\n        Output look like this::\n\n          {\n            'local_id':       'string',\n            'number':         'account num',\n            'description':    'descr',\n            'broker_id':      'may be missing - type dependent',\n            'routing_number': 'may be missing - type dependent,\n            'account_type':   'may be missing - type dependent,\n            'institution': {\n                # ... see :py:meth:`ofxclient.Institution.serialize`\n            }\n          }\n\n        :rtype: nested dictionary\n        \"\"\"\n        data = {\n            'local_id': self.local_id(),\n            'institution': self.institution.serialize(),\n            'number': self.number,\n            'description': self.description\n        }\n        if hasattr(self, 'broker_id'):\n            data['broker_id'] = self.broker_id\n        elif hasattr(self, 'routing_number'):\n            data['routing_number'] = self.routing_number\n            data['account_type'] = self.account_type\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_ofxparse(data, institution):\n\n        description = data.desc if hasattr(data, 'desc') else None\n        if data.type == AccountType.Bank:\n            return BankAccount(\n                institution=institution,\n                number=data.account_id,\n                routing_number=data.routing_number,\n                account_type=data.account_type,\n                description=description)\n        elif data.type == AccountType.CreditCard:\n            return CreditCardAccount(\n                institution=institution,\n                number=data.account_id,\n                description=description)\n        elif data.type == AccountType.Investment:\n            return BrokerageAccount(\n                institution=institution,\n                number=data.account_id,\n                broker_id=data.brokerid,\n                description=description)\n        raise ValueError(\"unknown account type: %s\" % data.type)", "response": "Instantiate an account object from an ofxparse. Account object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformulate the specific query needed for download MimeType", "response": "def _download_query(self, as_of):\n        \"\"\"Formulate the specific query needed for download\n\n        Not intended to be called by developers directly.\n\n        :param as_of: Date in 'YYYYMMDD' format\n        :type as_of: string\n        \"\"\"\n        c = self.institution.client()\n        q = c.brokerage_account_query(\n            number=self.number, date=as_of, broker_id=self.broker_id)\n        return q"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformulating the specific query needed for download", "response": "def _download_query(self, as_of):\n        \"\"\"Formulate the specific query needed for download\n\n        Not intended to be called by developers directly.\n\n        :param as_of: Date in 'YYYYMMDD' format\n        :type as_of: string\n        \"\"\"\n        c = self.institution.client()\n        q = c.bank_account_query(\n            number=self.number,\n            date=as_of,\n            account_type=self.account_type,\n            bank_id=self.routing_number)\n        return q"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformulating the specific query needed for download", "response": "def _download_query(self, as_of):\n        \"\"\"Formulate the specific query needed for download\n\n        Not intended to be called by developers directly.\n\n        :param as_of: Date in 'YYYYMMDD' format\n        :type as_of: string\n        \"\"\"\n        c = self.institution.client()\n        q = c.credit_card_account_query(number=self.number, date=as_of)\n        return q"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef combined_download(accounts, days=60):\n\n    client = Client(institution=None)\n\n    out_file = StringIO()\n    out_file.write(client.header())\n    out_file.write('<OFX>')\n    for a in accounts:\n        ofx = a.download(days=days).read()\n        stripped = ofx.partition('<OFX>')[2].partition('</OFX>')[0]\n        out_file.write(stripped)\n\n    out_file.write(\"</OFX>\")\n    out_file.seek(0)\n\n    return out_file", "response": "Download OFX files and combine them into one\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate query If you pass a 'with_messages' array those queries will be passed along otherwise this will just be an authentication probe query only.", "response": "def authenticated_query(\n        self,\n        with_message=None,\n        username=None,\n        password=None\n    ):\n        \"\"\"Authenticated query\n\n        If you pass a 'with_messages' array those queries will be passed along\n        otherwise this will just be an authentication probe query only.\n        \"\"\"\n        u = username or self.institution.username\n        p = password or self.institution.password\n\n        contents = ['OFX', self._signOn(username=u, password=p)]\n        if with_message:\n            contents.append(with_message)\n        return LINE_ENDING.join([self.header(), _tag(*contents)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbank account statement request", "response": "def bank_account_query(self, number, date, account_type, bank_id):\n        \"\"\"Bank account statement request\"\"\"\n        return self.authenticated_query(\n            self._bareq(number, date, account_type, bank_id)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps around _do_post to handle accounts that require AttributeNames sending back session cookies.", "response": "def post(self, query):\n        \"\"\"\n        Wrapper around ``_do_post()`` to handle accounts that require\n        sending back session cookies (``self.set_cookies`` True).\n        \"\"\"\n        res, response = self._do_post(query)\n        cookies = res.getheader('Set-Cookie', None)\n        if len(response) == 0 and cookies is not None and res.status == 200:\n            logging.debug('Got 0-length 200 response with Set-Cookies header; '\n                          'retrying request with cookies')\n            _, response = self._do_post(query, [('Cookie', cookies)])\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo a POST to the Institution.", "response": "def _do_post(self, query, extra_headers=[]):\n        \"\"\"\n        Do a POST to the Institution.\n\n        :param query: Body content to POST (OFX Query)\n        :type query: str\n        :param extra_headers: Extra headers to send with the request, as a list\n          of (Name, Value) header 2-tuples.\n        :type extra_headers: list\n        :return: 2-tuple of (HTTPResponse, str response body)\n        :rtype: tuple\n        \"\"\"\n        i = self.institution\n        logging.debug('posting data to %s' % i.url)\n        garbage, path = splittype(i.url)\n        host, selector = splithost(path)\n        h = HTTPSConnection(host, timeout=60)\n        # Discover requires a particular ordering of headers, so send the\n        # request step by step.\n        h.putrequest('POST', selector, skip_host=True,\n                     skip_accept_encoding=True)\n        headers = [\n            ('Content-Type', 'application/x-ofx'),\n            ('Host', host),\n            ('Content-Length', len(query)),\n            ('Connection', 'Keep-Alive')\n        ]\n        if self.accept:\n            headers.append(('Accept', self.accept))\n        if self.user_agent:\n            headers.append(('User-Agent', self.user_agent))\n        for ehname, ehval in extra_headers:\n            headers.append((ehname, ehval))\n        logging.debug('---- request headers ----')\n        for hname, hval in headers:\n            logging.debug('%s: %s', hname, hval)\n            h.putheader(hname, hval)\n        logging.debug('---- request body (query) ----')\n        logging.debug(query)\n        h.endheaders(query.encode())\n        res = h.getresponse()\n        response = res.read().decode('ascii', 'ignore')\n        logging.debug('---- response ----')\n        logging.debug(res.__dict__)\n        logging.debug('Headers: %s', res.getheaders())\n        logging.debug(response)\n        res.close()\n        return res, response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_raw_headers(self, headers: Dict) -> Tuple:\n        raw_headers = []\n        for k, v in headers.items():\n            raw_headers.append((k.encode('utf8'), v.encode('utf8')))\n        return tuple(raw_headers)", "response": "Convert a dict of headers to a tuple of tuples"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _request_mock(self, orig_self: ClientSession,\n                            method: str, url: 'Union[URL, str]',\n                            *args: Tuple,\n                            **kwargs: Dict) -> 'ClientResponse':\n        \"\"\"Return mocked response object or raise connection error.\"\"\"\n        url = normalize_url(merge_params(url, kwargs.get('params')))\n        url_str = str(url)\n        for prefix in self._passthrough:\n            if url_str.startswith(prefix):\n                return (await self.patcher.temp_original(\n                    orig_self, method, url, *args, **kwargs\n                ))\n\n        response = await self.match(method, url, **kwargs)\n        if response is None:\n            raise ClientConnectionError(\n                'Connection refused: {} {}'.format(method, url)\n            )\n        self._responses.append(response)\n        key = (method, url)\n        self.requests.setdefault(key, [])\n        self.requests[key].append(RequestCall(args, kwargs))\n        return response", "response": "Return mocked response object or raise connection error."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes url to make comparisons.", "response": "def normalize_url(url: 'Union[URL, str]') -> 'URL':\n    \"\"\"Normalize url to make comparisons.\"\"\"\n    url = URL(url)\n    return url.with_query(urlencode(sorted(parse_qsl(url.query_string))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, title, description, **kwargs):\n        case = Case(title=title, description=description, **kwargs)\n        response = self._thehive.create_case(case)\n\n        # Check for failed authentication\n        if response.status_code == requests.codes.unauthorized:\n            raise TheHiveException(\"Authentication failed\")\n\n        if self.status_ok(response.status_code):\n            return self(response.json()['id'])\n        else:\n            raise CaseException(\"Server returned {}: {}\".format(response.status_code, response.text))", "response": "Create an instance of the Case class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, case_id, **attributes):\n\n        response = self._thehive.do_patch(\"/api/case/{}\".format(case_id), **attributes)\n\n        if response.status_code == requests.codes.unauthorized:\n            raise TheHiveException(\"Authentication failed\")\n\n        if self.status_ok(response.status_code):\n            return self(response.json()['id'])\n        else:\n            raise CaseException(\"Server returned {}: {}\".format(response.status_code, response.text))", "response": "Update a case. iCal object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_case(self, case):\n\n        \"\"\"\n        :param case: The case details\n        :type case: Case defined in models.py\n        :return: TheHive case\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/case\"\n        data = case.jsonify()\n        try:\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise CaseException(\"Case create error: {}\".format(e))", "response": "Create a new Case in the Hive"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a case s attributes.", "response": "def update_case(self, case, fields=[]):\n        \"\"\"\n        Update a case.\n        :param case: The case to update. The case's `id` determines which case to update.\n        :param fields: Optional parameter, an array of fields names, the ones we want to update\n        :return:\n        \"\"\"\n        req = self.url + \"/api/case/{}\".format(case.id)\n\n        # Choose which attributes to send\n        update_keys = [\n            'title', 'description', 'severity', 'startDate', 'owner', 'flag', 'tlp', 'tags', 'status', 'resolutionStatus',\n            'impactStatus', 'summary', 'endDate', 'metrics', 'customFields'\n        ]\n        data = {k: v for k, v in case.__dict__.items() if (len(fields) > 0 and k in fields) or (len(fields) == 0 and k in update_keys)}\n        try:\n            return requests.patch(req, headers={'Content-Type': 'application/json'}, json=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException:\n            raise CaseException(\"Case update error: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_case_task(self, case_id, case_task):\n\n        \"\"\"\n        :param case_id: Case identifier\n        :param case_task: TheHive task\n        :type case_task: CaseTask defined in models.py\n        :return: TheHive task\n        :rtype: json\n\n        \"\"\"\n\n        req = self.url + \"/api/case/{}/task\".format(case_id)\n        data = case_task.jsonify()\n\n        try:\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise CaseTaskException(\"Case task create error: {}\".format(e))", "response": "Create a new CaseTask in the Hive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_case_task(self, task):\n        req = self.url + \"/api/case/task/{}\".format(task.id)\n\n        # Choose which attributes to send\n        update_keys = [\n            'title', 'description', 'status', 'order', 'user', 'owner', 'flag', 'endDate'\n        ]\n\n        data = {k: v for k, v in task.__dict__.items() if k in update_keys}\n\n        try:\n            return requests.patch(req, headers={'Content-Type': 'application/json'}, json=data,\n                                  proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise CaseTaskException(\"Case task update error: {}\".format(e))", "response": "Updates the Hive Task with the new attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_task_log(self, task_id, case_task_log):\n\n        \"\"\"\n        :param task_id: Task identifier\n        :param case_task_log: TheHive log\n        :type case_task_log: CaseTaskLog defined in models.py\n        :return: TheHive log\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/case/task/{}/log\".format(task_id)\n        data = {'_json': json.dumps({\"message\":case_task_log.message})}\n\n        if case_task_log.file:\n            f = {'attachment': (os.path.basename(case_task_log.file), open(case_task_log.file, 'rb'), magic.Magic(mime=True).from_file(case_task_log.file))}\n            try:\n                return requests.post(req, data=data,files=f, proxies=self.proxies, auth=self.auth, verify=self.cert)\n            except requests.exceptions.RequestException as e:\n                raise CaseTaskException(\"Case task log create error: {}\".format(e))\n        else:\n            try:\n                return requests.post(req, headers={'Content-Type': 'application/json'}, data=json.dumps({'message':case_task_log.message}), proxies=self.proxies, auth=self.auth, verify=self.cert)\n            except requests.exceptions.RequestException as e:\n                raise CaseTaskException(\"Case task log create error: {}\".format(e))", "response": "Creates the Hive log for the given task."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Case observable", "response": "def create_case_observable(self, case_id, case_observable):\n\n        \"\"\"\n        :param case_id: Case identifier\n        :param case_observable: TheHive observable\n        :type case_observable: CaseObservable defined in models.py\n        :return: TheHive observable\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/case/{}/artifact\".format(case_id)\n\n        if case_observable.dataType == 'file':\n            try:\n                mesg = json.dumps({ \"dataType\": case_observable.dataType,\n                    \"message\": case_observable.message,\n                    \"tlp\": case_observable.tlp,\n                    \"tags\": case_observable.tags,\n                    \"ioc\": case_observable.ioc\n                    })\n                data = {\"_json\": mesg}\n                return requests.post(req, data=data, files=case_observable.data[0], proxies=self.proxies, auth=self.auth, verify=self.cert)\n            except requests.exceptions.RequestException as e:\n                raise CaseObservableException(\"Case observable create error: {}\".format(e))\n        else:\n            try:\n                return requests.post(req, headers={'Content-Type': 'application/json'}, data=case_observable.jsonify(), proxies=self.proxies, auth=self.auth, verify=self.cert)\n            except requests.exceptions.RequestException as e:\n                raise CaseObservableException(\"Case observable create error: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the linked cases for the given case.", "response": "def get_linked_cases(self, case_id):\n        \"\"\"\n        :param case_id: Case identifier\n        :return: TheHive case(s)\n        :rtype: json\n        \"\"\"\n        req = self.url + \"/api/case/{}/links\".format(case_id)\n\n        try:\n            return requests.get(req, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise CaseException(\"Linked cases fetch error: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_case_template(self, name):\n\n        \"\"\"\n        :param name: Case template name\n        :return: TheHive case template\n        :rtype: json\n\n        \"\"\"\n\n        req = self.url + \"/api/case/template/_search\"\n        data = {\n            \"query\": And(Eq(\"name\", name), Eq(\"status\", \"Ok\"))\n        }\n\n        try:\n            response = requests.post(req, json=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n            json_response = response.json()\n\n            if response.status_code == 200 and len(json_response) > 0:\n                return response.json()[0]\n            else:\n                raise CaseTemplateException(\"Case template fetch error: Unable to find case template {}\".format(name))\n        except requests.exceptions.RequestException as e:\n            raise CaseTemplateException(\"Case template fetch error: {}\".format(e))", "response": "Get the details of a specific Case Template."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the Hive logs for the given task.", "response": "def get_task_logs(self, taskId):\n\n        \"\"\"\n        :param taskId: Task identifier\n        :type caseTaskLog: CaseTaskLog defined in models.py\n        :return: TheHive logs\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/case/task/{}/log\".format(taskId)\n        try:\n            return requests.get(req, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise CaseTaskException(\"Case task logs search error: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an alert in the Hive", "response": "def create_alert(self, alert):\n\n        \"\"\"\n        :param alert: TheHive alert\n        :type alert: Alert defined in models.py\n        :return: TheHive alert\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/alert\"\n        data = alert.jsonify()\n        try:\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise AlertException(\"Alert create error: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mark_alert_as_read(self, alert_id):\n        req = self.url + \"/api/alert/{}/markAsRead\".format(alert_id)\n\n        try:\n            return requests.post(req, headers={'Content-Type': 'application/json'}, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException:\n            raise AlertException(\"Mark alert as read error: {}\".format(e))", "response": "Mark an alert as read."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_alert(self, alert_id, alert, fields=[]):\n        req = self.url + \"/api/alert/{}\".format(alert_id)\n\n        # update only the alert attributes that are not read-only\n        update_keys = ['tlp', 'severity', 'tags', 'caseTemplate', 'title', 'description']\n\n        data = {k: v for k, v in alert.__dict__.items() if\n                (len(fields) > 0 and k in fields) or (len(fields) == 0 and k in update_keys)}\n\n        if hasattr(alert, 'artifacts'):\n            data['artifacts'] = [a.__dict__ for a in alert.artifacts]\n        try:\n            return requests.patch(req, headers={'Content-Type': 'application/json'}, json=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException:\n            raise AlertException(\"Alert update error: {}\".format(e))", "response": "Update an alert s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_alert(self, alert_id):\n        req = self.url + \"/api/alert/{}\".format(alert_id)\n\n        try:\n            return requests.get(req, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise AlertException(\"Alert fetch error: {}\".format(e))", "response": "Get the details of an alert."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_analyzer(self, cortex_id, artifact_id, analyzer_id):\n\n        \"\"\"\n        :param cortex_id: identifier of the Cortex server\n        :param artifact_id: identifier of the artifact as found with an artifact search\n        :param analyzer_id: name of the analyzer used by the job\n        :rtype: json\n        \"\"\"\n\n        req = self.url + \"/api/connector/cortex/job\"\n\n        try:\n            data = json.dumps({ \"cortexId\": cortex_id,\n                \"artifactId\": artifact_id,\n                \"analyzerId\": analyzer_id\n                })\n            return requests.post(req, headers={'Content-Type': 'application/json'}, data=data, proxies=self.proxies, auth=self.auth, verify=self.cert)\n        except requests.exceptions.RequestException as e:\n            raise TheHiveException(\"Analyzer run error: {}\".format(e))", "response": "Runs the analyzer for the given artifact and returns the result of the analyzer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self, headers=None):\n        return self.transport.forward_request(\n            method='GET', path='/', headers=headers)", "response": "Returns the information of the node being connected to via the the\n            root endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving information provided by the API root endpoint .", "response": "def api_info(self, headers=None):\n        \"\"\"Retrieves information provided by the API root endpoint\n        ``'/api/v1'``.\n\n        Args:\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: Details of the HTTP API provided by the BigchainDB\n            server.\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET',\n            path=self.api_prefix,\n            headers=headers,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare(*, operation='CREATE', signers=None,\n                recipients=None, asset=None, metadata=None, inputs=None):\n        \"\"\"Prepares a transaction payload, ready to be fulfilled.\n\n        Args:\n            operation (str): The operation to perform. Must be ``'CREATE'``\n                or ``'TRANSFER'``. Case insensitive. Defaults to ``'CREATE'``.\n            signers (:obj:`list` | :obj:`tuple` | :obj:`str`, optional):\n                One or more public keys representing the issuer(s) of\n                the asset being created. Only applies for ``'CREATE'``\n                operations. Defaults to ``None``.\n            recipients (:obj:`list` | :obj:`tuple` | :obj:`str`, optional):\n                One or more public keys representing the new recipients(s)\n                of the asset being created or transferred.\n                Defaults to ``None``.\n            asset (:obj:`dict`, optional): The asset to be created or\n                transferred. MUST be supplied for ``'TRANSFER'`` operations.\n                Defaults to ``None``.\n            metadata (:obj:`dict`, optional): Metadata associated with the\n                transaction. Defaults to ``None``.\n            inputs (:obj:`dict` | :obj:`list` | :obj:`tuple`, optional):\n                One or more inputs holding the condition(s) that this\n                transaction intends to fulfill. Each input is expected to\n                be a :obj:`dict`. Only applies to, and MUST be supplied for,\n                ``'TRANSFER'`` operations.\n\n        Returns:\n            dict: The prepared transaction.\n\n        Raises:\n            :class:`~.exceptions.BigchaindbException`: If ``operation`` is\n                not ``'CREATE'`` or ``'TRANSFER'``.\n\n        .. important::\n\n            **CREATE operations**\n\n            * ``signers`` MUST be set.\n            * ``recipients``, ``asset``, and ``metadata`` MAY be set.\n            * If ``asset`` is set, it MUST be in the form of::\n\n                {\n                    'data': {\n                        ...\n                    }\n                }\n\n            * The argument ``inputs`` is ignored.\n            * If ``recipients`` is not given, or evaluates to\n              ``False``, it will be set equal to ``signers``::\n\n                if not recipients:\n                    recipients = signers\n\n            **TRANSFER operations**\n\n            * ``recipients``, ``asset``, and ``inputs`` MUST be set.\n            * ``asset`` MUST be in the form of::\n\n                {\n                    'id': '<Asset ID (i.e. TX ID of its CREATE transaction)>'\n                }\n\n            * ``metadata`` MAY be set.\n            * The argument ``signers`` is ignored.\n\n        \"\"\"\n        return prepare_transaction(\n            operation=operation,\n            signers=signers,\n            recipients=recipients,\n            asset=asset,\n            metadata=metadata,\n            inputs=inputs,\n        )", "response": "Prepares a transaction payload ready to be fulfilled."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an asset id get its list of transactions.", "response": "def get(self, *, asset_id, operation=None, headers=None):\n        \"\"\"Given an asset id, get its list of transactions (and\n        optionally filter for only ``'CREATE'`` or ``'TRANSFER'``\n        transactions).\n\n        Args:\n            asset_id (str): Id of the asset.\n            operation (str): The type of operation the transaction\n                should be. Either ``'CREATE'`` or ``'TRANSFER'``.\n                Defaults to ``None``.\n            headers (dict): Optional headers to pass to the request.\n\n        Note:\n            Please note that the id of an asset in BigchainDB is\n            actually the id of the transaction which created the asset.\n            In other words, when querying for an asset id with the\n            operation set to ``'CREATE'``, only one transaction should\n            be expected. This transaction will be the transaction in\n            which the asset was created, and the transaction id will be\n            equal to the given asset id. Hence, the following calls to\n            :meth:`.retrieve` and :meth:`.get` should return the same\n            transaction.\n\n                >>> bdb = BigchainDB()\n                >>> bdb.transactions.retrieve('foo')\n                >>> bdb.transactions.get(asset_id='foo', operation='CREATE')\n\n            Since :meth:`.get` returns a list of transactions, it may\n            be more efficient to use :meth:`.retrieve` instead, if one\n            is only interested in the ``'CREATE'`` operation.\n\n        Returns:\n            list: List of transactions.\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET',\n            path=self.path,\n            params={'asset_id': asset_id, 'operation': operation},\n            headers=headers,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_async(self, transaction, headers=None):\n        return self.transport.forward_request(\n            method='POST',\n            path=self.path,\n            json=transaction,\n            params={'mode': 'async'},\n            headers=headers)", "response": "Sends a transaction to the Federation node with the mode async."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the transaction with the given id.", "response": "def retrieve(self, txid, headers=None):\n        \"\"\"Retrieves the transaction with the given id.\n\n        Args:\n            txid (str): Id of the transaction to retrieve.\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: The transaction with the given id.\n\n        \"\"\"\n        path = self.path + txid\n        return self.transport.forward_request(\n            method='GET', path=path, headers=None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget transaction outputs by public key.", "response": "def get(self, public_key, spent=None, headers=None):\n        \"\"\"Get transaction outputs by public key. The public_key parameter\n        must be a base58 encoded ed25519 public key associated with\n        transaction output ownership.\n\n        Args:\n            public_key (str): Public key for which unfulfilled\n                conditions are sought.\n            spent (bool): Indicate if the result set should include only spent\n                or only unspent outputs. If not specified (``None``) the\n                result includes all the outputs (both spent and unspent)\n                associated with the public key.\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            :obj:`list` of :obj:`str`: List of unfulfilled conditions.\n\n        Example:\n            Given a transaction with `id` ``da1b64a907ba54`` having an\n            `ed25519` condition (at index ``0``) with alice's public\n            key::\n\n                >>> bdb = BigchainDB()\n                >>> bdb.outputs.get(alice_pubkey)\n                ... ['../transactions/da1b64a907ba54/conditions/0']\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET',\n            path=self.path,\n            params={'public_key': public_key, 'spent': spent},\n            headers=headers,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, *, txid, headers=None):\n        block_list = self.transport.forward_request(\n            method='GET',\n            path=self.path,\n            params={'transaction_id': txid},\n            headers=headers,\n        )\n        return block_list[0] if len(block_list) else None", "response": "Get the block that contains the given transaction id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve(self, block_height, headers=None):\n        path = self.path + block_height\n        return self.transport.forward_request(\n            method='GET', path=path, headers=None)", "response": "Retrieves the block with the given height."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the assets that match a given text search string.", "response": "def get(self, *, search, limit=0, headers=None):\n        \"\"\"Retrieves the assets that match a given text search string.\n\n        Args:\n            search (str): Text search string.\n            limit (int): Limit the number of returned documents. Defaults to\n                zero meaning that it returns all the matching assets.\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            :obj:`list` of :obj:`dict`: List of assets that match the query.\n\n        \"\"\"\n        return self.transport.forward_request(\n            method='GET',\n            path=self.path,\n            params={'search': search, 'limit': limit},\n            headers=headers\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_transaction(*, operation='CREATE', signers=None,\n                        recipients=None, asset=None, metadata=None,\n                        inputs=None):\n    \"\"\"Prepares a transaction payload, ready to be fulfilled. Depending on\n    the value of ``operation``, simply dispatches to either\n    :func:`~.prepare_create_transaction` or\n    :func:`~.prepare_transfer_transaction`.\n\n    Args:\n        operation (str): The operation to perform. Must be ``'CREATE'``\n            or ``'TRANSFER'``. Case insensitive. Defaults to ``'CREATE'``.\n        signers (:obj:`list` | :obj:`tuple` | :obj:`str`, optional):\n            One or more public keys representing the issuer(s) of\n            the asset being created. Only applies for ``'CREATE'``\n            operations. Defaults to ``None``.\n        recipients (:obj:`list` | :obj:`tuple` | :obj:`str`, optional):\n            One or more public keys representing the new recipients(s)\n            of the asset being created or transferred.\n            Defaults to ``None``.\n        asset (:obj:`dict`, optional): The asset to be created or\n            transferred. MUST be supplied for ``'TRANSFER'`` operations.\n            Defaults to ``None``.\n        metadata (:obj:`dict`, optional): Metadata associated with the\n            transaction. Defaults to ``None``.\n        inputs (:obj:`dict` | :obj:`list` | :obj:`tuple`, optional):\n            One or more inputs holding the condition(s) that this\n            transaction intends to fulfill. Each input is expected to\n            be a :obj:`dict`. Only applies to, and MUST be supplied for,\n            ``'TRANSFER'`` operations.\n\n    Returns:\n        dict: The prepared transaction.\n\n    Raises:\n        :class:`~.exceptions.BigchaindbException`: If ``operation`` is\n            not ``'CREATE'`` or ``'TRANSFER'``.\n\n    .. important::\n\n        **CREATE operations**\n\n        * ``signers`` MUST be set.\n        * ``recipients``, ``asset``, and ``metadata`` MAY be set.\n        * If ``asset`` is set, it MUST be in the form of::\n\n            {\n                'data': {\n                    ...\n                }\n            }\n\n        * The argument ``inputs`` is ignored.\n        * If ``recipients`` is not given, or evaluates to\n          ``False``, it will be set equal to ``signers``::\n\n            if not recipients:\n                recipients = signers\n\n        **TRANSFER operations**\n\n        * ``recipients``, ``asset``, and ``inputs`` MUST be set.\n        * ``asset`` MUST be in the form of::\n\n            {\n                'id': '<Asset ID (i.e. TX ID of its CREATE transaction)>'\n            }\n\n        * ``metadata`` MAY be set.\n        * The argument ``signers`` is ignored.\n\n    \"\"\"\n    operation = _normalize_operation(operation)\n    return _prepare_transaction(\n        operation,\n        signers=signers,\n        recipients=recipients,\n        asset=asset,\n        metadata=metadata,\n        inputs=inputs,\n    )", "response": "Prepares a transaction payload ready to be fulfilled."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares a CREATE transaction payload ready to be fulfilled.", "response": "def prepare_create_transaction(*,\n                               signers,\n                               recipients=None,\n                               asset=None,\n                               metadata=None):\n    \"\"\"Prepares a ``\"CREATE\"`` transaction payload, ready to be\n    fulfilled.\n\n    Args:\n        signers (:obj:`list` | :obj:`tuple` | :obj:`str`): One\n            or more public keys representing the issuer(s) of the asset\n            being created.\n        recipients (:obj:`list` | :obj:`tuple` | :obj:`str`, optional):\n            One or more public keys representing the new recipients(s)\n            of the asset being created. Defaults to ``None``.\n        asset (:obj:`dict`, optional): The asset to be created.\n            Defaults to ``None``.\n        metadata (:obj:`dict`, optional): Metadata associated with the\n            transaction. Defaults to ``None``.\n\n    Returns:\n        dict: The prepared ``\"CREATE\"`` transaction.\n\n    .. important::\n\n        * If ``asset`` is set, it MUST be in the form of::\n\n                {\n                    'data': {\n                        ...\n                    }\n                }\n\n        * If ``recipients`` is not given, or evaluates to\n          ``False``, it will be set equal to ``signers``::\n\n            if not recipients:\n                recipients = signers\n\n    \"\"\"\n    if not isinstance(signers, (list, tuple)):\n        signers = [signers]\n    # NOTE: Needed for the time being. See\n    # https://github.com/bigchaindb/bigchaindb/issues/797\n    elif isinstance(signers, tuple):\n        signers = list(signers)\n\n    if not recipients:\n        recipients = [(signers, 1)]\n    elif not isinstance(recipients, (list, tuple)):\n        recipients = [([recipients], 1)]\n    # NOTE: Needed for the time being. See\n    # https://github.com/bigchaindb/bigchaindb/issues/797\n    elif isinstance(recipients, tuple):\n        recipients = [(list(recipients), 1)]\n\n    transaction = Transaction.create(\n        signers,\n        recipients,\n        metadata=metadata,\n        asset=asset['data'] if asset else None,\n    )\n    return transaction.to_dict()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_transfer_transaction(*,\n                                 inputs,\n                                 recipients,\n                                 asset,\n                                 metadata=None):\n    \"\"\"Prepares a ``\"TRANSFER\"`` transaction payload, ready to be\n    fulfilled.\n\n    Args:\n        inputs (:obj:`dict` | :obj:`list` | :obj:`tuple`): One or more\n            inputs holding the condition(s) that this transaction\n            intends to fulfill. Each input is expected to be a\n            :obj:`dict`.\n        recipients (:obj:`str` | :obj:`list` | :obj:`tuple`): One or\n            more public keys representing the new recipients(s) of the\n            asset being transferred.\n        asset (:obj:`dict`): A single-key dictionary holding the ``id``\n            of the asset being transferred with this transaction.\n        metadata (:obj:`dict`): Metadata associated with the\n            transaction. Defaults to ``None``.\n\n    Returns:\n        dict: The prepared ``\"TRANSFER\"`` transaction.\n\n    .. important::\n\n        * ``asset`` MUST be in the form of::\n\n            {\n                'id': '<Asset ID (i.e. TX ID of its CREATE transaction)>'\n            }\n\n    Example:\n\n        .. todo:: Replace this section with docs.\n\n        In case it may not be clear what an input should look like, say\n        Alice (public key: ``'3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf'``)\n        wishes to transfer an asset over to Bob\n        (public key: ``'EcRawy3Y22eAUSS94vLF8BVJi62wbqbD9iSUSUNU9wAA'``).\n        Let the asset creation transaction payload be denoted by\n        ``tx``::\n\n            # noqa E501\n            >>> tx\n                {'asset': {'data': {'msg': 'Hello BigchainDB!'}},\n                 'id': '9650055df2539223586d33d273cb8fd05bd6d485b1fef1caf7c8901a49464c87',\n                 'inputs': [{'fulfillment': {'public_key': '3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf',\n                                             'type': 'ed25519-sha-256'},\n                             'fulfills': None,\n                             'owners_before': ['3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf']}],\n                 'metadata': None,\n                 'operation': 'CREATE',\n                 'outputs': [{'amount': '1',\n                              'condition': {'details': {'public_key': '3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf',\n                                                        'type': 'ed25519-sha-256'},\n                                            'uri': 'ni:///sha-256;7ApQLsLLQgj5WOUipJg1txojmge68pctwFxvc3iOl54?fpt=ed25519-sha-256&cost=131072'},\n                              'public_keys': ['3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf']}],\n                 'version': '2.0'}\n\n        Then, the input may be constructed in this way::\n\n            output_index\n            output = tx['transaction']['outputs'][output_index]\n            input_ = {\n                'fulfillment': output['condition']['details'],\n                'input': {\n                    'output_index': output_index,\n                    'transaction_id': tx['id'],\n                },\n                'owners_before': output['owners_after'],\n            }\n\n        Displaying the input on the prompt would look like::\n\n            >>> input_\n            {'fulfillment': {\n              'public_key': '3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf',\n              'type': 'ed25519-sha-256'},\n             'input': {'output_index': 0,\n              'transaction_id': '9650055df2539223586d33d273cb8fd05bd6d485b1fef1caf7c8901a49464c87'},\n             'owners_before': ['3Cxh1eKZk3Wp9KGBWFS7iVde465UvqUKnEqTg2MW4wNf']}\n\n\n        To prepare the transfer:\n\n        >>> prepare_transfer_transaction(\n        ...     inputs=input_,\n        ...     recipients='EcRawy3Y22eAUSS94vLF8BVJi62wbqbD9iSUSUNU9wAA',\n        ...     asset=tx['transaction']['asset'],\n        ... )\n\n    \"\"\"\n    if not isinstance(inputs, (list, tuple)):\n        inputs = (inputs, )\n    if not isinstance(recipients, (list, tuple)):\n        recipients = [([recipients], 1)]\n\n    # NOTE: Needed for the time being. See\n    # https://github.com/bigchaindb/bigchaindb/issues/797\n    if isinstance(recipients, tuple):\n        recipients = [(list(recipients), 1)]\n\n    fulfillments = [\n        Input(_fulfillment_from_details(input_['fulfillment']),\n              input_['owners_before'],\n              fulfills=TransactionLink(\n                  txid=input_['fulfills']['transaction_id'],\n                  output=input_['fulfills']['output_index']))\n        for input_ in inputs\n    ]\n\n    transaction = Transaction.transfer(\n        fulfillments,\n        recipients,\n        asset_id=asset['id'],\n        metadata=metadata,\n    )\n    return transaction.to_dict()", "response": "Prepares a new transaction payload for the given set of inputs recipients asset and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fulfill_transaction(transaction, *, private_keys):\n    if not isinstance(private_keys, (list, tuple)):\n        private_keys = [private_keys]\n\n    # NOTE: Needed for the time being. See\n    # https://github.com/bigchaindb/bigchaindb/issues/797\n    if isinstance(private_keys, tuple):\n        private_keys = list(private_keys)\n\n    transaction_obj = Transaction.from_dict(transaction)\n    try:\n        signed_transaction = transaction_obj.sign(private_keys)\n    except KeypairMismatchException as exc:\n        raise MissingPrivateKeyError('A private key is missing!') from exc\n\n    return signed_transaction.to_dict()", "response": "Fulfills the given transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize the given operation string.", "response": "def _normalize_operation(operation):\n    \"\"\"Normalizes the given operation string. For now, this simply means\n    converting the given string to uppercase, looking it up in\n    :attr:`~.ops_map`, and returning the corresponding class if\n    present.\n\n    Args:\n        operation (str): The operation string to convert.\n\n    Returns:\n        The class corresponding to the given string,\n        :class:`~.CreateOperation` or :class:`~TransferOperation`.\n\n        .. important:: If the :meth:`str.upper` step, or the\n            :attr:`~.ops_map` lookup fails, the given ``operation``\n            argument is returned.\n\n    \"\"\"\n    try:\n        operation = operation.upper()\n    except AttributeError:\n        pass\n\n    try:\n        operation = ops_map[operation]()\n    except KeyError:\n        pass\n\n    return operation"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the given node url", "response": "def normalize_url(node):\n    \"\"\"Normalizes the given node url\"\"\"\n    if not node:\n        node = DEFAULT_NODE\n    elif '://' not in node:\n        node = '//{}'.format(node)\n    parts = urlparse(node, scheme='http', allow_fragments=False)\n    port = parts.port if parts.port else _get_default_port(parts.scheme)\n    netloc = '{}:{}'.format(parts.hostname, port)\n    return urlunparse((parts.scheme, netloc, parts.path, '', '', ''))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_node(node, headers=None):\n    headers = {} if headers is None else headers\n    if isinstance(node, str):\n        url = normalize_url(node)\n        return {'endpoint': url, 'headers': headers}\n\n    url = normalize_url(node['endpoint'])\n    node_headers = node.get('headers', {})\n    return {'endpoint': url, 'headers': {**headers, **node_headers}}", "response": "Normalizes given node as str or dict with headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize given dict or array of driver nodes", "response": "def normalize_nodes(*nodes, headers=None):\n    \"\"\"Normalizes given dict or array of driver nodes\"\"\"\n    if not nodes:\n        return (normalize_node(DEFAULT_NODE, headers),)\n\n    normalized_nodes = ()\n    for node in nodes:\n        normalized_nodes += (normalize_node(node, headers),)\n    return normalized_nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self, method, *, path=None, json=None,\n                params=None, headers=None, timeout=None,\n                backoff_cap=None, **kwargs):\n        \"\"\"Performs an HTTP request with the given parameters.\n\n           Implements exponential backoff.\n\n           If `ConnectionError` occurs, a timestamp equal to now +\n           the default delay (`BACKOFF_DELAY`) is assigned to the object.\n           The timestamp is in UTC. Next time the function is called, it either\n           waits till the timestamp is passed or raises `TimeoutError`.\n\n           If `ConnectionError` occurs two or more times in a row,\n           the retry count is incremented and the new timestamp is calculated\n           as now + the default delay multiplied by two to the power of the\n           number of retries.\n\n           If a request is successful, the backoff timestamp is removed,\n           the retry count is back to zero.\n\n        Args:\n            method (str): HTTP method (e.g.: ``'GET'``).\n            path (str): API endpoint path (e.g.: ``'/transactions'``).\n            json (dict): JSON data to send along with the request.\n            params (dict): Dictionary of URL (query) parameters.\n            headers (dict): Optional headers to pass to the request.\n            timeout (int): Optional timeout in seconds.\n            backoff_cap (int): The maximal allowed backoff delay in seconds\n                               to be assigned to a node.\n            kwargs: Optional keyword arguments.\n\n        \"\"\"\n        backoff_timedelta = self.get_backoff_timedelta()\n\n        if timeout is not None and timeout < backoff_timedelta:\n            raise TimeoutError\n\n        if backoff_timedelta > 0:\n            time.sleep(backoff_timedelta)\n\n        connExc = None\n        timeout = timeout if timeout is None else timeout - backoff_timedelta\n        try:\n            response = self._request(\n                method=method,\n                timeout=timeout,\n                url=self.node_url + path if path else self.node_url,\n                json=json,\n                params=params,\n                headers=headers,\n                **kwargs,\n            )\n        except ConnectionError as err:\n            connExc = err\n            raise err\n        finally:\n            self.update_backoff_time(success=connExc is None,\n                                     backoff_cap=backoff_cap)\n        return response", "response": "Performs an HTTP request to the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pick(self, connections):\n        if len(connections) == 1:\n            return connections[0]\n\n        def key(conn):\n            return (datetime.min\n                    if conn.backoff_time is None\n                    else conn.backoff_time)\n\n        return min(*connections, key=key)", "response": "Picks a connection with the earliest backoff time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward_request(self, method, path=None,\n                        json=None, params=None, headers=None):\n        \"\"\"Makes HTTP requests to the configured nodes.\n\n           Retries connection errors\n           (e.g. DNS failures, refused connection, etc).\n           A user may choose to retry other errors\n           by catching the corresponding\n           exceptions and retrying `forward_request`.\n\n           Exponential backoff is implemented individually for each node.\n           Backoff delays are expressed as timestamps stored on the object and\n           they are not reset in between multiple function calls.\n\n           Times out when `self.timeout` is expired, if not `None`.\n\n        Args:\n            method (str): HTTP method name (e.g.: ``'GET'``).\n            path (str): Path to be appended to the base url of a node. E.g.:\n                ``'/transactions'``).\n            json (dict): Payload to be sent with the HTTP request.\n            params (dict)): Dictionary of URL (query) parameters.\n            headers (dict): Optional headers to pass to the request.\n\n        Returns:\n            dict: Result of :meth:`requests.models.Response.json`\n\n        \"\"\"\n        error_trace = []\n        timeout = self.timeout\n        backoff_cap = NO_TIMEOUT_BACKOFF_CAP if timeout is None \\\n            else timeout / 2\n        while timeout is None or timeout > 0:\n            connection = self.connection_pool.get_connection()\n\n            start = time()\n            try:\n                response = connection.request(\n                    method=method,\n                    path=path,\n                    params=params,\n                    json=json,\n                    headers=headers,\n                    timeout=timeout,\n                    backoff_cap=backoff_cap,\n                )\n            except ConnectionError as err:\n                error_trace.append(err)\n                continue\n            else:\n                return response.data\n            finally:\n                elapsed = time() - start\n                if timeout is not None:\n                    timeout -= elapsed\n\n        raise TimeoutError(error_trace)", "response": "Makes HTTP requests to the configured nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_all_keys(obj_name, obj, validation_fun):\n    for key, value in obj.items():\n        validation_fun(obj_name, key)\n        if isinstance(value, dict):\n            validate_all_keys(obj_name, value, validation_fun)", "response": "Validate all keys in obj by using validation_fun."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_all_values_for_key(obj, key, validation_fun):\n    for vkey, value in obj.items():\n        if vkey == key:\n            validation_fun(value)\n        elif isinstance(value, dict):\n            validate_all_values_for_key(value, key, validation_fun)", "response": "Validate value for all occurrence of key in obj using validation_fun."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a UnspentOutput object containing the outputs of this transaction in a data structure containing relevant information for storing them in the UTXO set and performing validation.", "response": "def unspent_outputs(self):\n        \"\"\"UnspentOutput: The outputs of this transaction, in a data\n        structure containing relevant information for storing them in\n        a UTXO set, and performing validation.\n        \"\"\"\n        if self.operation == Transaction.CREATE:\n            self._asset_id = self._id\n        elif self.operation == Transaction.TRANSFER:\n            self._asset_id = self.asset['id']\n        return (UnspentOutput(\n            transaction_id=self._id,\n            output_index=output_index,\n            amount=output.amount,\n            asset_id=self._asset_id,\n            condition_uri=output.fulfillment.condition_uri,\n        ) for output_index, output in enumerate(self.outputs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the Inputs in the Transaction against given ArcGIS Outputs.", "response": "def inputs_valid(self, outputs=None):\n        \"\"\"Validates the Inputs in the Transaction against given\n        Outputs.\n\n            Note:\n                Given a `CREATE` Transaction is passed,\n                dummy values for Outputs are submitted for validation that\n                evaluate parts of the validation-checks to `True`.\n\n            Args:\n                outputs (:obj:`list` of :class:`~bigchaindb.common.\n                    transaction.Output`): A list of Outputs to check the\n                    Inputs against.\n\n            Returns:\n                bool: If all Inputs are valid.\n        \"\"\"\n        if self.operation == Transaction.CREATE:\n            # NOTE: Since in the case of a `CREATE`-transaction we do not have\n            #       to check for outputs, we're just submitting dummy\n            #       values to the actual method. This simplifies it's logic\n            #       greatly, as we do not have to check against `None` values.\n            return self._inputs_valid(['dummyvalue'\n                                       for _ in self.inputs])\n        elif self.operation == Transaction.TRANSFER:\n            return self._inputs_valid([output.fulfillment.condition_uri\n                                       for output in outputs])\n        else:\n            allowed_ops = ', '.join(self.__class__.ALLOWED_OPERATIONS)\n            raise TypeError('`operation` must be one of {}'\n                            .format(allowed_ops))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _input_valid(input_, operation, message, output_condition_uri=None):\n        ccffill = input_.fulfillment\n        try:\n            parsed_ffill = Fulfillment.from_uri(ccffill.serialize_uri())\n        except (TypeError, ValueError,\n                ParsingError, ASN1DecodeError, ASN1EncodeError):\n            return False\n\n        if operation == Transaction.CREATE:\n            # NOTE: In the case of a `CREATE` transaction, the\n            #       output is always valid.\n            output_valid = True\n        else:\n            output_valid = output_condition_uri == ccffill.condition_uri\n\n        message = sha3_256(message.encode())\n        if input_.fulfills:\n            message.update('{}{}'.format(\n                input_.fulfills.txid, input_.fulfills.output).encode())\n\n        # NOTE: We pass a timestamp to `.validate`, as in case of a timeout\n        #       condition we'll have to validate against it\n\n        # cryptoconditions makes no assumptions of the encoding of the\n        # message to sign or verify. It only accepts bytestrings\n        ffill_valid = parsed_ffill.validate(message=message.digest())\n        return output_valid and ffill_valid", "response": "Validates a single Input against a single Output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the ID of a transaction.", "response": "def validate_id(tx_body):\n        \"\"\"Validate the transaction ID of a transaction\n\n            Args:\n                tx_body (dict): The Transaction to be transformed.\n        \"\"\"\n        # NOTE: Remove reference to avoid side effects\n        tx_body = deepcopy(tx_body)\n        try:\n            proposed_tx_id = tx_body['id']\n        except KeyError:\n            raise InvalidHash('No transaction id found!')\n\n        tx_body['id'] = None\n\n        tx_body_serialized = Transaction._to_str(tx_body)\n        valid_tx_id = Transaction._to_hash(tx_body_serialized)\n\n        if proposed_tx_id != valid_tx_id:\n            err_msg = (\"The transaction's id '{}' isn't equal to \"\n                       \"the hash of its body, i.e. it's not valid.\")\n            raise InvalidHash(err_msg.format(proposed_tx_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a Python dictionary to a Transaction object.", "response": "def from_dict(cls, tx):\n        \"\"\"Transforms a Python dictionary to a Transaction object.\n\n            Args:\n                tx_body (dict): The Transaction to be transformed.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n        inputs = [Input.from_dict(input_) for input_ in tx['inputs']]\n        outputs = [Output.from_dict(output) for output in tx['outputs']]\n        return cls(tx['operation'], tx['asset'], inputs, outputs,\n                   tx['metadata'], tx['version'], hash_id=tx['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a dictionary to a query string.", "response": "def dict2query(dictionary):\n    \"\"\"\n    We want post vars of form:\n    {'foo': 'bar', 'nested': {'a': 'b', 'c': 'd'}}\n    to become:\n    foo=bar&nested[a]=b&nested[c]=d\n    \"\"\"\n    query = []\n    encoders = {dict: _dictionary_encoder}\n    for k, v in dictionary.iteritems():\n        if v.__class__ in encoders:\n            nested_query = encoders[v.__class__](k, v)\n            query += nested_query\n        else:\n            key = to_utf8(k)\n            value = to_utf8(v)\n            query.append('{}={}'.format(key, value))\n\n    return '&'.join(query)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self):\n\n        self.found_visible = False\n\n        is_multi_quote_header = self.MULTI_QUOTE_HDR_REGEX_MULTILINE.search(self.text)\n        if is_multi_quote_header:\n            self.text = self.MULTI_QUOTE_HDR_REGEX.sub(is_multi_quote_header.groups()[0].replace('\\n', ''), self.text)\n\n        # Fix any outlook style replies, with the reply immediately above the signature boundary line\n        #   See email_2_2.txt for an example\n        self.text = re.sub('([^\\n])(?=\\n ?[_-]{7,})', '\\\\1\\n', self.text, re.MULTILINE)\n\n        self.lines = self.text.split('\\n')\n        self.lines.reverse()\n\n        for line in self.lines:\n            self._scan_line(line)\n\n        self._finish_fragment()\n\n        self.fragments.reverse()\n\n        return self", "response": "Reads the text and returns the new instance of this class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string containing the reply message within the email.", "response": "def reply(self):\n        \"\"\" Captures reply message within email\n        \"\"\"\n        reply = []\n        for f in self.fragments:\n            if not (f.hidden or f.quoted):\n                reply.append(f.content)\n        return '\\n'.join(reply)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreviews each line in email message and determines fragment type", "response": "def _scan_line(self, line):\n        \"\"\" Reviews each line in email message and determines fragment type\n\n            line - a row of text from an email message\n        \"\"\"\n        is_quote_header = self.QUOTE_HDR_REGEX.match(line) is not None\n        is_quoted = self.QUOTED_REGEX.match(line) is not None\n        is_header = is_quote_header or self.HEADER_REGEX.match(line) is not None\n\n        if self.fragment and len(line.strip()) == 0:\n            if self.SIG_REGEX.match(self.fragment.lines[-1].strip()):\n                self.fragment.signature = True\n                self._finish_fragment()\n\n        if self.fragment \\\n                and ((self.fragment.headers == is_header and self.fragment.quoted == is_quoted) or\n                         (self.fragment.quoted and (is_quote_header or len(line.strip()) == 0))):\n\n            self.fragment.lines.append(line)\n        else:\n            self._finish_fragment()\n            self.fragment = Fragment(is_quoted, line, headers=is_header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _finish_fragment(self):\n\n        if self.fragment:\n            self.fragment.finish()\n            if self.fragment.headers:\n                # Regardless of what's been seen to this point, if we encounter a headers fragment,\n                # all the previous fragments should be marked hidden and found_visible set to False.\n                self.found_visible = False\n                for f in self.fragments:\n                    f.hidden = True\n            if not self.found_visible:\n                if self.fragment.quoted \\\n                        or self.fragment.headers \\\n                        or self.fragment.signature \\\n                        or (len(self.fragment.content.strip()) == 0):\n\n                    self.fragment.hidden = True\n                else:\n                    self.found_visible = True\n            self.fragments.append(self.fragment)\n        self.fragment = None", "response": "Finishes the current fragment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new block of content with lines belonging to fragment.", "response": "def finish(self):\n        \"\"\" Creates block of content with lines\n            belonging to fragment.\n        \"\"\"\n        self.lines.reverse()\n        self._content = '\\n'.join(self.lines)\n        self.lines = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _offset_format_timestamp1(src_tstamp_str, src_format, dst_format,\n                              ignore_unparsable_time=True, context=None):\n    \"\"\"\n    Convert a source timeStamp string into a destination timeStamp string,\n    attempting to apply the correct offset if both the server and local\n    timeZone are recognized,or no offset at all if they aren't or if\n    tz_offset is false (i.e. assuming they are both in the same TZ).\n\n    @param src_tstamp_str: the STR value containing the timeStamp.\n    @param src_format: the format to use when parsing the local timeStamp.\n    @param dst_format: the format to use when formatting the resulting\n     timeStamp.\n    @param server_to_client: specify timeZone offset direction (server=src\n                             and client=dest if True, or client=src and\n                             server=dest if False)\n    @param ignore_unparsable_time: if True, return False if src_tstamp_str\n                                   cannot be parsed using src_format or\n                                   formatted using dst_format.\n    @return: destination formatted timestamp, expressed in the destination\n             timezone if possible and if tz_offset is true, or src_tstamp_str\n             if timezone offset could not be determined.\n    \"\"\"\n    if not src_tstamp_str:\n        return False\n    res = src_tstamp_str\n    if src_format and dst_format:\n        try:\n            # dt_value needs to be a datetime.datetime object\\\n            # (so notime.struct_time or mx.DateTime.DateTime here!)\n            dt_value = datetime.datetime.strptime(src_tstamp_str, src_format)\n            if context.get('tz', False):\n                try:\n                    import pytz\n                    src_tz = pytz.timezone(context['tz'])\n                    dst_tz = pytz.timezone('UTC')\n                    src_dt = src_tz.localize(dt_value, is_dst=True)\n                    dt_value = src_dt.astimezone(dst_tz)\n                except Exception:\n                    pass\n            res = dt_value.strftime(dst_format)\n        except Exception:\n            # Normal ways to end up here are if strptime or strftime failed\n            if not ignore_unparsable_time:\n                return False\n            pass\n    return res", "response": "Convert a source timeStamp string into a destination timeStamp string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbases on isroom status will be updated.", "response": "def isroom_change(self):\n        '''\n        Based on isroom, status will be updated.\n        ----------------------------------------\n        @param self: object pointer\n        '''\n        if self.isroom is False:\n            self.status = 'occupied'\n        if self.isroom is True:\n            self.status = 'available'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, vals):\n        if 'isroom' in vals and vals['isroom'] is False:\n            vals.update({'color': 2, 'status': 'occupied'})\n        if 'isroom'in vals and vals['isroom'] is True:\n            vals.update({'color': 5, 'status': 'available'})\n        ret_val = super(HotelRoom, self).write(vals)\n        return ret_val", "response": "Overrides orm write method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef onchange_dates(self):\n        '''\n        This method gives the duration between check in and checkout\n        if customer will leave only for some hour it would be considers\n        as a whole day.If customer will check in checkout for more or equal\n        hours, which configured in company as additional hours than it would\n        be consider as full days\n        --------------------------------------------------------------------\n        @param self: object pointer\n        @return: Duration and checkout_date\n        '''\n        configured_addition_hours = 0\n        wid = self.warehouse_id\n        whouse_com_id = wid or wid.company_id\n        if whouse_com_id:\n            configured_addition_hours = wid.company_id.additional_hours\n        myduration = 0\n        chckin = self.checkin_date\n        chckout = self.checkout_date\n        if chckin and chckout:\n            server_dt = DEFAULT_SERVER_DATETIME_FORMAT\n            chkin_dt = datetime.datetime.strptime(chckin, server_dt)\n            chkout_dt = datetime.datetime.strptime(chckout, server_dt)\n            dur = chkout_dt - chkin_dt\n            sec_dur = dur.seconds\n            if (not dur.days and not sec_dur) or (dur.days and not sec_dur):\n                myduration = dur.days\n            else:\n                myduration = dur.days + 1\n            # To calculate additional hours in hotel room as per minutes\n            if configured_addition_hours > 0:\n                additional_hours = abs((dur.seconds / 60) / 60)\n                if additional_hours >= configured_addition_hours:\n                    myduration += 1\n        self.duration = myduration\n        self.duration_dummy = self.duration", "response": "This method gives the duration between check in and checkout for some hours. If customer will leave only for some hour it would be considers as full days. If customer will leave only for some hour it would be considers as full days."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, vals, check=True):\n        if not 'service_lines' and 'folio_id' in vals:\n            tmp_room_lines = vals.get('room_lines', [])\n            vals['order_policy'] = vals.get('hotel_policy', 'manual')\n            vals.update({'room_lines': []})\n            folio_id = super(HotelFolio, self).create(vals)\n            for line in (tmp_room_lines):\n                line[2].update({'folio_id': folio_id})\n            vals.update({'room_lines': tmp_room_lines})\n            folio_id.write(vals)\n        else:\n            if not vals:\n                vals = {}\n            vals['name'] = self.env['ir.sequence'].next_by_code('hotel.folio')\n            vals['duration'] = vals.get('duration',\n                                        0.0) or vals.get('duration_dummy',\n                                                         0.0)\n            folio_id = super(HotelFolio, self).create(vals)\n            folio_room_line_obj = self.env['folio.room.line']\n            h_room_obj = self.env['hotel.room']\n            try:\n                for rec in folio_id:\n                    if not rec.reservation_id:\n                        for room_rec in rec.room_lines:\n                            prod = room_rec.product_id.name\n                            room_obj = h_room_obj.search([('name', '=',\n                                                           prod)])\n                            room_obj.write({'isroom': False})\n                            vals = {'room_id': room_obj.id,\n                                    'check_in': rec.checkin_date,\n                                    'check_out': rec.checkout_date,\n                                    'folio_id': rec.id,\n                                    }\n                            folio_room_line_obj.create(vals)\n            except:\n                for rec in folio_id:\n                    for room_rec in rec.room_lines:\n                        prod = room_rec.product_id.name\n                        room_obj = h_room_obj.search([('name', '=', prod)])\n                        room_obj.write({'isroom': False})\n                        vals = {'room_id': room_obj.id,\n                                'check_in': rec.checkin_date,\n                                'check_out': rec.checkout_date,\n                                'folio_id': rec.id,\n                                }\n                        folio_room_line_obj.create(vals)\n        return folio_id", "response": "Override the create method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, vals):\n        product_obj = self.env['product.product']\n        h_room_obj = self.env['hotel.room']\n        folio_room_line_obj = self.env['folio.room.line']\n        room_lst = []\n        room_lst1 = []\n        for rec in self:\n            for res in rec.room_lines:\n                room_lst1.append(res.product_id.id)\n            if vals and vals.get('duration_dummy', False):\n                vals['duration'] = vals.get('duration_dummy', 0.0)\n            else:\n                vals['duration'] = rec.duration\n            for folio_rec in rec.room_lines:\n                room_lst.append(folio_rec.product_id.id)\n            new_rooms = set(room_lst).difference(set(room_lst1))\n            if len(list(new_rooms)) != 0:\n                room_list = product_obj.browse(list(new_rooms))\n                for rm in room_list:\n                    room_obj = h_room_obj.search([('name', '=', rm.name)])\n                    room_obj.write({'isroom': False})\n                    vals = {'room_id': room_obj.id,\n                            'check_in': rec.checkin_date,\n                            'check_out': rec.checkout_date,\n                            'folio_id': rec.id,\n                            }\n                    folio_room_line_obj.create(vals)\n            if len(list(new_rooms)) == 0:\n                room_list_obj = product_obj.browse(room_lst1)\n                for rom in room_list_obj:\n                    room_obj = h_room_obj.search([('name', '=', rom.name)])\n                    room_obj.write({'isroom': False})\n                    room_vals = {'room_id': room_obj.id,\n                                 'check_in': rec.checkin_date,\n                                 'check_out': rec.checkout_date,\n                                 'folio_id': rec.id,\n                                 }\n                    folio_romline_rec = (folio_room_line_obj.search\n                                         ([('folio_id', '=', rec.id)]))\n                    folio_romline_rec.write(room_vals)\n        return super(HotelFolio, self).write(vals)", "response": "Override orm write method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_dates(self):\n        '''\n        This method is used to validate the checkin_date and checkout_date.\n        -------------------------------------------------------------------\n        @param self: object pointer\n        @return: raise warning depending on the validation\n        '''\n        if self.checkin_date >= self.checkout_date:\n                raise ValidationError(_('Room line Check In Date Should be \\\n                less than the Check Out Date!'))\n        if self.folio_id.date_order and self.checkin_date:\n            if self.checkin_date <= self.folio_id.date_order:\n                raise ValidationError(_('Room line check in date should be \\\n                greater than the current date.'))", "response": "This method is used to validate the checkin_date and checkout_date."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unlink(self):\n        sale_line_obj = self.env['sale.order.line']\n        fr_obj = self.env['folio.room.line']\n        for line in self:\n            if line.order_line_id:\n                sale_unlink_obj = (sale_line_obj.browse\n                                   ([line.order_line_id.id]))\n                for rec in sale_unlink_obj:\n                    room_obj = self.env['hotel.room'\n                                        ].search([('name', '=', rec.name)])\n                    if room_obj.id:\n                        folio_arg = [('folio_id', '=', line.folio_id.id),\n                                     ('room_id', '=', room_obj.id)]\n                        folio_room_line_myobj = fr_obj.search(folio_arg)\n                        if folio_room_line_myobj.id:\n                            folio_room_line_myobj.unlink()\n                            room_obj.write({'isroom': True,\n                                            'status': 'available'})\n                sale_unlink_obj.unlink()\n        return super(HotelFolioLine, self).unlink()", "response": "Overrides orm unlink method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef product_id_change(self):\n        '''\n -        @param self: object pointer\n -        '''\n        context = dict(self._context)\n        if not context:\n            context = {}\n        if context.get('folio', False):\n            if self.product_id and self.folio_id.partner_id:\n                self.name = self.product_id.name\n                self.price_unit = self.product_id.list_price\n                self.product_uom = self.product_id.uom_id\n                tax_obj = self.env['account.tax']\n                pr = self.product_id\n                self.price_unit = tax_obj._fix_tax_included_price(pr.price,\n                                                                  pr.taxes_id,\n                                                                  self.tax_id)\n        else:\n            if not self.product_id:\n                return {'domain': {'product_uom': []}}\n            val = {}\n            pr = self.product_id.with_context(\n                lang=self.folio_id.partner_id.lang,\n                partner=self.folio_id.partner_id.id,\n                quantity=val.get('product_uom_qty') or self.product_uom_qty,\n                date=self.folio_id.date_order,\n                pricelist=self.folio_id.pricelist_id.id,\n                uom=self.product_uom.id\n            )\n            p = pr.with_context(pricelist=self.order_id.pricelist_id.id).price\n            if self.folio_id.pricelist_id and self.folio_id.partner_id:\n                obj = self.env['account.tax']\n                val['price_unit'] = obj._fix_tax_included_price(p,\n                                                                pr.taxes_id,\n                                                                self.tax_id)", "response": "Change the product_id of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_change_checkout(self):\n        '''\n        When you change checkin_date or checkout_date it will checked it\n        and update the qty of hotel folio line\n        -----------------------------------------------------------------\n        @param self: object pointer\n        '''\n        configured_addition_hours = 0\n        fwhouse_id = self.folio_id.warehouse_id\n        fwc_id = fwhouse_id or fwhouse_id.company_id\n        if fwc_id:\n            configured_addition_hours = fwhouse_id.company_id.additional_hours\n        myduration = 0\n        if not self.checkin_date:\n            self.checkin_date = time.strftime(DEFAULT_SERVER_DATETIME_FORMAT)\n        if not self.checkout_date:\n            self.checkout_date = time.strftime(DEFAULT_SERVER_DATETIME_FORMAT)\n        chckin = self.checkin_date\n        chckout = self.checkout_date\n        if chckin and chckout:\n            server_dt = DEFAULT_SERVER_DATETIME_FORMAT\n            chkin_dt = datetime.datetime.strptime(chckin, server_dt)\n            chkout_dt = datetime.datetime.strptime(chckout, server_dt)\n            dur = chkout_dt - chkin_dt\n            sec_dur = dur.seconds\n            if (not dur.days and not sec_dur) or (dur.days and not sec_dur):\n                myduration = dur.days\n            else:\n                myduration = dur.days + 1\n#            To calculate additional hours in hotel room as per minutes\n            if configured_addition_hours > 0:\n                additional_hours = abs((dur.seconds / 60) / 60)\n                if additional_hours >= configured_addition_hours:\n                    myduration += 1\n        self.product_uom_qty = myduration\n        hotel_room_obj = self.env['hotel.room']\n        hotel_room_ids = hotel_room_obj.search([])\n        avail_prod_ids = []\n        for room in hotel_room_ids:\n            assigned = False\n            for rm_line in room.room_line_ids:\n                if rm_line.status != 'cancel':\n                    if(self.checkin_date <= rm_line.check_in <=\n                       self.checkout_date) or (self.checkin_date <=\n                                               rm_line.check_out <=\n                                               self.checkout_date):\n                        assigned = True\n                    elif (rm_line.check_in <= self.checkin_date <=\n                          rm_line.check_out) or (rm_line.check_in <=\n                                                 self.checkout_date <=\n                                                 rm_line.check_out):\n                        assigned = True\n            if not assigned:\n                avail_prod_ids.append(room.product_id.id)\n        domain = {'product_id': [('id', 'in', avail_prod_ids)]}\n        return {'domain': domain}", "response": "When you change checkin_date or checkout_date it will checked it and update the qty of the product uom of the folio line."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride orm create method. @param self: The object pointer @param vals: dictionary of fields value. @return: new record set for hotel service line.", "response": "def create(self, vals, check=True):\n        \"\"\"\n        Overrides orm create method.\n        @param self: The object pointer\n        @param vals: dictionary of fields value.\n        @return: new record set for hotel service line.\n        \"\"\"\n        if 'folio_id' in vals:\n            folio = self.env['hotel.folio'].browse(vals['folio_id'])\n            vals.update({'order_id': folio.order_id.id})\n        return super(HotelServiceLine, self).create(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding orm unlink method.", "response": "def unlink(self):\n        \"\"\"\n        Overrides orm unlink method.\n        @param self: The object pointer\n        @return: True/False.\n        \"\"\"\n        s_line_obj = self.env['sale.order.line']\n        for line in self:\n            if line.service_line_id:\n                sale_unlink_obj = s_line_obj.browse([line.service_line_id.id])\n                sale_unlink_obj.unlink()\n        return super(HotelServiceLine, self).unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides the orm write method.", "response": "def write(self, vals):\n        \"\"\"\n        Overrides orm write method.\n        @param self: The object pointer\n        @param vals: dictionary of fields value.\n        Update Hotel Room Reservation line history\"\"\"\n        reservation_line_obj = self.env['hotel.room.reservation.line']\n        room_obj = self.env['hotel.room']\n        prod_id = vals.get('product_id') or self.product_id.id\n        chkin = vals.get('checkin_date') or self.checkin_date\n        chkout = vals.get('checkout_date') or self.checkout_date\n        is_reserved = self.is_reserved\n        if prod_id and is_reserved:\n            prod_domain = [('product_id', '=', prod_id)]\n            prod_room = room_obj.search(prod_domain, limit=1)\n            if (self.product_id and self.checkin_date and self.checkout_date):\n                old_prd_domain = [('product_id', '=', self.product_id.id)]\n                old_prod_room = room_obj.search(old_prd_domain, limit=1)\n                if prod_room and old_prod_room:\n                    # Check for existing room lines.\n                    srch_rmline = [('room_id', '=', old_prod_room.id),\n                                   ('check_in', '=', self.checkin_date),\n                                   ('check_out', '=', self.checkout_date),\n                                   ]\n                    rm_lines = reservation_line_obj.search(srch_rmline)\n                    if rm_lines:\n                        rm_line_vals = {'room_id': prod_room.id,\n                                        'check_in': chkin,\n                                        'check_out': chkout}\n                        rm_lines.write(rm_line_vals)\n        return super(HotelFolioLineExt, self).write(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides orm unlink method.", "response": "def unlink(self):\n        \"\"\"\n        Overrides orm unlink method.\n        @param self: The object pointer\n        @return: True/False.\n        \"\"\"\n        for reserv_rec in self:\n            if reserv_rec.state != 'draft':\n                raise ValidationError(_('You cannot delete Reservation in %s\\\n                                         state.') % (reserv_rec.state))\n        return super(HotelReservation, self).unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck in and out dates are not greater than the current date.", "response": "def check_in_out_dates(self):\n        \"\"\"\n        When date_order is less then check-in date or\n        Checkout date should be greater than the check-in date.\n        \"\"\"\n        if self.checkout and self.checkin:\n            if self.checkin < self.date_order:\n                raise ValidationError(_('Check-in date should be greater than \\\n                                         the current date.'))\n            if self.checkout < self.checkin:\n                raise ValidationError(_('Check-out date should be greater \\\n                                         than Check-in date.'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_change_checkout(self):\n        '''\n        When you change checkout or checkin update dummy field\n        -----------------------------------------------------------\n        @param self: object pointer\n        @return: raise warning depending on the validation\n        '''\n        checkout_date = time.strftime(dt)\n        checkin_date = time.strftime(dt)\n        if not (checkout_date and checkin_date):\n            return {'value': {}}\n        delta = timedelta(days=1)\n        dat_a = time.strptime(checkout_date, dt)[:5]\n        addDays = datetime(*dat_a) + delta\n        self.dummy = addDays.strftime(dt)", "response": "When you change checkout or checkin update dummy field\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef onchange_partner_id(self):\n        '''\n        When you change partner_id it will update the partner_invoice_id,\n        partner_shipping_id and pricelist_id of the hotel reservation as well\n        ---------------------------------------------------------------------\n        @param self: object pointer\n        '''\n        if not self.partner_id:\n            self.partner_invoice_id = False\n            self.partner_shipping_id = False\n            self.partner_order_id = False\n        else:\n            addr = self.partner_id.address_get(['delivery', 'invoice',\n                                                'contact'])\n            self.partner_invoice_id = addr['invoice']\n            self.partner_order_id = addr['contact']\n            self.partner_shipping_id = addr['delivery']\n            self.pricelist_id = self.partner_id.property_product_pricelist.id", "response": "When you change the partner_id it will update the invoice_id shipping_id and pricelist_id of the hotel reservation as well\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, vals):\n        if not vals:\n            vals = {}\n        vals['reservation_no'] = self.env['ir.sequence'].\\\n            next_by_code('hotel.reservation') or 'New'\n        return super(HotelReservation, self).create(vals)", "response": "Overrides orm create method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confirmed_reservation(self):\n        reservation_line_obj = self.env['hotel.room.reservation.line']\n        vals = {}\n        for reservation in self:\n            reserv_checkin = datetime.strptime(reservation.checkin, dt)\n            reserv_checkout = datetime.strptime(reservation.checkout, dt)\n            room_bool = False\n            for line_id in reservation.reservation_line:\n                for room_id in line_id.reserve:\n                    if room_id.room_reservation_line_ids:\n                        for reserv in room_id.room_reservation_line_ids.\\\n                                search([('status', 'in', ('confirm', 'done')),\n                                        ('room_id', '=', room_id.id)]):\n                            check_in = datetime.strptime(reserv.check_in, dt)\n                            check_out = datetime.strptime(reserv.check_out, dt)\n                            if check_in <= reserv_checkin <= check_out:\n                                room_bool = True\n                            if check_in <= reserv_checkout <= check_out:\n                                room_bool = True\n                            if reserv_checkin <= check_in and \\\n                                    reserv_checkout >= check_out:\n                                room_bool = True\n                            mytime = \"%Y-%m-%d\"\n                            r_checkin = datetime.strptime(reservation.checkin,\n                                                          dt).date()\n                            r_checkin = r_checkin.strftime(mytime)\n                            r_checkout = datetime.\\\n                                strptime(reservation.checkout, dt).date()\n                            r_checkout = r_checkout.strftime(mytime)\n                            check_intm = datetime.strptime(reserv.check_in,\n                                                           dt).date()\n                            check_outtm = datetime.strptime(reserv.check_out,\n                                                            dt).date()\n                            check_intm = check_intm.strftime(mytime)\n                            check_outtm = check_outtm.strftime(mytime)\n                            range1 = [r_checkin, r_checkout]\n                            range2 = [check_intm, check_outtm]\n                            overlap_dates = self.check_overlap(*range1) \\\n                                & self.check_overlap(*range2)\n                            overlap_dates = [datetime.strftime(dates,\n                                                               '%d/%m/%Y') for\n                                             dates in overlap_dates]\n                            if room_bool:\n                                raise ValidationError(_('You tried to Confirm '\n                                                        'Reservation with room'\n                                                        ' those already '\n                                                        'reserved in this '\n                                                        'Reservation Period. '\n                                                        'Overlap Dates are '\n                                                        '%s') % overlap_dates)\n                            else:\n                                self.state = 'confirm'\n                                vals = {'room_id': room_id.id,\n                                        'check_in': reservation.checkin,\n                                        'check_out': reservation.checkout,\n                                        'state': 'assigned',\n                                        'reservation_id': reservation.id,\n                                        }\n                                room_id.write({'isroom': False,\n                                               'status': 'occupied'})\n                        else:\n                            self.state = 'confirm'\n                            vals = {'room_id': room_id.id,\n                                    'check_in': reservation.checkin,\n                                    'check_out': reservation.checkout,\n                                    'state': 'assigned',\n                                    'reservation_id': reservation.id,\n                                    }\n                            room_id.write({'isroom': False,\n                                           'status': 'occupied'})\n                    else:\n                        self.state = 'confirm'\n                        vals = {'room_id': room_id.id,\n                                'check_in': reservation.checkin,\n                                'check_out': reservation.checkout,\n                                'state': 'assigned',\n                                'reservation_id': reservation.id,\n                                }\n                        room_id.write({'isroom': False,\n                                       'status': 'occupied'})\n                    reservation_line_obj.create(vals)\n        return True", "response": "This method creates a new record set for the given hotel room reservation line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_reservation_maill(self):\n        '''\n        This function opens a window to compose an email,\n        template message loaded by default.\n        @param self: object pointer\n        '''\n        assert len(self._ids) == 1, 'This is for a single id at a time.'\n        ir_model_data = self.env['ir.model.data']\n        try:\n            template_id = (ir_model_data.get_object_reference\n                           ('hotel_reservation',\n                            'mail_template_hotel_reservation')[1])\n        except ValueError:\n            template_id = False\n        try:\n            compose_form_id = (ir_model_data.get_object_reference\n                               ('mail',\n                                'email_compose_message_wizard_form')[1])\n        except ValueError:\n            compose_form_id = False\n        ctx = dict()\n        ctx.update({\n            'default_model': 'hotel.reservation',\n            'default_res_id': self._ids[0],\n            'default_use_template': bool(template_id),\n            'default_template_id': template_id,\n            'default_composition_mode': 'comment',\n            'force_send': True,\n            'mark_so_as_sent': True\n        })\n        return {\n            'type': 'ir.actions.act_window',\n            'view_type': 'form',\n            'view_mode': 'form',\n            'res_model': 'mail.compose.message',\n            'views': [(compose_form_id, 'form')],\n            'view_id': compose_form_id,\n            'target': 'new',\n            'context': ctx,\n            'force_send': True\n        }", "response": "This function opens a window to compose an email using the template message loaded by default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_folio(self):\n        hotel_folio_obj = self.env['hotel.folio']\n        room_obj = self.env['hotel.room']\n        for reservation in self:\n            folio_lines = []\n            checkin_date = reservation['checkin']\n            checkout_date = reservation['checkout']\n            if not self.checkin < self.checkout:\n                raise ValidationError(_('Checkout date should be greater \\\n                                         than the Check-in date.'))\n            duration_vals = (self.onchange_check_dates\n                             (checkin_date=checkin_date,\n                              checkout_date=checkout_date, duration=False))\n            duration = duration_vals.get('duration') or 0.0\n            folio_vals = {\n                'date_order': reservation.date_order,\n                'warehouse_id': reservation.warehouse_id.id,\n                'partner_id': reservation.partner_id.id,\n                'pricelist_id': reservation.pricelist_id.id,\n                'partner_invoice_id': reservation.partner_invoice_id.id,\n                'partner_shipping_id': reservation.partner_shipping_id.id,\n                'checkin_date': reservation.checkin,\n                'checkout_date': reservation.checkout,\n                'duration': duration,\n                'reservation_id': reservation.id,\n                'service_lines': reservation['folio_id']\n            }\n            for line in reservation.reservation_line:\n                for r in line.reserve:\n                    folio_lines.append((0, 0, {\n                        'checkin_date': checkin_date,\n                        'checkout_date': checkout_date,\n                        'product_id': r.product_id and r.product_id.id,\n                        'name': reservation['reservation_no'],\n                        'price_unit': r.list_price,\n                        'product_uom_qty': duration,\n                        'is_reserved': True}))\n                    res_obj = room_obj.browse([r.id])\n                    res_obj.write({'status': 'occupied', 'isroom': False})\n            folio_vals.update({'room_lines': folio_lines})\n            folio = hotel_folio_obj.create(folio_vals)\n            if folio:\n                for rm_line in folio.room_lines:\n                    rm_line.product_id_change()\n            self._cr.execute('insert into hotel_folio_reservation_rel'\n                             '(order_id, invoice_id) values (%s,%s)',\n                             (reservation.id, folio.id))\n            self.state = 'done'\n        return True", "response": "This method creates a new hotel folio record set for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_change_categ(self):\n        '''\n        When you change categ_id it check checkin and checkout are\n        filled or not if not then raise warning\n        -----------------------------------------------------------\n        @param self: object pointer\n        '''\n        hotel_room_obj = self.env['hotel.room']\n        hotel_room_ids = hotel_room_obj.search([('categ_id', '=',\n                                                 self.categ_id.id)])\n        room_ids = []\n        if not self.line_id.checkin:\n            raise ValidationError(_('Before choosing a room,\\n You have to \\\n                                     select a Check in date or a Check out \\\n                                     date in the reservation form.'))\n        for room in hotel_room_ids:\n            assigned = False\n            for line in room.room_reservation_line_ids:\n                if line.status != 'cancel':\n                    if(self.line_id.checkin <= line.check_in <=\n                        self.line_id.checkout) or (self.line_id.checkin <=\n                                                   line.check_out <=\n                                                   self.line_id.checkout):\n                        assigned = True\n                    elif(line.check_in <= self.line_id.checkin <=\n                         line.check_out) or (line.check_in <=\n                                             self.line_id.checkout <=\n                                             line.check_out):\n                        assigned = True\n            for rm_line in room.room_line_ids:\n                if rm_line.status != 'cancel':\n                    if(self.line_id.checkin <= rm_line.check_in <=\n                       self.line_id.checkout) or (self.line_id.checkin <=\n                                                  rm_line.check_out <=\n                                                  self.line_id.checkout):\n                        assigned = True\n                    elif(rm_line.check_in <= self.line_id.checkin <=\n                         rm_line.check_out) or (rm_line.check_in <=\n                                                self.line_id.checkout <=\n                                                rm_line.check_out):\n                        assigned = True\n            if not assigned:\n                room_ids.append(room.id)\n        domain = {'reserve': [('id', 'in', room_ids)]}\n        return {'domain': domain}", "response": "When you change a categ_id it check in and checkout are filled or not then raise warning\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides orm unlink method.", "response": "def unlink(self):\n        \"\"\"\n        Overrides orm unlink method.\n        @param self: The object pointer\n        @return: True/False.\n        \"\"\"\n        hotel_room_reserv_line_obj = self.env['hotel.room.reservation.line']\n        for reserv_rec in self:\n            for rec in reserv_rec.reserve:\n                hres_arg = [('room_id', '=', rec.id),\n                            ('reservation_id', '=', reserv_rec.line_id.id)]\n                myobj = hotel_room_reserv_line_obj.search(hres_arg)\n                if myobj.ids:\n                    rec.write({'isroom': True, 'status': 'available'})\n                    myobj.unlink()\n        return super(HotelReservationLine, self).unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride orm unlink method.", "response": "def unlink(self):\n        \"\"\"\n        Overrides orm unlink method.\n        @param self: The object pointer\n        @return: True/False.\n        \"\"\"\n        for room in self:\n            for reserv_line in room.room_reservation_line_ids:\n                if reserv_line.status == 'confirm':\n                    raise ValidationError(_('User is not able to delete the \\\n                                            room after the room in %s state \\\n                                            in reservation')\n                                          % (reserv_line.status))\n        return super(HotelRoom, self).unlink()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cron_room_line(self):\n        reservation_line_obj = self.env['hotel.room.reservation.line']\n        folio_room_line_obj = self.env['folio.room.line']\n        now = datetime.now()\n        curr_date = now.strftime(dt)\n        for room in self.search([]):\n            reserv_line_ids = [reservation_line.id for\n                               reservation_line in\n                               room.room_reservation_line_ids]\n            reserv_args = [('id', 'in', reserv_line_ids),\n                           ('check_in', '<=', curr_date),\n                           ('check_out', '>=', curr_date)]\n            reservation_line_ids = reservation_line_obj.search(reserv_args)\n            rooms_ids = [room_line.ids for room_line in room.room_line_ids]\n            rom_args = [('id', 'in', rooms_ids),\n                        ('check_in', '<=', curr_date),\n                        ('check_out', '>=', curr_date)]\n            room_line_ids = folio_room_line_obj.search(rom_args)\n            status = {'isroom': True, 'color': 5}\n            if reservation_line_ids.ids:\n                status = {'isroom': False, 'color': 2}\n            room.write(status)\n            if room_line_ids.ids:\n                status = {'isroom': False, 'color': 2}\n            room.write(status)\n            if reservation_line_ids.ids and room_line_ids.ids:\n                raise ValidationError(_('Please Check Rooms Status \\\n                                         for %s.' % (room.name)))\n        return True", "response": "This method is for scheduler to check status of the hotel room reservation line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_get(self, fields):\n        if self._context is None:\n            self._context = {}\n        res = super(RoomReservationSummary, self).default_get(fields)\n        # Added default datetime as today and date to as today + 30.\n        from_dt = datetime.today()\n        dt_from = from_dt.strftime(dt)\n        to_dt = from_dt + relativedelta(days=30)\n        dt_to = to_dt.strftime(dt)\n        res.update({'date_from': dt_from, 'date_to': dt_to})\n\n        if not self.date_from and self.date_to:\n            date_today = datetime.datetime.today()\n            first_day = datetime.datetime(date_today.year,\n                                          date_today.month, 1, 0, 0, 0)\n            first_temp_day = first_day + relativedelta(months=1)\n            last_temp_day = first_temp_day - relativedelta(days=1)\n            last_day = datetime.datetime(last_temp_day.year,\n                                         last_temp_day.month,\n                                         last_temp_day.day, 23, 59, 59)\n            date_froms = first_day.strftime(dt)\n            date_ends = last_day.strftime(dt)\n            res.update({'date_from': date_froms, 'date_to': date_ends})\n        return res", "response": "Get default values for the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_change_check_out(self):\n        '''\n        When you change checkout or checkin it will check whether\n        Checkout date should be greater than Checkin date\n        and update dummy field\n        -----------------------------------------------------------\n        @param self: object pointer\n        @return: raise warning depending on the validation\n        '''\n        if self.check_out and self.check_in:\n            if self.check_out < self.check_in:\n                raise ValidationError(_('Checkout date should be greater \\\n                                         than Checkin date.'))", "response": "Check whether the current checkout date is greater than Checkin date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_get(self, fields):\n        if self._context is None:\n            self._context = {}\n        res = super(QuickRoomReservation, self).default_get(fields)\n        if self._context:\n            keys = self._context.keys()\n            if 'date' in keys:\n                res.update({'check_in': self._context['date']})\n            if 'room_id' in keys:\n                roomid = self._context['room_id']\n                res.update({'room_id': int(roomid)})\n        return res", "response": "Get default values for the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclassify the contents of a directory as files and directories.", "response": "def _classify_directory_contents(filesystem, root):\n    \"\"\"Classify contents of a directory as files/directories.\n\n    Args:\n        filesystem: The fake filesystem used for implementation\n        root: (str) Directory to examine.\n\n    Returns:\n        (tuple) A tuple consisting of three values: the directory examined,\n        a list containing all of the directory entries, and a list\n        containing all of the non-directory entries.\n        (This is the same format as returned by the `os.walk` generator.)\n\n    Raises:\n        Nothing on its own, but be ready to catch exceptions generated by\n        underlying mechanisms like `os.listdir`.\n    \"\"\"\n    dirs = []\n    files = []\n    for entry in filesystem.listdir(root):\n        if filesystem.isdir(filesystem.joinpaths(root, entry)):\n            dirs.append(entry)\n        else:\n            files.append(entry)\n    return root, dirs, files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform an os.walk operation over the fake filesystem. Args: filesystem: The fake filesystem used for implementation top: The root directory from which to begin walk. topdown: Determines whether to return the tuples with the root as the first entry (`True`) or as the last, after all the child directory tuples (`False`). onerror: If not `None`, function which will be called to handle the `os.error` instance provided when `os.listdir()` fails. followlinks: If `True`, symbolic links are followed. Yields: (path, directories, nondirectories) for top and each of its subdirectories. See the documentation for the builtin os module for further details.", "response": "def walk(filesystem, top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Perform an os.walk operation over the fake filesystem.\n\n    Args:\n        filesystem: The fake filesystem used for implementation\n        top: The root directory from which to begin walk.\n        topdown: Determines whether to return the tuples with the root as\n            the first entry (`True`) or as the last, after all the child\n            directory tuples (`False`).\n      onerror: If not `None`, function which will be called to handle the\n            `os.error` instance provided when `os.listdir()` fails.\n      followlinks: If `True`, symbolic links are followed.\n\n    Yields:\n        (path, directories, nondirectories) for top and each of its\n        subdirectories.  See the documentation for the builtin os module\n        for further details.\n    \"\"\"\n\n    def do_walk(top_dir, top_most=False):\n        top_dir = filesystem.normpath(top_dir)\n        if not top_most and not followlinks and filesystem.islink(top_dir):\n            return\n        try:\n            top_contents = _classify_directory_contents(filesystem, top_dir)\n        except OSError as exc:\n            top_contents = None\n            if onerror is not None:\n                onerror(exc)\n\n        if top_contents is not None:\n            if topdown:\n                yield top_contents\n\n            for directory in top_contents[1]:\n                if not followlinks and filesystem.islink(directory):\n                    continue\n                for contents in do_walk(filesystem.joinpaths(top_dir,\n                                                             directory)):\n                    yield contents\n\n            if not topdown:\n                yield top_contents\n\n    return do_walk(top, top_most=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the inode number of the entry.", "response": "def inode(self):\n        \"\"\"Return the inode number of the entry.\"\"\"\n        if self._inode is None:\n            self.stat(follow_symlinks=False)\n        return self._inode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(self, follow_symlinks=True):\n        if follow_symlinks:\n            if self._statresult_symlink is None:\n                file_object = self._filesystem.resolve(self.path)\n                if self._filesystem.is_windows_fs:\n                    file_object.st_nlink = 0\n                self._statresult_symlink = file_object.stat_result.copy()\n            return self._statresult_symlink\n\n        if self._statresult is None:\n            file_object = self._filesystem.lresolve(self.path)\n            self._inode = file_object.st_ino\n            if self._filesystem.is_windows_fs:\n                file_object.st_nlink = 0\n            self._statresult = file_object.stat_result.copy()\n        return self._statresult", "response": "Return a stat_result object for this entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a walk over the fake filesystem.", "response": "def walk(self, top, topdown=True, onerror=None, followlinks=False):\n        \"\"\"Perform a walk operation over the fake filesystem.\n\n        Args:\n            top: The root directory from which to begin walk.\n            topdown: Determines whether to return the tuples with the root as\n                the first entry (`True`) or as the last, after all the child\n                directory tuples (`False`).\n          onerror: If not `None`, function which will be called to handle the\n                `os.error` instance provided when `os.listdir()` fails.\n          followlinks: If `True`, symbolic links are followed.\n\n        Yields:\n            (path, directories, nondirectories) for top and each of its\n            subdirectories.  See the documentation for the builtin os module\n            for further details.\n        \"\"\"\n        return walk(self.filesystem, top, topdown, onerror, followlinks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(clss, func, deprecated_name):\n\n        @Deprecator(func.__name__, deprecated_name)\n        def _old_function(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        setattr(clss, deprecated_name, _old_function)", "response": "Add the deprecated version of a member function to the given class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the fake module with the fake file system.", "response": "def init_module(filesystem):\n    \"\"\"Initializes the fake module with the fake file system.\"\"\"\n    # pylint: disable=protected-access\n    FakePath.filesystem = filesystem\n    FakePathlibModule.PureWindowsPath._flavour = _FakeWindowsFlavour(\n        filesystem)\n    FakePathlibModule.PurePosixPath._flavour = _FakePosixFlavour(filesystem)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef splitroot(self, path, sep=None):\n        if sep is None:\n            sep = self.filesystem.path_separator\n        if self.filesystem.is_windows_fs:\n            return self._splitroot_with_drive(path, sep)\n        return self._splitroot_posix(path, sep)", "response": "Split path into drive root and rest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the lower - case version of parts for a Windows filesystem.", "response": "def casefold_parts(self, parts):\n        \"\"\"Return the lower-case version of parts for a Windows filesystem.\"\"\"\n        if self.filesystem.is_windows_fs:\n            return [p.lower() for p in parts]\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the path absolute resolving any symlinks.", "response": "def resolve(self, path, strict):\n        \"\"\"Make the path absolute, resolving any symlinks.\"\"\"\n        if self.filesystem.is_windows_fs:\n            return self._resolve_windows(path, strict)\n        return self._resolve_posix(path, strict)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the path absolute resolving all symlinks on the way and also normalizing it.", "response": "def resolve(self, strict=None):\n        \"\"\"Make the path absolute, resolving all symlinks on the way and also\n        normalizing it (for example turning slashes into backslashes\n        under Windows).\n\n        Args:\n            strict: If False (default) no exception is raised if the path\n                does not exist.\n                New in Python 3.6.\n\n        Raises:\n            IOError: if the path doesn't exist (strict=True or Python < 3.6)\n        \"\"\"\n        if sys.version_info >= (3, 6) or pathlib2:\n            if strict is None:\n                strict = False\n        else:\n            if strict is not None:\n                raise TypeError(\n                    \"resolve() got an unexpected keyword argument 'strict'\")\n            strict = True\n        if self._closed:\n            self._raise_closed()\n        path = self._flavour.resolve(self, strict=strict)\n        if path is None:\n            self.stat()\n            path = str(self.absolute())\n        path = self.filesystem.absnormpath(path)\n        return FakePath(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None):\n        \"\"\"Open the file pointed by this path and return a fake file object.\n\n        Raises:\n            IOError: if the target object is a directory, the path is invalid\n                or permission is denied.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        return FakeFileOpen(self.filesystem, use_io=True)(\n            self._path(), mode, buffering, encoding, errors, newline)", "response": "Open the file pointed by this path and return a fake file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef touch(self, mode=0o666, exist_ok=True):\n        if self._closed:\n            self._raise_closed()\n        if self.exists():\n            if exist_ok:\n                self.filesystem.utime(self._path(), None)\n            else:\n                self.filesystem.raise_os_error(errno.EEXIST, self._path())\n        else:\n            fake_file = self.open('w')\n            fake_file.close()\n            self.chmod(mode)", "response": "Create a fake file for the path with the given access mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_module(old):\n    saved = sys.modules.pop(old.__name__, None)\n    new = __import__(old.__name__)\n    sys.modules[old.__name__] = saved\n    return new", "response": "Recompiles and creates new module object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the contents as string with the original encoding.", "response": "def contents(self):\n        \"\"\"Return the contents as string with the original encoding.\"\"\"\n        if not IS_PY2 and isinstance(self.byte_contents, bytes):\n            return self.byte_contents.decode(\n                self.encoding or locale.getpreferredencoding(False),\n                errors=self.errors)\n        return self.byte_contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the self. st_size attribute and replaces self. content with None.", "response": "def set_large_file_size(self, st_size):\n        \"\"\"Sets the self.st_size attribute and replaces self.content with None.\n\n        Provided specifically to simulate very large files without regards\n        to their content (which wouldn't fit in memory).\n        Note that read/write operations with such a file raise\n            :py:class:`FakeLargeFileIoException`.\n\n        Args:\n          st_size: (int) The desired file size\n\n        Raises:\n          IOError: if the st_size is not a non-negative integer,\n                   or if st_size exceeds the available file system space\n        \"\"\"\n        self._check_positive_int(st_size)\n        if self.st_size:\n            self.size = 0\n        if self.filesystem:\n            self.filesystem.change_disk_usage(st_size, self.name, self.st_dev)\n        self.st_size = st_size\n        self._byte_contents = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the file contents and size.", "response": "def _set_initial_contents(self, contents):\n        \"\"\"Sets the file contents and size.\n           Called internally after initial file creation.\n\n        Args:\n            contents: string, new content of file.\n\n        Returns:\n            True if the contents have been changed.\n\n        Raises:\n              IOError: if the st_size is not a non-negative integer,\n                   or if st_size exceeds the available file system space\n        \"\"\"\n        contents = self._encode_contents(contents)\n        changed = self._byte_contents != contents\n        st_size = len(contents)\n\n        if self._byte_contents:\n            self.size = 0\n        current_size = self.st_size or 0\n        self.filesystem.change_disk_usage(\n            st_size - current_size, self.name, self.st_dev)\n        self._byte_contents = contents\n        self.st_size = st_size\n        self.epoch += 1\n        return changed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the contents and size and increases the modification time.", "response": "def set_contents(self, contents, encoding=None):\n        \"\"\"Sets the file contents and size and increases the modification time.\n        Also executes the side_effects if available.\n\n        Args:\n          contents: (str, bytes, unicode) new content of file.\n          encoding: (str) the encoding to be used for writing the contents\n                    if they are a unicode string.\n                    If not given, the locale preferred encoding is used.\n\n        Raises:\n          IOError: if `st_size` is not a non-negative integer,\n                   or if it exceeds the available file system space.\n        \"\"\"\n        self.encoding = encoding\n        changed = self._set_initial_contents(contents)\n        if self._side_effect is not None:\n            self._side_effect(self)\n        return changed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the full path of the current object.", "response": "def path(self):\n        \"\"\"Return the full path of the current object.\"\"\"\n        names = []\n        obj = self\n        while obj:\n            names.insert(0, obj.name)\n            obj = obj.parent_dir\n        sep = self.filesystem._path_separator(self.name)\n        if names[0] == sep:\n            names.pop(0)\n            dir_path = sep.join(names)\n            # Windows paths with drive have a root separator entry\n            # which should be removed\n            is_drive = names and len(names[0]) == 2 and names[0][1] == ':'\n            if not is_drive:\n                dir_path = sep + dir_path\n        else:\n            dir_path = sep.join(names)\n        dir_path = self.filesystem.absnormpath(dir_path)\n        return dir_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ordered_dirs(self):\n        return [item[0] for item in sorted(\n            self.byte_contents.items(), key=lambda entry: entry[1].st_ino)]", "response": "Return the list of contained directory entries ordered by creation order."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_entry(self, path_object):\n        if (not is_root() and not self.st_mode & PERM_WRITE and\n                not self.filesystem.is_windows_fs):\n            exception = IOError if IS_PY2 else OSError\n            raise exception(errno.EACCES, 'Permission Denied', self.path)\n\n        if path_object.name in self.contents:\n            self.filesystem.raise_os_error(errno.EEXIST, self.path)\n\n        self.contents[path_object.name] = path_object\n        path_object.parent_dir = self\n        self.st_nlink += 1\n        path_object.st_nlink += 1\n        path_object.st_dev = self.st_dev\n        if path_object.st_nlink == 1:\n            self.filesystem.change_disk_usage(\n                path_object.size, path_object.name, self.st_dev)", "response": "Adds a child FakeFile to this directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the specified child file or directory entry.", "response": "def get_entry(self, pathname_name):\n        \"\"\"Retrieves the specified child file or directory entry.\n\n        Args:\n            pathname_name: The basename of the child object to retrieve.\n\n        Returns:\n            The fake file or directory object.\n\n        Raises:\n            KeyError: if no child exists by the specified name.\n        \"\"\"\n        pathname_name = self._normalized_entryname(pathname_name)\n        return self.contents[pathname_name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_entry(self, pathname_name, recursive=True):\n        pathname_name = self._normalized_entryname(pathname_name)\n        entry = self.get_entry(pathname_name)\n        if self.filesystem.is_windows_fs:\n            if entry.st_mode & PERM_WRITE == 0:\n                self.filesystem.raise_os_error(errno.EACCES, pathname_name)\n            if self.filesystem.has_open_file(entry):\n                self.filesystem.raise_os_error(errno.EACCES, pathname_name)\n        else:\n            if (not is_root() and (self.st_mode & (PERM_WRITE | PERM_EXE) !=\n                                   PERM_WRITE | PERM_EXE)):\n                self.filesystem.raise_os_error(errno.EACCES, pathname_name)\n\n        if recursive and isinstance(entry, FakeDirectory):\n            while entry.contents:\n                entry.remove_entry(list(entry.contents)[0])\n        elif entry.st_nlink == 1:\n            self.filesystem.change_disk_usage(\n                -entry.size, pathname_name, entry.st_dev)\n\n        self.st_nlink -= 1\n        entry.st_nlink -= 1\n        assert entry.st_nlink >= 0\n\n        del self.contents[pathname_name]", "response": "Removes the specified child file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_parent_object(self, dir_object):\n        obj = self\n        while obj:\n            if obj == dir_object:\n                return True\n            obj = obj.parent_dir\n        return False", "response": "Return True if dir_object is a direct or indirect parent\n            directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contents(self):\n        if not self.contents_read:\n            self.contents_read = True\n            base = self.path\n            for entry in os.listdir(self.source_path):\n                source_path = os.path.join(self.source_path, entry)\n                target_path = os.path.join(base, entry)\n                if os.path.isdir(source_path):\n                    self.filesystem.add_real_directory(\n                        source_path, self.read_only, target_path=target_path)\n                else:\n                    self.filesystem.add_real_file(\n                        source_path, self.read_only, target_path=target_path)\n        return self.byte_contents", "response": "Return the list of contained directory entries loading them\n            if not already loaded."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self, total_size=None):\n        self.root = FakeDirectory(self.path_separator, filesystem=self)\n        self.cwd = self.root.name\n\n        self.open_files = []\n        self._free_fd_heap = []\n        self._last_ino = 0\n        self._last_dev = 0\n        self.mount_points = {}\n        self.add_mount_point(self.root.name, total_size)\n        self._add_standard_streams()", "response": "Remove all file system contents and reset the root."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef raise_os_error(self, errno, filename=None, winerror=None):\n        message = self._error_message(errno)\n        if (winerror is not None and sys.platform == 'win32' and\n                self.is_windows_fs):\n            if IS_PY2:\n                raise WindowsError(winerror, message, filename)\n            raise OSError(errno, message, filename, winerror)\n        raise OSError(errno, message, filename)", "response": "Raises an exception from the given error code filename and winerror."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef raise_io_error(self, errno, filename=None):\n        raise IOError(errno, self._error_message(errno), filename)", "response": "Raises IOError.\n        The error message is constructed from the given error code and shall\n        start with the error in the real system.\n\n        Args:\n            errno: A numeric error code from the C variable errno.\n            filename: The name of the affected file, if any."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _matching_string(matched, string):\n        if string is None:\n            return string\n        if IS_PY2:\n            # pylint: disable=undefined-variable\n            if isinstance(matched, text_type):\n                return text_type(string)\n        else:\n            if isinstance(matched, bytes) and isinstance(string, str):\n                return string.encode(locale.getpreferredencoding(False))\n        return string", "response": "Return the string as byte depending on the type of matched assuming string is an ASCII string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_mount_point(self, path, total_size=None):\n        path = self.absnormpath(path)\n        if path in self.mount_points:\n            self.raise_os_error(errno.EEXIST, path)\n        self._last_dev += 1\n        self.mount_points[path] = {\n            'idev': self._last_dev, 'total_size': total_size, 'used_size': 0\n        }\n        # special handling for root path: has been created before\n        root_dir = (self.root if path == self.root.name\n                    else self.create_dir(path))\n        root_dir.st_dev = self._last_dev\n        return self.mount_points[path]", "response": "Add a new mount point for a filesystem device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the total used and free disk space in bytes as named tuple or placeholder values simulating unlimited space if not set.", "response": "def get_disk_usage(self, path=None):\n        \"\"\"Return the total, used and free disk space in bytes as named tuple,\n        or placeholder values simulating unlimited space if not set.\n\n        .. note:: This matches the return value of shutil.disk_usage().\n\n        Args:\n            path: The disk space is returned for the file system device where\n                `path` resides.\n                Defaults to the root path (e.g. '/' on Unix systems).\n        \"\"\"\n        DiskUsage = namedtuple('usage', 'total, used, free')\n        if path is None:\n            mount_point = self.mount_points[self.root.name]\n        else:\n            mount_point = self._mount_point_for_path(path)\n        if mount_point and mount_point['total_size'] is not None:\n            return DiskUsage(mount_point['total_size'],\n                             mount_point['used_size'],\n                             mount_point['total_size'] -\n                             mount_point['used_size'])\n        return DiskUsage(\n            1024 * 1024 * 1024 * 1024, 0, 1024 * 1024 * 1024 * 1024)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_disk_usage(self, total_size, path=None):\n        if path is None:\n            path = self.root.name\n        mount_point = self._mount_point_for_path(path)\n        if (mount_point['total_size'] is not None and\n                mount_point['used_size'] > total_size):\n            self.raise_io_error(errno.ENOSPC, path)\n        mount_point['total_size'] = total_size", "response": "Changes the total size of the file system preserving the used space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef change_disk_usage(self, usage_change, file_path, st_dev):\n        mount_point = self._mount_point_for_device(st_dev)\n        if mount_point:\n            total_size = mount_point['total_size']\n            if total_size is not None:\n                if total_size - mount_point['used_size'] < usage_change:\n                    self.raise_io_error(errno.ENOSPC, file_path)\n            mount_point['used_size'] += usage_change", "response": "Change the used disk space by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the os. stat - like tuple for the FakeFile object corresponding to entry_path.", "response": "def stat(self, entry_path, follow_symlinks=True):\n        \"\"\"Return the os.stat-like tuple for the FakeFile object of entry_path.\n\n        Args:\n            entry_path:  Path to filesystem object to retrieve.\n            follow_symlinks: If False and entry_path points to a symlink,\n                the link itself is inspected instead of the linked object.\n\n        Returns:\n            The FakeStatResult object corresponding to entry_path.\n\n        Raises:\n            OSError: if the filesystem object doesn't exist.\n        \"\"\"\n        # stat should return the tuple representing return value of os.stat\n        try:\n            file_object = self.resolve(\n                entry_path, follow_symlinks, allow_fd=True)\n            self.raise_for_filepath_ending_with_separator(\n                entry_path, file_object, follow_symlinks)\n\n            return file_object.stat_result.copy()\n        except IOError as io_error:\n            winerror = (io_error.winerror if hasattr(io_error, 'winerror')\n                        else io_error.errno)\n            self.raise_os_error(io_error.errno, entry_path, winerror=winerror)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chmod(self, path, mode, follow_symlinks=True):\n        try:\n            file_object = self.resolve(path, follow_symlinks, allow_fd=True)\n        except IOError as io_error:\n            if io_error.errno == errno.ENOENT:\n                self.raise_os_error(errno.ENOENT, path)\n            raise\n        if self.is_windows_fs:\n            if mode & PERM_WRITE:\n                file_object.st_mode = file_object.st_mode | 0o222\n            else:\n                file_object.st_mode = file_object.st_mode & 0o777555\n        else:\n            file_object.st_mode = ((file_object.st_mode & ~PERM_ALL) |\n                                   (mode & PERM_ALL))\n        file_object.st_ctime = time.time()", "response": "Change the permissions of a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges the access and modified times of a file.", "response": "def utime(self, path, times=None, ns=None, follow_symlinks=True):\n        \"\"\"Change the access and modified times of a file.\n\n        Args:\n            path: (str) Path to the file.\n            times: 2-tuple of int or float numbers, of the form (atime, mtime)\n                which is used to set the access and modified times in seconds.\n                If None, both times are set to the current time.\n            ns: 2-tuple of int numbers, of the form (atime, mtime)  which is\n                used to set the access and modified times in nanoseconds.\n                If `None`, both times are set to the current time.\n                New in Python 3.3.\n            follow_symlinks: If `False` and entry_path points to a symlink,\n                the link itself is queried instead of the linked object.\n                New in Python 3.3.\n\n            Raises:\n                TypeError: If anything other than the expected types is\n                    specified in the passed `times` or `ns` tuple,\n                    or if the tuple length is not equal to 2.\n                ValueError: If both times and ns are specified.\n        \"\"\"\n        self._handle_utime_arg_errors(ns, times)\n\n        try:\n            file_object = self.resolve(path, follow_symlinks, allow_fd=True)\n        except IOError as io_error:\n            if io_error.errno == errno.ENOENT:\n                self.raise_os_error(errno.ENOENT, path)\n            raise\n        if times is not None:\n            for file_time in times:\n                if not isinstance(file_time, (int, float)):\n                    raise TypeError('atime and mtime must be numbers')\n\n            file_object.st_atime = times[0]\n            file_object.st_mtime = times[1]\n        elif ns is not None:\n            for file_time in ns:\n                if not isinstance(file_time, int):\n                    raise TypeError('atime and mtime must be ints')\n\n            file_object.st_atime_ns = ns[0]\n            file_object.st_mtime_ns = ns[1]\n        else:\n            current_time = time.time()\n            file_object.st_atime = current_time\n            file_object.st_mtime = current_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a file object to the list of open files on the filesystem.", "response": "def _add_open_file(self, file_obj):\n        \"\"\"Add file_obj to the list of open files on the filesystem.\n        Used internally to manage open files.\n\n        The position in the open_files array is the file descriptor number.\n\n        Args:\n            file_obj: File object to be added to open files list.\n\n        Returns:\n            File descriptor number for the file object.\n        \"\"\"\n        if self._free_fd_heap:\n            open_fd = heapq.heappop(self._free_fd_heap)\n            self.open_files[open_fd] = [file_obj]\n            return open_fd\n\n        self.open_files.append([file_obj])\n        return len(self.open_files) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the file object with given descriptor from the list open files.", "response": "def _close_open_file(self, file_des):\n        \"\"\"Remove file object with given descriptor from the list\n        of open files.\n\n        Sets the entry in open_files to None.\n\n        Args:\n            file_des: Descriptor of file object to be removed from\n            open files list.\n        \"\"\"\n        self.open_files[file_des] = None\n        heapq.heappush(self._free_fd_heap, file_des)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_open_file(self, file_des):\n        if not is_int_type(file_des):\n            raise TypeError('an integer is required')\n        if (file_des >= len(self.open_files) or\n                self.open_files[file_des] is None):\n            self.raise_os_error(errno.EBADF, str(file_des))\n        return self.open_files[file_des][0]", "response": "Returns an open file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the given file object is in the list of open files.", "response": "def has_open_file(self, file_object):\n        \"\"\"Return True if the given file object is in the list of open files.\n\n        Args:\n            file_object: The FakeFile object to be checked.\n\n        Returns:\n            `True` if the file is open.\n        \"\"\"\n        return (file_object in [wrappers[0].get_object()\n                                for wrappers in self.open_files if wrappers])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normpath(self, path):\n        path = self.normcase(path)\n        drive, path = self.splitdrive(path)\n        sep = self._path_separator(path)\n        is_absolute_path = path.startswith(sep)\n        path_components = path.split(sep)\n        collapsed_path_components = []\n        dot = self._matching_string(path, '.')\n        dotdot = self._matching_string(path, '..')\n        for component in path_components:\n            if (not component) or (component == dot):\n                continue\n            if component == dotdot:\n                if collapsed_path_components and (\n                        collapsed_path_components[-1] != dotdot):\n                    # Remove an up-reference: directory/..\n                    collapsed_path_components.pop()\n                    continue\n                elif is_absolute_path:\n                    # Ignore leading .. components if starting from the\n                    # root directory.\n                    continue\n            collapsed_path_components.append(component)\n        collapsed_path = sep.join(collapsed_path_components)\n        if is_absolute_path:\n            collapsed_path = sep + collapsed_path\n        return drive + collapsed_path or dot", "response": "Mimics os. path. normpath using the specified path_separator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _original_path(self, path):\n\n        def components_to_path():\n            if len(path_components) > len(normalized_components):\n                normalized_components.extend(\n                    path_components[len(normalized_components):])\n            sep = self._path_separator(path)\n            normalized_path = sep.join(normalized_components)\n            if path.startswith(sep) and not normalized_path.startswith(sep):\n                normalized_path = sep + normalized_path\n            return normalized_path\n\n        if self.is_case_sensitive or not path:\n            return path\n        path_components = self._path_components(path)\n        normalized_components = []\n        current_dir = self.root\n        for component in path_components:\n            if not isinstance(current_dir, FakeDirectory):\n                return components_to_path()\n            dir_name, current_dir = self._directory_content(\n                current_dir, component)\n            if current_dir is None or (\n                            isinstance(current_dir, FakeDirectory) and\n                            current_dir._byte_contents is None and\n                            current_dir.st_size == 0):\n                return components_to_path()\n            normalized_components.append(dir_name)\n        return components_to_path()", "response": "Return a normalized case version of the given path for the current file system. For case - insensitive file systems return path unchanged. For case - sensitive file systems return path unchanged."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef absnormpath(self, path):\n        path = self.normcase(path)\n        cwd = self._matching_string(path, self.cwd)\n        if not path:\n            path = self.path_separator\n        elif not self._starts_with_root_path(path):\n            # Prefix relative paths with cwd, if cwd is not root.\n            root_name = self._matching_string(path, self.root.name)\n            empty = self._matching_string(path, '')\n            path = self._path_separator(path).join(\n                (cwd != root_name and cwd or empty, path))\n        if path == self._matching_string(path, '.'):\n            path = cwd\n        return self.normpath(path)", "response": "Absolutize and minimalize the given path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmimicing os. path. splitpath using the specified path_separator that was specified .", "response": "def splitpath(self, path):\n        \"\"\"Mimic os.path.splitpath using the specified path_separator.\n\n        Mimics os.path.splitpath using the path_separator that was specified\n        for this FakeFilesystem.\n\n        Args:\n            path:  (str) The path to split.\n\n        Returns:\n            (str) A duple (pathname, basename) for which pathname does not\n            end with a slash, and basename does not contain a slash.\n        \"\"\"\n        path = self.normcase(path)\n        sep = self._path_separator(path)\n        path_components = path.split(sep)\n        if not path_components:\n            return ('', '')\n\n        starts_with_drive = self._starts_with_drive_letter(path)\n        basename = path_components.pop()\n        colon = self._matching_string(path, ':')\n        if not path_components:\n            if starts_with_drive:\n                components = basename.split(colon)\n                return (components[0] + colon, components[1])\n            return ('', basename)\n        for component in path_components:\n            if component:\n                # The path is not the root; it contains a non-separator\n                # component. Strip all trailing separators.\n                while not path_components[-1]:\n                    path_components.pop()\n                if starts_with_drive:\n                    if not path_components:\n                        components = basename.split(colon)\n                        return (components[0] + colon, components[1])\n                    if (len(path_components) == 1 and\n                            path_components[0].endswith(colon)):\n                        return (path_components[0] + sep, basename)\n                return (sep.join(path_components), basename)\n        # Root path.  Collapse all leading separators.\n        return (sep, basename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the path into the drive part and the rest of the path.", "response": "def splitdrive(self, path):\n        \"\"\"Splits the path into the drive part and the rest of the path.\n\n        Taken from Windows specific implementation in Python 3.5\n        and slightly adapted.\n\n        Args:\n            path: the full path to be splitpath.\n\n        Returns:\n            A tuple of the drive part and the rest of the path, or of\n            an empty string and the full path if drive letters are\n            not supported or no drive is present.\n        \"\"\"\n        path = make_string_path(path)\n        if self.is_windows_fs:\n            if len(path) >= 2:\n                path = self.normcase(path)\n                sep = self._path_separator(path)\n                # UNC path handling is here since Python 2.7.8,\n                # back-ported from Python 3\n                if sys.version_info >= (2, 7, 8):\n                    if (path[0:2] == sep * 2) and (\n                            path[2:3] != sep):\n                        # UNC path handling - splits off the mount point\n                        # instead of the drive\n                        sep_index = path.find(sep, 2)\n                        if sep_index == -1:\n                            return path[:0], path\n                        sep_index2 = path.find(sep, sep_index + 1)\n                        if sep_index2 == sep_index + 1:\n                            return path[:0], path\n                        if sep_index2 == -1:\n                            sep_index2 = len(path)\n                        return path[:sep_index2], path[sep_index2:]\n                if path[1:2] == self._matching_string(path, ':'):\n                    return path[:2], path[2:]\n        return path[:0], path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\njoin the paths of all_paths with drive support.", "response": "def _join_paths_with_drive_support(self, *all_paths):\n        \"\"\"Taken from Python 3.5 os.path.join() code in ntpath.py\n        and slightly adapted\"\"\"\n        base_path = all_paths[0]\n        paths_to_add = all_paths[1:]\n        sep = self._path_separator(base_path)\n        seps = [sep, self._alternative_path_separator(base_path)]\n        result_drive, result_path = self.splitdrive(base_path)\n        for path in paths_to_add:\n            drive_part, path_part = self.splitdrive(path)\n            if path_part and path_part[:1] in seps:\n                # Second path is absolute\n                if drive_part or not result_drive:\n                    result_drive = drive_part\n                result_path = path_part\n                continue\n            elif drive_part and drive_part != result_drive:\n                if (self.is_case_sensitive or\n                        drive_part.lower() != result_drive.lower()):\n                    # Different drives => ignore the first path entirely\n                    result_drive = drive_part\n                    result_path = path_part\n                    continue\n                # Same drive in different case\n                result_drive = drive_part\n            # Second path is relative to the first\n            if result_path and result_path[-1:] not in seps:\n                result_path = result_path + sep\n            result_path = result_path + path_part\n        # add separator between UNC and non-absolute path\n        colon = self._matching_string(base_path, ':')\n        if (result_path and result_path[:1] not in seps and\n                result_drive and result_drive[-1:] != colon):\n            return result_drive + sep + result_path\n        return result_drive + result_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmimics os. path. join using the specified path_separator.", "response": "def joinpaths(self, *paths):\n        \"\"\"Mimic os.path.join using the specified path_separator.\n\n        Args:\n            *paths:  (str) Zero or more paths to join.\n\n        Returns:\n            (str) The paths joined by the path separator, starting with\n            the last absolute path in paths.\n        \"\"\"\n        if sys.version_info >= (3, 6):\n            paths = [os.fspath(path) for path in paths]\n        if len(paths) == 1:\n            return paths[0]\n        if self.is_windows_fs:\n            return self._join_paths_with_drive_support(*paths)\n        joined_path_segments = []\n        sep = self._path_separator(paths[0])\n        for path_segment in paths:\n            if self._starts_with_root_path(path_segment):\n                # An absolute path\n                joined_path_segments = [path_segment]\n            else:\n                if (joined_path_segments and\n                        not joined_path_segments[-1].endswith(sep)):\n                    joined_path_segments.append(sep)\n                if path_segment:\n                    joined_path_segments.append(path_segment)\n        return self._matching_string(paths[0], '').join(joined_path_segments)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbreaks the path into a list of component names.", "response": "def _path_components(self, path):\n        \"\"\"Breaks the path into a list of component names.\n\n        Does not include the root directory as a component, as all paths\n        are considered relative to the root directory for the FakeFilesystem.\n        Callers should basically follow this pattern:\n\n        .. code:: python\n\n            file_path = self.absnormpath(file_path)\n            path_components = self._path_components(file_path)\n            current_dir = self.root\n            for component in path_components:\n                if component not in current_dir.contents:\n                    raise IOError\n                _do_stuff_with_component(current_dir, component)\n                current_dir = current_dir.get_entry(component)\n\n        Args:\n            path:  Path to tokenize.\n\n        Returns:\n            The list of names split from path.\n        \"\"\"\n        if not path or path == self._path_separator(path):\n            return []\n        drive, path = self.splitdrive(path)\n        path_components = path.split(self._path_separator(path))\n        assert drive or path_components\n        if not path_components[0]:\n            if len(path_components) > 1 and not path_components[1]:\n                path_components = []\n            else:\n                # This is an absolute path.\n                path_components = path_components[1:]\n        if drive:\n            path_components.insert(0, drive)\n        return path_components"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the file_path starts with a drive letter.", "response": "def _starts_with_drive_letter(self, file_path):\n        \"\"\"Return True if file_path starts with a drive letter.\n\n        Args:\n            file_path: the full path to be examined.\n\n        Returns:\n            `True` if drive letter support is enabled in the filesystem and\n            the path starts with a drive letter.\n        \"\"\"\n        colon = self._matching_string(file_path, ':')\n        return (self.is_windows_fs and len(file_path) >= 2 and\n                file_path[:1].isalpha and (file_path[1:2]) == colon)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ends_with_path_separator(self, file_path):\n        if is_int_type(file_path):\n            return False\n        file_path = make_string_path(file_path)\n        return (file_path and\n                file_path not in (self.path_separator,\n                                  self.alternative_path_separator) and\n                (file_path.endswith(self._path_separator(file_path)) or\n                 self.alternative_path_separator is not None and\n                 file_path.endswith(\n                     self._alternative_path_separator(file_path))))", "response": "Return True if file_path ends with a valid path separator."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the corresponding object exists.", "response": "def exists(self, file_path, check_link=False):\n        \"\"\"Return true if a path points to an existing file system object.\n\n        Args:\n            file_path:  The path to examine.\n\n        Returns:\n            (bool) True if the corresponding object exists.\n\n        Raises:\n            TypeError: if file_path is None.\n        \"\"\"\n        if check_link and self.islink(file_path):\n            return True\n        file_path = make_string_path(file_path)\n        if file_path is None:\n            raise TypeError\n        if not file_path:\n            return False\n        if file_path == self.dev_null.name:\n            return not self.is_windows_fs\n        try:\n            if self.is_filepath_ending_with_separator(file_path):\n                return False\n            file_path = self.resolve_path(file_path)\n        except (IOError, OSError):\n            return False\n        if file_path == self.root.name:\n            return True\n\n        path_components = self._path_components(file_path)\n        current_dir = self.root\n        for component in path_components:\n            current_dir = self._directory_content(current_dir, component)[1]\n            if not current_dir:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfollowing a path and return the path to the entry in the cache.", "response": "def resolve_path(self, file_path, allow_fd=False, raw_io=True):\n        \"\"\"Follow a path, resolving symlinks.\n\n        ResolvePath traverses the filesystem along the specified file path,\n        resolving file names and symbolic links until all elements of the path\n        are exhausted, or we reach a file which does not exist.\n        If all the elements are not consumed, they just get appended to the\n        path resolved so far.\n        This gives us the path which is as resolved as it can be, even if the\n        file does not exist.\n\n        This behavior mimics Unix semantics, and is best shown by example.\n        Given a file system that looks like this:\n\n              /a/b/\n              /a/b/c -> /a/b2          c is a symlink to /a/b2\n              /a/b2/x\n              /a/c   -> ../d\n              /a/x   -> y\n\n         Then:\n              /a/b/x      =>  /a/b/x\n              /a/c        =>  /a/d\n              /a/x        =>  /a/y\n              /a/b/c/d/e  =>  /a/b2/d/e\n\n        Args:\n            file_path: The path to examine.\n            allow_fd: If `True`, `file_path` may be open file descriptor.\n            raw_io: `True` if called from low-level I/O functions.\n\n        Returns:\n            The resolved_path (string) or None.\n\n        Raises:\n            TypeError: if `file_path` is `None`.\n            IOError: if `file_path` is '' or a part of the path doesn't exist.\n        \"\"\"\n\n        if (allow_fd and sys.version_info >= (3, 3) and\n                isinstance(file_path, int)):\n            return self.get_open_file(file_path).get_object().path\n        file_path = make_string_path(file_path)\n        if file_path is None:\n            # file.open(None) raises TypeError, so mimic that.\n            raise TypeError('Expected file system path string, received None')\n        file_path = self._to_string(file_path)\n        if not file_path or not self._valid_relative_path(file_path):\n            # file.open('') raises IOError, so mimic that, and validate that\n            # all parts of a relative path exist.\n            self.raise_io_error(errno.ENOENT, file_path)\n        file_path = self.absnormpath(self._original_path(file_path))\n        if self._is_root_path(file_path):\n            return file_path\n        if file_path == self.dev_null.name:\n            return file_path\n        path_components = self._path_components(file_path)\n        resolved_components = self._resolve_components(path_components, raw_io)\n        return self._components_to_path(resolved_components)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfollows a link w. r. t. a path resolved so far.", "response": "def _follow_link(self, link_path_components, link):\n        \"\"\"Follow a link w.r.t. a path resolved so far.\n\n        The component is either a real file, which is a no-op, or a\n        symlink. In the case of a symlink, we have to modify the path\n        as built up so far\n          /a/b => ../c  should yield /a/../c (which will normalize to /a/c)\n          /a/b => x     should yield /a/x\n          /a/b => /x/y/z should yield /x/y/z\n        The modified path may land us in a new spot which is itself a\n        link, so we may repeat the process.\n\n        Args:\n            link_path_components: The resolved path built up to the link\n                so far.\n            link: The link object itself.\n\n        Returns:\n            (string) The updated path resolved after following the link.\n\n        Raises:\n            IOError: if there are too many levels of symbolic link\n        \"\"\"\n        link_path = link.contents\n        sep = self._path_separator(link_path)\n        # For links to absolute paths, we want to throw out everything\n        # in the path built so far and replace with the link. For relative\n        # links, we have to append the link to what we have so far,\n        if not self._starts_with_root_path(link_path):\n            # Relative path. Append remainder of path to what we have\n            # processed so far, excluding the name of the link itself.\n            # /a/b => ../c  should yield /a/../c\n            # (which will normalize to /c)\n            # /a/b => d should yield a/d\n            components = link_path_components[:-1]\n            components.append(link_path)\n            link_path = sep.join(components)\n        # Don't call self.NormalizePath(), as we don't want to prepend\n        # self.cwd.\n        return self.normpath(link_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for the specified FakeFile object within the fake file system with the specified path.", "response": "def get_object_from_normpath(self, file_path):\n        \"\"\"Search for the specified filesystem object within the fake\n        filesystem.\n\n        Args:\n            file_path: Specifies target FakeFile object to retrieve, with a\n                path that has already been normalized/resolved.\n\n        Returns:\n            The FakeFile object corresponding to file_path.\n\n        Raises:\n            IOError: if the object is not found.\n        \"\"\"\n        file_path = make_string_path(file_path)\n        if file_path == self.root.name:\n            return self.root\n        if file_path == self.dev_null.name:\n            return self.dev_null\n\n        file_path = self._original_path(file_path)\n        path_components = self._path_components(file_path)\n        target_object = self.root\n        try:\n            for component in path_components:\n                if S_ISLNK(target_object.st_mode):\n                    target_object = self.resolve(target_object.contents)\n                if not S_ISDIR(target_object.st_mode):\n                    if not self.is_windows_fs:\n                        self.raise_io_error(errno.ENOTDIR, file_path)\n                    self.raise_io_error(errno.ENOENT, file_path)\n                target_object = target_object.get_entry(component)\n        except KeyError:\n            self.raise_io_error(errno.ENOENT, file_path)\n        return target_object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_object(self, file_path):\n        file_path = make_string_path(file_path)\n        file_path = self.absnormpath(self._original_path(file_path))\n        return self.get_object_from_normpath(file_path)", "response": "Search for the specified filesystem object within the fake\n            filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self, file_path, follow_symlinks=True, allow_fd=False):\n        if isinstance(file_path, int):\n            if allow_fd and sys.version_info >= (3, 3):\n                return self.get_open_file(file_path).get_object()\n            raise TypeError('path should be string, bytes or '\n                            'os.PathLike (if supported), not int')\n\n        if follow_symlinks:\n            file_path = make_string_path(file_path)\n            return self.get_object_from_normpath(self.resolve_path(file_path))\n        return self.lresolve(file_path)", "response": "Resolves the specified path to a FakeFile object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lresolve(self, path):\n        path = make_string_path(path)\n        if path == self.root.name:\n            # The root directory will never be a link\n            return self.root\n\n        # remove trailing separator\n        path = self._path_without_trailing_separators(path)\n        path = self._original_path(path)\n\n        parent_directory, child_name = self.splitpath(path)\n        if not parent_directory:\n            parent_directory = self.cwd\n        try:\n            parent_obj = self.resolve(parent_directory)\n            assert parent_obj\n            if not isinstance(parent_obj, FakeDirectory):\n                if not self.is_windows_fs and isinstance(parent_obj, FakeFile):\n                    self.raise_io_error(errno.ENOTDIR, path)\n                self.raise_io_error(errno.ENOENT, path)\n            return parent_obj.get_entry(child_name)\n        except KeyError:\n            self.raise_io_error(errno.ENOENT, path)", "response": "Search for the specified object in the specified path and return the FakeFile object corresponding to that object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_object(self, file_path, file_object, error_fct=None):\n        error_fct = error_fct or self.raise_os_error\n        if not file_path:\n            target_directory = self.root\n        else:\n            target_directory = self.resolve(file_path)\n            if not S_ISDIR(target_directory.st_mode):\n                error = errno.ENOENT if self.is_windows_fs else errno.ENOTDIR\n                error_fct(error, file_path)\n        target_directory.add_entry(file_object)", "response": "Add a fake file or directory into the filesystem at file_path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename(self, old_file_path, new_file_path, force_replace=False):\n        ends_with_sep = self.ends_with_path_separator(old_file_path)\n        old_file_path = self.absnormpath(old_file_path)\n        new_file_path = self.absnormpath(new_file_path)\n        if not self.exists(old_file_path, check_link=True):\n            self.raise_os_error(errno.ENOENT, old_file_path, 2)\n        if ends_with_sep:\n            self._handle_broken_link_with_trailing_sep(old_file_path)\n\n        old_object = self.lresolve(old_file_path)\n        if not self.is_windows_fs:\n            self._handle_posix_dir_link_errors(\n                new_file_path, old_file_path, ends_with_sep)\n\n        if self.exists(new_file_path, check_link=True):\n            new_file_path = self._rename_to_existing_path(\n                force_replace, new_file_path, old_file_path,\n                old_object, ends_with_sep)\n\n        if not new_file_path:\n            return\n\n        old_dir, old_name = self.splitpath(old_file_path)\n        new_dir, new_name = self.splitpath(new_file_path)\n        if not self.exists(new_dir):\n            self.raise_os_error(errno.ENOENT, new_dir)\n        old_dir_object = self.resolve(old_dir)\n        new_dir_object = self.resolve(new_dir)\n        if old_dir_object.st_dev != new_dir_object.st_dev:\n            self.raise_os_error(errno.EXDEV, old_file_path)\n        if not S_ISDIR(new_dir_object.st_mode):\n            self.raise_os_error(\n                errno.EACCES if self.is_windows_fs else errno.ENOTDIR,\n                new_file_path)\n        if new_dir_object.has_parent_object(old_object):\n            self.raise_os_error(errno.EINVAL, new_file_path)\n\n        object_to_rename = old_dir_object.get_entry(old_name)\n        old_dir_object.remove_entry(old_name, recursive=False)\n        object_to_rename.name = new_name\n        new_name = new_dir_object._normalized_entryname(new_name)\n        if new_name in new_dir_object.contents:\n            # in case of overwriting remove the old entry first\n            new_dir_object.remove_entry(new_name)\n        new_dir_object.add_entry(object_to_rename)", "response": "Renames a FakeFile object at old_file_path to new_file_path preserving all properties."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_object(self, file_path):\n        file_path = self.absnormpath(self._original_path(file_path))\n        if self._is_root_path(file_path):\n            self.raise_os_error(errno.EBUSY, file_path)\n        try:\n            dirname, basename = self.splitpath(file_path)\n            target_directory = self.resolve(dirname)\n            target_directory.remove_entry(basename)\n        except KeyError:\n            self.raise_io_error(errno.ENOENT, file_path)\n        except AttributeError:\n            self.raise_io_error(errno.ENOTDIR, file_path)", "response": "Removes an existing file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a fake directory and all the parent directories.", "response": "def create_dir(self, directory_path, perm_bits=PERM_DEF):\n        \"\"\"Create `directory_path`, and all the parent directories.\n\n        Helper method to set up your test faster.\n\n        Args:\n            directory_path: The full directory path to create.\n            perm_bits: The permission bits as set by `chmod`.\n\n        Returns:\n            The newly created FakeDirectory object.\n\n        Raises:\n            OSError: if the directory already exists.\n        \"\"\"\n        directory_path = self.make_string_path(directory_path)\n        directory_path = self.absnormpath(directory_path)\n        self._auto_mount_drive_if_needed(directory_path)\n        if self.exists(directory_path, check_link=True):\n            self.raise_os_error(errno.EEXIST, directory_path)\n        path_components = self._path_components(directory_path)\n        current_dir = self.root\n\n        new_dirs = []\n        for component in path_components:\n            directory = self._directory_content(current_dir, component)[1]\n            if not directory:\n                new_dir = FakeDirectory(component, filesystem=self)\n                new_dirs.append(new_dir)\n                current_dir.add_entry(new_dir)\n                current_dir = new_dir\n            else:\n                if S_ISLNK(directory.st_mode):\n                    directory = self.resolve(directory.contents)\n                current_dir = directory\n                if directory.st_mode & S_IFDIR != S_IFDIR:\n                    self.raise_os_error(errno.ENOTDIR, current_dir.path)\n\n        # set the permission after creating the directories\n        # to allow directory creation inside a read-only directory\n        for new_dir in new_dirs:\n            new_dir.st_mode = S_IFDIR | perm_bits\n\n        self._last_ino += 1\n        current_dir.st_ino = self._last_ino\n        return current_dir"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_file(self, file_path, st_mode=S_IFREG | PERM_DEF_FILE,\n                    contents='', st_size=None, create_missing_dirs=True,\n                    apply_umask=False, encoding=None, errors=None,\n                    side_effect=None):\n        \"\"\"Create `file_path`, including all the parent directories along\n        the way.\n\n        This helper method can be used to set up tests more easily.\n\n        Args:\n            file_path: The path to the file to create.\n            st_mode: The stat constant representing the file type.\n            contents: the contents of the file. If not given and st_size is\n                None, an empty file is assumed.\n            st_size: file size; only valid if contents not given. If given,\n                the file is considered to be in \"large file mode\" and trying\n                to read from or write to the file will result in an exception.\n            create_missing_dirs: If `True`, auto create missing directories.\n            apply_umask: `True` if the current umask must be applied\n                on `st_mode`.\n            encoding: If `contents` is a unicode string, the encoding used\n                for serialization.\n            errors: The error mode used for encoding/decoding errors.\n            side_effect: function handle that is executed when file is written,\n                must accept the file object as an argument.\n\n        Returns:\n            The newly created FakeFile object.\n\n        Raises:\n            IOError: if the file already exists.\n            IOError: if the containing directory is required and missing.\n        \"\"\"\n        return self.create_file_internally(\n            file_path, st_mode, contents, st_size, create_missing_dirs,\n            apply_umask, encoding, errors, side_effect=side_effect)", "response": "Create a file in the internal file system."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a fake file that contains the contents of the real file.", "response": "def add_real_file(self, source_path, read_only=True, target_path=None):\n        \"\"\"Create `file_path`, including all the parent directories along the\n        way, for an existing real file. The contents of the real file are read\n        only on demand.\n\n        Args:\n            source_path: Path to an existing file in the real file system\n            read_only: If `True` (the default), writing to the fake file\n                raises an exception.  Otherwise, writing to the file changes\n                the fake file only.\n            target_path: If given, the path of the target direction,\n                otherwise it is equal to `source_path`.\n\n        Returns:\n            the newly created FakeFile object.\n\n        Raises:\n            OSError: if the file does not exist in the real file system.\n            IOError: if the file already exists in the fake file system.\n\n        .. note:: On most systems, accessing the fake file's contents may\n            update both the real and fake files' `atime` (access time).\n            In this particular case, `add_real_file()` violates the rule\n            that `pyfakefs` must not modify the real file system.\n        \"\"\"\n        target_path = target_path or source_path\n        source_path = make_string_path(source_path)\n        target_path = self.make_string_path(target_path)\n        real_stat = os.stat(source_path)\n        fake_file = self.create_file_internally(target_path,\n                                                read_from_real_fs=True)\n\n        # for read-only mode, remove the write/executable permission bits\n        fake_file.stat_result.set_from_stat_result(real_stat)\n        if read_only:\n            fake_file.st_mode &= 0o777444\n        fake_file.file_path = source_path\n        self.change_disk_usage(fake_file.size, fake_file.name,\n                               fake_file.st_dev)\n        return fake_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a fake directory corresponding to the real directory at the specified path.", "response": "def add_real_directory(self, source_path, read_only=True, lazy_read=True,\n                           target_path=None):\n        \"\"\"Create a fake directory corresponding to the real directory at the\n        specified path.  Add entries in the fake directory corresponding to\n        the entries in the real directory.\n\n        Args:\n            source_path: The path to the existing directory.\n            read_only: If set, all files under the directory are treated as\n                read-only, e.g. a write access raises an exception;\n                otherwise, writing to the files changes the fake files only\n                as usually.\n            lazy_read: If set (default), directory contents are only read when\n                accessed, and only until the needed subdirectory level.\n\n                .. note:: This means that the file system size is only updated\n                  at the time the directory contents are read; set this to\n                  `False` only if you are dependent on accurate file system\n                  size in your test\n            target_path: If given, the target directory, otherwise,\n                the target directory is the same as `source_path`.\n\n        Returns:\n            the newly created FakeDirectory object.\n\n        Raises:\n            OSError: if the directory does not exist in the real file system.\n            IOError: if the directory already exists in the fake file system.\n        \"\"\"\n        source_path = self._path_without_trailing_separators(source_path)\n        if not os.path.exists(source_path):\n            self.raise_io_error(errno.ENOENT, source_path)\n        target_path = target_path or source_path\n        if lazy_read:\n            parent_path = os.path.split(target_path)[0]\n            if self.exists(parent_path):\n                parent_dir = self.get_object(parent_path)\n            else:\n                parent_dir = self.create_dir(parent_path)\n            new_dir = FakeDirectoryFromRealDirectory(\n                source_path, self, read_only, target_path)\n            parent_dir.add_entry(new_dir)\n            self._last_ino += 1\n            new_dir.st_ino = self._last_ino\n        else:\n            new_dir = self.create_dir(target_path)\n            for base, _, files in os.walk(source_path):\n                new_base = os.path.join(new_dir.path,\n                                        os.path.relpath(base, source_path))\n                for fileEntry in files:\n                    self.add_real_file(os.path.join(base, fileEntry),\n                                       read_only,\n                                       os.path.join(new_base, fileEntry))\n        return new_dir"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a symlink pointed at the specified path.", "response": "def create_symlink(self, file_path, link_target, create_missing_dirs=True):\n        \"\"\"Create the specified symlink, pointed at the specified link target.\n\n        Args:\n            file_path:  path to the symlink to create\n            link_target:  the target of the symlink\n            create_missing_dirs: If `True`, any missing parent directories of\n                file_path will be created\n\n        Returns:\n            The newly created FakeFile object.\n\n        Raises:\n            OSError: if the symlink could not be created\n                (see :py:meth:`create_file`).\n            OSError: if on Windows before Python 3.2.\n        \"\"\"\n        if not self._is_link_supported():\n            raise OSError(\"Symbolic links are not supported \"\n                          \"on Windows before Python 3.2\")\n\n        # the link path cannot end with a path separator\n        file_path = self.make_string_path(file_path)\n        link_target = self.make_string_path(link_target)\n        file_path = self.normcase(file_path)\n        if self.ends_with_path_separator(file_path):\n            if self.exists(file_path):\n                self.raise_os_error(errno.EEXIST, file_path)\n            if self.exists(link_target):\n                if not self.is_windows_fs:\n                    self.raise_os_error(errno.ENOENT, file_path)\n            else:\n                if self.is_windows_fs:\n                    self.raise_os_error(errno.EINVAL, link_target)\n                if not self.exists(\n                        self._path_without_trailing_separators(file_path),\n                        check_link=True):\n                    self.raise_os_error(errno.ENOENT, link_target)\n                if self.is_macos:\n                    # to avoid EEXIST exception, remove the link\n                    # if it already exists\n                    if self.exists(file_path, check_link=True):\n                        self.remove_object(file_path)\n                else:\n                    self.raise_os_error(errno.EEXIST, link_target)\n\n        # resolve the link path only if it is not a link itself\n        if not self.islink(file_path):\n            file_path = self.resolve_path(file_path)\n        link_target = make_string_path(link_target)\n        return self.create_file_internally(\n            file_path, st_mode=S_IFLNK | PERM_DEF,\n            contents=link_target,\n            create_missing_dirs=create_missing_dirs,\n            raw_io=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef link(self, old_path, new_path):\n        if not self._is_link_supported():\n            raise OSError(\n                \"Links are not supported on Windows before Python 3.2\")\n        new_path_normalized = self.absnormpath(new_path)\n        if self.exists(new_path_normalized, check_link=True):\n            self.raise_os_error(errno.EEXIST, new_path)\n\n        new_parent_directory, new_basename = self.splitpath(\n            new_path_normalized)\n        if not new_parent_directory:\n            new_parent_directory = self.cwd\n\n        if not self.exists(new_parent_directory):\n            self.raise_os_error(errno.ENOENT, new_parent_directory)\n\n        if self.ends_with_path_separator(old_path):\n            error = errno.EINVAL if self.is_windows_fs else errno.ENOTDIR\n            self.raise_os_error(error, old_path)\n\n        if not self.is_windows_fs and self.ends_with_path_separator(new_path):\n            self.raise_os_error(errno.ENOENT, old_path)\n\n        # Retrieve the target file\n        try:\n            old_file = self.resolve(old_path)\n        except IOError:\n            self.raise_os_error(errno.ENOENT, old_path)\n\n        if old_file.st_mode & S_IFDIR:\n            self.raise_os_error(\n                errno.EACCES if self.is_windows_fs else errno.EPERM, old_path)\n\n        # abuse the name field to control the filename of the\n        # newly created link\n        old_file.name = new_basename\n        self.add_object(new_parent_directory, old_file)\n        return old_file", "response": "Create a hard link at old_path pointing at new_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the target of a symbolic link.", "response": "def readlink(self, path):\n        \"\"\"Read the target of a symlink.\n\n        Args:\n            path:  symlink to read the target of.\n\n        Returns:\n            the string representing the path to which the symbolic link points.\n\n        Raises:\n            TypeError: if path is None\n            OSError: (with errno=ENOENT) if path is not a valid path, or\n                (with errno=EINVAL) if path is valid, but is not a symlink,\n                or if the path ends with a path separator (Posix only)\n        \"\"\"\n        if path is None:\n            raise TypeError\n        try:\n            link_obj = self.lresolve(path)\n        except IOError as exc:\n            self.raise_os_error(exc.errno, path)\n        if S_IFMT(link_obj.st_mode) != S_IFLNK:\n            self.raise_os_error(errno.EINVAL, path)\n\n        if self.ends_with_path_separator(path):\n            if not self.is_windows_fs and self.exists(path):\n                self.raise_os_error(errno.EINVAL, path)\n            if not self.exists(link_obj.path):\n                if self.is_windows_fs:\n                    error = errno.EINVAL\n                elif self._is_circular_link(link_obj):\n                    if self.is_macos:\n                        return link_obj.path\n                    error = errno.ELOOP\n                else:\n                    error = errno.ENOENT\n                self.raise_os_error(error, link_obj.path)\n\n        return link_obj.contents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makedir(self, dir_name, mode=PERM_DEF):\n        dir_name = make_string_path(dir_name)\n        ends_with_sep = self.ends_with_path_separator(dir_name)\n        dir_name = self._path_without_trailing_separators(dir_name)\n        if not dir_name:\n            self.raise_os_error(errno.ENOENT, '')\n\n        if self.is_windows_fs:\n            dir_name = self.absnormpath(dir_name)\n        parent_dir, _ = self.splitpath(dir_name)\n        if parent_dir:\n            base_dir = self.normpath(parent_dir)\n            ellipsis = self._matching_string(\n                parent_dir, self.path_separator + '..')\n            if parent_dir.endswith(ellipsis) and not self.is_windows_fs:\n                base_dir, dummy_dotdot, _ = parent_dir.partition(ellipsis)\n            if not self.exists(base_dir):\n                self.raise_os_error(errno.ENOENT, base_dir)\n\n        dir_name = self.absnormpath(dir_name)\n        if self.exists(dir_name, check_link=True):\n            if self.is_windows_fs and dir_name == self.path_separator:\n                error_nr = errno.EACCES\n            else:\n                error_nr = errno.EEXIST\n            if ends_with_sep and self.is_macos and not self.exists(dir_name):\n                # to avoid EEXIST exception, remove the link\n                self.remove_object(dir_name)\n            else:\n                self.raise_os_error(error_nr, dir_name)\n        head, tail = self.splitpath(dir_name)\n\n        self.add_object(\n            head, FakeDirectory(tail, mode & ~self.umask, filesystem=self))", "response": "Create a leaf Fake directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makedirs(self, dir_name, mode=PERM_DEF, exist_ok=False):\n        ends_with_sep = self.ends_with_path_separator(dir_name)\n        dir_name = self.absnormpath(dir_name)\n        if (ends_with_sep and self.is_macos and\n                self.exists(dir_name, check_link=True) and\n                not self.exists(dir_name)):\n            # to avoid EEXIST exception, remove the link\n            self.remove_object(dir_name)\n\n        path_components = self._path_components(dir_name)\n\n        # Raise a permission denied error if the first existing directory\n        # is not writeable.\n        current_dir = self.root\n        for component in path_components:\n            if (component not in current_dir.contents\n                    or not isinstance(current_dir.contents, dict)):\n                break\n            else:\n                current_dir = current_dir.contents[component]\n        try:\n            self.create_dir(dir_name, mode & ~self.umask)\n        except (IOError, OSError) as e:\n            if (not exist_ok or\n                    not isinstance(self.resolve(dir_name), FakeDirectory)):\n                if self.is_windows_fs and e.errno == errno.ENOTDIR:\n                    e.errno = errno.ENOENT\n                self.raise_os_error(e.errno, e.filename)", "response": "Create a leaf Fake directory and any non - existent parent directories with the specified mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_of_type(self, path, st_flag, follow_symlinks=True):\n        path = make_string_path(path)\n        if path is None:\n            raise TypeError\n        try:\n            obj = self.resolve(path, follow_symlinks)\n            if obj:\n                self.raise_for_filepath_ending_with_separator(\n                    path, obj, macos_handling=not follow_symlinks)\n                return S_IFMT(obj.st_mode) == st_flag\n        except (IOError, OSError):\n            return False\n        return False", "response": "Helper function to implement is_of_type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if path identifies a directory.", "response": "def isdir(self, path, follow_symlinks=True):\n        \"\"\"Determine if path identifies a directory.\n\n        Args:\n            path: Path to filesystem object.\n\n        Returns:\n            `True` if path points to a directory (following symlinks).\n\n        Raises:\n            TypeError: if path is None.\n        \"\"\"\n        return self._is_of_type(path, S_IFDIR, follow_symlinks)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isfile(self, path, follow_symlinks=True):\n        return self._is_of_type(path, S_IFREG, follow_symlinks)", "response": "Determines if a path identifies a regular file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests that the target directory is actually a directory. Raise OSError if not.", "response": "def confirmdir(self, target_directory):\n        \"\"\"Test that the target is actually a directory, raising OSError\n        if not.\n\n        Args:\n            target_directory: Path to the target directory within the fake\n                filesystem.\n\n        Returns:\n            The FakeDirectory object corresponding to target_directory.\n\n        Raises:\n            OSError: if the target is not a directory.\n        \"\"\"\n        try:\n            directory = self.resolve(target_directory)\n        except IOError as exc:\n            self.raise_os_error(exc.errno, target_directory)\n        if not directory.st_mode & S_IFDIR:\n            if self.is_windows_fs and IS_PY2:\n                error_nr = errno.EINVAL\n            else:\n                error_nr = errno.ENOTDIR\n            self.raise_os_error(error_nr, target_directory, 267)\n        return directory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, path):\n        norm_path = self.absnormpath(path)\n        if self.ends_with_path_separator(path):\n            self._handle_broken_link_with_trailing_sep(norm_path)\n        if self.exists(norm_path):\n            obj = self.resolve(norm_path)\n            if S_IFMT(obj.st_mode) == S_IFDIR:\n                link_obj = self.lresolve(norm_path)\n                if S_IFMT(link_obj.st_mode) != S_IFLNK:\n                    if self.is_windows_fs:\n                        error = errno.EACCES\n                    elif self.is_macos:\n                        error = errno.EPERM\n                    else:\n                        error = errno.EISDIR\n                    self.raise_os_error(error, norm_path)\n\n                norm_path = make_string_path(norm_path)\n                if path.endswith(self.path_separator):\n                    if self.is_windows_fs:\n                        error = errno.EACCES\n                    elif self.is_macos:\n                        error = errno.EPERM\n                    else:\n                        error = errno.ENOTDIR\n                    self.raise_os_error(error, norm_path)\n            else:\n                self.raise_for_filepath_ending_with_separator(path, obj)\n\n        try:\n            self.remove_object(norm_path)\n        except IOError as exc:\n            self.raise_os_error(exc.errno, exc.filename)", "response": "Removes the FakeFile object at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rmdir(self, target_directory, allow_symlink=False):\n        if target_directory in (b'.', u'.'):\n            error_nr = errno.EACCES if self.is_windows_fs else errno.EINVAL\n            self.raise_os_error(error_nr, target_directory)\n        ends_with_sep = self.ends_with_path_separator(target_directory)\n        target_directory = self.absnormpath(target_directory)\n        if self.confirmdir(target_directory):\n            if not self.is_windows_fs and self.islink(target_directory):\n                if allow_symlink:\n                    return\n                if not ends_with_sep or not self.is_macos:\n                    self.raise_os_error(errno.ENOTDIR, target_directory)\n\n            dir_object = self.resolve(target_directory)\n            if dir_object.contents:\n                self.raise_os_error(errno.ENOTEMPTY, target_directory)\n            try:\n                self.remove_object(target_directory)\n            except IOError as exc:\n                self.raise_os_error(exc.errno, exc.filename)", "response": "Removes a leaf Fake directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef listdir(self, target_directory):\n        target_directory = self.resolve_path(target_directory, allow_fd=True)\n        directory = self.confirmdir(target_directory)\n        directory_contents = directory.contents\n        return list(directory_contents.keys())", "response": "Return a list of file names within the target directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dir():\n        dir = [\n            'abspath', 'dirname', 'exists', 'expanduser', 'getatime',\n            'getctime', 'getmtime', 'getsize', 'isabs', 'isdir', 'isfile',\n            'islink', 'ismount', 'join', 'lexists', 'normcase', 'normpath',\n            'realpath', 'relpath', 'split', 'splitdrive'\n        ]\n        if IS_PY2:\n            dir.append('walk')\n        if sys.platform != 'win32' or not IS_PY2:\n            dir.append('samefile')\n        return dir", "response": "Return the list of patched function names. Used for patching\n            functions imported from the module."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the size of the file object.", "response": "def getsize(self, path):\n        \"\"\"Return the file object size in bytes.\n\n        Args:\n          path:  path to the file object.\n\n        Returns:\n          file size in bytes.\n        \"\"\"\n        try:\n            file_obj = self.filesystem.resolve(path)\n            if (self.filesystem.ends_with_path_separator(path) and\n                    S_IFMT(file_obj.st_mode) != S_IFDIR):\n                error_nr = (errno.EINVAL if self.filesystem.is_windows_fs\n                            else errno.ENOTDIR)\n                self.filesystem.raise_os_error(error_nr, path)\n            return file_obj.st_size\n        except IOError as exc:\n            raise os.error(exc.errno, exc.strerror)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isabs(self, path):\n        if self.filesystem.is_windows_fs:\n            path = self.splitdrive(path)[1]\n        path = make_string_path(path)\n        sep = self.filesystem._path_separator(path)\n        altsep = self.filesystem._alternative_path_separator(path)\n        if self.filesystem.is_windows_fs:\n            return len(path) > 0 and path[:1] in (sep, altsep)\n        else:\n            return (path.startswith(sep) or\n                    altsep is not None and path.startswith(altsep))", "response": "Return True if path is an absolute pathname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the modification time of the fake file.", "response": "def getmtime(self, path):\n        \"\"\"Returns the modification time of the fake file.\n\n        Args:\n            path: the path to fake file.\n\n        Returns:\n            (int, float) the modification time of the fake file\n                         in number of seconds since the epoch.\n\n        Raises:\n            OSError: if the file does not exist.\n        \"\"\"\n        try:\n            file_obj = self.filesystem.resolve(path)\n            return file_obj.st_mtime\n        except IOError:\n            self.filesystem.raise_os_error(errno.ENOENT, winerror=3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the last access time of the fake file.", "response": "def getatime(self, path):\n        \"\"\"Returns the last access time of the fake file.\n\n        Note: Access time is not set automatically in fake filesystem\n            on access.\n\n        Args:\n            path: the path to fake file.\n\n        Returns:\n            (int, float) the access time of the fake file in number of seconds\n                since the epoch.\n\n        Raises:\n            OSError: if the file does not exist.\n        \"\"\"\n        try:\n            file_obj = self.filesystem.resolve(path)\n        except IOError:\n            self.filesystem.raise_os_error(errno.ENOENT)\n        return file_obj.st_atime"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getctime(self, path):\n        try:\n            file_obj = self.filesystem.resolve(path)\n        except IOError:\n            self.filesystem.raise_os_error(errno.ENOENT)\n        return file_obj.st_ctime", "response": "Returns the creation time of the fake file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the absolute version of a path.", "response": "def abspath(self, path):\n        \"\"\"Return the absolute version of a path.\"\"\"\n\n        def getcwd():\n            \"\"\"Return the current working directory.\"\"\"\n            # pylint: disable=undefined-variable\n            if IS_PY2 and isinstance(path, text_type):\n                return self.os.getcwdu()\n            elif not IS_PY2 and isinstance(path, bytes):\n                return self.os.getcwdb()\n            else:\n                return self.os.getcwd()\n\n        path = make_string_path(path)\n        sep = self.filesystem._path_separator(path)\n        altsep = self.filesystem._alternative_path_separator(path)\n        if not self.isabs(path):\n            path = self.join(getcwd(), path)\n        elif (self.filesystem.is_windows_fs and\n              path.startswith(sep) or altsep is not None and\n              path.startswith(altsep)):\n            cwd = getcwd()\n            if self.filesystem._starts_with_drive_letter(cwd):\n                path = self.join(cwd[:2], path)\n        return self.normpath(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting to lower case under windows replaces additional path separator.", "response": "def normcase(self, path):\n        \"\"\"Convert to lower case under windows, replaces additional path\n        separator.\"\"\"\n        path = self.filesystem.normcase(path)\n        if self.filesystem.is_windows_fs:\n            path = path.lower()\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef relpath(self, path, start=None):\n        if not path:\n            raise ValueError(\"no path specified\")\n        path = make_string_path(path)\n        if start is not None:\n            start = make_string_path(start)\n        else:\n            start = self.filesystem.cwd\n        if self.filesystem.alternative_path_separator is not None:\n            path = path.replace(self.filesystem.alternative_path_separator,\n                                self._os_path.sep)\n            start = start.replace(self.filesystem.alternative_path_separator,\n                                  self._os_path.sep)\n        path = path.replace(self.filesystem.path_separator, self._os_path.sep)\n        start = start.replace(\n            self.filesystem.path_separator, self._os_path.sep)\n        path = self._os_path.relpath(path, start)\n        return path.replace(self._os_path.sep, self.filesystem.path_separator)", "response": "This method is used to get the relative path of the base directory of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef realpath(self, filename):\n        if self.filesystem.is_windows_fs:\n            return self.abspath(filename)\n        filename = make_string_path(filename)\n        path, ok = self._joinrealpath(filename[:0], filename, {})\n        return self.abspath(path)", "response": "Return the canonical path of the specified filename eliminating any\n        symbolic links encountered in the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _joinrealpath(self, path, rest, seen):\n        curdir = self.filesystem._matching_string(path, '.')\n        pardir = self.filesystem._matching_string(path, '..')\n\n        sep = self.filesystem._path_separator(path)\n        if self.isabs(rest):\n            rest = rest[1:]\n            path = sep\n\n        while rest:\n            name, _, rest = rest.partition(sep)\n            if not name or name == curdir:\n                # current dir\n                continue\n            if name == pardir:\n                # parent dir\n                if path:\n                    path, name = self.filesystem.splitpath(path)\n                    if name == pardir:\n                        path = self.filesystem.joinpaths(path, pardir, pardir)\n                else:\n                    path = pardir\n                continue\n            newpath = self.filesystem.joinpaths(path, name)\n            if not self.filesystem.islink(newpath):\n                path = newpath\n                continue\n            # Resolve the symbolic link\n            if newpath in seen:\n                # Already seen this path\n                path = seen[newpath]\n                if path is not None:\n                    # use cached value\n                    continue\n                # The symlink is not resolved, so we must have a symlink loop.\n                # Return already resolved part + rest of the path unchanged.\n                return self.filesystem.joinpaths(newpath, rest), False\n            seen[newpath] = None  # not resolved symlink\n            path, ok = self._joinrealpath(\n                path, self.filesystem.readlink(newpath), seen)\n            if not ok:\n                return self.filesystem.joinpaths(path, rest), False\n            seen[newpath] = path  # resolved symlink\n        return path, True", "response": "Join two paths normalizing and eliminating any symbolic links\n            encountered in the second path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the argument with an initial component of ~ or ~user replaced by that user s home directory.", "response": "def expanduser(self, path):\n        \"\"\"Return the argument with an initial component of ~ or ~user\n        replaced by that user's home directory.\n        \"\"\"\n        return self._os_path.expanduser(path).replace(\n            self._os_path.sep, self.sep)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the given path is a mount point.", "response": "def ismount(self, path):\n        \"\"\"Return true if the given path is a mount point.\n\n        Args:\n            path: Path to filesystem object to be checked\n\n        Returns:\n            `True` if path is a mount point added to the fake file system.\n            Under Windows also returns True for drive and UNC roots\n            (independent of their existence).\n        \"\"\"\n        path = make_string_path(path)\n        if not path:\n            return False\n        normed_path = self.filesystem.absnormpath(path)\n        sep = self.filesystem._path_separator(path)\n        if self.filesystem.is_windows_fs:\n            if self.filesystem.alternative_path_separator is not None:\n                path_seps = (\n                    sep, self.filesystem._alternative_path_separator(path)\n                )\n            else:\n                path_seps = (sep, )\n            drive, rest = self.filesystem.splitdrive(normed_path)\n            if drive and drive[:1] in path_seps:\n                return (not rest) or (rest in path_seps)\n            if rest in path_seps:\n                return True\n        for mount_point in self.filesystem.mount_points:\n            if normed_path.rstrip(sep) == mount_point.rstrip(sep):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of patched function names. Used for patching functions imported from the module.", "response": "def dir():\n        \"\"\"Return the list of patched function names. Used for patching\n        functions imported from the module.\n        \"\"\"\n        dir = [\n            'access', 'chdir', 'chmod', 'chown', 'close', 'fstat', 'fsync',\n            'getcwd', 'lchmod', 'link', 'listdir', 'lstat', 'makedirs',\n            'mkdir', 'mknod', 'open', 'read', 'readlink', 'remove',\n            'removedirs', 'rename', 'rmdir', 'stat', 'symlink', 'umask',\n            'unlink', 'utime', 'walk', 'write'\n        ]\n        if IS_PY2:\n            dir += ['getcwdu']\n        else:\n            dir += ['getcwdb', 'replace']\n            if sys.platform.startswith('linux'):\n                dir += [\n                    'fdatasync', 'getxattr', 'listxattr',\n                    'removexattr', 'setxattr'\n                ]\n        if use_scandir:\n            dir += ['scandir']\n        return dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fdopen_ver2(self, file_des, mode='r',\n                     bufsize=None):  # pylint: disable=unused-argument\n        \"\"\"Returns an open file object connected to the file descriptor\n        file_des.\n\n        Args:\n            file_des: An integer file descriptor for the file object requested.\n            mode: Additional file flags. Currently checks to see if the mode\n                matches the mode of the requested file object.\n            bufsize: ignored. (Used for signature compliance with\n                __builtin__.fdopen)\n\n        Returns:\n            File object corresponding to file_des.\n\n        Raises:\n            OSError: if bad file descriptor or incompatible mode is given.\n            TypeError: if file descriptor is not an integer.\n        \"\"\"\n        if not is_int_type(file_des):\n            raise TypeError('an integer is required')\n\n        try:\n            return FakeFileOpen(self.filesystem).call(file_des, mode=mode)\n        except IOError as exc:\n            self.filesystem.raise_os_error(exc.errno, exc.filename)", "response": "Returns an open file object connected to the file descriptor file_des."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _umask(self):\n        if self.filesystem.is_windows_fs:\n            # windows always returns 0 - it has no real notion of umask\n            return 0\n        if sys.platform == 'win32':\n            # if we are testing Unix under Windows we assume a default mask\n            return 0o002\n        else:\n            # under Unix, we return the real umask;\n            # as there is no pure getter for umask, so we have to first\n            # set a mode to get the previous one and then re-set that\n            mask = os.umask(0)\n            os.umask(mask)\n            return mask", "response": "Return the current umask."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a file and return a file descriptor.", "response": "def open(self, file_path, flags, mode=None, dir_fd=None):\n        \"\"\"Return the file descriptor for a FakeFile.\n\n        Args:\n            file_path: the path to the file\n            flags: low-level bits to indicate io operation\n            mode: bits to define default permissions\n                Note: only basic modes are supported, OS-specific modes are\n                ignored\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `file_path` being relative to this directory.\n                New in Python 3.3.\n\n        Returns:\n            A file descriptor.\n\n        Raises:\n            IOError: if the path cannot be found\n            ValueError: if invalid mode is given\n            NotImplementedError: if `os.O_EXCL` is used without `os.O_CREAT`\n        \"\"\"\n        file_path = self._path_with_dir_fd(file_path, self.open, dir_fd)\n        if mode is None:\n            if self.filesystem.is_windows_fs:\n                mode = 0o666\n            else:\n                mode = 0o777 & ~self._umask()\n\n        open_modes = _OpenModes(\n            must_exist=not flags & os.O_CREAT,\n            can_read=not flags & os.O_WRONLY,\n            can_write=flags & (os.O_RDWR | os.O_WRONLY),\n            truncate=flags & os.O_TRUNC,\n            append=flags & os.O_APPEND,\n            must_not_exist=flags & os.O_EXCL\n        )\n        if open_modes.must_not_exist and open_modes.must_exist:\n            raise NotImplementedError(\n                'O_EXCL without O_CREAT mode is not supported')\n\n        if (not self.filesystem.is_windows_fs and\n                self.filesystem.exists(file_path)):\n            # handle opening directory - only allowed under Posix\n            # with read-only mode\n            obj = self.filesystem.resolve(file_path)\n            if isinstance(obj, FakeDirectory):\n                if ((not open_modes.must_exist and\n                     not self.filesystem.is_macos)\n                        or open_modes.can_write):\n                    self.filesystem.raise_os_error(errno.EISDIR, file_path)\n                dir_wrapper = FakeDirWrapper(obj, file_path, self.filesystem)\n                file_des = self.filesystem._add_open_file(dir_wrapper)\n                dir_wrapper.filedes = file_des\n                return file_des\n\n        # low level open is always binary\n        str_flags = 'b'\n        delete_on_close = False\n        if hasattr(os, 'O_TEMPORARY'):\n            delete_on_close = flags & os.O_TEMPORARY == os.O_TEMPORARY\n        fake_file = FakeFileOpen(\n            self.filesystem, delete_on_close=delete_on_close, raw_io=True)(\n            file_path, str_flags, open_modes=open_modes)\n        if fake_file.file_object != self.filesystem.dev_null:\n            self.chmod(file_path, mode)\n        return fake_file.fileno()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self, file_des):\n        file_handle = self.filesystem.get_open_file(file_des)\n        file_handle.close()", "response": "Closes a file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a number of bytes from a file descriptor.", "response": "def read(self, file_des, num_bytes):\n        \"\"\"Read number of bytes from a file descriptor, returns bytes read.\n\n        Args:\n            file_des: An integer file descriptor for the file object requested.\n            num_bytes: Number of bytes to read from file.\n\n        Returns:\n            Bytes read from file.\n\n        Raises:\n            OSError: bad file descriptor.\n            TypeError: if file descriptor is not an integer.\n        \"\"\"\n        file_handle = self.filesystem.get_open_file(file_des)\n        file_handle.raw_io = True\n        return file_handle.read(num_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite string to file descriptor returns number of bytes written.", "response": "def write(self, file_des, contents):\n        \"\"\"Write string to file descriptor, returns number of bytes written.\n\n        Args:\n            file_des: An integer file descriptor for the file object requested.\n            contents: String of bytes to write to file.\n\n        Returns:\n            Number of bytes written.\n\n        Raises:\n            OSError: bad file descriptor.\n            TypeError: if file descriptor is not an integer.\n        \"\"\"\n        file_handle = self.filesystem.get_open_file(file_des)\n        if isinstance(file_handle, FakeDirWrapper):\n            self.filesystem.raise_os_error(errno.EBADF, file_handle.file_path)\n\n        if isinstance(file_handle, FakePipeWrapper):\n            return file_handle.write(contents)\n\n        file_handle.raw_io = True\n        file_handle._sync_io()\n        file_handle.update_flush_pos()\n        file_handle.write(contents)\n        file_handle.flush()\n        return len(contents)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the os. stat - like tuple for the FakeFile object of file_des.", "response": "def fstat(self, file_des):\n        \"\"\"Return the os.stat-like tuple for the FakeFile object of file_des.\n\n        Args:\n            file_des: The file descriptor of filesystem object to retrieve.\n\n        Returns:\n            The FakeStatResult object corresponding to entry_path.\n\n        Raises:\n            OSError: if the filesystem object doesn't exist.\n        \"\"\"\n        # stat should return the tuple representing return value of os.stat\n        file_object = self.filesystem.get_open_file(file_des).get_object()\n        return file_object.stat_result.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges the umask of the current file entry.", "response": "def umask(self, new_mask):\n        \"\"\"Change the current umask.\n\n        Args:\n            new_mask: (int) The new umask value.\n\n        Returns:\n            The old umask.\n\n        Raises:\n            TypeError: if new_mask is of an invalid type.\n        \"\"\"\n        if not is_int_type(new_mask):\n            raise TypeError('an integer is required')\n        old_umask = self.filesystem.umask\n        self.filesystem.umask = new_mask\n        return old_umask"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the current working directory to target directory.", "response": "def chdir(self, target_directory):\n        \"\"\"Change current working directory to target directory.\n\n        Args:\n            target_directory: The path to new current working directory.\n\n        Raises:\n            OSError: if user lacks permission to enter the argument directory\n                or if the target is not a directory.\n        \"\"\"\n        target_directory = self.filesystem.resolve_path(\n            target_directory, allow_fd=True)\n        self.filesystem.confirmdir(target_directory)\n        directory = self.filesystem.resolve(target_directory)\n        # A full implementation would check permissions all the way\n        # up the tree.\n        if not is_root() and not directory.st_mode | PERM_EXE:\n            self.filesystem.raise_os_error(errno.EACCES, directory)\n        self.filesystem.cwd = target_directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readlink(self, path, dir_fd=None):\n        path = self._path_with_dir_fd(path, self.readlink, dir_fd)\n        return self.filesystem.readlink(path)", "response": "Read the target of a symlink."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stat(self, entry_path, dir_fd=None, follow_symlinks=None):\n        if follow_symlinks is None:\n            follow_symlinks = True\n        elif sys.version_info < (3, 3):\n            raise TypeError(\n                \"stat() got an unexpected keyword argument 'follow_symlinks'\")\n        entry_path = self._path_with_dir_fd(entry_path, self.stat, dir_fd)\n        return self.filesystem.stat(entry_path, follow_symlinks)", "response": "Returns the os. stat - like tuple for the FakeFile object corresponding to entry_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lstat(self, entry_path, dir_fd=None):\n        # stat should return the tuple representing return value of os.stat\n        entry_path = self._path_with_dir_fd(entry_path, self.lstat, dir_fd)\n        return self.filesystem.stat(entry_path, follow_symlinks=False)", "response": "Returns the os. stat - like tuple for entry_path not following symlinks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove(self, path, dir_fd=None):\n        path = self._path_with_dir_fd(path, self.remove, dir_fd)\n        self.filesystem.remove(path)", "response": "Remove the FakeFile object at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename(self, old_file_path, new_file_path, dir_fd=None):\n        old_file_path = self._path_with_dir_fd(\n            old_file_path, self.rename, dir_fd)\n        self.filesystem.rename(old_file_path, new_file_path)", "response": "Rename a FakeFile object at old_file_path to new_file_path preserving all properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a leaf Fake directory.", "response": "def rmdir(self, target_directory, dir_fd=None):\n        \"\"\"Remove a leaf Fake directory.\n\n        Args:\n            target_directory: (str) Name of directory to remove.\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `target_directory` being relative to this directory.\n                New in Python 3.3.\n\n        Raises:\n            OSError: if target_directory does not exist or is not a directory,\n            or as per FakeFilesystem.remove_object. Cannot remove '.'.\n        \"\"\"\n        target_directory = self._path_with_dir_fd(\n            target_directory, self.rmdir, dir_fd)\n        self.filesystem.rmdir(target_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a leaf fake directory and all empty intermediate ones.", "response": "def removedirs(self, target_directory):\n        \"\"\"Remove a leaf fake directory and all empty intermediate ones.\n\n        Args:\n            target_directory: the directory to be removed.\n\n        Raises:\n            OSError: if target_directory does not exist or is not a directory.\n            OSError: if target_directory is not empty.\n        \"\"\"\n        target_directory = self.filesystem.absnormpath(target_directory)\n        directory = self.filesystem.confirmdir(target_directory)\n        if directory.contents:\n            self.filesystem.raise_os_error(\n                errno.ENOTEMPTY, self.path.basename(target_directory))\n        else:\n            self.rmdir(target_directory)\n        head, tail = self.path.split(target_directory)\n        if not tail:\n            head, tail = self.path.split(head)\n        while head and tail:\n            head_dir = self.filesystem.confirmdir(head)\n            if head_dir.contents:\n                break\n            # only the top-level dir may not be a symlink\n            self.filesystem.rmdir(head, allow_symlink=True)\n            head, tail = self.path.split(head)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a leaf Fake directory.", "response": "def mkdir(self, dir_name, mode=PERM_DEF, dir_fd=None):\n        \"\"\"Create a leaf Fake directory.\n\n        Args:\n            dir_name: (str) Name of directory to create.\n                Relative paths are assumed to be relative to '/'.\n            mode: (int) Mode to create directory with.  This argument defaults\n                to 0o777.  The umask is applied to this mode.\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `dir_name` being relative to this directory.\n                New in Python 3.3.\n\n        Raises:\n            OSError: if the directory name is invalid or parent directory is\n                read only or as per FakeFilesystem.add_object.\n        \"\"\"\n        dir_name = self._path_with_dir_fd(dir_name, self.mkdir, dir_fd)\n        try:\n            self.filesystem.makedir(dir_name, mode)\n        except IOError as e:\n            if e.errno == errno.EACCES:\n                self.filesystem.raise_os_error(e.errno, dir_name)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a leaf Fake directory + create any non - existent parent dirs.", "response": "def makedirs(self, dir_name, mode=PERM_DEF, exist_ok=None):\n        \"\"\"Create a leaf Fake directory + create any non-existent parent dirs.\n\n        Args:\n            dir_name: (str) Name of directory to create.\n            mode: (int) Mode to create directory (and any necessary parent\n                directories) with. This argument defaults to 0o777.\n                The umask is applied to this mode.\n            exist_ok: (boolean) If exist_ok is False (the default), an OSError\n                is raised if the target directory already exists.\n                New in Python 3.2.\n\n        Raises:\n            OSError: if the directory already exists and exist_ok=False, or as\n                per :py:meth:`FakeFilesystem.create_dir`.\n        \"\"\"\n        if exist_ok is None:\n            exist_ok = False\n        elif sys.version_info < (3, 2):\n            raise TypeError(\"makedir() got an unexpected \"\n                            \"keyword argument 'exist_ok'\")\n        self.filesystem.makedirs(dir_name, mode, exist_ok)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _path_with_dir_fd(self, path, fct, dir_fd):\n        if dir_fd is not None:\n            if sys.version_info < (3, 3):\n                raise TypeError(\"%s() got an unexpected keyword \"\n                                \"argument 'dir_fd'\" % fct.__name__)\n            # check if fd is supported for the built-in real function\n            real_fct = getattr(os, fct.__name__)\n            if real_fct not in self.supports_dir_fd:\n                raise NotImplementedError(\n                    'dir_fd unavailable on this platform')\n            if isinstance(path, int):\n                raise ValueError(\"%s: Can't specify dir_fd without \"\n                                 \"matching path\" % fct.__name__)\n            if not self.path.isabs(path):\n                return self.path.join(\n                    self.filesystem.get_open_file(\n                        dir_fd).get_object().path, path)\n        return path", "response": "Return the path considering dir_fd. Raise on nmvalid parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef access(self, path, mode, dir_fd=None, follow_symlinks=None):\n        if follow_symlinks is not None and sys.version_info < (3, 3):\n            raise TypeError(\"access() got an unexpected \"\n                            \"keyword argument 'follow_symlinks'\")\n        path = self._path_with_dir_fd(path, self.access, dir_fd)\n        try:\n            stat_result = self.stat(path, follow_symlinks=follow_symlinks)\n        except OSError as os_error:\n            if os_error.errno == errno.ENOENT:\n                return False\n            raise\n        if is_root():\n            mode &= ~os.W_OK\n        return (mode & ((stat_result.st_mode >> 6) & 7)) == mode", "response": "Check if a file exists and has the specified permissions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chmod(self, path, mode, dir_fd=None, follow_symlinks=None):\n        if follow_symlinks is None:\n            follow_symlinks = True\n        elif sys.version_info < (3, 3):\n            raise TypeError(\n                \"chmod() got an unexpected keyword argument 'follow_symlinks'\")\n        path = self._path_with_dir_fd(path, self.chmod, dir_fd)\n        self.filesystem.chmod(path, mode, follow_symlinks)", "response": "Change the permissions of a file as encoded in integer mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the permissions of a file as encoded in integer mode.", "response": "def lchmod(self, path, mode):\n        \"\"\"Change the permissions of a file as encoded in integer mode.\n        If the file is a link, the permissions of the link are changed.\n\n        Args:\n          path: (str) Path to the file.\n          mode: (int) Permissions.\n        \"\"\"\n        if self.filesystem.is_windows_fs:\n            raise (NameError, \"name 'lchmod' is not defined\")\n        self.filesystem.chmod(path, mode, follow_symlinks=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the access and modified times of a file.", "response": "def utime(self, path, times=None, ns=None,\n              dir_fd=None, follow_symlinks=None):\n        \"\"\"Change the access and modified times of a file.\n\n        Args:\n            path: (str) Path to the file.\n            times: 2-tuple of int or float numbers, of the form (atime, mtime)\n                which is used to set the access and modified times in seconds.\n                If None, both times are set to the current time.\n            ns: 2-tuple of int numbers, of the form (atime, mtime)  which is\n                used to set the access and modified times in nanoseconds.\n                If None, both times are set to the current time.\n                New in Python 3.3.\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `path` being relative to this directory.\n                New in Python 3.3.\n            follow_symlinks: (bool) If `False` and `path` points to a symlink,\n                the link itself is queried instead of the linked object.\n                New in Python 3.3.\n\n            Raises:\n                TypeError: If anything other than the expected types is\n                    specified in the passed `times` or `ns` tuple,\n                    or if the tuple length is not equal to 2.\n                ValueError: If both times and ns are specified.\n        \"\"\"\n        if follow_symlinks is None:\n            follow_symlinks = True\n        elif sys.version_info < (3, 3):\n            raise TypeError(\n                \"utime() got an unexpected keyword argument 'follow_symlinks'\")\n        path = self._path_with_dir_fd(path, self.utime, dir_fd)\n        if ns is not None and sys.version_info < (3, 3):\n            raise TypeError(\"utime() got an unexpected keyword argument 'ns'\")\n\n        self.filesystem.utime(path, times, ns, follow_symlinks)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets ownership of a faked file or directory.", "response": "def chown(self, path, uid, gid, dir_fd=None, follow_symlinks=None):\n        \"\"\"Set ownership of a faked file.\n\n        Args:\n            path: (str) Path to the file or directory.\n            uid: (int) Numeric uid to set the file or directory to.\n            gid: (int) Numeric gid to set the file or directory to.\n            dir_fd: (int) If not `None`, the file descriptor of a directory,\n                with `path` being relative to this directory.\n                New in Python 3.3.\n            follow_symlinks: (bool) If `False` and path points to a symlink,\n                the link itself is changed instead of the linked object.\n                New in Python 3.3.\n\n        Raises:\n            OSError: if path does not exist.\n\n        `None` is also allowed for `uid` and `gid`.  This permits `os.rename`\n        to use `os.chown` even when the source file `uid` and `gid` are\n        `None` (unset).\n        \"\"\"\n        if follow_symlinks is None:\n            follow_symlinks = True\n        elif sys.version_info < (3, 3):\n            raise TypeError(\n                \"chown() got an unexpected keyword argument 'follow_symlinks'\")\n        path = self._path_with_dir_fd(path, self.chown, dir_fd)\n        try:\n            file_object = self.filesystem.resolve(\n                path, follow_symlinks, allow_fd=True)\n        except IOError as io_error:\n            if io_error.errno == errno.ENOENT:\n                self.filesystem.raise_os_error(errno.ENOENT, path)\n            raise\n        if not ((is_int_type(uid) or uid is None) and\n                (is_int_type(gid) or gid is None)):\n            raise TypeError(\"An integer is required\")\n        if uid != -1:\n            file_object.st_uid = uid\n        if gid != -1:\n            file_object.st_gid = gid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new file node named filename.", "response": "def mknod(self, filename, mode=None, device=None, dir_fd=None):\n        \"\"\"Create a filesystem node named 'filename'.\n\n        Does not support device special files or named pipes as the real os\n        module does.\n\n        Args:\n            filename: (str) Name of the file to create\n            mode: (int) Permissions to use and type of file to be created.\n                Default permissions are 0o666.  Only the stat.S_IFREG file type\n                is supported by the fake implementation.  The umask is applied\n                to this mode.\n            device: not supported in fake implementation\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `filename` being relative to this directory.\n                New in Python 3.3.\n\n        Raises:\n          OSError: if called with unsupported options or the file can not be\n          created.\n        \"\"\"\n        if self.filesystem.is_windows_fs:\n            raise(AttributeError, \"module 'os' has no attribute 'mknode'\")\n        if mode is None:\n            # note that a default value of 0o600 without a device type is\n            # documented - this is not how it seems to work\n            mode = S_IFREG | 0o600\n        if device or not mode & S_IFREG and not is_root():\n            self.filesystem.raise_os_error(errno.EPERM)\n\n        filename = self._path_with_dir_fd(filename, self.mknod, dir_fd)\n        head, tail = self.path.split(filename)\n        if not tail:\n            if self.filesystem.exists(head, check_link=True):\n                self.filesystem.raise_os_error(errno.EEXIST, filename)\n            self.filesystem.raise_os_error(errno.ENOENT, filename)\n        if tail in (b'.', u'.', b'..', u'..'):\n            self.filesystem.raise_os_error(errno.ENOENT, filename)\n        if self.filesystem.exists(filename, check_link=True):\n            self.filesystem.raise_os_error(errno.EEXIST, filename)\n        try:\n            self.filesystem.add_object(head, FakeFile(\n                tail, mode & ~self.filesystem.umask,\n                filesystem=self.filesystem))\n        except IOError as e:\n            self.filesystem.raise_os_error(e.errno, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a symlink pointing at the specified link target.", "response": "def symlink(self, link_target, path, dir_fd=None):\n        \"\"\"Creates the specified symlink, pointed at the specified link target.\n\n        Args:\n            link_target: The target of the symlink.\n            path: Path to the symlink to create.\n            dir_fd: If not `None`, the file descriptor of a directory,\n                with `link_target` being relative to this directory.\n                New in Python 3.3.\n\n        Raises:\n            OSError:  if the file already exists.\n        \"\"\"\n        link_target = self._path_with_dir_fd(link_target, self.symlink, dir_fd)\n        self.filesystem.create_symlink(\n            path, link_target, create_missing_dirs=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef link(self, oldpath, newpath, dir_fd=None):\n        oldpath = self._path_with_dir_fd(oldpath, self.link, dir_fd)\n        self.filesystem.link(oldpath, newpath)", "response": "Create a hard link at oldpath pointing at newpath."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fsync(self, file_des):\n        # Throw an error if file_des isn't valid\n        if 0 <= file_des < NR_STD_STREAMS:\n            self.filesystem.raise_os_error(errno.EINVAL)\n        file_object = self.filesystem.get_open_file(file_des)\n        if self.filesystem.is_windows_fs:\n            if (not hasattr(file_object, 'allow_update') or\n                    not file_object.allow_update):\n                self.filesystem.raise_os_error(\n                    errno.EBADF, file_object.file_path)", "response": "Perform fsync for a fake file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fdatasync(self, file_des):\n        # Throw an error if file_des isn't valid\n        if self.filesystem.is_windows_fs or self.filesystem.is_macos:\n            raise AttributeError(\"module 'os' has no attribute 'fdatasync'\")\n        if 0 <= file_des < NR_STD_STREAMS:\n            self.filesystem.raise_os_error(errno.EINVAL)\n        self.filesystem.get_open_file(file_des)", "response": "Perform fdatasync for a fake file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self, file, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None, closefd=True, opener=None):\n        \"\"\"Redirect the call to FakeFileOpen.\n        See FakeFileOpen.call() for description.\n        \"\"\"\n        if opener is not None and sys.version_info < (3, 3):\n            raise TypeError(\n                \"open() got an unexpected keyword argument 'opener'\")\n        fake_open = FakeFileOpen(self.filesystem, use_io=True)\n        return fake_open(file, mode, buffering, encoding, errors,\n                         newline, closefd, opener)", "response": "Open a file and return a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nflushing file contents to disk.", "response": "def flush(self):\n        \"\"\"Flush file contents to 'disk'.\"\"\"\n        self._check_open_file()\n        if self.allow_update and not self.is_stream:\n            contents = self._io.getvalue()\n            if self._append:\n                self._sync_io()\n                old_contents = (self.file_object.byte_contents\n                                if is_byte_string(contents) else\n                                self.file_object.contents)\n                contents = old_contents + contents[self._flush_pos:]\n                self._set_stream_contents(contents)\n                self.update_flush_pos()\n            else:\n                self._io.flush()\n            if self.file_object.set_contents(contents, self._encoding):\n                if self._filesystem.is_windows_fs:\n                    self._changed = True\n                else:\n                    current_time = time.time()\n                    self.file_object.st_ctime = current_time\n                    self.file_object.st_mtime = current_time\n            self._file_epoch = self.file_object.epoch\n\n            if not self.is_stream:\n                self._flush_related_files()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove read pointer in file.", "response": "def seek(self, offset, whence=0):\n        \"\"\"Move read/write pointer in 'file'.\"\"\"\n        self._check_open_file()\n        if not self._append:\n            self._io.seek(offset, whence)\n        else:\n            self._read_seek = offset\n            self._read_whence = whence\n        if not self.is_stream:\n            self.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the file s current position in bytes.", "response": "def tell(self):\n        \"\"\"Return the file's current position.\n\n        Returns:\n          int, file's current position in bytes.\n        \"\"\"\n        self._check_open_file()\n        if self._flushes_after_tell():\n            self.flush()\n\n        if not self._append:\n            return self._io.tell()\n        if self._read_whence:\n            write_seek = self._io.tell()\n            self._io.seek(self._read_seek, self._read_whence)\n            self._read_seek = self._io.tell()\n            self._read_whence = 0\n            self._io.seek(write_seek)\n        return self._read_seek"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the stream with changes to the file object contents.", "response": "def _sync_io(self):\n        \"\"\"Update the stream with changes to the file object contents.\"\"\"\n        if self._file_epoch == self.file_object.epoch:\n            return\n\n        if self._io.binary:\n            contents = self.file_object.byte_contents\n        else:\n            contents = self.file_object.contents\n\n        self._set_stream_contents(contents)\n        self._file_epoch = self.file_object.epoch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_wrappers(self, name):\n        io_attr = getattr(self._io, name)\n\n        def read_wrapper(*args, **kwargs):\n            \"\"\"Wrap all read calls to the stream object.\n\n            We do this to track the read pointer separate from the write\n            pointer.  Anything that wants to read from the stream object\n            while we're in append mode goes through this.\n\n            Args:\n                *args: pass through args\n                **kwargs: pass through kwargs\n            Returns:\n                Wrapped stream object method\n            \"\"\"\n            self._io.seek(self._read_seek, self._read_whence)\n            ret_value = io_attr(*args, **kwargs)\n            self._read_seek = self._io.tell()\n            self._read_whence = 0\n            self._io.seek(0, 2)\n            return ret_value\n\n        return read_wrapper", "response": "Wrap a stream attribute in a read wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _other_wrapper(self, name, writing):\n        io_attr = getattr(self._io, name)\n\n        def other_wrapper(*args, **kwargs):\n            \"\"\"Wrap all other calls to the stream Object.\n\n            We do this to track changes to the write pointer.  Anything that\n            moves the write pointer in a file open for appending should move\n            the read pointer as well.\n\n            Args:\n                *args: Pass through args.\n                **kwargs: Pass through kwargs.\n\n            Returns:\n                Wrapped stream object method.\n            \"\"\"\n            write_seek = self._io.tell()\n            ret_value = io_attr(*args, **kwargs)\n            if write_seek != self._io.tell():\n                self._read_seek = self._io.tell()\n                self._read_whence = 0\n            if not writing or not IS_PY2:\n                return ret_value\n\n        return other_wrapper", "response": "Wrap a stream attribute in an other_wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _truncate_wrapper(self):\n        io_attr = getattr(self._io, 'truncate')\n\n        def truncate_wrapper(*args, **kwargs):\n            \"\"\"Wrap truncate call to call flush after truncate.\"\"\"\n            if self._append:\n                self._io.seek(self._read_seek, self._read_whence)\n            size = io_attr(*args, **kwargs)\n            self.flush()\n            if not self.is_stream:\n                self.file_object.size = size\n                buffer_size = len(self._io.getvalue())\n                if buffer_size < size:\n                    self._io.seek(buffer_size)\n                    self._io.write('\\0' * (size - buffer_size))\n                    self.file_object.set_contents(\n                        self._io.getvalue(), self._encoding)\n                    self._flush_pos = size\n                    if self._filesystem.is_macos or sys.version_info[0] > 2:\n                        self._adapt_size_for_related_files(size - buffer_size)\n\n            self.flush()\n            if not IS_PY2:\n                return size\n\n        return truncate_wrapper", "response": "Wrap truncate to allow flush after truncate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping all write calls to the stream object.", "response": "def _write_wrapper(self, name):\n        \"\"\"Wrap write() to adapt return value for Python 2.\n\n        Returns:\n            Wrapper which is described below.\n        \"\"\"\n        io_attr = getattr(self._io, name)\n\n        def write_wrapper(*args, **kwargs):\n            \"\"\"Wrap all write calls to the stream object.\"\"\"\n            ret_value = io_attr(*args, **kwargs)\n            if not IS_PY2:\n                return ret_value\n\n        return write_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        self._filesystem.open_files[self.filedes].remove(self)\n        os.close(self.fd)", "response": "Close the pipe descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _call_ver2(self, file_path, mode='r', buffering=-1, flags=None,\n                   open_modes=None):\n        \"\"\"Limits args of open() or file() for Python 2.x versions.\"\"\"\n        # Backwards compatibility, mode arg used to be named flags\n        mode = flags or mode\n        return self.call(file_path, mode, buffering, open_modes=open_modes)", "response": "Calls the file_path with the given mode and buffering."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a file - like object containing the contents of the target file.", "response": "def call(self, file_, mode='r', buffering=-1, encoding=None,\n             errors=None, newline=None, closefd=True, opener=None,\n             open_modes=None):\n        \"\"\"Return a file-like object with the contents of the target\n        file object.\n\n        Args:\n            file_: Path to target file or a file descriptor.\n            mode: Additional file modes (all modes in `open()` are supported).\n            buffering: ignored. (Used for signature compliance with\n                __builtin__.open)\n            encoding: The encoding used to encode unicode strings / decode\n                bytes.\n            errors: (str) Defines how encoding errors are handled.\n            newline: Controls universal newlines, passed to stream object.\n            closefd: If a file descriptor rather than file name is passed,\n                and this is set to `False`, then the file descriptor is kept\n                open when file is closed.\n            opener: not supported.\n            open_modes: Modes for opening files if called from low-level API.\n\n        Returns:\n            A file-like object containing the contents of the target file.\n\n        Raises:\n            IOError, OSError depending on Python version / call mode:\n                - if the target object is a directory\n                - on an invalid path\n                - if the file does not exist when it should\n                - if the file exists but should not\n                - if permission is denied\n            ValueError: for an invalid mode or mode combination\n        \"\"\"\n        binary = 'b' in mode\n        newline, open_modes = self._handle_file_mode(mode, newline, open_modes)\n\n        file_object, file_path, filedes, real_path = self._handle_file_arg(\n            file_)\n        if not filedes:\n            closefd = True\n\n        error_fct = (self.filesystem.raise_os_error if self.raw_io\n                     else self.filesystem.raise_io_error)\n        if (open_modes.must_not_exist and\n                (file_object or self.filesystem.islink(file_path) and\n                 not self.filesystem.is_windows_fs)):\n            error_fct(errno.EEXIST, file_path)\n        if file_object:\n            if (not is_root() and\n                    ((open_modes.can_read and\n                      not file_object.st_mode & PERM_READ)\n                     or (open_modes.can_write and\n                         not file_object.st_mode & PERM_WRITE))):\n                error_fct(errno.EACCES, file_path)\n            if open_modes.can_write:\n                if open_modes.truncate:\n                    file_object.set_contents('')\n        else:\n            if open_modes.must_exist:\n                error_fct(errno.ENOENT, file_path)\n            if self.filesystem.islink(file_path):\n                link_object = self.filesystem.resolve(file_path,\n                                                      follow_symlinks=False)\n                target_path = link_object.contents\n            else:\n                target_path = file_path\n            if self.filesystem.ends_with_path_separator(target_path):\n                error = (errno.EINVAL if self.filesystem.is_windows_fs\n                         else errno.ENOENT if self.filesystem.is_macos\n                         else errno.EISDIR)\n                error_fct(error, file_path)\n            file_object = self.filesystem.create_file_internally(\n                real_path, create_missing_dirs=False,\n                apply_umask=True, raw_io=self.raw_io)\n\n        if S_ISDIR(file_object.st_mode):\n            if self.filesystem.is_windows_fs:\n                error_fct(errno.EACCES, file_path)\n            else:\n                error_fct(errno.EISDIR, file_path)\n\n        # If you print obj.name, the argument to open() must be printed.\n        # Not the abspath, not the filename, but the actual argument.\n        file_object.opened_as = file_path\n        if open_modes.truncate:\n            current_time = time.time()\n            file_object.st_mtime = current_time\n            if not self.filesystem.is_windows_fs:\n                file_object.st_ctime = current_time\n\n        fakefile = FakeFileWrapper(file_object,\n                                   file_path,\n                                   update=open_modes.can_write,\n                                   read=open_modes.can_read,\n                                   append=open_modes.append,\n                                   delete_on_close=self._delete_on_close,\n                                   filesystem=self.filesystem,\n                                   newline=newline,\n                                   binary=binary,\n                                   closefd=closefd,\n                                   encoding=encoding,\n                                   errors=errors,\n                                   raw_io=self.raw_io,\n                                   use_io=self._use_io)\n        if filedes is not None:\n            fakefile.filedes = filedes\n            # replace the file wrapper\n            self.filesystem.open_files[filedes].append(fakefile)\n        else:\n            fakefile.filedes = self.filesystem._add_open_file(fakefile)\n        return fakefile"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if val is of integer type.", "response": "def is_int_type(val):\n    \"\"\"Return True if `val` is of integer type.\"\"\"\n    try:               # Python 2\n        return isinstance(val, (int, long))\n    except NameError:  # Python 3\n        return isinstance(val, int)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copy(self):\n        stat_result = copy(self)\n        stat_result.use_float = self.use_float\n        return stat_result", "response": "Return a copy of the os. stat_result object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset values from a real os. stat_result.", "response": "def set_from_stat_result(self, stat_result):\n        \"\"\"Set values from a real os.stat_result.\n        Note: values that are controlled by the fake filesystem are not set.\n        This includes st_ino, st_dev and st_nlink.\n        \"\"\"\n        self.st_mode = stat_result.st_mode\n        self.st_uid = stat_result.st_uid\n        self.st_gid = stat_result.st_gid\n        self._st_size = stat_result.st_size\n        if sys.version_info < (3, 3):\n            self._st_atime_ns = self.long_type(stat_result.st_atime * 1e9)\n            self._st_mtime_ns = self.long_type(stat_result.st_mtime * 1e9)\n            self._st_ctime_ns = self.long_type(stat_result.st_ctime * 1e9)\n        else:\n            self._st_atime_ns = stat_result.st_atime_ns\n            self._st_mtime_ns = stat_result.st_mtime_ns\n            self._st_ctime_ns = stat_result.st_ctime_ns"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stat_float_times(cls, newvalue=None):\n        if newvalue is not None:\n            cls._stat_float_times = bool(newvalue)\n        return cls._stat_float_times", "response": "Determine whether a file s time stamps are reported as floats or ints."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the creation time in seconds.", "response": "def st_ctime(self):\n        \"\"\"Return the creation time in seconds.\"\"\"\n        ctime = self._st_ctime_ns / 1e9\n        return ctime if self.use_float else int(ctime)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef st_atime(self):\n        atime = self._st_atime_ns / 1e9\n        return atime if self.use_float else int(atime)", "response": "Return the access time in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef st_mtime(self):\n        mtime = self._st_mtime_ns / 1e9\n        return mtime if self.use_float else int(mtime)", "response": "Return the modification time in seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_content(qfile, refresh, *args, **kwds):\n    if not refresh and os.path.exists(qfile):\n        with open(qfile, 'rb') as f:\n            content = f.read()\n    else:\n        content = download(*args, **kwds).text.encode('utf-8')\n        with open(qfile, 'wb') as f:\n            f.write(content)\n    return content", "response": "Helper function to read the content of a file as xml."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef name_variants(self):\n        out = []\n        variant = namedtuple('Variant', 'name doc_count')\n        for var in chained_get(self._json, ['name-variants', 'name-variant'], []):\n            new = variant(name=var['$'], doc_count=var.get('@doc-count'))\n            out.append(new)\n        return out", "response": "A list of namedtuples representing variants of the affiliation name\n        with number of documents referring to this variant."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef date_created(self):\n        date_created = self.xml.find('institution-profile/date-created')\n        if date_created is not None:\n            date_created = (int(date_created.attrib['year']),\n                            int(date_created.attrib['month']),\n                            int(date_created.attrib['day']))\n        else:\n            date_created = (None, None, None)\n        return date_created", "response": "Date the Scopus record was created."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef url(self):\n        url = self.xml.find('coredata/link[@rel=\"scopus-affiliation\"]')\n        if url is not None:\n            url = url.get('href')\n        return url", "response": "URL to the affiliation s profile page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chained_get(container, path, default=None):\n    for key in path:\n        try:\n            container = container[key]\n        except (AttributeError, KeyError, TypeError):\n            return default\n    return container", "response": "This function is used to perform a series of. get methods on a dictionary\n    and return a default object type in the end."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_date_created(dct):\n    date = dct['date-created']\n    if date:\n        return (int(date['@year']), int(date['@month']), int(date['@day']))\n    else:\n        return (None, None, None)", "response": "Helper function to parse date - created from profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef affiliation_history(self):\n        affs = self._json.get('affiliation-history', {}).get('affiliation')\n        try:\n            return [d['@id'] for d in affs]\n        except TypeError:  # No affiliation history\n            return None", "response": "Unordered list of IDs of all affiliations the author was\niated with acccording to Scopus."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists with ( subject group ID number of documents)-tuples.", "response": "def classificationgroup(self):\n        \"\"\"List with (subject group ID, number of documents)-tuples.\"\"\"\n        path = ['author-profile', 'classificationgroup', 'classifications',\n                'classification']\n        out = [(item['$'], item['@frequency']) for item in\n               listify(chained_get(self._json, path, []))]\n        return out or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef historical_identifier(self):\n        hist = chained_get(self._json, [\"coredata\", 'historical-identifier'], [])\n        return [d['$'].split(\":\")[-1] for d in hist] or None", "response": "Scopus IDs of previous profiles now compromising this profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists of named tuples of authored publications in the form sourcetitle abbreviation type issn.", "response": "def journal_history(self):\n        \"\"\"List of named tuples of authored publications in the form\n        (sourcetitle, abbreviation, type, issn).  issn is only given\n        for journals.  abbreviation and issn may be None.\n        \"\"\"\n        jour = namedtuple('Journal', 'sourcetitle abbreviation type issn')\n        path = ['author-profile', 'journal-history', 'journal']\n        hist = [jour(sourcetitle=pub['sourcetitle'], issn=pub.get('issn'),\n                     abbreviation=pub.get('sourcetitle-abbrev'),\n                     type=pub['@type'])\n                for pub in listify(chained_get(self._json, path, []))]\n        return hist or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef name_variants(self):\n        fields = 'indexed_name initials surname given_name doc_count'\n        variant = namedtuple('Variant', fields)\n        path = ['author-profile', 'name-variant']\n        out = [variant(indexed_name=var['indexed-name'], surname=var['surname'],\n                       doc_count=var.get('@doc-count'), initials=var['initials'],\n                       given_name=var.get('given-name'))\n               for var in listify(chained_get(self._json, path, []))]\n        return out or None", "response": "List of named tuples containing variants of the author name with their number of documents published with that variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publication_range(self):\n        r = self._json['author-profile']['publication-range']\n        return (r['@start'], r['@end'])\n        return self._json['coredata'].get('orcid')", "response": "Tuple containing years of first and last publication."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subject_areas(self):\n        path = ['subject-areas', 'subject-area']\n        area = namedtuple('Subjectarea', 'area abbreviation code')\n        areas = [area(area=item['$'], code=item['@code'],\n                      abbreviation=item['@abbrev'])\n                 for item in chained_get(self._json, path, [])]\n        return areas or None", "response": "List of named tuples of subject areas in the form of subject - areas abbreviation and code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve basic information about co - authors as a list of namedtuples in the form of namedtuple.", "response": "def get_coauthors(self):\n        \"\"\"Retrieves basic information about co-authors as a list of\n        namedtuples in the form\n        (surname, given_name, id, areas, affiliation_id, name, city, country),\n        where areas is a list of subject area codes joined by \"; \".\n        Note: These information will not be cached and are slow for large\n        coauthor groups.\n        \"\"\"\n        # Get number of authors to search for\n        res = download(url=self.coauthor_link, accept='json')\n        data = loads(res.text)['search-results']\n        N = int(data.get('opensearch:totalResults', 0))\n        # Store information in namedtuples\n        fields = 'surname given_name id areas affiliation_id name city country'\n        coauth = namedtuple('Coauthor', fields)\n        coauthors = []\n        # Iterate over search results in chunks of 25 results\n        count = 0\n        while count < N:\n            params = {'start': count, 'count': 25}\n            res = download(url=self.coauthor_link, params=params, accept='json')\n            data = loads(res.text)['search-results'].get('entry', [])\n            # Extract information for each coauthor\n            for entry in data:\n                aff = entry.get('affiliation-current', {})\n                try:\n                    areas = [a['$'] for a in entry.get('subject-area', [])]\n                except TypeError:  # Only one subject area given\n                    areas = [entry['subject-area']['$']]\n                new = coauth(surname=entry['preferred-name']['surname'],\n                    given_name=entry['preferred-name'].get('given-name'),\n                    id=entry['dc:identifier'].split(':')[-1],\n                    areas='; '.join(areas), name=aff.get('affiliation-name'),\n                    affiliation_id=aff.get('affiliation-id'),\n                    city=aff.get('affiliation-city'),\n                    country=aff.get('affiliation-country'))\n                coauthors.append(new)\n            count += 25\n        return coauthors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_documents(self, subtypes=None, refresh=False):\n        search = ScopusSearch('au-id({})'.format(self.identifier), refresh)\n        if subtypes:\n            return [p for p in search.results if p.subtype in subtypes]\n        else:\n            return search.results", "response": "Return list of author s publications using ScopusSearch which\n        fit a specified set of document subtypes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef results(self):\n        out = []\n        fields = 'eid doi pii pubmed_id title subtype creator afid affilname '\\\n                 'affiliation_city affiliation_country author_count '\\\n                 'author_names author_ids author_afids coverDate '\\\n                 'coverDisplayDate publicationName issn source_id eIssn '\\\n                 'aggregationType volume issueIdentifier article_number '\\\n                 'pageRange description authkeywords citedby_count '\\\n                 'openaccess fund_acr fund_no fund_sponsor'\n        doc = namedtuple('Document', fields)\n        for item in self._json:\n            info = {}\n            # Parse affiliations\n            try:\n                info[\"affilname\"] = _join(item['affiliation'], 'affilname')\n                info[\"afid\"] = _join(item['affiliation'], 'afid')\n                info[\"aff_city\"] = _join(item['affiliation'], 'affiliation-city')\n                info[\"aff_country\"] = _join(item['affiliation'],\n                                            'affiliation-country')\n            except KeyError:\n                pass\n            # Parse authors\n            try:\n                # Deduplicate list of authors\n                authors = _deduplicate(item['author'])\n                # Extract information\n                surnames = _replace_none([d['surname'] for d in authors])\n                firstnames = _replace_none([d['given-name'] for d in authors])\n                info[\"auth_names\"] = \";\".join([\", \".join([t[0], t[1]]) for t in\n                                               zip(surnames, firstnames)])\n                info[\"auth_ids\"] = \";\".join([d['authid'] for d in authors])\n                affs = []\n                for auth in authors:\n                    aff = listify(_deduplicate(auth.get('afid', [])))\n                    affs.append('-'.join([d['$'] for d in aff]))\n                info[\"auth_afid\"] = (';'.join(affs) or None)\n            except KeyError:\n                pass\n            date = item.get('prism:coverDate')\n            if isinstance(date, list):\n                date = date[0].get('$')\n            new = doc(article_number=item.get('article-number'),\n                title=item.get('dc:title'), fund_sponsor=item.get('fund-sponsor'),\n                subtype=item.get('subtype'), issn=item.get('prism:issn'),\n                creator=item.get('dc:creator'), affilname=info.get(\"affilname\"),\n                author_names=info.get(\"auth_names\"), doi=item.get('prism:doi'),\n                coverDate=date, volume=item.get('prism:volume'),\n                coverDisplayDate=item.get('prism:coverDisplayDate'),\n                publicationName=item.get('prism:publicationName'),\n                source_id=item.get('source-id'), author_ids=info.get(\"auth_ids\"),\n                aggregationType=item.get('prism:aggregationType'),\n                issueIdentifier=item.get('prism:issueIdentifier'),\n                pageRange=item.get('prism:pageRange'),\n                author_afids=info.get(\"auth_afid\"), fund_no=item.get('fund-no'),\n                affiliation_country=info.get(\"aff_country\"),\n                citedby_count=item.get('citedby-count'),\n                openaccess=item.get('openaccess'), eIssn=item.get('prism:eIssn'),\n                author_count=item.get('author-count', {}).get('$'),\n                affiliation_city=info.get(\"aff_city\"), afid=info.get(\"afid\"),\n                description=item.get('dc:description'), pii=item.get('pii'),\n                authkeywords=item.get('authkeywords'), eid=item['eid'],\n                fund_acr=item.get('fund-acr'), pubmed_id=item.get('pubmed-id'))\n            out.append(new)\n        return out or None", "response": "A list of namedtuples that correspond to the current set of results."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authors(self):\n        authors = self.xml.find('authors', ns)\n        try:\n            return [_ScopusAuthor(author) for author in authors]\n        except TypeError:\n            return None", "response": "A list of scopus_api. _ScopusAuthor objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef citedby_url(self):\n        cite_link = self.coredata.find('link[@rel=\"scopus-citedby\"]', ns)\n        try:\n            return cite_link.get('href')\n        except AttributeError:  # cite_link is None\n            return None", "response": "URL to Scopus page listing citing papers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refcount(self):\n        refs = self.items.find('bibrecord/tail/bibliography', ns)\n        try:\n            return refs.attrib['refcount']\n        except AttributeError:  # refs is None\n            return None", "response": "Number of references of an article."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef references(self):\n        refs = self.items.find('bibrecord/tail/bibliography', ns)\n        if refs is not None:\n            eids = [r.find(\"ref-info/refd-itemidlist/itemid\", ns).text for r\n                    in refs.findall(\"reference\", ns)]\n            return [\"2-s2.0-\" + eid for eid in eids]\n        else:\n            return None", "response": "Return the EIDs of references of an article."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the subject areas of the article.", "response": "def subjectAreas(self):\n        \"\"\"List of subject areas of article.\n        Note: Requires the FULL view of the article.\n        \"\"\"\n        subjectAreas = self.xml.find('subject-areas', ns)\n        try:\n            return [a.text for a in subjectAreas]\n        except:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scopus_url(self):\n        scopus_url = self.coredata.find('link[@rel=\"scopus\"]', ns)\n        try:\n            return scopus_url.get('href')\n        except AttributeError:  # scopus_url is None\n            return None", "response": "URL to the abstract page on Scopus."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to get corresponding author information.", "response": "def get_corresponding_author_info(self):\n        \"\"\"Try to get corresponding author information.\n\n        Returns (scopus-id, name, email).\n        \"\"\"\n        resp = requests.get(self.scopus_url)\n        from lxml import html\n\n        parsed_doc = html.fromstring(resp.content)\n        for div in parsed_doc.body.xpath('.//div'):\n            for a in div.xpath('a'):\n                if '/cdn-cgi/l/email-protection' not in a.get('href', ''):\n                    continue\n                encoded_text = a.attrib['href'].replace('/cdn-cgi/l/email-protection#', '')\n                key = int(encoded_text[0:2], 16)\n                email = ''.join([chr(int('0x{}'.format(x), 16) ^ key)\n                                 for x in\n                                 map(''.join, zip(*[iter(encoded_text[2:])]*2))])\n                for aa in div.xpath('a'):\n                    if 'http://www.scopus.com/authid/detail.url' in aa.get('href', ''):\n                        scopus_url = aa.attrib['href']\n                        name = aa.text\n                    else:\n                        scopus_url, name = None, None\n\n        return (scopus_url, name, email)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef latex(self):\n        s = ('{authors}, \\\\textit{{{title}}}, {journal}, {volissue}, '\n             '{pages}, ({date}). {doi}, {scopus_url}.')\n        if len(self.authors) > 1:\n            authors = ', '.join([str(a.given_name) +\n                                 ' ' + str(a.surname)\n                                 for a in self.authors[0:-1]])\n            authors += (' and ' +\n                        str(self.authors[-1].given_name) +\n                        ' ' + str(self.authors[-1].surname))\n        else:\n            a = self.authors[0]\n            authors = str(a.given_name) + ' ' + str(a.surname)\n        title = self.title\n        journal = self.publicationName\n        volume = self.volume\n        issue = self.issueIdentifier\n        if volume and issue:\n            volissue = '\\\\textbf{{{0}({1})}}'.format(volume, issue)\n        elif volume:\n            volissue = '\\\\textbf{{0}}'.format(volume)\n        else:\n            volissue = 'no volume'\n        date = self.coverDate\n        if self.pageRange:\n            pages = 'p. {0}'.format(self.pageRange)\n        elif self.startingPage:\n            pages = 'p. {self.startingPage}'.format(self)\n        elif self.article_number:\n            pages = 'Art. No. {self.article_number}, '.format(self)\n        else:\n            pages = '(no pages found)'\n        doi = '\\\\href{{https://doi.org/{0}}}{{doi:{0}}}'.format(self.doi)\n        scopus_url = '\\\\href{{{0}}}{{scopus:{1}}}'.format(self.scopus_url,\n                                                          self.eid)\n\n        return s.format(**locals())", "response": "Return LaTeX representation of the abstract."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an HTML citation.", "response": "def html(self):\n        \"\"\"Returns an HTML citation.\"\"\"\n        s = (u'{authors}, {title}, {journal}, {volissue}, {pages}, '\n             '({date}). {doi}.')\n\n        au_link = ('<a href=\"https://www.scopus.com/authid/detail.url'\n                   '?origin=AuthorProfile&authorId={0}\">{1}</a>')\n\n        if len(self.authors) > 1:\n            authors = u', '.join([au_link.format(a.auid,\n                                                (str(a.given_name) +\n                                                 ' ' + str(a.surname)))\n                                 for a in self.authors[0:-1]])\n            authors += (u' and ' +\n                        au_link.format(self.authors[-1].auid,\n                                       (str(self.authors[-1].given_name) +\n                                        ' ' +\n                                        str(self.authors[-1].surname))))\n        else:\n            a = self.authors[0]\n            authors = au_link.format(a.auid,\n                                     str(a.given_name) + ' ' + str(a.surname))\n\n        title = u'<a href=\"{link}\">{title}</a>'.format(link=self.scopus_url,\n                                                      title=self.title)\n\n        jname = self.publicationName\n        sid = self.source_id\n        jlink = ('<a href=\"https://www.scopus.com/source/sourceInfo.url'\n                 '?sourceId={sid}\">{journal}</a>')\n        journal = jlink.format(sid=sid, journal=jname)\n\n        volume = self.volume\n        issue = self.issueIdentifier\n        if volume and issue:\n            volissue = u'<b>{0}({1})</b>'.format(volume, issue)\n        elif volume:\n            volissue = u'<b>{0}</b>'.format(volume)\n        else:\n            volissue = 'no volume'\n        date = self.coverDate\n        if self.pageRange:\n            pages = u'p. {0}'.format(self.pageRange)\n        elif self.startingPage:\n            pages = u'p. {self.startingPage}'.format(self=self)\n        elif self.article_number:\n            pages = u'Art. No. {self.article_number}, '.format(self=self)\n        else:\n            pages = '(no pages found)'\n        doi = '<a href=\"https://doi.org/{0}\">doi:{0}</a>'.format(self.doi)\n\n        html = s.format(**locals())\n        return html.replace('None', '')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bibtex(self):\n        if self.aggregationType != 'Journal':\n            raise ValueError('Only Journal articles supported.')\n        template = u'''@article{{{key},\n  author = {{{author}}},\n  title = {{{title}}},\n  journal = {{{journal}}},\n  year = {{{year}}},\n  volume = {{{volume}}},\n  number = {{{number}}},\n  pages = {{{pages}}},\n  doi = {{{doi}}}\n}}\n\n'''\n        if self.pageRange:\n            pages = self.pageRange\n        elif self.startingPage:\n            pages = self.startingPage\n        elif self.article_number:\n            pages = self.article_number\n        else:\n            pages = 'no pages found'\n        year = self.coverDate[0:4]\n        first = self.title.split()[0].title()\n        last = self.title.split()[-1].title()\n        key = ''.join([self.authors[0].surname, year, first, last])\n        authors = ' and '.join([\"{} {}\".format(a.given_name, a.surname)\n                                for a in self.authors])\n        bibtex = template.format(\n            key=key, author=authors, title=self.title,\n            journal=self.publicationName, year=year, volume=self.volume,\n            number=self.issueIdentifier, pages=pages, doi=self.doi)\n        return bibtex", "response": "Return the BibTeX representation of the item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ris(self):\n        if self.aggregationType != 'Journal':\n            raise ValueError('Only Journal articles supported.')\n        template = u'''TY  - JOUR\nTI  - {title}\nJO  - {journal}\nVL  - {volume}\nDA  - {date}\nSP  - {pages}\nPY  - {year}\nDO  - {doi}\nUR  - https://doi.org/{doi}\n'''\n        ris = template.format(\n            title=self.title, journal=self.publicationName,\n            volume=self.volume, date=self.coverDate, pages=self.pageRange,\n            year=self.coverDate[0:4], doi=self.doi)\n        for au in self.authors:\n            ris += 'AU  - {}\\n'.format(au.indexed_name)\n        if self.issueIdentifier is not None:\n            ris += 'IS  - {}\\n'.format(self.issueIdentifier)\n        ris += 'ER  - \\n\\n'\n        return ris", "response": "Return the RIS string representing an item in the Research Information System format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cc(self):\n        _years = range(self._start, self._end+1)\n        try:\n            return list(zip(_years, [d.get('$') for d in self._citeInfoMatrix['cc']]))\n        except AttributeError:  # No citations\n            return list(zip(_years, [0]*len(_years)))", "response": "List of tuples of yearly number of citations\n            for specified years."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef affiliations(self):\n        out = []\n        order = 'eid name variant documents city country parent'\n        aff = namedtuple('Affiliation', order)\n        for item in self._json:\n            name = item.get('affiliation-name')\n            variants = [d.get('$', \"\") for d in item.get('name-variant', [])\n                        if d.get('$', \"\") != name]\n            new = aff(eid=item['eid'], variant=\";\".join(variants),\n                      documents=item.get('document-count', '0'), name=name,\n                      city=item.get('city'), country=item.get('country'),\n                      parent=item.get('parent-affiliation-id'))\n            out.append(new)\n        return out or None", "response": "A list of namedtuples storing affiliations for each item in the JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting of ScopusAffiliation objects representing former affiliations of the author.", "response": "def affiliation_history(self):\n        \"\"\"List of ScopusAffiliation objects representing former\n        affiliations of the author.  Only affiliations with more than one\n        publication are considered.\n        \"\"\"\n        aff_ids = [e.attrib.get('affiliation-id') for e in\n                   self.xml.findall('author-profile/affiliation-history/affiliation')\n                   if e is not None and len(list(e.find(\"ip-doc\").iter())) > 1]\n        return [ScopusAffiliation(aff_id) for aff_id in aff_ids]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndate the Scopus record was created.", "response": "def date_created(self):\n        \"\"\"Date the Scopus record was created.\"\"\"\n        date_created = self.xml.find('author-profile/date-created', ns)\n        try:\n            return (int(date_created.attrib['year']),\n                    int(date_created.attrib['month']),\n                    int(date_created.attrib['day']))\n        except AttributeError:  # date_created is None\n            return (None, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subject_areas(self):\n        areas = self.xml.findall('subject-areas/subject-area')\n        freqs = self.xml.findall('author-profile/classificationgroup/'\n                                 'classifications[@type=\"ASJC\"]/classification')\n        c = {int(cls.text): int(cls.attrib['frequency']) for cls in freqs}\n        cats = [(a.text, c[int(a.get(\"code\"))], a.get(\"abbrev\"), a.get(\"code\"))\n                for a in areas]\n        cats.sort(reverse=True, key=itemgetter(1))\n        return cats", "response": "List of tuples of author subject areas in the form\n        area frequency abbreviation code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef publication_history(self):\n        pub_hist = self.xml.findall('author-profile/journal-history/')\n        hist = []\n        for pub in pub_hist:\n            try:\n                issn = pub.find(\"issn\").text\n            except AttributeError:\n                issn = None\n            try:\n                abbr = pub.find(\"sourcetitle-abbrev\").text\n            except AttributeError:\n                abbr = None\n            hist.append((pub.find(\"sourcetitle\").text, abbr, pub.get(\"type\"), issn))\n        return hist", "response": "Return a list of tuples of authored publications in the form of a list of tuples where the first element is the title abbreviation type and the second is the issn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn list of coauthors their scopus - id and research areas.", "response": "def get_coauthors(self):\n        \"\"\"Return list of coauthors, their scopus-id and research areas.\"\"\"\n        url = self.xml.find('coredata/link[@rel=\"coauthor-search\"]').get('href')\n        xml = download(url=url).text.encode('utf-8')\n        xml = ET.fromstring(xml)\n        coauthors = []\n        N = int(get_encoded_text(xml, 'opensearch:totalResults') or 0)\n\n        AUTHOR = namedtuple('Author',\n                            ['name', 'scopus_id', 'affiliation', 'categories'])\n\n        count = 0\n        while count < N:\n            params = {'start': count, 'count': 25}\n            xml = download(url=url, params=params).text.encode('utf-8')\n            xml = ET.fromstring(xml)\n\n            for entry in xml.findall('atom:entry', ns):\n\n                given_name = get_encoded_text(entry,\n                    'atom:preferred-name/atom:given-name')\n                surname = get_encoded_text(entry,\n                    'atom:preferred-name/atom:surname')\n                coauthor_name = u'{0} {1}'.format(given_name, surname)\n\n                scopus_id = get_encoded_text(entry,\n                    'dc:identifier').replace('AUTHOR_ID:', '')\n\n                affiliation = get_encoded_text(entry,\n                    'atom:affiliation-current/atom:affiliation-name')\n\n                # get categories for this author\n                s = u', '.join(['{0} ({1})'.format(subject.text,\n                                                   subject.attrib['frequency'])\n                               for subject in\n                               entry.findall('atom:subject-area', ns)])\n\n                coauthors += [AUTHOR(coauthor_name, scopus_id, affiliation, s)]\n            count += 25\n\n        return coauthors"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_document_eids(self, *args, **kwds):\n        search = ScopusSearch('au-id({})'.format(self.author_id),\n                              *args, **kwds)\n        return search.get_eids()", "response": "Return list of EIDs for the author using ScopusSearch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of ScopusAbstract objects using ScopusSearch.", "response": "def get_abstracts(self, refresh=True):\n        \"\"\"Return a list of ScopusAbstract objects using ScopusSearch.\"\"\"\n        return [ScopusAbstract(eid, refresh=refresh)\n                for eid in self.get_document_eids(refresh=refresh)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_journal_abstracts(self, refresh=True):\n        return [abstract for abstract in self.get_abstracts(refresh=refresh) if\n                abstract.aggregationType == 'Journal']", "response": "Return a list of ScopusAbstract objects using ScopusSearch and\n           but only if belonging to a Journal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_document_summary(self, N=None, cite_sort=True, refresh=True):\n        abstracts = self.get_abstracts(refresh=refresh)\n\n        if cite_sort:\n            counts = [(a, int(a.citedby_count)) for a in abstracts]\n            counts.sort(reverse=True, key=itemgetter(1))\n            abstracts = [a[0] for a in counts]\n\n        if N is None:\n            N = len(abstracts)\n\n        s = [u'{0} of {1} documents'.format(N, len(abstracts))]\n\n        for i in range(N):\n            s += ['{0:2d}. {1}\\n'.format(i + 1, str(abstracts[i]))]\n\n        return '\\n'.join(s)", "response": "Return a summary string of the author s documents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the author_impact_factor for the.", "response": "def author_impact_factor(self, year=2014, refresh=True):\n        \"\"\"Get author_impact_factor for the .\n\n        Parameters\n        ----------\n        year : int (optional, default=2014)\n            The year based for which the impact factor is to be calculated.\n\n        refresh : bool (optional, default=True)\n            Whether to refresh the cached search file (if it exists) or not.\n\n        Returns\n        -------\n        (ncites, npapers, aif) : tuple of integers\n            The citations count, publication count, and author impact factor.\n        \"\"\"\n        scopus_abstracts = self.get_journal_abstracts(refresh=refresh)\n\n        cites = [int(ab.citedby_count) for ab in scopus_abstracts]\n        years = [int(ab.coverDate.split('-')[0]) for ab in scopus_abstracts]\n\n        data = zip(years, cites, scopus_abstracts)\n        data = sorted(data, key=itemgetter(1), reverse=True)\n\n        # now get aif papers for year-1 and year-2\n        aif_data = [tup for tup in data if tup[0] in (year - 1, year - 2)]\n        Ncites = sum([tup[1] for tup in aif_data])\n        if len(aif_data) > 0:\n            return (Ncites, len(aif_data), Ncites / float(len(aif_data)))\n        else:\n            return (Ncites, len(aif_data), 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns number of papers with author as the first author.", "response": "def n_first_author_papers(self, refresh=True):\n        \"\"\"Return number of papers with author as the first author.\"\"\"\n        first_authors = [1 for ab in self.get_journal_abstracts(refresh=refresh)\n                         if ab.authors[0].scopusid == self.author_id]\n        return sum(first_authors)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnumbers of journal publications in a given year.", "response": "def n_yearly_publications(self, refresh=True):\n        \"\"\"Number of journal publications in a given year.\"\"\"\n        pub_years = [int(ab.coverDate.split('-')[0])\n                     for ab in self.get_journal_abstracts(refresh=refresh)]\n        return Counter(pub_years)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_org(aff):\n    try:\n        org = aff['organization']\n        if not isinstance(org, str):\n            try:\n                org = org['$']\n            except TypeError:  # Multiple names given\n                org = ', '.join([d['$'] for d in org if d])\n    except KeyError:  # Author group w/o affiliation\n        org = None\n    return org", "response": "Auxiliary function to extract org information from affiliation\n    for authorgroup."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a list of authors.", "response": "def _list_authors(lst):\n    \"\"\"Format a list of authors (Surname, Firstname and Firstname Surname).\"\"\"\n    authors = ', '.join([' '.join([a.given_name, a.surname]) for a in lst[0:-1]])\n    authors += ' and ' + ' '.join([lst[-1].given_name, lst[-1].surname])\n    return authors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authkeywords(self):\n        keywords = self._json['authkeywords']\n        if keywords is None:\n            return None\n        else:\n            try:\n                return [d['$'] for d in keywords['author-keyword']]\n            except TypeError:  # Singleton keyword\n                return [keywords['author-keyword']['$']]", "response": "List of author - provided keywords of the abstract."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authors(self):\n        out = []\n        fields = 'auid indexed_name surname given_name affiliation'\n        auth = namedtuple('Author', fields)\n        for item in chained_get(self._json, ['authors', 'author'], []):\n            affs = [a for a in listify(item.get('affiliation')) if a]\n            if affs:\n                aff = [aff.get('@id') for aff in affs]\n            else:\n                aff = None\n            new = auth(auid=item['@auid'], surname=item.get('ce:surname'),\n                indexed_name=item.get('ce:indexed-name'), affiliation=aff,\n                given_name=chained_get(item, ['preferred-name', 'ce:given-name']))\n            out.append(new)\n        return out or None", "response": "A list of namedtuples representing the article s authors in the the\n        form."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chemicals(self):\n        path = ['enhancement', 'chemicalgroup', 'chemicals']\n        items = listify(chained_get(self._head, path, []))\n        chemical = namedtuple('Chemical', 'source chemical_name cas_registry_number')\n        out = []\n        for item in items:\n            for chem in listify(item['chemical']):\n                number = chem.get('cas-registry-number')\n                try:  # Multiple numbers given\n                    num = \";\".join([n['$'] for n in number])\n                except TypeError:\n                    num = number\n                new = chemical(source=item['@source'], cas_registry_number=num,\n                               chemical_name=chem['chemical-name'])\n                out.append(new)\n        return out or None", "response": "List of namedtuples representing chemical entities in the form\n        source chemical_name cas_registry_number."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndates range of the conference the abstract belongs to represented by two tuples in the form YYYYMMDD YYYYMMDD.", "response": "def confdate(self):\n        \"\"\"Date range of the conference the abstract belongs to represented\n        by two tuples in the form (YYYY, MM, DD).\n        \"\"\"\n        date = self._confevent.get('confdate', {})\n        if len(date) > 0:\n            start = {k: int(v) for k, v in date['startdate'].items()}\n            end = {k: int(v) for k, v in date['enddate'].items()}\n            return ((start['@year'], start['@month'], start['@day']),\n                    (end['@year'], end['@month'], end['@day']))\n        else:\n            return ((None, None, None), (None, None, None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsponsor of the conference the abstract belongs to.", "response": "def confsponsor(self):\n        \"\"\"Sponsor(s) of the conference the abstract belongs to.\"\"\"\n        sponsors = chained_get(self._confevent, ['confsponsors', 'confsponsor'], [])\n        if len(sponsors) == 0:\n            return None\n        if isinstance(sponsors, list):\n            return [s['$'] for s in sponsors]\n        return sponsors"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contributor_group(self):\n        items = listify(chained_get(self._head, ['source', 'contributor-group'], []))\n        out = []\n        fields = 'given_name initials surname indexed_name role'\n        pers = namedtuple('Contributor', fields)\n        for item in items:\n            entry = item.get('contributor', {})\n            new = pers(indexed_name=entry.get('ce:indexed-name'),\n                role=entry.get('@role'), surname=entry.get('ce:surname'),\n                given_name=entry.get('ce:given-name'),\n                initials=entry.get('ce:initials'))\n            out.append(new)\n        return out or None", "response": "List of namedtuples representing contributors compiled by Scopus."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a namedtuple representing the author to whom correspondence should be addressed in the form of namedtuple.", "response": "def correspondence(self):\n        \"\"\"namedtuple representing the author to whom correspondence should\n        be addressed, in the form\n        (surname, initials, organization, country, city_group).\n        \"\"\"\n        fields = 'surname initials organization country city_group'\n        auth = namedtuple('Correspondence', fields)\n        corr = self._head.get('correspondence')\n        if corr is None:\n            return None\n        aff = corr.get('affiliation', {})\n        try:\n            org = aff['organization']\n            if isinstance(org, dict):\n                try:\n                    org = org['$']\n                except TypeError:  # Multiple names given\n                    org = [d['$'] for d in org]\n        except KeyError:\n            org = None\n        return auth(surname=corr.get('person', {}).get('ce:surname'),\n                    initials=corr.get('person', {}).get('ce:initials'),\n                    organization=org, country=aff.get('country'),\n                    city_group=aff.get('city-group'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist of namedtuples parsed funding information in the form", "response": "def funding(self):\n        \"\"\"List of namedtuples parsed funding information in the form\n        (agency string id acronym country).\n        \"\"\"\n        path = ['item', 'xocs:meta', 'xocs:funding-list', 'xocs:funding']\n        funds = listify(chained_get(self._json, path, []))\n        out = []\n        fund = namedtuple('Funding', 'agency string id acronym country')\n        for item in funds:\n            new = fund(agency=item.get('xocs:funding-agency'),\n                string=item.get('xocs:funding-agency-matched-string'),\n                id=item.get('xocs:funding-agency-id'),\n                acronym=item.get('xocs:funding-agency-acronym'),\n                country=item.get('xocs:funding-agency-country'))\n            out.append(new)\n        return out or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of ISBNs belonging to publicationName as variying length.", "response": "def isbn(self):\n        \"\"\"ISBNs belonging to publicationName as tuple of variying length,\n        (e.g. ISBN-10 or ISBN-13).\"\"\"\n        isbns = listify(chained_get(self._head, ['source', 'isbn'], []))\n        if len(isbns) == 0:\n            return None\n        else:\n            return tuple((i['$'] for i in isbns))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of index terms.", "response": "def idxterms(self):\n        \"\"\"List of index terms.\"\"\"\n        try:\n            terms = listify(self._json.get(\"idxterms\", {}).get('mainterm', []))\n        except AttributeError:  # idxterms is empty\n            return None\n        try:\n            return [d['$'] for d in terms]\n        except AttributeError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnames of the publisher of the abstract.", "response": "def publisher(self):\n        \"\"\"Name of the publisher of the abstract.\n        Note: Information provided in the FULL view of the article might be\n        more complete.\n        \"\"\"\n        # Return information from FULL view, fall back to other views\n        full = chained_get(self._head, ['source', 'publisher', 'publishername'])\n        if full is None:\n            return self._json['coredata'].get('dc:publisher')\n        else:\n            return full"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef references(self):\n        out = []\n        fields = 'position id doi title authors authors_auid '\\\n                 'authors_affiliationid sourcetitle publicationyear volume '\\\n                 'issue first last citedbycount text fulltext'\n        ref = namedtuple('Reference', fields)\n        path = ['item', 'bibrecord', 'tail', 'bibliography', 'reference']\n        items = listify(chained_get(self._json, path,\n                    self._json.get('references', {}).get('reference', [])))\n        for item in items:\n            info = item.get('ref-info', item)\n            volisspag = info.get('volisspag', {}) or {}\n            if isinstance(volisspag, list):\n                volisspag = volisspag[0]\n            # Parse author information\n            try:  # FULL view parsing\n                auth = listify(item['ref-info']['ref-authors']['author'])\n                authors = [', '.join([d['ce:surname'], d['ce:initials']])\n                           for d in auth]\n                auids = None\n                affids = None\n            except KeyError:  # REF view parsing\n                auth = (info.get('author-list') or {}).get('author', [])\n                authors = [', '.join(filter(None, [d.get('ce:surname'),\n                                                   d.get('ce:given-name')]))\n                           for d in auth]\n                auids = \"; \".join(filter(None, [d.get('@auid') for d in auth]))\n                affs = filter(None, [d.get('affiliation') for d in auth])\n                affids = \"; \".join([aff.get('@id') for aff in affs])\n            # Parse IDs\n            try:\n                ids = listify(info['refd-itemidlist']['itemid'])\n            except KeyError:\n                ids = []\n            try:\n                doi = _select_by_idtype(ids, 'DOI')[0]\n            except IndexError:\n                doi = info.get('ce:doi')\n            try:\n                scopus_id = _select_by_idtype(ids, 'SGR')[0]\n            except IndexError:\n                scopus_id = info.get('scopus-id')\n            # Combine information\n            new = ref(position=item.get('@id'),\n                      id=scopus_id,\n                      doi=doi,\n                      authors=\"; \".join(authors),\n                      authors_auid=auids or None,\n                      authors_affiliationid=affids or None,\n                      title=info.get('ref-title', {}).get('ref-titletext', info.get('title')),\n                      sourcetitle=info.get('ref-sourcetitle', info.get('sourcetitle')),\n                      publicationyear=info.get('ref-publicationyear', {}).get('@first'),\n                      volume=volisspag.get('voliss', {}).get('@volume'),\n                      issue=volisspag.get('voliss', {}).get('@issue'),\n                      first=volisspag.get('pagerange', {}).get('@first'),\n                      last=volisspag.get('pagerange', {}).get('@last'),\n                      citedbycount=info.get('citedby-count'),\n                      text=info.get('ref-text'),\n                      fulltext=item.get('ref-fulltext'))\n            out.append(new)\n        return out or None", "response": "List of namedtuples representing references in the article."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sequencebank(self):\n        path = ['enhancement', 'sequencebanks', 'sequencebank']\n        items = listify(chained_get(self._head, path, []))\n        bank = namedtuple('Sequencebank', 'name sequence_number type')\n        out = []\n        for item in items:\n            numbers = listify(item['sequence-number'])\n            for number in numbers:\n                new = bank(name=item['@name'], sequence_number=number['$'],\n                           type=number['@type'])\n                out.append(new)\n        return out or None", "response": "List of namedtuples representing biological entities defined in the text in the form name sequence_number type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting of namedtuples containing subject areas of the article in the form (", "response": "def subject_areas(self):\n        \"\"\"List of namedtuples containing subject areas of the article\n        in the form ().\n        Note: Requires the FULL view of the article.\n        \"\"\"\n        area = namedtuple('Area', 'area abbreviation code')\n        path = ['subject-areas', 'subject-area']\n        out = [area(area=item['$'], abbreviation=item['@abbrev'],\n                    code=item['@code'])\n               for item in listify(chained_get(self._json, path, []))]\n        return out or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bibtex(self):\n        if self.aggregationType != 'Journal':\n            raise ValueError('Only Journal articles supported.')\n        # Item key\n        year = self.coverDate[0:4]\n        first = self.title.split()[0].title()\n        last = self.title.split()[-1].title()\n        key = ''.join([self.authors[0].surname, year, first, last])\n        # Authors\n        authors = ' and '.join([\"{} {}\".format(a.given_name, a.surname)\n                                for a in self.authors])\n        # Pages\n        if self.pageRange:\n            pages = self.pageRange\n        elif self.startingPage:\n            pages = '{}-{}'.format(self.startingPage, self.endingPage)\n        else:\n            pages = '-'\n        # All information\n        bib = \"@article{{{key},\\n  author = {{{auth}}},\\n  title = \"\\\n              \"{{{{{title}}}}},\\n  journal = {{{jour}}},\\n  year = \"\\\n              \"{{{year}}},\\n  volume = {{{vol}}},\\n  number = {{{number}}},\"\\\n              \"\\n  pages = {{{pages}}}\".format(\n                key=key, auth=authors, title=self.title, year=year,\n                jour=self.publicationName, vol=self.volume,\n                number=self.issueIdentifier, pages=pages)\n        # DOI\n        if self.doi:\n            bib += \",\\n  doi = {{{}}}\".format(self.doi)\n        bib += \"}\"\n        return bib", "response": "Returns the BibTeX representation of the item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the bibliographic entry in html format.", "response": "def get_html(self):\n        \"\"\"Bibliographic entry in html format.\"\"\"\n        # Author links\n        au_link = ('<a href=\"https://www.scopus.com/authid/detail.url'\n                   '?origin=AuthorProfile&authorId={0}\">{1}</a>')\n        if len(self.authors) > 1:\n            authors = u', '.join([au_link.format(a.auid, a.given_name +\n                                                 ' ' + a.surname)\n                                 for a in self.authors[0:-1]])\n            authors += (u' and ' +\n                        au_link.format(self.authors[-1].auid,\n                                       (str(self.authors[-1].given_name) +\n                                        ' ' +\n                                        str(self.authors[-1].surname))))\n        else:\n            a = self.authors[0]\n            authors = au_link.format(a.auid, a.given_name + ' ' + a.surname)\n        title = u'<a href=\"{}\">{}</a>'.format(self.scopus_link, self.title)\n        if self.volume and self.issueIdentifier:\n            volissue = u'<b>{}({})</b>'.format(self.volume, self.issueIdentifier)\n        elif self.volume:\n            volissue = u'<b>{}</b>'.format(self.volume)\n        else:\n            volissue = 'no volume'\n        jlink = '<a href=\"https://www.scopus.com/source/sourceInfo.url'\\\n                '?sourceId={}\">{}</a>'.format(\n                    self.source_id, self.publicationName)\n        pages = _parse_pages(self, unicode=True)\n        s = \"{auth}, {title}, {jour}, {volissue}, {pages}, ({year}).\".format(\n                auth=authors, title=title, jour=jlink, volissue=volissue,\n                pages=pages, year=self.coverDate[:4])\n        if self.doi:\n            s += ' <a href=\"https://doi.org/{0}\">doi:{0}</a>.'.format(self.doi)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_latex(self):\n        if len(self.authors) > 1:\n            authors = _list_authors(self.authors)\n        else:\n            a = self.authors\n            authors = ' '.join([a.given_name, a.surname])\n        if self.volume and self.issueIdentifier:\n            volissue = '\\\\textbf{{{}({})}}'.format(self.volume, self.issueIdentifier)\n        elif self.volume:\n            volissue = '\\\\textbf{{{}}}'.format(self.volume)\n        else:\n            volissue = 'no volume'\n        pages = _parse_pages(self)\n        s = '{auth}, \\\\textit{{{title}}}, {jour}, {vol}, {pages} ({year}).'.format(\n                auth=authors, title=self.title, jour=self.publicationName,\n                vol=volissue, pages=pages, year=self.coverDate[:4])\n        if self.doi is not None:\n            s += ' \\\\href{{https://doi.org/{0}}}{{doi:{0}}}, '.format(self.doi)\n        s += '\\\\href{{{0}}}{{scopus:{1}}}.'.format(self.scopus_link, self.eid)\n        return s", "response": "Return LaTeX format of the bibliographic entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the RIS string for the item.", "response": "def get_ris(self):\n        \"\"\"Bibliographic entry in RIS (Research Information System Format)\n        format for journal articles.\n\n        Raises\n        ------\n        ValueError\n            If the item's aggregationType is not Journal.\n        \"\"\"\n        if self.aggregationType != 'Journal':\n            raise ValueError('Only Journal articles supported.')\n        # Basic information\n        ris = \"TY  - JOUR\\nTI  - {title}\\nJO  - {jour}\\nVL  - {vol}\\n\"\\\n              \"DA  - {date}\\nPY  - {year}\\nSP  - {pages}\\n\".format(\n                title=self.title, jour=self.publicationName, vol=self.volume,\n                date=self.coverDate, year=self.coverDate[0:4],\n                pages=self.pageRange)\n        # Authors\n        for au in self.authors:\n            ris += 'AU  - {}\\n'.format(au.indexed_name)\n        # DOI\n        if self.doi is not None:\n            ris += 'DO  - {0}\\nUR  - https://doi.org/{0}\\n'.format(self.doi)\n        # Issue\n        if self.issueIdentifier is not None:\n            ris += 'IS  - {}\\n'.format(self.issueIdentifier)\n        ris += 'ER  - \\n\\n'\n        return ris"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting out an org - mode report for search results.", "response": "def report(scopus_search, label):\n    \"\"\"Print out an org-mode report for search results.\n\n    Parameters\n    ----------\n    scopus_search : scopus.scopus_search.ScopusSearch\n        An object resulting from a ScopusSearch.\n\n    label : str\n        The label used in the document title (\"Report for ...\").\n    \"\"\"\n    text = \"Development of this class has been suspended;  Please use the new\"\\\n           \"package 'scopusreport' (https://scopusreport.readthedocs.io/en/latest/)\"\\\n           \"instead.\"\n    warnings.warn(text, DeprecationWarning)\n\n    counts = {}  # to count papers per author\n    journals = {}  # to count publications per journal\n    author_count = []  # to count a paper's number of authors for a histogram\n    paper_cites = {}\n    Ncites = 0\n    document_types = {}\n\n    papers = 0  # to count number of publications\n\n    for eid in scopus_search.EIDS:\n        a = ScopusAbstract(eid)\n\n        # Get types of documents\n        try:\n            document_types[a.aggregationType] += 1\n        except KeyError:\n            document_types[a.aggregationType] = 1\n\n        if a.aggregationType == 'Journal':\n            Ncites += int(a.citedby_count)  # get total cites\n            papers += 1\n\n            # get count for journals\n            jkey = (a.publicationName, a.source_id, a.issn)\n            try:\n                journals[jkey] += 1\n            except KeyError:\n                journals[jkey] = 1\n\n            # get authors per paper\n            author_count += [len(a.authors)]\n\n            # now count papers per author\n            for author in a.authors:\n                key = (author.indexed_name, author.auid)\n                try:\n                    counts[key] += 1\n                except KeyError:\n                    counts[key] = 1\n\n            # counting cites per paper\n            key = (a.title, a.scopus_link)\n            try:\n                paper_cites[key] += a.citedby_count\n            except KeyError:\n                paper_cites[key] = a.citedby_count\n\n    print('*** Report for {}\\n'.format(label))\n    print('#+attr_latex: :placement [H] :center nil')\n    print('#+caption: Types of documents found for {}.'.format(label))\n    print('| Document type | count |\\n|-')\n    for key in document_types:\n        print('| {0} | {1} |'.format(key, document_types[key]))\n\n    print('\\n\\n')\n    print('{0} articles ({2} citations) '\n          'found by {1} authors'.format(papers, len(counts), Ncites))\n\n    # Author counts {(name, scopus-id): count}\n    view = [('[[https://www.scopus.com/authid/detail.uri?authorId={0}][{1}]]'.format(\n             k[1], k[0]),  # org-mode link\n            v, k[1]) for k, v in counts.items()] # counts, scopus-id\n    view.sort(reverse=True, key=itemgetter(1))\n\n    print('\\n#+attr_latex: :placement [H] :center nil')\n    print('#+caption: Author publication counts for {0}.'.format(label))\n    print('| name | count | categories |')\n    print('|-')\n    for name, count, scopus_id in view[0:20]:\n        cats = ', '.join(['{0} ({1})'.format(cat[0], cat[1])\n                          for cat in ScopusAuthor(scopus_id).categories[0:3]])\n        print('| {0} | {1} | {2} |'.format(name, count, cats))\n\n    # journal view\n    s = '[[https://www.scopus.com/source/sourceInfo.url?sourceId={0}][{1}]]'\n    jview = [(s.format(k[1], k[0][0:50]),  # url\n              k[1],  # source_id\n              k[2],  # issn\n              v)  # count\n             for k, v in journals.items()]\n    jview.sort(reverse=True, key=itemgetter(3))\n\n    print('\\n\\n')\n    print('#+attr_latex: :placement [H] :center nil')\n    print('#+caption: Journal publication counts for {0}.'.format(label))\n    print('| Journal | count | IPP |')\n    print('|-')\n\n    for journal, sid, issn, count in jview[0:12]:\n        # issn may contain E-ISSN\n        issn_tokens = issn.split()\n        try:\n            JOURNAL = ScopusJournal(issn_tokens[0])\n        except:\n            JOURNAL = ScopusJournal(issn_tokens[1])\n        IPP = JOURNAL.IPP or 0\n        print('| {0} | {1} | {2} |'.format(journal, count, IPP))\n\n    # view of journals sorted by `IPP\n    JVIEW = []\n    for journal, sid, issn, count in jview:\n        issn_tokens = issn.split()\n        try:\n            JOURNAL = ScopusJournal(issn_tokens[0])\n        except:\n            JOURNAL = ScopusJournal(issn_tokens[1])\n        IPP = JOURNAL.IPP or 0\n        JVIEW.append([journal, count, IPP])\n    JVIEW.sort(reverse=True, key=itemgetter(2))\n\n    print('\\n\\n')\n    print('#+attr_latex: :placement [H] :center nil')\n    print('#+caption: Journal publication counts'\n          ' for {0} sorted by IPP.'.format(label))\n    print('| Journal | count | IPP |')\n    print('|-')\n    for journal, count, IPP in JVIEW[0:12]:\n        print('|{0}|{1}|{2}|'.format(journal, count, IPP))\n\n    # top cited papers\n    pview = [('[[{0}][{1}]]'.format(k[1], k[0][0:60]),\n              int(v))\n             for k, v in paper_cites.items()]\n    pview.sort(reverse=True, key=itemgetter(1))\n\n    # Compute department j-index\n    hindex = 0\n    for i, entry in enumerate(pview):\n        # entry is url, source_id, count\n        u, count = entry\n        if count > i + 1:\n            continue\n        else:\n            hindex = i + 1\n            break\n\n    print('\\n\\n#+attr_latex: :placement [H] :center nil')\n    print('#+caption: Top cited publication'\n          ' counts for {0}. j-index = {1}.'.format(label, hindex))\n    print('| title | cite count |\\n|-')\n    for title, count in pview[0:10]:\n        print('| {0} | {1} |'.format(title, count))\n\n    plt.figure()\n    plt.hist(author_count, 20)\n    plt.xlabel('# authors')\n    plt.ylabel('frequency')\n    plt.savefig('{0}-nauthors-per-publication.png'.format(label))\n\n    print('\\n\\n#+caption: Number of authors '\n          'on each publication for {}.'.format(label))\n    print('[[./{0}-nauthors-per-publication.png]]'.format(label))\n    print('''**** Bibliography  :noexport:\n     :PROPERTIES:\n     :VISIBILITY: folded\n     :END:''')\n    print(scopus_search.org_summary)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the configuration file.", "response": "def create_config():\n    \"\"\"Initiates process to generate configuration file.\"\"\"\n    file_exists = exists(CONFIG_FILE)\n    if not file_exists:\n        # Set directories\n        config.add_section('Directories')\n        defaults = [\n            ('AbstractRetrieval', expanduser('~/.scopus/abstract_retrieval')),\n            ('AffiliationSearch', expanduser('~/.scopus/affiliation_search')),\n            ('AuthorRetrieval', expanduser('~/.scopus/author_retrieval')),\n            ('AuthorSearch', expanduser('~/.scopus/author_search')),\n            ('CitationOverview', expanduser('~/.scopus/citation_overview')),\n            ('ContentAffiliationRetrieval', expanduser('~/.scopus/affiliation_retrieval')),\n            ('ScopusSearch', expanduser('~/.scopus/scopus_search'))\n        ]\n        for key, value in defaults:\n            config.set('Directories', key, value)\n            if not exists(value):\n                makedirs(value)\n        # Set authentication\n        config.add_section('Authentication')\n        prompt_key = \"Please enter your API Key, obtained from \"\\\n                     \"http://dev.elsevier.com/myapikey.html: \\n\"\n        if py3:\n            key = input(prompt_key)\n        else:\n            key = raw_input(prompt_key)\n        config.set('Authentication', 'APIKey', key)\n        prompt_token = \"API Keys are sufficient for most users.  If you \"\\\n                       \"have to use Authtoken authentication, please enter \"\\\n                       \"the token, otherwise press Enter: \\n\"\n        if py3:\n            token = input(prompt_token)\n        else:\n            token = raw_input(prompt_token)\n        if len(token) > 0:\n            config.set('Authentication', 'InstToken', token)\n        # Write out\n        with open(CONFIG_FILE, 'w') as f:\n            config.write(f)\n    else:\n        text = \"Configuration file already exists at {}; process to create \"\\\n               \"the file aborted.  Please open the file and edit the \"\\\n               \"entries manually.\".format(CONFIG_FILE)\n        raise FileExistsError(text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authors(self):\n        out = []\n        order = 'eid surname initials givenname affiliation documents '\\\n                'affiliation_id city country areas'\n        auth = namedtuple('Author', order)\n        for item in self._json:\n            name = item.get('preferred-name', {})\n            aff = item.get('affiliation-current', {})\n            fields = item.get('subject-area',\n                              [{'@abbrev': '', '@frequency': ''}])\n            if isinstance(fields, dict):\n                fields = [fields]\n            areas = [\"{} ({})\".format(d.get('@abbrev', ''), d.get('@frequency', ''))\n                     for d in fields]\n            new = auth(eid=item['eid'], initials=name.get('initials'),\n                       surname=name.get('surname'), areas=\"; \".join(areas),\n                       givenname=name.get('given-name'),\n                       documents=item.get('document-count', '0'),\n                       affiliation=aff.get('affiliation-name'),\n                       affiliation_id=aff.get('affiliation-id'),\n                       city=aff.get('affiliation-city'),\n                       country=aff.get('affiliation-country'))\n            out.append(new)\n        return out or None", "response": "A list of namedtuples storing author information for each subject."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn text for element at xpath in the container xml if it is there.", "response": "def get_encoded_text(container, xpath):\n    \"\"\"Return text for element at xpath in the container xml if it is there.\n\n    Parameters\n    ----------\n    container : xml.etree.ElementTree.Element\n        The element to be searched in.\n\n    xpath : str\n        The path to be looked for.\n\n    Returns\n    -------\n    result : str\n    \"\"\"\n    try:\n        return \"\".join(container.find(xpath, ns).itertext())\n    except AttributeError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_new_config(args):\n    config = cosmic_ray.commands.new_config()\n    config_str = serialize_config(config)\n    with open(args['<config-file>'], mode='wt') as handle:\n        handle.write(config_str)\n\n    return ExitCode.OK", "response": "usage : cosmic - ray new - config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_init(args):\n    config_file = args['<config-file>']\n\n    config = load_config(config_file)\n\n    modules = set(cosmic_ray.modules.find_modules(Path(config['module-path'])))\n\n    log.info('Modules discovered: %s', [m for m in modules])\n\n    db_name = get_db_name(args['<session-file>'])\n\n    with use_db(db_name) as database:\n        cosmic_ray.commands.init(modules, database, config)\n\n    return ExitCode.OK", "response": "Initialize a mutation testing session from a configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshows the configuration for in a session.", "response": "def handle_config(args):\n    \"\"\"usage: cosmic-ray config <session-file>\n\n    Show the configuration for in a session.\n    \"\"\"\n    session_file = get_db_name(args['<session-file>'])\n    with use_db(session_file) as database:\n        config = database.get_config()\n        print(serialize_config(config))\n\n    return ExitCode.OK"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_operators(args):\n    assert args\n    print('\\n'.join(cosmic_ray.plugins.operator_names()))\n\n    return ExitCode.OK", "response": "usage : { program } operators\n    List the available operator plugins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_execution_engines(args):\n    assert args\n    print('\\n'.join(cosmic_ray.plugins.execution_engine_names()))\n\n    return ExitCode.OK", "response": "usage : execution - engines\n    List the available execution - engines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_interceptors(args):\n    assert args\n    print('\\n'.join(cosmic_ray.plugins.interceptor_names()))\n\n    return ExitCode.OK", "response": "usage : { program interceptors }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_worker(args):\n    config = load_config(args.get('<config-file>'))\n\n    with open(os.devnull, 'w') as devnull:\n        with redirect_stdout(sys.stdout if args['--keep-stdout'] else devnull):\n            work_item = cosmic_ray.worker.worker(\n                Path(args['<module-path>']),\n                config.python_version, args['<operator>'],\n                int(args['<occurrence>']),\n                config.test_command,\n                None)\n\n    sys.stdout.write(json.dumps(work_item, cls=WorkItemJsonEncoder))\n\n    return ExitCode.OK", "response": "worker - worker worker."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(argv=None):\n    signal.signal(\n        signal.SIGINT,\n        lambda *args: sys.exit(_SIGNAL_EXIT_CODE_BASE + signal.SIGINT))\n\n    if hasattr(signal, 'SIGINFO'):\n        signal.signal(\n            getattr(signal, 'SIGINFO'),\n            lambda *args: report_progress(sys.stderr))\n\n    try:\n        return docopt_subcommands.main(\n            commands=dsc,\n            argv=argv,\n            doc_template=DOC_TEMPLATE,\n            exit_at_end=False)\n    except docopt.DocoptExit as exc:\n        print(exc, file=sys.stderr)\n        return ExitCode.USAGE\n    except FileNotFoundError as exc:\n        print(exc, file=sys.stderr)\n        return ExitCode.NO_INPUT\n    except PermissionError as exc:\n        print(exc, file=sys.stderr)\n        return ExitCode.NO_PERM\n    except cosmic_ray.config.ConfigError as exc:\n        print(repr(exc), file=sys.stderr)\n        if exc.__cause__ is not None:\n            print(exc.__cause__, file=sys.stderr)\n        return ExitCode.CONFIG\n    except subprocess.CalledProcessError as exc:\n        print('Error in subprocess', file=sys.stderr)\n        print(exc, file=sys.stderr)\n        return exc.returncode", "response": "Invoke cosmic ray dsc and return the exit code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies the numeric value on node.", "response": "def mutate(self, node, index):\n        \"\"\"Modify the numeric value on `node`.\"\"\"\n\n        assert index < len(OFFSETS), 'received count with no associated offset'\n        assert isinstance(node, parso.python.tree.Number)\n\n        val = eval(node.value) + OFFSETS[index]  # pylint: disable=W0123\n        return parso.python.tree.Number(' ' + str(val), node.start_pos)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prohibited(from_op, to_op):\n    \"Determines if from_op is allowed to be mutated to to_op.\"\n    # 'not' can only be removed but not replaced with\n    # '+', '-' or '~' b/c that may lead to strange results\n    if from_op is UnaryOperators.Not:\n        if to_op is not UnaryOperators.Nothing:\n            return True\n\n    # '+1' => '1' yields equivalent mutations\n    if from_op is UnaryOperators.UAdd:\n        if to_op is UnaryOperators.Nothing:\n            return True\n\n    return False", "response": "Determines if from_op is allowed to be mutated to to_op."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_config(filename=None):\n    try:\n        with _config_stream(filename) as handle:\n            filename = handle.name\n            return deserialize_config(handle.read())\n    except (OSError, toml.TomlDecodeError, UnicodeDecodeError) as exc:\n        raise ConfigError(\n            'Error loading configuration from {}'.format(filename)) from exc", "response": "Load a configuration from a file or stdin."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a filename this returns a stream from which a configuration can be read.", "response": "def _config_stream(filename):\n    \"\"\"Given a configuration's filename, this returns a stream from which a configuration can be read.\n\n    If `filename` is `None` or '-' then stream will be `sys.stdin`. Otherwise,\n    it's the open file handle for the filename.\n    \"\"\"\n    if filename is None or filename == '-':\n        log.info('Reading config from stdin')\n        yield sys.stdin\n    else:\n        with open(filename, mode='rt') as handle:\n            log.info('Reading config from %r', filename)\n            yield handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sub(self, *segments):\n        \"Get a sub-configuration.\"\n        d = self\n        for segment in segments:\n            try:\n                d = d[segment]\n            except KeyError:\n                return ConfigDict({})\n        return d", "response": "Get a sub - configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef python_version(self):\n        v = self.get('python-version', '')\n        if v == '':\n            v = \"{}.{}\".format(sys.version_info.major, sys.version_info.minor)\n        return v", "response": "Get the configured Python version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mutate(self, node, index):\n        assert index == 0\n        assert isinstance(node, ForStmt)\n\n        empty_list = parso.parse(' []')\n        node.children[3] = empty_list\n        return node", "response": "Modify the For loop to evaluate to None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_operator(name):\n    sep = name.index('/')\n    provider_name = name[:sep]\n    operator_name = name[sep + 1:]\n\n    provider = OPERATOR_PROVIDERS[provider_name]\n    return provider[operator_name]", "response": "Get an operator class from a provider plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef operator_names():\n    return tuple('{}/{}'.format(provider_name, operator_name)\n                 for provider_name, provider in OPERATOR_PROVIDERS.items()\n                 for operator_name in provider)", "response": "Get all operator names."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_execution_engine(name):\n    manager = driver.DriverManager(\n        namespace='cosmic_ray.execution_engines',\n        name=name,\n        invoke_on_load=True,\n        on_load_failure_callback=_log_extension_loading_failure,\n    )\n\n    return manager.driver", "response": "Get the execution engine by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef use_db(path, mode=WorkDB.Mode.create):\n    database = WorkDB(path, mode)\n    try:\n        yield database\n    finally:\n        database.close()", "response": "Context manager that opens a DB in file path in mode mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the configuration for the session.", "response": "def set_config(self, config):\n        \"\"\"Set (replace) the configuration for the session.\n\n        Args:\n          config: Configuration object\n        \"\"\"\n        with self._conn:\n            self._conn.execute(\"DELETE FROM config\")\n            self._conn.execute('INSERT INTO config VALUES(?)',\n                               (serialize_config(config),))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the work parameters for the session.", "response": "def get_config(self):\n        \"\"\"Get the work parameters (if set) for the session.\n\n        Returns: a Configuration object.\n\n        Raises:\n          ValueError: If is no config set for the session.\n        \"\"\"\n        rows = list(self._conn.execute(\"SELECT * FROM config\"))\n        if not rows:\n            raise ValueError(\"work-db has no config\")\n        (config_str,) = rows[0]\n\n        return deserialize_config(config_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_work_item(self, work_item):\n        with self._conn:\n            self._conn.execute(\n                '''\n                INSERT INTO work_items\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                ''', _work_item_to_row(work_item))", "response": "Adds a WorkItem to the list of available work items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclearing all work items from the session.", "response": "def clear(self):\n        \"\"\"Clear all work items from the session.\n\n        This removes any associated results as well.\n        \"\"\"\n        with self._conn:\n            self._conn.execute('DELETE FROM results')\n            self._conn.execute('DELETE FROM work_items')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef results(self):\n        \"An iterable of all `(job-id, WorkResult)`s.\"\n        cur = self._conn.cursor()\n        rows = cur.execute(\"SELECT * FROM results\")\n        for row in rows:\n            yield (row['job_id'], _row_to_work_result(row))", "response": "An iterable of all ( job - id WorkResult s."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_result(self, job_id, result):\n        with self._conn:\n            try:\n                self._conn.execute(\n                    '''\n                    REPLACE INTO results\n                    VALUES (?, ?, ?, ?, ?)\n                    ''', _work_result_to_row(job_id, result))\n            except sqlite3.IntegrityError as exc:\n                raise KeyError('Can not add result with job-id {}'.format(\n                    job_id)) from exc", "response": "Set the result for a job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pending_work_items(self):\n        \"Iterable of all pending work items.\"\n        pending = self._conn.execute(\n            \"SELECT * FROM work_items WHERE job_id NOT IN (SELECT job_id FROM results)\"\n        )\n        return (_row_to_work_item(p) for p in pending)", "response": "Iterable of all pending work items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef report_xml():\n    arguments = docopt.docopt(report_xml.__doc__, version='cr-rate 1.0')\n    with use_db(arguments['<session-file>'], WorkDB.Mode.open) as db:\n        xml_elem = _create_xml_report(db)\n        xml_elem.write(\n            sys.stdout.buffer, encoding='utf-8', xml_declaration=True)", "response": "cr - xml\nUsage : cr - xml <session - file >\nPrint an XML formatted report of test results for continuos integration systems\n"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(db_name):\n    try:\n        with use_db(db_name, mode=WorkDB.Mode.open) as work_db:\n            _update_progress(work_db)\n            config = work_db.get_config()\n            engine = get_execution_engine(config.execution_engine_name)\n\n            def on_task_complete(job_id, work_result):\n                work_db.set_result(job_id, work_result)\n                _update_progress(work_db)\n                log.info(\"Job %s complete\", job_id)\n\n            log.info(\"Beginning execution\")\n            engine(\n                work_db.pending_work_items,\n                config,\n                on_task_complete=on_task_complete)\n            log.info(\"Execution finished\")\n\n    except FileNotFoundError as exc:\n        raise FileNotFoundError(\n            str(exc).replace('Requested file', 'Corresponding database',\n                             1)) from exc", "response": "Execute any pending work in the database stored in db_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the AST for the code in a file.", "response": "def get_ast(module_path, python_version):\n    \"\"\"Get the AST for the code in a file.\n\n    Args:\n        module_path: pathlib.Path to the file containing the code.\n        python_version: Python version as a \"MAJ.MIN\" string.\n\n    Returns: The parso parse tree for the code in `module_path`.\n    \"\"\"\n    with module_path.open(mode='rt', encoding='utf-8') as handle:\n        source = handle.read()\n\n    return parso.parse(source, version=python_version)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine if a node is the None keyword.", "response": "def is_none(node):\n    \"Determine if a node is the `None` keyword.\"\n    return isinstance(node, parso.python.tree.Keyword) and node.value == 'None'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwalking a parse tree calling visit for each node.", "response": "def walk(self, node):\n        \"Walk a parse tree, calling visit for each node.\"\n        node = self.visit(node)\n\n        if node is None:\n            return None\n\n        if isinstance(node, parso.tree.BaseNode):\n            walked = map(self.walk, node.children)\n            node.children = [child for child in walked if child is not None]\n\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_survival_rate():\n    arguments = docopt.docopt(\n        format_survival_rate.__doc__, version='cr-rate 1.0')\n    with use_db(arguments['<session-file>'], WorkDB.Mode.open) as db:\n        rate = survival_rate(db)\n\n    print('{:.2f}'.format(rate))", "response": "cr-rate\n\n    Usage: cr-rate <session-file>\n\n    Calculate the survival rate of a session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef survival_rate(work_db):\n    kills = sum(r.is_killed for _, r in work_db.results)\n    num_results = work_db.num_results\n\n    if not num_results:\n        return 0\n\n    return (1 - kills / num_results) * 100", "response": "Calcuate the survival rate for the results in a WorkDB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report_html():\n    arguments = docopt.docopt(report_html.__doc__, version='cr-rate 1.0')\n    with use_db(arguments['<session-file>'], WorkDB.Mode.open) as db:\n        doc = _generate_html_report(db)\n\n    print(doc.getvalue())", "response": "cr - html\nUsage : cr - html <session - file >\nPrint an HTML formatted report of test results.\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if a string is of the form <int. <int > False otherwise.", "response": "def _validate_python_version(s):\n    \"Return True if a string is of the form <int>.<int>, False otherwise.\"\n    if not s:\n        return True\n    toks = s.split('.')\n    if len(toks) != 2:\n        return False\n    try:\n        int(toks[0])\n        int(toks[1])\n    except ValueError:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_config():\n    config = ConfigDict()\n    config[\"module-path\"] = qprompt.ask_str(\n        \"Top-level module path\",\n        blk=False,\n        vld=os.path.exists,\n        hlp=MODULE_PATH_HELP)\n\n    python_version = qprompt.ask_str(\n        'Python version (blank for auto detection)',\n        vld=_validate_python_version,\n        hlp=PYTHON_VERSION_HELP)\n    config['python-version'] = python_version\n\n    timeout = qprompt.ask_str(\n        'Test execution timeout (seconds)',\n        vld=float,\n        blk=False,\n        hlp=\"The number of seconds to let a test run before terminating it.\")\n    config['timeout'] = float(timeout)\n    config['excluded-modules'] = []\n\n    config[\"test-command\"] = qprompt.ask_str(\n        \"Test command\",\n        blk=False,\n        hlp=TEST_COMMAND_HELP)\n\n    menu = qprompt.Menu()\n    for at_pos, engine_name in enumerate(execution_engine_names()):\n        menu.add(str(at_pos), engine_name)\n    config[\"execution-engine\"] = ConfigDict()\n    config['execution-engine']['name'] = menu.show(header=\"Execution engine\", returns=\"desc\")\n\n    config[\"cloning\"] = ConfigDict()\n    config['cloning']['method'] = 'copy'\n    config['cloning']['commands'] = []\n\n    return config", "response": "Prompt user for config variables and generate new config."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all modules in the module represented by module_path.", "response": "def find_modules(module_path):\n    \"\"\"Find all modules in the module (possibly package) represented by `module_path`.\n\n    Args:\n        module_path: A pathlib.Path to a Python package or module.\n\n    Returns: An iterable of paths Python modules (i.e. *py files).\n    \"\"\"\n    if module_path.is_file():\n        if module_path.suffix == '.py':\n            yield module_path\n    elif module_path.is_dir():\n        pyfiles = glob.glob('{}/**/*.py'.format(module_path), recursive=True)\n        yield from (Path(pyfile) for pyfile in pyfiles)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclear and initialize a work - db with work items.", "response": "def init(module_paths, work_db, config):\n    \"\"\"Clear and initialize a work-db with work items.\n\n    Any existing data in the work-db will be cleared and replaced with entirely\n    new work orders. In particular, this means that any results in the db are\n    removed.\n\n    Args:\n      module_paths: iterable of pathlib.Paths of modules to mutate.\n      work_db: A `WorkDB` instance into which the work orders will be saved.\n      config: The configuration for the new session.\n    \"\"\"\n    operator_names = cosmic_ray.plugins.operator_names()\n    work_db.set_config(config=config)\n\n    work_db.clear()\n\n    for module_path in module_paths:\n        module_ast = get_ast(\n            module_path, python_version=config.python_version)\n\n        for op_name in operator_names:\n            operator = get_operator(op_name)(config.python_version)\n            visitor = WorkDBInitVisitor(module_path, op_name, work_db,\n                                        operator)\n            visitor.walk(module_ast)\n\n    apply_interceptors(work_db, config.sub('interceptors').get('enabled', ()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying each registered interceptor to the WorkDB.", "response": "def apply_interceptors(work_db, enabled_interceptors):\n    \"\"\"Apply each registered interceptor to the WorkDB.\"\"\"\n    names = (name for name in interceptor_names() if name in enabled_interceptors)\n    for name in names:\n        interceptor = get_interceptor(name)\n        interceptor(work_db)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _allowed(to_op, from_op, rhs):\n    \"Determine if a mutation from `from_op` to `to_op` is allowed given a particular `rhs` node.\"\n    if is_none(rhs):\n        return to_op in _RHS_IS_NONE_OPS.get(from_op, ())\n\n    if is_number(rhs):\n        return to_op in _RHS_IS_INTEGER_OPS\n\n    return True", "response": "Determine if a mutation from from_op to to_op is allowed given a particular rhs node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmutating the OCCURRENCE-th site for OPERATOR_NAME in MODULE_PATH, run the tests, and report the results. This is fundamentally the single-mutation-and-test-run process implementation. There are three high-level ways that a worker can finish. First, it could fail exceptionally, meaning that some uncaught exception made its way from some part of the operation to terminate the function. This function will intercept all exceptions and return it in a non-exceptional structure. Second, the mutation testing machinery may determine that there is no OCCURENCE-th instance for OPERATOR_NAME in the module under test. In this case there is no way to report a test result (i.e. killed, survived, or incompetent) so a special value is returned indicating that no mutation is possible. Finally, and hopefully normally, the worker will find that it can run a test. It will do so and report back the result - killed, survived, or incompetent - in a structured way. Args: module_name: The path to the module to mutate python_version: The version of Python to use when interpreting the code in `module_path`. A string of the form \"MAJOR.MINOR\", e.g. \"3.6\" for Python 3.6.x. operator_name: The name of the operator plugin to use occurrence: The occurrence of the operator to apply test_command: The command to execute to run the tests timeout: The maximum amount of time (seconds) to let the tests run Returns: A WorkResult Raises: This will generally not raise any exceptions. Rather, exceptions will be reported using the 'exception' result-type in the return value.", "response": "def worker(module_path,\n           python_version,\n           operator_name,\n           occurrence,\n           test_command,\n           timeout):\n    \"\"\"Mutate the OCCURRENCE-th site for OPERATOR_NAME in MODULE_PATH, run the\n    tests, and report the results.\n\n    This is fundamentally the single-mutation-and-test-run process\n    implementation.\n\n    There are three high-level ways that a worker can finish. First, it could\n    fail exceptionally, meaning that some uncaught exception made its way from\n    some part of the operation to terminate the function. This function will\n    intercept all exceptions and return it in a non-exceptional structure.\n\n    Second, the mutation testing machinery may determine that there is no\n    OCCURENCE-th instance for OPERATOR_NAME in the module under test. In this\n    case there is no way to report a test result (i.e. killed, survived, or\n    incompetent) so a special value is returned indicating that no mutation is\n    possible.\n\n    Finally, and hopefully normally, the worker will find that it can run a\n    test. It will do so and report back the result - killed, survived, or\n    incompetent - in a structured way.\n\n    Args:\n        module_name: The path to the module to mutate\n        python_version: The version of Python to use when interpreting the code in `module_path`.\n            A string of the form \"MAJOR.MINOR\", e.g. \"3.6\" for Python 3.6.x.\n        operator_name: The name of the operator plugin to use\n        occurrence: The occurrence of the operator to apply\n        test_command: The command to execute to run the tests\n        timeout: The maximum amount of time (seconds) to let the tests run\n\n    Returns: A WorkResult\n\n    Raises: This will generally not raise any exceptions. Rather, exceptions\n        will be reported using the 'exception' result-type in the return value.\n\n    \"\"\"\n    try:\n        operator_class = cosmic_ray.plugins.get_operator(operator_name)\n        operator = operator_class(python_version)\n\n        with cosmic_ray.mutating.use_mutation(module_path, operator,\n                                              occurrence) as (original_code,\n                                                              mutated_code):\n            if mutated_code is None:\n                return WorkResult(worker_outcome=WorkerOutcome.NO_TEST)\n\n            test_outcome, output = run_tests(test_command, timeout)\n\n            diff = _make_diff(original_code, mutated_code, module_path)\n\n            return WorkResult(\n                output=output,\n                diff='\\n'.join(diff),\n                test_outcome=test_outcome,\n                worker_outcome=WorkerOutcome.NORMAL)\n\n    except Exception:  # noqa # pylint: disable=broad-except\n        return WorkResult(\n            output=traceback.format_exc(),\n            test_outcome=TestOutcome.INCOMPETENT,\n            worker_outcome=WorkerOutcome.EXCEPTION)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreport progress from any currently installed reporters.", "response": "def report_progress(stream=None):\n    \"\"\"Report progress from any currently installed reporters.\n\n    Args:\n        stream: The text stream (default: sys.stderr) to which\n            progress will be reported.\n    \"\"\"\n    if stream is None:\n        stream = sys.stderr\n    for reporter in _reporters:\n        reporter(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reports_progress(reporter):\n\n    def decorator(func):  # pylint: disable=missing-docstring\n        @wraps(func)\n        def wrapper(*args, **kwargs):  # pylint: disable=missing-docstring\n            with progress_reporter(reporter):\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator", "response": "A decorator factory for mark functions which report progress."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a set of tags for the current git repo.", "response": "def tags():\n    \"Get a set of tags for the current git repo.\"\n    result = [t.decode('ascii') for t in subprocess.check_output([\n        'git', 'tag'\n    ]).split(b\"\\n\")]\n    assert len(set(result)) == len(result)\n    return set(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_tag_and_push(version):\n    \"Create a git tag for `version` and push it to origin.\"\n    assert version not in tags()\n    git('config', 'user.name', 'Travis CI on behalf of Austin Bingham')\n    git('config', 'user.email', 'austin@sixty-north.com')\n    git('config', 'core.sshCommand', 'ssh -i deploy_key')\n    git(\n        'remote', 'add', 'ssh-origin',\n        'git@github.com:sixty-north/cosmic-ray.git'\n    )\n    git('tag', version)\n\n    subprocess.check_call([\n        'ssh-agent', 'sh', '-c',\n        'chmod 0600 deploy_key && ' +\n        'ssh-add deploy_key && ' +\n        # 'git push ssh-origin HEAD:master &&'\n        'git push ssh-origin --tags'\n    ])", "response": "Create a git tag for version and push it to origin."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the ( version - string version - info ) from version_file.", "response": "def read_version(version_file):\n    \"Read the `(version-string, version-info)` from `version_file`.\"\n    vars = {}\n    with open(version_file) as f:\n        exec(f.read(), {}, vars)\n    return (vars['__version__'], vars['__version_info__'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef worker_task(work_item, config):\n    global _workspace\n\n    _ensure_workspace(config)\n\n    result = worker(\n        work_item.module_path,\n        config.python_version,\n        work_item.operator_name,\n        work_item.occurrence,\n        config.test_command,\n        config.timeout)\n    return work_item.job_id, result", "response": "The celery task which runs a single mutation and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a suite of tests for a given set of work items.", "response": "def execute_work_items(work_items, config):\n    \"\"\"Execute a suite of tests for a given set of work items.\n\n    Args:\n      work_items: An iterable of `work_db.WorkItem`s.\n      config: The configuration to use for the test execution.\n\n    Returns: An iterable of WorkItems.\n    \"\"\"\n    return celery.group(\n        worker_task.s(work_item, config)\n        for work_item in work_items\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cloned_workspace(clone_config, chdir=True):\n    workspace = ClonedWorkspace(clone_config)\n    original_dir = os.getcwd()\n    if chdir:\n        os.chdir(workspace.clone_dir)\n\n    try:\n        yield workspace\n    finally:\n        os.chdir(original_dir)\n        workspace.cleanup()", "response": "Create a cloned workspace and yield it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clone_with_git(repo_uri, dest_path):\n    log.info('Cloning git repo %s to %s', repo_uri, dest_path)\n    git.Repo.clone_from(repo_uri, dest_path, depth=1)", "response": "Create a clone by cloning a git repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone_with_copy(src_path, dest_path):\n    log.info('Cloning directory tree %s to %s', src_path, dest_path)\n    shutil.copytree(src_path, dest_path)", "response": "Clone a directory tree with copying it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new virtual environment in venv_dir.", "response": "def _build_env(venv_dir):\n    \"\"\"Create a new virtual environment in `venv_dir`.\n\n    This uses the base prefix of any virtual environment that you may be using\n    when you call this.\n    \"\"\"\n    # NB: We had to create the because the venv modules wasn't doing what we\n    # needed. In particular, if we used it create a venv from an existing venv,\n    # it *always* created symlinks back to the original venv's python\n    # executables. Then, when you used those linked executables, you ended up\n    # interacting with the original venv. I could find no way around this, hence\n    # this function.\n    prefix = getattr(sys, 'real_prefix', sys.prefix)\n    python = Path(prefix) / 'bin' / 'python'\n    command = '{} -m venv {}'.format(python, venv_dir)\n    try:\n        log.info('Creating virtual environment: %s', command)\n        subprocess.run(command.split(),\n                       stdout=subprocess.PIPE,\n                       stderr=subprocess.STDOUT,\n                       check=True)\n    except subprocess.CalledProcessError as exc:\n        log.error(\"Error creating virtual environment: %s\", exc.output)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_variables(self, text):\n        variables = {\n            'python-executable': str(self._venv_path / 'bin' / 'python')\n        }\n        return text.format(**variables)", "response": "Replace variable placeholders in text with values from the virtual env."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cleanup(self):\n        \"Remove the directory containin the clone and virtual environment.\"\n        log.info('Removing temp dir %s', self._tempdir.name)\n        self._tempdir.cleanup()", "response": "Remove the directory containin the clone and virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_dict(self):\n        \"Get the WorkResult as a dict.\"\n        return {\n            'output': self.output,\n            'test_outcome': self.test_outcome,\n            'worker_outcome': self.worker_outcome,\n            'diff': self.diff,\n        }", "response": "Get the WorkResult as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget fields as a dict.", "response": "def as_dict(self):\n        \"\"\"Get fields as a dict.\n        \"\"\"\n        return {\n            'module_path': str(self.module_path),\n            'operator_name': self.operator_name,\n            'occurrence': self.occurrence,\n            'start_pos': self.start_pos,\n            'end_pos': self.end_pos,\n            'job_id': self.job_id,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlook for WorkItems in `work_db` that should not be mutated due to spor metadata. For each WorkItem, find anchors for the item's file/line/columns. If an anchor exists with metadata containing `{mutate: False}` then the WorkItem is marked as SKIPPED.", "response": "def intercept(work_db):\n    \"\"\"Look for WorkItems in `work_db` that should not be mutated due to spor metadata.\n\n    For each WorkItem, find anchors for the item's file/line/columns. If an\n    anchor exists with metadata containing `{mutate: False}` then the WorkItem\n    is marked as SKIPPED.\n    \"\"\"\n\n    @lru_cache()\n    def file_contents(file_path):\n        \"A simple cache of file contents.\"\n        with file_path.open(mode=\"rt\") as handle:\n            return handle.readlines()\n\n    for item in work_db.work_items:\n        try:\n            repo = open_repository(item.module_path)\n        except ValueError:\n            log.info(\"No spor repository for %s\", item.module_path)\n            continue\n\n        for _, anchor in repo.items():\n            if anchor.file_path != item.module_path.absolute():\n                continue\n\n            metadata = anchor.metadata\n\n            lines = file_contents(item.module_path)\n            if _item_in_context(\n                    lines, item,\n                    anchor.context) and not metadata.get(\"mutate\", True):\n                log.info(\n                    \"spor skipping %s %s %s %s %s %s\",\n                    item.job_id,\n                    item.operator_name,\n                    item.occurrence,\n                    item.module_path,\n                    item.start_pos,\n                    item.end_pos,\n                )\n\n                work_db.set_result(\n                    item.job_id,\n                    WorkResult(\n                        output=None,\n                        test_outcome=None,\n                        diff=None,\n                        worker_outcome=WorkerOutcome.SKIPPED,\n                    ),\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiguring out the offset into a file for a particular line and column.", "response": "def _line_and_col_to_offset(lines, line, col):\n    \"\"\"Figure out the offset into a file for a particular line and col.\n\n    This can return offsets that don't actually exist in the file. If you\n    specify a line that exists and a col that is past the end of that line, this\n    will return a \"fake\" offset. This is to account for the fact that a\n    WorkItem's end_pos is one-past the end of a mutation, and hence potentially\n    one-past the end of a file.\n\n    Args:\n        lines: A sequence of the lines in a file.\n        line: A one-based index indicating the line in the file.\n        col: A zero-based index indicating the column on `line`.\n\n    Raises: ValueError: If the specified line found in the file.\n    \"\"\"\n\n    offset = 0\n    for index, contents in enumerate(lines, 1):\n        if index == line:\n            return offset + col\n\n        offset += len(contents)\n\n    raise ValueError(\"Offset {}:{} not found\".format(line, col))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if a WorkItem falls within a context.", "response": "def _item_in_context(lines, item, context):\n    \"\"\"Determines if a WorkItem falls within an anchor.\n\n    This only returns True if a WorkItems start-/stop-pos range is *completely*\n    within an anchor, not just if it overalaps.\n    \"\"\"\n    start_offset = _line_and_col_to_offset(lines, item.start_pos[0],\n                                           item.start_pos[1])\n    stop_offset = _line_and_col_to_offset(lines, item.end_pos[0],\n                                          item.end_pos[1])\n    width = stop_offset - start_offset\n\n    return start_offset >= context.offset and width <= len(context.topic)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_mutation(module_path, operator, occurrence):\n    module_ast = get_ast(module_path, python_version=operator.python_version)\n    original_code = module_ast.get_code()\n    visitor = MutationVisitor(occurrence, operator)\n    mutated_ast = visitor.walk(module_ast)\n\n    mutated_code = None\n    if visitor.mutation_applied:\n        mutated_code = mutated_ast.get_code()\n        with module_path.open(mode='wt', encoding='utf-8') as handle:\n            handle.write(mutated_code)\n            handle.flush()\n\n    return original_code, mutated_code", "response": "Apply a specific mutation to a module on disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_requirements():\n    requirements_file = os.path.join(os.getcwd(), 'requirements.txt')\n    requirements = []\n    links=[]\n    try:\n        with open(requirements_file) as reqfile:\n            for line in reqfile.readlines():\n                line = line.strip()\n                if line.startswith('#'):\n                    continue\n                elif line.startswith(\n                        ('https://', 'git://', 'hg://', 'svn://')):\n                    links.append(line)\n                else:\n                    requirements.append(line)\n\n    except (IOError, OSError) as error:\n        print(error)\n\n    if python26():\n        # Required to make `collections.OrderedDict` available on Python<=2.6\n        requirements.append('ordereddict==1.1#a0ed854ee442051b249bfad0f638bbec')\n\n    # Don't try to install psutil on PyPy:\n    if _isPyPy:\n        for line in requirements[:]:\n            if line.startswith('psutil'):\n                print(\"Not installing %s on PyPy...\" % line)\n                requirements.remove(line)\n\n    return requirements, links", "response": "Extract the list of requirements from our requirements. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a batchfile for our new key.", "response": "def createBatchfile(keyparams=allparams):\n    \"\"\"Create the batchfile for our new key.\n\n    :params dict keyparams: A dictionary of arguments for creating the key. It\n                            should probably be ``allparams``.\n    :rtype: str\n    :returns: A string containing the entire GnuPG batchfile.\n    \"\"\"\n    useparams = {}\n    for key, value in keyparams.items():\n        if value:\n            useparams.update({key: value})\n    batchfile = gpg.gen_key_input(separate_keyring=True,\n                                  save_batchfile=True,\n                                  **useparams)\n    log.info(\"Generated GnuPG batch file:\\n%s\" % batchfile)\n    return batchfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new GPG key from a GnuPG batchfile.", "response": "def createKey(batchfile):\n    \"\"\"Create a new keypair from a **batchfile**.\n\n    Writes the new keys into keyrings named after ``NAME_EMAIL`` inside the\n    ``NEWKEY_DIR``.\n\n    :params str batchfile: A GnuPG batchfile. See :func:`createBatchfile`.\n    \"\"\"\n    key = gpg.gen_key(batchfile)\n    fingerprint = key.fingerprint\n\n    if not fingerprint:\n        log.error(\"Key creation seems to have failed: %s\" % key.status)\n        return None, None\n    return key, fingerprint"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef displayNewKey(key):\n\n    if key.keyring:\n        gpg.keyring = key.keyring\n    if key.secring:\n        gpg.secring = key.secring\n\n    # Using '--fingerprint' twice will display subkey fingerprints too:\n    gpg.options = ['--fingerprint', '--fingerprint']\n    keylist = gpg.list_keys(secret=True)\n\n    # `result` is a `gnupg._parsers.ListKeys`, which is list-like, so iterate\n    # over all the keys and display their info:\n    for gpgkey in keylist:\n        for k, v in gpgkey.items():\n            log.info(\"%s: %s\" % (k.capitalize(), v))\n\n    return keylist", "response": "Display details of the new key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exportNewKey(fingerprint):\n    log.info(\"Exporting key: %s\" % fingerprint)\n\n    keyfn = os.path.join(gpg.homedir,\n                         fingerprint + '-8192-bit-key') + os.path.extsep\n\n    pubkey = gpg.export_keys(fingerprint)\n    seckey = gpg.export_keys(fingerprint, secret=True)\n    subkey = gpg.export_keys(fingerprint, secret=True, subkeys=True)\n\n    with open(keyfn + 'pub' + os.path.extsep + 'asc', 'w') as fh:\n        fh.write(pubkey)\n    with open(keyfn + 'sec' + os.path.extsep + 'asc', 'w') as fh:\n        fh.write(seckey)\n    with open(keyfn + 'sub' + os.path.extsep + 'asc', 'w') as fh:\n        fh.write(subkey)", "response": "Export the new keys into. asc files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that a given keyserver is a known protocol and does not contain shell escape characters.", "response": "def _check_keyserver(location):\n    \"\"\"Check that a given keyserver is a known protocol and does not contain\n    shell escape characters.\n\n    :param str location: A string containing the default keyserver. This\n                         should contain the desired keyserver protocol which\n                         is supported by the keyserver, for example, the\n                         default is ``'hkp://wwwkeys .pgp.net'``.\n    :rtype: :obj:`str` or :obj:`None`\n    :returns: A string specifying the protocol and keyserver hostname, if the\n              checks passed. If not, returns None.\n    \"\"\"\n    protocols = ['hkp://', 'hkps://', 'http://', 'https://', 'ldap://',\n                 'mailto:'] ## xxx feels like i\u00b4m forgetting one...\n    for proto in protocols:\n        if location.startswith(proto):\n            url = location.replace(proto, str())\n            host, slash, extra = url.partition('/')\n            if extra: log.warn(\"URI text for %s: '%s'\" % (host, extra))\n            log.debug(\"Got host string for keyserver setting: '%s'\" % host)\n\n            host = _fix_unsafe(host)\n            if host:\n                log.debug(\"Cleaned host string: '%s'\" % host)\n                keyserver = proto + host\n                return keyserver\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_preferences(prefs, pref_type=None):\n    if prefs is None: return\n\n    cipher   = frozenset(['AES256', 'AES192', 'AES128',\n                          'CAMELLIA256', 'CAMELLIA192',\n                          'TWOFISH', '3DES'])\n    digest   = frozenset(['SHA512', 'SHA384', 'SHA256', 'SHA224', 'RMD160',\n                          'SHA1'])\n    compress = frozenset(['BZIP2', 'ZLIB', 'ZIP', 'Uncompressed'])\n    trust    = frozenset(['gpg', 'classic', 'direct', 'always', 'auto'])\n    pinentry = frozenset(['loopback'])\n    all      = frozenset([cipher, digest, compress, trust, pinentry])\n\n    if isinstance(prefs, str):\n        prefs = set(prefs.split())\n    elif isinstance(prefs, list):\n        prefs = set(prefs)\n    else:\n        msg = \"prefs must be list of strings, or space-separated string\"\n        log.error(\"parsers._check_preferences(): %s\" % message)\n        raise TypeError(message)\n\n    if not pref_type:\n        pref_type = 'all'\n\n    allowed = str()\n\n    if pref_type == 'cipher':\n        allowed += ' '.join(prefs.intersection(cipher))\n    if pref_type == 'digest':\n        allowed += ' '.join(prefs.intersection(digest))\n    if pref_type == 'compress':\n        allowed += ' '.join(prefs.intersection(compress))\n    if pref_type == 'trust':\n        allowed += ' '.join(prefs.intersection(trust))\n    if pref_type == 'pinentry':\n        allowed += ' '.join(prefs.intersection(pinentry))\n    if pref_type == 'all':\n        allowed += ' '.join(prefs.intersection(all))\n\n    return allowed", "response": "Check the preferences for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds characters used to escape from a string into a shell and wrap them in quotes if they exist.", "response": "def _fix_unsafe(shell_input):\n    \"\"\"Find characters used to escape from a string into a shell, and wrap them in\n    quotes if they exist. Regex pilfered from Python3 :mod:`shlex` module.\n\n    :param str shell_input: The input intended for the GnuPG process.\n    \"\"\"\n    _unsafe = re.compile(r'[^\\w@%+=:,./-]', 256)\n    try:\n        if len(_unsafe.findall(shell_input)) == 0:\n            return shell_input.strip()\n        else:\n            clean = \"'\" + shell_input.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n            return clean\n    except TypeError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _hyphenate(input, add_prefix=False):\n    ret  = '--' if add_prefix else ''\n    ret += input.replace('_', '-')\n    return ret", "response": "Change underscores to hyphens so that GPG option names can be easily migrated to GPG option names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that an option or argument given to GPG is allowed.", "response": "def _is_allowed(input):\n    \"\"\"Check that an option or argument given to GPG is in the set of allowed\n    options, the latter being a strict subset of the set of all options known\n    to GPG.\n\n    :param str input: An input meant to be parsed as an option or flag to the\n                      GnuPG process. Should be formatted the same as an option\n                      or flag to the commandline gpg, i.e. \"--encrypt-files\".\n\n    :ivar frozenset gnupg_options: All known GPG options and flags.\n\n    :ivar frozenset allowed: All allowed GPG options and flags, e.g. all GPG\n                             options and flags which we are willing to\n                             acknowledge and parse. If we want to support a\n                             new option, it will need to have its own parsing\n                             class and its name will need to be added to this\n                             set.\n\n    :raises: :exc:`UsageError` if **input** is not a subset of the hard-coded\n             set of all GnuPG options in :func:`_get_all_gnupg_options`.\n\n             :exc:`ProtectedOption` if **input** is not in the set of allowed\n             options.\n\n    :rtype: str\n    :return: The original **input** parameter, unmodified and unsanitized, if\n             no errors occur.\n    \"\"\"\n    gnupg_options = _get_all_gnupg_options()\n    allowed = _get_options_group(\"allowed\")\n\n    ## these are the allowed options we will handle so far, all others should\n    ## be dropped. this dance is so that when new options are added later, we\n    ## merely add the to the _allowed list, and the `` _allowed.issubset``\n    ## assertion will check that GPG will recognise them\n    try:\n        ## check that allowed is a subset of all gnupg_options\n        assert allowed.issubset(gnupg_options)\n    except AssertionError:\n        raise UsageError(\"'allowed' isn't a subset of known options, diff: %s\"\n                         % allowed.difference(gnupg_options))\n\n    ## if we got a list of args, join them\n    ##\n    ## see TODO file, tag :cleanup:\n    if not isinstance(input, str):\n        input = ' '.join([x for x in input])\n\n    if isinstance(input, str):\n        if input.find('_') > 0:\n            if not input.startswith('--'):\n                hyphenated = _hyphenate(input, add_prefix=True)\n            else:\n                hyphenated = _hyphenate(input)\n        else:\n            hyphenated = input\n            ## xxx we probably want to use itertools.dropwhile here\n            try:\n                assert hyphenated in allowed\n            except AssertionError as ae:\n                dropped = _fix_unsafe(hyphenated)\n                log.warn(\"_is_allowed(): Dropping option '%s'...\" % dropped)\n                raise ProtectedOption(\"Option '%s' not supported.\" % dropped)\n            else:\n                return input\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the thing is a string according to what version of Python is running in.", "response": "def _is_string(thing):\n    \"\"\"Python character arrays are a mess.\n\n    If Python2, check if **thing** is an :obj:`unicode` or a :obj:`str`.\n    If Python3, check if **thing** is a :obj:`str`.\n\n    :param thing: The thing to check.\n    :returns: ``True`` if **thing** is a string according to whichever version\n              of Python we're running in.\n    \"\"\"\n    if _util._py3k: return isinstance(thing, str)\n    else: return isinstance(thing, basestring)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sanitise(*args):\n\n    ## see TODO file, tag :cleanup:sanitise:\n\n    def _check_option(arg, value):\n        \"\"\"Check that a single ``arg`` is an allowed option.\n\n        If it is allowed, quote out any escape characters in ``value``, and\n        add the pair to :ivar:`sanitised`. Otherwise, drop them.\n\n        :param str arg: The arguments which will be passed to the GnuPG\n                        process, and, optionally their corresponding values.\n                        The values are any additional arguments following the\n                        GnuPG option or flag. For example, if we wanted to\n                        pass ``\"--encrypt --recipient isis@leap.se\"`` to\n                        GnuPG, then ``\"--encrypt\"`` would be an arg without a\n                        value, and ``\"--recipient\"`` would also be an arg,\n                        with a value of ``\"isis@leap.se\"``.\n\n        :ivar list checked: The sanitised, allowed options and values.\n        :rtype: str\n        :returns: A string of the items in ``checked``, delimited by spaces.\n        \"\"\"\n        checked = str()\n        none_options        = _get_options_group(\"none_options\")\n        hex_options         = _get_options_group(\"hex_options\")\n        hex_or_none_options = _get_options_group(\"hex_or_none_options\")\n\n        if not _util._py3k:\n            if not isinstance(arg, list) and isinstance(arg, unicode):\n                arg = str(arg)\n\n        try:\n            flag = _is_allowed(arg)\n            assert flag is not None, \"_check_option(): got None for flag\"\n        except (AssertionError, ProtectedOption) as error:\n            log.warn(\"_check_option(): %s\" % str(error))\n        else:\n            checked += (flag + ' ')\n\n            if _is_string(value):\n                values = value.split(' ')\n                for v in values:\n                    ## these can be handled separately, without _fix_unsafe(),\n                    ## because they are only allowed if they pass the regex\n                    if (flag in none_options) and (v is None):\n                        continue\n\n                    if flag in hex_options:\n                        if _is_hex(v): checked += (v + \" \")\n                        else:\n                            log.debug(\"'%s %s' not hex.\" % (flag, v))\n                            if (flag in hex_or_none_options) and (v is None):\n                                log.debug(\"Allowing '%s' for all keys\" % flag)\n                        continue\n\n                    elif flag in ['--keyserver']:\n                        host = _check_keyserver(v)\n                        if host:\n                            log.debug(\"Setting keyserver: %s\" % host)\n                            checked += (v + \" \")\n                        else: log.debug(\"Dropping keyserver: %s\" % v)\n                        continue\n\n                    ## the rest are strings, filenames, etc, and should be\n                    ## shell escaped:\n                    val = _fix_unsafe(v)\n                    try:\n                        assert not val is None\n                        assert not val.isspace()\n                        assert not v is None\n                        assert not v.isspace()\n                    except:\n                        log.debug(\"Dropping %s %s\" % (flag, v))\n                        continue\n\n                    if flag in ['--encrypt', '--encrypt-files', '--decrypt',\n                                '--decrypt-files', '--import', '--verify']:\n                        if ( (_util._is_file(val))\n                             or\n                             ((flag == '--verify') and (val == '-')) ):\n                            checked += (val + \" \")\n                        else:\n                            log.debug(\"%s not file: %s\" % (flag, val))\n\n                    elif flag in ['--cipher-algo', '--personal-cipher-prefs',\n                                  '--personal-cipher-preferences']:\n                        legit_algos = _check_preferences(val, 'cipher')\n                        if legit_algos: checked += (legit_algos + \" \")\n                        else: log.debug(\"'%s' is not cipher\" % val)\n\n                    elif flag in ['--compress-algo', '--compression-algo',\n                                  '--personal-compress-prefs',\n                                  '--personal-compress-preferences']:\n                        legit_algos = _check_preferences(val, 'compress')\n                        if legit_algos: checked += (legit_algos + \" \")\n                        else: log.debug(\"'%s' not compress algo\" % val)\n\n                    elif flag == '--trust-model':\n                        legit_models = _check_preferences(val, 'trust')\n                        if legit_models: checked += (legit_models + \" \")\n                        else: log.debug(\"%r is not a trust model\", val)\n\n                    elif flag == '--pinentry-mode':\n                        legit_modes = _check_preferences(val, 'pinentry')\n                        if legit_modes: checked += (legit_modes + \" \")\n                        else: log.debug(\"%r is not a pinentry mode\", val)\n\n                    else:\n                        checked += (val + \" \")\n                        log.debug(\"_check_option(): No checks for %s\" % val)\n\n        return checked.rstrip(' ')\n\n    is_flag = lambda x: x.startswith('--')\n\n    def _make_filo(args_string):\n        filo = arg.split(' ')\n        filo.reverse()\n        log.debug(\"_make_filo(): Converted to reverse list: %s\" % filo)\n        return filo\n\n    def _make_groups(filo):\n        groups = {}\n        while len(filo) >= 1:\n            last = filo.pop()\n            if is_flag(last):\n                log.debug(\"Got arg: %s\" % last)\n                if last == '--verify':\n                    groups[last] = str(filo.pop())\n                    ## accept the read-from-stdin arg:\n                    if len(filo) >= 1 and filo[len(filo)-1] == '-':\n                        groups[last] += str(' - ') ## gross hack\n                        filo.pop()\n                else:\n                    groups[last] = str()\n                while len(filo) > 1 and not is_flag(filo[len(filo)-1]):\n                    log.debug(\"Got value: %s\" % filo[len(filo)-1])\n                    groups[last] += (filo.pop() + \" \")\n                else:\n                    if len(filo) == 1 and not is_flag(filo[0]):\n                        log.debug(\"Got value: %s\" % filo[0])\n                        groups[last] += filo.pop()\n            else:\n                log.warn(\"_make_groups(): Got solitary value: %s\" % last)\n                groups[\"xxx\"] = last\n        return groups\n\n    def _check_groups(groups):\n        log.debug(\"Got groups: %s\" % groups)\n        checked_groups = []\n        for a,v in groups.items():\n            v = None if len(v) == 0 else v\n            safe = _check_option(a, v)\n            if safe is not None and not safe.strip() == \"\":\n                log.debug(\"Appending option: %s\" % safe)\n                checked_groups.append(safe)\n            else:\n                log.warn(\"Dropped option: '%s %s'\" % (a,v))\n        return checked_groups\n\n    if args is not None:\n        option_groups = {}\n        for arg in args:\n            ## if we're given a string with a bunch of options in it split\n            ## them up and deal with them separately\n            if (not _util._py3k and isinstance(arg, basestring)) \\\n                    or (_util._py3k and isinstance(arg, str)):\n                log.debug(\"Got arg string: %s\" % arg)\n                if arg.find(' ') > 0:\n                    filo = _make_filo(arg)\n                    option_groups.update(_make_groups(filo))\n                else:\n                    option_groups.update({ arg: \"\" })\n            elif isinstance(arg, list):\n                log.debug(\"Got arg list: %s\" % arg)\n                arg.reverse()\n                option_groups.update(_make_groups(arg))\n            else:\n                log.warn(\"Got non-str/list arg: '%s', type '%s'\"\n                         % (arg, type(arg)))\n        checked = _check_groups(option_groups)\n        sanitised = ' '.join(x for x in checked)\n        return sanitised\n    else:\n        log.debug(\"Got None for args\")", "response": "Sanitize a single argument or a key portion of a kwarg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_options_group(group=None):\n\n    #: These expect a hexidecimal keyid as their argument, and can be parsed\n    #: with :func:`_is_hex`.\n    hex_options = frozenset(['--check-sigs',\n                             '--default-key',\n                             '--default-recipient',\n                             '--delete-keys',\n                             '--delete-secret-keys',\n                             '--delete-secret-and-public-keys',\n                             '--desig-revoke',\n                             '--export',\n                             '--export-secret-keys',\n                             '--export-secret-subkeys',\n                             '--fingerprint',\n                             '--gen-revoke',\n                             '--hidden-encrypt-to',\n                             '--hidden-recipient',\n                             '--list-key',\n                             '--list-keys',\n                             '--list-public-keys',\n                             '--list-secret-keys',\n                             '--list-sigs',\n                             '--recipient',\n                             '--recv-keys',\n                             '--send-keys',\n                             '--edit-key',\n                             '--sign-key',\n                             ])\n    #: These options expect value which are left unchecked, though still run\n    #: through :func:`_fix_unsafe`.\n    unchecked_options = frozenset(['--list-options',\n                                   '--passphrase-fd',\n                                   '--status-fd',\n                                   '--verify-options',\n                                   '--command-fd',\n                               ])\n    #: These have their own parsers and don't really fit into a group\n    other_options = frozenset(['--debug-level',\n                               '--keyserver',\n\n                           ])\n    #: These should have a directory for an argument\n    dir_options = frozenset(['--homedir',\n                             ])\n    #: These expect a keyring or keyfile as their argument\n    keyring_options = frozenset(['--keyring',\n                                 '--primary-keyring',\n                                 '--secret-keyring',\n                                 '--trustdb-name',\n                                 ])\n    #: These expect a filename (or the contents of a file as a string) or None\n    #: (meaning that they read from stdin)\n    file_or_none_options = frozenset(['--decrypt',\n                                      '--decrypt-files',\n                                      '--encrypt',\n                                      '--encrypt-files',\n                                      '--import',\n                                      '--verify',\n                                      '--verify-files',\n                                      '--output',\n                                      ])\n    #: These options expect a string. see :func:`_check_preferences`.\n    pref_options = frozenset(['--digest-algo',\n                              '--cipher-algo',\n                              '--compress-algo',\n                              '--compression-algo',\n                              '--cert-digest-algo',\n                              '--personal-digest-prefs',\n                              '--personal-digest-preferences',\n                              '--personal-cipher-prefs',\n                              '--personal-cipher-preferences',\n                              '--personal-compress-prefs',\n                              '--personal-compress-preferences',\n                              '--pinentry-mode',\n                              '--print-md',\n                              '--trust-model',\n                              ])\n    #: These options expect no arguments\n    none_options = frozenset(['--allow-loopback-pinentry',\n                              '--always-trust',\n                              '--armor',\n                              '--armour',\n                              '--batch',\n                              '--check-sigs',\n                              '--check-trustdb',\n                              '--clearsign',\n                              '--debug-all',\n                              '--default-recipient-self',\n                              '--detach-sign',\n                              '--export',\n                              '--export-ownertrust',\n                              '--export-secret-keys',\n                              '--export-secret-subkeys',\n                              '--fingerprint',\n                              '--fixed-list-mode',\n                              '--gen-key',\n                              '--import-ownertrust',\n                              '--list-config',\n                              '--list-key',\n                              '--list-keys',\n                              '--list-packets',\n                              '--list-public-keys',\n                              '--list-secret-keys',\n                              '--list-sigs',\n                              '--lock-multiple',\n                              '--lock-never',\n                              '--lock-once',\n                              '--no-default-keyring',\n                              '--no-default-recipient',\n                              '--no-emit-version',\n                              '--no-options',\n                              '--no-tty',\n                              '--no-use-agent',\n                              '--no-verbose',\n                              '--print-mds',\n                              '--quiet',\n                              '--sign',\n                              '--symmetric',\n                              '--throw-keyids',\n                              '--use-agent',\n                              '--verbose',\n                              '--version',\n                              '--with-colons',\n                              '--yes',\n                              ])\n    #: These options expect either None or a hex string\n    hex_or_none_options = hex_options.intersection(none_options)\n    allowed = hex_options.union(unchecked_options, other_options, dir_options,\n                                keyring_options, file_or_none_options,\n                                pref_options, none_options)\n\n    if group and group in locals().keys():\n        return locals()[group]", "response": "Get a specific group of options which are allowed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all options from the GnuPG tree.", "response": "def _get_all_gnupg_options():\n    \"\"\"Get all GnuPG options and flags.\n\n    This is hardcoded within a local scope to reduce the chance of a tampered\n    GnuPG binary reporting falsified option sets, i.e. because certain options\n    (namedly the ``--no-options`` option, which prevents the usage of gpg.conf\n    files) are necessary and statically specified in\n    :meth:`gnupg._meta.GPGBase._make_args`, if the inputs into Python are\n    already controlled, and we were to summon the GnuPG binary to ask it for\n    its options, it would be possible to receive a falsified options set\n    missing the ``--no-options`` option in response. This seems unlikely, and\n    the method is stupid and ugly, but at least we'll never have to debug\n    whether or not an option *actually* disappeared in a different GnuPG\n    version, or some funny business is happening.\n\n    These are the options as of GnuPG 1.4.12; the current stable branch of the\n    2.1.x tree contains a few more -- if you need them you'll have to add them\n    in here.\n\n    :type gnupg_options: frozenset\n    :ivar gnupg_options: All known GPG options and flags.\n    :rtype: frozenset\n    :returns: ``gnupg_options``\n    \"\"\"\n    three_hundred_eighteen = (\"\"\"\n--allow-freeform-uid              --multifile\n--allow-multiple-messages         --no\n--allow-multisig-verification     --no-allow-freeform-uid\n--allow-non-selfsigned-uid        --no-allow-multiple-messages\n--allow-secret-key-import         --no-allow-non-selfsigned-uid\n--always-trust                    --no-armor\n--armor                           --no-armour\n--armour                          --no-ask-cert-expire\n--ask-cert-expire                 --no-ask-cert-level\n--ask-cert-level                  --no-ask-sig-expire\n--ask-sig-expire                  --no-auto-check-trustdb\n--attribute-fd                    --no-auto-key-locate\n--attribute-file                  --no-auto-key-retrieve\n--auto-check-trustdb              --no-batch\n--auto-key-locate                 --no-comments\n--auto-key-retrieve               --no-default-keyring\n--batch                           --no-default-recipient\n--bzip2-compress-level            --no-disable-mdc\n--bzip2-decompress-lowmem         --no-emit-version\n--card-edit                       --no-encrypt-to\n--card-status                     --no-escape-from-lines\n--cert-digest-algo                --no-expensive-trust-checks\n--cert-notation                   --no-expert\n--cert-policy-url                 --no-force-mdc\n--change-pin                      --no-force-v3-sigs\n--charset                         --no-force-v4-certs\n--check-sig                       --no-for-your-eyes-only\n--check-sigs                      --no-greeting\n--check-trustdb                   --no-groups\n--cipher-algo                     --no-literal\n--clearsign                       --no-mangle-dos-filenames\n--command-fd                      --no-mdc-warning\n--command-file                    --no-options\n--comment                         --no-permission-warning\n--completes-needed                --no-pgp2\n--compress-algo                   --no-pgp6\n--compression-algo                --no-pgp7\n--compress-keys                   --no-pgp8\n--compress-level                  --no-random-seed-file\n--compress-sigs                   --no-require-backsigs\n--ctapi-driver                    --no-require-cross-certification\n--dearmor                         --no-require-secmem\n--dearmour                        --no-rfc2440-text\n--debug                           --no-secmem-warning\n--debug-all                       --no-show-notation\n--debug-ccid-driver               --no-show-photos\n--debug-level                     --no-show-policy-url\n--decrypt                         --no-sig-cache\n--decrypt-files                   --no-sig-create-check\n--default-cert-check-level        --no-sk-comments\n--default-cert-expire             --no-strict\n--default-cert-level              --notation-data\n--default-comment                 --not-dash-escaped\n--default-key                     --no-textmode\n--default-keyserver-url           --no-throw-keyid\n--default-preference-list         --no-throw-keyids\n--default-recipient               --no-tty\n--default-recipient-self          --no-use-agent\n--default-sig-expire              --no-use-embedded-filename\n--delete-keys                     --no-utf8-strings\n--delete-secret-and-public-keys   --no-verbose\n--delete-secret-keys              --no-version\n--desig-revoke                    --openpgp\n--detach-sign                     --options\n--digest-algo                     --output\n--disable-ccid                    --override-session-key\n--disable-cipher-algo             --passphrase\n--disable-dsa2                    --passphrase-fd\n--disable-mdc                     --passphrase-file\n--disable-pubkey-algo             --passphrase-repeat\n--display                         --pcsc-driver\n--display-charset                 --personal-cipher-preferences\n--dry-run                         --personal-cipher-prefs\n--dump-options                    --personal-compress-preferences\n--edit-key                        --personal-compress-prefs\n--emit-version                    --personal-digest-preferences\n--enable-dsa2                     --personal-digest-prefs\n--enable-progress-filter          --pgp2\n--enable-special-filenames        --pgp6\n--enarmor                         --pgp7\n--enarmour                        --pgp8\n--encrypt                         --photo-viewer\n--encrypt-files                   --pipemode\n--encrypt-to                      --preserve-permissions\n--escape-from-lines               --primary-keyring\n--exec-path                       --print-md\n--exit-on-status-write-error      --print-mds\n--expert                          --quick-random\n--export                          --quiet\n--export-options                  --reader-port\n--export-ownertrust               --rebuild-keydb-caches\n--export-secret-keys              --recipient\n--export-secret-subkeys           --recv-keys\n--fast-import                     --refresh-keys\n--fast-list-mode                  --remote-user\n--fetch-keys                      --require-backsigs\n--fingerprint                     --require-cross-certification\n--fixed-list-mode                 --require-secmem\n--fix-trustdb                     --rfc1991\n--force-mdc                       --rfc2440\n--force-ownertrust                --rfc2440-text\n--force-v3-sigs                   --rfc4880\n--force-v4-certs                  --run-as-shm-coprocess\n--for-your-eyes-only              --s2k-cipher-algo\n--gen-key                         --s2k-count\n--gen-prime                       --s2k-digest-algo\n--gen-random                      --s2k-mode\n--gen-revoke                      --search-keys\n--gnupg                           --secret-keyring\n--gpg-agent-info                  --send-keys\n--gpgconf-list                    --set-filename\n--gpgconf-test                    --set-filesize\n--group                           --set-notation\n--help                            --set-policy-url\n--hidden-encrypt-to               --show-keyring\n--hidden-recipient                --show-notation\n--homedir                         --show-photos\n--honor-http-proxy                --show-policy-url\n--ignore-crc-error                --show-session-key\n--ignore-mdc-error                --sig-keyserver-url\n--ignore-time-conflict            --sign\n--ignore-valid-from               --sign-key\n--import                          --sig-notation\n--import-options                  --sign-with\n--import-ownertrust               --sig-policy-url\n--interactive                     --simple-sk-checksum\n--keyid-format                    --sk-comments\n--keyring                         --skip-verify\n--keyserver                       --status-fd\n--keyserver-options               --status-file\n--lc-ctype                        --store\n--lc-messages                     --strict\n--limit-card-insert-tries         --symmetric\n--list-config                     --temp-directory\n--list-key                        --textmode\n--list-keys                       --throw-keyid\n--list-only                       --throw-keyids\n--list-options                    --trustdb-name\n--list-ownertrust                 --trusted-key\n--list-packets                    --trust-model\n--list-public-keys                --try-all-secrets\n--list-secret-keys                --ttyname\n--list-sig                        --ttytype\n--list-sigs                       --ungroup\n--list-trustdb                    --update-trustdb\n--load-extension                  --use-agent\n--local-user                      --use-embedded-filename\n--lock-multiple                   --user\n--lock-never                      --utf8-strings\n--lock-once                       --verbose\n--logger-fd                       --verify\n--logger-file                     --verify-files\n--lsign-key                       --verify-options\n--mangle-dos-filenames            --version\n--marginals-needed                --warranty\n--max-cert-depth                  --with-colons\n--max-output                      --with-fingerprint\n--merge-only                      --with-key-data\n--min-cert-level                  --yes\n\"\"\").split()\n\n    # These are extra options which only exist for GnuPG>=2.0.0\n    three_hundred_eighteen.append('--export-ownertrust')\n    three_hundred_eighteen.append('--import-ownertrust')\n\n    # These are extra options which only exist for GnuPG>=2.1.0\n    three_hundred_eighteen.append('--pinentry-mode')\n    three_hundred_eighteen.append('--allow-loopback-pinentry')\n\n    gnupg_options = frozenset(three_hundred_eighteen)\n    return gnupg_options"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate NODATA status codes from GnuPG to messages.", "response": "def nodata(status_code):\n    \"\"\"Translate NODATA status codes from GnuPG to messages.\"\"\"\n    lookup = {\n        '1': 'No armored data.',\n        '2': 'Expected a packet but did not find one.',\n        '3': 'Invalid packet found, this may indicate a non OpenPGP message.',\n        '4': 'Signature expected but not found.' }\n    for key, value in lookup.items():\n        if str(status_code) == key:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef progress(status_code):\n    lookup = {\n        'pk_dsa': 'DSA key generation',\n        'pk_elg': 'Elgamal key generation',\n        'primegen': 'Prime generation',\n        'need_entropy': 'Waiting for new entropy in the RNG',\n        'tick': 'Generic tick without any special meaning - still working.',\n        'starting_agent': 'A gpg-agent was started.',\n        'learncard': 'gpg-agent or gpgsm is learning the smartcard data.',\n        'card_busy': 'A smartcard is still working.' }\n    for key, value in lookup.items():\n        if str(status_code) == key:\n            return value", "response": "Translate PROGRESS status codes from GnuPG to messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clean_key_expiration_option(self):\n        allowed_entry = re.findall('^(\\d+)(|w|m|y)$', self._expiration_time)\n        if not allowed_entry:\n            raise UsageError(\"Key expiration option: %s is not valid\" % self._expiration_time)", "response": "validates the expiration option supplied"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses series of inputs normally supplied on gpg - interactive - key", "response": "def gpg_interactive_input(self, sub_keys_number):\n        \"\"\" processes series of inputs normally supplied on --edit-key but passed through stdin\n            this ensures that no other --edit-key command is actually passing through.\n        \"\"\"\n        deselect_sub_key = \"key 0\\n\"\n\n        _input = self._main_key_command()\n        for sub_key_number in range(1, sub_keys_number + 1):\n            _input += self._sub_key_command(sub_key_number) + deselect_sub_key\n        return \"%ssave\\n\" % _input"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the status key from the attached GnuPG process.", "response": "def _handle_status(self, key, value):\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\"USERID_HINT\", \"NEED_PASSPHRASE\",\n                   \"GET_HIDDEN\", \"SIGEXPIRED\", \"KEYEXPIRED\",\n                   \"GOOD_PASSPHRASE\", \"GOT_IT\", \"GET_LINE\"):\n            pass\n        elif key in (\"BAD_PASSPHRASE\", \"MISSING_PASSPHRASE\"):\n            self.status = key.replace(\"_\", \" \").lower()\n        else:\n            self.status = 'failed'\n            raise ValueError(\"Unknown status message: %r\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_status(self, key, value):\n        if key in (\"GOOD_PASSPHRASE\"):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"KEY_NOT_CREATED\":\n            self.status = 'key not created'\n        elif key == \"KEY_CREATED\":\n            (self.type, self.fingerprint) = value.split()\n            self.status = 'key created'\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(' ', 1)[0])\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn((\"GnuPG has just attempted to launch whichever pinentry \"\n                      \"program you have configured, in order to obtain the \"\n                      \"passphrase for this key.  If you did not use the \"\n                      \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                      \"see Issues #122 and #137:\"\n                      \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                      \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"))\n            self.status = 'key not created'\n        elif (key.startswith(\"TRUST_\") or\n              key.startswith(\"PKA_TRUST_\") or\n              key == \"NEWSIG\"):\n            pass\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)\n\n        if self.type in ('B', 'P'):\n            self.primary_created = True\n        if self.type in ('B', 'S'):\n            self.subkey_created = True", "response": "Handles the status message from the attached GnuPG process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_status(self, key, value):\n        if key in (\"DELETE_PROBLEM\", \"KEY_CONSIDERED\"):\n            self.status = self.problem_reason.get(value, \"Unknown error: %r\"\n                                                  % value)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)", "response": "Handles the status key and value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the status message from the attached GnuPG process.", "response": "def _handle_status(self, key, value):\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n                \"USERID_HINT\",\n                \"NEED_PASSPHRASE\",\n                \"BAD_PASSPHRASE\",\n                \"GOOD_PASSPHRASE\",\n                \"MISSING_PASSPHRASE\",\n                \"PINENTRY_LAUNCHED\",\n                \"BEGIN_SIGNING\",\n                \"CARDCTRL\",\n                \"INV_SGNR\",\n                \"SIGEXPIRED\",\n                \"KEY_CONSIDERED\",\n            ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"SIG_CREATED\":\n            (self.sig_type, self.sig_algo, self.sig_hash_algo,\n             self.what, self.timestamp, self.fingerprint) = value.split()\n        elif key == \"KEYEXPIRED\":\n            self.status = \"skipped signing key, key expired\"\n            if (value is not None) and (len(value) > 0):\n                 self.status += \" on {}\".format(str(value))\n        elif key == \"KEYREVOKED\":\n            self.status = \"skipped signing key, key revoked\"\n            if (value is not None) and (len(value) > 0):\n                 self.status += \" on {}\".format(str(value))\n        elif key == \"NODATA\":\n            self.status = nodata(value)\n        elif key == \"PROGRESS\":\n            self.status = progress(value.split(' ', 1)[0])\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_status(self, key, value):\n        if key == \"IMPORTED\":\n            # this duplicates info we already see in import_ok & import_problem\n            pass\n        elif key == \"PINENTRY_LAUNCHED\":\n            log.warn((\"GnuPG has just attempted to launch whichever pinentry \"\n                      \"program you have configured, in order to obtain the \"\n                      \"passphrase for this key.  If you did not use the \"\n                      \"`passphrase=` parameter, please try doing so.  Otherwise, \"\n                      \"see Issues #122 and #137:\"\n                      \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/122\"\n                      \"\\nhttps://github.com/isislovecruft/python-gnupg/issues/137\"))\n        elif key == \"KEY_CONSIDERED\":\n            self.results.append({\n                'status': key.replace(\"_\", \" \").lower(),\n            })\n        elif key == \"NODATA\":\n            self.results.append({'fingerprint': None,\n                                 'status': 'No valid data found'})\n        elif key == \"IMPORT_OK\":\n            reason, fingerprint = value.split()\n            reasons = []\n            for code, text in self._ok_reason.items():\n                if int(reason) == int(code):\n                    reasons.append(text)\n            reasontext = '\\n'.join(reasons) + \"\\n\"\n            self.results.append({'fingerprint': fingerprint,\n                                 'status': reasontext})\n            self.fingerprints.append(fingerprint)\n        elif key == \"IMPORT_PROBLEM\":\n            try:\n                reason, fingerprint = value.split()\n            except:\n                reason = value\n                fingerprint = '<unknown>'\n            self.results.append({'fingerprint': fingerprint,\n                                 'status': self._problem_reason[reason]})\n        elif key == \"IMPORT_RES\":\n            import_res = value.split()\n            for x in self.counts.keys():\n                self.counts[x] = int(import_res.pop(0))\n        elif key == \"KEYEXPIRED\":\n            res = {'fingerprint': None,\n                   'status': 'Key expired'}\n            self.results.append(res)\n        ## Accoring to docs/DETAILS L859, SIGEXPIRED is obsolete:\n        ## \"Removed on 2011-02-04. This is deprecated in favor of KEYEXPIRED.\"\n        elif key == \"SIGEXPIRED\":\n            res = {'fingerprint': None,\n                   'status': 'Signature expired'}\n            self.results.append(res)\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)", "response": "Handles the status message from the attached GnuPG process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the status message received from the GnuPG process.", "response": "def _handle_status(self, key, value):\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises ValueError: if the status message is unknown.\n        \"\"\"\n        informational_keys = [\"KEY_CONSIDERED\"]\n        if key in (\"EXPORTED\"):\n            self.fingerprints.append(value)\n        elif key == \"EXPORT_RES\":\n            export_res = value.split()\n            for x in self.counts.keys():\n                self.counts[x] += int(export_res.pop(0))\n        elif key not in informational_keys:\n            raise ValueError(\"Unknown status message: %r\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the status message from the attached GnuPG process.", "response": "def _handle_status(self, key, value):\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in self.TRUST_LEVELS:\n            self.trust_text = key\n            self.trust_level = self.TRUST_LEVELS[key]\n        elif key in (\n                \"RSA_OR_IDEA\",\n                \"NODATA\",\n                \"IMPORT_RES\",\n                \"PLAINTEXT\",\n                \"PLAINTEXT_LENGTH\",\n                \"POLICY_URL\",\n                \"DECRYPTION_INFO\",\n                \"DECRYPTION_KEY\",\n                \"DECRYPTION_OKAY\",\n                \"INV_SGNR\",\n                \"PROGRESS\",\n                \"PINENTRY_LAUNCHED\",\n                \"SUCCESS\",\n                \"UNEXPECTED\",\n                \"ENCRYPTION_COMPLIANCE_MODE\",\n                \"VERIFICATION_COMPLIANCE_MODE\",\n            ):\n            pass\n        elif key == \"KEY_CONSIDERED\":\n            self.status = '\\n'.join([self.status, \"key considered\"])\n        elif key == \"NEWSIG\":\n            # Reset\n            self.status = None\n            self.valid = False\n            self.key_id, self.username = None, None\n        elif key == \"BADSIG\":\n            self.valid = False\n            self.status = 'signature bad'\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"GOODSIG\":\n            self.valid = True\n            self.status = 'signature good'\n            self.key_id, self.username = value.split(None, 1)\n        elif key == \"VALIDSIG\":\n            self.valid = True\n            (self.fingerprint,\n             self.creation_date,\n             self.sig_timestamp,\n             self.expire_timestamp) = value.split()[:4]\n            # may be different if signature is made with a subkey\n            self.pubkey_fingerprint = value.split()[-1]\n            self.status = 'signature valid'\n        elif key == \"SIG_ID\":\n            (self.signature_id,\n             self.creation_date, self.timestamp) = value.split()\n        elif key == \"ERRSIG\":\n            self.valid = False\n            (self.key_id,\n             algo, hash_algo,\n             cls,\n             self.timestamp) = value.split()[:5]\n            self.status = 'signature error'\n        elif key == \"DECRYPTION_FAILED\":\n            self.valid = False\n            self.key_id = value\n            self.status = 'decryption failed'\n        elif key in (\"WARNING\", \"ERROR\", \"FAILURE\"):\n            if key in (\"ERROR\", \"FAILURE\"):\n                self.valid = False\n            # The status will hold a (rather indecipherable and bad\n            # design, imho) location (in the GnuPG C code), GnuPG\n            # error_code, e.g. \"151011327_EOF\", and (possibly, in the\n            # case of WARNING or ERROR) additional text.\n            # Have fun figuring out what it means.\n            self.status = value\n            log.warn(\"%s status emitted from gpg process: %s\" % (key, value))\n        elif key == \"NO_PUBKEY\":\n            self.valid = False\n            self.key_id = value\n            self.status = 'no public key'\n        # These are useless in Verify, since they are spit out for any\n        # pub/subkeys on the key, not just the one doing the signing.\n        # if we want to check for signatures make with expired key,\n        # the relevant flags are REVKEYSIG and KEYREVOKED.\n        elif key in (\"KEYEXPIRED\", \"SIGEXPIRED\"):\n            pass\n        # The signature has an expiration date which has already passed\n        # (EXPKEYSIG), or the signature has been revoked (REVKEYSIG):\n        elif key in (\"EXPKEYSIG\", \"REVKEYSIG\"):\n            self.valid = False\n            self.key_id = value.split()[0]\n            self.status = (('%s %s') % (key[:3], key[3:])).lower()\n        # This is super annoying, and bad design on the part of GnuPG, in my\n        # opinion.\n        #\n        # This flag can get triggered if a valid signature is made, and then\n        # later the key (or subkey) which created the signature is\n        # revoked. When this happens, GnuPG will output:\n        #\n        # REVKEYSIG 075BFD18B365D34C Test Expired Key <test@python-gnupg.git>\n        # VALIDSIG DAB69B05F591640B7F4DCBEA075BFD18B365D34C 2014-09-26 1411700539 0 4 0 1 2 00 4BA800F77452A6C29447FF20F4AF76ACBBE22CE2\n        # KEYREVOKED\n        #\n        # Meaning that we have a timestamp for when the signature was created,\n        # and we know that the signature is valid, but since GnuPG gives us no\n        # timestamp for when the key was revoked... we have no ability to\n        # determine if the valid signature was made *before* the signing key\n        # was revoked or *after*. Meaning that if you are like me and you sign\n        # all your software releases and git commits, and you also practice\n        # good opsec by doing regular key rotations, your old signatures made\n        # by your expired/revoked keys (even though they were created when the\n        # key was still good) are considered bad because GnuPG is a\n        # braindamaged piece of shit.\n        #\n        # Software engineering, motherfuckers, DO YOU SPEAK IT?\n        #\n        # The signing key which created the signature has since been revoked\n        # (KEYREVOKED), and we're going to ignore it (but add something to the\n        # status message):\n        elif key in (\"KEYREVOKED\"):\n            self.status = '\\n'.join([self.status, \"key revoked\"])\n        # SIG_SUBPACKET <type> <flags> <len> <data>\n        # This indicates that a signature subpacket was seen.  The format is\n        # the same as the \"spk\" record above.\n        #\n        # [...]\n        #\n        # SPK - Signature subpacket records\n        #\n        # - Field 2 :: Subpacket number as per RFC-4880 and later.\n        # - Field 3 :: Flags in hex.  Currently the only two bits assigned\n        #              are 1, to indicate that the subpacket came from the\n        #              hashed part of the signature, and 2, to indicate the\n        #              subpacket was marked critical.\n        # - Field 4 :: Length of the subpacket.  Note that this is the\n        #              length of the subpacket, and not the length of field\n        #              5 below.  Due to the need for %-encoding, the length\n        #              of field 5 may be up to 3x this value.\n        # - Field 5 :: The subpacket data.  Printable ASCII is shown as\n        #              ASCII, but other values are rendered as %XX where XX\n        #              is the hex value for the byte.\n        elif key in (\"SIG_SUBPACKET\"):\n            fields = value.split()\n            try:\n                subpacket_number = fields[0]\n                self.subpackets[subpacket_number] = {'flags': None,\n                                                     'length': None,\n                                                     'data': None}\n            except IndexError:\n                # We couldn't parse the subpacket type (an RFC4880\n                # identifier), so we shouldn't continue parsing.\n                pass\n            else:\n                # Pull as much data as we can parse out of the subpacket:\n                try:\n                    self.subpackets[subpacket_number]['flags'] = fields[1]\n                    self.subpackets[subpacket_number]['length'] = fields[2]\n                    self.subpackets[subpacket_number]['data'] = fields[3]\n                except IndexError:\n                    pass\n        # NOTATION_\n        # There are actually two related status codes to convey notation\n        # data:\n        #\n        # - NOTATION_NAME <name>\n        # - NOTATION_DATA <string>\n        #\n        # <name> and <string> are %XX escaped; the data may be split among\n        # several NOTATION_DATA lines.\n        elif key.startswith(\"NOTATION_\"):\n            if key.endswith(\"NAME\"):\n                self.notations[value] = str()\n                self._last_notation_name = value\n            elif key.endswith(\"DATA\"):\n                if self._last_notation_name is not None:\n                    # Append the NOTATION_DATA to any previous data we\n                    # received for that NOTATION_NAME:\n                    self.notations[self._last_notation_name] += value\n                else:\n                    pass\n        else:\n            raise ValueError(\"Unknown status message: %r %r\" % (key, value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_status(self, key, value):\n        if key in (\n                \"ENC_TO\",\n                \"USERID_HINT\",\n                \"GOODMDC\",\n                \"END_DECRYPTION\",\n                \"BEGIN_SIGNING\",\n                \"NO_SECKEY\",\n                \"ERROR\",\n                \"NODATA\",\n                \"CARDCTRL\",\n            ):\n            # in the case of ERROR, this is because a more specific error\n            # message will have come first\n            pass\n        elif key in (\n                \"NEED_PASSPHRASE\",\n                \"BAD_PASSPHRASE\",\n                \"GOOD_PASSPHRASE\",\n                \"MISSING_PASSPHRASE\",\n                \"DECRYPTION_FAILED\",\n                \"KEY_NOT_CREATED\",\n                \"KEY_CONSIDERED\",\n            ):\n            self.status = key.replace(\"_\", \" \").lower()\n        elif key == \"NEED_TRUSTDB\":\n            self._gpg._create_trustdb()\n        elif key == \"NEED_PASSPHRASE_SYM\":\n            self.status = 'need symmetric passphrase'\n        elif key == \"BEGIN_DECRYPTION\":\n            self.status = 'decryption incomplete'\n        elif key == \"BEGIN_ENCRYPTION\":\n            self.status = 'encryption incomplete'\n        elif key == \"DECRYPTION_OKAY\":\n            self.status = 'decryption ok'\n            self.ok = True\n        elif key == \"END_ENCRYPTION\":\n            self.status = 'encryption ok'\n            self.ok = True\n        elif key == \"INV_RECP\":\n            self.status = 'invalid recipient'\n        elif key == \"KEYEXPIRED\":\n            self.status = 'key expired'\n        elif key == \"KEYREVOKED\":\n            self.status = 'key revoked'\n        elif key == \"SIG_CREATED\":\n            self.status = 'sig created'\n        elif key == \"SIGEXPIRED\":\n            self.status = 'sig expired'\n        elif key == \"PLAINTEXT\":\n            fmt, dts = value.split(' ', 1)\n            if dts.find(' ') > 0:\n                self.data_timestamp, self.data_filename = dts.split(' ', 1)\n            else:\n                self.data_timestamp = dts\n            ## GnuPG gives us a hex byte for an ascii char corresponding to\n            ## the data format of the resulting plaintext,\n            ## i.e. '62'\u2192'b':= binary data\n            self.data_format = chr(int(str(fmt), 16))\n        else:\n            super(Crypt, self)._handle_status(key, value)", "response": "Handles the status message from the attached GnuPG process."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the status message received from the attached GnuPG process.", "response": "def _handle_status(self, key, value):\n        \"\"\"Parse a status code from the attached GnuPG process.\n\n        :raises: :exc:`~exceptions.ValueError` if the status message is unknown.\n        \"\"\"\n        if key in (\n                'NO_SECKEY',\n                'BEGIN_DECRYPTION',\n                'DECRYPTION_FAILED',\n                'END_DECRYPTION',\n                'GOOD_PASSPHRASE',\n                'BAD_PASSPHRASE',\n                'KEY_CONSIDERED'\n            ):\n            pass\n        elif key == 'NODATA':\n            self.status = nodata(value)\n        elif key == 'ENC_TO':\n            key, _, _ = value.split()\n            if not self.key:\n                self.key = key\n            self.encrypted_to.append(key)\n        elif key in ('NEED_PASSPHRASE', 'MISSING_PASSPHRASE'):\n            self.need_passphrase = True\n        elif key == 'NEED_PASSPHRASE_SYM':\n            self.need_passphrase_sym = True\n        elif key == 'USERID_HINT':\n            self.userid_hint = value.strip().split()\n        else:\n            raise ValueError(\"Unknown status message: %r\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_trustdb(cls):\n    trustdb = os.path.join(cls.homedir, 'trustdb.gpg')\n    if not os.path.isfile(trustdb):\n        log.info(\"GnuPG complained that your trustdb file was missing. %s\"\n                 % \"This is likely due to changing to a new homedir.\")\n        log.info(\"Creating trustdb.gpg file in your GnuPG homedir.\")\n        cls.fix_trustdb(trustdb)", "response": "Create the trustdb file in our homedir if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_ownertrust(cls, trustdb=None):\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, 'trustdb.gpg')\n\n    try:\n        os.rename(trustdb, trustdb + '.bak')\n    except (OSError, IOError) as err:\n        log.debug(str(err))\n\n    export_proc = cls._open_subprocess(['--export-ownertrust'])\n    tdb = open(trustdb, 'wb')\n    _util._threaded_copy_data(export_proc.stdout, tdb)\n    export_proc.wait()", "response": "Export ownertrust to a trustdb file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_ownertrust(cls, trustdb=None):\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, 'trustdb.gpg')\n\n    import_proc = cls._open_subprocess(['--import-ownertrust'])\n\n    try:\n        tdb = open(trustdb, 'rb')\n    except (OSError, IOError):\n        log.error(\"trustdb file %s does not exist!\" % trustdb)\n\n    _util._threaded_copy_data(tdb, import_proc.stdin)\n    import_proc.wait()", "response": "Import ownertrust from a trustdb file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_trustdb(cls, trustdb=None):\n    if trustdb is None:\n        trustdb = os.path.join(cls.homedir, 'trustdb.gpg')\n    export_proc = cls._open_subprocess(['--export-ownertrust'])\n    import_proc = cls._open_subprocess(['--import-ownertrust'])\n    _util._threaded_copy_data(export_proc.stdout, import_proc.stdin)\n    export_proc.wait()\n    import_proc.wait()", "response": "Attempt to repair a broken trustdb. gpg file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(self, message, *args, **kwargs):\n    if self.isEnabledFor(GNUPG_STATUS_LEVEL):\n        self._log(GNUPG_STATUS_LEVEL, message, args, **kwargs)", "response": "Log a message at the status level."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a logger for python - gnupg at a specific message level.", "response": "def create_logger(level=logging.NOTSET):\n    \"\"\"Create a logger for python-gnupg at a specific message level.\n\n    :type level: :obj:`int` or :obj:`str`\n    :param level: A string or an integer for the lowest level to include in\n                  logs.\n\n    **Available levels:**\n\n    ==== ======== ========================================\n    int   str     description\n    ==== ======== ========================================\n    0    NOTSET   Disable all logging.\n    9    GNUPG    Log GnuPG's internal status messages.\n    10   DEBUG    Log module level debuging messages.\n    20   INFO     Normal user-level messages.\n    30   WARN     Warning messages.\n    40   ERROR    Error messages and tracebacks.\n    50   CRITICAL Unhandled exceptions and tracebacks.\n    ==== ======== ========================================\n    \"\"\"\n    _test = os.path.join(os.path.join(os.getcwd(), 'pretty_bad_protocol'), 'test')\n    _now  = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n    _fn   = os.path.join(_test, \"%s_test_gnupg.log\" % _now)\n    _fmt  = \"%(relativeCreated)-4d L%(lineno)-4d:%(funcName)-18.18s %(levelname)-7.7s %(message)s\"\n\n    ## Add the GNUPG_STATUS_LEVEL LogRecord to all Loggers in the module:\n    logging.addLevelName(GNUPG_STATUS_LEVEL, \"GNUPG\")\n    logging.Logger.status = status\n\n    if level > logging.NOTSET:\n        logging.basicConfig(level=level, filename=_fn,\n                            filemode=\"a\", format=_fmt)\n        logging.logThreads = True\n        if hasattr(logging,'captureWarnings'):\n            logging.captureWarnings(True)\n        colouriser = _ansistrm.ColorizingStreamHandler\n        colouriser.level_map[9]  = (None, 'blue', False)\n        colouriser.level_map[10] = (None, 'cyan', False)\n        handler = colouriser(sys.stderr)\n        handler.setLevel(level)\n\n        formatr = logging.Formatter(_fmt)\n        handler.setFormatter(formatr)\n    else:\n        handler = NullHandler()\n\n    log = logging.getLogger('gnupg')\n    log.addHandler(handler)\n    log.setLevel(level)\n    log.info(\"Log opened: %s UTC\" % datetime.ctime(datetime.utcnow()))\n    return log"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a process running under the current euid.", "response": "def _find_agent(cls):\n        \"\"\"Discover if a gpg-agent process for the current euid is running.\n\n        If there is a matching gpg-agent process, set a :class:`psutil.Process`\n        instance containing the gpg-agent process' information to\n        ``cls._agent_proc``.\n\n        For Unix systems, we check that the effective UID of this\n        ``python-gnupg`` process is also the owner of the gpg-agent\n        process. For Windows, we check that the usernames of the owners are\n        the same. (Sorry Windows users; maybe you should switch to anything\n        else.)\n\n        .. note: This function will only run if the psutil_ Python extension\n            is installed. Because psutil won't run with the PyPy interpreter,\n            use of it is optional (although highly recommended).\n\n        .. _psutil: https://pypi.python.org/pypi/psutil\n\n        :returns: True if there exists a gpg-agent process running under the\n                  same effective user ID as that of this program. Otherwise,\n                  returns False.\n        \"\"\"\n        if not psutil:\n            return False\n\n        this_process = psutil.Process(os.getpid())\n        ownership_match = False\n\n        if _util._running_windows:\n            identity = this_process.username()\n        else:\n            identity = this_process.uids\n\n        for proc in psutil.process_iter():\n            try:\n                # In my system proc.name & proc.is_running are methods\n                if (proc.name() == \"gpg-agent\") and proc.is_running():\n                    log.debug(\"Found gpg-agent process with pid %d\" % proc.pid)\n                    if _util._running_windows:\n                        if proc.username() == identity:\n                            ownership_match = True\n                    else:\n                        # proc.uids & identity are methods to\n                        if proc.uids() == identity():\n                            ownership_match = True\n            except psutil.Error as err:\n                # Exception when getting proc info, possibly because the\n                # process is zombie / process no longer exist. Just ignore it.\n                log.warn(\"Error while attempting to find gpg-agent process: %s\" % err)\n            # Next code must be inside for operator.\n            # Otherwise to _agent_proc will be saved not \"gpg-agent\" process buth an other.\n            if ownership_match:\n                log.debug(\"Effective UIDs of this process and gpg-agent match\")\n                setattr(cls, '_agent_proc', proc)\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_preference_list(self, prefs):\n        prefs = _check_preferences(prefs)\n        if prefs is not None:\n            self._prefs = prefs", "response": "Set the default preference list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the directory to use as GnuPG s homedir.", "response": "def _homedir_setter(self, directory):\n        \"\"\"Set the directory to use as GnuPG's homedir.\n\n        If unspecified, use $HOME/.config/python-gnupg. If specified, ensure\n        that the ``directory`` does not contain various shell escape\n        characters. If ``directory`` is not found, it will be automatically\n        created. Lastly, the ``direcory`` will be checked that the EUID has\n        read and write permissions for it.\n\n        :param str directory: A relative or absolute path to the directory to\n                            use for storing/accessing GnuPG's files, including\n                            keyrings and the trustdb.\n        :raises: :exc:`~exceptions.RuntimeError` if unable to find a suitable\n                 directory to use.\n        \"\"\"\n        if not directory:\n            log.debug(\"GPGBase._homedir_setter(): Using default homedir: '%s'\"\n                      % _util._conf)\n            directory = _util._conf\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._homedir_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._homedir_setter(): Check existence of '%s'\" % hd)\n            _util._create_if_necessary(hd)\n\n        if self.ignore_homedir_permissions:\n            self._homedir = hd\n        else:\n            try:\n                log.debug(\"GPGBase._homedir_setter(): checking permissions\")\n                assert _util._has_readwrite(hd), \\\n                    \"Homedir '%s' needs read/write permissions\" % hd\n            except AssertionError as ae:\n                msg = (\"Unable to set '%s' as GnuPG homedir\" % directory)\n                log.debug(\"GPGBase.homedir.setter(): %s\" % msg)\n                log.debug(str(ae))\n                raise RuntimeError(str(ae))\n            else:\n                log.info(\"Setting homedir to '%s'\" % hd)\n                self._homedir = hd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generated_keys_setter(self, directory):\n        if not directory:\n            directory = os.path.join(self.homedir, 'generated-keys')\n            log.debug(\"GPGBase._generated_keys_setter(): Using '%s'\"\n                      % directory)\n\n        hd = _parsers._fix_unsafe(directory)\n        log.debug(\"GPGBase._generated_keys_setter(): got directory '%s'\" % hd)\n\n        if hd:\n            log.debug(\"GPGBase._generated_keys_setter(): Check exists '%s'\"\n                      % hd)\n            _util._create_if_necessary(hd)\n\n        try:\n            log.debug(\"GPGBase._generated_keys_setter(): check permissions\")\n            assert _util._has_readwrite(hd), \\\n                \"Keys dir '%s' needs read/write permissions\" % hd\n        except AssertionError as ae:\n            msg = (\"Unable to set '%s' as generated keys dir\" % directory)\n            log.debug(\"GPGBase._generated_keys_setter(): %s\" % msg)\n            log.debug(str(ae))\n            raise RuntimeError(str(ae))\n        else:\n            log.info(\"Setting homedir to '%s'\" % hd)\n            self.__generated_keys = hd", "response": "Set the directory for storing generated keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_sane_and_get_gpg_version(self):\n        proc = self._open_subprocess([\"--list-config\", \"--with-colons\"])\n        result = self._result_map['list'](self)\n        self._read_data(proc.stdout, result)\n        if proc.returncode:\n            raise RuntimeError(\"Error invoking gpg: %s\" % result.data)\n        else:\n            try:\n                proc.terminate()\n            except OSError:\n                log.error((\"Could neither invoke nor terminate a gpg process... \"\n                           \"Are you sure you specified the corrent (and full) \"\n                           \"path to the gpg binary?\"))\n\n        version_line = result.data.partition(b':version:')[2].decode()\n        if not version_line:\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % result.data)\n        self.binary_version = version_line.split('\\n')[0]\n        if not _VERSION_RE.match(self.binary_version):\n            raise RuntimeError(\"Got invalid version line from gpg: %s\\n\" % self.binary_version)\n        log.debug(\"Using GnuPG version %s\" % self.binary_version)", "response": "Check that everything runs alright and grab the GPG binary s version number while we re at it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_args(self, args, passphrase=False):\n        ## see TODO file, tag :io:makeargs:\n        cmd = [self.binary,\n               '--no-options --no-emit-version --no-tty --status-fd 2']\n\n        if self.homedir: cmd.append('--homedir \"%s\"' % self.homedir)\n\n        if self.keyring:\n            cmd.append('--no-default-keyring --keyring %s' % self.keyring)\n        if self.secring:\n            cmd.append('--secret-keyring %s' % self.secring)\n\n        if passphrase: cmd.append('--batch --passphrase-fd 0')\n\n        if self.use_agent is True: cmd.append('--use-agent')\n        elif self.use_agent is False: cmd.append('--no-use-agent')\n\n        # The arguments for debugging and verbosity should be placed into the\n        # cmd list before the options/args in order to resolve Issue #76:\n        # https://github.com/isislovecruft/python-gnupg/issues/76\n        if self.verbose:\n            cmd.append('--debug-all')\n\n            if (isinstance(self.verbose, str) or\n                (isinstance(self.verbose, int) and (self.verbose >= 1))):\n                # GnuPG<=1.4.18 parses the `--debug-level` command in a way\n                # that is incompatible with all other GnuPG versions. :'(\n                if self.binary_version and (self.binary_version <= '1.4.18'):\n                    cmd.append('--debug-level=%s' % self.verbose)\n                else:\n                    cmd.append('--debug-level %s' % self.verbose)\n\n        if self.options:\n            [cmd.append(opt) for opt in iter(_sanitise_list(self.options))]\n        if args:\n            [cmd.append(arg) for arg in iter(_sanitise_list(args))]\n\n        return cmd", "response": "Make a list of command line elements for GPG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a pipe to a GPG subprocess and return the file objects for the GnuPG record.", "response": "def _open_subprocess(self, args=None, passphrase=False):\n        \"\"\"Open a pipe to a GPG subprocess and return the file objects for\n        communicating with it.\n\n        :param list args: A list of strings of options and flags to pass to\n                          ``GPG.binary``. This is input safe, meaning that\n                          these values go through strict checks (see\n                          ``parsers._sanitise_list``) before being passed to to\n                          the input file descriptor for the GnuPG process.\n                          Each string should be given exactly as it would be on\n                          the commandline interface to GnuPG,\n                          e.g. [\"--cipher-algo AES256\", \"--default-key\n                          A3ADB67A2CDB8B35\"].\n\n        :param bool passphrase: If True, the passphrase will be sent to the\n                                stdin file descriptor for the attached GnuPG\n                                process.\n        \"\"\"\n        ## see http://docs.python.org/2/library/subprocess.html#converting-an\\\n        ##    -argument-sequence-to-a-string-on-windows\n        cmd = shlex.split(' '.join(self._make_args(args, passphrase)))\n        log.debug(\"Sending command to GnuPG process:%s%s\" % (os.linesep, cmd))\n\n        if platform.system() == \"Windows\":\n            # TODO figure out what the hell is going on there.\n            expand_shell = True\n        else:\n            expand_shell = False\n\n        environment = {\n            'LANGUAGE': os.environ.get('LANGUAGE') or 'en',\n            'GPG_TTY': os.environ.get('GPG_TTY') or '',\n            'DISPLAY': os.environ.get('DISPLAY') or '',\n            'GPG_AGENT_INFO': os.environ.get('GPG_AGENT_INFO') or '',\n            'GPG_TTY': os.environ.get('GPG_TTY') or '',\n            'GPG_PINENTRY_PATH': os.environ.get('GPG_PINENTRY_PATH') or '',\n        }\n\n        return subprocess.Popen(cmd, shell=expand_shell, stdin=subprocess.PIPE,\n                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                                env=environment)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread all the stderr output from GPG and parses the status lines.", "response": "def _read_response(self, stream, result):\n        \"\"\"Reads all the stderr output from GPG, taking notice only of lines\n        that begin with the magic [GNUPG:] prefix.\n\n        Calls methods on the response object for each valid token found, with\n        the arg being the remainder of the status line.\n\n        :param stream: A byte-stream, file handle, or a\n                       :data:`subprocess.PIPE` for parsing the status codes\n                       from the GnuPG process.\n\n        :param result: The result parser class from :mod:`~gnupg._parsers` \u2015\n                       the ``handle_status()`` method of that class will be\n                       called in order to parse the output of ``stream``.\n        \"\"\"\n        # All of the userland messages (i.e. not status-fd lines) we're not\n        # interested in passing to our logger\n        userland_messages_to_ignore = []\n\n        if self.ignore_homedir_permissions:\n            userland_messages_to_ignore.append('unsafe ownership on homedir')\n\n        lines = []\n\n        while True:\n            line = stream.readline()\n            if len(line) == 0:\n                break\n            lines.append(line)\n            line = line.rstrip()\n\n            if line.startswith('[GNUPG:]'):\n                line = _util._deprefix(line, '[GNUPG:] ', log.status)\n                keyword, value = _util._separate_keyword(line)\n                result._handle_status(keyword, value)\n            elif line.startswith('gpg:'):\n                line = _util._deprefix(line, 'gpg: ')\n                keyword, value = _util._separate_keyword(line)\n\n                # Silence warnings from gpg we're supposed to ignore\n                ignore = any(msg in value for msg in userland_messages_to_ignore)\n\n                if not ignore:\n                    # Log gpg's userland messages at our own levels:\n                    if keyword.upper().startswith(\"WARNING\"):\n                        log.warn(\"%s\" % value)\n                    elif keyword.upper().startswith(\"FATAL\"):\n                        log.critical(\"%s\" % value)\n                        # Handle the gpg2 error where a missing trustdb.gpg is,\n                        # for some stupid reason, considered fatal:\n                        if value.find(\"trustdb.gpg\") and value.find(\"No such file\"):\n                            result._handle_status('NEED_TRUSTDB', '')\n            else:\n                if self.verbose:\n                    log.info(\"%s\" % line)\n                else:\n                    log.debug(\"%s\" % line)\n        result.stderr = ''.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_data(self, stream, result):\n        chunks = []\n        log.debug(\"Reading data from stream %r...\" % stream.__repr__())\n\n        while True:\n            data = stream.read(1024)\n            if len(data) == 0:\n                break\n            chunks.append(data)\n            log.debug(\"Read %4d bytes\" % len(data))\n\n        # Join using b'' or '', as appropriate\n        result.data = type(data)().join(chunks)\n        log.debug(\"Finishing reading from stream %r...\" % stream.__repr__())\n        log.debug(\"Read %4d bytes total\" % len(result.data))", "response": "Incrementally read from stream and store read data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_verbose(self, verbose):\n        string_levels = ('basic', 'advanced', 'expert', 'guru')\n\n        if verbose is True:\n            # The caller wants logging, but we need a valid --debug-level\n            # for gpg. Default to \"basic\", and warn about the ambiguity.\n            verbose = 'basic'\n\n        if (isinstance(verbose, str) and not (verbose in string_levels)):\n            verbose = 'basic'\n\n        self.verbose = verbose", "response": "Check and set our verbose attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrain the subprocesses output streams writing the collected output to the result.", "response": "def _collect_output(self, process, result, writer=None, stdin=None):\n        \"\"\"Drain the subprocesses output streams, writing the collected output\n        to the result. If a writer thread (writing to the subprocess) is given,\n        make sure it's joined before returning. If a stdin stream is given,\n        close it before returning.\n        \"\"\"\n        stderr = codecs.getreader(self._encoding)(process.stderr)\n        rr = threading.Thread(target=self._read_response,\n                              args=(stderr, result))\n        rr.setDaemon(True)\n        log.debug('stderr reader: %r', rr)\n        rr.start()\n\n        stdout = process.stdout\n        dr = threading.Thread(target=self._read_data, args=(stdout, result))\n        dr.setDaemon(True)\n        log.debug('stdout reader: %r', dr)\n        dr.start()\n\n        dr.join()\n        rr.join()\n        if writer is not None:\n            writer.join()\n        process.wait()\n        if stdin is not None:\n            try:\n                stdin.close()\n            except IOError:\n                pass\n        stderr.close()\n        stdout.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle a call to GPG - pass input data collect output data.", "response": "def _handle_io(self, args, file, result, passphrase=False, binary=False):\n        \"\"\"Handle a call to GPG - pass input data, collect output data.\"\"\"\n        p = self._open_subprocess(args, passphrase)\n        if not binary:\n            stdin = codecs.getwriter(self._encoding)(p.stdin)\n        else:\n            stdin = p.stdin\n        if passphrase:\n            _util._write_passphrase(stdin, passphrase, self._encoding)\n        writer = _util._threaded_copy_data(file, stdin)\n        self._collect_output(p, result, writer, stdin)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _recv_keys(self, keyids, keyserver=None):\n        if not keyserver:\n            keyserver = self.keyserver\n\n        args = ['--keyserver {0}'.format(keyserver),\n                '--recv-keys {0}'.format(keyids)]\n        log.info('Requesting keys from %s: %s' % (keyserver, keyids))\n\n        result = self._result_map['import'](self)\n        proc = self._open_subprocess(args)\n        self._collect_output(proc, result)\n        log.debug('recv_keys result: %r', result.__dict__)\n        return result", "response": "Import keys from a keyserver."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsign a file with the specified key and pass it to the appropriate key.", "response": "def _sign_file(self, file, default_key=None, passphrase=None,\n                   clearsign=True, detach=False, binary=False,\n                   digest_algo='SHA512'):\n        \"\"\"Create a signature for a file.\n\n        :param file: The file stream (i.e. it's already been open()'d) to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                ``$ gpg --with-colons --list-config\n                                digestname``. The default, if unspecified, is\n                                ``'SHA512'``.\n        \"\"\"\n        log.debug(\"_sign_file():\")\n        if binary:\n            log.info(\"Creating binary signature for file %s\" % file)\n            args = ['--sign']\n        else:\n            log.info(\"Creating ascii-armoured signature for file %s\" % file)\n            args = ['--sign --armor']\n\n        if clearsign:\n            args.append(\"--clearsign\")\n            if detach:\n                log.warn(\"Cannot use both --clearsign and --detach-sign.\")\n                log.warn(\"Using default GPG behaviour: --clearsign only.\")\n        elif detach and not clearsign:\n            args.append(\"--detach-sign\")\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.append(str(\"--digest-algo %s\" % digest_algo))\n\n        ## We could use _handle_io here except for the fact that if the\n        ## passphrase is bad, gpg bails and you can't write the message.\n        result = self._result_map['sign'](self)\n\n        ## If the passphrase is an empty string, the message up to and\n        ## including its first newline will be cut off before making it to the\n        ## GnuPG process. Therefore, if the passphrase='' or passphrase=b'',\n        ## we set passphrase=None.  See Issue #82:\n        ## https://github.com/isislovecruft/python-gnupg/issues/82\n        if _util._is_string(passphrase):\n            passphrase = passphrase if len(passphrase) > 0 else None\n        elif _util._is_bytes(passphrase):\n            passphrase = s(passphrase) if len(passphrase) > 0 else None\n        else:\n            passphrase = None\n\n        proc = self._open_subprocess(args, passphrase is not None)\n        try:\n            if passphrase:\n                _util._write_passphrase(proc.stdin, passphrase, self._encoding)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n        except IOError as ioe:\n            log.exception(\"Error writing message: %s\" % str(ioe))\n            writer = None\n        self._collect_output(proc, result, writer, proc.stdin)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencrypt the message read from the file - like object data to the set of recipients.", "response": "def _encrypt(self, data, recipients,\n                 default_key=None,\n                 passphrase=None,\n                 armor=True,\n                 encrypt=True,\n                 symmetric=False,\n                 always_trust=True,\n                 output=None,\n                 throw_keyids=False,\n                 hidden_recipients=None,\n                 cipher_algo='AES256',\n                 digest_algo='SHA512',\n                 compress_algo='ZLIB'):\n        \"\"\"Encrypt the message read from the file-like object **data**.\n\n        :param str data: The file or bytestream to encrypt.\n\n        :param str recipients: The recipients to encrypt to. Recipients must\n                               be specified keyID/fingerprint.\n\n        .. warning:: Care should be taken in Python2 to make sure that the\n                     given fingerprints for **recipients** are in fact strings\n                     and not unicode objects.\n\n        :param str default_key: The keyID/fingerprint of the key to use for\n                                signing. If given, **data** will be encrypted\n                                *and* signed.\n\n        :param str passphrase: If given, and **default_key** is also given,\n                               use this passphrase to unlock the secret\n                               portion of the **default_key** to sign the\n                               encrypted **data**.  Otherwise, if\n                               **default_key** is not given, but **symmetric**\n                               is ``True``, then use this passphrase as the\n                               passphrase for symmetric encryption. Signing\n                               and symmetric encryption should *not* be\n                               combined when sending the **data** to other\n                               recipients, else the passphrase to the secret\n                               key would be shared with them.\n\n        :param bool armor: If True, ascii armor the output; otherwise, the\n                           output will be in binary format. (Default: True)\n\n        :param bool encrypt: If True, encrypt the **data** using the\n                             **recipients** public keys. (Default: True)\n\n        :param bool symmetric: If True, encrypt the **data** to **recipients**\n                               using a symmetric key. See the **passphrase**\n                               parameter. Symmetric encryption and public key\n                               encryption can be used simultaneously, and will\n                               result in a ciphertext which is decryptable\n                               with either the symmetric **passphrase** or one\n                               of the corresponding private keys.\n\n        :param bool always_trust: If True, ignore trust warnings on\n                                  **recipients** keys. If False, display trust\n                                  warnings. (default: True)\n\n        :type output: str or file-like object\n        :param output: The output file to write to. If not specified, the\n                       encrypted output is returned, and thus should be stored\n                       as an object in Python. For example:\n\n        >>> import shutil\n        >>> import gnupg\n        >>> if os.path.exists(\"doctests\"):\n        ...     shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_settings = gpg.gen_key_input(key_type='RSA',\n        ...                                  key_length=1024,\n        ...                                  key_usage='ESCA',\n        ...                                  passphrase='foo')\n        >>> key = gpg.gen_key(key_settings)\n        >>> message = \"The crow flies at midnight.\"\n        >>> encrypted = str(gpg.encrypt(message, key.fingerprint))\n        >>> assert encrypted != message\n        >>> assert not encrypted.isspace()\n        >>> decrypted = str(gpg.decrypt(encrypted, passphrase='foo'))\n        >>> assert not decrypted.isspace()\n        >>> decrypted\n        'The crow flies at midnight.'\n\n\n        :param bool throw_keyids: If True, make all **recipients** keyids be\n            zero'd out in packet information. This is the same as using\n            **hidden_recipients** for all **recipients**. (Default: False).\n\n        :param list hidden_recipients: A list of recipients that should have\n            their keyids zero'd out in packet information.\n\n        :param str cipher_algo: The cipher algorithm to use. To see available\n                                algorithms with your version of GnuPG, do:\n                                :command:`$ gpg --with-colons --list-config\n                                ciphername`. The default **cipher_algo**, if\n                                unspecified, is ``'AES256'``.\n\n        :param str digest_algo: The hash digest to use. Again, to see which\n                                hashes your GnuPG is capable of using, do:\n                                :command:`$ gpg --with-colons --list-config\n                                digestname`.  The default, if unspecified, is\n                                ``'SHA512'``.\n\n        :param str compress_algo: The compression algorithm to use. Can be one\n                                  of ``'ZLIB'``, ``'BZIP2'``, ``'ZIP'``, or\n                                  ``'Uncompressed'``.\n        \"\"\"\n        args = []\n\n        ## FIXME: GnuPG appears to ignore the --output directive when being\n        ## programmatically driven. We'll handle the IO ourselves to fix this\n        ## for now.\n        output_filename = None\n        if output:\n            if getattr(output, 'fileno', None) is not None:\n                ## avoid overwrite confirmation message\n                if getattr(output, 'name', None) is not None:\n                    output_filename = output.name\n                    if os.path.exists(output.name):\n                        os.remove(output.name)\n                    #args.append('--output %s' % output.name)\n            else:\n                output_filename = output\n                if os.path.exists(output):\n                    os.remove(output)\n                #args.append('--output %s' % output)\n\n        if armor: args.append('--armor')\n        if always_trust: args.append('--always-trust')\n        if cipher_algo: args.append('--cipher-algo %s' % cipher_algo)\n        if compress_algo: args.append('--compress-algo %s' % compress_algo)\n\n        if default_key:\n            args.append('--sign')\n            args.append('--default-key %s' % default_key)\n            if digest_algo:\n                args.append('--digest-algo %s' % digest_algo)\n\n        ## both can be used at the same time for an encrypted file which\n        ## is decryptable with a passphrase or secretkey.\n        if symmetric: args.append('--symmetric')\n        if encrypt: args.append('--encrypt')\n        if throw_keyids: args.append('--throw-keyids')\n\n        if len(recipients) >= 1:\n            log.debug(\"GPG.encrypt() called for recipients '%s' with type '%s'\"\n                      % (recipients, type(recipients)))\n\n            if isinstance(recipients, (list, tuple)):\n                for recp in recipients:\n                    if not _util._py3k:\n                        if isinstance(recp, unicode):\n                            try:\n                                assert _parsers._is_hex(str(recp))\n                            except AssertionError:\n                                log.info(\"Can't accept recipient string: %s\"\n                                         % recp)\n                            else:\n                                self._add_recipient_string(args, hidden_recipients, str(recp))\n                                continue\n                            ## will give unicode in 2.x as '\\uXXXX\\uXXXX'\n                            if isinstance(hidden_recipients, (list, tuple)):\n                                if [s for s in hidden_recipients if recp in str(s)]:\n                                    args.append('--hidden-recipient %r' % recp)\n                                else:\n                                    args.append('--recipient %r' % recp)\n                            else:\n                                args.append('--recipient %r' % recp)\n                            continue\n                    if isinstance(recp, str):\n                        self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif (not _util._py3k) and isinstance(recp, basestring):\n                for recp in recipients.split('\\x20'):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n\n            elif _util._py3k and isinstance(recp, str):\n                for recp in recipients.split(' '):\n                    self._add_recipient_string(args, hidden_recipients, recp)\n                    ## ...and now that we've proven py3k is better...\n            else:\n                log.debug(\"Don't know what to do with recipients: %r\"\n                          % recipients)\n\n        result = self._result_map['crypt'](self)\n        log.debug(\"Got data '%s' with type '%s'.\" % (data, type(data)))\n        self._handle_io(args, data, result, passphrase=passphrase, binary=True)\n        # Avoid writing raw encrypted bytes to terminal loggers and breaking\n        # them in that adorable way where they spew hieroglyphics until reset:\n        if armor:\n            log.debug(\"\\n%s\" % result.data)\n\n        if output_filename:\n            log.info(\"Writing encrypted output to file: %s\" % output_filename)\n            with open(output_filename, 'wb') as fh:\n                fh.write(result.data)\n                fh.flush()\n                log.info(\"Encrypted output written successfully.\")\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding functions for encoding translations for a specific codec.", "response": "def find_encodings(enc=None, system=False):\n    \"\"\"Find functions for encoding translations for a specific codec.\n\n    :param str enc: The codec to find translation functions for. It will be\n                    normalized by converting to lowercase, excluding\n                    everything which is not ascii, and hyphens will be\n                    converted to underscores.\n\n    :param bool system: If True, find encodings based on the system's stdin\n                        encoding, otherwise assume utf-8.\n\n    :raises: :exc:LookupError if the normalized codec, ``enc``, cannot be\n             found in Python's encoding translation map.\n    \"\"\"\n    if not enc:\n        enc = 'utf-8'\n\n    if system:\n        if getattr(sys.stdin, 'encoding', None) is None:\n            enc = sys.stdin.encoding\n            log.debug(\"Obtained encoding from stdin: %s\" % enc)\n        else:\n            enc = 'ascii'\n\n    ## have to have lowercase to work, see\n    ## http://docs.python.org/dev/library/codecs.html#standard-encodings\n    enc = enc.lower()\n    codec_alias = encodings.normalize_encoding(enc)\n\n    codecs.register(encodings.search_function)\n    coder = codecs.lookup(codec_alias)\n\n    return coder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy data from one stream to another.", "response": "def _copy_data(instream, outstream):\n    \"\"\"Copy data from one stream to another.\n\n    :type instream: :class:`io.BytesIO` or :class:`io.StringIO` or file\n    :param instream: A byte stream or open file to read from.\n    :param file outstream: The file descriptor of a tmpfile to write to.\n    \"\"\"\n    sent = 0\n\n    while True:\n        if ((_py3k and isinstance(instream, str)) or\n            (not _py3k and isinstance(instream, basestring))):\n            data = instream[:1024]\n            instream = instream[1024:]\n        else:\n            data = instream.read(1024)\n        if len(data) == 0:\n            break\n\n        sent += len(data)\n        if ((_py3k and isinstance(data, str)) or\n            (not _py3k and isinstance(data, basestring))):\n            encoded = binary(data)\n        else:\n            encoded = data\n        log.debug(\"Sending %d bytes of data...\" % sent)\n        log.debug(\"Encoded data (type %s):\\n%s\" % (type(encoded), encoded))\n\n        if not _py3k:\n            try:\n                outstream.write(encoded)\n            except IOError as ioe:\n                # Can get 'broken pipe' errors even when all data was sent\n                if 'Broken pipe' in str(ioe):\n                    log.error('Error sending data: Broken pipe')\n                else:\n                    log.exception(ioe)\n                break\n            else:\n                log.debug(\"Wrote data type <type 'str'> to outstream.\")\n        else:\n            try:\n                outstream.write(bytes(encoded))\n            except TypeError as te:\n                # XXX FIXME This appears to happen because\n                # _threaded_copy_data() sometimes passes the `outstream` as an\n                # object with type <_io.BufferredWriter> and at other times\n                # with type <encodings.utf_8.StreamWriter>.  We hit the\n                # following error when the `outstream` has type\n                # <encodings.utf_8.StreamWriter>.\n                if not \"convert 'bytes' object to str implicitly\" in str(te):\n                    log.error(str(te))\n                try:\n                    outstream.write(encoded.decode())\n                except TypeError as yate:\n                    # We hit the \"'str' does not support the buffer interface\"\n                    # error in Python3 when the `outstream` is an io.BytesIO and\n                    # we try to write a str to it.  We don't care about that\n                    # error, we'll just try again with bytes.\n                    if not \"does not support the buffer interface\" in str(yate):\n                        log.error(str(yate))\n                except IOError as ioe:\n                    # Can get 'broken pipe' errors even when all data was sent\n                    if 'Broken pipe' in str(ioe):\n                        log.error('Error sending data: Broken pipe')\n                    else:\n                        log.exception(ioe)\n                    break\n                else:\n                    log.debug(\"Wrote data type <class 'str'> outstream.\")\n            except IOError as ioe:\n                # Can get 'broken pipe' errors even when all data was sent\n                if 'Broken pipe' in str(ioe):\n                    log.error('Error sending data: Broken pipe')\n                else:\n                    log.exception(ioe)\n                break\n            else:\n                log.debug(\"Wrote data type <class 'bytes'> to outstream.\")\n\n    try:\n        outstream.close()\n    except IOError as ioe:\n        log.error(\"Unable to close outstream %s:\\r\\t%s\" % (outstream, ioe))\n    else:\n        log.debug(\"Closed outstream: %d bytes sent.\" % sent)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the specified directory if necessary.", "response": "def _create_if_necessary(directory):\n    \"\"\"Create the specified directory, if necessary.\n\n    :param str directory: The directory to use.\n    :rtype: bool\n    :returns: True if no errors occurred and the directory was created or\n              existed beforehand, False otherwise.\n    \"\"\"\n\n    if not os.path.isabs(directory):\n        log.debug(\"Got non-absolute path: %s\" % directory)\n        directory = os.path.abspath(directory)\n\n    if not os.path.isdir(directory):\n        log.info(\"Creating directory: %s\" % directory)\n        try:\n            os.makedirs(directory, 0x1C0)\n        except OSError as ose:\n            log.error(ose, exc_info=1)\n            return False\n        else:\n            log.debug(\"Created directory.\")\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an email address suitable for a UID on a GnuPG key.", "response": "def create_uid_email(username=None, hostname=None):\n    \"\"\"Create an email address suitable for a UID on a GnuPG key.\n\n    :param str username: The username portion of an email address.  If None,\n                         defaults to the username of the running Python\n                         process.\n\n    :param str hostname: The FQDN portion of an email address. If None, the\n                         hostname is obtained from gethostname(2).\n\n    :rtype: str\n    :returns: A string formatted as <username>@<hostname>.\n    \"\"\"\n    if hostname:\n        hostname = hostname.replace(' ', '_')\n    if not username:\n        try: username = os.environ['LOGNAME']\n        except KeyError: username = os.environ['USERNAME']\n\n        if not hostname: hostname = gethostname()\n\n        uid = \"%s@%s\" % (username.replace(' ', '_'), hostname)\n    else:\n        username = username.replace(' ', '_')\n        if (not hostname) and (username.find('@') == 0):\n            uid = \"%s@%s\" % (username, gethostname())\n        elif hostname:\n            uid = \"%s@%s\" % (username, hostname)\n        else:\n            uid = username\n\n    return uid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the prefix string from the beginning of a line.", "response": "def _deprefix(line, prefix, callback=None):\n    \"\"\"Remove the prefix string from the beginning of line, if it exists.\n\n    :param string line: A line, such as one output by GnuPG's status-fd.\n    :param string prefix: A substring to remove from the beginning of\n        ``line``. Case insensitive.\n    :type callback: callable\n    :param callback: Function to call if the prefix is found. The signature to\n        callback will be only one argument, the ``line`` without the ``prefix``, i.e.\n        ``callback(line)``.\n    :rtype: string\n    :returns: If the prefix was found, the ``line`` without the prefix is\n        returned. Otherwise, the original ``line`` is returned.\n    \"\"\"\n    try:\n        assert line.upper().startswith(u''.join(prefix).upper())\n    except AssertionError:\n        log.debug(\"Line doesn't start with prefix '%s':\\n%s\" % (prefix, line))\n        return line\n    else:\n        newline = line[len(prefix):]\n        if callback is not None:\n            try:\n                callback(newline)\n            except Exception as exc:\n                log.exception(exc)\n        return newline"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the absolute path to the GnuPG binary.", "response": "def _find_binary(binary=None):\n    \"\"\"Find the absolute path to the GnuPG binary.\n\n    Also run checks that the binary is not a symlink, and check that\n    our process real uid has exec permissions.\n\n    :param str binary: The path to the GnuPG binary.\n    :raises: :exc:`~exceptions.RuntimeError` if it appears that GnuPG is not\n             installed.\n    :rtype: str\n    :returns: The absolute path to the GnuPG binary to use, if no exceptions\n              occur.\n    \"\"\"\n    found = None\n    if binary is not None:\n        if os.path.isabs(binary) and os.path.isfile(binary):\n            return binary\n        if not os.path.isabs(binary):\n            try:\n                found = _which(binary)\n                log.debug(\"Found potential binary paths: %s\"\n                          % '\\n'.join([path for path in found]))\n                found = found[0]\n            except IndexError as ie:\n                log.info(\"Could not determine absolute path of binary: '%s'\"\n                          % binary)\n        elif os.access(binary, os.X_OK):\n            found = binary\n    if found is None:\n        try: found = _which('gpg', abspath_only=True, disallow_symlinks=True)[0]\n        except IndexError as ie:\n            log.error(\"Could not find binary for 'gpg'.\")\n            try: found = _which('gpg2')[0]\n            except IndexError as ie:\n                log.error(\"Could not find binary for 'gpg2'.\")\n    if found is None:\n        raise RuntimeError(\"GnuPG is not installed!\")\n\n    return found"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the size of the file is greater than zero.", "response": "def _is_file(filename):\n    \"\"\"Check that the size of the thing which is supposed to be a filename has\n    size greater than zero, without following symbolic links or using\n    :func:os.path.isfile.\n\n    :param filename: An object to check.\n    :rtype: bool\n    :returns: True if **filename** is file-like, False otherwise.\n    \"\"\"\n    try:\n        statinfo = os.lstat(filename)\n        log.debug(\"lstat(%r) with type=%s gave us %r\"\n                  % (repr(filename), type(filename), repr(statinfo)))\n        if not (statinfo.st_size > 0):\n            raise ValueError(\"'%s' appears to be an empty file!\" % filename)\n    except OSError as oserr:\n        log.error(oserr)\n        if filename == '-':\n            log.debug(\"Got '-' for filename, assuming sys.stdin...\")\n            return True\n    except (ValueError, TypeError, IOError) as err:\n        log.error(err)\n    else:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_string(thing):\n    if (_py3k and isinstance(thing, str)):\n        return True\n    if (not _py3k and isinstance(thing, basestring)):\n        return True\n    return False", "response": "Check that **thing** is a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_gpg1(version):\n    (major, minor, micro) = _match_version_string(version)\n    if major == 1:\n        return True\n    return False", "response": "Returns True if using GnuPG version 1. x."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_gpg2(version):\n    (major, minor, micro) = _match_version_string(version)\n    if major == 2:\n        return True\n    return False", "response": "Returns True if using GnuPG version 2. x."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_binary_stream(thing, encoding=None, armor=True):\n    if _py3k:\n        if isinstance(thing, str):\n            thing = thing.encode(encoding)\n    else:\n        if type(thing) is not str:\n            thing = thing.encode(encoding)\n\n    try:\n        rv = BytesIO(thing)\n    except NameError:\n        rv = StringIO(thing)\n\n    return rv", "response": "Encode the thing and turn it into a stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a passphrase and write it to a file.", "response": "def _make_passphrase(length=None, save=False, file=None):\n    \"\"\"Create a passphrase and write it to a file that only the user can read.\n\n    This is not very secure, and should not be relied upon for actual key\n    passphrases.\n\n    :param int length: The length in bytes of the string to generate.\n\n    :param file file: The file to save the generated passphrase in. If not\n        given, defaults to 'passphrase-<the real user id>-<seconds since\n        epoch>' in the top-level directory.\n    \"\"\"\n    if not length:\n        length = 40\n\n    passphrase = _make_random_string(length)\n\n    if save:\n        ruid, euid, suid = os.getresuid()\n        gid = os.getgid()\n        now = mktime(localtime())\n\n        if not file:\n            filename = str('passphrase-%s-%s' % uid, now)\n            file = os.path.join(_repo, filename)\n\n        with open(file, 'a') as fh:\n            fh.write(passphrase)\n            fh.flush()\n            fh.close()\n            os.chmod(file, stat.S_IRUSR | stat.S_IWUSR)\n            os.chown(file, ruid, gid)\n\n        log.warn(\"Generated passphrase saved to %s\" % file)\n    return passphrase"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_random_string(length):\n    chars = string.ascii_lowercase + string.ascii_uppercase + string.digits\n    return ''.join(random.choice(chars) for x in range(length))", "response": "Returns a random lowercase uppercase alphanumerical string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmatch a binary version string into major minor and micro integers.", "response": "def _match_version_string(version):\n    \"\"\"Sort a binary version string into major, minor, and micro integers.\n\n    :param str version: A version string in the form x.x.x\n    :raises GnuPGVersionError: if the **version** string couldn't be parsed.\n    :rtype: tuple\n\n    :returns: A 3-tuple of integers, representing the (MAJOR, MINOR, MICRO)\n        version numbers. For example::\n\n            _match_version_string(\"2.1.3\")\n\n        would return ``(2, 1, 3)``.\n    \"\"\"\n    matched = _VERSION_STRING_REGEX.match(version)\n    g = matched.groups()\n    major, minor, micro = g[0], g[2], g[4]\n\n    # If, for whatever reason, the binary didn't tell us its version, then\n    # these might be (None, None, None), and so we should avoid typecasting\n    # them when that is the case.\n    if major and minor and micro:\n        major, minor, micro = int(major), int(minor), int(micro)\n    else:\n        raise GnuPGVersionError(\"Could not parse GnuPG version from: %r\" %\n                                version)\n\n    return (major, minor, micro)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _next_year():\n    now = datetime.now().__str__()\n    date = now.split(' ', 1)[0]\n    year, month, day = date.split('-', 2)\n    next_year = str(int(year)+1)\n    return '-'.join((next_year, month, day))", "response": "Returns the date of this day next one year."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsplit the line and return the first word and the rest.", "response": "def _separate_keyword(line):\n    \"\"\"Split the line, and return (first_word, the_rest).\"\"\"\n    try:\n        first, rest = line.split(None, 1)\n    except ValueError:\n        first = line.strip()\n        rest = ''\n    return first, rest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _threaded_copy_data(instream, outstream):\n    copy_thread = threading.Thread(target=_copy_data,\n                                   args=(instream, outstream))\n    copy_thread.setDaemon(True)\n    log.debug('%r, %r, %r', copy_thread, instream, outstream)\n    copy_thread.start()\n    return copy_thread", "response": "A thread that copies data from one stream to another in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching the PATH for the given executable file.", "response": "def _which(executable, flags=os.X_OK, abspath_only=False, disallow_symlinks=False):\n    \"\"\"Borrowed from Twisted's :mod:twisted.python.proutils .\n\n    Search PATH for executable files with the given name.\n\n    On newer versions of MS-Windows, the PATHEXT environment variable will be\n    set to the list of file extensions for files considered executable. This\n    will normally include things like \".EXE\". This fuction will also find files\n    with the given name ending with any of these extensions.\n\n    On MS-Windows the only flag that has any meaning is os.F_OK. Any other\n    flags will be ignored.\n\n    Note: This function does not help us prevent an attacker who can already\n    manipulate the environment's PATH settings from placing malicious code\n    higher in the PATH. It also does happily follows links.\n\n    :param str name: The name for which to search.\n    :param int flags: Arguments to L{os.access}.\n    :rtype: list\n    :returns: A list of the full paths to files found, in the order in which\n              they were found.\n    \"\"\"\n    def _can_allow(p):\n        if not os.access(p, flags):\n            return False\n        if abspath_only and not os.path.abspath(p):\n            log.warn('Ignoring %r (path is not absolute)', p)\n            return False\n        if disallow_symlinks and os.path.islink(p):\n            log.warn('Ignoring %r (path is a symlink)', p)\n            return False\n        return True\n\n    result = []\n    exts = filter(None, os.environ.get('PATHEXT', '').split(os.pathsep))\n    path = os.environ.get('PATH', None)\n    if path is None:\n        return []\n    for p in os.environ.get('PATH', '').split(os.pathsep):\n        p = os.path.join(p, executable)\n        if _can_allow(p):\n            result.append(p)\n        for e in exts:\n            pext = p + e\n            if _can_allow(pext):\n                result.append(pext)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the passphrase from memory to the GnuPG process stdin.", "response": "def _write_passphrase(stream, passphrase, encoding):\n    \"\"\"Write the passphrase from memory to the GnuPG process' stdin.\n\n    :type stream: file, :class:`~io.BytesIO`, or :class:`~io.StringIO`\n    :param stream: The input file descriptor to write the password to.\n    :param str passphrase: The passphrase for the secret key material.\n    :param str encoding: The data encoding expected by GnuPG. Usually, this\n                         is ``sys.getfilesystemencoding()``.\n    \"\"\"\n    passphrase = '%s\\n' % passphrase\n    passphrase = passphrase.encode(encoding)\n    stream.write(passphrase)\n    log.debug(\"Wrote passphrase on stdin.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsigning a message with a GnuPG key.", "response": "def sign(self, data, **kwargs):\n        \"\"\"Create a signature for a message string or file.\n\n        Note that this method is not for signing other keys. (In GnuPG's\n        terms, what we all usually call 'keysigning' is actually termed\n        'certification'...) Even though they are cryptographically the same\n        operation, GnuPG differentiates between them, presumedly because these\n        operations are also the same as the decryption operation. If the\n        ``key_usage``s ``C (certification)``, ``S (sign)``, and ``E\n        (encrypt)``, were all the same key, the key would \"wear down\" through\n        frequent signing usage -- since signing data is usually done often --\n        meaning that the secret portion of the keypair, also used for\n        decryption in this scenario, would have a statistically higher\n        probability of an adversary obtaining an oracle for it (or for a\n        portion of the rounds in the cipher algorithm, depending on the family\n        of cryptanalytic attack used).\n\n        In simpler terms: this function isn't for signing your friends' keys,\n        it's for something like signing an email.\n\n        :type data: :obj:`str` or :obj:`file`\n        :param data: A string or file stream to sign.\n        :param str default_key: The key to sign with.\n        :param str passphrase: The passphrase to pipe to stdin.\n        :param bool clearsign: If True, create a cleartext signature.\n        :param bool detach: If True, create a detached signature.\n        :param bool binary: If True, do not ascii armour the output.\n        :param str digest_algo: The hash digest to use. Again, to see which\n            hashes your GnuPG is capable of using, do:\n            :command:`$ gpg --with-colons --list-config digestname`.\n            The default, if unspecified, is ``'SHA512'``.\n        \"\"\"\n        if 'default_key' in kwargs:\n            log.info(\"Signing message '%r' with keyid: %s\"\n                     % (data, kwargs['default_key']))\n        else:\n            log.warn(\"No 'default_key' given! Using first key on secring.\")\n\n        if hasattr(data, 'read'):\n            result = self._sign_file(data, **kwargs)\n        elif not _is_stream(data):\n            stream = _make_binary_stream(data, self._encoding)\n            result = self._sign_file(stream, **kwargs)\n            stream.close()\n        else:\n            log.warn(\"Unable to sign message '%s' with type %s\"\n                     % (data, type(data)))\n            result = None\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify the signature on the contents of the string data.", "response": "def verify(self, data):\n        \"\"\"Verify the signature on the contents of the string ``data``.\n\n        >>> gpg = GPG(homedir=\"doctests\")\n        >>> input = gpg.gen_key_input(Passphrase='foo')\n        >>> key = gpg.gen_key(input)\n        >>> assert key\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='bar')\n        >>> assert not sig\n        >>> sig = gpg.sign('hello',keyid=key.fingerprint,passphrase='foo')\n        >>> assert sig\n        >>> verify = gpg.verify(sig.data)\n        >>> assert verify\n\n        \"\"\"\n        f = _make_binary_stream(data, self._encoding)\n        result = self.verify_file(f)\n        f.close()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify the signature on the contents of a file or file - like object.", "response": "def verify_file(self, file, sig_file=None):\n        \"\"\"Verify the signature on the contents of a file or file-like\n        object. Can handle embedded signatures as well as detached\n        signatures. If using detached signatures, the file containing the\n        detached signature should be specified as the ``sig_file``.\n\n        :param file file: A file descriptor object.\n\n        :param str sig_file: A file containing the GPG signature data for\n            ``file``. If given, ``file`` is verified via this detached\n            signature. Its type will be checked with :func:`_util._is_file`.\n        \"\"\"\n\n        result = self._result_map['verify'](self)\n\n        if sig_file is None:\n            log.debug(\"verify_file(): Handling embedded signature\")\n            args = [\"--verify\"]\n            proc = self._open_subprocess(args)\n            writer = _util._threaded_copy_data(file, proc.stdin)\n            self._collect_output(proc, result, writer, stdin=proc.stdin)\n        else:\n            if not _util._is_file(sig_file):\n                log.debug(\"verify_file(): '%r' is not a file\" % sig_file)\n                return result\n            log.debug('verify_file(): Handling detached verification')\n            sig_fh = None\n            try:\n                sig_fh = open(sig_file, 'rb')\n                args = [\"--verify %s -\" % sig_fh.name]\n                proc = self._open_subprocess(args)\n                writer = _util._threaded_copy_data(file, proc.stdin)\n                self._collect_output(proc, result, writer, stdin=proc.stdin)\n            finally:\n                if sig_fh and not sig_fh.closed:\n                    sig_fh.close()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports the keys into our keyring.", "response": "def import_keys(self, key_data):\n        \"\"\"\n        Import the key_data into our keyring.\n\n        >>> import shutil\n        >>> shutil.rmtree(\"doctests\")\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> inpt = gpg.gen_key_input()\n        >>> key1 = gpg.gen_key(inpt)\n        >>> print1 = str(key1.fingerprint)\n        >>> pubkey1 = gpg.export_keys(print1)\n        >>> seckey1 = gpg.export_keys(print1,secret=True)\n        >>> key2 = gpg.gen_key(inpt)\n        >>> print2 = key2.fingerprint\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> pubkeys = gpg.list_keys()\n        >>> assert print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> str(gpg.delete_keys(print1))\n        'Must delete secret key first'\n        >>> str(gpg.delete_keys(print1,secret=True))\n        'ok'\n        >>> str(gpg.delete_keys(print1))\n        'ok'\n        >>> pubkeys = gpg.list_keys()\n        >>> assert not print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(pubkey1)\n        >>> pubkeys = gpg.list_keys()\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert not print1 in seckeys.fingerprints\n        >>> assert print1 in pubkeys.fingerprints\n        >>> result = gpg.import_keys(seckey1)\n        >>> assert result\n        >>> seckeys = gpg.list_keys(secret=True)\n        >>> assert print1 in seckeys.fingerprints\n        \"\"\"\n        ## xxx need way to validate that key_data is actually a valid GPG key\n        ##     it might be possible to use --list-packets and parse the output\n\n        result = self._result_map['import'](self)\n        log.info('Importing: %r', key_data[:256])\n        data = _make_binary_stream(key_data, self._encoding)\n        self._handle_io(['--import'], data, result, binary=True)\n        data.close()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports keys from a keyserver.", "response": "def recv_keys(self, *keyids, **kwargs):\n        \"\"\"Import keys from a keyserver.\n\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key = gpg.recv_keys('3FF0DB166A7476EA', keyserver='hkp://pgp.mit.edu')\n        >>> assert key\n\n        :param str keyids: Each ``keyids`` argument should be a string\n             containing a keyid to request.\n        :param str keyserver: The keyserver to request the ``keyids`` from;\n             defaults to `gnupg.GPG.keyserver`.\n        \"\"\"\n        if keyids:\n            keys = ' '.join([key for key in keyids])\n            return self._recv_keys(keys, **kwargs)\n        else:\n            log.error(\"No keyids requested for --recv-keys!\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_keys(self, fingerprints, secret=False, subkeys=False):\n        which = 'keys'\n        if secret:\n            which = 'secret-keys'\n        if subkeys:\n            which = 'secret-and-public-keys'\n\n        if _is_list_or_tuple(fingerprints):\n            fingerprints = ' '.join(fingerprints)\n\n        args = ['--batch']\n        args.append(\"--delete-{0} {1}\".format(which, fingerprints))\n\n        result = self._result_map['delete'](self)\n        p = self._open_subprocess(args)\n        self._collect_output(p, result, stdin=p.stdin)\n        return result", "response": "Delete a key or a list of keys from the current keyring."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports the specified keyids.", "response": "def export_keys(self, keyids, secret=False, subkeys=False):\n        \"\"\"Export the indicated ``keyids``.\n\n        :param str keyids: A keyid or fingerprint in any format that GnuPG will\n            accept.\n        :param bool secret: If True, export only the secret key.\n        :param bool subkeys: If True, export the secret subkeys.\n        \"\"\"\n        which = ''\n        if subkeys:\n            which = '-secret-subkeys'\n        elif secret:\n            which = '-secret-keys'\n\n        if _is_list_or_tuple(keyids):\n            keyids = ' '.join(['%s' % k for k in keyids])\n\n        args = [\"--armor\"]\n        args.append(\"--export{0} {1}\".format(which, keyids))\n\n        p = self._open_subprocess(args)\n        ## gpg --export produces no status-fd output; stdout will be empty in\n        ## case of failure\n        #stdout, stderr = p.communicate()\n        result = self._result_map['export'](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        log.debug('Exported:%s%r' % (os.linesep, result.fingerprints))\n        return result.data.decode(self._encoding, self._decode_errors)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_keys(self, secret=False):\n        which = 'public-keys'\n        if secret:\n            which = 'secret-keys'\n\n        args = []\n        args.append(\"--fixed-list-mode\")\n        args.append(\"--fingerprint\")\n        args.append(\"--with-colons\")\n        args.append(\"--list-options no-show-photos\")\n        args.append(\"--list-%s\" % (which))\n\n        p = self._open_subprocess(args)\n\n        # there might be some status thingumy here I should handle... (amk)\n        # ...nope, unless you care about expired sigs or keys (stevegt)\n\n        # Get the response information\n        result = self._result_map['list'](self)\n        self._collect_output(p, result, stdin=p.stdin)\n        lines = result.data.decode(self._encoding,\n                                   self._decode_errors).splitlines()\n        self._parse_keys(result)\n        return result", "response": "List the keys currently in the keyring."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_packets(self, raw_data):\n        args = [\"--list-packets\"]\n        result = self._result_map['packets'](self)\n        self._handle_io(args, _make_binary_stream(raw_data, self._encoding),\n                        result)\n        return result", "response": "List the packet contents of a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sign_key(self, keyid, default_key=None, passphrase=None):\n\n        args = []\n        input_command = \"\"\n        if passphrase:\n            passphrase_arg = \"--passphrase-fd 0\"\n            input_command = \"%s\\n\" % passphrase\n            args.append(passphrase_arg)\n\n        if default_key:\n            args.append(str(\"--default-key %s\" % default_key))\n\n        args.extend([\"--command-fd 0\", \"--sign-key %s\" % keyid])\n\n        p = self._open_subprocess(args)\n        result = self._result_map['signing'](self)\n        confirm_command = \"%sy\\n\" % input_command\n        p.stdin.write(b(confirm_command))\n        self._collect_output(p, result, stdin=p.stdin)\n        return result", "response": "sign a public key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the expiration time of a GnuPG key.", "response": "def expire(self, keyid, expiration_time='1y', passphrase=None, expire_subkeys=True):\n        \"\"\"Changes GnuPG key expiration by passing in new time period (from now) through\n            subprocess's stdin\n\n        >>> import gnupg\n        >>> gpg = gnupg.GPG(homedir=\"doctests\")\n        >>> key_input = gpg.gen_key_input()\n        >>> key = gpg.gen_key(key_input)\n        >>> gpg.expire(key.fingerprint, '2w', 'good passphrase')\n\n        :param str keyid: key shortID, longID, email_address or fingerprint\n        :param str expiration_time: 0 or number of days (d), or weeks (*w) , or months (*m)\n                or years (*y) for when to expire the key, from today.\n        :param str passphrase: passphrase used when creating the key, leave None otherwise\n        :param bool expire_subkeys: to indicate whether the subkeys will also change the\n                expiration time by the same period -- default is True\n\n        :returns: The result giving status of the change in expiration...\n                the new expiration date can be obtained by .list_keys()\n        \"\"\"\n\n        passphrase = passphrase.encode(self._encoding) if passphrase else passphrase\n\n        try:\n            sub_keys_number = len(self.list_sigs(keyid)[0]['subkeys']) if expire_subkeys else 0\n        except IndexError:\n            sub_keys_number = 0\n\n        expiration_input = KeyExpirationInterface(expiration_time, passphrase).gpg_interactive_input(sub_keys_number)\n\n        args = [\"--command-fd 0\", \"--edit-key %s\" % keyid]\n        p = self._open_subprocess(args)\n        p.stdin.write(b(expiration_input))\n\n        result = self._result_map['expire'](self)\n        p.stdin.write(b(expiration_input))\n\n        self._collect_output(p, result, stdin=p.stdin)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_key(self, input):\n        args = [\"--gen-key --cert-digest-algo SHA512 --batch\"]\n        key = self._result_map['generate'](self)\n        f = _make_binary_stream(input, self._encoding)\n        self._handle_io(args, f, key, binary=True)\n        f.close()\n\n        fpr = str(key.fingerprint)\n        if len(fpr) == 20:\n            for d in map(lambda x: os.path.dirname(x),\n                         [self.temp_keyring, self.temp_secring]):\n                if not os.path.exists(d):\n                    os.makedirs(d)\n\n            if self.temp_keyring:\n                if os.path.isfile(self.temp_keyring):\n                    prefix = os.path.join(self.temp_keyring, fpr)\n                    try: os.rename(self.temp_keyring, prefix+\".pubring\")\n                    except OSError as ose: log.error(str(ose))\n\n            if self.temp_secring:\n                if os.path.isfile(self.temp_secring):\n                    prefix = os.path.join(self.temp_secring, fpr)\n                    try: os.rename(self.temp_secring, prefix+\".secring\")\n                    except OSError as ose: log.error(str(ose))\n\n        log.info(\"Key created. Fingerprint: %s\" % fpr)\n        key.keyring = self.temp_keyring\n        key.secring = self.temp_secring\n        self.temp_keyring = None\n        self.temp_secring = None\n\n        return key", "response": "Generate a GnuPG key through batch file key generation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_key_input(self, separate_keyring=False, save_batchfile=False,\n                      testing=False, **kwargs):\n        \"\"\"Generate a batch file for input to :meth:`~gnupg.GPG.gen_key`.\n\n        The GnuPG batch file key generation feature allows unattended key\n        generation by creating a file with special syntax and then providing it\n        to: :command:`gpg --gen-key --batch`. Batch files look like this:\n\n        |  Name-Real: Alice\n        |  Name-Email: alice@inter.net\n        |  Expire-Date: 2014-04-01\n        |  Key-Type: RSA\n        |  Key-Length: 4096\n        |  Key-Usage: cert\n        |  Subkey-Type: RSA\n        |  Subkey-Length: 4096\n        |  Subkey-Usage: encrypt,sign,auth\n        |  Passphrase: sekrit\n        |  %pubring foo.gpg\n        |  %secring sec.gpg\n        |  %commit\n\n        which is what this function creates for you. All of the available,\n        non-control parameters are detailed below (control parameters are the\n        ones which begin with a '%'). For example, to generate the batch file\n        example above, use like this:\n\n        >>> import gnupg\n        GnuPG logging disabled...\n        >>> from __future__ import print_function\n        >>> gpg = gnupg.GPG(homedir='doctests')\n        >>> alice = { 'name_real': 'Alice',\n        ...     'name_email': 'alice@inter.net',\n        ...     'expire_date': '2014-04-01',\n        ...     'key_type': 'RSA',\n        ...     'key_length': 4096,\n        ...     'key_usage': '',\n        ...     'subkey_type': 'RSA',\n        ...     'subkey_length': 4096,\n        ...     'subkey_usage': 'encrypt,sign,auth',\n        ...     'passphrase': 'sekrit'}\n        >>> alice_input = gpg.gen_key_input(**alice)\n        >>> print(alice_input)\n        Key-Type: RSA\n        Subkey-Type: RSA\n        Subkey-Usage: encrypt,sign,auth\n        Expire-Date: 2014-04-01\n        Passphrase: sekrit\n        Name-Real: Alice\n        Name-Email: alice@inter.net\n        Key-Length: 4096\n        Subkey-Length: 4096\n        %pubring ./doctests/alice.pubring.gpg\n        %secring ./doctests/alice.secring.gpg\n        %commit\n        <BLANKLINE>\n        >>> alice_key = gpg.gen_key(alice_input)\n        >>> assert alice_key is not None\n        >>> assert alice_key.fingerprint is not None\n        >>> message = \"no one else can read my sekrit message\"\n        >>> encrypted = gpg.encrypt(message, alice_key.fingerprint)\n        >>> assert isinstance(encrypted.data, str)\n\n        :param bool separate_keyring: Specify for the new key to be written to\n            a separate pubring.gpg and secring.gpg. If True,\n            :meth:`~gnupg.GPG.gen_key` will automatically rename the separate\n            keyring and secring to whatever the fingerprint of the generated\n            key ends up being, suffixed with '.pubring' and '.secring'\n            respectively.\n\n        :param bool save_batchfile: Save a copy of the generated batch file to\n            disk in a file named <name_real>.batch, where <name_real> is the\n            ``name_real`` parameter stripped of punctuation, spaces, and\n            non-ascii characters.\n\n        :param bool testing: Uses a faster, albeit insecure random number\n            generator to create keys. This should only be used for testing\n            purposes, for keys which are going to be created and then soon\n            after destroyed, and never for the generation of actual use keys.\n\n        :param str name_real: The name field of the UID in the generated key.\n        :param str name_comment: The comment in the UID of the generated key.\n\n        :param str name_email: The email in the UID of the generated key.\n            (default: ``$USER`` @ :command:`hostname` ) Remember to use UTF-8\n            encoding for the entirety of the UID. At least one of\n            ``name_real``, ``name_comment``, or ``name_email`` must be\n            provided, or else no user ID is created.\n\n        :param str key_type: One of 'RSA', 'DSA', 'ELG-E', or 'default'.\n            (default: 'RSA', if using GnuPG v1.x, otherwise 'default') Starts\n            a new parameter block by giving the type of the primary key. The\n            algorithm must be capable of signing. This is a required\n            parameter. The algorithm may either be an OpenPGP algorithm number\n            or a string with the algorithm name. The special value \u2018default\u2019\n            may be used for algo to create the default key type; in this case\n            a ``key_usage`` should not be given and 'default' must also be\n            used for ``subkey_type``.\n\n        :param int key_length: The requested length of the generated key in\n            bits. (Default: 4096)\n\n        :param str key_grip: hexstring This is an optional hexidecimal string\n            which is used to generate a CSR or certificate for an already\n            existing key. ``key_length`` will be ignored if this parameter\n            is given.\n\n        :param str key_usage: Space or comma delimited string of key\n            usages. Allowed values are \u2018encrypt\u2019, \u2018sign\u2019, and \u2018auth\u2019. This is\n            used to generate the key flags. Please make sure that the\n            algorithm is capable of this usage. Note that OpenPGP requires\n            that all primary keys are capable of certification, so no matter\n            what usage is given here, the \u2018cert\u2019 flag will be on. If no\n            \u2018Key-Usage\u2019 is specified and the \u2018Key-Type\u2019 is not \u2018default\u2019, all\n            allowed usages for that particular algorithm are used; if it is\n            not given but \u2018default\u2019 is used the usage will be \u2018sign\u2019.\n\n        :param str subkey_type: This generates a secondary key\n            (subkey). Currently only one subkey can be handled. See also\n            ``key_type`` above.\n\n        :param int subkey_length: The length of the secondary subkey in bits.\n\n        :param str subkey_usage: Key usage for a subkey; similar to\n            ``key_usage``.\n\n        :type expire_date: :obj:`int` or :obj:`str`\n        :param expire_date: Can be specified as an iso-date or as\n            <int>[d|w|m|y] Set the expiration date for the key (and the\n            subkey). It may either be entered in ISO date format (2000-08-15)\n            or as number of days, weeks, month or years. The special notation\n            \"seconds=N\" is also allowed to directly give an Epoch\n            value. Without a letter days are assumed. Note that there is no\n            check done on the overflow of the type used by OpenPGP for\n            timestamps. Thus you better make sure that the given value make\n            sense. Although OpenPGP works with time intervals, GnuPG uses an\n            absolute value internally and thus the last year we can represent\n            is 2105.\n\n        :param str creation_date: Set the creation date of the key as stored\n            in the key information and which is also part of the fingerprint\n            calculation. Either a date like \"1986-04-26\" or a full timestamp\n            like \"19860426T042640\" may be used. The time is considered to be\n            UTC. If it is not given the current time is used.\n\n        :param str passphrase: The passphrase for the new key. The default is\n            to not use any passphrase. Note that GnuPG>=2.1.x will not allow\n            you to specify a passphrase for batch key generation -- GnuPG will\n            ignore the **passphrase** parameter, stop, and ask the user for\n            the new passphrase.  However, we can put the command\n            ``%no-protection`` into the batch key generation file to allow a\n            passwordless key to be created, which can then have its passphrase\n            set later with ``--edit-key``.\n\n        :param str preferences: Set the cipher, hash, and compression\n            preference values for this key. This expects the same type of\n            string as the sub-command \u2018setpref\u2019 in the --edit-key menu.\n\n        :param str revoker: Should be given as 'algo:fpr' (case sensitive).\n            Add a designated revoker to the generated key. Algo is the public\n            key algorithm of the designated revoker (i.e. RSA=1, DSA=17, etc.)\n            fpr is the fingerprint of the designated revoker. The optional\n            \u2018sensitive\u2019 flag marks the designated revoker as sensitive\n            information. Only v4 keys may be designated revokers.\n\n        :param str keyserver: This is an optional parameter that specifies the\n            preferred keyserver URL for the key.\n\n        :param str handle: This is an optional parameter only used with the\n            status lines ``KEY_CREATED`` and ``KEY_NOT_CREATED``. string may\n            be up to 100 characters and should not contain spaces. It is\n            useful for batch key generation to associate a key parameter block\n            with a status line.\n\n        :rtype: str\n        :returns: A suitable input string for the :meth:`GPG.gen_key` method,\n            the latter of which will create the new keypair.\n\n        See `this GnuPG Manual section`__ for more details.\n\n        __ http://www.gnupg.org/documentation/manuals/gnupg-devel/Unattended-GPG-key-generation.html\n        \"\"\"\n        #: A boolean for determining whether to set subkey_type to 'default'\n        default_type = False\n\n        parms = {}\n\n        ## if using GnuPG version 1.x, then set the default 'Key-Type' to\n        ## 'RSA' because it doesn't understand 'default'\n        parms.setdefault('Key-Type', 'default')\n        if _util._is_gpg1(self.binary_version):\n            parms.setdefault('Key-Type', 'RSA')\n        log.debug(\"GnuPG v%s detected: setting default key type to %s.\"\n                  % (self.binary_version, parms['Key-Type']))\n        parms.setdefault('Key-Length', 4096)\n        parms.setdefault('Name-Real', \"Autogenerated Key\")\n        parms.setdefault('Expire-Date', _util._next_year())\n\n        name_email = kwargs.get('name_email')\n        uidemail = _util.create_uid_email(name_email)\n        parms.setdefault('Name-Email', uidemail)\n\n        if testing:\n            ## This specific comment string is required by (some? all?)\n            ## versions of GnuPG to use the insecure PRNG:\n            parms.setdefault('Name-Comment', 'insecure!')\n\n        for key, val in list(kwargs.items()):\n            key = key.replace('_','-').title()\n            ## to set 'cert', 'Key-Usage' must be blank string\n            if not key in ('Key-Usage', 'Subkey-Usage'):\n                if type('')(val).strip():\n                    parms[key] = val\n\n        ## if Key-Type is 'default', make Subkey-Type also be 'default'\n        if parms['Key-Type'] == 'default':\n            default_type = True\n            for field in ('Key-Usage', 'Subkey-Usage',):\n                try: parms.pop(field)  ## toss these out, handle manually\n                except KeyError: pass\n\n        ## Key-Type must come first, followed by length\n        out  = \"Key-Type: %s\\n\" % parms.pop('Key-Type')\n        out += \"Key-Length: %d\\n\" % parms.pop('Key-Length')\n        if 'Subkey-Type' in parms.keys():\n            out += \"Subkey-Type: %s\\n\" % parms.pop('Subkey-Type')\n        else:\n            if default_type:\n                out += \"Subkey-Type: default\\n\"\n        if 'Subkey-Length' in parms.keys():\n            out += \"Subkey-Length: %s\\n\" % parms.pop('Subkey-Length')\n\n        for key, val in list(parms.items()):\n            out += \"%s: %s\\n\" % (key, val)\n\n        ## There is a problem where, in the batch files, if the '%%pubring'\n        ## and '%%secring' are given as any static string, i.e. 'pubring.gpg',\n        ## that file will always get rewritten without confirmation, killing\n        ## off any keys we had before. So in the case where we wish to\n        ## generate a bunch of keys and then do stuff with them, we should not\n        ## give 'pubring.gpg' as our keyring file, otherwise we will lose any\n        ## keys we had previously.\n\n        if separate_keyring:\n            ring = str(uidemail + '_' + str(_util._utc_epoch()))\n            self.temp_keyring = os.path.join(self.homedir, ring+'.pubring')\n            self.temp_secring = os.path.join(self.homedir, ring+'.secring')\n            out += \"%%pubring %s\\n\" % self.temp_keyring\n            out += \"%%secring %s\\n\" % self.temp_secring\n\n        if testing:\n            ## see TODO file, tag :compatibility:gen_key_input:\n            ##\n            ## Add version detection before the '%no-protection' flag.\n            out += \"%no-protection\\n\"\n            out += \"%transient-key\\n\"\n\n        out += \"%commit\\n\"\n\n        ## if we've been asked to save a copy of the batch file:\n        if save_batchfile and parms['Name-Email'] != uidemail:\n            asc_uid  = encodings.normalize_encoding(parms['Name-Email'])\n            filename = _fix_unsafe(asc_uid) + _util._now() + '.batch'\n            save_as  = os.path.join(self._batch_dir, filename)\n            readme = os.path.join(self._batch_dir, 'README')\n\n            if not os.path.exists(self._batch_dir):\n                os.makedirs(self._batch_dir)\n\n                ## the following pulls the link to GnuPG's online batchfile\n                ## documentation from this function's docstring and sticks it\n                ## in a README file in the batch directory:\n\n                if getattr(self.gen_key_input, '__doc__', None) is not None:\n                    docs = self.gen_key_input.__doc__\n                else:\n                    docs = str() ## docstring=None if run with \"python -OO\"\n                links = '\\n'.join(x.strip() for x in docs.splitlines()[-2:])\n                explain = \"\"\"\nThis directory was created by python-gnupg, on {}, and\nit contains saved batch files, which can be given to GnuPG to automatically\ngenerate keys. Please see\n{}\"\"\".format(_util.now(), links) ## sometimes python is awesome.\n\n                with open(readme, 'a+') as fh:\n                    [fh.write(line) for line in explain]\n\n            with open(save_as, 'a+') as batch_file:\n                [batch_file.write(line) for line in out]\n\n        return out", "response": "Generate a batch file for input to a GnuPG key store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encrypt(self, data, *recipients, **kwargs):\n        if _is_stream(data):\n            stream = data\n        else:\n            stream = _make_binary_stream(data, self._encoding)\n        result = self._encrypt(stream, recipients, **kwargs)\n        stream.close()\n        return result", "response": "Encrypt the message contained in data to the given recipients."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrypt(self, message, **kwargs):\n        stream = _make_binary_stream(message, self._encoding)\n        result = self.decrypt_file(stream, **kwargs)\n        stream.close()\n        return result", "response": "Decrypt the contents of a string or file - like object message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decrypt_file(self, filename, always_trust=False, passphrase=None,\n                     output=None):\n        \"\"\"Decrypt the contents of a file-like object ``filename`` .\n\n        :param str filename: A file-like object to decrypt.\n        :param bool always_trust: Instruct GnuPG to ignore trust checks.\n        :param str passphrase: The passphrase for the secret key used for decryption.\n        :param str output: A filename to write the decrypted output to.\n        \"\"\"\n        args = [\"--decrypt\"]\n        if output:  # write the output to a file with the specified name\n            if os.path.exists(output):\n                os.remove(output) # to avoid overwrite confirmation message\n            args.append('--output %s' % output)\n        if always_trust:\n            args.append(\"--always-trust\")\n        result = self._result_map['crypt'](self)\n        self._handle_io(args, filename, result, passphrase, binary=True)\n        log.debug('decrypt result: %r', result.data)\n        return result", "response": "Decrypt the contents of a file - like object filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_key_by_email(self, email, secret=False):\n        for key in self.list_keys(secret=secret):\n            for uid in key['uids']:\n                if re.search(email, uid):\n                    return key\n        raise LookupError(\"GnuPG public key for email %s not found!\" % email)", "response": "Find user s public key based on their email address."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a key by a fingerprint of one of its subkeys.", "response": "def find_key_by_subkey(self, subkey):\n        \"\"\"Find a key by a fingerprint of one of its subkeys.\n\n        :param str subkey: The fingerprint of the subkey to search for.\n        \"\"\"\n        for key in self.list_keys():\n            for sub in key['subkeys']:\n                if sub[0] == subkey:\n                    return key\n        raise LookupError(\n            \"GnuPG public key for subkey %s not found!\" % subkey)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending keys to a keyserver.", "response": "def send_keys(self, keyserver, *keyids):\n        \"\"\"Send keys to a keyserver.\"\"\"\n        result = self._result_map['list'](self)\n        log.debug('send_keys: %r', keyids)\n        data = _util._make_binary_stream(\"\", self._encoding)\n        args = ['--keyserver', keyserver, '--send-keys']\n        args.extend(keyids)\n        self._handle_io(args, data, result, binary=True)\n        log.debug('send_keys result: %r', result.__dict__)\n        data.close()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the key to which raw_data is encrypted to.", "response": "def encrypted_to(self, raw_data):\n        \"\"\"Return the key to which raw_data is encrypted to.\"\"\"\n        # TODO: make this support multiple keys.\n        result = self._gpg.list_packets(raw_data)\n        if not result.key:\n            raise LookupError(\n                \"Content is not encrypted to a GnuPG key!\")\n        try:\n            return self.find_key_by_keyid(result.key)\n        except:\n            return self.find_key_by_subkey(result.key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_all_template(im_source, im_search, threshold=0.5, maxcnt=0, rgb=False, bgremove=False):\n    '''\n    Locate image position with cv2.templateFind\n\n    Use pixel match to find pictures.\n\n    Args:\n        im_source(string): \u56fe\u50cf\u3001\u7d20\u6750\n        im_search(string): \u9700\u8981\u67e5\u627e\u7684\u56fe\u7247\n        threshold: \u9608\u503c\uff0c\u5f53\u76f8\u8bc6\u5ea6\u5c0f\u4e8e\u8be5\u9608\u503c\u7684\u65f6\u5019\uff0c\u5c31\u5ffd\u7565\u6389\n\n    Returns:\n        A tuple of found [(point, score), ...]\n\n    Raises:\n        IOError: when file read error\n    '''\n    # method = cv2.TM_CCORR_NORMED\n    # method = cv2.TM_SQDIFF_NORMED\n    method = cv2.TM_CCOEFF_NORMED\n\n    if rgb:\n        s_bgr = cv2.split(im_search) # Blue Green Red\n        i_bgr = cv2.split(im_source)\n        weight = (0.3, 0.3, 0.4)\n        resbgr = [0, 0, 0]\n        for i in range(3): # bgr\n            resbgr[i] = cv2.matchTemplate(i_bgr[i], s_bgr[i], method)\n        res = resbgr[0]*weight[0] + resbgr[1]*weight[1] + resbgr[2]*weight[2]\n    else:\n        s_gray = cv2.cvtColor(im_search, cv2.COLOR_BGR2GRAY)\n        i_gray = cv2.cvtColor(im_source, cv2.COLOR_BGR2GRAY)\n        # \u8fb9\u754c\u63d0\u53d6(\u6765\u5b9e\u73b0\u80cc\u666f\u53bb\u9664\u7684\u529f\u80fd)\n        if bgremove:\n            s_gray = cv2.Canny(s_gray, 100, 200)\n            i_gray = cv2.Canny(i_gray, 100, 200)\n\n        res = cv2.matchTemplate(i_gray, s_gray, method)\n    w, h = im_search.shape[1], im_search.shape[0]\n\n    result = []\n    while True:\n        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n            top_left = min_loc\n        else:\n            top_left = max_loc\n        if DEBUG: \n            print('templmatch_value(thresh:%.1f) = %.3f' %(threshold, max_val)) # not show debug\n        if max_val < threshold:\n            break\n        # calculator middle point\n        middle_point = (top_left[0]+w/2, top_left[1]+h/2)\n        result.append(dict(\n            result=middle_point,\n            rectangle=(top_left, (top_left[0], top_left[1] + h), (top_left[0] + w, top_left[1]), (top_left[0] + w, top_left[1] + h)),\n            confidence=max_val\n        ))\n        if maxcnt and len(result) >= maxcnt:\n            break\n        # floodfill the already found area\n        cv2.floodFill(res, None, max_loc, (-1000,), max_val-threshold+0.1, 1, flags=cv2.FLOODFILL_FIXED_RANGE)\n    return result", "response": "Find all the template in the image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a SIFT in the given image.", "response": "def find_sift(im_source, im_search, min_match_count=4):\n    '''\n    SIFT\u7279\u5f81\u70b9\u5339\u914d\n    '''\n    res = find_all_sift(im_source, im_search, min_match_count, maxcnt=1)\n    if not res:\n        return None\n    return res[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all sifts in a source image.", "response": "def find_all_sift(im_source, im_search, min_match_count=4, maxcnt=0):\n    '''\n    \u4f7f\u7528sift\u7b97\u6cd5\u8fdb\u884c\u591a\u4e2a\u76f8\u540c\u5143\u7d20\u7684\u67e5\u627e\n    Args:\n        im_source(string): \u56fe\u50cf\u3001\u7d20\u6750\n        im_search(string): \u9700\u8981\u67e5\u627e\u7684\u56fe\u7247\n        threshold: \u9608\u503c\uff0c\u5f53\u76f8\u8bc6\u5ea6\u5c0f\u4e8e\u8be5\u9608\u503c\u7684\u65f6\u5019\uff0c\u5c31\u5ffd\u7565\u6389\n        maxcnt: \u9650\u5236\u5339\u914d\u7684\u6570\u91cf\n\n    Returns:\n        A tuple of found [(point, rectangle), ...]\n        A tuple of found [{\"point\": point, \"rectangle\": rectangle, \"confidence\": 0.76}, ...]\n        rectangle is a 4 points list\n    '''\n    sift = _sift_instance()\n    flann = cv2.FlannBasedMatcher({'algorithm': FLANN_INDEX_KDTREE, 'trees': 5}, dict(checks=50))\n\n    kp_sch, des_sch = sift.detectAndCompute(im_search, None)\n    if len(kp_sch) < min_match_count:\n        return None\n\n    kp_src, des_src = sift.detectAndCompute(im_source, None)\n    if len(kp_src) < min_match_count:\n        return None\n\n    h, w = im_search.shape[1:]\n\n    result = []\n    while True:\n        # \u5339\u914d\u4e24\u4e2a\u56fe\u7247\u4e2d\u7684\u7279\u5f81\u70b9\uff0ck=2\u8868\u793a\u6bcf\u4e2a\u7279\u5f81\u70b9\u53d62\u4e2a\u6700\u5339\u914d\u7684\u70b9\n        matches = flann.knnMatch(des_sch, des_src, k=2)\n        good = []\n        for m, n in matches:\n            # \u5254\u9664\u6389\u8ddf\u7b2c\u4e8c\u5339\u914d\u592a\u63a5\u8fd1\u7684\u7279\u5f81\u70b9\n            if m.distance < 0.9 * n.distance:\n                good.append(m)\n\n        if len(good) < min_match_count:\n            break\n\n        sch_pts = np.float32([kp_sch[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n        img_pts = np.float32([kp_src[m.trainIdx].pt for m in good]).reshape(-1, 1, 2) \n\n        # M\u662f\u8f6c\u5316\u77e9\u9635\n        M, mask = cv2.findHomography(sch_pts, img_pts, cv2.RANSAC, 5.0)\n        matches_mask = mask.ravel().tolist()\n\n        # \u8ba1\u7b97\u56db\u4e2a\u89d2\u77e9\u9635\u53d8\u6362\u540e\u7684\u5750\u6807\uff0c\u4e5f\u5c31\u662f\u5728\u5927\u56fe\u4e2d\u7684\u5750\u6807\n        h, w = im_search.shape[:2]\n        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n        dst = cv2.perspectiveTransform(pts, M)\n\n        # trans numpy arrary to python list\n        # [(a, b), (a1, b1), ...]\n        pypts = []\n        for npt in dst.astype(int).tolist():\n            pypts.append(tuple(npt[0]))\n\n        lt, br = pypts[0], pypts[2]\n        middle_point = (lt[0] + br[0]) / 2, (lt[1] + br[1]) / 2\n\n        result.append(dict(\n            result=middle_point,\n            rectangle=pypts,\n            confidence=(matches_mask.count(1), len(good)) #min(1.0 * matches_mask.count(1) / 10, 1.0)\n        ))\n\n        if maxcnt and len(result) >= maxcnt:\n            break\n        \n        # \u4ece\u7279\u5f81\u70b9\u4e2d\u5220\u6389\u90a3\u4e9b\u5df2\u7ecf\u5339\u914d\u8fc7\u7684, \u7528\u4e8e\u5bfb\u627e\u591a\u4e2a\u76ee\u6807\n        qindexes, tindexes = [], []\n        for m in good:\n            qindexes.append(m.queryIdx) # need to remove from kp_sch\n            tindexes.append(m.trainIdx) # need to remove from kp_img\n\n        def filter_index(indexes, arr):\n            r = np.ndarray(0, np.float32)\n            for i, item in enumerate(arr):\n                if i not in qindexes:\n                    r = np.append(r, item)\n            return r\n        kp_src = filter_index(tindexes, kp_src)\n        des_src = filter_index(tindexes, des_src)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_all(im_source, im_search, maxcnt=0):\n    '''\n    \u4f18\u5148Template\uff0c\u4e4b\u540eSift\n    @ return [(x,y), ...]\n    '''\n    result = find_all_template(im_source, im_search, maxcnt=maxcnt)\n    if not result:\n        result = find_all_sift(im_source, im_search, maxcnt=maxcnt)\n    if not result:\n        return []\n    return [match[\"result\"] for match in result]", "response": "find all the images in the image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef brightness(im):\n    '''\n    Return the brightness of an image\n    Args:\n        im(numpy): image\n\n    Returns:\n        float, average brightness of an image\n    '''\n    im_hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n    h, s, v = cv2.split(im_hsv) \n    height, weight = v.shape[:2]\n    total_bright = 0\n    for i in v:\n        total_bright = total_bright+sum(i)\n    return float(total_bright)/(height*weight)", "response": "Returns the brightness of an image\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Pulseaudio context and initialize all necessary attributes.", "response": "def init(self):\n        \"\"\"Creates context, when context is ready context_notify_cb is called\"\"\"\n        # Wrap callback methods in appropriate ctypefunc instances so\n        # that the Pulseaudio C API can call them\n        self._context_notify_cb = pa_context_notify_cb_t(\n            self.context_notify_cb)\n        self._sink_info_cb = pa_sink_info_cb_t(self.sink_info_cb)\n        self._update_cb = pa_context_subscribe_cb_t(self.update_cb)\n        self._success_cb = pa_context_success_cb_t(self.success_cb)\n        self._server_info_cb = pa_server_info_cb_t(self.server_info_cb)\n\n        # Create the mainloop thread and set our context_notify_cb\n        # method to be called when there's updates relating to the\n        # connection to Pulseaudio\n        _mainloop = pa_threaded_mainloop_new()\n        _mainloop_api = pa_threaded_mainloop_get_api(_mainloop)\n        context = pa_context_new(_mainloop_api, \"i3pystatus_pulseaudio\".encode(\"ascii\"))\n\n        pa_context_set_state_callback(context, self._context_notify_cb, None)\n        pa_context_connect(context, None, 0, None)\n        pa_threaded_mainloop_start(_mainloop)\n\n        self.colors = self.get_hex_color_range(self.color_muted, self.color_unmuted, 100)\n        self.sinks = []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_update(self, context):\n        pa_operation_unref(pa_context_get_sink_info_by_name(\n            context, self.current_sink.encode(), self._sink_info_cb, None))", "response": "Requests a sink info update"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when the server info is received from the server.", "response": "def server_info_cb(self, context, server_info_p, userdata):\n        \"\"\"Retrieves the default sink and calls request_update\"\"\"\n        server_info = server_info_p.contents\n        self.request_update(context)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnotify about changes in the context", "response": "def context_notify_cb(self, context, _):\n        \"\"\"Checks wether the context is ready\n\n        -Queries server information (server_info_cb is called)\n        -Subscribes to property changes on all sinks (update_cb is called)\n        \"\"\"\n        state = pa_context_get_state(context)\n\n        if state == PA_CONTEXT_READY:\n            pa_operation_unref(\n                pa_context_get_server_info(context, self._server_info_cb, None))\n\n            pa_context_set_subscribe_callback(context, self._update_cb, None)\n\n            pa_operation_unref(pa_context_subscribe(\n                context, PA_SUBSCRIPTION_EVENT_CHANGE | PA_SUBSCRIPTION_MASK_SINK | PA_SUBSCRIPTION_MASK_SERVER, self._success_cb, None))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate self. output based on sink_info_p. contents.", "response": "def sink_info_cb(self, context, sink_info_p, _, __):\n        \"\"\"Updates self.output\"\"\"\n        if sink_info_p:\n            sink_info = sink_info_p.contents\n            volume_percent = round(100 * sink_info.volume.values[0] / 0x10000)\n            volume_db = pa_sw_volume_to_dB(sink_info.volume.values[0])\n            self.currently_muted = sink_info.mute\n\n            if volume_db == float('-Infinity'):\n                volume_db = \"-\u221e\"\n            else:\n                volume_db = int(volume_db)\n\n            muted = self.muted if sink_info.mute else self.unmuted\n\n            if self.multi_colors and not sink_info.mute:\n                color = self.get_gradient(volume_percent, self.colors)\n            else:\n                color = self.color_muted if sink_info.mute else self.color_unmuted\n\n            if muted and self.format_muted is not None:\n                output_format = self.format_muted\n            else:\n                output_format = self.format\n\n            if self.bar_type == 'vertical':\n                volume_bar = make_vertical_bar(volume_percent, self.vertical_bar_width)\n            elif self.bar_type == 'horizontal':\n                volume_bar = make_bar(volume_percent)\n            else:\n                raise Exception(\"bar_type must be 'vertical' or 'horizontal'\")\n\n            selected = \"\"\n            dump = subprocess.check_output(\"pacmd dump\".split(), universal_newlines=True)\n            for line in dump.split(\"\\n\"):\n                if line.startswith(\"set-default-sink\"):\n                    default_sink = line.split()[1]\n                    if default_sink == self.current_sink:\n                        selected = self.format_selected\n\n            self.output = {\n                \"color\": color,\n                \"full_text\": output_format.format(\n                    muted=muted,\n                    volume=volume_percent,\n                    db=volume_db,\n                    volume_bar=volume_bar,\n                    selected=selected),\n            }\n\n            self.send_output()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cycle_interface(self, increment=1):\n        interfaces = [i for i in netifaces.interfaces() if i not in self.ignore_interfaces]\n        if self.interface in interfaces:\n            next_index = (interfaces.index(self.interface) + increment) % len(interfaces)\n            self.interface = interfaces[next_index]\n        elif len(interfaces) > 0:\n            self.interface = interfaces[0]\n\n        if self.network_traffic:\n            self.network_traffic.clear_counters()\n            self.kbs_arr = [0.0] * self.graph_width", "response": "Cycle through available interfaces in increment steps. Sign indicates direction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart timer. If timer is already running it will increase remaining time instead. :param int seconds: Initial time.", "response": "def start(self, seconds=300):\n        \"\"\"\n        Starts timer.\n        If timer is already running it will increase remaining time instead.\n\n        :param int seconds: Initial time.\n        \"\"\"\n        if self.state is TimerState.stopped:\n            self.compare = time.time() + abs(seconds)\n            self.state = TimerState.running\n        elif self.state is TimerState.running:\n            self.increase(seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef increase(self, seconds):\n        if self.state is TimerState.running:\n            new_compare = self.compare + seconds\n            if new_compare > time.time():\n                self.compare = new_compare", "response": "Increase the time remaining time value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        if self.state is not TimerState.stopped:\n            if self.on_reset and self.state is TimerState.overflow:\n                if callable(self.on_reset):\n                    self.on_reset()\n                else:\n                    execute(self.on_reset)\n            self.state = TimerState.stopped", "response": "Stop timer and execute ``on_reset`` if overflow occured."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_line(self, message):\n\n        self.out.write(message + \"\\n\")\n        self.out.flush()", "response": "Unbuffered printing to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a line from the input stream. Raises KeyboardInterrupt if the input stream is interrupted. Raises EOFError if the input stream is reached. Raises EOFError if the input stream is empty.", "response": "def read_line(self):\n        \"\"\"\n        Interrupted respecting reader for stdin.\n\n        Raises EOFError if the end of stream has been reached\n        \"\"\"\n\n        try:\n            line = self.inp.readline().strip()\n        except KeyboardInterrupt:\n            raise EOFError()\n\n        # i3status sends EOF, or an empty line\n        if not line:\n            raise EOFError()\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the treshold interval of the modules.", "response": "def compute_treshold_interval(self):\n        \"\"\"\n        Current method is to compute average from all intervals.\n        \"\"\"\n\n        intervals = [m.interval for m in self.modules if hasattr(m, \"interval\")]\n        if len(intervals) > 0:\n            self.treshold_interval = round(sum(intervals) / len(intervals))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling this method will send the status line to i3bar immediately without waiting for timeout (1s by default).", "response": "def async_refresh(self):\n        \"\"\"\n        Calling this method will send the status line to i3bar immediately\n        without waiting for timeout (1s by default).\n        \"\"\"\n\n        self.refresh_cond.acquire()\n        self.refresh_cond.notify()\n        self.refresh_cond.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refresh_signal_handler(self, signo, frame):\n\n        if signo != signal.SIGUSR1:\n            return\n\n        for module in self.modules:\n            if hasattr(module, \"interval\"):\n                if module.interval > self.treshold_interval:\n                    thread = Thread(target=module.run)\n                    thread.start()\n                else:\n                    module.run()\n            else:\n                module.run()\n\n        self.async_refresh()", "response": "This callback is called when SIGUSR1 signal is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef suspend_signal_handler(self, signo, frame):\n        if signo != signal.SIGUSR2:\n            return\n        self.stopped = not self.stopped\n        if self.stopped:\n            [m.suspend() for m in IntervalModule.managers.values()]\n        else:\n            [m.resume() for m in IntervalModule.managers.values()]", "response": "Suspends the i3bar process when the signal is SIGSTOP or SIGUSR2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self):\n\n        for line in self.io.read():\n            with self.parse_line(line) as j:\n                yield j", "response": "Iterate over all JSON input."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_line(self, line):\n\n        prefix = \"\"\n        # ignore comma at start of lines\n        if line.startswith(\",\"):\n            line, prefix = line[1:], \",\"\n\n        j = json.loads(line)\n        yield j\n        self.io.write_line(prefix + json.dumps(j))", "response": "Parse a single line of JSON and write modified JSON back."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef formatters(self):\n        event_dict = dict(\n            title=self.title,\n            remaining=self.time_remaining,\n            humanize_remaining=self.humanize_time_remaining,\n        )\n\n        def is_formatter(x):\n            return inspect.ismethod(x) and hasattr(x, 'formatter') and getattr(x, 'formatter')\n\n        for method_name, method in inspect.getmembers(self, is_formatter):\n            event_dict[method_name] = method()\n        return event_dict", "response": "Build a dictionary containing all the key - value pairs that will be exposed to the user via formatters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride this method to do more interesting things with the event.", "response": "def on_click(self, event):\n        \"\"\" Override this method to do more interesting things with the event. \"\"\"\n        DesktopNotification(\n            title=event.title,\n            body=\"{} until {}!\".format(event.time_remaining, event.title),\n            icon='dialog-information',\n            urgency=1,\n            timeout=0,\n        ).display()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_urgent(self):\n        if not self.current_event:\n            return False\n        now = datetime.now(tz=self.current_event.start.tzinfo)\n        alert_time = now + timedelta(seconds=self.urgent_seconds)\n        urgent = alert_time > self.current_event.start\n        if urgent and self.urgent_blink:\n            urgent = now.second % 2 == 0 and not self.urgent_acknowledged\n        return urgent", "response": "Determines whether or not the urgent flag is set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the object with the closest available location code and station ID.", "response": "def init(self):\n        '''\n        Use the location_code to perform a geolookup and find the closest\n        station. If the location is a pws or icao station ID, no lookup will be\n        peformed.\n        '''\n        try:\n            for no_lookup in ('pws', 'icao'):\n                sid = self.location_code.partition(no_lookup + ':')[-1]\n                if sid:\n                    self.station_id = self.location_code\n                    return\n        except AttributeError:\n            # Numeric or some other type, either way we'll just stringify\n            # it below and perform a lookup.\n            pass\n\n        self.get_station_id()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the forecast data for the current weather station.", "response": "def get_forecast(self):\n        '''\n        If configured to do so, make an API request to retrieve the forecast\n        data for the configured/queried weather station, and return the low and\n        high temperatures. Otherwise, return two empty strings.\n        '''\n        no_data = ('', '')\n        if self.forecast:\n            query_url = STATION_QUERY_URL % (self.api_key,\n                                             'forecast',\n                                             self.station_id)\n            try:\n                response = self.api_request(query_url)['forecast']\n                response = response['simpleforecast']['forecastday'][0]\n            except (KeyError, IndexError, TypeError):\n                self.logger.error(\n                    'No forecast data found for %s', self.station_id)\n                self.data['update_error'] = self.update_error\n                return no_data\n\n            unit = 'celsius' if self.units == 'metric' else 'fahrenheit'\n            low_temp = response.get('low', {}).get(unit, '')\n            high_temp = response.get('high', {}).get(unit, '')\n            return low_temp, high_temp\n        else:\n            return no_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing geolocation to get the station ID", "response": "def get_station_id(self):\n        '''\n        Use geolocation to get the station ID\n        '''\n        extra_opts = '/pws:0' if not self.use_pws else ''\n        api_url = GEOLOOKUP_URL % (self.api_key,\n                                   extra_opts,\n                                   self.location_code)\n        response = self.api_request(api_url)\n        station_type = 'pws' if self.use_pws else 'airport'\n        try:\n            stations = response['location']['nearby_weather_stations']\n            nearest = stations[station_type]['station'][0]\n        except (KeyError, IndexError):\n            raise Exception(\n                'No locations matched location_code %s' % self.location_code)\n\n        self.logger.error('nearest = %s', nearest)\n        if self.use_pws:\n            nearest_pws = nearest.get('id', '')\n            if not nearest_pws:\n                raise Exception('No id entry for nearest PWS')\n            self.station_id = 'pws:%s' % nearest_pws\n        else:\n            nearest_airport = nearest.get('icao', '')\n            if not nearest_airport:\n                raise Exception('No icao entry for nearest airport')\n            self.station_id = 'icao:%s' % nearest_airport"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_weather(self):\n        '''\n        Query the configured/queried station and return the weather data\n        '''\n        if self.station_id is None:\n            # Failed to get the nearest station ID when first launched, so\n            # retry it.\n            self.get_station_id()\n\n        self.data['update_error'] = ''\n        try:\n            query_url = STATION_QUERY_URL % (self.api_key,\n                                             'conditions',\n                                             self.station_id)\n            try:\n                response = self.api_request(query_url)['current_observation']\n                self.forecast_url = response.pop('ob_url', None)\n            except KeyError:\n                self.logger.error('No weather data found for %s', self.station_id)\n                self.data['update_error'] = self.update_error\n                return\n\n            if self.forecast:\n                query_url = STATION_QUERY_URL % (self.api_key,\n                                                 'forecast',\n                                                 self.station_id)\n                try:\n                    forecast = self.api_request(query_url)['forecast']\n                    forecast = forecast['simpleforecast']['forecastday'][0]\n                except (KeyError, IndexError, TypeError):\n                    self.logger.error(\n                        'No forecast data found for %s', self.station_id)\n                    # This is a non-fatal error, so don't return but do set the\n                    # error flag.\n                    self.data['update_error'] = self.update_error\n\n                unit = 'celsius' if self.units == 'metric' else 'fahrenheit'\n                low_temp = forecast.get('low', {}).get(unit, '')\n                high_temp = forecast.get('high', {}).get(unit, '')\n            else:\n                low_temp = high_temp = ''\n\n            if self.units == 'metric':\n                temp_unit = 'c'\n                speed_unit = 'kph'\n                distance_unit = 'km'\n                pressure_unit = 'mb'\n            else:\n                temp_unit = 'f'\n                speed_unit = 'mph'\n                distance_unit = 'mi'\n                pressure_unit = 'in'\n\n            def _find(key, data=None, default=''):\n                if data is None:\n                    data = response\n                return str(data.get(key, default))\n\n            try:\n                observation_epoch = _find('observation_epoch') or _find('local_epoch')\n                observation_time = datetime.fromtimestamp(int(observation_epoch))\n            except (TypeError, ValueError):\n                log.debug(\n                    'Observation time \\'%s\\' is not a UNIX timestamp',\n                    observation_epoch\n                )\n                observation_time = datetime.fromtimestamp(0)\n\n            self.data['city'] = _find('city', response['observation_location'])\n            self.data['condition'] = _find('weather')\n            self.data['observation_time'] = observation_time\n            self.data['current_temp'] = _find('temp_' + temp_unit).split('.')[0]\n            self.data['low_temp'] = low_temp\n            self.data['high_temp'] = high_temp\n            self.data['temp_unit'] = '\u00b0' + temp_unit.upper()\n            self.data['feelslike'] = _find('feelslike_' + temp_unit)\n            self.data['dewpoint'] = _find('dewpoint_' + temp_unit)\n            self.data['wind_speed'] = _find('wind_' + speed_unit)\n            self.data['wind_unit'] = speed_unit\n            self.data['wind_direction'] = _find('wind_dir')\n            self.data['wind_gust'] = _find('wind_gust_' + speed_unit)\n            self.data['pressure'] = _find('pressure_' + pressure_unit)\n            self.data['pressure_unit'] = pressure_unit\n            self.data['pressure_trend'] = _find('pressure_trend')\n            self.data['visibility'] = _find('visibility_' + distance_unit)\n            self.data['visibility_unit'] = distance_unit\n            self.data['humidity'] = _find('relative_humidity').rstrip('%')\n            self.data['uv_index'] = _find('UV')\n        except Exception:\n            # Don't let an uncaught exception kill the update thread\n            self.logger.error(\n                'Uncaught error occurred while checking weather. '\n                'Exception follows:', exc_info=True\n            )\n            self.data['update_error'] = self.update_error", "response": "Query the configured station and return the weather data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfigure out the date to use for API requests.", "response": "def get_api_date(self):\n        '''\n        Figure out the date to use for API requests. Assumes yesterday's date\n        if between midnight and 10am Eastern time. Override this function in a\n        subclass to change how the API date is calculated.\n        '''\n        # NOTE: If you are writing your own function to get the date, make sure\n        # to include the first if block below to allow for the ``date``\n        # parameter to hard-code a date.\n        api_date = None\n        if self.date is not None and not isinstance(self.date, datetime):\n            try:\n                api_date = datetime.strptime(self.date, '%Y-%m-%d')\n            except (TypeError, ValueError):\n                self.logger.warning('Invalid date \\'%s\\'', self.date)\n\n        if api_date is None:\n            utc_time = pytz.utc.localize(datetime.utcnow())\n            eastern = pytz.timezone('US/Eastern')\n            api_date = eastern.normalize(utc_time.astimezone(eastern))\n            if api_date.hour < 10:\n                # The scores on NHL.com change at 10am Eastern, if it's before\n                # that time of day then we will use yesterday's date.\n                api_date -= timedelta(days=1)\n        self.date = api_date"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the weather data from the DOM.", "response": "def handle_data(self, content):\n        '''\n        Sometimes the weather data is set under an attribute of the \"window\"\n        DOM object. Sometimes it appears as part of a javascript function.\n        Catch either possibility.\n        '''\n        if self.weather_data is not None:\n            # We've already found weather data, no need to continue parsing\n            return\n        content = content.strip().rstrip(';')\n        try:\n            tag_text = self.get_starttag_text().lower()\n        except AttributeError:\n            tag_text = ''\n        if tag_text.startswith('<script'):\n            # Look for feed information embedded as a javascript variable\n            begin = content.find('window.__data')\n            if begin != -1:\n                self.logger.debug('Located window.__data')\n                # Look for end of JSON dict and end of javascript statement\n                end = content.find('};', begin)\n                if end == -1:\n                    self.logger.debug('Failed to locate end of javascript statement')\n                else:\n                    # Strip the \"window.__data=\" from the beginning\n                    json_data = self.load_json(\n                        content[begin:end + 1].split('=', 1)[1].lstrip()\n                    )\n                    if json_data is not None:\n                        def _find_weather_data(data):\n                            '''\n                            Helper designed to minimize impact of potential\n                            structural changes to this data.\n                            '''\n                            if isinstance(data, dict):\n                                if 'Observation' in data and 'DailyForecast' in data:\n                                    return data\n                                else:\n                                    for key in data:\n                                        ret = _find_weather_data(data[key])\n                                        if ret is not None:\n                                            return ret\n                                return None\n\n                        weather_data = _find_weather_data(json_data)\n                        if weather_data is None:\n                            self.logger.debug(\n                                'Failed to locate weather data in the '\n                                'following data structure: %s', json_data\n                            )\n                        else:\n                            self.weather_data = weather_data\n                            return\n\n            for line in content.splitlines():\n                line = line.strip().rstrip(';')\n                if line.startswith('var adaptorParams'):\n                    # Strip off the \"var adaptorParams = \" from the beginning,\n                    # and the javascript semicolon from the end. This will give\n                    # us JSON that we can load.\n                    weather_data = self.load_json(line.split('=', 1)[1].lstrip())\n                    if weather_data is not None:\n                        self.weather_data = weather_data\n                        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_weather(self):\n        '''\n        Fetches the current weather from wxdata.weather.com service.\n        '''\n\n        if self.units not in ('imperial', 'metric'):\n            raise Exception(\"units must be one of (imperial, metric)!\")\n\n        if self.location_code is None:\n            self.logger.error(\n                'A location_code is required to check Weather.com. See the '\n                'documentation for more information.'\n            )\n            self.data['update_error'] = self.update_error\n            return\n        self.data['update_error'] = ''\n        try:\n\n            self.parser.get_weather_data(self.forecast_url)\n            if self.parser.weather_data is None:\n                self.logger.error(\n                    'Failed to read weather data from page. Run module with '\n                    'debug logging to get more information.'\n                )\n                self.data['update_error'] = self.update_error\n                return\n\n            try:\n                observed = self.parser.weather_data['Observation']\n                # Observation data stored under a sub-key containing the\n                # lat/long coordinates. For example:\n                #\n                # geocode:41.77,-88.35:language:en-US:units:e\n                #\n                # Since this is the only key under \"Observation\", we can just\n                # use next(iter(observed)) to get it.\n                observed = observed[next(iter(observed))]['data']['vt1observation']\n            except KeyError:\n                self.logger.error(\n                    'Failed to retrieve current conditions from API response. '\n                    'Run module with debug logging to get more information.'\n                )\n                self.data['update_error'] = self.update_error\n                return\n\n            try:\n                forecast = self.parser.weather_data['DailyForecast']\n                # Same as above, use next(iter(forecast)) to drill down to the\n                # correct nested dict level.\n                forecast = forecast[next(iter(forecast))]\n                forecast = forecast['data']['vt1dailyForecast'][0]\n            except (IndexError, KeyError):\n                self.logger.error(\n                    'Failed to retrieve forecast data from API response. '\n                    'Run module with debug logging to get more information.'\n                )\n                self.data['update_error'] = self.update_error\n                return\n\n            try:\n                self.city_name = self.parser.weather_data['Location']\n                # Again, same technique as above used to get down to the\n                # correct nested dict level.\n                self.city_name = self.city_name[next(iter(self.city_name))]\n                self.city_name = self.city_name['data']['prsntNm']\n            except KeyError:\n                self.logger.warning(\n                    'Failed to get city name from API response, falling back '\n                    'to location code \\'%s\\'', self.location_code\n                )\n                self.city_name = self.location_code\n\n            # Cut off the timezone from the end of the string (it's after the last\n            # space, hence the use of rpartition). International timezones (or ones\n            # outside the system locale) don't seem to be handled well by\n            # datetime.datetime.strptime().\n            try:\n                observation_time_str = str(observed.get('observationTime', ''))\n                observation_time = datetime.strptime(observation_time_str,\n                                                     '%Y-%d-%yT%H:%M:%S%z')\n            except (ValueError, AttributeError):\n                observation_time = datetime.fromtimestamp(0)\n\n            try:\n                pressure_trend_str = observed.get('barometerTrend', '').lower()\n            except AttributeError:\n                pressure_trend_str = ''\n\n            if pressure_trend_str == 'rising':\n                pressure_trend = '+'\n            elif pressure_trend_str == 'falling':\n                pressure_trend = '-'\n            else:\n                pressure_trend = ''\n\n            try:\n                high_temp = forecast.get('day', {}).get('temperature', '')\n            except (AttributeError, IndexError):\n                high_temp = ''\n            else:\n                if high_temp is None:\n                    # In the mid-afternoon, the high temp disappears from the\n                    # forecast, so just set high_temp to an empty string.\n                    high_temp = ''\n\n            try:\n                low_temp = forecast.get('night', {}).get('temperature', '')\n            except (AttributeError, IndexError):\n                low_temp = ''\n\n            if self.units == 'imperial':\n                temp_unit = '\u00b0F'\n                wind_unit = 'mph'\n                pressure_unit = 'in'\n                visibility_unit = 'mi'\n            else:\n                temp_unit = '\u00b0C'\n                wind_unit = 'kph'\n                pressure_unit = 'mb'\n                visibility_unit = 'km'\n\n            self.data['city'] = self.city_name\n            self.data['condition'] = str(observed.get('phrase', ''))\n            self.data['observation_time'] = observation_time\n            self.data['current_temp'] = str(observed.get('temperature', ''))\n            self.data['low_temp'] = str(low_temp)\n            self.data['high_temp'] = str(high_temp)\n            self.data['temp_unit'] = temp_unit\n            self.data['feelslike'] = str(observed.get('feelsLike', ''))\n            self.data['dewpoint'] = str(observed.get('dewPoint', ''))\n            self.data['wind_speed'] = str(observed.get('windSpeed', ''))\n            self.data['wind_unit'] = wind_unit\n            self.data['wind_direction'] = str(observed.get('windDirCompass', ''))\n            # Gust can be None, using \"or\" to ensure empty string in this case\n            self.data['wind_gust'] = str(observed.get('gust', '') or '')\n            self.data['pressure'] = str(observed.get('altimeter', ''))\n            self.data['pressure_unit'] = pressure_unit\n            self.data['pressure_trend'] = pressure_trend\n            self.data['visibility'] = str(observed.get('visibility', ''))\n            self.data['visibility_unit'] = visibility_unit\n            self.data['humidity'] = str(observed.get('humidity', ''))\n            self.data['uv_index'] = str(observed.get('uvIndex', ''))\n        except Exception:\n            # Don't let an uncaught exception kill the update thread\n            self.logger.error(\n                'Uncaught error occurred while checking weather. '\n                'Exception follows:', exc_info=True\n            )\n            self.data['update_error'] = self.update_error", "response": "Checks the current weather from wxdata. weather. com and returns the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main_loop(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        port = int(getattr(self, 'port', 1738))\n        sock.bind(('127.0.0.1', port))\n        while True:\n            data, addr = sock.recvfrom(512)\n            color = data.decode().strip()\n            self.color = self.colors.get(color, color)", "response": "Main loop for the AAF object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the module should execute.", "response": "def should_execute(self, workload):\n        \"\"\"\n        If we have been suspended by i3bar, only execute those modules that set the keep_alive flag to a truthy\n        value. See the docs on the suspend_signal_handler method of the io module for more information.\n        \"\"\"\n        if not self._suspended.is_set():\n            return True\n        workload = unwrap_workload(workload)\n        return hasattr(workload, 'keep_alive') and getattr(workload, 'keep_alive')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sensors():\n    import sensors\n    found_sensors = list()\n\n    def get_subfeature_value(feature, subfeature_type):\n        subfeature = chip.get_subfeature(feature, subfeature_type)\n        if subfeature:\n            return chip.get_value(subfeature.number)\n\n    for chip in sensors.get_detected_chips():\n        for feature in chip.get_features():\n            if feature.type == sensors.FEATURE_TEMP:\n                try:\n                    name = chip.get_label(feature)\n                    max = get_subfeature_value(feature, sensors.SUBFEATURE_TEMP_MAX)\n                    current = get_subfeature_value(feature, sensors.SUBFEATURE_TEMP_INPUT)\n                    critical = get_subfeature_value(feature, sensors.SUBFEATURE_TEMP_CRIT)\n                    if critical:\n                        found_sensors.append(Sensor(name=name, current=current, maximum=max, critical=critical))\n                except sensors.SensorsException:\n                    continue\n    return found_sensors", "response": "Detect and return a list of Sensor objects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_output_sensors(self):\n        data = dict()\n        found_sensors = get_sensors()\n        if len(found_sensors) == 0:\n            raise Exception(\"No sensors detected! \"\n                            \"Ensure lm-sensors is installed and check the output of the `sensors` command.\")\n        for sensor in found_sensors:\n            data[sensor.name] = self.format_sensor(sensor)\n            data[\"{}_bar\".format(sensor.name)] = self.format_sensor_bar(sensor)\n        data['temp'] = max((s.current for s in found_sensors))\n        return {\n            'full_text': self.format.format(**data),\n            'urgent': self.get_urgent(found_sensors),\n            'color': self.color if not self.dynamic_color else None,\n        }", "response": "Build the output using lm_sensors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if any sensors should set the urgent flag.", "response": "def get_urgent(self, sensors):\n        \"\"\" Determine if any sensors should set the urgent flag. \"\"\"\n        if self.urgent_on not in ('warning', 'critical'):\n            raise Exception(\"urgent_on must be one of (warning, critical)\")\n        for sensor in sensors:\n            if self.urgent_on == 'warning' and sensor.is_warning():\n                return True\n            elif self.urgent_on == 'critical' and sensor.is_critical():\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_sensor(self, sensor):\n        current_val = sensor.current\n        if self.pango_enabled:\n            percentage = self.percentage(sensor.current, sensor.critical)\n            if self.dynamic_color:\n                color = self.colors[int(percentage)]\n                return self.format_pango(color, current_val)\n        return current_val", "response": "Format a sensor value. If pango is enabled color is per sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds and format a sensor bar. If pango is enabled bar color is per sensor.", "response": "def format_sensor_bar(self, sensor):\n        \"\"\" Build and format a sensor bar. If pango is enabled bar color is per sensor.\"\"\"\n        percentage = self.percentage(sensor.current, sensor.critical)\n        bar = make_vertical_bar(int(percentage))\n        if self.pango_enabled:\n            if self.dynamic_color:\n                color = self.colors[int(percentage)]\n                return self.format_pango(color, bar)\n        return bar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the sum of unread messages across all registered backends", "response": "def run(self):\n        \"\"\"\n        Returns the sum of unread messages across all registered backends\n        \"\"\"\n        unread = 0\n        current_unread = 0\n        for id, backend in enumerate(self.backends):\n            temp = backend.unread or 0\n            unread = unread + temp\n            if id == self.current_backend:\n                current_unread = temp\n\n        if not unread:\n            color = self.color\n            urgent = \"false\"\n            if self.hide_if_null:\n                self.output = None\n                return\n        else:\n            color = self.color_unread\n            urgent = \"true\"\n\n        format = self.format\n        if unread > 1:\n            format = self.format_plural\n\n        account_name = getattr(self.backends[self.current_backend], \"account\", \"No name\")\n\n        self.output = {\n            \"full_text\": format.format(unread=unread, current_unread=current_unread, account=account_name),\n            \"urgent\": urgent,\n            \"color\": color,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts output to key value pairs", "response": "def parse_output(self, line):\n        \"\"\"Convert output to key value pairs\"\"\"\n\n        try:\n            key, value = line.split(\":\")\n            self.update_value(key.strip(), value.strip())\n        except ValueError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_value(self, key, value):\n\n        if key == \"Status\":\n            self._inhibited = value != \"Enabled\"\n        elif key == \"Color temperature\":\n            self._temperature = int(value.rstrip(\"K\"), 10)\n        elif key == \"Period\":\n            self._period = value\n        elif key == \"Brightness\":\n            self._brightness = value\n        elif key == \"Location\":\n            location = []\n            for x in value.split(\", \"):\n                v, d = x.split(\" \")\n                location.append(float(v) * (1 if d in \"NE\" else -1))\n            self._location = (location)", "response": "Parse the key value pairs to update their values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggle_inhibit(self):\n        if self.inhibit:\n            self._controller.set_inhibit(False)\n            self.inhibit = False\n        else:\n            self._controller.set_inhibit(True)\n            self.inhibit = True", "response": "Enable or disable redshift"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_through_shell(command, enable_shell=False):\n\n    if not enable_shell and isinstance(command, str):\n        command = shlex.split(command)\n\n    returncode = None\n    stderr = None\n    try:\n        proc = subprocess.Popen(command, stderr=subprocess.PIPE,\n                                stdout=subprocess.PIPE, shell=enable_shell)\n        out, stderr = proc.communicate()\n        out = out.decode(\"UTF-8\")\n        stderr = stderr.decode(\"UTF-8\")\n\n        returncode = proc.returncode\n\n    except OSError as e:\n        out = e.strerror\n        stderr = e.strerror\n        logging.getLogger(\"i3pystatus.core.command\").exception(\"\")\n    except subprocess.CalledProcessError as e:\n        out = e.output\n        logging.getLogger(\"i3pystatus.core.command\").exception(\"\")\n\n    return CommandResult(returncode, out, stderr)", "response": "Runs a command through a shell."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(command, detach=False):\n\n    if detach:\n        if not isinstance(command, str):\n            msg = \"Detached mode expects a string as command, not {}\".format(\n                  command)\n            logging.getLogger(\"i3pystatus.core.command\").error(msg)\n            raise AttributeError(msg)\n        command = [\"i3-msg\", \"exec\", command]\n    else:\n        if isinstance(command, str):\n            command = shlex.split(command)\n\n    try:\n        subprocess.Popen(command, stdin=subprocess.DEVNULL,\n                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except OSError:\n        logging.getLogger(\"i3pystatus.core.command\").exception(\"\")\n    except subprocess.CalledProcessError:\n        logging.getLogger(\"i3pystatus.core.command\").exception(\"\")", "response": "Executes a command in background."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_hex_color_range(start_color, end_color, quantity):\n        raw_colors = [c.hex for c in list(Color(start_color).range_to(Color(end_color), quantity))]\n        colors = []\n        for color in raw_colors:\n\n            # i3bar expects the full Hex value but for some colors the colour\n            # module only returns partial values. So we need to convert these colors to the full\n            # Hex value.\n            if len(color) == 4:\n                fixed_color = \"#\"\n                for c in color[1:]:\n                    fixed_color += c * 2\n                colors.append(fixed_color)\n            else:\n                colors.append(color)\n        return colors", "response": "Generates a list of quantity Hex colors from start_color to end_color."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gradient(self, value, colors, upper_limit=100):\n        index = int(self.percentage(value, upper_limit))\n        if index >= len(colors):\n            return colors[-1]\n        elif index < 0:\n            return colors[0]\n        else:\n            return colors[index]", "response": "Map a value to a color code"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_method_of(method, object):\n    if not callable(method) or not hasattr(method, \"__name__\"):\n        return False\n    if inspect.ismethod(method):\n        return method.__self__ is object\n    for cls in inspect.getmro(object.__class__):\n        if cls.__dict__.get(method.__name__, None) is method:\n            return True\n    return False", "response": "Decide whether method is contained within the MRO of object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_click(self, button, **kwargs):\n\n        actions = ['leftclick', 'middleclick', 'rightclick',\n                   'upscroll', 'downscroll']\n        try:\n            action = actions[button - 1]\n        except (TypeError, IndexError):\n            self.__log_button_event(button, None, None, \"Other button\")\n            action = \"otherclick\"\n\n        m_click = self.__multi_click\n\n        with m_click.lock:\n            double = m_click.check_double(button)\n            double_action = 'double%s' % action\n\n            if double:\n                action = double_action\n\n            # Get callback function\n            cb = getattr(self, 'on_%s' % action, None)\n\n            double_handler = getattr(self, 'on_%s' % double_action, None)\n            delay_execution = (not double and double_handler)\n\n            if delay_execution:\n                m_click.set_timer(button, cb, **kwargs)\n            else:\n                self.__button_callback_handler(button, cb, **kwargs)", "response": "This method is called by i3bar when a button is clicked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the full_text and short_text attributes of self. output to pango format.", "response": "def text_to_pango(self):\n        \"\"\"\n        Replaces all ampersands in `full_text` and `short_text` attributes of\n        `self.output` with `&amp;`.\n\n        It is called internally when pango markup is used.\n\n        Can be called multiple times (`&amp;` won't change to `&amp;amp;`).\n        \"\"\"\n        def replace(s):\n            s = s.split(\"&\")\n            out = s[0]\n            for i in range(len(s) - 1):\n                if s[i + 1].startswith(\"amp;\"):\n                    out += \"&\" + s[i + 1]\n                else:\n                    out += \"&amp;\" + s[i + 1]\n            return out\n\n        if \"full_text\" in self.output.keys():\n            self.output[\"full_text\"] = replace(self.output[\"full_text\"])\n        if \"short_text\" in self.output.keys():\n            self.output[\"short_text\"] = replace(self.output[\"short_text\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef form_b(self, n: float)->tuple:\n        unit = 'bps'\n        kilo = 1000\n        mega = 1000000\n        giga = 1000000000\n        bps = 0\n\n        if self.units == 'bytes' or self.units == 'B':\n            unit = 'Bps'\n            kilo = 8000\n            mega = 8000000\n            giga = 8000000000\n\n        if n < kilo:\n            bps = float(n)\n\n        if n >= kilo and n < mega:\n            unit = \"K\" + unit\n            bps = float(n / 1024.0)\n\n        if n >= mega and n < giga:\n            unit = \"M\" + unit\n            bps = float(n / (1024.0 * 1024.0))\n\n        if n >= giga:\n            unit = \"G\" + unit\n            bps = float(n / (1024.0 * 1024.0 * 1024.0))\n\n        return bps, unit", "response": "formats a bps as bps / kbps / mbps etc and returns a tuple of float - number of mbps etc str - units"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget protected settings from keyring if they are not already set.", "response": "def get_protected_settings(self, settings_source):\n        \"\"\"\n        Attempt to retrieve protected settings from keyring if they are not already set.\n        \"\"\"\n        user_backend = settings_source.get('keyring_backend')\n        found_settings = dict()\n        for setting_name in self.__PROTECTED_SETTINGS:\n            # Nothing to do if the setting is already defined.\n            if settings_source.get(setting_name):\n                continue\n\n            setting = None\n            identifier = \"%s.%s\" % (self.__name__, setting_name)\n            if hasattr(self, 'required') and setting_name in getattr(self, 'required'):\n                setting = self.get_setting_from_keyring(identifier, user_backend)\n            elif hasattr(self, setting_name):\n                setting = self.get_setting_from_keyring(identifier, user_backend)\n            if setting:\n                found_settings.update({setting_name: setting})\n        return found_settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a protected setting from a keyring.", "response": "def get_setting_from_keyring(self, setting_identifier, keyring_backend=None):\n        \"\"\"\n        Retrieves a protected setting from keyring\n        :param setting_identifier: must be in the format package.module.Class.setting\n        \"\"\"\n        # If a custom keyring backend has been defined, use it.\n        if keyring_backend:\n            return keyring_backend.get_password(setting_identifier, getpass.getuser())\n\n        # Otherwise try and use default keyring.\n        try:\n            import keyring\n        except ImportError:\n            pass\n        else:\n            return keyring.get_password(setting_identifier, getpass.getuser())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(self, module, *args, **kwargs):\n        from i3pystatus.text import Text\n\n        if not module:\n            return\n\n        # Merge the module's hints with the default hints\n        # and overwrite any duplicates with the hint from the module\n        hints = self.default_hints.copy() if self.default_hints else {}\n        hints.update(kwargs.get('hints', {}))\n        if hints:\n            kwargs['hints'] = hints\n\n        try:\n            return self.modules.append(module, *args, **kwargs)\n        except Exception as e:\n            log.exception(e)\n            return self.modules.append(Text(\n                color=\"#FF0000\",\n                text=\"{i3py_mod}: Fatal Error - {ex}({msg})\".format(\n                    i3py_mod=module,\n                    ex=e.__class__.__name__,\n                    msg=e\n                )\n            ))", "response": "Register a new module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction processes the cpuinfo file and creates a dictionary of values for the module", "response": "def createvaluesdict(self):\n        \"\"\"\n        function processes the /proc/cpuinfo file, use file=/sys to use kernel >=4.13 location\n        :return: dictionary used as the full-text output for the module\n        \"\"\"\n        cpus_offline = 0\n        if self.file == '/sys':\n            with open('/sys/devices/system/cpu/online') as f:\n                line = f.readline()\n                cpus_online = [int(cpu) for cpu in line.split(',') if cpu.find('-') < 0]\n                cpus_online_range = [cpu_range for cpu_range in line.split(',') if cpu_range.find('-') > 0]\n\n            for cpu_range in cpus_online_range:\n                cpus_online += [cpu for cpu in range(int(cpu_range.split('-')[0]), int(cpu_range.split('-')[1]) + 1)]\n\n            mhz_values = [0.0 for cpu in range(max(cpus_online) + 1)]\n            ghz_values = [0.0 for cpu in range(max(cpus_online) + 1)]\n            for cpu in cpus_online:\n                with open('/sys/devices/system/cpu/cpu{}/cpufreq/scaling_cur_freq'.format(cpu)) as f:\n                    line = f.readline()\n                    mhz_values[cpu] = float(line.rstrip()) / 1000.0\n                    ghz_values[cpu] = float(line.rstrip()) / 1000000.0\n            cpus_offline = mhz_values.count(0.0)\n        else:\n            with open(self.file) as f:\n                mhz_values = [float(line.split(':')[1]) for line in f if line.startswith('cpu MHz')]\n                ghz_values = [value / 1000.0 for value in mhz_values]\n\n        mhz = {\"core{}\".format(key): \"{0:4.3f}\".format(value) for key, value in enumerate(mhz_values)}\n        ghz = {\"core{}g\".format(key): \"{0:1.2f}\".format(value) for key, value in enumerate(ghz_values)}\n        cdict = mhz.copy()\n        cdict.update(ghz)\n        cdict['avg'] = \"{0:4.3f}\".format(sum(mhz_values) / (len(mhz_values) - cpus_offline))\n        cdict['avgg'] = \"{0:1.2f}\".format(sum(ghz_values) / (len(ghz_values) - cpus_offline), 2)\n        return cdict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the URL used to connect to SABnzbd.", "response": "def init(self):\n        \"\"\"Initialize the URL used to connect to SABnzbd.\"\"\"\n        self.url = self.url.format(host=self.host, port=self.port,\n                                   api_key=self.api_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        try:\n            answer = urlopen(self.url + \"&mode=queue\").read().decode()\n        except (HTTPError, URLError) as error:\n            self.output = {\n                \"full_text\": str(error.reason),\n                \"color\": \"#FF0000\"\n            }\n            return\n\n        answer = json.loads(answer)\n\n        # if answer[\"status\"] exists and is False, an error occured\n        if not answer.get(\"status\", True):\n            self.output = {\n                \"full_text\": answer[\"error\"],\n                \"color\": \"#FF0000\"\n            }\n            return\n\n        queue = answer[\"queue\"]\n        self.status = queue[\"status\"]\n\n        if self.is_paused():\n            color = self.color_paused\n        elif self.is_downloading():\n            color = self.color_downloading\n        else:\n            color = self.color\n\n        if self.is_downloading():\n            full_text = self.format.format(**queue)\n        else:\n            full_text = self.format_paused.format(**queue)\n\n        self.output = {\n            \"full_text\": full_text,\n            \"color\": color\n        }", "response": "Connect to SABnzbd and get the data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntoggles between pausing or resuming downloading.", "response": "def pause_resume(self):\n        \"\"\"Toggle between pausing or resuming downloading.\"\"\"\n        if self.is_paused():\n            urlopen(self.url + \"&mode=resume\")\n        else:\n            urlopen(self.url + \"&mode=pause\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open_browser(self):\n        webbrowser.open(\n            \"http://{host}:{port}/\".format(host=self.host, port=self.port))", "response": "Open the URL of SABnzbd inside a browser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying nvidia - smi for a specific GPU.", "response": "def query_nvidia_smi(gpu_number) -> GPUUsageInfo:\n    \"\"\"\n    :return:\n        all memory fields are in megabytes,\n        temperature in degrees celsius,\n        fan speed is integer percent from 0 to 100 inclusive,\n        usage_gpu and usage_mem are integer percents from 0 to 100 inclusive\n        (usage_mem != used_mem, usage_mem is about read/write access load)\n        read more in 'nvidia-smi --help-query-gpu'.\n\n        Any field can be None if such information is not supported by nvidia-smi for current GPU\n\n        Returns None if call failed (no nvidia-smi or query format was changed)\n\n        Raises exception with readable comment\n    \"\"\"\n    params = [\"memory.total\", \"memory.free\", \"memory.used\",\n              \"temperature.gpu\", \"fan.speed\",\n              \"utilization.gpu\", \"utilization.memory\"]\n    try:\n        output = subprocess.check_output([\"nvidia-smi\",\n                                          \"--query-gpu={}\".format(','.join(params)),\n                                          \"--format=csv,noheader,nounits\"])\n    except FileNotFoundError:\n        raise Exception(\"No nvidia-smi\")\n    except subprocess.CalledProcessError:\n        raise Exception(\"nvidia-smi call failed\")\n\n    output = output.decode('utf-8').split(\"\\n\")[gpu_number].strip()\n    values = output.split(\", \")\n\n    # If value contains 'not' - it is not supported for this GPU (in fact, for now nvidia-smi returns '[Not Supported]')\n    values = [None if (\"not\" in value.lower()) else int(value) for value in values]\n\n    return GPUUsageInfo(*values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncapturing scrollup and scorlldown to move in groups", "response": "def on_click(self, button, **kwargs):\n        \"\"\"\n        Capture scrollup and scorlldown to move in groups\n        Pass everthing else to the module itself\n        \"\"\"\n        if button in (4, 5):\n            return super().on_click(button, **kwargs)\n        else:\n            activemodule = self.get_active_module()\n            if not activemodule:\n                return\n            return activemodule.on_click(button, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the system timezone for use when no timezone is explicitly provided. Requires pytz", "response": "def _get_system_tz(self):\n        '''\n        Get the system timezone for use when no timezone is explicitly provided\n\n        Requires pytz, if not available then no timezone will be set when not\n        explicitly provided.\n        '''\n        if not HAS_PYTZ:\n            return None\n\n        def _etc_localtime():\n            try:\n                with open('/etc/localtime', 'rb') as fp:\n                    return pytz.tzfile.build_tzinfo('system', fp)\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:\n                    self.logger.error(\n                        'Unable to read from /etc/localtime: %s', exc.strerror\n                    )\n            except pytz.UnknownTimeZoneError:\n                self.logger.error(\n                    '/etc/localtime contains unrecognized tzinfo'\n                )\n            return None\n\n        def _etc_timezone():\n            try:\n                with open('/etc/timezone', 'r') as fp:\n                    tzname = fp.read().strip()\n                return pytz.timezone(tzname)\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:\n                    self.logger.error(\n                        'Unable to read from /etc/localtime: %s', exc.strerror\n                    )\n            except pytz.UnknownTimeZoneError:\n                self.logger.error(\n                    '/etc/timezone contains unrecognized timezone \\'%s\\'',\n                    tzname\n                )\n            return None\n\n        return _etc_localtime() or _etc_timezone()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the weather using the configured backend", "response": "def check_weather(self):\n        '''\n        Check the weather using the configured backend\n        '''\n        self.output['full_text'] = \\\n            self.refresh_icon + self.output.get('full_text', '')\n        self.backend.check_weather()\n        self.refresh_display()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisambiguates similarly - named weather conditions and return the icon and color that match.", "response": "def get_color_data(self, condition):\n        '''\n        Disambiguate similarly-named weather conditions, and return the icon\n        and color that match.\n        '''\n        if condition not in self.color_icons:\n            # Check for similarly-named conditions if no exact match found\n            condition_lc = condition.lower()\n            if 'cloudy' in condition_lc or 'clouds' in condition_lc:\n                if 'partly' in condition_lc:\n                    condition = 'Partly Cloudy'\n                else:\n                    condition = 'Cloudy'\n            elif condition_lc == 'overcast':\n                condition = 'Cloudy'\n            elif 'thunder' in condition_lc or 't-storm' in condition_lc:\n                condition = 'Thunderstorm'\n            elif 'snow' in condition_lc:\n                condition = 'Snow'\n            elif 'rain' in condition_lc or 'showers' in condition_lc:\n                condition = 'Rainy'\n            elif 'sunny' in condition_lc:\n                condition = 'Sunny'\n            elif 'clear' in condition_lc or 'fair' in condition_lc:\n                condition = 'Fair'\n            elif 'fog' in condition_lc:\n                condition = 'Fog'\n\n        return self.color_icons['default'] \\\n            if condition not in self.color_icons \\\n            else self.color_icons[condition]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_cpu_timings(self):\n        timings = {}\n        with open('/proc/stat', 'r') as file_obj:\n            for line in file_obj:\n                if 'cpu' in line:\n                    line = line.strip().split()\n                    timings[line[0]] = [int(x) for x in line[1:]]\n\n        return timings", "response": "reads and parses / proc / stat\n        returns dictionary with all available cores including global average\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating usage of a specific CPU.", "response": "def calculate_usage(self, cpu, total, busy):\n        \"\"\"\n        calculates usage\n        \"\"\"\n        diff_total = total - self.prev_total[cpu]\n        diff_busy = busy - self.prev_busy[cpu]\n\n        self.prev_total[cpu] = total\n        self.prev_busy[cpu] = busy\n\n        if diff_total == 0:\n            return 0\n        else:\n            return int(diff_busy / diff_total * 100)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_format_all(self, usage):\n        format_string = \" \"\n        core_strings = []\n        for core, usage in usage.items():\n            if core == 'usage_cpu' and self.exclude_average:\n                continue\n            elif core == 'usage':\n                continue\n\n            core = core.replace('usage_', '')\n            string = self.formatter.format(self.format_all,\n                                           core=core,\n                                           usage=usage)\n            core_strings.append(string)\n\n        core_strings = sorted(core_strings)\n\n        return format_string.join(core_strings)", "response": "generates string for format all"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses /proc/stat and calcualtes total and busy time (more specific USER_HZ see man 5 proc for further informations )", "response": "def get_usage(self):\n        \"\"\"\n        parses /proc/stat and calcualtes total and busy time\n        (more specific USER_HZ see man 5 proc for further informations )\n        \"\"\"\n        usage = {}\n\n        for cpu, timings in self.get_cpu_timings().items():\n            cpu_total = sum(timings)\n            del timings[3:5]\n            cpu_busy = sum(timings)\n            cpu_usage = self.calculate_usage(cpu, cpu_total, cpu_busy)\n\n            usage['usage_' + cpu] = cpu_usage\n\n        # for backward compatibility\n        usage['usage'] = usage['usage_cpu']\n\n        return usage"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes the internal list of events from Google.", "response": "def refresh_events(self):\n        \"\"\"\n        Retrieve the next N events from Google.\n        \"\"\"\n        now = datetime.datetime.now(tz=pytz.UTC)\n        try:\n            now, later = self.get_timerange_formatted(now)\n            events_result = self.service.events().list(\n                calendarId='primary',\n                timeMin=now,\n                timeMax=later,\n                maxResults=10,\n                singleEvents=True,\n                orderBy='startTime',\n                timeZone='utc'\n            ).execute()\n            self.events.clear()\n            for event in events_result.get('items', []):\n                self.events.append(GoogleCalendarEvent(event))\n        except HttpError as e:\n            if e.resp.status in (500, 503):\n                self.logger.warn(\"GoogleCalendar received %s while retrieving events\" % e.resp.status)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_timerange_formatted(self, now):\n        later = now + datetime.timedelta(days=self.days)\n        return now.isoformat(), later.isoformat()", "response": "Return two ISO8601 formatted date strings one for timeMin and the other for timeMax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a prefix from a string", "response": "def lchop(string, prefix):\n    \"\"\"Removes a prefix from string\n\n    :param string: String, possibly prefixed with prefix\n    :param prefix: Prefix to remove from string\n    :returns: string without the prefix\n    \"\"\"\n    if string.startswith(prefix):\n        return string[len(prefix):]\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef popwhile(predicate, iterable):\n    while iterable:\n        item = iterable.pop()\n        if predicate(item):\n            yield item\n        else:\n            break", "response": "Generator function yielding items of iterable while predicate holds for each item"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrounds all values in a dict containing only numeric types to places decimal places.", "response": "def round_dict(dic, places):\n    \"\"\"\n    Rounds all values in a dict containing only numeric types to `places` decimal places.\n    If places is None, round to INT.\n    \"\"\"\n    if places is None:\n        for key, value in dic.items():\n            dic[key] = round(value)\n    else:\n        for key, value in dic.items():\n            dic[key] = round(value, places)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flatten(l):\n    l = list(l)\n    i = 0\n    while i < len(l):\n        while isinstance(l[i], list):\n            if not l[i]:\n                l.pop(i)\n                i -= 1\n                break\n            else:\n                l[i:i + 1] = l[i]\n        i += 1\n    return l", "response": "Flattens a hierarchy of nested lists into a single list containing all elements in order\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning for advanced formatting strings with partial formatting.", "response": "def formatp(string, **kwargs):\n    \"\"\"\n    Function for advanced format strings with partial formatting\n\n    This function consumes format strings with groups enclosed in brackets. A\n    group enclosed in brackets will only become part of the result if all fields\n    inside the group evaluate True in boolean contexts.\n\n    Groups can be nested. The fields in a nested group do not count as fields in\n    the enclosing group, i.e. the enclosing group will evaluate to an empty\n    string even if a nested group would be eligible for formatting. Nesting is\n    thus equivalent to a logical or of all enclosing groups with the enclosed\n    group.\n\n    Escaped brackets, i.e. \\\\\\\\[ and \\\\\\\\] are copied verbatim to output.\n\n    :param string: Format string\n    :param kwargs: keyword arguments providing data for the format string\n    :returns: Formatted string\n    \"\"\"\n\n    def build_stack(string):\n        \"\"\"\n        Builds a stack with OpeningBracket, ClosingBracket and String tokens.\n        Tokens have a level property denoting their nesting level.\n        They also have a string property containing associated text (empty for\n        all tokens but String tokens).\n        \"\"\"\n\n        class Token:\n            string = \"\"\n\n        class OpeningBracket(Token):\n            pass\n\n        class ClosingBracket(Token):\n            pass\n\n        class String(Token):\n            def __init__(self, str):\n                self.string = str\n\n        TOKENS = {\n            \"[\": OpeningBracket,\n            \"]\": ClosingBracket,\n        }\n\n        stack = []\n\n        # Index of next unconsumed char\n        next = 0\n        # Last consumed char\n        prev = \"\"\n        # Current char\n        char = \"\"\n        # Current level\n        level = 0\n\n        while next < len(string):\n            prev = char\n            char = string[next]\n            next += 1\n\n            if prev != \"\\\\\" and char in TOKENS:\n                token = TOKENS[char]()\n                token.index = next\n                if char == \"]\":\n                    level -= 1\n                token.level = level\n                if char == \"[\":\n                    level += 1\n                stack.append(token)\n            else:\n                if stack and isinstance(stack[-1], String):\n                    stack[-1].string += char\n                else:\n                    token = String(char)\n                    token.level = level\n                    stack.append(token)\n        return stack\n\n    def build_tree(items, level=0):\n        \"\"\"\n        Builds a list-of-lists tree (in forward order) from a stack (reversed order),\n        and formats the elements on the fly, discarding everything not eligible for\n        inclusion.\n        \"\"\"\n        subtree = []\n\n        while items:\n            nested = []\n            while items[0].level > level:\n                nested.append(items.pop(0))\n            if nested:\n                subtree.append(build_tree(nested, level + 1))\n\n            item = items.pop(0)\n            if item.string:\n                string = item.string\n                if level == 0:\n                    subtree.append(string.format(**kwargs))\n                else:\n                    fields = re.findall(r\"({(\\w+)[^}]*})\", string)\n                    successful_fields = 0\n                    for fieldspec, fieldname in fields:\n                        if kwargs.get(fieldname, False):\n                            successful_fields += 1\n                    if successful_fields == len(fields):\n                        subtree.append(string.format(**kwargs))\n                    else:\n                        return []\n        return subtree\n\n    def merge_tree(items):\n        return \"\".join(flatten(items)).replace(r\"\\]\", \"]\").replace(r\"\\[\", \"[\")\n\n    stack = build_stack(string)\n    tree = build_tree(stack, 0)\n    return merge_tree(tree)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_graph(values, lower_limit=0.0, upper_limit=100.0, style=\"blocks\"):\n\n    values = [float(n) for n in values]\n    mn, mx = min(values), max(values)\n    mn = mn if lower_limit is None else min(mn, float(lower_limit))\n    mx = mx if upper_limit is None else max(mx, float(upper_limit))\n    extent = mx - mn\n\n    if style == 'blocks':\n        bar = '_\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588'\n        bar_count = len(bar) - 1\n        if extent == 0:\n            graph = '_' * len(values)\n        else:\n            graph = ''.join(bar[int((n - mn) / extent * bar_count)] for n in values)\n    elif style in ['braille-fill', 'braille-peak', 'braille-snake']:\n        # idea from https://github.com/asciimoo/drawille\n        # unicode values from http://en.wikipedia.org/wiki/Braille\n\n        vpad = values if len(values) % 2 == 0 else values + [mn]\n        vscale = [round(4 * (vp - mn) / extent) for vp in vpad]\n        l = len(vscale) // 2\n\n        # do the 2-character collapse separately for clarity\n        if 'fill' in style:\n            vbits = [[0, 0x40, 0x44, 0x46, 0x47][vs] for vs in vscale]\n        elif 'peak' in style:\n            vbits = [[0, 0x40, 0x04, 0x02, 0x01][vs] for vs in vscale]\n        else:\n            assert('snake' in style)\n            # there are a few choices for what to put last in vb2.\n            # arguable vscale[-1] from the _previous_ call is best.\n            vb2 = [vscale[0]] + vscale + [0]\n            vbits = []\n            for i in range(1, l + 1):\n                c = 0\n                for j in range(min(vb2[i - 1], vb2[i], vb2[i + 1]), vb2[i] + 1):\n                    c |= [0, 0x40, 0x04, 0x02, 0x01][j]\n                vbits.append(c)\n\n        # 2-character collapse\n        graph = ''\n        for i in range(0, l, 2):\n            b1 = vbits[i]\n            b2 = vbits[i + 1]\n            if b2 & 0x40:\n                b2 = b2 - 0x30\n            b2 = b2 << 3\n            graph += chr(0x2800 + b1 + b2)\n    else:\n        raise NotImplementedError(\"Graph drawing style '%s' unimplemented.\" % style)\n    return graph", "response": "Creates a graph of unicode characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string that can be used to draw a vertical bar.", "response": "def make_vertical_bar(percentage, width=1):\n    \"\"\"\n    Draws a vertical bar made of unicode characters.\n\n    :param value: A value between 0 and 100\n    :param width: How many characters wide the bar should be.\n    :returns: Bar as a String\n    \"\"\"\n    bar = ' _\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588'\n    percentage //= 10\n    percentage = int(percentage)\n    if percentage < 0:\n        output = bar[0]\n    elif percentage >= len(bar):\n        output = bar[-1]\n    else:\n        output = bar[percentage]\n    return output * width"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string that is a bar of the given percentage.", "response": "def make_bar(percentage):\n    \"\"\"\n    Draws a bar made of unicode box characters.\n\n    :param percentage: A value between 0 and 100\n    :returns: Bar as a string\n    \"\"\"\n\n    bars = [' ', '\u258f', '\u258e', '\u258d', '\u258c', '\u258b', '\u258b', '\u258a', '\u258a', '\u2588']\n    tens = int(percentage / 10)\n    ones = int(percentage) - tens * 10\n    result = tens * '\u2588'\n    if(ones >= 1):\n        result = result + bars[ones]\n    result = result + (10 - len(result)) * ' '\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_glyph(number, glyphs=\"\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588\", lower_bound=0, upper_bound=100, enable_boundary_glyphs=False):\n\n    # Handle edge cases first\n    if lower_bound >= upper_bound:\n        raise Exception(\"Invalid upper/lower bounds\")\n    elif number <= lower_bound:\n        return glyphs[0]\n    elif number >= upper_bound:\n        return glyphs[-1]\n\n    if enable_boundary_glyphs:\n        # Trim first and last items from glyphs as boundary conditions already\n        # handled\n        glyphs = glyphs[1:-1]\n\n    # Determine a value 0 - 1 that represents the position in the range\n    adjusted_value = (number - lower_bound) / (upper_bound - lower_bound)\n\n    # Determine the closest glyph to show\n    # As we have positive indices, we can use int for floor rounding\n    # Adjusted_value should always be < 1\n    glyph_index = int(len(glyphs) * adjusted_value)\n\n    return glyphs[glyph_index]", "response": "Creates a glyph from a list of glyphs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_open(url_or_command):\n    from urllib.parse import urlparse\n    scheme = urlparse(url_or_command).scheme\n    if scheme == 'http' or scheme == 'https':\n        import webbrowser\n        import os\n        # webbrowser.open() sometimes prints a message for some reason and confuses i3\n        # Redirect stdout briefly to prevent this from happening.\n        savout = os.dup(1)\n        os.close(1)\n        os.open(os.devnull, os.O_RDWR)\n        try:\n            webbrowser.open(url_or_command)\n        finally:\n            os.dup2(savout, 1)\n    else:\n        import subprocess\n        subprocess.Popen(url_or_command, shell=True)", "response": "Open the specified paramater in the web browser."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_module(function):\n    @functools.wraps(function)\n    def call_wrapper(*args, **kwargs):\n        stack = inspect.stack()\n        caller_frame_info = stack[1]\n        self = caller_frame_info[0].f_locals[\"self\"]\n        # not completly sure whether this is necessary\n        # see note in Python docs about stack frames\n        del stack\n        function(self, *args, **kwargs)\n    return call_wrapper", "response": "Decorator for retrieving the self argument from the stack."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters all fundamental typekind handlers", "response": "def init_fundamental_types(self):\n        \"\"\"Registers all fundamental typekind handlers\"\"\"\n        for _id in range(2, 25):\n            setattr(self, TypeKind.from_id(_id).name,\n                    self._handle_fundamental_types)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle POD types nodes.", "response": "def _handle_fundamental_types(self, typ):\n        \"\"\"\n        Handles POD types nodes.\n        see init_fundamental_types for the registration.\n        \"\"\"\n        ctypesname = self.get_ctypes_name(typ.kind)\n        if typ.kind == TypeKind.VOID:\n            size = align = 1\n        else:\n            size = typ.get_size()\n            align = typ.get_align()\n        return typedesc.FundamentalType(ctypesname, size, align)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _array_handler(self, _cursor_type):\n        # The element type has been previously declared\n        # we need to get the canonical typedef, in some cases\n        _type = _cursor_type.get_canonical()\n        size = _type.get_array_size()\n        if size == -1 and _type.kind == TypeKind.INCOMPLETEARRAY:\n            size = 0\n            # FIXME: Incomplete Array handling at end of record.\n            # https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html\n            # FIXME VARIABLEARRAY DEPENDENTSIZEDARRAY\n        _array_type = _type.get_array_element_type()  # .get_canonical()\n        if self.is_fundamental_type(_array_type):\n            _subtype = self.parse_cursor_type(_array_type)\n        elif self.is_pointer_type(_array_type):\n            # code.interact(local=locals())\n            # pointers to POD have no declaration ??\n            # FIXME test_struct_with_pointer x_n_t g[1]\n            _subtype = self.parse_cursor_type(_array_type)\n        elif self.is_array_type(_array_type):\n            _subtype = self.parse_cursor_type(_array_type)\n        else:\n            _subtype_decl = _array_type.get_declaration()\n            _subtype = self.parse_cursor(_subtype_decl)\n            # if _subtype_decl.kind == CursorKind.NO_DECL_FOUND:\n            #    pass\n            #_subtype_name = self.get_unique_name(_subtype_decl)\n            #_subtype = self.get_registered(_subtype_name)\n        obj = typedesc.ArrayType(_subtype, size)\n        obj.location = _subtype.location\n        return obj", "response": "Handles all array types."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef FUNCTIONNOPROTO(self, _cursor_type):\n        # id, returns, attributes\n        returns = _cursor_type.get_result()\n        # if self.is_fundamental_type(returns):\n        returns = self.parse_cursor_type(returns)\n        attributes = []\n        obj = typedesc.FunctionType(returns, attributes)\n        # argument_types cant be asked. no arguments.\n        self.set_location(obj, None)\n        return obj", "response": "Handles function with no prototype."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle unexposed types. Returns the canonical type instead.", "response": "def UNEXPOSED(self, _cursor_type):\n        \"\"\"\n        Handles unexposed types.\n        Returns the canonical type instead.\n        \"\"\"\n        _decl = _cursor_type.get_declaration()\n        name = self.get_unique_name(_decl)  # _cursor)\n        if self.is_registered(name):\n            obj = self.get_registered(name)\n        else:\n            obj = self.parse_cursor(_decl)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef INIT_LIST_EXPR(self, cursor):\n        values = [self.parse_cursor(child)\n                  for child in list(cursor.get_children())]\n        return values", "response": "Returns a list of literal values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ENUM_CONSTANT_DECL(self, cursor):\n        name = cursor.displayname\n        value = cursor.enum_value\n        pname = self.get_unique_name(cursor.semantic_parent)\n        parent = self.get_registered(pname)\n        obj = typedesc.EnumValue(name, value, parent)\n        parent.add_value(obj)\n        return obj", "response": "Gets the enumeration values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the enumeration declaration.", "response": "def ENUM_DECL(self, cursor):\n        \"\"\"Gets the enumeration declaration.\"\"\"\n        name = self.get_unique_name(cursor)\n        if self.is_registered(name):\n            return self.get_registered(name)\n        align = cursor.type.get_align()\n        size = cursor.type.get_size()\n        obj = self.register(name, typedesc.Enumeration(name, size, align))\n        self.set_location(obj, cursor)\n        self.set_comment(obj, cursor)\n        # parse all children\n        for child in cursor.get_children():\n            self.parse_cursor(child)  # FIXME, where is the starElement\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a typedesc object from a Variable declaration.", "response": "def _VAR_DECL_type(self, cursor):\n        \"\"\"Generates a typedesc object from a Variable declaration.\"\"\"\n        # Get the type\n        _ctype = cursor.type.get_canonical()\n        log.debug('VAR_DECL: _ctype: %s ', _ctype.kind)\n        # FIXME: Need working int128, long_double, etc.\n        if self.is_fundamental_type(_ctype):\n            ctypesname = self.get_ctypes_name(_ctype.kind)\n            _type = typedesc.FundamentalType(ctypesname, 0, 0)\n        elif self.is_unexposed_type(_ctype):\n            st = 'PATCH NEEDED: %s type is not exposed by clang' % (\n                self.get_unique_name(cursor))\n            log.error(st)\n            raise RuntimeError(st)\n        elif self.is_array_type(_ctype) or _ctype.kind == TypeKind.RECORD:\n            _type = self.parse_cursor_type(_ctype)\n        elif self.is_pointer_type(_ctype):\n            # for example, extern Function pointer\n            if self.is_unexposed_type(_ctype.get_pointee()):\n                _type = self.parse_cursor_type(\n                    _ctype.get_canonical().get_pointee())\n            elif _ctype.get_pointee().kind == TypeKind.FUNCTIONPROTO:\n                # Function pointers\n                # Arguments are handled in here\n                _type = self.parse_cursor_type(_ctype.get_pointee())\n            else:  # Pointer to Fundamental types, structs....\n                _type = self.parse_cursor_type(_ctype)\n        else:\n            # What else ?\n            raise NotImplementedError(\n                'What other type of variable? %s' %\n                (_ctype.kind))\n        log.debug('VAR_DECL: _type: %s ', _type)\n        return _type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _VAR_DECL_value(self, cursor, _type):\n        # always expect list [(k,v)] as init value.from list(cursor.get_children())\n        # get the init_value and special cases\n        init_value = self._get_var_decl_init_value(cursor.type,\n                                                   list(cursor.get_children()))\n        _ctype = cursor.type.get_canonical()\n        if self.is_unexposed_type(_ctype):\n            # string are not exposed\n            init_value = '%s # UNEXPOSED TYPE. PATCH NEEDED.' % (init_value)\n        elif (self.is_pointer_type(_ctype) and\n                      _ctype.get_pointee().kind == TypeKind.FUNCTIONPROTO):\n            # Function pointers argument are handled at type creation time\n            # but we need to put a CFUNCTYPE as a value of the name variable\n            init_value = _type\n        elif self.is_array_type(_ctype):\n            # an integer litteral will be the size\n            # an string litteral will be the value\n            # any list member will be children of a init_list_expr\n            # FIXME Move that code into typedesc\n            def countof(k, l):\n                return [item[0] for item in l].count(k)\n\n            if (countof(CursorKind.INIT_LIST_EXPR, init_value) == 1):\n                init_value = dict(init_value)[CursorKind.INIT_LIST_EXPR]\n            elif (countof(CursorKind.STRING_LITERAL, init_value) == 1):\n                # we have a initialised c_array\n                init_value = dict(init_value)[CursorKind.STRING_LITERAL]\n            else:\n                # ignore size alone\n                init_value = []\n            # check the array size versus elements.\n            if _type.size < len(init_value):\n                _type.size = len(init_value)\n        elif init_value == []:\n            # catch case.\n            init_value = None\n        else:\n            log.debug('VAR_DECL: default init_value: %s', init_value)\n            if len(init_value) > 0:\n                init_value = init_value[0][1]\n        return init_value", "response": "Handles Variable value initialization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the initialisation value for a VAR_DECL.", "response": "def _get_var_decl_init_value(self, _ctype, children):\n        \"\"\"\n        Gathers initialisation values by parsing children nodes of a VAR_DECL.\n        \"\"\"\n\n        # FIXME TU for INIT_LIST_EXPR\n        # FIXME: always return [(child.kind,child.value),...]\n        # FIXME: simplify this redondant code.\n        init_value = []\n        children = list(children)  # weird requirement, list iterator error.\n        log.debug('_get_var_decl_init_value: children #: %d', len(children))\n        for child in children:\n            # early stop cases.\n            _tmp = None\n            try:\n                _tmp = self._get_var_decl_init_value_single(_ctype, child)\n            except CursorKindException:\n                log.debug(\n                    '_get_var_decl_init_value: children init value skip on %s',\n                    child.kind)\n                continue\n            if _tmp is not None:\n                init_value.append(_tmp)\n        return init_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles of a single child for initialization value. Accepted types are expressions and declarations", "response": "def _get_var_decl_init_value_single(self, _ctype, child):\n        \"\"\"\n        Handling of a single child for initialization value.\n        Accepted types are expressions and declarations\n        \"\"\"\n        init_value = None\n        # FIXME: always return (child.kind, child.value)\n        log.debug(\n            '_get_var_decl_init_value_single: _ctype: %s Child.kind: %s',\n            _ctype.kind,\n            child.kind)\n        # shorcuts.\n        if not child.kind.is_expression() and not child.kind.is_declaration():\n            raise CursorKindException(child.kind)\n        if child.kind == CursorKind.CALL_EXPR:\n            raise CursorKindException(child.kind)\n        # POD init values handling.\n        # As of clang 3.3, int, double literals are exposed.\n        # float, long double, char , char* are not exposed directly in level1.\n        # but really it depends...\n        if child.kind.is_unexposed():\n            # recurse until we find a literal kind\n            init_value = self._get_var_decl_init_value(_ctype,\n                                                       child.get_children())\n            if len(init_value) == 0:\n                init_value = None\n            elif len(init_value) == 1:\n                init_value = init_value[0]\n            else:\n                log.error('_get_var_decl_init_value_single: Unhandled case')\n                assert len(init_value) <= 1\n        else:  # literal or others\n            _v = self.parse_cursor(child)\n            if isinstance(\n                    _v, list) and child.kind not in [CursorKind.INIT_LIST_EXPR, CursorKind.STRING_LITERAL]:\n                log.warning(\n                    '_get_var_decl_init_value_single: TOKENIZATION BUG CHECK: %s',\n                    _v)\n                _v = _v[0]\n            init_value = (child.kind, _v)\n        log.debug(\n            '_get_var_decl_init_value_single: returns %s',\n            str(init_value))\n        return init_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse all literal associated with this cursor.", "response": "def _literal_handling(self, cursor):\n        \"\"\"Parse all literal associated with this cursor.\n\n        Literal handling is usually useful only for initialization values.\n\n        We can't use a shortcut by getting tokens\n            # init_value = ' '.join([t.spelling for t in children[0].get_tokens()\n            # if t.spelling != ';'])\n        because some literal might need cleaning.\"\"\"\n        # use a shortcut - does not work on unicode var_decl\n        # if cursor.kind == CursorKind.STRING_LITERAL:\n        #     value = cursor.displayname\n        #     value = self._clean_string_literal(cursor, value)\n        #     return value\n        tokens = list(cursor.get_tokens())\n        log.debug('literal has %d tokens.[ %s ]', len(tokens),\n                  str([str(t.spelling) for t in tokens]))\n        final_value = []\n        # code.interact(local=locals())\n        log.debug('cursor.type:%s', cursor.type.kind.name)\n        for i, token in enumerate(tokens):\n            value = token.spelling\n            log.debug('token:%s tk.kd:%11s tk.cursor.kd:%15s cursor.kd:%15s',\n                      token.spelling, token.kind.name, token.cursor.kind.name,\n                      cursor.kind.name)\n            # Punctuation is probably not part of the init_value,\n            # but only in specific case: ';' endl, or part of list_expr\n            if (token.kind == TokenKind.PUNCTUATION and\n                    (token.cursor.kind == CursorKind.INVALID_FILE or\n                             token.cursor.kind == CursorKind.INIT_LIST_EXPR)):\n                log.debug('IGNORE token %s', value)\n                continue\n            elif token.kind == TokenKind.COMMENT:\n                log.debug('Ignore comment %s', value)\n                continue\n            # elif token.cursor.kind == CursorKind.VAR_DECL:\n            elif token.location not in cursor.extent:\n                log.debug(\n                    'FIXME BUG: token.location not in cursor.extent %s',\n                    value)\n                # FIXME\n                # there is most probably a BUG in clang or python-clang\n                # when on #define with no value, a token is taken from\n                # next line. Which break stuff.\n                # example:\n                #   #define A\n                #   extern int i;\n                # // this will give \"extern\" the last token of Macro(\"A\")\n                # Lexer is choking ?\n                # FIXME BUG: token.location not in cursor.extent\n                # code.interact(local=locals())\n                continue\n            # Cleanup specific c-lang or c++ prefix/suffix for POD types.\n            if token.cursor.kind == CursorKind.INTEGER_LITERAL:\n                # strip type suffix for constants\n                value = value.replace('L', '').replace('U', '')\n                value = value.replace('l', '').replace('u', '')\n                if value[:2] == '0x' or value[:2] == '0X':\n                    value = '0x%s' % value[2:]  # \"int(%s,16)\"%(value)\n                else:\n                    value = int(value)\n            elif token.cursor.kind == CursorKind.FLOATING_LITERAL:\n                # strip type suffix for constants\n                value = value.replace('f', '').replace('F', '')\n                value = float(value)\n            elif (token.cursor.kind == CursorKind.CHARACTER_LITERAL or\n                          token.cursor.kind == CursorKind.STRING_LITERAL):\n                value = self._clean_string_literal(token.cursor, value)\n            elif token.cursor.kind == CursorKind.MACRO_INSTANTIATION:\n                # get the macro value\n                value = self.get_registered(value).body\n                # already cleaned value = self._clean_string_literal(token.cursor, value)\n            elif token.cursor.kind == CursorKind.MACRO_DEFINITION:\n                if i == 0:\n                    # ignore, macro name\n                    pass\n                elif token.kind == TokenKind.LITERAL:\n                    # and just clean it\n                    value = self._clean_string_literal(token.cursor, value)\n                elif token.kind == TokenKind.IDENTIFIER:\n                    # parse that, try to see if there is another Macro in there.\n                    value = self.get_registered(value).body\n\n            # add token\n            final_value.append(value)\n        # return the EXPR\n        # code.interact(local=locals())\n        if len(final_value) == 1:\n            return final_value[0]\n        # Macro definition of a string using multiple macro\n        if isinstance(final_value, list) and cursor.kind == CursorKind.STRING_LITERAL:\n            final_value = ''.join(final_value)\n        return final_value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string with the literal that are part of the operation.", "response": "def _operator_handling(self, cursor):\n        \"\"\"Returns a string with the literal that are part of the operation.\"\"\"\n        values = self._literal_handling(cursor)\n        retval = ''.join([str(val) for val in values])\n        return retval"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles Structure declaration. Its a wrapper to _record_decl.", "response": "def STRUCT_DECL(self, cursor, num=None):\n        \"\"\"\n        Handles Structure declaration.\n        Its a wrapper to _record_decl.\n        \"\"\"\n        return self._record_decl(cursor, typedesc.Structure, num)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef UNION_DECL(self, cursor, num=None):\n        return self._record_decl(cursor, typedesc.Union, num)", "response": "Handles Union declaration.\n        Its a wrapper to _record_decl."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _record_decl(self, cursor, _output_type, num=None):\n        name = self.get_unique_name(cursor)\n        # FIXME, handling anonymous field by adding a child id.\n        if num is not None:\n            name = \"%s_%d\", name, num\n        # TODO unittest: try redefinition.\n        # Find if a record definition was already parsed and registered\n        if (self.is_registered(name) and\n                    self.get_registered(name).members is not None):\n            log.debug(\n                '_record_decl: %s is already registered with members',\n                name)\n            return self.get_registered(name)\n        # FIXME: lets ignore bases for now.\n        # bases = attrs.get(\"bases\", \"\").split() # that for cpp ?\n        bases = []  # FIXME: support CXX\n        size = cursor.type.get_size()\n        align = cursor.type.get_align()\n        if size == -2: #\n            # CXTypeLayoutError_Incomplete = -2\n            # produce an empty structure declaration\n            size = align = 0\n            log.debug('_record_decl: name: %s CXTypeLayoutError_Incomplete', name)\n            obj = _output_type(name, align, None, bases, size, packed=False)\n            self.set_location(obj, cursor)\n            self.set_comment(obj, cursor)\n            return self.register(name, obj)\n\n        elif size < 0 or align < 0:\n            # CXTypeLayoutError_Invalid = -1,\n            # CXTypeLayoutError_Dependent = -3,\n            # CXTypeLayoutError_NotConstantSize = -4,\n            # CXTypeLayoutError_InvalidFieldName = -5\n            errs = dict([(-1, \"Invalid\"), (-3, \"Dependent\"),\n                         (-4, \"NotConstantSize\"), (-5, \"InvalidFieldName\")])\n            loc = \"%s:%s\" % (cursor.location.file, cursor.location.line)\n            log.error('Structure %s is %s %s align:%d size:%d',\n                      name, errs[size], loc, align, size)\n            raise InvalidDefinitionError('Structure %s is %s %s align:%d size:%d',\n                                         name, errs[size], loc, align, size)\n        else:\n            log.debug('_record_decl: name: %s size:%d', name, size)\n        # Declaration vs Definition point\n        # when a struct decl happen before the definition, we have no members\n        # in the first declaration instance.\n        obj = None\n        if not self.is_registered(name):\n            if not cursor.is_definition():\n                # just save the spot, don't look at members == None\n                log.debug('cursor %s is not on a definition', name)\n                obj = _output_type(name, align, None, bases, size, packed=False)\n                return self.register(name, obj)\n            else:\n                log.debug('cursor %s is a definition', name)\n                # save the type in the registry. Useful for not looping in case of\n                # members with forward references\n                obj = _output_type(name, align, None, bases, size, packed=False)\n                self.register(name, obj)\n                self.set_location(obj, cursor)\n                self.set_comment(obj, cursor)\n                declared_instance = True\n        else:\n            obj = self.get_registered(name)\n            declared_instance = False\n        # capture members declaration\n        members = []\n        # Go and recurse through fields\n        fields = list(cursor.type.get_fields())\n        decl_f = [f.type.get_declaration() for f in fields]\n        log.debug('Fields: %s',\n                  str(['%s/%s' % (f.kind.name, f.spelling) for f in fields]))\n        for field in fields:\n            log.debug('creating FIELD_DECL for %s/%s', field.kind.name, field.spelling)\n            members.append(self.FIELD_DECL(field))\n        # FIXME BUG clang: anonymous structure field with only one anonymous field\n        # is not a FIELD_DECL. does not appear in get_fields() !!!\n        #\n        # check for other stuff\n        for child in cursor.get_children():\n            if child in fields:\n                continue\n            elif child in decl_f:\n                continue\n            elif child.kind == CursorKind.PACKED_ATTR:\n                obj.packed = True\n                log.debug('PACKED record')\n                continue  # dont mess with field calculations\n            else:  # could be others.... struct_decl, etc...\n                log.debug(\n                    'Unhandled field %s in record %s',\n                    child.kind, name)\n                continue\n        log.debug('_record_decl: %d members', len(members))\n        # by now, the type is registered.\n        if not declared_instance:\n            log.debug('_record_decl: %s was previously registered', name)\n        obj = self.get_registered(name)\n        obj.members = members\n        # obj.packed = packed\n        # final fixup\n        self._fixup_record(obj)\n        return obj", "response": "Handles record type declaration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fixup_record_bitfields_type(self, s):\n        # phase 1, make bitfield, relying upon padding.\n        bitfields = []\n        bitfield_members = []\n        current_bits = 0\n        for m in s.members:\n            if m.is_bitfield:\n                bitfield_members.append(m)\n                if m.is_padding:\n                    # compiler says this ends the bitfield\n                    size = current_bits\n                    bitfields.append((size, bitfield_members))\n                    bitfield_members = []\n                    current_bits = 0\n                else:\n                    # size of padding is not included\n                    current_bits += m.bits\n            elif len(bitfield_members) == 0:\n                # no opened bitfield\n                continue\n            else:\n                # we reach the end of the bitfield. Make calculations.\n                size = current_bits\n                bitfields.append((size, bitfield_members))\n                bitfield_members = []\n                current_bits = 0\n        if current_bits != 0:\n            size = current_bits\n            bitfields.append((size, bitfield_members))\n\n        # compilers tend to reduce the size of the bitfield\n        # to the bf_size\n        # set the proper type name for the bitfield.\n        for bf_size, members in bitfields:\n            name = members[0].type.name\n            pad_bits = 0\n            if bf_size <= 8:  # use 1 byte - type = char\n                # prep the padding bitfield size\n                pad_bits = 8 - bf_size\n            elif bf_size <= 16:  # use 2 byte\n                pad_bits = 16 - bf_size\n            elif bf_size <= 32:  # use 2 byte\n                pad_bits = 32 - bf_size\n            elif bf_size <= 64:  # use 2 byte\n                name = 'c_uint64'  # also the 3 bytes + char thing\n                pad_bits = 64 - bf_size\n            else:\n                name = 'c_uint64'\n                pad_bits = bf_size % 64 - bf_size\n            # change the type to harmonise the bitfield\n            log.debug('_fixup_record_bitfield_size: fix type to %s', name)\n            # set the whole bitfield to the appropriate type size.\n            for m in members:\n                m.type.name = name\n                if m.is_padding:\n                    # this is the last field.\n                    # reduce the size of this padding field to the\n                    m.bits = pad_bits\n            # and remove padding if the size is 0\n            if members[-1].is_padding and members[-1].bits == 0:\n                s.members.remove(members[-1])\n\n        # phase 2 - integrate the special 3 Bytes + char fix\n        for bf_size, members in bitfields:\n            if True or bf_size == 24:\n                # we need to check for a 3bytes + char corner case\n                m = members[-1]\n                i = s.members.index(m)\n                if len(s.members) > i + 1:\n                    # has to exists, no arch is aligned on 24 bits.\n                    next_member = s.members[i + 1]\n                    if next_member.bits == 8:\n                        # next_member field is a char.\n                        # it will be aggregated in a 32 bits space\n                        # we need to make it a member of 32bit bitfield\n                        next_member.is_bitfield = True\n                        next_member.comment = \"Promoted to bitfield member and type (was char)\"\n                        next_member.type = m.type\n                        log.info(\"%s.%s promoted to bitfield member and type\", s.name, next_member.name)\n                        continue\n        #\n        return", "response": "This method fixes the bitfield packing issue for python ctypes by changing the bitfield type of the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes padding fields for a specifed size.", "response": "def _make_padding(\n            self, members, padding_nb, offset, length, prev_member=None):\n        \"\"\"Make padding Fields for a specifed size.\"\"\"\n        name = 'PADDING_%d' % padding_nb\n        padding_nb += 1\n        log.debug(\"_make_padding: for %d bits\", length)\n        if (length % 8) != 0 or (prev_member is not None and prev_member.is_bitfield):\n            # add a padding to align with the bitfield type\n            # then multiple bytes if required.\n            # pad_length = (length % 8)\n            typename = prev_member.type.name\n            padding = typedesc.Field(name,\n                                     typedesc.FundamentalType(typename, 1, 1),\n                                     # offset, pad_length, is_bitfield=True)\n                                     offset, length, is_bitfield=True, is_padding=True)\n            members.append(padding)\n            # check for multiple bytes\n            # if (length//8) > 0:\n            #    padding_nb = self._make_padding(members, padding_nb, offset+pad_length,\n            #                            (length//8)*8, prev_member=padding)\n            return padding_nb\n        elif length > 8:\n            pad_bytes = length // 8\n            padding = typedesc.Field(name,\n                                     typedesc.ArrayType(\n                                         typedesc.FundamentalType(\n                                             self.get_ctypes_name(TypeKind.CHAR_U), length, 1),\n                                         pad_bytes),\n                                     offset, length, is_padding=True)\n            members.append(padding)\n            return padding_nb\n        # simple char padding\n        padding = typedesc.Field(name,\n                                 typedesc.FundamentalType(\n                                     self.get_ctypes_name(\n                                         TypeKind.CHAR_U),\n                                     1,\n                                     1),\n                                 offset, length, is_padding=True)\n        members.append(padding)\n        return padding_nb"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling Field declarations. Some specific treatment for a bitfield.", "response": "def FIELD_DECL(self, cursor):\n        \"\"\"\n        Handles Field declarations.\n        Some specific treatment for a bitfield.\n        \"\"\"\n        # name, type\n        parent = cursor.semantic_parent\n        # field name:\n        # either its cursor.spelling or it is an anonymous field\n        # we do NOT rely on get_unique_name for a Field name.\n        # Anonymous Field:\n        #    We have to create a name\n        #    it will be the indice of the field (_0,_1,...)\n        # offset of field:\n        #    we will need it late. get the offset of the field in the record\n        name = cursor.spelling\n        # after dealing with anon bitfields, it could happen. an unnammed bitfield member is not is_anonymous()\n        if cursor.is_anonymous() or (name == '' and cursor.is_bitfield()):\n            # get offset by iterating all fields of parent\n            # corner case for anonymous fields\n            # if offset == -5: use field.get_offset_of()\n            offset = cursor.get_field_offsetof()\n            prev = fieldnum = -1\n            for i, _f in enumerate(parent.type.get_fields()):\n                if _f == cursor:\n                    fieldnum = i\n                    break\n                prev = _f\n            # make a name\n            if fieldnum == -1:\n                raise ValueError(\"Anonymous field was not found in get_fields()\")\n            name = \"_%d\" % fieldnum\n            log.debug(\"FIELD_DECL: anonymous field renamed to %s\", name)\n        else:\n            offset = parent.type.get_offset(name)\n        # some debug\n        if offset < 0:\n            log.error('FIELD_DECL: BAD RECORD, Bad offset: %d for %s', offset, name)\n            # incomplete record definition, gives us an error here on fields.\n            # BUG clang bindings ?\n        # FIXME if c++ class ?\n        log.debug('FIELD_DECL: field offset is %d', offset)\n\n        # bitfield checks\n        bits = None\n        if cursor.is_bitfield():\n            log.debug('FIELD_DECL: field is part of a bitfield')\n            bits = cursor.get_bitfield_width()\n        else:\n            bits = cursor.type.get_size() * 8\n            if bits < 0:\n                log.warning('Bad source code, bitsize == %d <0 on %s', bits, name)\n                bits = 0\n        log.debug('FIELD_DECL: field is %d bits', bits)\n        # try to get a representation of the type\n        # _canonical_type = cursor.type.get_canonical()\n        # t-t-t-t-\n        _type = None\n        _canonical_type = cursor.type.get_canonical()\n        _decl = cursor.type.get_declaration()\n        if (self.is_array_type(_canonical_type) or\n                self.is_fundamental_type(_canonical_type) or\n                self.is_pointer_type(_canonical_type)):\n            _type = self.parse_cursor_type(_canonical_type)\n        else:\n            children = list(cursor.get_children())\n            log.debug('FIELD_DECL: we now look for the declaration name.'\n                      'kind %s', _decl.kind)\n            if len(children) > 0 and _decl.kind == CursorKind.NO_DECL_FOUND:\n                # constantarray of typedef of pointer , and other cases ?\n                _decl_name = self.get_unique_name(\n                    list(\n                        cursor.get_children())[0])\n            else:\n                _decl_name = self.get_unique_name(_decl)\n            log.debug('FIELD_DECL: the declaration name %s', _decl_name)\n            # rename anonymous field type name\n            # 2015-06-26 handled in get_name\n            # if cursor.is_anonymous():\n            #    _decl_name += name\n            #    log.debug('FIELD_DECL: IS_ANONYMOUS the declaration name %s',_decl_name)\n            if self.is_registered(_decl_name):\n                log.debug(\n                    'FIELD_DECL: used type from cache: %s',\n                    _decl_name)\n                _type = self.get_registered(_decl_name)\n                # then we shortcut\n            else:\n                # is it always the case ?\n                log.debug(\"FIELD_DECL: name:'%s'\", _decl_name)\n                log.debug(\"FIELD_DECL: %s: nb children:%s\", cursor.type.kind,\n                          len(children))\n                # recurse into the right function\n                _type = self.parse_cursor_type(_canonical_type)\n                if _type is None:\n                    log.warning(\"Field %s is an %s type - ignoring field type\",\n                                name, _canonical_type.kind.name)\n                    return None\n        if cursor.is_anonymous():\n            # we have to unregister the _type and register a alternate named\n            # type.\n            self.parser.remove_registered(_type.name)\n            _type.name = _decl_name\n            self.register(_decl_name, _type)\n        return typedesc.Field(name, _type, offset, bits,\n                              is_bitfield=cursor.is_bitfield(),\n                              is_anonymous=cursor.is_anonymous())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a MACRO_DEFINITION statement.", "response": "def MACRO_DEFINITION(self, cursor):\n        \"\"\"\n        Parse MACRO_DEFINITION, only present if the TranslationUnit is\n        used with TranslationUnit.PARSE_DETAILED_PROCESSING_RECORD.\n        \"\"\"\n        # TODO: optionalize macro parsing. It takes a LOT of time.\n        # ignore system macro\n        if (not hasattr(cursor, 'location') or cursor.location is None or\n                cursor.location.file is None):\n            return False\n        name = self.get_unique_name(cursor)\n        # if name == 'A':\n        #    code.interact(local=locals())\n        # Tokens !!! .kind = {IDENTIFIER, KEYWORD, LITERAL, PUNCTUATION,\n        # COMMENT ? } etc. see TokenKinds.def\n        comment = None\n        tokens = self._literal_handling(cursor)\n        # Macro name is tokens[0]\n        # get Macro value(s)\n        value = True\n        if isinstance(tokens, list):\n            if len(tokens) == 2:\n                value = tokens[1]\n            else:\n                # just merge the list of tokens\n                value = ''.join(tokens[1:])\n        # macro comment maybe in tokens. Not in cursor.raw_comment\n        for t in cursor.get_tokens():\n            if t.kind == TokenKind.COMMENT:\n                comment = t.spelling\n        # special case. internal __null\n        # FIXME, there are probable a lot of others.\n        # why not Cursor.kind GNU_NULL_EXPR child instead of a token ?\n        if name == 'NULL' or value == '__null':\n            value = None\n        log.debug('MACRO: #define %s %s', tokens[0], value)\n        obj = typedesc.Macro(name, None, value)\n        try:\n            self.register(name, obj)\n        except DuplicateDefinitionException:\n            log.info(\n                'Redefinition of %s %s->%s',\n                name, self.parser.all[name].args, value)\n            # HACK\n            self.parser.all[name] = obj\n        self.set_location(obj, cursor)\n        # set the comment in the obj\n        obj.comment = comment\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the location of the user object.", "response": "def set_location(self, obj, cursor):\n        \"\"\" Location is also used for codegeneration ordering.\"\"\"\n        if (hasattr(cursor, 'location') and cursor.location is not None and\n                cursor.location.file is not None):\n            obj.location = (cursor.location.file.name, cursor.location.line)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_comment(self, obj, cursor):\n        if isinstance(obj, typedesc.T):\n            obj.comment = cursor.brief_comment\n        return", "response": "Set the comment of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms an USR into a valid python name.", "response": "def make_python_name(self, name):\n        \"\"\"Transforms an USR into a valid python name.\"\"\"\n        # FIXME see cindex.SpellingCache\n        for k, v in [('<', '_'), ('>', '_'), ('::', '__'), (',', ''), (' ', ''),\n                     (\"$\", \"DOLLAR\"), (\".\", \"DOT\"), (\"@\", \"_\"), (\":\", \"_\"),\n                     ('-', '_')]:\n            if k in name:  # template\n                name = name.replace(k, v)\n            # FIXME: test case ? I want this func to be neutral on C valid\n            # names.\n            if name.startswith(\"__\"):\n                return \"_X\" + name\n        if len(name) == 0:\n            pass\n        elif name[0] in \"01234567879\":\n            return \"_\" + name\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a name for unknown type.", "response": "def _make_unknown_name(self, cursor):\n        '''Creates a name for unname type'''\n        parent = cursor.lexical_parent\n        pname = self.get_unique_name(parent)\n        log.debug('_make_unknown_name: Got parent get_unique_name %s',pname)\n        # we only look at types declarations\n        _cursor_decl = cursor.type.get_declaration()\n        # we had the field index from the parent record, as to differenciate\n        # between unnamed siblings of a same struct\n        _i = 0\n        found = False\n        # Look at the parent fields to find myself\n        for m in parent.get_children():\n            # FIXME: make the good indices for fields\n            log.debug('_make_unknown_name child %d %s %s %s',_i,m.kind, m.type.kind,m.location)\n            if m.kind not in [CursorKind.STRUCT_DECL,CursorKind.UNION_DECL,\n                              CursorKind.CLASS_DECL]:#,\n                              #CursorKind.FIELD_DECL]:\n                continue\n            if m == _cursor_decl:\n                found = True\n                break\n            _i+=1\n        if not found:\n            raise NotImplementedError(\"_make_unknown_name BUG %s\"%cursor.location)\n        # truncate parent name to remove the first part (union or struct)\n        _premainer = '_'.join(pname.split('_')[1:])\n        name = '%s_%d'%(_premainer,_i)\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_unique_name(self, cursor):\n        name = ''\n        if cursor.kind in [CursorKind.UNEXPOSED_DECL]:\n            return ''\n        # covers most cases\n        name = cursor.spelling\n        # if its a record decl or field decl and its type is unnamed\n        if cursor.spelling == '':\n            # a unnamed object at the root TU\n            if (cursor.semantic_parent\n                and cursor.semantic_parent.kind == CursorKind.TRANSLATION_UNIT):\n                name = self.make_python_name(cursor.get_usr())\n                log.debug('get_unique_name: root unnamed type kind %s',cursor.kind)\n            elif cursor.kind in [CursorKind.STRUCT_DECL,CursorKind.UNION_DECL,\n                                 CursorKind.CLASS_DECL,CursorKind.FIELD_DECL]:\n                name = self._make_unknown_name(cursor)\n                log.debug('Unnamed cursor type, got name %s',name)\n            else:\n                log.debug('Unnamed cursor, No idea what to do')\n                #import code\n                #code.interact(local=locals())\n                return ''\n        if cursor.kind in [CursorKind.STRUCT_DECL,CursorKind.UNION_DECL,\n                                 CursorKind.CLASS_DECL]:\n            names= {CursorKind.STRUCT_DECL: 'struct',\n                    CursorKind.UNION_DECL: 'union',\n                    CursorKind.CLASS_DECL: 'class',\n                    CursorKind.TYPE_REF: ''}\n            name = '%s_%s'%(names[cursor.kind],name)\n        log.debug('get_unique_name: name \"%s\"',name)\n        return name", "response": "get the spelling or create a unique name for a cursor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_literal_kind_affinity(self, literal_kind):\n        ''' return the list of fundamental types that are adequate for which\n        this literal_kind is adequate'''\n        if literal_kind == CursorKind.INTEGER_LITERAL:\n            return [TypeKind.USHORT, TypeKind.UINT, TypeKind.ULONG,\n                    TypeKind.ULONGLONG, TypeKind.UINT128,\n                    TypeKind.SHORT, TypeKind.INT, TypeKind.LONG,\n                    TypeKind.LONGLONG, TypeKind.INT128, ]\n        elif literal_kind == CursorKind.STRING_LITERAL:\n            return [TypeKind.CHAR16, TypeKind.CHAR32, TypeKind.CHAR_S,\n                    TypeKind.SCHAR, TypeKind.WCHAR]  # DEBUG\n        elif literal_kind == CursorKind.CHARACTER_LITERAL:\n            return [TypeKind.CHAR_U, TypeKind.UCHAR]\n        elif literal_kind == CursorKind.FLOATING_LITERAL:\n            return [TypeKind.FLOAT, TypeKind.DOUBLE, TypeKind.LONGDOUBLE]\n        elif literal_kind == CursorKind.IMAGINARY_LITERAL:\n            return []\n        return []", "response": "return the list of fundamental types that are adequate for which\n        this literal_kind is adequate"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning true if source and target are newer than .", "response": "def is_newer(source, target):\n    \"\"\"Return true if 'source' exists and is more recently modified than\n    'target', or if 'source' exists and 'target' doesn't.  Return false if\n    both exist and 'target' is the same age or younger than 'source'.\n    Raise ValueError if 'source' does not exist.\n    \"\"\"\n    if not os.path.exists(source):\n        raise ValueError(\"file '%s' does not exist\" % source)\n    if not os.path.exists(target):\n        return 1\n\n    from stat import ST_MTIME\n    mtime1 = os.stat(source)[ST_MTIME]\n    mtime2 = os.stat(target)[ST_MTIME]\n\n    return mtime1 > mtime2"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads 1 file and parses it into a set of entries.", "response": "def parse(self, filename):\n        \"\"\"\n        . reads 1 file\n        . if there is a compilation error, print a warning\n        . get root cursor and recurse\n        . for each STRUCT_DECL, register a new struct type\n        . for each UNION_DECL, register a new union type\n        . for each TYPEDEF_DECL, register a new alias/typdef to the underlying type\n            - underlying type is cursor.type.get_declaration() for Record\n        . for each VAR_DECL, register a Variable\n        . for each TYPEREF ??\n        \"\"\"\n        index = Index.create()\n        self.tu = index.parse(filename, self.flags, options=self.tu_options)\n        if not self.tu:\n            log.warning(\"unable to load input\")\n            return\n        if len(self.tu.diagnostics) > 0:\n            for x in self.tu.diagnostics:\n                log.warning(x.spelling)\n                if x.severity > 2:\n                    log.warning(\"Source code has some error. Please fix.\")\n                    log.warning(x.spelling)\n                    # code.interact(local=locals())\n                    break\n        root = self.tu.cursor\n        for node in root.get_children():\n            self.startElement(node)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(self, name, obj):\n        if name in self.all:\n            log.debug('register: %s already existed: %s', name, obj.name)\n            # code.interact(local=locals())\n            raise DuplicateDefinitionException(\n                'register: %s already existed: %s' % (name, obj.name))\n        log.debug('register: %s ', name)\n        self.all[name] = obj\n        return obj", "response": "Registers an unique type description"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfixing clang types to ctypes convertion for this parsing isntance. Some architecture dependent size types ahve to be changed if the target architecture is not the same as local", "response": "def make_ctypes_convertor(self, _flags):\n        \"\"\"\n        Fix clang types to ctypes convertion for this parsing isntance.\n        Some architecture dependent size types ahve to be changed if the target\n        architecture is not the same as local\n        \"\"\"\n        tu = util.get_tu('''\ntypedef short short_t;\ntypedef int int_t;\ntypedef long long_t;\ntypedef long long longlong_t;\ntypedef float float_t;\ntypedef double double_t;\ntypedef long double longdouble_t;\ntypedef void* pointer_t;''', flags=_flags)\n        size = util.get_cursor(tu, 'short_t').type.get_size() * 8\n        self.ctypes_typename[TypeKind.SHORT] = 'c_int%d' % (size)\n        self.ctypes_typename[TypeKind.USHORT] = 'c_uint%d' % (size)\n        self.ctypes_sizes[TypeKind.SHORT] = size\n        self.ctypes_sizes[TypeKind.USHORT] = size\n\n        size = util.get_cursor(tu, 'int_t').type.get_size() * 8\n        self.ctypes_typename[TypeKind.INT] = 'c_int%d' % (size)\n        self.ctypes_typename[TypeKind.UINT] = 'c_uint%d' % (size)\n        self.ctypes_sizes[TypeKind.INT] = size\n        self.ctypes_sizes[TypeKind.UINT] = size\n\n        size = util.get_cursor(tu, 'long_t').type.get_size() * 8\n        self.ctypes_typename[TypeKind.LONG] = 'c_int%d' % (size)\n        self.ctypes_typename[TypeKind.ULONG] = 'c_uint%d' % (size)\n        self.ctypes_sizes[TypeKind.LONG] = size\n        self.ctypes_sizes[TypeKind.ULONG] = size\n\n        size = util.get_cursor(tu, 'longlong_t').type.get_size() * 8\n        self.ctypes_typename[TypeKind.LONGLONG] = 'c_int%d' % (size)\n        self.ctypes_typename[TypeKind.ULONGLONG] = 'c_uint%d' % (size)\n        self.ctypes_sizes[TypeKind.LONGLONG] = size\n        self.ctypes_sizes[TypeKind.ULONGLONG] = size\n\n        # FIXME : Float && http://en.wikipedia.org/wiki/Long_double\n        size0 = util.get_cursor(tu, 'float_t').type.get_size() * 8\n        size1 = util.get_cursor(tu, 'double_t').type.get_size() * 8\n        size2 = util.get_cursor(tu, 'longdouble_t').type.get_size() * 8\n        # 2014-01 stop generating crap.\n        # 2015-01 reverse until better solution is found\n        # the idea is that a you cannot assume a c_double will be same format as a c_long_double.\n        # at least this pass size TU\n        if size1 != size2:\n            self.ctypes_typename[TypeKind.LONGDOUBLE] = 'c_long_double_t'\n        else:\n            self.ctypes_typename[TypeKind.LONGDOUBLE] = 'c_double'\n\n        self.ctypes_sizes[TypeKind.FLOAT] = size0\n        self.ctypes_sizes[TypeKind.DOUBLE] = size1\n        self.ctypes_sizes[TypeKind.LONGDOUBLE] = size2\n\n        # save the target pointer size.\n        size = util.get_cursor(tu, 'pointer_t').type.get_size() * 8\n        self.ctypes_sizes[TypeKind.POINTER] = size\n        self.ctypes_sizes[TypeKind.NULLPTR] = size\n\n        log.debug('ARCH sizes: long:%s longdouble:%s',\n                  self.ctypes_typename[TypeKind.LONG],\n                  self.ctypes_typename[TypeKind.LONGDOUBLE])\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nobtain a translation unit from a source file.", "response": "def get_tu(source, lang='c', all_warnings=False, flags=None):\n    \"\"\"Obtain a translation unit from source and language.\n\n    By default, the translation unit is created from source file \"t.<ext>\"\n    where <ext> is the default file extension for the specified language. By\n    default it is C, so \"t.c\" is the default file name.\n\n    Supported languages are {c, cpp, objc}.\n\n    all_warnings is a convenience argument to enable all compiler warnings.\n    \"\"\"\n    args = list(flags or [])\n    name = 't.c'\n    if lang == 'cpp':\n        name = 't.cpp'\n        args.append('-std=c++11')\n    elif lang == 'objc':\n        name = 't.m'\n    elif lang != 'c':\n        raise Exception('Unknown language: %s' % lang)\n\n    if all_warnings:\n        args += ['-Wall', '-Wextra']\n\n    return TranslationUnit.from_source(name, args, unsaved_files=[(name,\n                                                                   source)])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cursor(source, spelling):\n    children = []\n    if isinstance(source, Cursor):\n        children = source.get_children()\n    else:\n        # Assume TU\n        children = source.cursor.get_children()\n\n    for cursor in children:\n        if cursor.spelling == spelling:\n            return cursor\n\n        # Recurse into children.\n        result = get_cursor(cursor, spelling)\n        if result is not None:\n            return result\n\n    return None", "response": "Obtain a cursor from a source object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nobtain all cursors from a source object with a specific spelling.", "response": "def get_cursors(source, spelling):\n    \"\"\"Obtain all cursors from a source object with a specific spelling.\n\n    This provides a convenient search mechanism to find all cursors with specific\n    spelling within a source. The first argument can be either a\n    TranslationUnit or Cursor instance.\n\n    If no cursors are found, an empty list is returned.\n    \"\"\"\n    cursors = []\n    children = []\n    if isinstance(source, Cursor):\n        children = source.get_children()\n    else:\n        # Assume TU\n        children = source.cursor.get_children()\n\n    for cursor in children:\n        if cursor.spelling == spelling:\n            cursors.append(cursor)\n\n        # Recurse into children.\n        cursors.extend(get_cursors(cursor, spelling))\n\n    return cursors"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable_fundamental_type_wrappers(self):\n        # 2015-01 reactivating header templates\n        #log.warning('enable_fundamental_type_wrappers deprecated - replaced by generate_headers')\n        # return # FIXME ignore\n        self.enable_fundamental_type_wrappers = lambda: True\n        import pkgutil\n        headers = pkgutil.get_data(\n            'ctypeslib',\n            'data/fundamental_type_name.tpl').decode()\n        from clang.cindex import TypeKind\n        size = str(self.parser.get_ctypes_size(TypeKind.LONGDOUBLE) // 8)\n        headers = headers.replace('__LONG_DOUBLE_SIZE__', size)\n        print(headers, file=self.imports)\n        return", "response": "Enable type wrapping for the fundamental types."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling the pointer type for the current locale.", "response": "def enable_pointer_type(self):\n        \"\"\"\n        If a type is a pointer, a platform-independent POINTER_T type needs\n        to be in the generated code.\n        \"\"\"\n        # 2015-01 reactivating header templates\n        #log.warning('enable_pointer_type deprecated - replaced by generate_headers')\n        # return # FIXME ignore\n        self.enable_pointer_type = lambda: True\n        import pkgutil\n        headers = pkgutil.get_data('ctypeslib', 'data/pointer_type.tpl').decode()\n        import ctypes\n        from clang.cindex import TypeKind\n        # assuming a LONG also has the same sizeof than a pointer.\n        word_size = self.parser.get_ctypes_size(TypeKind.POINTER) // 8\n        word_type = self.parser.get_ctypes_name(TypeKind.ULONG)\n        # pylint: disable=protected-access\n        word_char = getattr(ctypes, word_type)._type_\n        # replacing template values\n        headers = headers.replace('__POINTER_SIZE__', str(word_size))\n        headers = headers.replace('__REPLACEMENT_TYPE__', word_type)\n        headers = headers.replace('__REPLACEMENT_TYPE_CHAR__', word_char)\n        print(headers, file=self.imports)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string that can be used to refer to the type t.", "response": "def type_name(self, t, generate=True):\n        \"\"\"\n        Returns a string containing an expression that can be used to\n        refer to the type. Assumes the 'from ctypes import *'\n        namespace is available.\n        \"\"\"\n        # no Test case for these\n        # elif isinstance(t, typedesc.Argument):\n        # elif isinstance(t, typedesc.CvQualifiedType):\n        # elif isinstance(t, typedesc.Variable):\n        #   return \"%s\" % self.type_name(t.typ, generate)\n        # elif isinstance(t, typedesc.Enumeration):\n        #   return t.name\n\n        if isinstance(t, typedesc.FundamentalType):\n            return self.FundamentalType(t)\n        elif isinstance(t, typedesc.ArrayType):\n            return \"%s * %s\" % (self.type_name(t.typ, generate), t.size)\n        elif isinstance(t, typedesc.PointerType):\n            self.enable_pointer_type()\n            return \"POINTER_T(%s)\" % (self.type_name(t.typ, generate))\n        elif isinstance(t, typedesc.FunctionType):\n            args = [\n                self.type_name(\n                    x,\n                    generate) for x in [\n                    t.returns] +\n                list(\n                    t.iterArgTypes())]\n            if \"__stdcall__\" in t.attributes:\n                return \"ctypes.WINFUNCTYPE(%s)\" % \", \".join(args)\n            else:\n                return \"ctypes.CFUNCTYPE(%s)\" % \", \".join(args)\n        # elif isinstance(t, typedesc.Structure):\n        # elif isinstance(t, typedesc.Typedef):\n        # elif isinstance(t, typedesc.Union):\n        return t.name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling aliases. No test cases yet.", "response": "def Alias(self, alias):\n        \"\"\"Handles Aliases. No test cases yet\"\"\"\n        # FIXME\n        if self.generate_comments:\n            self.print_comment(alias)\n        print(\"%s = %s # alias\" % (alias.name, alias.alias), file=self.stream)\n        self._aliases += 1\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Macro(self, macro):\n        if macro.location is None:\n            log.info('Ignoring %s with no location', macro.name)\n            return\n        if self.generate_locations:\n            print(\"# %s:%s\" % (macro.location), file=self.stream)\n        if self.generate_comments:\n            self.print_comment(macro)\n        print(\"%s = %s # macro\" % (macro.name, macro.body), file=self.stream)\n        self.macros += 1\n        self.names.add(macro.name)\n        return", "response": "Handles a macro. No test cases else that defines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a typed has already been declared in the python output", "response": "def get_undeclared_type(self, item):\n        \"\"\"\n        Checks if a typed has already been declared in the python output\n        or is a builtin python type.\n        \"\"\"\n        if item in self.done:\n            return None\n        if isinstance(item, typedesc.FundamentalType):\n            return None\n        if isinstance(item, typedesc.PointerType):\n            return self.get_undeclared_type(item.typ)\n        if isinstance(item, typedesc.ArrayType):\n            return self.get_undeclared_type(item.typ)\n        # else its an undeclared structure.\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef FundamentalType(self, _type):\n        log.debug('HERE in FundamentalType for %s %s', _type, _type.name)\n        if _type.name in [\"None\", \"c_long_double_t\", \"c_uint128\", \"c_int128\"]:\n            self.enable_fundamental_type_wrappers()\n            return _type.name\n        return \"ctypes.%s\" % (_type.name)", "response": "Returns the proper ctypes class name for a fundamental type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate(self, item, *args):\n        if item in self.done:\n            return\n        # verbose output with location.\n        if self.generate_locations and item.location:\n            print(\"# %s:%d\" % item.location, file=self.stream)\n        if self.generate_comments:\n            self.print_comment(item)\n        log.debug(\"generate %s, %s\", item.__class__.__name__, item.name)\n        #\n        #log.debug('generate: %s( %s )', type(item).__name__, name)\n        #if name in self.known_symbols:\n        #    log.debug('item is in known_symbols %s'% name )\n        #    mod = self.known_symbols[name]\n        #    print >> self.imports, \"from %s import %s\" % (mod, name)\n        #    self.done.add(item)\n        #    if isinstance(item, typedesc.Structure):\n        #        self.done.add(item.get_head())\n        #        self.done.add(item.get_body())\n        #    return\n        #\n        # to avoid infinite recursion, we have to mark it as done\n        # before actually generating the code.\n        self.done.add(item)\n        # go to specific treatment\n        mth = getattr(self, type(item).__name__)\n        mth(item, *args)\n        return", "response": "wraps execution of specific methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots random phase and Gaussian random variable spectra.", "response": "def example():\n    \"\"\"Plot random phase and Gaussian random variable spectra.\"\"\"\n    ldata = 200\n    degrees = np.arange(ldata+1, dtype=float)\n    degrees[0] = np.inf\n    power = degrees**(-1)\n\n    clm1 = pyshtools.SHCoeffs.from_random(power, exact_power=False)\n    clm2 = pyshtools.SHCoeffs.from_random(power, exact_power=True)\n\n    fig, ax = plt.subplots()\n    ax.plot(clm1.spectrum(unit='per_l'), label='Normal distributed power')\n    ax.plot(clm2.spectrum(unit='per_l'), label='Exact power')\n    ax.set(xscale='log', yscale='log', xlabel='degree l',\n           ylabel='power per degree l')\n    ax.grid(which='both')\n    ax.legend()\n\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the radial component of the gravity field.", "response": "def plot_rad(self, colorbar=True, cb_orientation='vertical',\n                 cb_label='$g_r$, m s$^{-2}$', ax=None, show=True, fname=None,\n                 **kwargs):\n        \"\"\"\n        Plot the radial component of the gravity field.\n\n        Usage\n        -----\n        x.plot_rad([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$g_r$, m s$^{-2}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if ax is None:\n            fig, axes = self.rad.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.rad.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_theta(self, colorbar=True, cb_orientation='vertical',\n                   cb_label='$g_\\\\theta$, m s$^{-2}$', ax=None, show=True,\n                   fname=None, **kwargs):\n        \"\"\"\n        Plot the theta component of the gravity field.\n\n        Usage\n        -----\n        x.plot_theta([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$g_\\\\theta$, m s$^{-2}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if ax is None:\n            fig, axes = self.theta.plot(colorbar=colorbar,\n                                        cb_orientation=cb_orientation,\n                                        cb_label=cb_label, show=False,\n                                        **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.theta.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                            cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the theta component of the gravity field."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_phi(self, colorbar=True, cb_orientation='vertical',\n                 cb_label='$g_\\phi$, m s$^{-2}$', ax=None, show=True,\n                 fname=None, **kwargs):\n        \"\"\"\n        Plot the phi component of the gravity field.\n\n        Usage\n        -----\n        x.plot_phi([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$g_\\phi$, m s$^{-2}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if ax is None:\n            fig, axes = self.phi.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.phi.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the phi component of the gravity field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_total(self, colorbar=True, cb_orientation='vertical',\n                   cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the total gravity disturbance.\n\n        Usage\n        -----\n        x.plot_total([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = 'gravity disturbance'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n\n        Notes\n        -----\n        If the normal gravity is removed from the total gravitational\n        acceleration, the output will be displayed in mGals.\n        \"\"\"\n        if self.normal_gravity is True:\n            if cb_label is None:\n                cb_label = 'Gravity disturbance, mGal'\n        else:\n            if cb_label is None:\n                cb_label = 'Gravity disturbance, m s$^{-2}$'\n\n        if ax is None:\n            if self.normal_gravity is True:\n                fig, axes = (self.total*1.e5).plot(\n                    colorbar=colorbar, cb_orientation=cb_orientation,\n                    cb_label=cb_label, show=False, **kwargs)\n            else:\n                fig, axes = self.total.plot(\n                    colorbar=colorbar, cb_orientation=cb_orientation,\n                    cb_label=cb_label, show=False, **kwargs)\n\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            if self.normal_gravity is True:\n                (self.total*1.e5).plot(\n                    colorbar=colorbar, cb_orientation=cb_orientation,\n                    cb_label=cb_label, ax=ax, **kwargs)\n            else:\n                self.total.plot(\n                    colorbar=colorbar, cb_orientation=cb_orientation,\n                    cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the total gravity disturbance for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_pot(self, colorbar=True, cb_orientation='vertical',\n                 cb_label='Potential, m$^2$ s$^{-2}$', ax=None, show=True,\n                 fname=None, **kwargs):\n        \"\"\"\n        Plot the gravitational potential.\n\n        Usage\n        -----\n        x.plot_pot([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = 'potential, m s$^{-1}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if ax is None:\n            fig, axes = self.pot.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.pot.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the gravitational potential of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, colorbar=True, cb_orientation='horizontal',\n             tick_interval=[60, 60], minor_tick_interval=[20, 20],\n             xlabel='Longitude', ylabel='Latitude',\n             axes_labelsize=9, tick_labelsize=8, show=True, fname=None,\n             **kwargs):\n        \"\"\"\n        Plot the three vector components of the gravity field and the gravity\n        disturbance.\n\n        Usage\n        -----\n        x.plot([tick_interval, minor_tick_interval, xlabel, ylabel,\n                colorbar, cb_orientation, cb_label, axes_labelsize,\n                tick_labelsize, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [60, 60]\n            Intervals to use when plotting the major x and y ticks. If set to\n            None, major ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = [20, 20]\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude'\n            Label for the latitude axis.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        axes_labelsize : int, optional, default = 9\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = 8\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if colorbar is True:\n            if cb_orientation == 'horizontal':\n                scale = 0.8\n            else:\n                scale = 0.5\n        else:\n            scale = 0.6\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                    _mpl.rcParams['figure.figsize'][0] * scale)\n\n        fig, ax = _plt.subplots(2, 2, figsize=figsize)\n        self.plot_rad(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[0], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_theta(colorbar=colorbar, cb_orientation=cb_orientation,\n                        ax=ax.flat[1], tick_interval=tick_interval,\n                        xlabel=xlabel, ylabel=ylabel,\n                        axes_labelsize=axes_labelsize,\n                        tick_labelsize=tick_labelsize,\n                        minor_tick_interval=minor_tick_interval,\n                        **kwargs)\n        self.plot_phi(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[2], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      tick_labelsize=tick_labelsize,**kwargs)\n        self.plot_total(colorbar=colorbar, cb_orientation=cb_orientation,\n                        ax=ax.flat[3], tick_interval=tick_interval,\n                        xlabel=xlabel, ylabel=ylabel,\n                        axes_labelsize=axes_labelsize,\n                        tick_labelsize=tick_labelsize,\n                        minor_tick_interval=minor_tick_interval,\n                        **kwargs)\n        fig.tight_layout(pad=0.5)\n\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n        return fig, ax", "response": "Plots the image of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpand the function on a grid using the first n Slepian coefficients.", "response": "def expand(self, nmax=None, grid='DH2', zeros=None):\n        \"\"\"\n        Expand the function on a grid using the first n Slepian coefficients.\n\n        Usage\n        -----\n        f = x.expand([nmax, grid, zeros])\n\n        Returns\n        -------\n        f : SHGrid class instance\n\n        Parameters\n        ----------\n        nmax : int, optional, default = x.nmax\n            The number of expansion coefficients to use when calculating the\n            spherical harmonic coefficients.\n        grid : str, optional, default = 'DH2'\n            'DH' or 'DH1' for an equisampled lat/lon grid with nlat=nlon, 'DH2'\n            for an equidistant lat/lon grid with nlon=2*nlat, or 'GLQ' for a\n            Gauss-Legendre quadrature grid.\n        zeros : ndarray, optional, default = None\n            The cos(colatitude) nodes used in the Gauss-Legendre Quadrature\n            grids.\n        \"\"\"\n        if type(grid) != str:\n            raise ValueError('grid must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(grid))))\n\n        if nmax is None:\n            nmax = self.nmax\n\n        if self.galpha.kind == 'cap':\n            shcoeffs = _shtools.SlepianCoeffsToSH(self.falpha,\n                                                  self.galpha.coeffs, nmax)\n        else:\n            shcoeffs = _shtools.SlepianCoeffsToSH(self.falpha,\n                                                  self.galpha.tapers, nmax)\n\n        if grid.upper() in ('DH', 'DH1'):\n            gridout = _shtools.MakeGridDH(shcoeffs, sampling=1,\n                                          norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='DH', copy=False)\n        elif grid.upper() == 'DH2':\n            gridout = _shtools.MakeGridDH(shcoeffs, sampling=2,\n                                          norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='DH', copy=False)\n        elif grid.upper() == 'GLQ':\n            if zeros is None:\n                zeros, weights = _shtools.SHGLQ(self.galpha.lmax)\n            gridout = _shtools.MakeGridGLQ(shcoeffs, zeros,\n                                           norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='GLQ', copy=False)\n        else:\n            raise ValueError(\n                \"grid must be 'DH', 'DH1', 'DH2', or 'GLQ'. \" +\n                \"Input value was {:s}\".format(repr(grid)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the spherical harmonic coefficients of the current object.", "response": "def to_shcoeffs(self, nmax=None, normalization='4pi', csphase=1):\n        \"\"\"\n        Return the spherical harmonic coefficients using the first n Slepian\n        coefficients.\n\n        Usage\n        -----\n\n        s = x.to_shcoeffs([nmax])\n\n        Returns\n        -------\n        s : SHCoeffs class instance\n            The spherical harmonic coefficients obtained from using the first\n            n Slepian expansion coefficients.\n\n        Parameters\n        ----------\n        nmax : int, optional, default = x.nmax\n            The maximum number of expansion coefficients to use when\n            calculating the spherical harmonic coefficients.\n        normalization : str, optional, default = '4pi'\n            Normalization of the output class: '4pi', 'ortho' or 'schmidt' for\n            geodesy 4pi-normalized, orthonormalized, or Schmidt semi-normalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        \"\"\"\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in set(['4pi', 'ortho', 'schmidt']):\n            raise ValueError(\n                \"normalization must be '4pi', 'ortho' \" +\n                \"or 'schmidt'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        if nmax is None:\n            nmax = self.nmax\n\n        if self.galpha.kind == 'cap':\n            shcoeffs = _shtools.SlepianCoeffsToSH(self.falpha,\n                                                  self.galpha.coeffs, nmax)\n        else:\n            shcoeffs = _shtools.SlepianCoeffsToSH(self.falpha,\n                                                  self.galpha.tapers, nmax)\n\n        temp = SHCoeffs.from_array(shcoeffs, normalization='4pi', csphase=1)\n\n        if normalization != '4pi' or csphase != 1:\n            return temp.convert(normalization=normalization, csphase=csphase)\n        else:\n            return temp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the spectrum of the current state of the object.", "response": "def plot_spectrum(self, nmax=None, convention='power', unit='per_l',\n                      base=10., lmax=None, xscale='lin', yscale='log',\n                      grid=True, legend=None, axes_labelsize=None,\n                      tick_labelsize=None, show=True, ax=None, fname=None,\n                      **kwargs):\n        \"\"\"\n        Plot the spectrum as a function of spherical harmonic degree.\n\n        Usage\n        -----\n        x.plot_spectrum([nmax, convention, unit, base, lmax, xscale, yscale,\n                         grid, axes_labelsize, tick_labelsize, legend, show,\n                         ax, fname, **kwargs])\n\n        Parameters\n        ----------\n        nmax : int, optional, default = x.nmax\n            The maximum number of expansion coefficients to use when\n            calculating the spherical harmonic coefficients.\n        convention : str, optional, default = 'power'\n            The type of spectrum to plot: 'power' for power spectrum,\n            'energy' for energy spectrum, and 'l2norm' for the l2 norm\n            spectrum.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', plot the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', plot the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', plot the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum, and\n            the base to use for logarithmic axes.\n        lmax : int, optional, default = self.lmax\n            The maximum spherical harmonic degree to plot.\n        xscale : str, optional, default = 'lin'\n            Scale of the x axis: 'lin' for linear or 'log' for logarithmic.\n        yscale : str, optional, default = 'log'\n            Scale of the y axis: 'lin' for linear or 'log' for logarithmic.\n        grid : bool, optional, default = True\n            If True, plot grid lines.\n        legend : str, optional, default = None\n            Text to use for the legend.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot to the screen.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        **kwargs : keyword arguments, optional\n            Keyword arguments for pyplot.plot().\n\n        Description\n        -----------\n        This method plots either the power spectrum, energy spectrum, or\n        l2-norm spectrum. Total power is defined as the integral of the\n        function squared over all space, divided by the area the function\n        spans. If the mean of the function is zero, this is equivalent to the\n        variance of the function. The total energy is the integral of the\n        function squared over all space and is 4pi times the total power. For\n        normalized coefficients ('4pi', 'ortho', or 'schmidt'), the l2-norm is\n        the sum of the magnitude of the coefficients squared.\n\n        The output spectrum can be expresed using one of three units. 'per_l'\n        returns the contribution to the total spectrum from all angular orders\n        at degree l. 'per_lm' returns the average contribution to the total\n        spectrum from a single coefficient at degree l, which is equal to the\n        'per_l' spectrum divided by (2l+1). 'per_dlogl' returns the\n        contribution to the total spectrum from all angular orders over an\n        infinitessimal logarithmic degree band. The contrubution in the band\n        dlog_a(l) is spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base,\n        and where spectrum(l, 'per_dlogl) is equal to\n        spectrum(l, 'per_l')*l*log(a).\n        \"\"\"\n        temp = self.to_shcoeffs(nmax=nmax)\n\n        if ax is None:\n            fig, axes = temp.plot_spectrum(convention=convention, unit=unit,\n                                           base=base, lmax=lmax, xscale=xscale,\n                                           yscale=yscale, grid=grid,\n                                           axes_labelsize=axes_labelsize,\n                                           tick_labelsize=tick_labelsize,\n                                           legend=legend, show=show, ax=ax,\n                                           fname=fname, **kwargs)\n            return fig, axes\n        else:\n            temp.plot_spectrum(convention=convention, unit=unit,\n                               base=base, lmax=lmax, xscale=xscale,\n                               yscale=yscale, grid=grid,\n                               axes_labelsize=axes_labelsize,\n                               tick_labelsize=tick_labelsize,\n                               legend=legend, show=show, ax=ax,\n                               fname=fname, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining error message to print when a SHTOOLS Fortran 95 routine exits improperly.", "response": "def _shtools_status_message(status):\n    '''\n    Determine error message to print when a SHTOOLS Fortran 95 routine exits\n    improperly.\n    '''\n    if (status == 1):\n        errmsg = 'Improper dimensions of input array.'\n    elif (status == 2):\n        errmsg = 'Improper bounds for input variable.'\n    elif (status == 3):\n        errmsg = 'Error allocating memory.'\n    elif (status == 4):\n        errmsg = 'File IO error.'\n    else:\n        errmsg = 'Unhandled Fortran 95 error.'\n    return errmsg"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a new instance of the SlepianCap class from the given parameters.", "response": "def from_cap(cls, theta, lmax, clat=None, clon=None, nmax=None,\n                 theta_degrees=True, coord_degrees=True, dj_matrix=None):\n        \"\"\"\n        Construct spherical cap Slepian functions.\n\n        Usage\n        -----\n        x = Slepian.from_cap(theta, lmax, [clat, clon, nmax, theta_degrees,\n                                           coord_degrees, dj_matrix])\n\n        Returns\n        -------\n        x : Slepian class instance\n\n        Parameters\n        ----------\n        theta : float\n            Angular radius of the spherical-cap localization domain (default\n            in degrees).\n        lmax : int\n            Spherical harmonic bandwidth of the Slepian functions.\n        clat, clon : float, optional, default = None\n            Latitude and longitude of the center of the rotated spherical-cap\n            Slepian functions (default in degrees).\n        nmax : int, optional, default (lmax+1)**2\n            Number of Slepian functions to compute.\n        theta_degrees : bool, optional, default = True\n            True if theta is in degrees.\n        coord_degrees : bool, optional, default = True\n            True if clat and clon are in degrees.\n        dj_matrix : ndarray, optional, default = None\n            The djpi2 rotation matrix computed by a call to djpi2.\n        \"\"\"\n        if theta_degrees:\n            tapers, eigenvalues, taper_order = _shtools.SHReturnTapers(\n                _np.radians(theta), lmax)\n        else:\n            tapers, eigenvalues, taper_order = _shtools.SHReturnTapers(\n                theta, lmax)\n\n        return SlepianCap(theta, tapers, eigenvalues, taper_order, clat, clon,\n                          nmax, theta_degrees, coord_degrees, dj_matrix,\n                          copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a Slepian class instance from a mask of spherical harmonic bandwidth and a sampled grid of the Slepian functions.", "response": "def from_mask(cls, dh_mask, lmax, nmax=None):\n        \"\"\"\n        Construct Slepian functions that are optimally concentrated within\n        the region specified by a mask.\n\n        Usage\n        -----\n        x = Slepian.from_mask(dh_mask, lmax, [nmax])\n\n        Returns\n        -------\n        x : Slepian class instance\n\n        Parameters\n        ----------\n        dh_mask :ndarray, shape (nlat, nlon)\n            A Driscoll and Healy (1994) sampled grid describing the\n            concentration region R. All elements should either be 1 (for inside\n            the concentration region) or 0 (for outside the concentration\n            region). The grid must have dimensions nlon=nlat or nlon=2*nlat,\n            where nlat is even.\n        lmax : int\n            The spherical harmonic bandwidth of the Slepian functions.\n        nmax : int, optional, default = (lmax+1)**2\n            The number of best-concentrated eigenvalues and eigenfunctions to\n            return.\n        \"\"\"\n        if nmax is None:\n            nmax = (lmax + 1)**2\n        else:\n            if nmax > (lmax + 1)**2:\n                raise ValueError('nmax must be less than or equal to ' +\n                                 '(lmax + 1)**2. lmax = {:d} and nmax = {:d}'\n                                 .format(lmax, nmax))\n\n        if dh_mask.shape[0] % 2 != 0:\n            raise ValueError('The number of latitude bands in dh_mask ' +\n                             'must be even. nlat = {:d}'\n                             .format(dh_mask.shape[0]))\n\n        if dh_mask.shape[1] == dh_mask.shape[0]:\n            _sampling = 1\n        elif dh_mask.shape[1] == 2 * dh_mask.shape[0]:\n            _sampling = 2\n        else:\n            raise ValueError('dh_mask must be dimensioned as (n, n) or ' +\n                             '(n, 2 * n). Input shape is ({:d}, {:d})'\n                             .format(dh_mask.shape[0], dh_mask.shape[1]))\n\n        mask_lm = _shtools.SHExpandDH(dh_mask, sampling=_sampling, lmax_calc=0)\n        area = mask_lm[0, 0, 0] * 4 * _np.pi\n\n        tapers, eigenvalues = _shtools.SHReturnTapersMap(dh_mask, lmax,\n                                                         ntapers=nmax)\n\n        return SlepianMask(tapers, eigenvalues, area, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expand(self, flm, nmax=None):\n        if nmax is None:\n            nmax = (self.lmax+1)**2\n        elif nmax is not None and nmax > (self.lmax+1)**2:\n            raise ValueError(\n                \"nmax must be less than or equal to (lmax+1)**2 \" +\n                \"where lmax is {:s}. Input value is {:s}\"\n                .format(repr(self.lmax), repr(nmax))\n                )\n\n        coeffsin = flm.to_array(normalization='4pi', csphase=1, lmax=self.lmax)\n\n        return self._expand(coeffsin, nmax)", "response": "Returns the expansion coefficients of the global function f using the spherical harmonic function flm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spectra(self, alpha=None, nmax=None, convention='power', unit='per_l',\n                base=10.):\n        \"\"\"\n        Return the spectra of one or more Slepian functions.\n\n        Usage\n        -----\n        spectra = x.spectra([alpha, nmax, convention, unit, base])\n\n        Returns\n        -------\n        spectra : ndarray, shape (lmax+1, nmax)\n             A matrix with each column containing the spectrum of a Slepian\n             function, and where the functions are arranged with increasing\n             concentration factors. If alpha is set, only a single vector is\n             returned, whereas if nmax is set, the first nmax spectra are\n             returned.\n\n        Parameters\n        ----------\n        alpha : int, optional, default = None\n            The function number of the output spectrum, where alpha=0\n            corresponds to the best concentrated Slepian function.\n        nmax : int, optional, default = 1\n            The number of best concentrated Slepian function power spectra\n            to return.\n        convention : str, optional, default = 'power'\n            The type of spectrum to return: 'power' for power spectrum,\n            'energy' for energy spectrum, and 'l2norm' for the l2 norm\n            spectrum.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', return the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', return the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', return the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum.\n\n        Description\n        -----------\n        This function returns either the power spectrum, energy spectrum, or\n        l2-norm spectrum of one or more of the Slepian funtions. Total power\n        is defined as the integral of the function squared over all space,\n        divided by the area the function spans. If the mean of the function is\n        zero, this is equivalent to the variance of the function. The total\n        energy is the integral of the function squared over all space and is\n        4pi times the total power. The l2-norm is the sum of the magnitude of\n        the coefficients squared.\n\n        The output spectrum can be expresed using one of three units. 'per_l'\n        returns the contribution to the total spectrum from all angular orders\n        at degree l. 'per_lm' returns the average contribution to the total\n        spectrum from a single coefficient at degree l. The 'per_lm' spectrum\n        is equal to the 'per_l' spectrum divided by (2l+1). 'per_dlogl' returns\n        the contribution to the total spectrum from all angular orders over an\n        infinitessimal logarithmic degree band. The contrubution in the band\n        dlog_a(l) is spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base,\n        and where spectrum(l, 'per_dlogl) is equal to\n        spectrum(l, 'per_l')*l*log(a).\n\n         \"\"\"\n        if alpha is None:\n            if nmax is None:\n                nmax = self.nmax\n            spectra = _np.zeros((self.lmax+1, nmax))\n\n            for iwin in range(nmax):\n                coeffs = self.to_array(iwin)\n                spectra[:, iwin] = _spectrum(coeffs, normalization='4pi',\n                                             convention=convention, unit=unit,\n                                             base=base)\n        else:\n            coeffs = self.to_array(alpha)\n            spectra = _spectrum(coeffs, normalization='4pi',\n                                convention=convention, unit=unit, base=base)\n\n        return spectra", "response": "Return the spectra of one or more Slepian functions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_spectra(self, nmax, convention='power', unit='per_l', base=10.,\n                     maxcolumns=3, xscale='lin', yscale='log', grid=True,\n                     xlim=(None, None), ylim=(None, None), show=True,\n                     title=True, axes_labelsize=None, tick_labelsize=None,\n                     title_labelsize=None, ax=None, fname=None):\n        \"\"\"\n        Plot the spectra of the best-concentrated Slepian functions.\n\n        Usage\n        -----\n        x.plot_spectra(nmax, [convention, unit, base, maxcolumns, xscale,\n                              yscale, grid, xlim, ylim, show, title,\n                              axes_labelsize, tick_labelsize, title_labelsize,\n                              ax, fname])\n\n        Parameters\n        ----------\n        nmax : int\n            The number of Slepian functions to plot.\n        convention : str, optional, default = 'power'\n            The type of spectra to plot: 'power' for power spectrum, and\n            'energy' for energy spectrum.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', return the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', return the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', return the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum.\n        maxcolumns : int, optional, default = 3\n            The maximum number of columns to use when plotting the spectra\n            of multiple localization windows.\n        xscale : str, optional, default = 'lin'\n            Scale of the x axis: 'lin' for linear or 'log' for logarithmic.\n        yscale : str, optional, default = 'log'\n            Scale of the y axis: 'lin' for linear or 'log' for logarithmic.\n        grid : bool, optional, default = True\n            If True, plot grid lines.\n        xlim : tuple, optional, default = (None, None)\n            The upper and lower limits used for the x axis.\n        ylim : tuple, optional, default = (None, None)\n            The lower and upper limits used for the y axis.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        title : bool, optional, default = True\n            If True, plot a legend on top of each subplot providing the taper\n            number and 1 minus the concentration factor.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        title_labelsize : int, optional, default = None\n            The font size for the subplot titles.\n        ax : matplotlib axes object, optional, default = None\n            An array of matplotlib axes objects where the plots will appear.\n        fname : str, optional, default = None\n            If present, save the image to the file.\n        \"\"\"\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n        if title_labelsize is None:\n            title_labelsize = _mpl.rcParams['axes.titlesize']\n\n        degrees = self.degrees()\n        spectrum = self.spectra(nmax=nmax, convention=convention, unit=unit,\n                                base=base)\n\n        ncolumns = min(maxcolumns, nmax)\n        nrows = _np.ceil(nmax / ncolumns).astype(int)\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                   _mpl.rcParams['figure.figsize'][0]\n                   * 0.7 * nrows / ncolumns + 0.41)\n\n        if ax is None:\n            fig, axes = _plt.subplots(nrows, ncolumns, figsize=figsize,\n                                      sharex='all', sharey='all')\n        else:\n            if hasattr(ax, 'flatten') and ax.size < nmax:\n                raise ValueError('ax.size must be greater or equal to nmax. ' +\n                                 'nmax = {:s}'.format(repr(nmax)) +\n                                 ' and ax.size = {:s}.'.format(repr(ax.size)))\n            axes = ax\n\n        if ax is None:\n            if nrows > 1:\n                for axtemp in axes[:-1, :].flatten():\n                    for xlabel_i in axtemp.get_xticklabels():\n                        xlabel_i.set_visible(False)\n                    axtemp.set_xlabel('', visible=False)\n                for axtemp in axes[:, 1:].flatten():\n                    for ylabel_i in axtemp.get_yticklabels():\n                        ylabel_i.set_visible(False)\n                    axtemp.set_ylabel('', visible=False)\n            elif nmax > 1:\n                for axtemp in axes[1:].flatten():\n                    for ylabel_i in axtemp.get_yticklabels():\n                        ylabel_i.set_visible(False)\n                    axtemp.set_ylabel('', visible=False)\n\n        if ylim == (None, None):\n            upper = spectrum[:, :min(self.nmax, nmax)].max()\n            lower = upper * 1.e-6\n            ylim = (lower, 5 * upper)\n        if xlim == (None, None):\n            if xscale == 'lin':\n                xlim = (degrees[0], degrees[-1])\n\n        for alpha in range(min(self.nmax, nmax)):\n            evalue = self.eigenvalues[alpha]\n            if min(self.nmax, nmax) == 1 and ax is None:\n                axtemp = axes\n            elif hasattr(axes, 'flatten'):\n                axtemp = axes.flatten()[alpha]\n            else:\n                axtemp = axes[alpha]\n            if (convention == 'power'):\n                axtemp.set_ylabel('Power', fontsize=axes_labelsize)\n            else:\n                axtemp.set_ylabel('Energy', fontsize=axes_labelsize)\n\n            if yscale == 'log':\n                axtemp.set_yscale('log', basey=base)\n\n            if xscale == 'log':\n                axtemp.set_xscale('log', basex=base)\n                axtemp.plot(degrees[1:], spectrum[1:, alpha],\n                            label='#{:d} [loss={:2.2g}]'\n                            .format(alpha, 1-evalue))\n            else:\n                axtemp.plot(degrees[0:], spectrum[0:, alpha],\n                            label='#{:d} [loss={:2.2g}]'\n                            .format(alpha, 1-evalue))\n            axtemp.set_xlabel('Spherical harmonic degree',\n                              fontsize=axes_labelsize)\n            axtemp.set(xlim=xlim, ylim=ylim)\n            axtemp.minorticks_on()\n            axtemp.grid(grid, which='major')\n            axtemp.tick_params(labelsize=tick_labelsize)\n            if title is True:\n                axtemp.set_title('#{:d} [loss={:2.2g}]'\n                                 .format(alpha, 1-evalue),\n                                 fontsize=title_labelsize)\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes", "response": "Plot the spectra of the best - concentrated Slepian functions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpand the Slepian coefficients of a function.", "response": "def _expand(self, coeffsin, nmax):\n        \"\"\"\n        Determine the Slepian expansion coefficients of a function.\n        \"\"\"\n        if self.coeffs is None:\n            self.rotate(clat=90., clon=0., nrot=nmax)\n            falpha = _shtools.SlepianCoeffs(self.coeffs, coeffsin, self.nrot)\n        else:\n            falpha = _shtools.SlepianCoeffs(self.coeffs, coeffsin, self.nrot)\n\n        return SlepianCoeffs(falpha, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _taper2coeffs(self, alpha):\n        taperm = self.orders[alpha]\n        coeffs = _np.zeros((2, self.lmax + 1, self.lmax + 1))\n        if taperm < 0:\n            coeffs[1, :, abs(taperm)] = self.tapers[:, alpha]\n        else:\n            coeffs[0, :, abs(taperm)] = self.tapers[:, alpha]\n\n        return coeffs", "response": "Return the spherical harmonic coefficients of the unrotated Slepian\n        function i as an array where i = 0 is the best concentrated function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the spherical harmonic coefficients of Slepian function i as an array.", "response": "def _to_array(self, alpha, normalization='4pi', csphase=1):\n        \"\"\"\n        Return the spherical harmonic coefficients of Slepian function i as an\n        array, where i = 0 is the best concentrated function.\n        \"\"\"\n        if self.coeffs is None:\n            coeffs = _np.copy(self._taper2coeffs(alpha))\n        else:\n            if alpha > self.nrot - 1:\n                raise ValueError('alpha must be less than or equal to ' +\n                                 'nrot - 1. alpha = {:d}, nrot = {:d}'\n                                 .format(alpha, self.nrot))\n            coeffs = _shtools.SHVectorToCilm(self.coeffs[:, alpha])\n\n        if normalization == 'schmidt':\n            for l in range(self.lmax + 1):\n                coeffs[:, l, :l+1] *= _np.sqrt(2.0 * l + 1.0)\n        elif normalization == 'ortho':\n            coeffs *= _np.sqrt(4.0 * _np.pi)\n\n        if csphase == -1:\n            for m in range(self.lmax + 1):\n                if m % 2 == 1:\n                    coeffs[:, :, m] = - coeffs[:, :, m]\n\n        return coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _expand(self, coeffsin, nmax):\n        falpha = _shtools.SlepianCoeffs(self.tapers, coeffsin, nmax)\n\n        return SlepianCoeffs(falpha, self)", "response": "Expands the Slepian coefficients of a function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_array(self, alpha, normalization='4pi', csphase=1):\n        coeffs = _shtools.SHVectorToCilm(self.tapers[:, alpha])\n\n        if normalization == 'schmidt':\n            for l in range(self.lmax + 1):\n                coeffs[:, l, :l+1] *= _np.sqrt(2.0 * l + 1.0)\n        elif normalization == 'ortho':\n            coeffs *= _np.sqrt(4.0 * _np.pi)\n\n        if csphase == -1:\n            for m in range(self.lmax + 1):\n                if m % 2 == 1:\n                    coeffs[:, :, m] = - coeffs[:, :, m]\n\n        return coeffs", "response": "Return the spherical harmonic coefficients of Slepian function i as an array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the total magnetic intensity of the entry.", "response": "def plot_total(self, colorbar=True, cb_orientation='vertical',\n                   cb_label='$|B|$, nT', ax=None, show=True, fname=None,\n                   **kwargs):\n        \"\"\"\n        Plot the total magnetic intensity.\n\n        Usage\n        -----\n        x.plot_total([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$|B|$, nT'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if ax is None:\n            fig, axes = self.total.plot(\n                colorbar=colorbar, cb_orientation=cb_orientation,\n                cb_label=cb_label, show=False, **kwargs)\n\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.total.plot(\n                colorbar=colorbar, cb_orientation=cb_orientation,\n                cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spharm(lmax, theta, phi, normalization='4pi', kind='real', csphase=1,\n           packed=False, degrees=True):\n    \"\"\"\n    Compute all the spherical harmonic functions up to a maximum degree.\n\n    Usage\n    -----\n    ylm = spharm (lmax, theta, phi, [normalization, kind, csphase, packed,\n                                     degrees])\n\n    Returns\n    -------\n    ylm : float or complex, dimension (2, lmax+1, lmax+1) or\n                                      (2, (lmax+1)*(lmax+2)/2)\n        An array of spherical harmonic functions, ylm[i, l, m], where l and m\n        are the spherical harmonic degree and (positive) order, respectively.\n        The index i provides the positive (0) and negative (1) order. If packed\n        is True, the array is 2-dimensional with the index of the second column\n        corresponding to l*(l+1)/2+m.\n\n    Parameters\n    ----------\n    lmax : integer\n        The maximum degree of the spherical harmonic functions to be computed.\n    theta : float\n        The colatitude in degrees.\n    phi : float\n        The longitude in degrees.\n    normalization : str, optional, default = '4pi'\n        '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n        orthonormalized, Schmidt semi-normalized, or unnormalized spherical\n        harmonic functions, respectively.\n    kind : str, optional, default = 'real'\n        'real' or 'complex' spherical harmonic coefficients.\n    csphase : optional, integer, default = 1\n        If 1 (default), the Condon-Shortley phase will be excluded. If -1, the\n        Condon-Shortley phase of (-1)^m will be appended to the spherical\n        harmonic functions.\n    packed : optional, bool, default = False\n        If True, return a 2-dimensional packed array where the index of the\n        second column corresponds to l*(l+1)/2+m, where l and m are\n        respectively the degree and order.\n    degrees : optional, bool, default = True\n        If True, `colat` and `phi` are expressed in degrees.\n\n    Description\n    -----------\n    spharm will calculate all of the spherical harmonic functions up to degree\n    lmax for a given colatitude theta and longitude phi. Three parameters\n    determine how the spherical harmonic functions are defined. normalization\n    can be either '4pi' (default), 'ortho', 'schmidt', or 'unnorm' for 4pi\n    normalized, orthonormalized, Schmidt semi-normalized, or unnormalized\n    spherical harmonic functions, respectively. kind can be either 'real' or\n    'complex', and csphase determines whether to include or exclude (default)\n    the Condon-Shortley phase factor.\n\n    By default, the routine will return a 3-dimensional array, ylm[i, l, m],\n    where l and m are the spherical harmonic degree and (positive) order,\n    respectively. The index i=0 corresponds to the positive orders, whereas i=1\n    corresponds to the negative orders. If the optional parameter packed is set\n    to True, the output will instead be a 2-dimensional array where the indices\n    of the second column correspond to l*(l+1)/2+m.\n\n    The spherical harmonic functions are calculated using the standard three-\n    term recursion formula, and in order to prevent overflows, the scaling\n    approach of Holmes and Featherstone (2002) is utilized. The resulting\n    functions are accurate to about degree 2800. See Wieczorek and Meschede\n    (2018) for exact definitions on how the spherical harmonic functions are\n    defined.\n\n    References\n    ----------\n    Holmes, S. A., and W. E. Featherstone, A unified approach to the Clenshaw\n    summation and the recursive computation of very high degree and order\n    normalised associated Legendre functions, J. Geodesy, 76, 279-299,\n    doi:10.1007/s00190-002-0216-2, 2002.\n\n    Wieczorek, M. A., and M. Meschede. SHTools \u2014 Tools for working with\n    spherical harmonics, Geochem., Geophys., Geosyst., 19, 2574-2592,\n    doi:10.1029/2018GC007529, 2018.\n    \"\"\"\n    if lmax < 0:\n        raise ValueError(\n            \"lmax must be greater or equal to 0. Input value was {:s}.\"\n            .format(repr(lmax))\n            )\n\n    if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n        raise ValueError(\n            \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n            \"or 'unnorm'. Input value was {:s}.\"\n            .format(repr(normalization))\n            )\n\n    if kind.lower() not in ('real', 'complex'):\n        raise ValueError(\n            \"kind must be 'real' or 'complex'. \" +\n            \"Input value was {:s}.\".format(repr(kind))\n            )\n\n    if csphase != 1 and csphase != -1:\n        raise ValueError(\n            \"csphase must be either 1 or -1. Input value was {:s}.\"\n            .format(repr(csphase))\n            )\n\n    if normalization.lower() == 'unnorm' and lmax > 85:\n        _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                       \"are stable only for degrees less than or equal \" +\n                       \"to 85. lmax for the coefficients will be set to \" +\n                       \"85. Input value was {:d}.\".format(lmax),\n                       category=RuntimeWarning)\n        lmax = 85\n\n    if degrees is True:\n        theta = _np.deg2rad(theta)\n        phi = _np.deg2rad(phi)\n\n    if kind.lower() == 'real':\n        p = _legendre(lmax, _np.cos(theta), normalization=normalization,\n                      csphase=csphase, cnorm=0, packed=packed)\n    else:\n        p = _legendre(lmax, _np.cos(theta), normalization=normalization,\n                      csphase=csphase, cnorm=1, packed=packed)\n\n    if packed is False:\n        if kind.lower() == 'real':\n            ylm = _np.zeros((2, lmax+1, lmax+1), dtype=_np.float_)\n            ylm[0, :, :] = p[:, :]\n            ylm[1, :, :] = p[:, :]\n            for m in range(lmax+1):\n                ylm[0, m:lmax+1, m] *= _np.cos(m*phi)\n                ylm[1, m:lmax+1, m] *= _np.sin(m*phi)\n        else:\n            ylm = _np.zeros((2, lmax+1, lmax+1), dtype=_np.complex_)\n            ylm[0, :, :] = p[:, :]\n            for m in range(lmax+1):\n                ylm[0, m:lmax+1, m] *= (_np.cos(m*phi) + 1j * _np.sin(m*phi))\n                ylm[1, m:lmax+1, m] = ylm[0, m:lmax+1, m].conj()\n                if _np.mod(m, 2) == 1:\n                    ylm[1, m:lmax+1, m] = - ylm[1, m:lmax+1, m]\n\n    else:\n        if kind.lower() == 'real':\n            ylm = _np.zeros((2, (lmax+1)*(lmax+2)//2), dtype=_np.float_)\n            ylm[0, :] = p[:]\n            ylm[1, :] = p[:]\n            for m in range(lmax+1):\n                cos = _np.cos(m*phi)\n                sin = _np.sin(m*phi)\n                for l in range(m, lmax+1):\n                    ind = l*(l+1)//2+m\n                    ylm[0, ind] *= cos\n                    ylm[1, ind] *= sin\n        else:\n            ylm = _np.zeros((2, (lmax+1)*(lmax+2)//2), dtype=_np.complex_)\n            ylm[0, :] = p[:]\n            ylm[1, :] = p[:]\n            for m in range(lmax+1):\n                eimphi = (_np.cos(m*phi) + 1j * _np.sin(m*phi))\n                for l in range(m, lmax+1):\n                    ind = l*(l+1)//2+m\n                    ylm[0, ind] *= eimphi\n                    ylm[1, ind] = ylm[0, ind].conj()\n                    if _np.mod(m, 2) == 1:\n                        ylm[1, ind] = - ylm[1, ind]\n\n    return ylm", "response": "Compute all the spherical harmonic functions up to a maximum degree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the spherical harmonic function for a specific degree and order. Usage ----- ylm = spharm (l, m, theta, phi, [normalization, kind, csphase, degrees]) Returns ------- ylm : float or complex The spherical harmonic function ylm, where l and m are the spherical harmonic degree and order, respectively. Parameters ---------- l : integer The spherical harmonic degree. m : integer The spherical harmonic order. theta : float The colatitude in degrees. phi : float The longitude in degrees. normalization : str, optional, default = '4pi' '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized, orthonormalized, Schmidt semi-normalized, or unnormalized spherical harmonic functions, respectively. kind : str, optional, default = 'real' 'real' or 'complex' spherical harmonic coefficients. csphase : optional, integer, default = 1 If 1 (default), the Condon-Shortley phase will be excluded. If -1, the Condon-Shortley phase of (-1)^m will be appended to the spherical harmonic functions. degrees : optional, bool, default = True If True, colat and phi are expressed in degrees. Description ----------- spharm_lm will calculate the spherical harmonic function for a specific degree l and order m, and for a given colatitude theta and longitude phi. Three parameters determine how the spherical harmonic functions are defined. normalization can be either '4pi' (default), 'ortho', 'schmidt', or 'unnorm' for 4pi normalized, orthonormalized, Schmidt semi-normalized, or unnormalized spherical harmonic functions, respectively. kind can be either 'real' or 'complex', and csphase determines whether to include or exclude (default) the Condon-Shortley phase factor. The spherical harmonic functions are calculated using the standard three-term recursion formula, and in order to prevent overflows, the scaling approach of Holmes and Featherstone (2002) is utilized. The resulting functions are accurate to about degree 2800. See Wieczorek and Meschede (2018) for exact definitions on how the spherical harmonic functions are defined. References ---------- Holmes, S. A., and W. E. Featherstone, A unified approach to the Clenshaw summation and the recursive computation of very high degree and order normalised associated Legendre functions, J. Geodesy, 76, 279-299, doi:10.1007/s00190-002-0216-2, 2002. Wieczorek, M. A., and M. Meschede. SHTools \u2014 Tools for working with spherical harmonics, Geochem., Geophys., Geosyst., 19, 2574-2592, doi:10.1029/2018GC007529, 2018.", "response": "def spharm_lm(l, m, theta, phi, normalization='4pi', kind='real', csphase=1,\n              degrees=True):\n    \"\"\"\n    Compute the spherical harmonic function for a specific degree and order.\n\n    Usage\n    -----\n    ylm = spharm (l, m, theta, phi, [normalization, kind, csphase, degrees])\n\n    Returns\n    -------\n    ylm : float or complex\n        The spherical harmonic function ylm, where l and m are the spherical\n        harmonic degree and order, respectively.\n\n    Parameters\n    ----------\n    l : integer\n        The spherical harmonic degree.\n    m : integer\n        The spherical harmonic order.\n    theta : float\n        The colatitude in degrees.\n    phi : float\n        The longitude in degrees.\n    normalization : str, optional, default = '4pi'\n        '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n        orthonormalized, Schmidt semi-normalized, or unnormalized spherical\n        harmonic functions, respectively.\n    kind : str, optional, default = 'real'\n        'real' or 'complex' spherical harmonic coefficients.\n    csphase : optional, integer, default = 1\n        If 1 (default), the Condon-Shortley phase will be excluded. If -1, the\n        Condon-Shortley phase of (-1)^m will be appended to the spherical\n        harmonic functions.\n    degrees : optional, bool, default = True\n        If True, colat and phi are expressed in degrees.\n\n    Description\n    -----------\n    spharm_lm will calculate the spherical harmonic function for a specific\n    degree l and order m, and for a given colatitude theta and longitude phi.\n    Three parameters determine how the spherical harmonic functions are\n    defined. normalization can be either '4pi' (default), 'ortho', 'schmidt',\n    or 'unnorm' for 4pi normalized, orthonormalized, Schmidt semi-normalized,\n    or unnormalized spherical harmonic functions, respectively. kind can be\n    either 'real' or 'complex', and csphase determines whether to include or\n    exclude (default) the Condon-Shortley phase factor.\n\n    The spherical harmonic functions are calculated using the standard\n    three-term recursion formula, and in order to prevent overflows, the\n    scaling approach of Holmes and Featherstone (2002) is utilized.\n    The resulting functions are accurate to about degree 2800. See Wieczorek\n    and Meschede (2018) for exact definitions on how the spherical harmonic\n    functions are defined.\n\n    References\n    ----------\n    Holmes, S. A., and W. E. Featherstone, A unified approach to the Clenshaw\n    summation and the recursive computation of very high degree and order\n    normalised associated Legendre functions, J. Geodesy, 76, 279-299,\n    doi:10.1007/s00190-002-0216-2, 2002.\n\n    Wieczorek, M. A., and M. Meschede. SHTools \u2014 Tools for working with\n    spherical harmonics, Geochem., Geophys., Geosyst., 19, 2574-2592,\n    doi:10.1029/2018GC007529, 2018.\n    \"\"\"\n    if l < 0:\n        raise ValueError(\n            \"The degree l must be greater or equal than 0. Input value was {:s}.\"\n            .format(repr(l))\n            )\n\n    if m > l:\n        raise ValueError(\n            \"The order m must be less than or equal to the degree l. \" +\n            \"Input values were l={:s} and m={:s}.\".format(repr(l), repr(m))\n            )\n\n    if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n        raise ValueError(\n            \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n            \"or 'unnorm'. Input value was {:s}.\"\n            .format(repr(normalization))\n            )\n\n    if kind.lower() not in ('real', 'complex'):\n        raise ValueError(\n            \"kind must be 'real' or 'complex'. \" +\n            \"Input value was {:s}.\".format(repr(kind))\n            )\n\n    if csphase != 1 and csphase != -1:\n        raise ValueError(\n            \"csphase must be either 1 or -1. Input value was {:s}.\"\n            .format(repr(csphase))\n            )\n\n    if normalization.lower() == 'unnorm' and lmax > 85:\n        _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                       \"are stable only for degrees less than or equal \" +\n                       \"to 85. lmax for the coefficients will be set to \" +\n                       \"85. Input value was {:d}.\".format(lmax),\n                       category=RuntimeWarning)\n        lmax = 85\n\n    ind = (l*(l+1))//2 + abs(m)\n\n    if degrees is True:\n        theta = _np.deg2rad(theta)\n        phi = _np.deg2rad(phi)\n\n    if kind.lower() == 'real':\n        p = _legendre(l, _np.cos(theta), normalization=normalization,\n                      csphase=csphase, cnorm=0, packed=True)\n        if m >= 0:\n            ylm = p[ind] * _np.cos(m*phi)\n        else:\n            ylm = p[ind] * _np.sin(abs(m)*phi)\n\n    else:\n        p = _legendre(l, _np.cos(theta), normalization=normalization,\n                      csphase=csphase, cnorm=1, packed=True)\n        ylm = p[ind] * (_np.cos(m*phi) + 1j * _np.sin(abs(m)*phi))  # Yl|m|\n\n        if m < 0:\n            ylm = ylm.conj()\n            if _np.mod(m, 2) == 1:\n                ylm = - ylm\n\n    return ylm"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_array(self, coeffs, r0, errors=None, normalization='schmidt',\n                   csphase=1, lmax=None, copy=True):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients from an input\n        array.\n\n        Usage\n        -----\n        x = SHMagCoeffs.from_array(array, r0, [errors, normalization, csphase,\n                                               lmax, copy])\n\n        Returns\n        -------\n        x : SHMagCoeffs class instance.\n\n        Parameters\n        ----------\n        array : ndarray, shape (2, lmaxin+1, lmaxin+1).\n            The input spherical harmonic coefficients.\n        r0 : float\n            The reference radius of the spherical harmonic coefficients.\n        errors : ndarray, optional, default = None\n            The uncertainties of the spherical harmonic coefficients.\n        normalization : str, optional, default = 'schmidt'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        lmax : int, optional, default = None\n            The maximum spherical harmonic degree to include in the returned\n            class instance. This must be less than or equal to lmaxin.\n        copy : bool, optional, default = True\n            If True, make a copy of array when initializing the class instance.\n            If False, initialize the class instance with a reference to array.\n\n        Notes\n        -----\n        The coefficients in the input array are assumed to have units of nT.\n        \"\"\"\n        if _np.iscomplexobj(coeffs):\n            raise TypeError('The input array must be real.')\n\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. '\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \"\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be either 1 or -1. Input value was {:s}.\"\n                .format(repr(csphase))\n                )\n\n        if errors is not None:\n            if coeffs.shape != errors.shape:\n                raise ValueError(\n                    \"The shape of coeffs and errors must be the same.\"\n                    \"Shape of coeffs = {:s}, shape of errors = {:s}\"\n                    .format(repr(coeffs.shape), repr(coeffs.errors))\n                    )\n\n        lmaxin = coeffs.shape[1] - 1\n        if lmax is None:\n            lmax = lmaxin\n        else:\n            if lmax > lmaxin:\n                lmax = lmaxin\n\n        if normalization.lower() == 'unnorm' and lmax > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \"\n                           \"are stable only for degrees less than or equal \"\n                           \"to 85. lmax for the coefficients will be set to \"\n                           \"85. Input value was {:d}.\".format(lmax),\n                           category=RuntimeWarning)\n            lmax = 85\n\n        if errors is not None:\n            clm = SHMagRealCoeffs(coeffs[:, 0:lmax+1, 0:lmax+1], r0=r0,\n                                  errors=errors[:, 0:lmax+1, 0:lmax+1],\n                                  normalization=normalization.lower(),\n                                  csphase=csphase, copy=copy)\n        else:\n            clm = SHMagRealCoeffs(coeffs[:, 0:lmax+1, 0:lmax+1], r0=r0,\n                                  normalization=normalization.lower(),\n                                  csphase=csphase, copy=copy)\n        return clm", "response": "Initialize the class instance from an array of spherical harmonic coefficients."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_coeffs(self, values, ls, ms):\n        # Ensure that the type is correct\n        values = _np.array(values)\n        ls = _np.array(ls)\n        ms = _np.array(ms)\n\n        mneg_mask = (ms < 0).astype(_np.int)\n        self.coeffs[mneg_mask, ls, _np.abs(ms)] = values", "response": "Sets the spherical harmonic coefficients in - place to specified values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_array(self, normalization=None, csphase=None, lmax=None):\n        if normalization is None:\n            normalization = self.normalization\n        if csphase is None:\n            csphase = self.csphase\n        if lmax is None:\n            lmax = self.lmax\n\n        coeffs = _convert(self.coeffs, normalization_in=self.normalization,\n                          normalization_out=normalization,\n                          csphase_in=self.csphase, csphase_out=csphase,\n                          lmax=lmax)\n\n        if self.errors is not None:\n            errors = _convert(self.errors, normalization_in=self.normalization,\n                              normalization_out=normalization,\n                              csphase_in=self.csphase, csphase_out=csphase,\n                              lmax=lmax)\n            return coeffs, errors\n        else:\n            return coeffs", "response": "Return the spherical harmonic coefficients and errors of the class instance as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(self, normalization=None, csphase=None, lmax=None):\n        if normalization is None:\n            normalization = self.normalization\n        if csphase is None:\n            csphase = self.csphase\n        if lmax is None:\n            lmax = self.lmax\n\n        # check argument consistency\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. '\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"normalization must be '4pi', 'ortho', 'schmidt', or \"\n                \"'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization)))\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase)))\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization=normalization.lower(),\n                                           csphase=csphase, lmax=lmax)\n            return SHMagCoeffs.from_array(\n                coeffs, r0=self.r0, errors=errors,\n                normalization=normalization.lower(),\n                csphase=csphase, copy=False)\n        else:\n            coeffs = self.to_array(normalization=normalization.lower(),\n                                   csphase=csphase, lmax=lmax)\n            return SHMagCoeffs.from_array(\n                coeffs, r0=self.r0, normalization=normalization.lower(),\n                csphase=csphase, copy=False)", "response": "Convert a spherical harmonic degree to a new class instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an empty SHMagCoeffs class instance with the coefficients are zero padded to a different lmax.", "response": "def pad(self, lmax):\n        \"\"\"\n        Return an SHMagCoeffs class where the coefficients are zero padded or\n        truncated to a different lmax.\n\n        Usage\n        -----\n        clm = x.pad(lmax)\n\n        Returns\n        -------\n        clm : SHMagCoeffs class instance\n\n        Parameters\n        ----------\n        lmax : int\n            Maximum spherical harmonic degree to output.\n        \"\"\"\n        clm = self.copy()\n\n        if lmax <= self.lmax:\n            clm.coeffs = clm.coeffs[:, :lmax+1, :lmax+1]\n            clm.mask = clm.mask[:, :lmax+1, :lmax+1]\n            if self.errors is not None:\n                clm.errors = clm.errors[:, :lmax+1, :lmax+1]\n        else:\n            clm.coeffs = _np.pad(clm.coeffs, ((0, 0), (0, lmax - self.lmax),\n                                 (0, lmax - self.lmax)), 'constant')\n            if self.errors is not None:\n                clm.errors = _np.pad(\n                    clm.errors, ((0, 0), (0, lmax - self.lmax),\n                                 (0, lmax - self.lmax)), 'constant')\n            mask = _np.zeros((2, lmax + 1, lmax + 1), dtype=_np.bool)\n            for l in _np.arange(lmax + 1):\n                mask[:, l, :l + 1] = True\n            mask[1, :, 0] = False\n            clm.mask = mask\n\n        clm.lmax = lmax\n        return clm"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_ref(self, r0=None, lmax=None):\n        if lmax is None:\n            lmax = self.lmax\n\n        clm = self.pad(lmax)\n\n        if r0 is not None and r0 != self.r0:\n            for l in _np.arange(lmax+1):\n                clm.coeffs[:, l, :l+1] *= (self.r0 / r0)**(l+2)\n                if self.errors is not None:\n                    clm.errors[:, l, :l+1] *= (self.r0 / r0)**(l+2)\n            clm.r0 = r0\n\n        return clm", "response": "Returns a new class instance with a different reference radius."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand(self, a=None, f=None, lmax=None, lmax_calc=None, sampling=2):\n        if a is None:\n            a = self.r0\n        if f is None:\n            f = 0.\n        if lmax is None:\n            lmax = self.lmax\n        if lmax_calc is None:\n            lmax_calc = lmax\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization='schmidt', csphase=1)\n        else:\n            coeffs = self.to_array(normalization='schmidt', csphase=1)\n\n        rad, theta, phi, total, pot = _MakeMagGridDH(\n            coeffs, self.r0, a=a, f=f, lmax=lmax,\n            lmax_calc=lmax_calc, sampling=sampling)\n\n        return _SHMagGrid(rad, theta, phi, total, pot, a, f, lmax, lmax_calc)", "response": "This method creates a 2D cylindrical map on a flattened ellipsoid a - b - a - f."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates 2D cylindrical maps on a flattened ellipsoid of the 9 components of the magnetic field tensor in a local north-oriented reference frame, and return an SHMagTensor class instance. Usage ----- tensor = x.tensor([a, f, lmax, lmax_calc, sampling]) Returns ------- tensor : SHMagTensor class instance. Parameters ---------- a : optional, float, default = self.r0 The semi-major axis of the flattened ellipsoid on which the field is computed. f : optional, float, default = 0 The flattening of the reference ellipsoid: f=(a-b)/a. lmax : optional, integer, default = self.lmax The maximum spherical harmonic degree that determines the number of samples of the output grids, n=2lmax+2, and the latitudinal sampling interval, 90/(lmax+1). lmax_calc : optional, integer, default = lmax The maximum spherical harmonic degree used in evaluating the functions. This must be less than or equal to lmax. sampling : optional, integer, default = 2 If 1 the output grids are equally sampled (n by n). If 2 (default), the grids are equally spaced in degrees (n by 2n). Description ----------- This method will create 2-dimensional cylindrical maps for the 9 components of the magnetic field tensor and return an SHMagTensor class instance. The components are (Vxx, Vxy, Vxz) (Vyx, Vyy, Vyz) (Vzx, Vzy, Vzz) where the reference frame is north-oriented, where x points north, y points west, and z points upward (all tangent or perpendicular to a sphere of radius r, where r is the local radius of the flattened ellipsoid). The magnetic potential is defined as V = r0 Sum_{l=0}^lmax (r0/r)^(l+1) Sum_{m=-l}^l C_{lm} Y_{lm}, where r0 is the reference radius of the spherical harmonic coefficients Clm, and the vector magnetic field is B = - Grad V. The components of the tensor are calculated according to eq. 1 in Petrovskaya and Vershkov (2006), which is based on eq. 3.28 in Reed (1973) (noting that Reed's equations are in terms of latitude and that the y axis points east): Vzz = Vrr Vxx = 1/r Vr + 1/r^2 Vtt Vyy = 1/r Vr + 1/r^2 /tan(t) Vt + 1/r^2 /sin(t)^2 Vpp Vxy = 1/r^2 /sin(t) Vtp - cos(t)/sin(t)^2 /r^2 Vp Vxz = 1/r^2 Vt - 1/r Vrt Vyz = 1/r^2 /sin(t) Vp - 1/r /sin(t) Vrp where r, t, p stand for radius, theta, and phi, respectively, and subscripts on V denote partial derivatives. The output grids are in units of nT / m. References ---------- Reed, G.B., Application of kinematical geodesy for determining the short wave length components of the gravity field by satellite gradiometry, Ohio State University, Dept. of Geod. Sciences, Rep. No. 201, Columbus, Ohio, 1973. Petrovskaya, M.S. and A.N. Vershkov, Non-singular expressions for the gravity gradients in the local north-oriented and orbital reference frames, J. Geod., 80, 117-127, 2006.", "response": "def tensor(self, a=None, f=None, lmax=None, lmax_calc=None, sampling=2):\n        \"\"\"\n        Create 2D cylindrical maps on a flattened ellipsoid of the 9\n        components of the magnetic field tensor in a local north-oriented\n        reference frame, and return an SHMagTensor class instance.\n\n        Usage\n        -----\n        tensor = x.tensor([a, f, lmax, lmax_calc, sampling])\n\n        Returns\n        -------\n        tensor : SHMagTensor class instance.\n\n        Parameters\n        ----------\n        a : optional, float, default = self.r0\n            The semi-major axis of the flattened ellipsoid on which the field\n            is computed.\n        f : optional, float, default = 0\n            The flattening of the reference ellipsoid: f=(a-b)/a.\n        lmax : optional, integer, default = self.lmax\n            The maximum spherical harmonic degree that determines the number of\n            samples of the output grids, n=2lmax+2, and the latitudinal\n            sampling interval, 90/(lmax+1).\n        lmax_calc : optional, integer, default = lmax\n            The maximum spherical harmonic degree used in evaluating the\n            functions. This must be less than or equal to lmax.\n        sampling : optional, integer, default = 2\n            If 1 the output grids are equally sampled (n by n). If 2 (default),\n            the grids are equally spaced in degrees (n by 2n).\n\n        Description\n        -----------\n        This method will create 2-dimensional cylindrical maps for the 9\n        components of the magnetic field tensor and return an SHMagTensor\n        class instance. The components are\n\n            (Vxx, Vxy, Vxz)\n            (Vyx, Vyy, Vyz)\n            (Vzx, Vzy, Vzz)\n\n        where the reference frame is north-oriented, where x points north, y\n        points west, and z points upward (all tangent or perpendicular to a\n        sphere of radius r, where r is the local radius of the flattened\n        ellipsoid). The magnetic potential is defined as\n\n            V = r0 Sum_{l=0}^lmax (r0/r)^(l+1) Sum_{m=-l}^l C_{lm} Y_{lm},\n\n        where r0 is the reference radius of the spherical harmonic coefficients\n        Clm, and the vector magnetic field is\n\n            B = - Grad V.\n\n        The components of the tensor are calculated according to eq. 1 in\n        Petrovskaya and Vershkov (2006), which is based on eq. 3.28 in Reed\n        (1973) (noting that Reed's equations are in terms of latitude and that\n        the y axis points east):\n\n            Vzz = Vrr\n            Vxx = 1/r Vr + 1/r^2 Vtt\n            Vyy = 1/r Vr + 1/r^2 /tan(t) Vt + 1/r^2 /sin(t)^2 Vpp\n            Vxy = 1/r^2 /sin(t) Vtp - cos(t)/sin(t)^2 /r^2 Vp\n            Vxz = 1/r^2 Vt - 1/r Vrt\n            Vyz = 1/r^2 /sin(t) Vp - 1/r /sin(t) Vrp\n\n        where r, t, p stand for radius, theta, and phi, respectively, and\n        subscripts on V denote partial derivatives. The output grids are in\n        units of nT / m.\n\n        References\n        ----------\n        Reed, G.B., Application of kinematical geodesy for determining\n        the short wave length components of the gravity field by satellite\n        gradiometry, Ohio State University, Dept. of Geod. Sciences, Rep. No.\n        201, Columbus, Ohio, 1973.\n\n        Petrovskaya, M.S. and A.N. Vershkov, Non-singular expressions for the\n        gravity gradients in the local north-oriented and orbital reference\n        frames, J. Geod., 80, 117-127, 2006.\n        \"\"\"\n        if a is None:\n            a = self.r0\n        if f is None:\n            f = 0.\n        if lmax is None:\n            lmax = self.lmax\n        if lmax_calc is None:\n            lmax_calc = lmax\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization='schmidt', csphase=1)\n        else:\n            coeffs = self.to_array(normalization='schmidt', csphase=1)\n\n        vxx, vyy, vzz, vxy, vxz, vyz = _MakeMagGradGridDH(\n            coeffs, self.r0, a=a, f=f, lmax=lmax, lmax_calc=lmax_calc,\n            sampling=sampling)\n\n        return _SHMagTensor(vxx, vyy, vzz, vxy, vxz, vyz, a, f, lmax,\n                            lmax_calc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spectrum(clm, normalization='4pi', degrees=None, lmax=None,\n             convention='power', unit='per_l', base=10.):\n    \"\"\"\n    Return the spectrum of the spherical harmonic coefficients as a function\n    of spherical harmonic degree.\n\n    Usage\n    -----\n    array = spectrum(clm, [normalization, degrees, lmax, convention,\n                           unit, base])\n\n    Returns\n    -------\n    array : ndarray, shape (len(degrees))\n        1-D ndarray of the spectrum.\n\n    Parameters\n    ----------\n    clm : ndarray, shape (2, lmax + 1, lmax + 1)\n        ndarray containing the spherical harmonic coefficients.\n    normalization : str, optional, default = '4pi'\n        '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n        orthonormalized, Schmidt semi-normalized, or unnormalized coefficients,\n        respectively.\n    lmax : int, optional, default = len(clm[0,:,0]) - 1.\n        Maximum spherical harmonic degree to output.\n    degrees : ndarray, optional, default = numpy.arange(lmax+1)\n        Array containing the spherical harmonic degrees where the spectrum\n        is computed.\n    convention : str, optional, default = 'power'\n        The type of spectrum to return: 'power' for power spectrum, 'energy'\n        for energy spectrum, and 'l2norm' for the l2-norm spectrum.\n    unit : str, optional, default = 'per_l'\n        If 'per_l', return the total contribution to the spectrum for each\n        spherical harmonic degree l. If 'per_lm', return the average\n        contribution to the spectrum for each coefficient at spherical\n        harmonic degree l. If 'per_dlogl', return the spectrum per log\n        interval dlog_a(l).\n    base : float, optional, default = 10.\n        The logarithm base when calculating the 'per_dlogl' spectrum.\n\n    Description\n    -----------\n    This function returns either the power spectrum, energy spectrum, or\n    l2-norm spectrum. Total power is defined as the integral of the\n    function squared over all space, divided by the area the function\n    spans. If the mean of the function is zero, this is equivalent to the\n    variance of the function. The total energy is the integral of the\n    function squared over all space and is 4pi times the total power. The\n    l2-norm is the sum of the magnitude of the coefficients squared.\n\n    The output spectrum can be expresed using one of three units. 'per_l'\n    returns the contribution to the total spectrum from all angular orders\n    at degree l. 'per_lm' returns the average contribution to the total\n    spectrum from a single coefficient at degree l, and is equal to the\n    'per_l' spectrum divided by (2l+1). 'per_dlogl' returns the contribution to\n    the total spectrum from all angular orders over an infinitessimal\n    logarithmic degree band. The contrubution in the band dlog_a(l) is\n    spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base, and where\n    spectrum(l, 'per_dlogl) is equal to spectrum(l, 'per_l')*l*log(a).\n    \"\"\"\n    if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n        raise ValueError(\"The normalization must be '4pi', 'ortho', \" +\n                         \"'schmidt', or 'unnorm'. Input value was {:s}.\"\n                         .format(repr(normalization)))\n\n    if convention.lower() not in ('power', 'energy', 'l2norm'):\n        raise ValueError(\"convention must be 'power', 'energy', or \" +\n                         \"'l2norm'. Input value was {:s}\"\n                         .format(repr(convention)))\n\n    if unit.lower() not in ('per_l', 'per_lm', 'per_dlogl'):\n        raise ValueError(\"unit must be 'per_l', 'per_lm', or 'per_dlogl'.\" +\n                         \"Input value was {:s}\".format(repr(unit)))\n\n    if lmax is None:\n        lmax = len(clm[0, :, 0]) - 1\n\n    if degrees is None:\n        degrees = _np.arange(lmax+1)\n\n    array = _np.empty(len(degrees))\n\n    if normalization.lower() == 'unnorm':\n        if convention.lower() == 'l2norm':\n            raise ValueError(\"convention can not be set to 'l2norm' when \" +\n                             \"using unnormalized harmonics.\")\n\n        for i, l in enumerate(degrees):\n            ms = _np.arange(l+1)\n            conv = _factorial(l+ms) / (2. * l + 1.) / _factorial(l-ms)\n\n            if _np.iscomplexobj(clm):\n                array[i] = (conv[0:l + 1] * clm[0, l, 0:l + 1] *\n                            clm[0, l, 0:l + 1].conjugate()).real.sum() + \\\n                           (conv[1:l + 1] * clm[1, l, 1:l + 1] *\n                            clm[1, l, 1:l + 1].conjugate()).real.sum()\n            else:\n                conv[1:l + 1] = conv[1:l + 1] / 2.\n                array[i] = (conv[0:l + 1] * clm[0, l, 0:l+1]**2).sum() + \\\n                           (conv[1:l + 1] * clm[1, l, 1:l+1]**2).sum()\n\n    else:\n        for i, l in enumerate(degrees):\n            if _np.iscomplexobj(clm):\n                array[i] = (clm[0, l, 0:l + 1] *\n                            clm[0, l, 0:l + 1].conjugate()).real.sum() + \\\n                           (clm[1, l, 1:l + 1] *\n                            clm[1, l, 1:l + 1].conjugate()).real.sum()\n            else:\n                array[i] = (clm[0, l, 0:l+1]**2).sum() + \\\n                           (clm[1, l, 1:l+1]**2).sum()\n\n        if convention.lower() == 'l2norm':\n            return array\n        else:\n            if normalization.lower() == '4pi':\n                pass\n            elif normalization.lower() == 'schmidt':\n                array /= (2. * degrees + 1.)\n            elif normalization.lower() == 'ortho':\n                array /= (4. * _np.pi)\n\n    if convention.lower() == 'energy':\n        array *= 4. * _np.pi\n\n    if unit.lower() == 'per_l':\n        pass\n    elif unit.lower() == 'per_lm':\n        array /= (2. * degrees + 1.)\n    elif unit.lower() == 'per_dlogl':\n        array *= degrees * _np.log(base)\n\n    return array", "response": "This function returns the spectrum of the spherical harmonic coefficients in clm."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef legendre(lmax, z, normalization='4pi', csphase=1, cnorm=0, packed=False):\n    if lmax < 0:\n        raise ValueError(\n            \"lmax must be greater or equal to 0. Input value was {:s}.\"\n            .format(repr(lmax))\n            )\n\n    if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n        raise ValueError(\n            \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n            \"or 'unnorm'. Input value was {:s}.\"\n            .format(repr(normalization))\n            )\n\n    if csphase != 1 and csphase != -1:\n        raise ValueError(\n            \"csphase must be either 1 or -1. Input value was {:s}.\"\n            .format(repr(csphase))\n            )\n\n    if cnorm != 0 and cnorm != 1:\n        raise ValueError(\n            \"cnorm must be either 0 or 1. Input value was {:s}.\"\n            .format(repr(cnorm))\n            )\n\n    if normalization.lower() == 'unnorm' and lmax > 85:\n        _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                       \"are stable only for degrees less than or equal \" +\n                       \"to 85. lmax for the coefficients will be set to \" +\n                       \"85. Input value was {:d}.\".format(lmax),\n                       category=RuntimeWarning)\n        lmax = 85\n\n    if normalization == '4pi':\n        p = _PlmBar(lmax, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'ortho':\n        p = _PlmON(lmax, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'schmidt':\n        p = _PlmSchmidt(lmax, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'unnorm':\n        p = _PLegendreA(lmax, z, csphase=csphase, cnorm=cnorm)\n\n    if packed is True:\n        return p\n    else:\n        plm = _np.zeros((lmax+1, lmax+1))\n        for l in range(lmax+1):\n            for m in range(l+1):\n                plm[l, m] = p[(l*(l+1))//2+m]\n\n        return plm", "response": "Returns a 1 - dimensional array with the associated Legendre functions up to a given maximum degree and order."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the associated Legendre function for a specific degree and order. Usage ----- plm = legendre_lm (l, m, z, [normalization, csphase, cnorm]) Returns ------- plm : float The associated Legendre functions for degree l and order m. Parameters ---------- l : integer The spherical harmonic degree. m : integer The spherical harmonic order. z : float The argument of the associated Legendre functions. normalization : str, optional, default = '4pi' '4pi', 'ortho', 'schmidt', or 'unnorm' for use with geodesy 4pi normalized, orthonormalized, Schmidt semi-normalized, or unnormalized spherical harmonic functions, respectively. csphase : optional, integer, default = 1 If 1 (default), the Condon-Shortley phase will be excluded. If -1, the Condon-Shortley phase of (-1)^m will be appended to the associated Legendre functions. cnorm : optional, integer, default = 0 If 1, the complex normalization of the associated Legendre functions will be used. The default is to use the real normalization. Description ----------- legendre_lm will calculate the associated Legendre function for a specific degree l and order m. The Legendre functions are used typically as a part of the spherical harmonic functions, and three parameters determine how they are defined. normalization can be either '4pi' (default), 'ortho', 'schmidt', or 'unnorm' for use with 4pi normalized, orthonormalized, Schmidt semi-normalized, or unnormalized spherical harmonic functions, respectively. csphase determines whether to include or exclude (default) the Condon-Shortley phase factor. cnorm determines whether to normalize the Legendre functions for use with real (default) or complex spherical harmonic functions. The Legendre functions are calculated using the standard three-term recursion formula, and in order to prevent overflows, the scaling approach of Holmes and Featherstone (2002) is utilized. The resulting functions are accurate to about degree 2800. See Wieczorek and Meschede (2018) for exact definitions on how the Legendre functions are defined. References ---------- Holmes, S. A., and W. E. Featherstone, A unified approach to the Clenshaw summation and the recursive computation of very high degree and order normalised associated Legendre functions, J. Geodesy, 76, 279-299, doi:10.1007/s00190-002-0216-2, 2002. Wieczorek, M. A., and M. Meschede. SHTools \u2014 Tools for working with spherical harmonics, Geochem., Geophys., Geosyst., 19, 2574-2592, doi:10.1029/2018GC007529, 2018.", "response": "def legendre_lm(l, m, z, normalization='4pi', csphase=1, cnorm=0):\n    \"\"\"\n    Compute the associated Legendre function for a specific degree and order.\n\n    Usage\n    -----\n    plm = legendre_lm (l, m, z, [normalization, csphase, cnorm])\n\n    Returns\n    -------\n    plm : float\n        The associated Legendre functions for degree l and order m.\n\n    Parameters\n    ----------\n    l : integer\n        The spherical harmonic degree.\n    m : integer\n        The spherical harmonic order.\n    z : float\n        The argument of the associated Legendre functions.\n    normalization : str, optional, default = '4pi'\n        '4pi', 'ortho', 'schmidt', or 'unnorm' for use with geodesy 4pi\n        normalized, orthonormalized, Schmidt semi-normalized, or unnormalized\n        spherical harmonic functions, respectively.\n    csphase : optional, integer, default = 1\n        If 1 (default), the Condon-Shortley phase will be excluded. If -1, the\n        Condon-Shortley phase of (-1)^m will be appended to the associated\n        Legendre functions.\n    cnorm : optional, integer, default = 0\n        If 1, the complex normalization of the associated Legendre functions\n        will be used. The default is to use the real normalization.\n\n    Description\n    -----------\n    legendre_lm will calculate the associated Legendre function for a specific\n    degree l and order m. The Legendre functions are used typically as a part\n    of the spherical harmonic functions, and three parameters determine how\n    they are defined. normalization can be either '4pi' (default), 'ortho',\n    'schmidt', or 'unnorm' for use with 4pi normalized, orthonormalized,\n    Schmidt semi-normalized, or unnormalized spherical harmonic functions,\n    respectively. csphase determines whether to include or exclude (default)\n    the Condon-Shortley phase factor. cnorm determines whether to normalize\n    the Legendre functions for use with real (default) or complex spherical\n    harmonic functions.\n\n    The Legendre functions are calculated using the standard three-term\n    recursion formula, and in order to prevent overflows, the scaling approach\n    of Holmes and Featherstone (2002) is utilized. The resulting functions are\n    accurate to about degree 2800. See Wieczorek and Meschede (2018) for exact\n    definitions on how the Legendre functions are defined.\n\n    References\n    ----------\n    Holmes, S. A., and W. E. Featherstone, A unified approach to the Clenshaw\n    summation and the recursive computation of very high degree and order\n    normalised associated Legendre functions, J. Geodesy, 76, 279-299,\n    doi:10.1007/s00190-002-0216-2, 2002.\n\n    Wieczorek, M. A., and M. Meschede. SHTools \u2014 Tools for working with\n    spherical harmonics, Geochem., Geophys., Geosyst., 19, 2574-2592,\n    doi:10.1029/2018GC007529, 2018.\n    \"\"\"\n    if l < 0:\n        raise ValueError(\n            \"The degree l must be greater or equal to 0. Input value was {:s}.\"\n            .format(repr(l))\n            )\n\n    if m < 0:\n        raise ValueError(\n            \"The order m must be greater or equal to 0. Input value was {:s}.\"\n            .format(repr(m))\n            )\n\n    if m > l:\n        raise ValueError(\n            \"The order m must be less than or equal to the degree l. \" +\n            \"Input values were l={:s} and m={:s}.\".format(repr(l), repr(m))\n            )\n\n    if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n        raise ValueError(\n            \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n            \"or 'unnorm'. Input value was {:s}.\"\n            .format(repr(normalization))\n            )\n\n    if csphase != 1 and csphase != -1:\n        raise ValueError(\n            \"csphase must be either 1 or -1. Input value was {:s}.\"\n            .format(repr(csphase))\n            )\n\n    if cnorm != 0 and cnorm != 1:\n        raise ValueError(\n            \"cnorm must be either 0 or 1. Input value was {:s}.\"\n            .format(repr(cnorm))\n            )\n\n    if normalization.lower() == 'unnorm' and lmax > 85:\n        _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                       \"are stable only for degrees less than or equal \" +\n                       \"to 85. lmax for the coefficients will be set to \" +\n                       \"85. Input value was {:d}.\".format(lmax),\n                       category=RuntimeWarning)\n        lmax = 85\n\n    if normalization == '4pi':\n        p = _PlmBar(l, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'ortho':\n        p = _PlmON(l, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'schmidt':\n        p = _PlmSchmidt(l, z, csphase=csphase, cnorm=cnorm)\n    elif normalization == 'unnorm':\n        p = _PLegendreA(l, z, csphase=csphase, cnorm=cnorm)\n\n    return p[(l*(l+1))//2+m]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shread(filename, lmax=None, error=False, header=False, skip=0):\n\n    # determine lmax by reading the last non-comment line of the file\n    with open(filename, 'rb') as f:\n        line = ''\n        if f.seek(0, os.SEEK_END) == 0:\n            raise RuntimeError('File is empty.')\n        else:\n            f.seek(-1, os.SEEK_CUR)\n\n        # read backwards to end of preceding line and then read the line\n        while _iscomment(line):\n            while f.read(1) != b'\\n':\n                try:\n                    f.seek(-2, os.SEEK_CUR)\n                except:\n                    f.seek(-1, os.SEEK_CUR)  # beginning of file\n                    break\n\n            if f.tell() <= 1:\n                line = f.readline().decode()\n                line = line.replace(',', ' ')\n                if _iscomment(line):\n                    raise RuntimeError('Encountered beginning of file ' +\n                                       'while attempting to determine lmax.')\n                break\n            else:\n                line = f.readline().decode()\n                line = line.replace(',', ' ')\n                try:\n                    f.seek(-len(line)-2, os.SEEK_CUR)\n                except:\n                    raise RuntimeError('Encountered beginning of file ' +\n                                       'while attempting to determine lmax.')\n\n    lmaxfile = int(line.split()[0])\n    if lmax is not None:\n        lmaxout = min(lmax, lmaxfile)\n    else:\n        lmaxout = lmaxfile\n\n    # determine if coefficients are real or complex\n    try:\n        num = float(line.split()[2])\n        coeffs = _np.zeros((2, lmaxout+1, lmaxout+1))\n        kind = 'real'\n        if error is True:\n            errors = _np.zeros((2, lmaxout+1, lmaxout+1))\n    except ValueError:\n        try:\n            num = complex(line.split()[2])\n            coeffs = _np.zeros((2, lmaxout+1, lmaxout+1), dtype=complex)\n            kind = 'complex'\n            if error is True:\n                errors = _np.zeros((2, lmaxout+1, lmaxout+1), dtype=complex)\n        except ValueError:\n            raise ValueError('Coefficients can not be converted to ' +\n                             'either float or complex. Coefficient ' +\n                             'is {:s}\\n'.format(line.split()[2]) +\n                             'Unformatted string is {:s}'.format(line))\n\n    # determine lstart and read header\n    with open(filename, 'r') as f:\n        if skip != 0:\n            for i in range(skip):\n                line = f.readline()\n                if line == '':\n                    raise RuntimeError('End of file encountered when ' +\n                                       'skipping lines.')\n\n        if header is True:\n            line = f.readline()\n            if line == '':\n                    raise RuntimeError('End of file encountered when ' +\n                                       'reading header line.')\n            line = line.replace(',', ' ')\n            header_list = line.split()\n\n        line = f.readline()\n        if line == '':\n            raise RuntimeError('End of file encountered when determining ' +\n                               'value of lstart.')\n        line = line.replace(',', ' ')\n        while _iscomment(line):\n            line = f.readline()\n            if line == '':\n                raise RuntimeError('End of file encountered when ' +\n                                   'determining value of lstart.')\n            line = line.replace(',', ' ')\n        lstart = int(line.split()[0])\n\n    # read coefficients one line at a time\n    with open(filename, 'r') as f:\n        if skip != 0:\n            for i in range(skip):\n                f.readline()\n        if header is True:\n            f.readline()\n\n        for degree in range(lstart, lmaxout+1):\n            for order in range(degree+1):\n                line = f.readline()\n                if line == '':\n                    raise RuntimeError('End of file encountered at ' +\n                                       'degree and order {:d}, {:d}.'\n                                       .format(degree, order))\n                line = line.replace(',', ' ')\n                while _iscomment(line):\n                    line = f.readline()\n                    if line == '':\n                        raise RuntimeError('End of file encountered at ' +\n                                           'degree and order {:d}, {:d}.'\n                                           .format(degree, order))\n                    line = line.replace(',', ' ')\n\n                l = int(line.split()[0])\n                m = int(line.split()[1])\n\n                if degree != l or order != m:\n                    raise RuntimeError('Degree and order from file do not ' +\n                                       'correspond to expected values.\\n ' +\n                                       'Read {:d}, {:d}. Expected {:d}, {:d}.'\n                                       .format(degree, order, l, m))\n\n                if kind == 'real':\n                    coeffs[0, l, m] = float(line.split()[2])\n                    if m > 0:\n                        coeffs[1, l, m] = float(line.split()[3])\n                else:\n                    coeffs[0, l, m] = complex(line.split()[2])\n                    if m > 0:\n                        coeffs[1, l, m] = complex(line.split()[3])\n\n                if error:\n                    if len(line.split()) < 6:\n                        raise RuntimeError('When reading errors, ' +\n                                           'each line must ' +\n                                           'contain at least 6 elements. ' +\n                                           'Last line is: {:s}'.format(line))\n\n                    if kind == 'real':\n                        errors[0, l, m] = float(line.split()[4])\n                        errors[1, l, m] = float(line.split()[5])\n                    else:\n                        errors[0, l, m] = complex(line.split()[4])\n                        errors[1, l, m] = complex(line.split()[5])\n\n    if error is True and header is True:\n        return coeffs, errors, lmaxout, header_list\n    elif error is True and header is False:\n        return coeffs, errors, lmaxout\n    elif error is False and header is True:\n        return coeffs, lmaxout, header_list\n    else:\n        return coeffs, lmaxout", "response": "Reads a spherical harmonic coefficients from a text file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if a line is a comment line.", "response": "def _iscomment(line):\n    \"\"\"\n    Determine if a line is a comment line. A valid line contains at least three\n    words, with the first two being integers. Note that Python 2 and 3 deal\n    with strings differently.\n    \"\"\"\n    if line.isspace():\n        return True\n    elif len(line.split()) >= 3:\n        try:  # python 3 str\n            if line.split()[0].isdecimal() and line.split()[1].isdecimal():\n                return False\n        except:  # python 2 str\n            if (line.decode().split()[0].isdecimal() and\n                    line.split()[1].decode().isdecimal()):\n                return False\n        return True\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the f2py doc into a single f2py document", "response": "def process_f2pydoc(f2pydoc):\n    \"\"\"\n    this function replace all optional _d0 arguments with their default values\n    in the function signature. These arguments are not intended to be used and\n    signify merely the array dimensions of the associated argument.\n    \"\"\"\n    # ---- split f2py document in its parts\n    # 0=Call Signature\n    # 1=Parameters\n    # 2=Other (optional) Parameters (only if present)\n    # 3=Returns\n    docparts = re.split('\\n--', f2pydoc)\n\n    if len(docparts) == 4:\n        doc_has_optionals = True\n    elif len(docparts) == 3:\n        doc_has_optionals = False\n    else:\n        print('-- uninterpretable f2py documentation --')\n        return f2pydoc\n\n    # ---- replace arguments with _d suffix with empty string in ----\n    # ---- function signature (remove them): ----\n    docparts[0] = re.sub('[\\[(,]\\w+_d\\d', '', docparts[0])\n\n    # ---- replace _d arguments of the return arrays with their default value:\n    if doc_has_optionals:\n\n        returnarray_dims = re.findall('[\\[(,](\\w+_d\\d)', docparts[3])\n        for arg in returnarray_dims:\n            searchpattern = arg + ' : input.*\\n.*Default: (.*)\\n'\n            match = re.search(searchpattern, docparts[2])\n            if match:\n                default = match.group(1)\n                docparts[3] = re.sub(arg, default, docparts[3])\n                docparts[2] = re.sub(searchpattern, '', docparts[2])\n\n    # ---- remove all optional _d# from optional argument list:\n    if doc_has_optionals:\n        searchpattern = '\\w+_d\\d : input.*\\n.*Default: (.*)\\n'\n        docparts[2] = re.sub(searchpattern, '', docparts[2])\n\n    # ---- combine doc parts to a single string\n    processed_signature = '\\n--'.join(docparts)\n\n    return processed_signature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(coeffs_in, normalization_in=None, normalization_out=None,\n            csphase_in=None, csphase_out=None, lmax=None):\n        \"\"\"\n        Convert an array of spherical harmonic coefficients to a different\n        normalization convention.\n\n        Usage\n        -----\n        coeffs_out = convert(coeffs_in, [normalization_in, normalization_out,\n                                         csphase_in, csphase_out, lmax])\n\n        Returns\n        -------\n        coeffs_out : ndarray, size (2, lmax+1, lmax+1)\n            An array of spherical harmonic coefficients with the new\n            normalization convention.\n\n        Parameters\n        ----------\n        coeffs_in : ndarray\n            The array of imput spherical harmonic coefficients.\n        normalization_in : str, optional, default = None\n            Normalization of the output coefficients: '4pi', 'ortho'\n            'schmidt', or 'unnorm', for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        normalization_out : str, optional, default = None\n            Normalization of the output coefficients: '4pi', 'ortho'\n            'schmidt', or 'unnorm', for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase_in : int, optional, default = None\n            Condon-Shortley phase convention of the input coefficients: 1 to\n            exclude the phase factor, or -1 to include it.\n        csphase_out : int, optional, default = None\n            Condon-Shortley phase convention of the output coefficients: 1 to\n            exclude the phase factor, or -1 to include it.\n        lmax : int, optional, default = coeffs.shape[1] - 1\n            Maximum spherical harmonic degree to output. If lmax is larger than\n            that of the input coefficients, the output array will be zero\n            padded.\n\n        Description\n        -----------\n        This routine will convert an array of spherical harmonic coefficients\n        to a different normalization convention and different Condon-Shortley\n        phase convention. Optionally, a different maximum spherical harmonic\n        degree can be specified. If this degree is smaller than that of the\n        input coefficients, the input coefficients will be truncated. If this\n        degree is larger than the input coefficients, then the output\n        coefficients will be zero padded.\n        \"\"\"\n\n        # check argument consistency\n        if normalization_in is not None:\n            if type(normalization_in) != str:\n                raise ValueError('normalization_in must be a string. ' +\n                                 'Input type was {:s}'\n                                 .format(str(type(normalization_in))))\n            if normalization_in.lower() not in ('4pi', 'ortho', 'schmidt',\n                                                'unnorm'):\n                raise ValueError(\n                    \"normalization_in must be '4pi', 'ortho', 'schmidt', or \" +\n                    \"'unnorm'. Provided value was {:s}\"\n                    .format(repr(normalization_in))\n                    )\n            if normalization_out is None:\n                raise ValueError(\"normalization_in and normalization_out \" +\n                                 \"must both be specified.\")\n        if normalization_out is not None:\n            if type(normalization_out) != str:\n                raise ValueError('normalization_out must be a string. ' +\n                                 'Input type was {:s}'\n                                 .format(str(type(normalization_out))))\n            if normalization_out.lower() not in ('4pi', 'ortho', 'schmidt',\n                                                 'unnorm'):\n                raise ValueError(\n                    \"normalization_out must be '4pi', 'ortho', 'schmidt', or\" +\n                    \" 'unnorm'. Provided value was {:s}\"\n                    .format(repr(normalization_out))\n                    )\n            if normalization_in is None:\n                raise ValueError(\"normalization_in and normalization_out \" +\n                                 \"must both be specified.\")\n        if csphase_in is not None:\n            if csphase_in != 1 and csphase_in != -1:\n                raise ValueError(\n                    \"csphase_in must be 1 or -1. Input value was {:s}\"\n                    .format(repr(csphase_in)))\n            if csphase_out is None:\n                raise ValueError(\"csphase_in and csphase_out must both be \" +\n                                 \"specified.\")\n        if csphase_out is not None:\n            if csphase_out != 1 and csphase_out != -1:\n                raise ValueError(\n                    \"csphase_out must be 1 or -1. Input value was {:s}\"\n                    .format(repr(csphase_out)))\n            if csphase_in is None:\n                raise ValueError(\"csphase_in and csphase_out must both be \" +\n                                 \"specified.\")\n\n        lmaxin = coeffs_in.shape[1] - 1\n\n        if ((normalization_in == 'unnorm' or normalization_out ==\n                'unnorm') and lmaxin > 85):\n            _warnings.warn(\"Conversions with unnormalized coefficients are \" +\n                           \"stable only for degrees less than or equal to \" +\n                           \"85. lmax of the input coefficients will be \" +\n                           \"truncated after degree 85. The spherical \" +\n                           \"harmonic degree of the input coefficients was \" +\n                           \"{:d}.\".format(lmaxin), category=RuntimeWarning)\n            lmaxin = 85\n\n        if lmax is None:\n            lmaxout = lmaxin\n        else:\n            lmaxout = lmax\n\n        lconv = min(lmaxin, lmaxout)\n        degrees = _np.arange(lconv + 1)\n\n        if _np.iscomplexobj(coeffs_in):\n            coeffs = _np.zeros((2, lmaxout+1, lmaxout+1), dtype=complex)\n        else:\n            coeffs = _np.zeros((2, lmaxout+1, lmaxout+1))\n\n        coeffs[:, :lconv+1, :lconv+1] = coeffs_in[:, :lconv+1, :lconv+1]\n\n        if normalization_in == normalization_out:\n            pass\n        elif normalization_in == '4pi' and normalization_out == 'schmidt':\n            for l in degrees:\n                coeffs[:, l, :l+1] *= _np.sqrt(2. * l + 1.)\n        elif normalization_in == '4pi' and normalization_out == 'ortho':\n            coeffs *= _np.sqrt(4. * _np.pi)\n        elif normalization_in == '4pi' and normalization_out == 'unnorm':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = (2. * l + 1.) * _factorial(l-ms) / _factorial(l+ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] *= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n        elif normalization_in == 'schmidt' and normalization_out == '4pi':\n            for l in degrees:\n                coeffs[:, l, :l+1] /= _np.sqrt(2. * l + 1.)\n        elif normalization_in == 'schmidt' and normalization_out == 'ortho':\n            for l in degrees:\n                coeffs[:, l, :l+1] *= _np.sqrt(4. * _np.pi / (2. * l + 1.))\n        elif normalization_in == 'schmidt' and normalization_out == 'unnorm':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = _factorial(l-ms) / _factorial(l+ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] *= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n        elif normalization_in == 'ortho' and normalization_out == '4pi':\n            coeffs /= _np.sqrt(4. * _np.pi)\n        elif normalization_in == 'ortho' and normalization_out == 'schmidt':\n            for l in degrees:\n                coeffs[:, l, :l+1] *= _np.sqrt((2. * l + 1.) / (4. * _np.pi))\n        elif normalization_in == 'ortho' and normalization_out == 'unnorm':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = (2. * l + 1.) * _factorial(l-ms) \\\n                    / 4. / _np.pi / _factorial(l+ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] *= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n        elif normalization_in == 'unnorm' and normalization_out == '4pi':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = _factorial(l+ms) / (2. * l + 1.) / _factorial(l-ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] /= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n        elif normalization_in == 'unnorm' and normalization_out == 'schmidt':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = _factorial(l+ms) / _factorial(l-ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] /= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n        elif normalization_in == 'unnorm' and normalization_out == 'ortho':\n            for l in degrees:\n                ms = _np.arange(l+1)\n                conv = 4. * _np.pi * _factorial(l+ms) / (2. * l + 1.) / \\\n                    _factorial(l-ms)\n                if not _np.iscomplexobj(coeffs):\n                    conv[1:] /= 2.\n                coeffs[:, l, :l+1] *= _np.sqrt(conv)\n\n        if csphase_in != csphase_out:\n            for m in degrees:\n                if m % 2 == 1:\n                    coeffs[:, m:lconv+1, m] = - coeffs[:, m:lconv+1, m]\n\n        return coeffs", "response": "This routine converts an array of spherical harmonic coefficients to a different Condon - Shortley version of the new spherical harmonic degree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a new instance of the class instance for the given spherical cap localization windows.", "response": "def from_cap(cls, theta, lwin, clat=None, clon=None, nwin=None,\n                 theta_degrees=True, coord_degrees=True, dj_matrix=None,\n                 weights=None):\n        \"\"\"\n        Construct spherical cap localization windows.\n\n        Usage\n        -----\n        x = SHWindow.from_cap(theta, lwin, [clat, clon, nwin, theta_degrees,\n                                            coord_degrees, dj_matrix, weights])\n\n        Returns\n        -------\n        x : SHWindow class instance\n\n        Parameters\n        ----------\n        theta : float\n            Angular radius of the spherical cap localization domain (default\n            in degrees).\n        lwin : int\n            Spherical harmonic bandwidth of the localization windows.\n        clat, clon : float, optional, default = None\n            Latitude and longitude of the center of the rotated spherical cap\n            localization windows (default in degrees).\n        nwin : int, optional, default (lwin+1)**2\n            Number of localization windows.\n        theta_degrees : bool, optional, default = True\n            True if theta is in degrees.\n        coord_degrees : bool, optional, default = True\n            True if clat and clon are in degrees.\n        dj_matrix : ndarray, optional, default = None\n            The djpi2 rotation matrix computed by a call to djpi2.\n        weights : ndarray, optional, default = None\n            Taper weights used with the multitaper spectral analyses.\n        \"\"\"\n        if theta_degrees:\n            tapers, eigenvalues, taper_order = _shtools.SHReturnTapers(\n                _np.radians(theta), lwin)\n        else:\n            tapers, eigenvalues, taper_order = _shtools.SHReturnTapers(\n                theta, lwin)\n\n        return SHWindowCap(theta, tapers, eigenvalues, taper_order,\n                           clat, clon, nwin, theta_degrees, coord_degrees,\n                           dj_matrix, weights, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_mask(cls, dh_mask, lwin, nwin=None, weights=None):\n        if nwin is None:\n            nwin = (lwin + 1)**2\n        else:\n            if nwin > (lwin + 1)**2:\n                raise ValueError('nwin must be less than or equal to ' +\n                                 '(lwin + 1)**2. lwin = {:d} and nwin = {:d}'\n                                 .format(lwin, nwin))\n\n        if dh_mask.shape[0] % 2 != 0:\n            raise ValueError('The number of latitude bands in dh_mask ' +\n                             'must be even. nlat = {:d}'\n                             .format(dh_mask.shape[0]))\n\n        if dh_mask.shape[1] == dh_mask.shape[0]:\n            _sampling = 1\n        elif dh_mask.shape[1] == 2 * dh_mask.shape[0]:\n            _sampling = 2\n        else:\n            raise ValueError('dh_mask must be dimensioned as (n, n) or ' +\n                             '(n, 2 * n). Input shape is ({:d}, {:d})'\n                             .format(dh_mask.shape[0], dh_mask.shape[1]))\n\n        mask_lm = _shtools.SHExpandDH(dh_mask, sampling=_sampling, lmax_calc=0)\n        area = mask_lm[0, 0, 0] * 4 * _np.pi\n\n        tapers, eigenvalues = _shtools.SHReturnTapersMap(dh_mask, lwin,\n                                                         ntapers=nwin)\n\n        return SHWindowMask(tapers, eigenvalues, weights, area, copy=False)", "response": "Construct a new instance of the appropriate localization windows from a mask."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the spherical harmonic coefficients of the taper i as a numpy array.", "response": "def to_array(self, itaper, normalization='4pi', csphase=1):\n        \"\"\"\n        Return the spherical harmonic coefficients of taper i as a numpy\n        array.\n\n        Usage\n        -----\n        coeffs = x.to_array(itaper, [normalization, csphase])\n\n        Returns\n        -------\n        coeffs : ndarray, shape (2, lwin+1, lwin+11)\n            3-D numpy ndarray of the spherical harmonic coefficients of the\n            window.\n\n        Parameters\n        ----------\n        itaper : int\n            Taper number, where itaper=0 is the best concentrated.\n        normalization : str, optional, default = '4pi'\n            Normalization of the output coefficients: '4pi', 'ortho' or\n            'schmidt' for geodesy 4pi normalized, orthonormalized, or Schmidt\n            semi-normalized coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        \"\"\"\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt'):\n            raise ValueError(\n                \"normalization must be '4pi', 'ortho' \" +\n                \"or 'schmidt'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        return self._to_array(\n            itaper, normalization=normalization.lower(), csphase=csphase)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the spherical harmonic coefficients of taper i as a SHCoeffs class instance.", "response": "def to_shcoeffs(self, itaper, normalization='4pi', csphase=1):\n        \"\"\"\n        Return the spherical harmonic coefficients of taper i as a SHCoeffs\n        class instance.\n\n        Usage\n        -----\n        clm = x.to_shcoeffs(itaper, [normalization, csphase])\n\n        Returns\n        -------\n        clm : SHCoeffs class instance\n\n        Parameters\n        ----------\n        itaper : int\n            Taper number, where itaper=0 is the best concentrated.\n        normalization : str, optional, default = '4pi'\n            Normalization of the output class: '4pi', 'ortho' or 'schmidt' for\n            geodesy 4pi-normalized, orthonormalized, or Schmidt semi-normalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        \"\"\"\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in set(['4pi', 'ortho', 'schmidt']):\n            raise ValueError(\n                \"normalization must be '4pi', 'ortho' \" +\n                \"or 'schmidt'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        coeffs = self.to_array(itaper, normalization=normalization.lower(),\n                               csphase=csphase)\n        return SHCoeffs.from_array(coeffs, normalization=normalization.lower(),\n                                   csphase=csphase, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates the coefficients of taper i on a spherical grid and return a SHGrid class instance.", "response": "def to_shgrid(self, itaper, grid='DH2', zeros=None):\n        \"\"\"\n        Evaluate the coefficients of taper i on a spherical grid and return\n        a SHGrid class instance.\n\n        Usage\n        -----\n        f = x.to_shgrid(itaper, [grid, zeros])\n\n        Returns\n        -------\n        f : SHGrid class instance\n\n        Parameters\n        ----------\n        itaper : int\n            Taper number, where itaper=0 is the best concentrated.\n        grid : str, optional, default = 'DH2'\n            'DH' or 'DH1' for an equisampled lat/lon grid with nlat=nlon, 'DH2'\n            for an equidistant lat/lon grid with nlon=2*nlat, or 'GLQ' for a\n            Gauss-Legendre quadrature grid.\n        zeros : ndarray, optional, default = None\n            The cos(colatitude) nodes used in the Gauss-Legendre Quadrature\n            grids.\n\n        Description\n        -----------\n        For more information concerning the spherical harmonic expansions and\n        the properties of the output grids, see the documentation for\n        SHExpandDH and SHExpandGLQ.\n        \"\"\"\n        if type(grid) != str:\n            raise ValueError('grid must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(grid))))\n\n        if grid.upper() in ('DH', 'DH1'):\n            gridout = _shtools.MakeGridDH(self.to_array(itaper), sampling=1,\n                                          norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='DH', copy=False)\n        elif grid.upper() == 'DH2':\n            gridout = _shtools.MakeGridDH(self.to_array(itaper), sampling=2,\n                                          norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='DH', copy=False)\n        elif grid.upper() == 'GLQ':\n            if zeros is None:\n                zeros, weights = _shtools.SHGLQ(self.lwin)\n            gridout = _shtools.MakeGridGLQ(self.to_array(itaper), zeros,\n                                           norm=1, csphase=1)\n            return SHGrid.from_array(gridout, grid='GLQ', copy=False)\n        else:\n            raise ValueError(\n                \"grid must be 'DH', 'DH1', 'DH2', or 'GLQ'. \" +\n                \"Input value was {:s}\".format(repr(grid)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the multitaper spectrum estimate and standard error of the multitaper spectrum for the given local field.", "response": "def multitaper_spectrum(self, clm, k, convention='power', unit='per_l',\n                            **kwargs):\n        \"\"\"\n        Return the multitaper spectrum estimate and standard error.\n\n        Usage\n        -----\n        mtse, sd = x.multitaper_spectrum(clm, k, [convention, unit, lmax,\n                                                  taper_wt, clat, clon,\n                                                  coord_degrees])\n\n        Returns\n        -------\n        mtse : ndarray, shape (lmax-lwin+1)\n            The localized multitaper spectrum estimate, where lmax is the\n            spherical-harmonic bandwidth of clm, and lwin is the\n            spherical-harmonic bandwidth of the localization windows.\n        sd : ndarray, shape (lmax-lwin+1)\n            The standard error of the localized multitaper spectrum\n            estimate.\n\n        Parameters\n        ----------\n        clm : SHCoeffs class instance\n            SHCoeffs class instance containing the spherical harmonic\n            coefficients of the global field to analyze.\n        k : int\n            The number of tapers to be utilized in performing the multitaper\n            spectral analysis.\n        convention : str, optional, default = 'power'\n            The type of output spectra: 'power' for power spectra, and\n            'energy' for energy spectra.\n        unit : str, optional, default = 'per_l'\n            The units of the output spectra. If 'per_l', the spectra contain\n            the total contribution for each spherical harmonic degree l. If\n            'per_lm', the spectra contain the average contribution for each\n            coefficient at spherical harmonic degree l.\n        lmax : int, optional, default = clm.lmax\n            The maximum spherical-harmonic degree of clm to use.\n        taper_wt : ndarray, optional, default = None\n            1-D numpy array of the weights used in calculating the multitaper\n            spectral estimates and standard error.\n        clat, clon : float, optional, default = 90., 0.\n            Latitude and longitude of the center of the spherical-cap\n            localization windows.\n        coord_degrees : bool, optional, default = True\n            True if clat and clon are in degrees.\n        \"\"\"\n        return self._multitaper_spectrum(clm, k, convention=convention,\n                                         unit=unit, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multitaper_cross_spectrum(self, clm, slm, k, convention='power',\n                                  unit='per_l', **kwargs):\n        \"\"\"\n        Return the multitaper cross-spectrum estimate and standard error.\n\n        Usage\n        -----\n        mtse, sd = x.multitaper_cross_spectrum(clm, slm, k, [convention, unit,\n                                                             lmax, taper_wt,\n                                                             clat, clon,\n                                                             coord_degrees])\n\n        Returns\n        -------\n        mtse : ndarray, shape (lmax-lwin+1)\n            The localized multitaper cross-spectrum estimate, where lmax is the\n            smaller of the two spherical-harmonic bandwidths of clm and slm,\n            and lwin is the spherical-harmonic bandwidth of the localization\n            windows.\n        sd : ndarray, shape (lmax-lwin+1)\n            The standard error of the localized multitaper cross-spectrum\n            estimate.\n\n        Parameters\n        ----------\n        clm : SHCoeffs class instance\n            SHCoeffs class instance containing the spherical harmonic\n            coefficients of the first global field to analyze.\n        slm : SHCoeffs class instance\n            SHCoeffs class instance containing the spherical harmonic\n            coefficients of the second global field to analyze.\n        k : int\n            The number of tapers to be utilized in performing the multitaper\n            spectral analysis.\n        convention : str, optional, default = 'power'\n            The type of output spectra: 'power' for power spectra, and\n            'energy' for energy spectra.\n        unit : str, optional, default = 'per_l'\n            The units of the output spectra. If 'per_l', the spectra contain\n            the total contribution for each spherical harmonic degree l. If\n            'per_lm', the spectra contain the average contribution for each\n            coefficient at spherical harmonic degree l.\n        lmax : int, optional, default = min(clm.lmax, slm.lmax)\n            The maximum spherical-harmonic degree of the input coefficients\n            to use.\n        taper_wt : ndarray, optional, default = None\n            The weights used in calculating the multitaper cross-spectral\n            estimates and standard error.\n        clat, clon : float, optional, default = 90., 0.\n            Latitude and longitude of the center of the spherical-cap\n            localization windows.\n        coord_degrees : bool, optional, default = True\n            True if clat and clon are in degrees.\n        \"\"\"\n        return self._multitaper_cross_spectrum(clm, slm, k,\n                                               convention=convention,\n                                               unit=unit, **kwargs)", "response": "Returns the multitaper cross - spectrum estimate and standard error of the multitaper cross - spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef biased_spectrum(self, power, k, convention='power', unit='per_l',\n                        **kwargs):\n        \"\"\"\n        Calculate the multitaper (cross-)spectrum expectation of a\n        localized function.\n\n        Usage\n        -----\n        outspectrum = x.biased_spectrum(spectrum, k, [unit, power, taper_wt,\n                                                      save_cg, ldata])\n\n        Returns\n        -------\n        outspectrum : ndarray, shape (ldata+lwin+1)\n            The expectation of the windowed spectrum, where ldata is the\n            spherical-harmonic bandwidth of the input spectrum, and lwin is the\n            spherical-harmonic bandwidth of the localization windows.\n\n        Parameters\n        ----------\n        spectrum : ndarray, shape (ldata+1)\n            The global spectrum.\n        k : int\n            The number of best-concentrated localization windows to use in\n            constructing the windowed spectrum.\n        convention : str, optional, default = 'power'\n            The type of input global and output biased spectra: 'power' for\n            power spectra, and 'energy' for energy spectra.\n        unit : str, optional, default = 'per_l'\n            The units of the input global and output biased spectra. If\n            'per_l', the spectra contain the total contribution for each\n            spherical harmonic degree l. If 'per_lm', the spectra contain the\n            average contribution for each coefficient at spherical harmonic\n            degree l.\n        taper_wt : ndarray, optional, default = None\n            The weights used in calculating the multitaper spectral estimates\n            and standard error.\n        save_cg : int, optional, default = 0\n            If 1, the Clebsch-Gordon coefficients will be precomputed and saved\n            for future use. If 0, the Clebsch-Gordon coefficients will be\n            recomputed for each call.\n        ldata : int, optional, default = len(power)-1\n            The maximum degree of the global unwindowed spectrum.\n        \"\"\"\n        return self._biased_spectrum(power, k, convention=convention,\n                                     unit=unit, **kwargs)", "response": "Calculates the multitaper spectral expectation of the localization function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spectra(self, itaper=None, nwin=None, convention='power', unit='per_l',\n                base=10.):\n        \"\"\"\n        Return the spectra of one or more localization windows.\n\n        Usage\n        -----\n        spectra = x.spectra([itaper, nwin, convention, unit, base])\n\n        Returns\n        -------\n        spectra : ndarray, shape (lwin+1, nwin)\n             A matrix with each column containing the spectrum of a\n             localization window, and where the windows are arranged with\n             increasing concentration factors. If itaper is set, only a single\n             vector is returned, whereas if nwin is set, the first nwin spectra\n             are returned.\n\n        Parameters\n        ----------\n        itaper : int, optional, default = None\n            The taper number of the output spectrum, where itaper=0\n            corresponds to the best concentrated taper.\n        nwin : int, optional, default = 1\n            The number of best concentrated localization window power spectra\n            to return.\n        convention : str, optional, default = 'power'\n            The type of spectrum to return: 'power' for power spectrum,\n            'energy' for energy spectrum, and 'l2norm' for the l2 norm\n            spectrum.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', return the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', return the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', return the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum.\n\n        Description\n        -----------\n        This function returns either the power spectrum, energy spectrum, or\n        l2-norm spectrum of one or more of the localization windows.\n        Total power is defined as the integral of the function squared over all\n        space, divided by the area the function spans. If the mean of the\n        function is zero, this is equivalent to the variance of the function.\n        The total energy is the integral of the function squared over all space\n        and is 4pi times the total power. The l2-norm is the sum of the\n        magnitude of the coefficients squared.\n\n        The output spectrum can be expresed using one of three units. 'per_l'\n        returns the contribution to the total spectrum from all angular orders\n        at degree l. 'per_lm' returns the average contribution to the total\n        spectrum from a single coefficient at degree l. The 'per_lm' spectrum\n        is equal to the 'per_l' spectrum divided by (2l+1). 'per_dlogl' returns\n        the contribution to the total spectrum from all angular orders over an\n        infinitessimal logarithmic degree band. The contrubution in the band\n        dlog_a(l) is spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base,\n        and where spectrum(l, 'per_dlogl) is equal to\n        spectrum(l, 'per_l')*l*log(a).\n\n         \"\"\"\n        if itaper is None:\n            if nwin is None:\n                nwin = self.nwin\n            spectra = _np.zeros((self.lwin+1, nwin))\n\n            for iwin in range(nwin):\n                coeffs = self.to_array(iwin)\n                spectra[:, iwin] = _spectrum(coeffs, normalization='4pi',\n                                             convention=convention, unit=unit,\n                                             base=base)\n        else:\n            coeffs = self.to_array(itaper)\n            spectra = _spectrum(coeffs, normalization='4pi',\n                                convention=convention, unit=unit, base=base)\n\n        return spectra", "response": "Returns the spectra of one or more localization windows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef coupling_matrix(self, lmax, nwin=None, weights=None, mode='full'):\n        if weights is not None:\n            if nwin is not None:\n                if len(weights) != nwin:\n                    raise ValueError(\n                        'Length of weights must be equal to nwin. ' +\n                        'len(weights) = {:d}, nwin = {:d}'.format(len(weights),\n                                                                  nwin))\n            else:\n                if len(weights) != self.nwin:\n                    raise ValueError(\n                        'Length of weights must be equal to nwin. ' +\n                        'len(weights) = {:d}, nwin = {:d}'.format(len(weights),\n                                                                  self.nwin))\n\n        if mode == 'full':\n            return self._coupling_matrix(lmax, nwin=nwin, weights=weights)\n        elif mode == 'same':\n            cmatrix = self._coupling_matrix(lmax, nwin=nwin,\n                                            weights=weights)\n            return cmatrix[:lmax+1, :]\n        elif mode == 'valid':\n            cmatrix = self._coupling_matrix(lmax, nwin=nwin,\n                                            weights=weights)\n            return cmatrix[:lmax - self.lwin+1, :]\n        else:\n            raise ValueError(\"mode has to be 'full', 'same' or 'valid', not \"\n                             \"{}\".format(mode))", "response": "Returns the coupling matrix of the first nwin tapers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the best concentrated localization windows.", "response": "def plot_windows(self, nwin, lmax=None, maxcolumns=3,\n                     tick_interval=[60, 45], minor_tick_interval=None,\n                     xlabel='Longitude', ylabel='Latitude',\n                     axes_labelsize=None, tick_labelsize=None,\n                     title_labelsize=None, grid=False, show=True, title=True,\n                     ax=None, fname=None):\n        \"\"\"\n        Plot the best-concentrated localization windows.\n\n        Usage\n        -----\n        x.plot_windows(nwin, [lmax, maxcolumns, tick_interval,\n                              minor_tick_interval, xlabel, ylabel, grid, show,\n                              title, axes_labelsize, tick_labelsize,\n                              title_labelsize, ax, fname])\n\n        Parameters\n        ----------\n        nwin : int\n            The number of localization windows to plot.\n        lmax : int, optional, default = self.lwin\n            The maximum degree to use when plotting the windows, which controls\n            the number of samples in latitude and longitude.\n        maxcolumns : int, optional, default = 3\n            The maximum number of columns to use when plotting multiple\n            localization windows.\n        tick_interval : list or tuple, optional, default = [60, 45]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = None\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        grid : bool, optional, default = False\n            If True, plot grid lines.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        title : bool, optional, default = True\n            If True, plot a title on top of each subplot providing the taper\n            number and 1 minus the concentration factor.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        title_labelsize : int, optional, default = None\n            The font size for the subplot titles.\n        ax : matplotlib axes object, optional, default = None\n            An array of matplotlib axes objects where the plots will appear.\n        fname : str, optional, default = None\n            If present, save the image to the specified file.\n        \"\"\"\n        if self.kind == 'cap':\n            if self.nwinrot is not None and self.nwinrot <= nwin:\n                nwin = self.nwinrot\n\n        ncolumns = min(maxcolumns, nwin)\n        nrows = _np.ceil(nwin / ncolumns).astype(int)\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                   _mpl.rcParams['figure.figsize'][0]\n                   * 0.55 * nrows / ncolumns + 0.41)\n\n        if ax is None:\n            fig, axes = _plt.subplots(nrows, ncolumns, figsize=figsize,\n                                      sharex='all', sharey='all')\n        else:\n            if hasattr(ax, 'flatten') and ax.size < nwin:\n                raise ValueError('ax.size must be greater or equal to nwin. ' +\n                                 'nwin = {:s}'.format(repr(nwin)) +\n                                 ' and ax.size = {:s}.'.format(repr(ax.size)))\n            axes = ax\n\n        if tick_interval is None:\n            xticks = []\n            yticks = []\n        else:\n            xticks = _np.linspace(0, 360, num=360//tick_interval[0]+1,\n                                  endpoint=True)\n            yticks = _np.linspace(-90, 90, num=180//tick_interval[1]+1,\n                                  endpoint=True)\n\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n        if title_labelsize is None:\n            title_labelsize = _mpl.rcParams['axes.titlesize']\n\n        if minor_tick_interval is None:\n            minor_xticks = []\n            minor_yticks = []\n        else:\n            minor_xticks = _np.linspace(\n                0, 360, num=360//minor_tick_interval[0]+1, endpoint=True)\n            minor_yticks = _np.linspace(\n                -90, 90, num=180//minor_tick_interval[1]+1, endpoint=True)\n\n        deg = '$^{\\circ}$'\n        xticklabels = [str(int(y)) + deg for y in xticks]\n        yticklabels = [str(int(y)) + deg for y in yticks]\n\n        if ax is None:\n            if nrows > 1:\n                for axtemp in axes[:-1, :].flatten():\n                    for xlabel_i in axtemp.get_xticklabels():\n                        xlabel_i.set_visible(False)\n                    axtemp.set_xlabel('', visible=False)\n                for axtemp in axes[:, 1:].flatten():\n                    for ylabel_i in axtemp.get_yticklabels():\n                        ylabel_i.set_visible(False)\n                    axtemp.set_ylabel('', visible=False)\n            elif nwin > 1:\n                for axtemp in axes[1:].flatten():\n                    for ylabel_i in axtemp.get_yticklabels():\n                        ylabel_i.set_visible(False)\n                    axtemp.set_ylabel('', visible=False)\n\n        for itaper in range(min(self.nwin, nwin)):\n            evalue = self.eigenvalues[itaper]\n            if min(self.nwin, nwin) == 1 and ax is None:\n                axtemp = axes\n            elif hasattr(axes, 'flatten'):\n                axtemp = axes.flatten()[itaper]\n            else:\n                axtemp = axes[itaper]\n            gridout = _shtools.MakeGridDH(self.to_array(itaper), sampling=2,\n                                          lmax=lmax, norm=1, csphase=1)\n            axtemp.imshow(gridout, origin='upper',\n                          extent=(0., 360., -90., 90.))\n            axtemp.set(xticks=xticks, yticks=yticks)\n            axtemp.set_xlabel(xlabel, fontsize=axes_labelsize)\n            axtemp.set_ylabel(ylabel, fontsize=axes_labelsize)\n            axtemp.set_xticklabels(xticklabels, fontsize=tick_labelsize)\n            axtemp.set_yticklabels(yticklabels, fontsize=tick_labelsize)\n            axtemp.set_xticks(minor_xticks, minor=True)\n            axtemp.set_yticks(minor_yticks, minor=True)\n            axtemp.grid(grid, which='major')\n            if title is True:\n                axtemp.set_title('#{:d} [loss={:2.2g}]'\n                                 .format(itaper, 1-evalue),\n                                 fontsize=title_labelsize)\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_coupling_matrix(self, lmax, nwin=None, weights=None, mode='full',\n                             axes_labelsize=None, tick_labelsize=None,\n                             show=True, ax=None, fname=None):\n        \"\"\"\n        Plot the multitaper coupling matrix.\n\n        This matrix relates the global power spectrum to the expectation of\n        the localized multitaper spectrum.\n\n        Usage\n        -----\n        x.plot_coupling_matrix(lmax, [nwin, weights, mode, axes_labelsize,\n                                      tick_labelsize, show, ax, fname])\n\n        Parameters\n        ----------\n        lmax : int\n            Spherical harmonic bandwidth of the global power spectrum.\n        nwin : int, optional, default = x.nwin\n            Number of tapers used in the mutlitaper spectral analysis.\n        weights : ndarray, optional, default = x.weights\n            Taper weights used with the multitaper spectral analyses.\n        mode : str, opitonal, default = 'full'\n            'full' returns a biased output spectrum of size lmax+lwin+1. The\n            input spectrum is assumed to be zero for degrees l>lmax.\n            'same' returns a biased output spectrum with the same size\n            (lmax+1) as the input spectrum. The input spectrum is assumed to be\n            zero for degrees l>lmax.\n            'valid' returns a biased spectrum with size lmax-lwin+1. This\n            returns only that part of the biased spectrum that is not\n            influenced by the input spectrum beyond degree lmax.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        ax : matplotlib axes object, optional, default = None\n            An array of matplotlib axes objects where the plots will appear.\n        fname : str, optional, default = None\n            If present, save the image to the specified file.\n        \"\"\"\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                   _mpl.rcParams['figure.figsize'][0])\n\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n\n        if ax is None:\n            fig = _plt.figure(figsize=figsize)\n            axes = fig.add_subplot(111)\n        else:\n            axes = ax\n\n        axes.imshow(self.coupling_matrix(lmax, nwin=nwin, weights=weights,\n                                         mode=mode), aspect='auto')\n        axes.set_xlabel('Input power', fontsize=axes_labelsize)\n        axes.set_ylabel('Output power', fontsize=axes_labelsize)\n        axes.tick_params(labelsize=tick_labelsize)\n        axes.minorticks_on()\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes", "response": "Plot the multitaper coupling matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the spherical harmonic coefficients of the unrotated taper i", "response": "def _taper2coeffs(self, itaper):\n        \"\"\"\n        Return the spherical harmonic coefficients of the unrotated taper i\n        as an array, where i = 0 is the best concentrated.\n        \"\"\"\n        taperm = self.orders[itaper]\n        coeffs = _np.zeros((2, self.lwin + 1, self.lwin + 1))\n        if taperm < 0:\n            coeffs[1, :, abs(taperm)] = self.tapers[:, itaper]\n        else:\n            coeffs[0, :, abs(taperm)] = self.tapers[:, itaper]\n\n        return coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the spherical harmonic coefficients of taper i as an array.", "response": "def _to_array(self, itaper, normalization='4pi', csphase=1):\n        \"\"\"\n        Return the spherical harmonic coefficients of taper i as an\n        array, where i = 0 is the best concentrated.\n        \"\"\"\n        if self.coeffs is None:\n            coeffs = _np.copy(self._taper2coeffs(itaper))\n        else:\n            if itaper > self.nwinrot - 1:\n                raise ValueError('itaper must be less than or equal to ' +\n                                 'nwinrot - 1. itaper = {:d}, nwinrot = {:d}'\n                                 .format(itaper, self.nwinrot))\n            coeffs = _shtools.SHVectorToCilm(self.coeffs[:, itaper])\n\n        if normalization == 'schmidt':\n            for l in range(self.lwin + 1):\n                coeffs[:, l, :l+1] *= _np.sqrt(2.0 * l + 1.0)\n        elif normalization == 'ortho':\n            coeffs *= _np.sqrt(4.0 * _np.pi)\n\n        if csphase == -1:\n            for m in range(self.lwin + 1):\n                if m % 2 == 1:\n                    coeffs[:, :, m] = - coeffs[:, :, m]\n\n        return coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rotate(self, clat, clon, coord_degrees=True, dj_matrix=None,\n               nwinrot=None):\n        \"\"\"\"\n        Rotate the spherical-cap windows centered on the North pole to clat\n        and clon, and save the spherical harmonic coefficients in the\n        attribute coeffs.\n\n        Usage\n        -----\n        x.rotate(clat, clon [coord_degrees, dj_matrix, nwinrot])\n\n        Parameters\n        ----------\n        clat, clon : float\n            Latitude and longitude of the center of the rotated spherical-cap\n            localization windows (default in degrees).\n        coord_degrees : bool, optional, default = True\n            True if clat and clon are in degrees.\n        dj_matrix : ndarray, optional, default = None\n            The djpi2 rotation matrix computed by a call to djpi2.\n        nwinrot : int, optional, default = (lwin+1)**2\n            The number of best concentrated windows to rotate, where lwin is\n            the spherical harmonic bandwidth of the localization windows.\n\n        Description\n        -----------\n        This function will take the spherical-cap localization windows\n        centered at the North pole (and saved in the attributes tapers and\n        orders), rotate each function to the coordinate (clat, clon), and save\n        the spherical harmonic coefficients in the attribute coeffs. Each\n        column of coeffs contains a single window, and the coefficients are\n        ordered according to the convention in SHCilmToVector.\n        \"\"\"\n        self.coeffs = _np.zeros(((self.lwin + 1)**2, self.nwin))\n        self.clat = clat\n        self.clon = clon\n        self.coord_degrees = coord_degrees\n\n        if nwinrot is not None:\n            self.nwinrot = nwinrot\n        else:\n            self.nwinrot = self.nwin\n\n        if self.coord_degrees:\n            angles = _np.radians(_np.array([0., -(90. - clat), -clon]))\n        else:\n            angles = _np.array([0., -(_np.pi/2. - clat), -clon])\n\n        if dj_matrix is None:\n            if self.dj_matrix is None:\n                self.dj_matrix = _shtools.djpi2(self.lwin + 1)\n                dj_matrix = self.dj_matrix\n            else:\n                dj_matrix = self.dj_matrix\n\n        if ((coord_degrees is True and clat == 90. and clon == 0.) or\n                (coord_degrees is False and clat == _np.pi/2. and clon == 0.)):\n            for i in range(self.nwinrot):\n                coeffs = self._taper2coeffs(i)\n                self.coeffs[:, i] = _shtools.SHCilmToVector(coeffs)\n\n        else:\n            coeffs = _shtools.SHRotateTapers(self.tapers, self.orders,\n                                             self.nwinrot, angles, dj_matrix)\n            self.coeffs = coeffs", "response": "This function rotates the spherical - cap localization windows centered at clat and clon and saves the spherical harmonic coefficients in the attribute coeffs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _coupling_matrix(self, lmax, nwin=None, weights=None):\n        if nwin is None:\n            nwin = self.nwin\n\n        if weights is None:\n            weights = self.weights\n\n        if weights is None:\n            return _shtools.SHMTCouplingMatrix(lmax, self.tapers**2, k=nwin)\n        else:\n            return _shtools.SHMTCouplingMatrix(lmax, self.tapers**2, k=nwin,\n                                               taper_wt=self.weights)", "response": "Return the coupling matrix of the first nwin tapers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the multitaper spectrum estimate and standard error for an input SHCoeffs class instance.", "response": "def _multitaper_spectrum(self, clm, k, convention='power', unit='per_l',\n                             clat=None, clon=None, coord_degrees=True,\n                             lmax=None, taper_wt=None):\n        \"\"\"\n        Return the multitaper spectrum estimate and standard error for an\n        input SHCoeffs class instance.\n        \"\"\"\n        if lmax is None:\n            lmax = clm.lmax\n\n        if (clat is not None and clon is not None and clat == self.clat and\n                clon == self.clon and coord_degrees is self.coord_degrees and\n                k <= self.nwinrot):\n            # use the already stored coeffs\n            pass\n        elif (clat is None and clon is None) and \\\n                (self.clat is not None and self.clon is not None and\n                 k <= self.nwinrot):\n            # use the already stored coeffs\n            pass\n        else:\n            if clat is None:\n                clat = self.clat\n            if clon is None:\n                clon = self.clon\n            if (clat is None and clon is not None) or \\\n                    (clat is not None and clon is None):\n                raise ValueError('clat and clon must both be input. ' +\n                                 'clat = {:s}, clon = {:s}'\n                                 .format(repr(clat), repr(clon)))\n            if clat is None and clon is None:\n                self.rotate(clat=90., clon=0., coord_degrees=True, nwinrot=k)\n            else:\n                self.rotate(clat=clat, clon=clon, coord_degrees=coord_degrees,\n                            nwinrot=k)\n\n        sh = clm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n\n        if taper_wt is None:\n            mtse, sd = _shtools.SHMultiTaperMaskSE(sh, self.coeffs,\n                                                   lmax=lmax, k=k)\n        else:\n            mtse, sd = _shtools.SHMultiTaperMaskSE(sh, self.coeffs, lmax=lmax,\n                                                   k=k, taper_wt=taper_wt)\n\n        if (unit == 'per_l'):\n            pass\n        elif (unit == 'per_lm'):\n            degree_l = _np.arange(len(mtse))\n            mtse /= (2.0 * degree_l + 1.0)\n            sd /= (2.0 * degree_l + 1.0)\n        else:\n            raise ValueError(\n                \"unit must be 'per_l' or 'per_lm'.\" +\n                \"Input value was {:s}\".format(repr(unit)))\n\n        if (convention == 'power'):\n            return mtse, sd\n        elif (convention == 'energy'):\n            return mtse * 4.0 * _np.pi, sd * 4.0 * _np.pi\n        else:\n            raise ValueError(\n                \"convention must be 'power' or 'energy'.\" +\n                \"Input value was {:s}\".format(repr(convention)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _multitaper_cross_spectrum(self, clm, slm, k, convention='power',\n                                   unit='per_l', clat=None, clon=None,\n                                   coord_degrees=True, lmax=None,\n                                   taper_wt=None):\n        \"\"\"\n        Return the multitaper cross-spectrum estimate and standard error for\n        two input SHCoeffs class instances.\n        \"\"\"\n        if lmax is None:\n            lmax = min(clm.lmax, slm.lmax)\n\n        if (clat is not None and clon is not None and clat == self.clat and\n                clon == self.clon and coord_degrees is self.coord_degrees and\n                k <= self.nwinrot):\n            # use the already stored coeffs\n            pass\n        elif (clat is None and clon is None) and \\\n                (self.clat is not None and self.clon is not None and\n                 k <= self.nwinrot):\n            # use the already stored coeffs\n            pass\n        else:\n            if clat is None:\n                clat = self.clat\n            if clon is None:\n                clon = self.clon\n            if (clat is None and clon is not None) or \\\n                    (clat is not None and clon is None):\n                raise ValueError('clat and clon must both be input. ' +\n                                 'clat = {:s}, clon = {:s}'\n                                 .format(repr(clat), repr(clon)))\n            if clat is None and clon is None:\n                self.rotate(clat=90., clon=0., coord_degrees=True, nwinrot=k)\n            else:\n                self.rotate(clat=clat, clon=clon, coord_degrees=coord_degrees,\n                            nwinrot=k)\n\n        sh1 = clm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n        sh2 = slm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n\n        if taper_wt is None:\n            mtse, sd = _shtools.SHMultiTaperMaskCSE(sh1, sh2, self.coeffs,\n                                                    lmax1=lmax, lmax2=lmax,\n                                                    k=k)\n        else:\n            mtse, sd = _shtools.SHMultiTaperMaskCSE(sh1, sh2, self.coeffs,\n                                                    lmax1=lmax, lmax2=lmax,\n                                                    k=k, taper_wt=taper_wt)\n\n        if (unit == 'per_l'):\n            pass\n        elif (unit == 'per_lm'):\n            degree_l = _np.arange(len(mtse))\n            mtse /= (2.0 * degree_l + 1.0)\n            sd /= (2.0 * degree_l + 1.0)\n        else:\n            raise ValueError(\n                \"unit must be 'per_l' or 'per_lm'.\" +\n                \"Input value was {:s}\".format(repr(unit)))\n\n        if (convention == 'power'):\n            return mtse, sd\n        elif (convention == 'energy'):\n            return mtse * 4.0 * _np.pi, sd * 4.0 * _np.pi\n        else:\n            raise ValueError(\n                \"convention must be 'power' or 'energy'.\" +\n                \"Input value was {:s}\".format(repr(convention)))", "response": "This method computes the multitaper cross - spectrum estimate and standard error for a given set of class instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_array(self, itaper, normalization='4pi', csphase=1):\n        coeffs = _shtools.SHVectorToCilm(self.tapers[:, itaper])\n\n        if normalization == 'schmidt':\n            for l in range(self.lwin + 1):\n                coeffs[:, l, :l+1] *= _np.sqrt(2.0 * l + 1.0)\n        elif normalization == 'ortho':\n            coeffs *= _np.sqrt(4.0 * _np.pi)\n\n        if csphase == -1:\n            for m in range(self.lwin + 1):\n                if m % 2 == 1:\n                    coeffs[:, :, m] = - coeffs[:, :, m]\n\n        return coeffs", "response": "Return the spherical harmonic coefficients of taper i as an array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _coupling_matrix(self, lmax, nwin=None, weights=None):\n        if nwin is None:\n            nwin = self.nwin\n\n        if weights is None:\n            weights = self.weights\n\n        tapers_power = _np.zeros((self.lwin+1, nwin))\n        for i in range(nwin):\n            tapers_power[:, i] = _spectrum(self.to_array(i),\n                                           normalization='4pi',\n                                           convention='power', unit='per_l')\n\n        if weights is None:\n            return _shtools.SHMTCouplingMatrix(lmax, tapers_power, k=nwin)\n        else:\n            return _shtools.SHMTCouplingMatrix(lmax, tapers_power, k=nwin,\n                                               taper_wt=self.weights)", "response": "Return the coupling matrix of the first nwin tapers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the multitaper spectrum estimate and standard error for an analytical class instance.", "response": "def _multitaper_spectrum(self, clm, k, convention='power', unit='per_l',\n                             lmax=None, taper_wt=None):\n        \"\"\"\n        Return the multitaper spectrum estimate and standard error for an\n        input SHCoeffs class instance.\n        \"\"\"\n        if lmax is None:\n            lmax = clm.lmax\n\n        sh = clm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n\n        if taper_wt is None:\n            mtse, sd = _shtools.SHMultiTaperMaskSE(sh, self.tapers, lmax=lmax,\n                                                   k=k)\n        else:\n            mtse, sd = _shtools.SHMultiTaperMaskSE(sh, self.tapers, lmax=lmax,\n                                                   k=k, taper_wt=taper_wt)\n\n        if (unit == 'per_l'):\n            pass\n        elif (unit == 'per_lm'):\n            degree_l = _np.arange(len(mtse))\n            mtse /= (2.0 * degree_l + 1.0)\n            sd /= (2.0 * degree_l + 1.0)\n        else:\n            raise ValueError(\n                \"unit must be 'per_l' or 'per_lm'.\" +\n                \"Input value was {:s}\".format(repr(unit)))\n\n        if (convention == 'power'):\n            return mtse, sd\n        elif (convention == 'energy'):\n            return mtse * 4.0 * _np.pi, sd * 4.0 * _np.pi\n        else:\n            raise ValueError(\n                \"convention must be 'power' or 'energy'.\" +\n                \"Input value was {:s}\".format(repr(convention)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _multitaper_cross_spectrum(self, clm, slm, k, convention='power',\n                                   unit='per_l', lmax=None, taper_wt=None):\n        \"\"\"\n        Return the multitaper cross-spectrum estimate and standard error for\n        two input SHCoeffs class instances.\n        \"\"\"\n        if lmax is None:\n            lmax = min(clm.lmax, slm.lmax)\n\n        sh1 = clm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n        sh2 = slm.to_array(normalization='4pi', csphase=1, lmax=lmax)\n\n        if taper_wt is None:\n            mtse, sd = _shtools.SHMultiTaperMaskCSE(sh1, sh2, self.tapers,\n                                                    lmax=lmax, k=k)\n        else:\n            mtse, sd = _shtools.SHMultiTaperMaskCSE(sh1, sh2, self.tapers,\n                                                    lmax=lmax, k=k,\n                                                    taper_wt=taper_wt)\n\n        if (unit == 'per_l'):\n            pass\n        elif (unit == 'per_lm'):\n            degree_l = _np.arange(len(mtse))\n            mtse /= (2.0 * degree_l + 1.0)\n            sd /= (2.0 * degree_l + 1.0)\n        else:\n            raise ValueError(\n                \"unit must be 'per_l' or 'per_lm'.\" +\n                \"Input value was {:s}\".format(repr(unit)))\n\n        if (convention == 'power'):\n            return mtse, sd\n        elif (convention == 'energy'):\n            return mtse * 4.0 * _np.pi, sd * 4.0 * _np.pi\n        else:\n            raise ValueError(\n                \"convention must be 'power' or 'energy'.\" +\n                \"Input value was {:s}\".format(repr(convention)))", "response": "Returns the multitaper cross - spectrum estimate and standard error for a given set of class instances."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _biased_spectrum(self, spectrum, k, convention='power', unit='per_l',\n                         **kwargs):\n        \"\"\"\n        Calculate the multitaper (cross-) spectrum expectation of a function\n        localized by arbitary windows.\n        \"\"\"\n        # The equation is not modified if the in- and out- spectra are power\n        # or energy. However, the convention can not be l2norm, which depends\n        # upon the normalization of the coefficients.\n        if (convention != 'power' and convention != 'energy'):\n            raise ValueError(\n                \"convention must be 'power' or 'energy'.\" +\n                \"Input value was {:s}\".format(repr(convention)))\n\n        if (unit == 'per_l'):\n            outspectrum = _shtools.SHBiasKMask(self.tapers, spectrum, k=k,\n                                               **kwargs)\n        elif (unit == 'per_lm'):\n            degree_l = _np.arange(len(spectrum))\n            temp = spectrum * (2.0 * degree_l + 1.0)\n            outspectrum = _shtools.SHBiasKMask(self.tapers, temp, k=k,\n                                               **kwargs)\n            outspectrum /= (2.0 * degree_l + 1.0)\n        else:\n            raise ValueError(\n                \"unit must be 'per_l' or 'per_lm'.\" +\n                \"Input value was {:s}\".format(repr(unit)))\n\n        return outspectrum", "response": "Calculate the multitaper expectation of a function\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the three invariants of the tensor.", "response": "def compute_invar(self):\n        \"\"\"\n        Compute the three invariants (I0, I1, I2) of the tensor, as well as\n        the quantity I = -(I2/2)**2 / (I1/3)**3.\n        \"\"\"\n        self.i0 = self.vxx + self.vyy + self.vzz\n        self.i1 = (self.vxx*self.vyy + self.vyy*self.vzz + self.vxx*self.vzz -\n                   self.vxy**2 - self.vyz**2 - self.vxz**2)\n        self.i2 = (self.vxx*(self.vyy*self.vzz - self.vyz**2) +\n                   self.vxy*(self.vyz*self.vxz - self.vxy*self.vzz) +\n                   self.vxz*(self.vxy*self.vyz - self.vxz*self.vyy))\n        self.i = (-1.) * (self.i2 / 2.)**2\n        self.i.data[1:, :] /= (self.i1.data[1:, :] / 3.)**3"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the three eigenvalues of the tensor.", "response": "def compute_eig(self):\n        \"\"\"\n        Compute the three eigenvalues of the tensor: eig1, eig2, ei3.\n        \"\"\"\n        self.eig1 = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                       grid='DH')\n        self.eig2 = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                       grid='DH')\n        self.eig3 = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                       grid='DH')\n\n        for i in range(self.nlat):\n            for j in range(self.nlon):\n                a = _np.array([[self.vxx.data[i, j],\n                                self.vxy.data[i, j],\n                                self.vxz.data[i, j]],\n                               [self.vyx.data[i, j],\n                                self.vyy.data[i, j],\n                                self.vyz.data[i, j]],\n                               [self.vzx.data[i, j],\n                                self.vzy.data[i, j],\n                                self.vzz.data[i, j]]])\n\n                eigs = _eigvalsh(a)\n\n                self.eig1.data[i, j] = eigs[2]\n                self.eig2.data[i, j] = eigs[1]\n                self.eig3.data[i, j] = eigs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_eigh(self):\n        self.eigh1 = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                        grid='DH')\n        self.eigh2 = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                        grid='DH')\n        self.eighh = _SHGrid.from_array(_np.zeros_like(self.vxx.data),\n                                        grid='DH')\n\n        for i in range(self.nlat):\n            for j in range(self.nlon):\n                a = _np.array([[self.vxx.data[i, j],\n                                self.vxy.data[i, j]],\n                               [self.vyx.data[i, j],\n                                self.vyy.data[i, j]]])\n\n                eigs = _eigvalsh(a)\n\n                self.eigh1.data[i, j] = eigs[1]\n                self.eigh2.data[i, j] = eigs[0]\n\n                if abs(eigs[0]) >= abs(eigs[1]):\n                    self.eighh.data[i, j] = eigs[0]\n                else:\n                    self.eighh.data[i, j] = eigs[1]", "response": "Compute the two horizontal eigenvalues of the tensor and the combined maximum absolute value of the two tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_vxx(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vxx component of the tensor.\n\n        Usage\n        -----\n        x.plot_vxx([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = False\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{xx}$'\n            Text label for the colorbar..\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vxx_label\n\n        if ax is None:\n            fig, axes = self.vxx.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vxx.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vxx component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the Vyy component of the tensor.", "response": "def plot_vyy(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vyy component of the tensor.\n\n        Usage\n        -----\n        x.plot_vyy([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{yy}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vyy_label\n\n        if ax is None:\n            fig, axes = self.vyy.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vyy.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_vzz(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vzz component of the tensor.\n\n        Usage\n        -----\n        x.plot_vzz([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{zz}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vzz_label\n\n        if ax is None:\n            fig, axes = self.vzz.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vzz.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vzz component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_vxy(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vxy component of the tensor.\n\n        Usage\n        -----\n        x.plot_vxy([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{xy}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vxy_label\n\n        if ax is None:\n            fig, axes = self.vxy.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vxy.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vxy component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_vyx(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vyx component of the tensor.\n\n        Usage\n        -----\n        x.plot_vyx([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{yx}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vyx_label\n\n        if ax is None:\n            fig, axes = self.vyx.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vyx.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vyx component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_vxz(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vxz component of the tensor.\n\n        Usage\n        -----\n        x.plot_vxz([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{xz}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vxz_label\n\n        if ax is None:\n            fig, axes = self.vxz.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vxz.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vxz component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the Vzx component of the tensor.", "response": "def plot_vzx(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vzx component of the tensor.\n\n        Usage\n        -----\n        x.plot_vzx([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{zx}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vzx_label\n\n        if ax is None:\n            fig, axes = self.vzx.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vzx.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_vyz(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vyz component of the tensor.\n\n        Usage\n        -----\n        x.plot_vyz([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{yz}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vyz_label\n\n        if ax is None:\n            fig, axes = self.vyz.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vyz.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vyz component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_vzy(self, colorbar=True, cb_orientation='vertical',\n                 cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the Vzy component of the tensor.\n\n        Usage\n        -----\n        x.plot_vzy([tick_interval, xlabel, ylabel, ax, colorbar,\n                    cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$V_{zy}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._vzy_label\n\n        if ax is None:\n            fig, axes = self.vzy.plot(colorbar=colorbar,\n                                      cb_orientation=cb_orientation,\n                                      cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.vzy.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                          cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the Vzy component of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the 9 components of the tensor.", "response": "def plot(self, colorbar=True, cb_orientation='horizontal',\n             tick_interval=[90, 90], minor_tick_interval=[30, 30],\n             xlabel='Longitude', ylabel='Latitude',\n             axes_labelsize=8, tick_labelsize=8, show=True, fname=None,\n             **kwargs):\n        \"\"\"\n        Plot the 9 components of the tensor.\n\n        Usage\n        -----\n        x.plot([tick_interval, minor_tick_interval, xlabel, ylabel,\n                colorbar, cb_orientation, cb_label, axes_labelsize,\n                tick_labelsize, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [90, 90]\n            Intervals to use when plotting the major x and y ticks. If set to\n            None, major ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude'\n            Label for the latitude axis.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        axes_labelsize : int, optional, default = 8\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = 8\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n       \"\"\"\n        if colorbar is True:\n            if cb_orientation == 'horizontal':\n                scale = 0.9\n            else:\n                scale = 0.45\n        else:\n            scale = 0.55\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                    _mpl.rcParams['figure.figsize'][0] * scale)\n\n        fig, ax = _plt.subplots(3, 3, figsize=figsize)\n        self.plot_vxx(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[0], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vxy(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[1], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vxz(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[2], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vyx(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[3], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vyy(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[4], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vyz(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[5], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vzx(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[6], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vzy(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[7], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_vzz(colorbar=colorbar, cb_orientation=cb_orientation,\n                      ax=ax.flat[8], tick_interval=tick_interval,\n                      xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n\n        fig.tight_layout(pad=0.5)\n\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n        return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_i0(self, colorbar=True, cb_orientation='vertical',\n                cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the first invariant I0 (the trace) of the tensor\n\n            I0 = vxx + vyy + vzz\n\n        which should be identically zero.\n\n        Usage\n        -----\n        x.plot_i0([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation,\n                   cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = 'Tr $V_{ij}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._i0_label\n\n        if self.i0 is None:\n            self.compute_invar()\n\n        if ax is None:\n            fig, axes = self.i0.plot(colorbar=colorbar,\n                                     cb_orientation=cb_orientation,\n                                     cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.i0.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                         cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the first invariant I0 of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_i1(self, colorbar=True, cb_orientation='vertical',\n                cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the second invariant I1 of the tensor:\n\n            I1 = vxx*vyy + vyy*vzz + vxx*vzz - vxy**2 - vyz**2 - vxz**2\n\n        Usage\n        -----\n        x.plot_i1([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation,\n                   cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$I_1$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._i1_label\n\n        if self.i1 is None:\n            self.compute_invar()\n\n        if ax is None:\n            fig, axes = self.i1.plot(colorbar=colorbar,\n                                     cb_orientation=cb_orientation,\n                                     cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.i1.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                         cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the second invariant I1 of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the third invariant I2 of the tensor.", "response": "def plot_i2(self, colorbar=True, cb_orientation='vertical',\n                cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the third invariant I2 (the determinant) of the tensor:\n\n           I2 = vxx*(vyy*vzz - vyz**2) + vxy*(vyz*vxz - vxy*vzz)\n                + vxz*(vxy*vyz - vxz*vyy)\n\n        Usage\n        -----\n        x.plot_i2([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation,\n                   cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = 'det $V_{ij}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._i2_label\n\n        if self.i2 is None:\n            self.compute_invar()\n\n        if ax is None:\n            fig, axes = self.i2.plot(colorbar=colorbar,\n                                     cb_orientation=cb_orientation,\n                                     cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.i2.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                         cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the dimensionless quantity I of Pedersen and Rasmussen and the corresponding I of Pedersen and Rasmussen.", "response": "def plot_i(self, colorbar=True, cb_orientation='vertical',\n               cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the dimensionless quantity I of Pedersen and Rasmussen (1990)\n\n           I = -(I2/2)**2 / (I1/3)**3\n\n        that is bounded by 0 and 1.\n\n        Usage\n        -----\n        x.plot_i([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation,\n                  cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$-(I_2/2)^{2} / (I_1/3)^{3}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._i_label\n\n        if self.i is None:\n            self.compute_invar()\n\n        if ax is None:\n            fig, axes = self.i.plot(colorbar=colorbar,\n                                    cb_orientation=cb_orientation,\n                                    cb_label=cb_label, show=False, **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.i.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                        cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_invar(self, colorbar=True, cb_orientation='horizontal',\n             tick_interval=[60, 60], minor_tick_interval=[20, 20],\n             xlabel='Longitude', ylabel='Latitude',\n             axes_labelsize=9, tick_labelsize=8, show=True, fname=None,\n             **kwargs):\n        \"\"\"\n        Plot the three invariants of the tensor and the derived quantity I.\n\n        Usage\n        -----\n        x.plot_invar([tick_interval, minor_tick_interval, xlabel, ylabel,\n                colorbar, cb_orientation, cb_label, axes_labelsize,\n                tick_labelsize, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [60, 60]\n            Intervals to use when plotting the major x and y ticks. If set to\n            None, major ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = [20, 20]\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude'\n            Label for the latitude axis.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        axes_labelsize : int, optional, default = 9\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = 8\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if colorbar is True:\n            if cb_orientation == 'horizontal':\n                scale = 0.8\n            else:\n                scale = 0.5\n        else:\n            scale = 0.6\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                    _mpl.rcParams['figure.figsize'][0] * scale)\n\n        fig, ax = _plt.subplots(2, 2, figsize=figsize)\n\n        self.plot_i0(colorbar=colorbar, cb_orientation=cb_orientation,\n                     ax=ax.flat[0], tick_interval=tick_interval,\n                     xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_i1(colorbar=colorbar, cb_orientation=cb_orientation,\n                     ax=ax.flat[1], tick_interval=tick_interval,\n                     xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_i2(colorbar=colorbar, cb_orientation=cb_orientation,\n                     ax=ax.flat[2], tick_interval=tick_interval,\n                     xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n        self.plot_i(colorbar=colorbar, cb_orientation=cb_orientation,\n                    ax=ax.flat[3], tick_interval=tick_interval,\n                    xlabel=xlabel, ylabel=ylabel,\n                      axes_labelsize=axes_labelsize,\n                      tick_labelsize=tick_labelsize,\n                      minor_tick_interval=minor_tick_interval,\n                      **kwargs)\n\n        fig.tight_layout(pad=0.5)\n\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n        return fig, ax", "response": "Plots the three invariants of the derived quantity I."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_eig1(self, colorbar=True, cb_orientation='vertical',\n                  cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the first eigenvalue of the tensor.\n\n        Usage\n        -----\n        x.plot_eig1([tick_interval, xlabel, ylabel, ax, colorbar,\n                     cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_1$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eig1_label\n\n        if self.eig1 is None:\n            self.compute_eig()\n\n        if ax is None:\n            fig, axes = self.eig1.plot(colorbar=colorbar,\n                                       cb_orientation=cb_orientation,\n                                       cb_label=cb_label, show=False,\n                                       **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eig1.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                           cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the first eigenvalue of the tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the second eigenvalue of the tensor.", "response": "def plot_eig2(self, colorbar=True, cb_orientation='vertical',\n                  cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the second eigenvalue of the tensor.\n\n        Usage\n        -----\n        x.plot_eig2([tick_interval, xlabel, ylabel, ax, colorbar,\n                     cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_2$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eig2_label\n\n        if self.eig2 is None:\n            self.compute_eig()\n\n        if ax is None:\n            fig, axes = self.eig2.plot(colorbar=colorbar,\n                                       cb_orientation=cb_orientation,\n                                       cb_label=cb_label, show=False,\n                                       **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eig2.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                           cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the third eigenvalue of the tensor.", "response": "def plot_eig3(self, colorbar=True, cb_orientation='vertical',\n                  cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the third eigenvalue of the tensor.\n\n        Usage\n        -----\n        x.plot_eig3([tick_interval, xlabel, ylabel, ax, colorbar,\n                     cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_3$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eig3_label\n\n        if self.eig3 is None:\n            self.compute_eig()\n\n        if ax is None:\n            fig, axes = self.eig3.plot(colorbar=colorbar,\n                                       cb_orientation=cb_orientation,\n                                       cb_label=cb_label, show=False,\n                                       **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eig3.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                           cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the three eigenvalues of the tensor.", "response": "def plot_eigs(self, colorbar=True, cb_orientation='vertical',\n             tick_interval=[60, 60], minor_tick_interval=[20, 20],\n             xlabel='Longitude', ylabel='Latitude',\n             axes_labelsize=9, tick_labelsize=8, show=True, fname=None,\n             **kwargs):\n        \"\"\"\n        Plot the three eigenvalues of the tensor.\n\n        Usage\n        -----\n        x.plot_eigs([tick_interval, minor_tick_interval, xlabel, ylabel,\n                     colorbar, cb_orientation, cb_label, axes_labelsize,\n                     tick_labelsize, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [60, 60]\n            Intervals to use when plotting the major x and y ticks. If set to\n            None, major ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = [20, 20]\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude'\n            Label for the latitude axis.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        axes_labelsize : int, optional, default = 9\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = 8\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if colorbar is True:\n            if cb_orientation == 'horizontal':\n                scale = 2.3\n            else:\n                scale = 1.4\n        else:\n            scale = 1.65\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                    _mpl.rcParams['figure.figsize'][0] * scale)\n\n        fig, ax = _plt.subplots(3, 1, figsize=figsize)\n\n        self.plot_eig1(colorbar=colorbar, cb_orientation=cb_orientation,\n                       ax=ax.flat[0], xlabel=xlabel, ylabel=ylabel,\n                       tick_interval=tick_interval,\n                       axes_labelsize=axes_labelsize,\n                       tick_labelsize=tick_labelsize,\n                       minor_tick_interval=minor_tick_interval,\n                       **kwargs)\n        self.plot_eig2(colorbar=colorbar, cb_orientation=cb_orientation,\n                       ax=ax.flat[1], xlabel=xlabel, ylabel=ylabel,\n                       tick_interval=tick_interval,\n                       axes_labelsize=axes_labelsize,\n                       tick_labelsize=tick_labelsize,\n                       minor_tick_interval=minor_tick_interval,\n                       **kwargs)\n        self.plot_eig3(colorbar=colorbar, cb_orientation=cb_orientation,\n                       ax=ax.flat[2], xlabel=xlabel, ylabel=ylabel,\n                       tick_interval=tick_interval,\n                       axes_labelsize=axes_labelsize,\n                       tick_labelsize=tick_labelsize,\n                       minor_tick_interval=minor_tick_interval,\n                       **kwargs)\n\n        fig.tight_layout(pad=0.5)\n\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n        return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_eigh1(self, colorbar=True, cb_orientation='vertical',\n                   cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the first eigenvalue of the horizontal tensor.\n\n        Usage\n        -----\n        x.plot_eigh1([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_{h1}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eigh1_label\n\n        if self.eigh1 is None:\n            self.compute_eigh()\n\n        if ax is None:\n            fig, axes = self.eigh1.plot(colorbar=colorbar,\n                                        cb_orientation=cb_orientation,\n                                        cb_label=cb_label, show=False,\n                                        **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eigh1.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                            cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the first eigenvalue of the horizontal tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the second eigenvalue of the horizontal tensor.", "response": "def plot_eigh2(self, colorbar=True, cb_orientation='vertical',\n                   cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the second eigenvalue of the horizontal tensor.\n\n        Usage\n        -----\n        x.plot_eigh2([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_{h2}$, Eotvos$^{-1}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eigh2_label\n\n        if self.eigh2 is None:\n            self.compute_eigh()\n\n        if ax is None:\n            fig, axes = self.eigh2.plot(colorbar=colorbar,\n                                        cb_orientation=cb_orientation,\n                                        cb_label=cb_label, show=False,\n                                        **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eigh2.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                            cb_label=cb_label, ax=ax, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_eighh(self, colorbar=True, cb_orientation='vertical',\n                   cb_label=None, ax=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the maximum absolute value eigenvalue of the horizontal tensor.\n\n        Usage\n        -----\n        x.plot_eighh([tick_interval, xlabel, ylabel, ax, colorbar,\n                      cb_orientation, cb_label, show, fname])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = '$\\lambda_{hh}$'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if cb_label is None:\n            cb_label = self._eighh_label\n\n        if self.eighh is None:\n            self.compute_eigh()\n\n        if ax is None:\n            fig, axes = self.eighh.plot(colorbar=colorbar,\n                                        cb_orientation=cb_orientation,\n                                        cb_label=cb_label, show=False,\n                                        **kwargs)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes\n\n        else:\n            self.eighh.plot(colorbar=colorbar, cb_orientation=cb_orientation,\n                            cb_label=cb_label, ax=ax, **kwargs)", "response": "Plots the maximum absolute value eigenvalue of the horizontal tensor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the two eigenvalues and maximum absolute value eigenvalue of the current object.", "response": "def plot_eigh(self, colorbar=True, cb_orientation='vertical',\n                  tick_interval=[60, 60], minor_tick_interval=[20, 20],\n                  xlabel='Longitude', ylabel='Latitude',\n                  axes_labelsize=9, tick_labelsize=8, show=True, fname=None,\n                  **kwargs):\n        \"\"\"\n        Plot the two eigenvalues and maximum absolute value eigenvalue of the\n        horizontal tensor.\n\n        Usage\n        -----\n        x.plot_eigh([tick_interval, minor_tick_interval, xlabel, ylabel,\n                     colorbar, cb_orientation, cb_label, axes_labelsize,\n                     tick_labelsize, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [60, 60]\n            Intervals to use when plotting the major x and y ticks. If set to\n            None, major ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = [20, 20]\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude'\n            Label for the latitude axis.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        axes_labelsize : int, optional, default = 9\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = 8\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        if colorbar is True:\n            if cb_orientation == 'horizontal':\n                scale = 2.3\n            else:\n                scale = 1.4\n        else:\n            scale = 1.65\n        figsize = (_mpl.rcParams['figure.figsize'][0],\n                    _mpl.rcParams['figure.figsize'][0] * scale)\n\n        fig, ax = _plt.subplots(3, 1, figsize=figsize)\n\n        self.plot_eigh1(colorbar=colorbar, cb_orientation=cb_orientation,\n                        ax=ax.flat[0], xlabel=xlabel, ylabel=ylabel,\n                        tick_interval=tick_interval,\n                        tick_labelsize=tick_labelsize,\n                        minor_tick_interval=minor_tick_interval,\n                        **kwargs)\n        self.plot_eigh2(colorbar=colorbar, cb_orientation=cb_orientation,\n                        ax=ax.flat[1], xlabel=xlabel, ylabel=ylabel,\n                        tick_interval=tick_interval,\n                        tick_labelsize=tick_labelsize,\n                        minor_tick_interval=minor_tick_interval,\n                        **kwargs)\n        self.plot_eighh(colorbar=colorbar, cb_orientation=cb_orientation,\n                        ax=ax.flat[2], xlabel=xlabel, ylabel=ylabel,\n                        tick_interval=tick_interval,\n                        tick_labelsize=tick_labelsize,\n                        minor_tick_interval=minor_tick_interval,\n                        **kwargs)\n\n        fig.tight_layout(pad=0.5)\n\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n        return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_version():\n    d = os.path.dirname(__file__)\n    # get release number from VERSION\n    with open(os.path.join(d, 'VERSION')) as f:\n        vre = re.compile('.Version: (.+)$', re.M)\n        version = vre.search(f.read()).group(1)\n\n    if os.path.isdir(os.path.join(d, '.git')):\n        # Get the version using \"git describe\".\n        cmd = 'git describe --tags'\n        try:\n            git_version = check_output(cmd.split()).decode().strip()[1:]\n        except CalledProcessError:\n            print('Unable to get version number from git tags\\n'\n                  'Setting to x.x')\n            git_version = 'x.x'\n\n        # PEP440 compatibility\n        if '-' in git_version:\n            git_revision = check_output(['git', 'rev-parse', 'HEAD'])\n            git_revision = git_revision.strip().decode('ascii')\n            # add post0 if the version is released\n            # otherwise add dev0 if the version is not yet released\n            if ISRELEASED:\n                version += '.post0+' + git_revision[:7]\n            else:\n                version += '.dev0+' + git_revision[:7]\n\n    return version", "response": "Get the version number from the VERSION file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting fortran flags depending on the compiler.", "response": "def get_compiler_flags():\n    \"\"\"Set fortran flags depending on the compiler.\"\"\"\n    compiler = get_default_fcompiler()\n    if compiler == 'absoft':\n        flags = ['-m64', '-O3', '-YEXT_NAMES=LCS', '-YEXT_SFX=_',\n                 '-fpic', '-speed_math=10']\n    elif compiler == 'gnu95':\n        flags = ['-m64', '-fPIC', '-O3', '-ffast-math']\n    elif compiler == 'intel':\n        flags = ['-m64', '-fpp', '-free', '-O3', '-Tf']\n    elif compiler == 'g95':\n        flags = ['-O3', '-fno-second-underscore']\n    elif compiler == 'pg':\n        flags = ['-fast']\n    else:\n        flags = ['-m64', '-O3']\n    return flags"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef configuration(parent_package='', top_path=None):\n    config = Configuration('', parent_package, top_path)\n\n    F95FLAGS = get_compiler_flags()\n\n    kwargs = {\n        'libraries': [],\n        'include_dirs': [],\n        'library_dirs': [],\n    }\n    kwargs['extra_compile_args'] = F95FLAGS\n    kwargs['f2py_options'] = ['--quiet']\n\n    # numpy.distutils.fcompiler.FCompiler doesn't support .F95 extension\n    compiler = FCompiler(get_default_fcompiler())\n    compiler.src_extensions.append('.F95')\n    compiler.language_map['.F95'] = 'f90'\n\n    # collect all Fortran sources\n    files = os.listdir('src')\n    exclude_sources = ['PlanetsConstants.f95', 'PythonWrapper.f95']\n    sources = [os.path.join('src', file) for file in files if\n               file.lower().endswith(('.f95', '.c')) and file not in\n               exclude_sources]\n\n    # (from http://stackoverflow.com/questions/14320220/\n    #              testing-python-c-libraries-get-build-path)):\n    build_lib_dir = \"{dirname}.{platform}-{version[0]}.{version[1]}\"\n    dirparams = {'dirname': 'temp',\n                 'platform': sysconfig.get_platform(),\n                 'version': sys.version_info}\n    libdir = os.path.join('build', build_lib_dir.format(**dirparams))\n    print('searching SHTOOLS in:', libdir)\n\n    # Fortran compilation\n    config.add_library('SHTOOLS',\n                       sources=sources,\n                       **kwargs)\n\n    # SHTOOLS\n    kwargs['libraries'].extend(['SHTOOLS'])\n    kwargs['include_dirs'].extend([libdir])\n    kwargs['library_dirs'].extend([libdir])\n\n    # FFTW info\n    fftw_info = get_info('fftw', notfound_action=2)\n    dict_append(kwargs, **fftw_info)\n\n    if sys.platform != 'win32':\n        kwargs['libraries'].extend(['m'])\n\n    # BLAS / Lapack info\n    lapack_info = get_info('lapack_opt', notfound_action=2)\n    blas_info = get_info('blas_opt', notfound_action=2)\n    dict_append(kwargs, **blas_info)\n    dict_append(kwargs, **lapack_info)\n\n    config.add_extension('pyshtools._SHTOOLS',\n                         sources=['src/pyshtools.pyf',\n                                  'src/PythonWrapper.f95'],\n                         **kwargs)\n\n    return config", "response": "Configure all packages that need to be built."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        print('---- BUILDING ----')\n        _build.run(self)\n\n        # build documentation\n        print('---- BUILDING DOCS ----')\n        docdir = os.path.join(self.build_lib, 'pyshtools', 'doc')\n        self.mkpath(docdir)\n        doc_builder = os.path.join(self.build_lib, 'pyshtools', 'make_docs.py')\n        doc_source = '.'\n        check_call([sys.executable, doc_builder, doc_source, self.build_lib])\n\n        print('---- ALL DONE ----')", "response": "Build the Fortran library all python extensions and the docs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the Fortran library all python extensions and the docs.", "response": "def run(self):\n        \"\"\"Build the Fortran library, all python extensions and the docs.\"\"\"\n        print('---- CUSTOM DEVELOP ----')\n        _develop.run(self)\n\n        # build documentation\n        print('---- BUILDING DOCS ----')\n        docdir = os.path.join(self.setup_path, 'pyshtools', 'doc')\n        self.mkpath(docdir)\n        doc_builder = os.path.join(self.setup_path, 'pyshtools',\n                                   'make_docs.py')\n        doc_source = '.'\n        check_call([sys.executable, doc_builder, doc_source, self.setup_path])\n\n        print('---- ALL DONE ----')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _time_variable_part(epoch, ref_epoch, trnd, periodic):\n    delta_t = epoch - ref_epoch\n    trend = trnd * delta_t\n    periodic_sum = _np.zeros_like(trnd)\n    for period in periodic:\n        for trifunc in periodic[period]:\n            coeffs = periodic[period][trifunc]\n            if trifunc == 'acos':\n                periodic_sum += coeffs * _np.cos(2 * _np.pi / period * delta_t)\n            elif trifunc == 'asin':\n                periodic_sum += coeffs * _np.sin(2 * _np.pi / period * delta_t)\n    return trend + periodic_sum", "response": "This function computes the time - variable part of the coefficients of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_icgem_gfc(filename, errors=None, lmax=None, epoch=None):\n\n    # read header\n    header = {}\n    header_keys = ['modelname', 'product_type', 'earth_gravity_constant',\n                   'gravity_constant', 'radius', 'max_degree', 'errors',\n                   'tide_system', 'norm', 'format']\n\n    with open(filename, 'r') as f:\n        for line in f:\n            if 'end_of_head' in line:\n                break\n            for key in header_keys:\n                if key in line:\n                    header[key] = line.strip().split()[1]\n\n        if header['product_type'] != 'gravity_field':\n            raise ValueError(\n                'This function reads only gravity_field data product.')\n\n        is_v2 = False\n        if 'format' in header and header['format'] == 'icgem2.0':\n            is_v2 = True\n\n        if epoch is None and is_v2:\n            raise ValueError(\n                'Epoch must be specified for the \"icgem2.0\" format.')\n        elif epoch is not None:\n            epoch = _yyyymmdd_to_year_fraction(epoch)\n\n        if 'earth_gravity_constant' in header:\n            gravity_constant = float(header['earth_gravity_constant'])\n        elif 'gravity_constant' in header:\n            gravity_constant = float(header['gravity_constant'])\n        else:\n            raise ValueError(\n                'No standard gravitational constant in the header.')\n\n        radius = float(header['radius'])\n\n        lmax_model = int(header['max_degree'])\n        if lmax is None or lmax < 0 or lmax > lmax_model:\n            lmax = lmax_model\n\n        if errors is not None:\n            valid_err = ('calibrated', 'formal', 'calibrated_and_formal')\n            if header['errors'] == 'no':\n                raise ValueError('This model has no errors.')\n            elif errors not in valid_err[:-1]:\n                raise ValueError(\n                    'Errors can be either \"formal\", \"calibrated\" or None.')\n            elif header['errors'] in valid_err and errors in valid_err[:-1]:\n                if (errors, header['errors']) == valid_err[1:]:\n                    err_cols = (7, 8)\n                elif header['errors'] != errors:\n                    raise ValueError(\n                        'This model has no {} errors.'.format(errors))\n                else:\n                    err_cols = (5, 6)\n\n        cilm = _np.tile(_np.zeros((lmax + 1, lmax + 1)), (4, 1, 1))\n        ref_epoch = _np.zeros((lmax + 1, lmax + 1))\n        trnd = _np.zeros_like(cilm)\n        periodic = {}\n\n        # read coefficients\n        for line in f:\n            line = line.replace('D', 'E').strip().split()\n\n            l, m = int(line[1]), int(line[2])\n            if m > lmax:\n                break\n            if l > lmax:\n                continue\n\n            key = line[0]\n\n            value_cs = [float(line[3]), float(line[4]), 0, 0]\n            if errors:\n                value_cs[2:] = float(line[err_cols[0]]),\\\n                    float(line[err_cols[1]])\n\n            if key == 'gfc':\n                cilm[:, l, m] = value_cs\n            elif key == 'gfct':\n                if is_v2:\n                    t0i = _yyyymmdd_to_year_fraction(line[-2])\n                    t1i = _yyyymmdd_to_year_fraction(line[-1])\n                    if not t0i <= epoch < t1i:\n                        continue\n                else:\n                    t0i = _yyyymmdd_to_year_fraction(line[-1])\n\n                cilm[:, l, m] = value_cs\n                ref_epoch[l, m] = t0i\n            elif key == 'trnd':\n                if is_v2:\n                    t0i = _yyyymmdd_to_year_fraction(line[-2])\n                    t1i = _yyyymmdd_to_year_fraction(line[-1])\n                    if not t0i <= epoch < t1i:\n                        continue\n                trnd[:, l, m] = value_cs\n            elif key in ('acos', 'asin'):\n                if is_v2:\n                    t0i = _yyyymmdd_to_year_fraction(line[-3])\n                    t1i = _yyyymmdd_to_year_fraction(line[-2])\n                    if not t0i <= epoch < t1i:\n                        continue\n\n                period = float(line[-1])\n                if period not in periodic:\n                    arr = _np.zeros_like(cilm)\n                    periodic[period] = {'acos': arr,\n                                        'asin': arr.copy()}\n\n                periodic[period][key][:, l, m] = value_cs\n\n    if epoch is None:\n        epoch = ref_epoch\n\n    cilm += _time_variable_part(epoch, ref_epoch, trnd, periodic)\n\n    if errors:\n        return cilm[:2], gravity_constant, radius, cilm[2:]\n    else:\n        return cilm[:2], gravity_constant, radius", "response": "Reads spherical harmonic coefficients from an ICGEM GFC ascii - formatted file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modify_subroutine(subroutine):\n    # print('\\n----',subroutine['name'],'----')\n\n    #-- use original function from shtools:\n    subroutine['use'] = {'shtools': {'map': {subroutine['name']: subroutine['name']}, 'only': 1}}\n\n    #-- loop through variables:\n    for varname, varattribs in subroutine['vars'].items():\n        #-- prefix function return variables with 'py'\n        if varname == subroutine['name']:\n            subroutine['vars']['py' + varname] = subroutine['vars'].pop(varname)\n            varname = 'py' + varname\n            # print('prefix added:',varname)\n        #-- change assumed to explicit:\n        if has_assumed_shape(varattribs):\n            make_explicit(subroutine, varname, varattribs)\n            # print('assumed shape variable modified to:',varname,varattribs['dimension'])\n\n    #-- add py prefix to subroutine:\n    subroutine['name'] = 'py' + subroutine['name']", "response": "loops through variables of a subroutine and modifies them"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef figstyle(rel_width=0.75, screen_dpi=114, aspect_ratio=4/3,\n             max_width=7.48031):\n    \"\"\"\n    Set matplotlib parameters for creating publication quality graphics.\n\n    Usage\n    -----\n    figstyle([rel_width, screen_dpi, aspect_ratio, max_width])\n\n    Parameters\n    ----------\n    rel_width : float, optional, default = 0.75\n        The relative width of the plot (from 0 to 1) wih respect to max_width.\n    screen_dpi : int, optional, default = 114\n        The screen resolution of the display in dpi, which determines the\n        size of the plot on the display.\n    aspect_ratio : float, optional, default = 4/3\n        The aspect ratio of the plot.\n    max_width : float, optional, default = 7.48031\n        The maximum width of the usable area of a journal page in inches.\n\n    Description\n    -----------\n    This function sets a variety of matplotlib parameters for creating\n    publication quality graphics. The default parameters are tailored to\n    AGU/Wiley-Blackwell journals that accept relative widths of 0.5, 0.75,\n    and 1. To reset the maplotlib parameters to their default values, use\n\n        matplotlib.pyplot.style.use('default')\n    \"\"\"\n    width_x = max_width * rel_width\n    width_y = max_width * rel_width / aspect_ratio\n\n    shtools = {\n        # fonts\n        'font.size': 10,\n        'font.family': 'sans-serif',\n        'font.sans-serif': ['Myriad Pro', 'DejaVu Sans',\n                            'Bitstream Vera Sans',\n                            'Verdana', 'Arial', 'Helvetica'],\n        'axes.titlesize': 10,\n        'axes.labelsize': 10,\n        'xtick.labelsize': 8,\n        'ytick.labelsize': 8,\n        'legend.fontsize': 9,\n        'text.usetex': False,\n        'axes.formatter.limits': (-3, 3),\n        # figure\n        'figure.dpi': screen_dpi,\n        'figure.figsize': (width_x, width_y),\n        # line and tick widths\n        'axes.linewidth': 1,\n        'lines.linewidth': 1.5,\n        'xtick.major.width': 0.6,\n        'ytick.major.width': 0.6,\n        'xtick.minor.width': 0.6,\n        'xtick.minor.width': 0.6,\n        'xtick.top': True,\n        'ytick.right': True,\n        # grids\n        'grid.linewidth': 0.3,\n        'grid.color': 'k',\n        'grid.linestyle': '-',\n        # legends\n        'legend.framealpha': 1.,\n        'legend.edgecolor': 'k',\n        # images\n        'image.lut': 65536,  # 16 bit\n        # savefig\n        'savefig.bbox': 'tight',\n        'savefig.pad_inches': 0.02,\n        'savefig.dpi': 600,\n        'savefig.format': 'pdf'\n        }\n\n    _plt.style.use([shtools])", "response": "Function to set matplotlib parameters for creating a new AGU - Wiley - Blackwell journal page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_zeros(self, lmax, kind='real', normalization='4pi', csphase=1):\n        if kind.lower() not in ('real', 'complex'):\n            raise ValueError(\n                \"Kind must be 'real' or 'complex'. \" +\n                \"Input value was {:s}.\"\n                .format(repr(kind))\n                )\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be either 1 or -1. Input value was {:s}.\"\n                .format(repr(csphase))\n                )\n\n        if normalization.lower() == 'unnorm' and lmax > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                           \"are stable only for degrees less than or equal \" +\n                           \"to 85. lmax for the coefficients will be set to \" +\n                           \"85. Input value was {:d}.\".format(lmax),\n                           category=RuntimeWarning)\n            lmax = 85\n\n        nl = lmax + 1\n        if kind.lower() == 'real':\n            coeffs = _np.zeros((2, nl, nl))\n        else:\n            coeffs = _np.zeros((2, nl, nl), dtype=complex)\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind):\n                return cls(coeffs, normalization=normalization.lower(),\n                           csphase=csphase)", "response": "Initialize a new instance of the class with spherical harmonic coefficients set to zero from degree 0 to lmax."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the class instance from an array of spherical harmonic coefficients.", "response": "def from_array(self, coeffs, normalization='4pi', csphase=1, lmax=None,\n                   copy=True):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients from an input\n        array.\n\n        Usage\n        -----\n        x = SHCoeffs.from_array(array, [normalization, csphase, lmax, copy])\n\n        Returns\n        -------\n        x : SHCoeffs class instance.\n\n        Parameters\n        ----------\n        array : ndarray, shape (2, lmaxin+1, lmaxin+1).\n            The input spherical harmonic coefficients.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        lmax : int, optional, default = None\n            The maximum spherical harmonic degree to include in the returned\n            class instance. This must be less than or equal to lmaxin.\n        copy : bool, optional, default = True\n            If True, make a copy of array when initializing the class instance.\n            If False, initialize the class instance with a reference to array.\n        \"\"\"\n        if _np.iscomplexobj(coeffs):\n            kind = 'complex'\n        else:\n            kind = 'real'\n\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be either 1 or -1. Input value was {:s}.\"\n                .format(repr(csphase))\n                )\n\n        lmaxin = coeffs.shape[1] - 1\n        if lmax is None:\n            lmax = lmaxin\n        else:\n            if lmax > lmaxin:\n                lmax = lmaxin\n\n        if normalization.lower() == 'unnorm' and lmax > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                           \"are stable only for degrees less than or equal \" +\n                           \"to 85. lmax for the coefficients will be set to \" +\n                           \"85. Input value was {:d}.\".format(lmax),\n                           category=RuntimeWarning)\n            lmax = 85\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind):\n                return cls(coeffs[:, 0:lmax+1, 0:lmax+1],\n                           normalization=normalization.lower(),\n                           csphase=csphase, copy=copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_random(self, power, lmax=None, kind='real', normalization='4pi',\n                    csphase=1, exact_power=False, seed=None):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients as random\n        variables with a given spectrum.\n\n        Usage\n        -----\n        x = SHCoeffs.from_random(power, [lmax, kind, normalization, csphase,\n                                         exact_power, seed])\n\n        Returns\n        -------\n        x : SHCoeffs class instance.\n\n        Parameters\n        ----------\n        power : ndarray, shape (L+1)\n            numpy array of shape (L+1) that specifies the expected power per\n            degree l of the random coefficients, where L is the maximum\n            spherical harmonic bandwidth.\n        lmax : int, optional, default = len(power) - 1\n            The maximum spherical harmonic degree l of the output coefficients.\n            The coefficients will be set to zero for degrees greater than L.\n        kind : str, optional, default = 'real'\n            'real' or 'complex' spherical harmonic coefficients.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        exact_power : bool, optional, default = False\n            The total variance of the coefficients is set exactly to the input\n            power. The distribution of power at degree l amongst the angular\n            orders is random, but the total power is fixed.\n        seed : int, optional, default = None\n            Set the seed for the numpy random number generator.\n\n        Description\n        -----------\n        This routine returns a random realization of spherical harmonic\n        coefficients obtained from a normal distribution. The variance of\n        each coefficient at degree l is equal to the total power at degree\n        l divided by the number of coefficients at that degree. The power\n        spectrum of the random realization can be fixed exactly to the input\n        spectrum by setting exact_power to True.\n        \"\"\"\n        # check if all arguments are correct\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The input normalization must be '4pi', 'ortho', 'schmidt', \" +\n                \"or 'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        if kind.lower() not in ('real', 'complex'):\n            raise ValueError(\n                \"kind must be 'real' or 'complex'. \" +\n                \"Input value was {:s}.\".format(repr(kind)))\n\n        if lmax is None:\n            nl = len(power)\n            lmax = nl - 1\n        else:\n            if lmax <= len(power) - 1:\n                nl = lmax + 1\n            else:\n                nl = len(power)\n        degrees = _np.arange(nl)\n\n        if normalization.lower() == 'unnorm' and nl - 1 > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                           \"are stable only for degrees less than or equal \" +\n                           \"to 85. lmax for the coefficients will be set to \" +\n                           \"85. Input value was {:d}.\".format(nl-1),\n                           category=RuntimeWarning)\n            nl = 85 + 1\n            lmax = 85\n\n        # Create coefficients with unit variance, which returns an expected\n        # total power per degree of (2l+1) for 4pi normalized harmonics.\n        if seed is not None:\n            _np.random.seed(seed=seed)\n        if kind.lower() == 'real':\n            coeffs = _np.empty((2, nl, nl))\n            for l in degrees:\n                coeffs[:2, l, :l+1] = _np.random.normal(size=(2, l+1))\n        elif kind.lower() == 'complex':\n            # - need to divide by sqrt 2 as there are two terms for each coeff.\n            coeffs = _np.empty((2, nl, nl), dtype=complex)\n            for l in degrees:\n                coeffs[:2, l, :l+1] = (_np.random.normal(size=(2, l+1)) +\n                                       1j * _np.random.normal(size=(2, l+1))\n                                       ) / _np.sqrt(2.)\n\n        if exact_power:\n            power_per_l = _spectrum(coeffs, normalization='4pi', unit='per_l')\n            coeffs *= _np.sqrt(\n                power[0:nl] / power_per_l)[_np.newaxis, :, _np.newaxis]\n        else:\n            coeffs *= _np.sqrt(\n                power[0:nl] / (2 * degrees + 1))[_np.newaxis, :, _np.newaxis]\n\n        if normalization.lower() == '4pi':\n            pass\n        elif normalization.lower() == 'ortho':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='ortho')\n        elif normalization.lower() == 'schmidt':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='schmidt')\n        elif normalization.lower() == 'unnorm':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='unnorm')\n\n        if lmax > nl - 1:\n            coeffs = _np.pad(coeffs, ((0, 0), (0, lmax - nl + 1),\n                             (0, lmax - nl + 1)), 'constant')\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind):\n                return cls(coeffs, normalization=normalization.lower(),\n                           csphase=csphase)", "response": "This routine initializes a new instance of the class with a given power distribution and a given spectrum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_file(self, fname, lmax=None, format='shtools', kind='real',\n                  normalization='4pi', skip=0, header=False,\n                  csphase=1, **kwargs):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients from a file.\n\n        Usage\n        -----\n        x = SHCoeffs.from_file(filename, [format='shtools', lmax,\n                                          normalization, csphase, skip,\n                                          header])\n        x = SHCoeffs.from_file(filename, [format='npy', normalization,\n                                          csphase, **kwargs])\n\n        Returns\n        -------\n        x : SHCoeffs class instance.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the file, including path.\n        format : str, optional, default = 'shtools'\n            'shtools' format or binary numpy 'npy' format.\n        lmax : int, optional, default = None\n            The maximum spherical harmonic degree to read from 'shtools'\n            formatted files.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        skip : int, optional, default = 0\n            Number of lines to skip at the beginning of the file when format is\n            'shtools'.\n        header : bool, optional, default = False\n            If True, read a list of values from the header line of an 'shtools'\n            formatted file.\n        **kwargs : keyword argument list, optional for format = 'npy'\n            Keyword arguments of numpy.load() when format is 'npy'.\n\n        Description\n        -----------\n        If format='shtools', spherical harmonic coefficients will be read from\n        a text file. The optional parameter `skip` specifies how many lines\n        should be skipped before attempting to parse the file, the optional\n        parameter `header` specifies whether to read a list of values from a\n        header line, and the optional parameter `lmax` specifies the maximum\n        degree to read from the file. All lines that do not start with 2\n        integers and that are less than 3 words long will be treated as\n        comments and ignored. For this format, each line of the file must\n        contain\n\n        l, m, coeffs[0, l, m], coeffs[1, l, m]\n\n        where l and m are the spherical harmonic degree and order,\n        respectively. The terms coeffs[1, l, 0] can be neglected as they are\n        zero. For more information, see `shio.shread()`.\n\n        If format='npy', a binary numpy 'npy' file will be read using\n        numpy.load().\n        \"\"\"\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. '\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The input normalization must be '4pi', 'ortho', 'schmidt', \"\n                \"or 'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        header_list = None\n        if format.lower() == 'shtools':\n            if header is True:\n                coeffs, lmaxout, header_list = _shread(fname, lmax=lmax,\n                                                       skip=skip, header=True)\n            else:\n                coeffs, lmaxout = _shread(fname, lmax=lmax, skip=skip)\n        elif format.lower() == 'npy':\n            coeffs = _np.load(fname, **kwargs)\n            lmaxout = coeffs.shape[1] - 1\n        else:\n            raise NotImplementedError(\n                'format={:s} not implemented'.format(repr(format)))\n\n        if normalization.lower() == 'unnorm' and lmaxout > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \" +\n                           \"are stable only for degrees less than or equal \" +\n                           \"to 85. lmax for the coefficients will be set to \" +\n                           \"85. Input value was {:d}.\".format(lmaxout),\n                           category=RuntimeWarning)\n            lmaxout = 85\n\n        if _np.iscomplexobj(coeffs):\n            kind = 'complex'\n        else:\n            kind = 'real'\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind):\n                return cls(coeffs, normalization=normalization.lower(),\n                           csphase=csphase, header=header_list)", "response": "Initialize a new instance of the class from a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_file(self, filename, format='shtools', header=None, **kwargs):\n        if format is 'shtools':\n            with open(filename, mode='w') as file:\n                if header is not None:\n                    file.write(header + '\\n')\n                for l in range(self.lmax+1):\n                    for m in range(l+1):\n                        file.write('{:d}, {:d}, {:.16e}, {:.16e}\\n'\n                                   .format(l, m, self.coeffs[0, l, m],\n                                           self.coeffs[1, l, m]))\n        elif format is 'npy':\n            _np.save(filename, self.coeffs, **kwargs)\n        else:\n            raise NotImplementedError(\n                'format={:s} not implemented'.format(repr(format)))", "response": "Save the spherical harmonic coefficients to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_array(self, normalization=None, csphase=None, lmax=None):\n        if normalization is None:\n            normalization = self.normalization\n        if csphase is None:\n            csphase = self.csphase\n        if lmax is None:\n            lmax = self.lmax\n\n        coeffs = _convert(self.coeffs, normalization_in=self.normalization,\n                          normalization_out=normalization,\n                          csphase_in=self.csphase, csphase_out=csphase,\n                          lmax=lmax)\n\n        return coeffs", "response": "Return the spherical harmonic coefficients as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spectrum(self, lmax=None, convention='power', unit='per_l', base=10.):\n        return _spectrum(self.coeffs, normalization=self.normalization,\n                         convention=convention, unit=unit, base=base,\n                         lmax=lmax)", "response": "Return the spectrum of the current entry in the logarithm of the spherical harmonic degree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef volume(self, lmax=None):\n        if self.coeffs[0, 0, 0] == 0:\n            raise ValueError('The volume of the object can not be calculated '\n                             'when the degree and order 0 term is equal to '\n                             'zero.')\n\n        if self.kind == 'complex':\n            raise ValueError('The volume of the object can not be calculated '\n                             'for complex functions.')\n\n        if lmax is None:\n            lmax = self.lmax\n\n        r0 = self.coeffs[0, 0, 0]\n        grid = self.expand(lmax=3*lmax) - r0\n        h200 = (grid**2).expand(lmax_calc=0).coeffs[0, 0, 0]\n        h300 = (grid**3).expand(lmax_calc=0).coeffs[0, 0, 0]\n\n        volume = 4 * _np.pi / 3 * (h300 + 3 * r0 * h200 + r0**3)\n        return volume", "response": "Calculates the volume of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrotates either the coordinate system used to express the spherical harmonic coefficients or the physical body, and return a new class instance. Usage ----- x_rotated = x.rotate(alpha, beta, gamma, [degrees, convention, body, dj_matrix]) Returns ------- x_rotated : SHCoeffs class instance Parameters ---------- alpha, beta, gamma : float The three Euler rotation angles in degrees. degrees : bool, optional, default = True True if the Euler angles are in degrees, False if they are in radians. convention : str, optional, default = 'y' The convention used for the rotation of the second angle, which can be either 'x' or 'y' for a rotation about the x or y axes, respectively. body : bool, optional, default = False If true, rotate the physical body and not the coordinate system. dj_matrix : ndarray, optional, default = None The djpi2 rotation matrix computed by a call to djpi2. Description ----------- This method will take the spherical harmonic coefficients of a function, rotate the coordinate frame by the three Euler anlges, and output the spherical harmonic coefficients of the new function. If the optional parameter body is set to True, then the physical body will be rotated instead of the coordinate system. The rotation of a coordinate system or body can be viewed in two complementary ways involving three successive rotations. Both methods have the same initial and final configurations, and the angles listed in both schemes are the same. Scheme A: (I) Rotation about the z axis by alpha. (II) Rotation about the new y axis by beta. (III) Rotation about the new z axis by gamma. Scheme B: (I) Rotation about the z axis by gamma. (II) Rotation about the initial y axis by beta. (III) Rotation about the initial z axis by alpha. Here, the 'y convention' is employed, where the second rotation is with respect to the y axis. When using the 'x convention', the second rotation is instead with respect to the x axis. The relation between the Euler angles in the x and y conventions is given by alpha_y=alpha_x-pi/2, beta_y=beta_x, and gamma_y=gamma_x+pi/2. To perform the inverse transform associated with the three angles (alpha, beta, gamma), one would perform an additional rotation using the angles (-gamma, -beta, -alpha). The rotations can be viewed either as a rotation of the coordinate system or the physical body. To rotate the physical body without rotation of the coordinate system, set the optional parameter body to True. This rotation is accomplished by performing the inverse rotation using the angles (-gamma, -beta, -alpha).", "response": "def rotate(self, alpha, beta, gamma, degrees=True, convention='y',\n               body=False, dj_matrix=None):\n        \"\"\"\n        Rotate either the coordinate system used to express the spherical\n        harmonic coefficients or the physical body, and return a new class\n        instance.\n\n        Usage\n        -----\n        x_rotated = x.rotate(alpha, beta, gamma, [degrees, convention,\n                             body, dj_matrix])\n\n        Returns\n        -------\n        x_rotated : SHCoeffs class instance\n\n        Parameters\n        ----------\n        alpha, beta, gamma : float\n            The three Euler rotation angles in degrees.\n        degrees : bool, optional, default = True\n            True if the Euler angles are in degrees, False if they are in\n            radians.\n        convention : str, optional, default = 'y'\n            The convention used for the rotation of the second angle, which\n            can be either 'x' or 'y' for a rotation about the x or y axes,\n            respectively.\n        body : bool, optional, default = False\n            If true, rotate the physical body and not the coordinate system.\n        dj_matrix : ndarray, optional, default = None\n            The djpi2 rotation matrix computed by a call to djpi2.\n\n        Description\n        -----------\n        This method will take the spherical harmonic coefficients of a\n        function, rotate the coordinate frame by the three Euler anlges, and\n        output the spherical harmonic coefficients of the new function. If\n        the optional parameter body is set to True, then the physical body will\n        be rotated instead of the coordinate system.\n\n        The rotation of a coordinate system or body can be viewed in two\n        complementary ways involving three successive rotations. Both methods\n        have the same initial and final configurations, and the angles listed\n        in both schemes are the same.\n\n        Scheme A:\n\n        (I) Rotation about the z axis by alpha.\n        (II) Rotation about the new y axis by beta.\n        (III) Rotation about the new z axis by gamma.\n\n        Scheme B:\n\n        (I) Rotation about the z axis by gamma.\n        (II) Rotation about the initial y axis by beta.\n        (III) Rotation about the initial z axis by alpha.\n\n        Here, the 'y convention' is employed, where the second rotation is with\n        respect to the y axis. When using the 'x convention', the second\n        rotation is instead with respect to the x axis. The relation between\n        the Euler angles in the x and y conventions is given by\n\n        alpha_y=alpha_x-pi/2, beta_y=beta_x, and gamma_y=gamma_x+pi/2.\n\n        To perform the inverse transform associated with the three angles\n        (alpha, beta, gamma), one would perform an additional rotation using\n        the angles (-gamma, -beta, -alpha).\n\n        The rotations can be viewed either as a rotation of the coordinate\n        system or the physical body. To rotate the physical body without\n        rotation of the coordinate system, set the optional parameter body to\n        True. This rotation is accomplished by performing the inverse rotation\n        using the angles (-gamma, -beta, -alpha).\n        \"\"\"\n        if type(convention) != str:\n            raise ValueError('convention must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(convention))))\n\n        if convention.lower() not in ('x', 'y'):\n            raise ValueError(\n                \"convention must be either 'x' or 'y'. \" +\n                \"Provided value was {:s}\".format(repr(convention))\n                )\n\n        if convention is 'y':\n            if body is True:\n                angles = _np.array([-gamma, -beta, -alpha])\n            else:\n                angles = _np.array([alpha, beta, gamma])\n        elif convention is 'x':\n            if body is True:\n                angles = _np.array([-gamma - _np.pi/2, -beta,\n                                    -alpha + _np.pi/2])\n            else:\n                angles = _np.array([alpha - _np.pi/2, beta, gamma + _np.pi/2])\n\n        if degrees:\n            angles = _np.radians(angles)\n\n        if self.lmax > 1200:\n            _warnings.warn(\"The rotate() method is accurate only to about\" +\n                           \" spherical harmonic degree 1200. \" +\n                           \"lmax = {:d}\".format(self.lmax),\n                           category=RuntimeWarning)\n\n        rot = self._rotate(angles, dj_matrix)\n        return rot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert(self, normalization=None, csphase=None, lmax=None, kind=None,\n                check=True):\n        \"\"\"\n        Return a SHCoeffs class instance with a different normalization\n        convention.\n\n        Usage\n        -----\n        clm = x.convert([normalization, csphase, lmax, kind, check])\n\n        Returns\n        -------\n        clm : SHCoeffs class instance\n\n        Parameters\n        ----------\n        normalization : str, optional, default = x.normalization\n            Normalization of the output class: '4pi', 'ortho', 'schmidt', or\n            'unnorm', for geodesy 4pi normalized, orthonormalized, Schmidt\n            semi-normalized, or unnormalized coefficients, respectively.\n        csphase : int, optional, default = x.csphase\n            Condon-Shortley phase convention for the output class: 1 to exclude\n            the phase factor, or -1 to include it.\n        lmax : int, optional, default = x.lmax\n            Maximum spherical harmonic degree to output.\n        kind : str, optional, default = clm.kind\n            'real' or 'complex' spherical harmonic coefficients for the output\n            class.\n        check : bool, optional, default = True\n            When converting complex coefficients to real coefficients, if True,\n            check if function is entirely real.\n\n        Description\n        -----------\n        This method will return a new class instance of the spherical\n        harmonic coefficients using a different normalization and\n        Condon-Shortley phase convention. The coefficients can be converted\n        between real and complex form, and a different maximum spherical\n        harmonic degree of the output coefficients can be specified. If this\n        maximum degree is smaller than the maximum degree of the original\n        class, the coefficients will be truncated. Conversely, if this degree\n        is larger than the maximum degree of the original class, the\n        coefficients of the new class will be zero padded.\n        \"\"\"\n        if normalization is None:\n            normalization = self.normalization\n        if csphase is None:\n            csphase = self.csphase\n        if lmax is None:\n            lmax = self.lmax\n        if kind is None:\n            kind = self.kind\n\n        # check argument consistency\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"normalization must be '4pi', 'ortho', 'schmidt', or \" +\n                \"'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization)))\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase)))\n\n        if (kind != self.kind):\n            if (kind == 'complex'):\n                temp = self._make_complex()\n            else:\n                temp = self._make_real(check=check)\n            coeffs = temp.to_array(normalization=normalization.lower(),\n                                   csphase=csphase, lmax=lmax)\n        else:\n            coeffs = self.to_array(normalization=normalization.lower(),\n                                   csphase=csphase, lmax=lmax)\n\n        return SHCoeffs.from_array(coeffs,\n                                   normalization=normalization.lower(),\n                                   csphase=csphase, copy=False)", "response": "Convert a single spherical harmonic degree to real."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating the spherical harmonic coefficients on a global grid or for a list of coordinates.", "response": "def expand(self, grid='DH', lat=None, colat=None, lon=None, degrees=True,\n               zeros=None, lmax=None, lmax_calc=None):\n        \"\"\"\n        Evaluate the spherical harmonic coefficients either on a global grid\n        or for a list of coordinates.\n\n        Usage\n        -----\n        f = x.expand([grid, lmax, lmax_calc, zeros])\n        g = x.expand(lat=lat, lon=lon, [lmax_calc, degrees])\n        g = x.expand(colat=colat, lon=lon, [lmax_calc, degrees])\n\n        Returns\n        -------\n        f : SHGrid class instance\n        g : float, ndarray, or list\n\n        Parameters\n        ----------\n        lat : int, float, ndarray, or list, optional, default = None\n            Latitude coordinates where the function is to be evaluated.\n        colat : int, float, ndarray, or list, optional, default = None\n            Colatitude coordinates where the function is to be evaluated.\n        lon : int, float, ndarray, or list, optional, default = None\n            Longitude coordinates where the function is to be evaluated.\n        degrees : bool, optional, default = True\n            True if lat, colat and lon are in degrees, False if in radians.\n        grid : str, optional, default = 'DH'\n            'DH' or 'DH1' for an equisampled lat/lon grid with nlat=nlon,\n            'DH2' for an equidistant lat/lon grid with nlon=2*nlat, or 'GLQ'\n            for a Gauss-Legendre quadrature grid.\n        lmax : int, optional, default = x.lmax\n            The maximum spherical harmonic degree, which determines the grid\n            spacing of the output grid.\n        lmax_calc : int, optional, default = x.lmax\n            The maximum spherical harmonic degree to use when evaluating the\n            function.\n        zeros : ndarray, optional, default = None\n            The cos(colatitude) nodes used in the Gauss-Legendre Quadrature\n            grids.\n\n        Description\n        -----------\n        This method either (1) evaluates the spherical harmonic coefficients on\n        a global grid and returns an SHGrid class instance, or (2) evaluates\n        the spherical harmonic coefficients for a list of (co)latitude and\n        longitude coordinates. For the first case, the grid type is defined\n        by the optional parameter grid, which can be 'DH', 'DH2' or 'GLQ'.For\n        the second case, the optional parameters lon and either colat or lat\n        must be provided.\n        \"\"\"\n        if lat is not None and colat is not None:\n            raise ValueError('lat and colat can not both be specified.')\n\n        if lat is not None and lon is not None:\n            if lmax_calc is None:\n                lmax_calc = self.lmax\n\n            values = self._expand_coord(lat=lat, lon=lon, degrees=degrees,\n                                        lmax_calc=lmax_calc)\n            return values\n\n        if colat is not None and lon is not None:\n            if lmax_calc is None:\n                lmax_calc = self.lmax\n\n            if type(colat) is list:\n                lat = list(map(lambda x: 90 - x, colat))\n            else:\n                lat = 90 - colat\n\n            values = self._expand_coord(lat=lat, lon=lon, degrees=degrees,\n                                        lmax_calc=lmax_calc)\n            return values\n\n        else:\n            if lmax is None:\n                lmax = self.lmax\n            if lmax_calc is None:\n                lmax_calc = lmax\n\n            if type(grid) != str:\n                raise ValueError('grid must be a string. ' +\n                                 'Input type was {:s}'\n                                 .format(str(type(grid))))\n\n            if grid.upper() in ('DH', 'DH1'):\n                gridout = self._expandDH(sampling=1, lmax=lmax,\n                                         lmax_calc=lmax_calc)\n            elif grid.upper() == 'DH2':\n                gridout = self._expandDH(sampling=2, lmax=lmax,\n                                         lmax_calc=lmax_calc)\n            elif grid.upper() == 'GLQ':\n                gridout = self._expandGLQ(zeros=zeros, lmax=lmax,\n                                          lmax_calc=lmax_calc)\n            else:\n                raise ValueError(\n                    \"grid must be 'DH', 'DH1', 'DH2', or 'GLQ'. \" +\n                    \"Input value was {:s}\".format(repr(grid)))\n\n            return gridout"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the spectrum of the current state of the object.", "response": "def plot_spectrum(self, convention='power', unit='per_l', base=10.,\n                      lmax=None, xscale='lin', yscale='log', grid=True,\n                      legend=None, axes_labelsize=None, tick_labelsize=None,\n                      show=True, ax=None, fname=None, **kwargs):\n        \"\"\"\n        Plot the spectrum as a function of spherical harmonic degree.\n\n        Usage\n        -----\n        x.plot_spectrum([convention, unit, base, lmax, xscale, yscale, grid,\n                         axes_labelsize, tick_labelsize, legend, show, ax,\n                         fname, **kwargs])\n\n        Parameters\n        ----------\n        convention : str, optional, default = 'power'\n            The type of spectrum to plot: 'power' for power spectrum,\n            'energy' for energy spectrum, and 'l2norm' for the l2 norm\n            spectrum.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', plot the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', plot the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', plot the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum, and\n            the base to use for logarithmic axes.\n        lmax : int, optional, default = self.lmax\n            The maximum spherical harmonic degree to plot.\n        xscale : str, optional, default = 'lin'\n            Scale of the x axis: 'lin' for linear or 'log' for logarithmic.\n        yscale : str, optional, default = 'log'\n            Scale of the y axis: 'lin' for linear or 'log' for logarithmic.\n        grid : bool, optional, default = True\n            If True, plot grid lines.\n        legend : str, optional, default = None\n            Text to use for the legend.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot to the screen.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        **kwargs : keyword arguments, optional\n            Keyword arguments for pyplot.plot().\n\n        Description\n        -----------\n        This method plots either the power spectrum, energy spectrum, or\n        l2-norm spectrum. Total power is defined as the integral of the\n        function squared over all space, divided by the area the function\n        spans. If the mean of the function is zero, this is equivalent to the\n        variance of the function. The total energy is the integral of the\n        function squared over all space and is 4pi times the total power. For\n        normalized coefficients ('4pi', 'ortho', or 'schmidt'), the l2-norm is\n        the sum of the magnitude of the coefficients squared.\n\n        The output spectrum can be expresed using one of three units. 'per_l'\n        returns the contribution to the total spectrum from all angular orders\n        at degree l. 'per_lm' returns the average contribution to the total\n        spectrum from a single coefficient at degree l, which is equal to the\n        'per_l' spectrum divided by (2l+1). 'per_dlogl' returns the\n        contribution to the total spectrum from all angular orders over an\n        infinitessimal logarithmic degree band. The contrubution in the band\n        dlog_a(l) is spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base,\n        and where spectrum(l, 'per_dlogl) is equal to\n        spectrum(l, 'per_l')*l*log(a).\n        \"\"\"\n        if lmax is None:\n            lmax = self.lmax\n\n        spectrum = self.spectrum(convention=convention, unit=unit, base=base,\n                                 lmax=lmax)\n        ls = _np.arange(lmax + 1)\n\n        if ax is None:\n            fig, axes = _plt.subplots(1, 1)\n        else:\n            axes = ax\n\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n\n        axes.set_xlabel('Spherical harmonic degree', fontsize=axes_labelsize)\n        if convention == 'Energy':\n            axes.set_ylabel('Energy', fontsize=axes_labelsize)\n            if legend is None:\n                if (unit == 'per_l'):\n                    legend = 'Energy per degree'\n                elif (unit == 'per_lm'):\n                    legend = 'Energy per coefficient'\n                elif (unit == 'per_dlogl'):\n                    legend = 'Energy per log bandwidth'\n        elif convention == 'l2norm':\n            axes.set_ylabel('l2 norm', fontsize=axes_labelsize)\n            if legend is None:\n                if (unit == 'per_l'):\n                    legend = 'l2 norm per degree'\n                elif (unit == 'per_lm'):\n                    legend = 'l2 norm per coefficient'\n                elif (unit == 'per_dlogl'):\n                    legend = 'l2 norm per log bandwidth'\n        else:\n            axes.set_ylabel('Power', fontsize=axes_labelsize)\n            if legend is None:\n                if (unit == 'per_l'):\n                    legend = 'Power per degree'\n                elif (unit == 'per_lm'):\n                    legend = 'Power per coefficient'\n                elif (unit == 'per_dlogl'):\n                    legend = 'Power per log bandwidth'\n\n        if xscale == 'log':\n            axes.set_xscale('log', basex=base)\n        if yscale == 'log':\n            axes.set_yscale('log', basey=base)\n\n        if xscale == 'log':\n            axes.plot(ls[1:lmax+1], spectrum[1:lmax+1], label=legend, **kwargs)\n        else:\n            axes.plot(ls[:lmax+1], spectrum[:lmax+1], label=legend, **kwargs)\n            axes.set(xlim=(ls[0], ls[lmax]))\n\n        axes.grid(grid, which='major')\n        axes.minorticks_on()\n        axes.tick_params(labelsize=tick_labelsize)\n        axes.legend()\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the spectrum of the current object.", "response": "def plot_spectrum2d(self, convention='power', xscale='lin', yscale='lin',\n                        grid=True, axes_labelsize=None, tick_labelsize=None,\n                        vscale='log', vrange=None, vmin=None, vmax=None,\n                        lmax=None, show=True, ax=None, fname=None):\n        \"\"\"\n        Plot the spectrum as a function of spherical harmonic degree and order.\n\n        Usage\n        -----\n        x.plot_spectrum2d([convention, xscale, yscale, grid, axes_labelsize,\n                           tick_labelsize, vscale, vrange, vmin, vmax, lmax,\n                           show, ax, fname])\n\n        Parameters\n        ----------\n        convention : str, optional, default = 'power'\n            The type of spectrum to plot: 'power' for power spectrum,\n            'energy' for energy spectrum, and 'l2norm' for the l2 norm\n            spectrum.\n        xscale : str, optional, default = 'lin'\n            Scale of the l axis: 'lin' for linear or 'log' for logarithmic.\n        yscale : str, optional, default = 'lin'\n            Scale of the m axis: 'lin' for linear or 'log' for logarithmic.\n        grid : bool, optional, default = True\n            If True, plot grid lines.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        vscale : str, optional, default = 'log'\n            Scale of the color axis: 'lin' for linear or 'log' for logarithmic.\n        vrange : (float, float), optional, default = None\n            Colormap range (min, max) relative to the maximum value. If None,\n            scale the image to the maximum and minimum values.\n        vmin : float, optional, default=None\n            The minmum range of the colormap. If None, the minimum value of the\n            spectrum will be used.\n        vmax : float, optional, default=None\n            The maximum range of the colormap. If None, the maximum value of\n            the spectrum will be used.\n        lmax : int, optional, default = self.lmax\n            The maximum spherical harmonic degree to plot.\n        show : bool, optional, default = True\n            If True, plot to the screen.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n\n        Description\n        -----------\n        This method plots either the power, energy, or l2-norm for each\n        spherical harmonic degree and order of the function. Total power is\n        defined as the integral of the function squared over all space,\n        divided by the area the function spans. If the mean of the function is\n        zero, this is equivalent to the variance of the function. The total\n        energy is the integral of the function squared over all space and is\n        4pi times the total power. For normalized coefficients ('4pi',\n        'ortho', or 'schmidt'), the l2-norm is the sum of the magnitude of the\n        coefficients squared.\n        \"\"\"\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n\n        if lmax is None:\n            lmax = self.lmax\n        degrees = _np.arange(lmax + 1)\n\n        # Create the matrix of the spectrum for each coefficient\n        spectrum = _np.empty((lmax + 1, 2 * lmax + 1))\n        mpositive = _np.abs(self.coeffs[0, :lmax + 1, :lmax + 1])**2\n        mpositive[~self.mask[0, :lmax + 1, :lmax + 1]] = _np.nan\n        mnegative = _np.abs(self.coeffs[1, :lmax + 1, :lmax + 1])**2\n        mnegative[~self.mask[1, :lmax + 1, :lmax + 1]] = _np.nan\n\n        spectrum[:, :lmax] = _np.fliplr(mnegative)[:, :lmax]\n        spectrum[:, lmax:] = mpositive\n\n        if (convention.lower() == 'l2norm'):\n            if self.normalization == 'unnorm':\n                raise ValueError(\"convention can not be set to 'l2norm' \" +\n                                 \"when using unnormalized harmonics.\")\n            else:\n                pass\n        elif convention.lower() in ('power', 'energy'):\n            if self.normalization == '4pi':\n                pass\n            elif self.normalization == 'schmidt':\n                for l in degrees:\n                    spectrum[l, :] /= (2. * l + 1.)\n            elif self.normalization == 'ortho':\n                for l in degrees:\n                    spectrum[l, :] /= (4. * _np.pi)\n            elif self.normalization == 'unnorm':\n                for l in degrees:\n                    ms = _np.arange(l+1)\n                    conv = _factorial(l+ms) / (2. * l + 1.) / _factorial(l-ms)\n                    if self.kind == 'real':\n                        conv[1:l + 1] = conv[1:l + 1] / 2.\n                    spectrum[l, lmax-l:lmax] *= conv[::-1][0:l]\n                    spectrum[l, lmax:lmax+l+1] *= conv[0:l+1]\n            else:\n                raise ValueError(\n                    \"normalization must be '4pi', 'ortho', 'schmidt', \" +\n                    \"or 'unnorm'. Input value was {:s}\"\n                    .format(repr(self.normalization)))\n        else:\n            raise ValueError(\n                \"convention must be 'power', 'energy', or 'l2norm'. \" +\n                \"Input value was {:s}\".format(repr(convention)))\n\n        if convention == 'energy':\n            spectrum *= 4.0 * _np.pi\n\n        spectrum_masked = _np.ma.masked_invalid(spectrum)\n\n        # need to add one extra value to each in order for pcolormesh\n        # to plot the last row and column.\n        ls = _np.arange(lmax+2).astype(_np.float)\n        ms = _np.arange(-lmax, lmax + 2, dtype=_np.float)\n        lgrid, mgrid = _np.meshgrid(ls, ms, indexing='ij')\n        lgrid -= 0.5\n        mgrid -= 0.5\n\n        if ax is None:\n            fig, axes = _plt.subplots()\n        else:\n            axes = ax\n\n        if vrange is not None:\n            vmin = _np.nanmax(spectrum) * vrange[0]\n            vmax = _np.nanmax(spectrum) * vrange[1]\n        else:\n            if vmin is None:\n                _temp = spectrum\n                _temp[_temp == 0] = _np.NaN\n                vmin = _np.nanmin(_temp)\n            if vmax is None:\n                vmax = _np.nanmax(spectrum)\n\n        if vscale.lower() == 'log':\n            norm = _mpl.colors.LogNorm(vmin, vmax, clip=True)\n            # Clipping is required to avoid an invalid value error\n        elif vscale.lower() == 'lin':\n            norm = _plt.Normalize(vmin, vmax)\n        else:\n            raise ValueError(\n                \"vscale must be 'lin' or 'log'. \" +\n                \"Input value was {:s}\".format(repr(vscale)))\n\n        if (xscale == 'lin'):\n            cmesh = axes.pcolormesh(lgrid, mgrid, spectrum_masked,\n                                    norm=norm, cmap='viridis')\n            axes.set(xlim=(-0.5, lmax + 0.5))\n        elif (xscale == 'log'):\n            cmesh = axes.pcolormesh(lgrid[1:], mgrid[1:], spectrum_masked[1:],\n                                    norm=norm, cmap='viridis')\n            axes.set(xscale='log', xlim=(1., lmax + 0.5))\n        else:\n            raise ValueError(\n                \"xscale must be 'lin' or 'log'. \" +\n                \"Input value was {:s}\".format(repr(xscale)))\n\n        if (yscale == 'lin'):\n            axes.set(ylim=(-lmax - 0.5, lmax + 0.5))\n        elif (yscale == 'log'):\n            axes.set(yscale='symlog', ylim=(-lmax - 0.5, lmax + 0.5))\n        else:\n            raise ValueError(\n                \"yscale must be 'lin' or 'log'. \" +\n                \"Input value was {:s}\".format(repr(yscale)))\n\n        cb = _plt.colorbar(cmesh, ax=ax)\n\n        if (convention == 'energy'):\n            cb.set_label('Energy per coefficient', fontsize=axes_labelsize)\n        elif (convention == 'power'):\n            cb.set_label('Power per coefficient', fontsize=axes_labelsize)\n        else:\n            cb.set_label('Magnitude-squared coefficient',\n                         fontsize=axes_labelsize)\n\n        cb.ax.tick_params(labelsize=tick_labelsize)\n        axes.set_xlabel('Spherical harmonic degree', fontsize=axes_labelsize)\n        axes.set_ylabel('Spherical harmonic order', fontsize=axes_labelsize)\n        axes.tick_params(labelsize=tick_labelsize)\n        axes.minorticks_on()\n        axes.grid(grid, which='major')\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_complex(self):\n        rcomplex_coeffs = _shtools.SHrtoc(self.coeffs,\n                                          convention=1, switchcs=0)\n\n        # These coefficients are using real floats, and need to be\n        # converted to complex form.\n        complex_coeffs = _np.zeros((2, self.lmax+1, self.lmax+1),\n                                   dtype='complex')\n        complex_coeffs[0, :, :] = (rcomplex_coeffs[0, :, :] + 1j *\n                                   rcomplex_coeffs[1, :, :])\n        complex_coeffs[1, :, :] = complex_coeffs[0, :, :].conjugate()\n        for m in self.degrees():\n            if m % 2 == 1:\n                complex_coeffs[1, :, m] = - complex_coeffs[1, :, m]\n\n        # complex_coeffs is initialized in this function and can be\n        # passed as reference\n        return SHCoeffs.from_array(complex_coeffs,\n                                   normalization=self.normalization,\n                                   csphase=self.csphase, copy=False)", "response": "Convert the real SHCoeffs class to the complex class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating the function at the coordinates lat and lon.", "response": "def _expand_coord(self, lat, lon, lmax_calc, degrees):\n        \"\"\"Evaluate the function at the coordinates lat and lon.\"\"\"\n        if self.normalization == '4pi':\n            norm = 1\n        elif self.normalization == 'schmidt':\n            norm = 2\n        elif self.normalization == 'unnorm':\n            norm = 3\n        elif self.normalization == 'ortho':\n            norm = 4\n        else:\n            raise ValueError(\n                \"Normalization must be '4pi', 'ortho', 'schmidt', or \" +\n                \"'unnorm'. Input value was {:s}\"\n                .format(repr(self.normalization)))\n\n        if degrees is True:\n            latin = lat\n            lonin = lon\n        else:\n            latin = _np.rad2deg(lat)\n            lonin = _np.rad2deg(lon)\n\n        if type(lat) is not type(lon):\n            raise ValueError('lat and lon must be of the same type. ' +\n                             'Input types are {:s} and {:s}'\n                             .format(repr(type(lat)), repr(type(lon))))\n\n        if type(lat) is int or type(lat) is float or type(lat) is _np.float_:\n            return _shtools.MakeGridPoint(self.coeffs, lat=latin, lon=lonin,\n                                          lmax=lmax_calc, norm=norm,\n                                          csphase=self.csphase)\n        elif type(lat) is _np.ndarray:\n            values = _np.empty_like(lat, dtype=float)\n            for v, latitude, longitude in _np.nditer([values, latin, lonin],\n                                                     op_flags=['readwrite']):\n                v[...] = _shtools.MakeGridPoint(self.coeffs, lat=latitude,\n                                                lon=longitude,\n                                                lmax=lmax_calc, norm=norm,\n                                                csphase=self.csphase)\n            return values\n        elif type(lat) is list:\n            values = []\n            for latitude, longitude in zip(latin, lonin):\n                values.append(\n                    _shtools.MakeGridPoint(self.coeffs, lat=latitude,\n                                           lon=longitude,\n                                           lmax=lmax_calc, norm=norm,\n                                           csphase=self.csphase))\n            return values\n        else:\n            raise ValueError('lat and lon must be either an int, float, ' +\n                             'ndarray, or list. ' +\n                             'Input types are {:s} and {:s}'\n                             .format(repr(type(lat)), repr(type(lon))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the complex SHCoeffs class to the real class.", "response": "def _make_real(self, check=True):\n        \"\"\"Convert the complex SHCoeffs class to the real class.\"\"\"\n        # Test if the coefficients correspond to a real grid.\n        # This is not very elegant, and the equality condition\n        # is probably not robust to round off errors.\n        if check:\n            for l in self.degrees():\n                if self.coeffs[0, l, 0] != self.coeffs[0, l, 0].conjugate():\n                    raise RuntimeError('Complex coefficients do not ' +\n                                       'correspond to a real field. ' +\n                                       'l = {:d}, m = 0: {:e}'\n                                       .format(l, self.coeffs[0, l, 0]))\n                for m in _np.arange(1, l + 1):\n                    if m % 2 == 1:\n                        if (self.coeffs[0, l, m] != -\n                                self.coeffs[1, l, m].conjugate()):\n                            raise RuntimeError('Complex coefficients do not ' +\n                                               'correspond to a real field. ' +\n                                               'l = {:d}, m = {:d}: {:e}, {:e}'\n                                               .format(\n                                                   l, m, self.coeffs[0, l, 0],\n                                                   self.coeffs[1, l, 0]))\n                    else:\n                        if (self.coeffs[0, l, m] !=\n                                self.coeffs[1, l, m].conjugate()):\n                            raise RuntimeError('Complex coefficients do not ' +\n                                               'correspond to a real field. ' +\n                                               'l = {:d}, m = {:d}: {:e}, {:e}'\n                                               .format(\n                                                   l, m, self.coeffs[0, l, 0],\n                                                   self.coeffs[1, l, 0]))\n\n        coeffs_rc = _np.zeros((2, self.lmax + 1, self.lmax + 1))\n        coeffs_rc[0, :, :] = self.coeffs[0, :, :].real\n        coeffs_rc[1, :, :] = self.coeffs[0, :, :].imag\n        real_coeffs = _shtools.SHctor(coeffs_rc, convention=1,\n                                      switchcs=0)\n        return SHCoeffs.from_array(real_coeffs,\n                                   normalization=self.normalization,\n                                   csphase=self.csphase)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrotate the coefficients by the Euler angles alpha beta and gamma.", "response": "def _rotate(self, angles, dj_matrix):\n        \"\"\"Rotate the coefficients by the Euler angles alpha, beta, gamma.\"\"\"\n        # Note that the current method is EXTREMELY inefficient. The complex\n        # coefficients are expanded onto real and imaginary grids, each of\n        # the two components are rotated separately as real data, the rotated\n        # real data are re-expanded on new real and complex grids, they are\n        # combined to make a complex grid, and the resultant is expanded\n        # in complex spherical harmonics.\n        if dj_matrix is None:\n            dj_matrix = _shtools.djpi2(self.lmax + 1)\n\n        cgrid = self.expand(grid='DH')\n        rgrid, igrid = cgrid.data.real, cgrid.data.imag\n        rgridcoeffs = _shtools.SHExpandDH(rgrid, norm=1, sampling=1, csphase=1)\n        igridcoeffs = _shtools.SHExpandDH(igrid, norm=1, sampling=1, csphase=1)\n\n        rgridcoeffs_rot = _shtools.SHRotateRealCoef(\n            rgridcoeffs, angles, dj_matrix)\n        igridcoeffs_rot = _shtools.SHRotateRealCoef(\n            igridcoeffs, angles, dj_matrix)\n\n        rgrid_rot = _shtools.MakeGridDH(rgridcoeffs_rot, norm=1,\n                                        sampling=1, csphase=1)\n        igrid_rot = _shtools.MakeGridDH(igridcoeffs_rot, norm=1,\n                                        sampling=1, csphase=1)\n        grid_rot = rgrid_rot + 1j * igrid_rot\n\n        if self.normalization == '4pi':\n            norm = 1\n        elif self.normalization == 'schmidt':\n            norm = 2\n        elif self.normalization == 'unnorm':\n            norm = 3\n        elif self.normalization == 'ortho':\n            norm = 4\n        else:\n            raise ValueError(\n                \"Normalization must be '4pi', 'ortho', 'schmidt', or \" +\n                \"'unnorm'. Input value was {:s}\"\n                .format(repr(self.normalization)))\n\n        coeffs_rot = _shtools.SHExpandDHC(grid_rot, norm=norm,\n                                          csphase=self.csphase)\n\n        return SHCoeffs.from_array(coeffs_rot,\n                                   normalization=self.normalization,\n                                   csphase=self.csphase, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the coefficients on a Driscoll and Healy ( 1994 ) grid.", "response": "def _expandDH(self, sampling, lmax, lmax_calc):\n        \"\"\"Evaluate the coefficients on a Driscoll and Healy (1994) grid.\"\"\"\n        if self.normalization == '4pi':\n            norm = 1\n        elif self.normalization == 'schmidt':\n            norm = 2\n        elif self.normalization == 'unnorm':\n            norm = 3\n        elif self.normalization == 'ortho':\n            norm = 4\n        else:\n            raise ValueError(\n                \"Normalization must be '4pi', 'ortho', 'schmidt', or \" +\n                \"'unnorm'. Input value was {:s}\"\n                .format(repr(self.normalization)))\n\n        data = _shtools.MakeGridDHC(self.coeffs, sampling=sampling,\n                                    norm=norm, csphase=self.csphase, lmax=lmax,\n                                    lmax_calc=lmax_calc)\n        gridout = SHGrid.from_array(data, grid='DH', copy=False)\n        return gridout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expandGLQ(self, zeros, lmax, lmax_calc):\n        if self.normalization == '4pi':\n            norm = 1\n        elif self.normalization == 'schmidt':\n            norm = 2\n        elif self.normalization == 'unnorm':\n            norm = 3\n        elif self.normalization == 'ortho':\n            norm = 4\n        else:\n            raise ValueError(\n                \"Normalization must be '4pi', 'ortho', 'schmidt', or \" +\n                \"'unnorm'. Input value was {:s}\"\n                .format(repr(self.normalization)))\n\n        if zeros is None:\n            zeros, weights = _shtools.SHGLQ(self.lmax)\n\n        data = _shtools.MakeGridGLQC(self.coeffs, zeros, norm=norm,\n                                     csphase=self.csphase, lmax=lmax,\n                                     lmax_calc=lmax_calc)\n        gridout = SHGrid.from_array(data, grid='GLQ', copy=False)\n        return gridout", "response": "Evaluate the coefficients on a Gauss - Legendre quadrature grid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_array(self, array, grid='DH', copy=True):\n        if _np.iscomplexobj(array):\n            kind = 'complex'\n        else:\n            kind = 'real'\n\n        if type(grid) != str:\n            raise ValueError('grid must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(grid))))\n\n        if grid.upper() not in set(['DH', 'GLQ']):\n            raise ValueError(\n                \"grid must be 'DH' or 'GLQ'. Input value was {:s}.\"\n                .format(repr(grid))\n                )\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind) and cls.isgrid(grid):\n                return cls(array, copy=copy)", "response": "Initialize the class instance from an array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the class instance from a file containing the gridded data.", "response": "def from_file(self, fname, binary=False, **kwargs):\n        \"\"\"\n        Initialize the class instance from gridded data in a file.\n\n        Usage\n        -----\n        x = SHGrid.from_file(fname, [binary, **kwargs])\n\n        Returns\n        -------\n        x : SHGrid class instance\n\n        Parameters\n        ----------\n        fname : str\n            The filename containing the gridded data. For text files (default)\n            the file is read using the numpy routine loadtxt(), whereas for\n            binary files, the file is read using numpy.load(). The dimensions\n            of the array must be nlon=nlat or nlon=2*nlat for Driscoll and\n            Healy grids, or nlon=2*nlat-1 for Gauss-Legendre Quadrature grids.\n        binary : bool, optional, default = False\n            If False, read a text file. If True, read a binary 'npy' file.\n        **kwargs : keyword arguments, optional\n            Keyword arguments of numpy.loadtxt() or numpy.load().\n        \"\"\"\n        if binary is False:\n            data = _np.loadtxt(fname, **kwargs)\n        elif binary is True:\n            data = _np.load(fname, **kwargs)\n        else:\n            raise ValueError('binary must be True or False. '\n                             'Input value is {:s}'.format(binary))\n\n        if _np.iscomplexobj(data):\n            kind = 'complex'\n        else:\n            kind = 'real'\n\n        if (data.shape[1] == data.shape[0]) or (data.shape[1] ==\n                                                2 * data.shape[0]):\n            grid = 'DH'\n        elif data.shape[1] == 2 * data.shape[0] - 1:\n            grid = 'GLQ'\n        else:\n            raise ValueError('Input grid must be dimensioned as ' +\n                             '(nlat, nlon). For DH grids, nlon = nlat or ' +\n                             'nlon = 2 * nlat. For GLQ grids, nlon = ' +\n                             '2 * nlat - 1. Input dimensions are nlat = ' +\n                             '{:d}, nlon = {:d}'.format(data.shape[0],\n                                                        data.shape[1]))\n\n        for cls in self.__subclasses__():\n            if cls.istype(kind) and cls.isgrid(grid):\n                return cls(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_file(self, filename, binary=False, **kwargs):\n        if binary is False:\n            _np.savetxt(filename, self.data, **kwargs)\n        elif binary is True:\n            _np.save(filename, self.data, **kwargs)\n        else:\n            raise ValueError('binary must be True or False. '\n                             'Input value is {:s}'.format(binary))", "response": "Save gridded data to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lats(self, degrees=True):\n        if degrees is False:\n            return _np.radians(self._lats())\n        else:\n            return self._lats()", "response": "Return the latitudes of each row of the gridded data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the longitudes of each column of the gridded data.", "response": "def lons(self, degrees=True):\n        \"\"\"\n        Return the longitudes of each column of the gridded data.\n\n        Usage\n        -----\n        lons = x.get_lon([degrees])\n\n        Returns\n        -------\n        lons : ndarray, shape (nlon)\n            1-D numpy array of size nlon containing the longitude of each row\n            of the gridded data.\n\n        Parameters\n        -------\n        degrees : bool, optional, default = True\n            If True, the output will be in degrees. If False, the output will\n            be in radians.\n        \"\"\"\n        if degrees is False:\n            return _np.radians(self._lons())\n        else:\n            return self._lons()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot3d(self, elevation=20, azimuth=30, cmap='RdBu_r', show=True,\n               fname=None):\n        \"\"\"\n        Plot the raw data on a 3d sphere.\n\n        This routines becomes slow for large grids because it is based on\n        matplotlib3d.\n\n        Usage\n        -----\n        x.plot3d([elevation, azimuth, show, fname])\n\n        Parameters\n        ----------\n        elevation : float, optional, default = 20\n            elev parameter for the 3d projection.\n        azimuth : float, optional, default = 30\n            azim parameter for the 3d projection.\n        cmap : str, optional, default = 'RdBu_r'\n            Name of the color map to use.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, save the image to the specified file.\n        \"\"\"\n        from mpl_toolkits.mplot3d import Axes3D\n\n        nlat, nlon = self.nlat, self.nlon\n        cmap = _plt.get_cmap(cmap)\n\n        if self.kind == 'real':\n            data = self.data\n        elif self.kind == 'complex':\n            data = _np.abs(self.data)\n        else:\n            raise ValueError('Grid has to be either real or complex, not {}'\n                             .format(self.kind))\n\n        lats = self.lats()\n        lons = self.lons()\n\n        if self.grid == 'DH':\n            # add south pole\n            lats_circular = _np.append(lats, [-90.])\n        elif self.grid == 'GLQ':\n            # add north and south pole\n            lats_circular = _np.hstack(([90.], lats, [-90.]))\n        lons_circular = _np.append(lons, [lons[0]])\n\n        nlats_circular = len(lats_circular)\n        nlons_circular = len(lons_circular)\n\n        sshape = nlats_circular, nlons_circular\n\n        # make uv sphere and store all points\n        u = _np.radians(lons_circular)\n        v = _np.radians(90. - lats_circular)\n\n        x = _np.sin(v)[:, None] * _np.cos(u)[None, :]\n        y = _np.sin(v)[:, None] * _np.sin(u)[None, :]\n        z = _np.cos(v)[:, None] * _np.ones_like(lons_circular)[None, :]\n\n        points = _np.vstack((x.flatten(), y.flatten(), z.flatten()))\n\n        # fill data for all points. 0 lon has to be repeated (circular mesh)\n        # and the south pole has to be added in the DH grid\n        if self.grid == 'DH':\n            magn_point = _np.zeros((nlat + 1, nlon + 1))\n            magn_point[:-1, :-1] = data\n            magn_point[-1, :] = _np.mean(data[-1])  # not exact !\n            magn_point[:-1, -1] = data[:, 0]\n        if self.grid == 'GLQ':\n            magn_point = _np.zeros((nlat + 2, nlon + 1))\n            magn_point[1:-1, :-1] = data\n            magn_point[0, :] = _np.mean(data[0])  # not exact !\n            magn_point[-1, :] = _np.mean(data[-1])  # not exact !\n            magn_point[1:-1, -1] = data[:, 0]\n\n        # compute face color, which is the average of all neighbour points\n        magn_face = 1./4. * (magn_point[1:, 1:] + magn_point[:-1, 1:] +\n                             magn_point[1:, :-1] + magn_point[:-1, :-1])\n\n        magnmax_face = _np.max(_np.abs(magn_face))\n        magnmax_point = _np.max(_np.abs(magn_point))\n\n        # compute colours and displace the points\n        norm = _plt.Normalize(-magnmax_face / 2., magnmax_face / 2., clip=True)\n        colors = cmap(norm(magn_face.flatten()))\n        colors = colors.reshape(nlats_circular - 1, nlons_circular - 1, 4)\n        points *= (1. + magn_point.flatten() / magnmax_point / 2.)\n        x = points[0].reshape(sshape)\n        y = points[1].reshape(sshape)\n        z = points[2].reshape(sshape)\n\n        # plot 3d radiation pattern\n        fig = _plt.figure()\n        ax3d = fig.add_subplot(1, 1, 1, projection='3d')\n\n        ax3d.plot_surface(x, y, z, rstride=1, cstride=1, facecolors=colors)\n        ax3d.set(xlim=(-1., 1.), ylim=(-1., 1.), zlim=(-1., 1.),\n                 xticks=[-1, 1], yticks=[-1, 1], zticks=[-1, 1])\n        ax3d.set_axis_off()\n        ax3d.view_init(elev=elevation, azim=azimuth)\n\n        # show or save output\n        fig.tight_layout(pad=0.5)\n        if show:\n            fig.show()\n\n        if fname is not None:\n            fig.savefig(fname)\n\n        return fig, ax3d", "response": "Plot the raw data on a 3d sphere."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self, tick_interval=[30, 30], minor_tick_interval=None,\n             ax=None, ax2=None, colorbar=False, cb_orientation='vertical',\n             cb_label=None, grid=False, axes_labelsize=None,\n             tick_labelsize=None, show=True, fname=None, **kwargs):\n        \"\"\"\n        Plot the raw data using a simple cylindrical projection.\n\n        Usage\n        -----\n        x.plot([tick_interval, minor_tick_interval, ax, ax2, colorbar,\n                cb_orientation, cb_label, grid, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        minor_tick_interval : list or tuple, optional, default = None\n            Intervals to use when plotting the minor x and y ticks. If set to\n            None, minor ticks will not be plotted.\n        xlabel : str, optional, default = 'Longitude' or 'GLQ longitude index'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'Latitude' or 'GLQ latitude index'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear. If the\n            grid is complex, the real component of the grid will be plotted\n            on this axes.\n        ax2 : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear. If the\n            grid is complex, the complex component of the grid will be plotted\n            on this axes.\n        colorbar : bool, optional, default = False\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar; either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = None\n            Text label for the colorbar.\n        grid : bool, optional, default = False\n            If True, plot major grid lines.\n        axes_labelsize : int, optional, default = None\n            The font size for the x and y axes labels.\n        tick_labelsize : int, optional, default = None\n            The font size for the x and y tick labels.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to plt.imshow(), such as cmap.\n        \"\"\"\n        if tick_interval is None:\n            xticks = []\n            yticks = []\n        elif self.grid == 'GLQ':\n            xticks = _np.linspace(0, self.nlon-1,\n                                  num=self.nlon//tick_interval[0]+1,\n                                  endpoint=True, dtype=int)\n            yticks = _np.linspace(0, self.nlat-1,\n                                  num=self.nlat//tick_interval[1]+1,\n                                  endpoint=True, dtype=int)\n        else:\n            xticks = _np.linspace(0, 360, num=360//tick_interval[0]+1,\n                                  endpoint=True)\n            yticks = _np.linspace(-90, 90, num=180//tick_interval[1]+1,\n                                  endpoint=True)\n\n        if axes_labelsize is None:\n            axes_labelsize = _mpl.rcParams['axes.labelsize']\n        if tick_labelsize is None:\n            tick_labelsize = _mpl.rcParams['xtick.labelsize']\n\n        if minor_tick_interval is None:\n            minor_xticks = []\n            minor_yticks = []\n        elif self.grid == 'GLQ':\n            minor_xticks = _np.linspace(\n                0, self.nlon-1, num=self.nlon//minor_tick_interval[0]+1,\n                endpoint=True, dtype=int)\n            minor_yticks = _np.linspace(\n                0, self.nlat-1, num=self.nlat//minor_tick_interval[1]+1,\n                endpoint=True, dtype=int)\n        else:\n            minor_xticks = _np.linspace(\n                0, 360, num=360//minor_tick_interval[0]+1, endpoint=True)\n            minor_yticks = _np.linspace(\n                -90, 90, num=180//minor_tick_interval[1]+1, endpoint=True)\n\n        if ax is None and ax2 is None:\n            fig, axes = self._plot(xticks=xticks, yticks=yticks,\n                                   minor_xticks=minor_xticks,\n                                   minor_yticks=minor_yticks,\n                                   colorbar=colorbar,\n                                   cb_orientation=cb_orientation,\n                                   cb_label=cb_label, grid=grid,\n                                   axes_labelsize=axes_labelsize,\n                                   tick_labelsize=tick_labelsize, **kwargs)\n        else:\n            if self.kind == 'complex':\n                if (ax is None and ax2 is not None) or (ax2 is None and\n                                                        ax is not None):\n                    raise ValueError('For complex grids, one must specify ' +\n                                     'both optional arguments axes and axes2.')\n            self._plot(xticks=xticks, yticks=yticks, minor_xticks=minor_xticks,\n                       minor_yticks=minor_yticks, ax=ax, ax2=ax2,\n                       colorbar=colorbar, cb_orientation=cb_orientation,\n                       cb_label=cb_label, grid=grid,\n                       axes_labelsize=axes_labelsize,\n                       tick_labelsize=tick_labelsize, **kwargs)\n\n        if ax is None:\n            fig.tight_layout(pad=0.5)\n            if show:\n                fig.show()\n\n            if fname is not None:\n                fig.savefig(fname)\n            return fig, axes", "response": "Plot the raw data of the entry in the base class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expand(self, normalization='4pi', csphase=1, **kwargs):\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. ' +\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be either 1 or -1. Input value was {:s}.\"\n                .format(repr(csphase))\n                )\n\n        return self._expand(normalization=normalization, csphase=csphase,\n                            **kwargs)", "response": "Expand the grid into spherical harmonics."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _lats(self):\n        lats = _np.linspace(90.0, -90.0 + 180.0 / self.nlat, num=self.nlat)\n        return lats", "response": "Return the latitudes of the gridded data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _lons(self):\n        lons = _np.linspace(0.0, 360.0 - 360.0 / self.nlon, num=self.nlon)\n        return lons", "response": "Return the longitudes of the gridded data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _expand(self, normalization, csphase, **kwargs):\n        if normalization.lower() == '4pi':\n            norm = 1\n        elif normalization.lower() == 'schmidt':\n            norm = 2\n        elif normalization.lower() == 'unnorm':\n            norm = 3\n        elif normalization.lower() == 'ortho':\n            norm = 4\n        else:\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \" +\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        cilm = _shtools.SHExpandDH(self.data, norm=norm, csphase=csphase,\n                                   sampling=self.sampling,\n                                   **kwargs)\n        coeffs = SHCoeffs.from_array(cilm,\n                                     normalization=normalization.lower(),\n                                     csphase=csphase, copy=False)\n        return coeffs", "response": "Expand the grid into real spherical harmonics."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the raw data using a simply cylindrical projection.", "response": "def _plot(self, xticks=[], yticks=[], minor_xticks=[], minor_yticks=[],\n              xlabel='Longitude', ylabel='Latitude', ax=None, ax2=None,\n              colorbar=None, cb_orientation=None, cb_label=None, grid=False,\n              axes_labelsize=None, tick_labelsize=None, **kwargs):\n        \"\"\"Plot the raw data using a simply cylindrical projection.\"\"\"\n\n        if ax is None:\n            if colorbar is True:\n                if cb_orientation == 'horizontal':\n                    scale = 0.67\n                else:\n                    scale = 0.5\n            else:\n                scale = 0.55\n            figsize = (_mpl.rcParams['figure.figsize'][0],\n                       _mpl.rcParams['figure.figsize'][0] * scale)\n            fig, axes = _plt.subplots(1, 1, figsize=figsize)\n        else:\n            axes = ax\n\n        deg = '$^{\\circ}$'\n        xticklabels = [str(int(y)) + deg for y in xticks]\n        yticklabels = [str(int(y)) + deg for y in yticks]\n\n        cim = axes.imshow(self.data, origin='upper',\n                          extent=(0., 360., -90., 90.), **kwargs)\n        axes.set(xticks=xticks, yticks=yticks)\n        axes.set_xlabel(xlabel, fontsize=axes_labelsize)\n        axes.set_ylabel(ylabel, fontsize=axes_labelsize)\n        axes.set_xticklabels(xticklabels, fontsize=tick_labelsize)\n        axes.set_yticklabels(yticklabels, fontsize=tick_labelsize)\n        axes.set_xticks(minor_xticks, minor=True)\n        axes.set_yticks(minor_yticks, minor=True)\n        axes.grid(grid, which='major')\n\n        if colorbar is True:\n            if cb_orientation == 'vertical':\n                divider = _make_axes_locatable(axes)\n                cax = divider.append_axes(\"right\", size=\"2.5%\", pad=0.15)\n                cbar = _plt.colorbar(cim, cax=cax, orientation=cb_orientation)\n            else:\n                divider = _make_axes_locatable(axes)\n                cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n                cbar = _plt.colorbar(cim, cax=cax,\n                                     orientation=cb_orientation)\n            if cb_label is not None:\n                cbar.set_label(cb_label, fontsize=axes_labelsize)\n            cbar.ax.tick_params(labelsize=tick_labelsize)\n\n        if ax is None:\n            return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the raw data using a simply cylindrical projection.", "response": "def _plot(self, xticks=[], yticks=[], minor_xticks=[], minor_yticks=[],\n              xlabel='Longitude', ylabel='Latitude', ax=None, ax2=None,\n              colorbar=None, cb_label=None, cb_orientation=None, grid=False,\n              axes_labelsize=None, tick_labelsize=None, **kwargs):\n        \"\"\"Plot the raw data using a simply cylindrical projection.\"\"\"\n        if ax is None:\n            if colorbar is True:\n                if cb_orientation == 'horizontal':\n                    scale = 1.5\n                else:\n                    scale = 1.1\n            else:\n                scale = 1.2\n            figsize = (_mpl.rcParams['figure.figsize'][0],\n                       _mpl.rcParams['figure.figsize'][0]*scale)\n            fig, axes = _plt.subplots(2, 1, figsize=figsize)\n            axreal = axes.flat[0]\n            axcomplex = axes.flat[1]\n        else:\n            axreal = ax\n            axcomplex = ax2\n\n        deg = '$^{\\circ}$'\n        xticklabels = [str(int(y)) + deg for y in xticks]\n        yticklabels = [str(int(y)) + deg for y in yticks]\n\n        cim1 = axreal.imshow(self.data.real, origin='upper',\n                             extent=(0., 360., -90., 90.), **kwargs)\n        axreal.set(title='Real component', xticks=xticks, yticks=yticks)\n        axreal.set_xlabel(xlabel, fontsize=axes_labelsize)\n        axreal.set_ylabel(ylabel, fontsize=axes_labelsize)\n        axreal.set_xticklabels(xticklabels, fontsize=tick_labelsize)\n        axreal.set_yticklabels(yticklabels, fontsize=tick_labelsize)\n        axreal.set_xticks(minor_xticks, minor=True)\n        axreal.set_yticks(minor_yticks, minor=True)\n        axreal.grid(grid, which='major')\n        cim2 = axcomplex.imshow(self.data.imag, origin='upper',\n                                extent=(0., 360., -90., 90.), **kwargs)\n        axcomplex.set(title='Imaginary component', xticks=xticks,\n                      yticks=yticks)\n        axcomplex.set_xlabel(xlabel, fontsize=axes_labelsize)\n        axcomplex.set_ylabel(ylabel, fontsize=axes_labelsize)\n        axcomplex.set_xticklabels(xticklabels, fontsize=tick_labelsize)\n        axcomplex.set_yticklabels(yticklabels, fontsize=tick_labelsize)\n        axcomplex.set_xticks(minor_xticks, minor=True)\n        axcomplex.set_yticks(minor_yticks, minor=True)\n        axcomplex.grid(grid, which='major')\n\n        if colorbar is True:\n            if cb_orientation == 'vertical':\n                divider1 = _make_axes_locatable(axreal)\n                cax1 = divider1.append_axes(\"right\", size=\"2.5%\", pad=0.05)\n                cbar1 = _plt.colorbar(cim1, cax=cax1,\n                                      orientation=cb_orientation)\n                divider2 = _make_axes_locatable(axcomplex)\n                cax2 = divider2.append_axes(\"right\", size=\"2.5%\", pad=0.05)\n                cbar2 = _plt.colorbar(cim2, cax=cax2,\n                                      orientation=cb_orientation)\n            else:\n                divider1 = _make_axes_locatable(axreal)\n                cax1 = divider1.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n                cbar1 = _plt.colorbar(cim1, cax=cax1,\n                                      orientation=cb_orientation)\n                divider2 = _make_axes_locatable(axcomplex)\n                cax2 = divider2.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n                cbar2 = _plt.colorbar(cim2, cax=cax2,\n                                      orientation=cb_orientation)\n\n            if cb_label is not None:\n                cbar1.set_label(cb_label, fontsize=axes_labelsize)\n                cbar2.set_label(cb_label, fontsize=axes_labelsize)\n            cbar1.ax.tick_params(labelsize=tick_labelsize)\n            cbar2.ax.tick_params(labelsize=tick_labelsize)\n\n        if ax is None:\n            return fig, axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lats(self):\n        lats = 90. - _np.arccos(self.zeros) * 180. / _np.pi\n        return lats", "response": "Return a vector containing the latitudes of each row\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the geoid. Usage ----- x.plot([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation, cb_label, show, fname, **kwargs]) Parameters ---------- tick_interval : list or tuple, optional, default = [30, 30] Intervals to use when plotting the x and y ticks. If set to None, ticks will not be plotted. xlabel : str, optional, default = 'longitude' Label for the longitude axis. ylabel : str, optional, default = 'latitude' Label for the latitude axis. ax : matplotlib axes object, optional, default = None A single matplotlib axes object where the plot will appear. colorbar : bool, optional, default = True If True, plot a colorbar. cb_orientation : str, optional, default = 'vertical' Orientation of the colorbar: either 'vertical' or 'horizontal'. cb_label : str, optional, default = 'geoid, m' Text label for the colorbar. show : bool, optional, default = True If True, plot the image to the screen. fname : str, optional, default = None If present, and if axes is not specified, save the image to the specified file. kwargs : optional Keyword arguements that will be sent to the SHGrid.plot() and plt.imshow() methods.", "response": "def plot(self, colorbar=True, cb_orientation='vertical',\n             cb_label='geoid, m', show=True, **kwargs):\n        \"\"\"\n        Plot the geoid.\n\n        Usage\n        -----\n        x.plot([tick_interval, xlabel, ylabel, ax, colorbar, cb_orientation,\n                cb_label, show, fname, **kwargs])\n\n        Parameters\n        ----------\n        tick_interval : list or tuple, optional, default = [30, 30]\n            Intervals to use when plotting the x and y ticks. If set to None,\n            ticks will not be plotted.\n        xlabel : str, optional, default = 'longitude'\n            Label for the longitude axis.\n        ylabel : str, optional, default = 'latitude'\n            Label for the latitude axis.\n        ax : matplotlib axes object, optional, default = None\n            A single matplotlib axes object where the plot will appear.\n        colorbar : bool, optional, default = True\n            If True, plot a colorbar.\n        cb_orientation : str, optional, default = 'vertical'\n            Orientation of the colorbar: either 'vertical' or 'horizontal'.\n        cb_label : str, optional, default = 'geoid, m'\n            Text label for the colorbar.\n        show : bool, optional, default = True\n            If True, plot the image to the screen.\n        fname : str, optional, default = None\n            If present, and if axes is not specified, save the image to the\n            specified file.\n        kwargs : optional\n            Keyword arguements that will be sent to the SHGrid.plot()\n            and plt.imshow() methods.\n        \"\"\"\n        return self.geoid.plot(colorbar=colorbar,\n                               cb_orientation=cb_orientation,\n                               cb_label=cb_label, show=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting YYYMMDD. DD date string or float to YYYY. YY.", "response": "def _yyyymmdd_to_year_fraction(date):\n    \"\"\"Convert YYYMMDD.DD date string or float to YYYY.YYY\"\"\"\n    date = str(date)\n    if '.' in date:\n        date, residual = str(date).split('.')\n        residual = float('0.' + residual)\n    else:\n        residual = 0.0\n\n    date = _datetime.datetime.strptime(date, '%Y%m%d')\n    date += _datetime.timedelta(days=residual)\n\n    year = date.year\n    year_start = _datetime.datetime(year=year, month=1, day=1)\n    next_year_start = _datetime.datetime(year=year + 1, month=1, day=1)\n    year_duration = next_year_start - year_start\n\n    year_elapsed = date - year_start\n    fraction = year_elapsed / year_duration\n\n    return year + fraction"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef example():\n    # --- input data filename ---\n    infile = os.path.join(os.path.dirname(__file__),\n                          '../../ExampleDataFiles/MarsTopo719.shape')\n    coeffs, lmax = shio.shread(infile)\n\n    # --- plot grid ---\n    grid = expand.MakeGridDH(coeffs, csphase=-1)\n    fig_map = plt.figure()\n    plt.imshow(grid)\n\n    # ---- compute spectrum ----\n    ls = np.arange(lmax + 1)\n    pspectrum = spectralanalysis.spectrum(coeffs, unit='per_l')\n    pdensity = spectralanalysis.spectrum(coeffs, unit='per_lm')\n\n    # ---- plot spectrum ----\n    fig_spectrum, ax = plt.subplots(1, 1)\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_xlabel('degree l')\n    ax.grid(True, which='both')\n\n    ax.plot(ls[1:], pspectrum[1:], label='power per degree l')\n    ax.plot(ls[1:], pdensity[1:], label='power per degree l and order m')\n\n    ax.legend()\n\n    fig_map.savefig('SHRtopography_mars.png')\n    fig_spectrum.savefig('SHRspectrum_mars.png')\n    print('mars topography and spectrum saved')", "response": "This example shows the power spectrum of Mars topography data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_zeros(self, lmax, gm, r0, omega=None, errors=False,\n                   normalization='4pi', csphase=1):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients set to zero\n        from degree 1 to lmax, and set the degree 0 term to 1.\n\n        Usage\n        -----\n        x = SHGravCoeffs.from_zeros(lmax, gm, r0, [omega, errors,\n                                                   normalization, csphase])\n\n        Returns\n        -------\n        x : SHGravCoeffs class instance.\n\n        Parameters\n        ----------\n        lmax : int\n            The maximum spherical harmonic degree l of the coefficients.\n        gm : float\n            The gravitational constant times the mass that is associated with\n            the gravitational potential coefficients.\n        r0 : float\n            The reference radius of the spherical harmonic coefficients.\n        omega : float, optional, default = None\n            The angular rotation rate of the body.\n        errors : bool, optional, default = False\n            If True, initialize the attribute errors with zeros.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n             coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        \"\"\"\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The normalization must be '4pi', 'ortho', 'schmidt', \"\n                \"or 'unnorm'. Input value was {:s}.\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be either 1 or -1. Input value was {:s}.\"\n                .format(repr(csphase))\n                )\n\n        if normalization.lower() == 'unnorm' and lmax > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \"\n                           \"are stable only for degrees less than or equal \"\n                           \"to 85. lmax for the coefficients will be set to \"\n                           \"85. Input value was {:d}.\".format(lmax),\n                           category=RuntimeWarning)\n            lmax = 85\n\n        coeffs = _np.zeros((2, lmax + 1, lmax + 1))\n        coeffs[0, 0, 0] = 1.0\n\n        if errors is False:\n            clm = SHGravRealCoeffs(coeffs, gm=gm, r0=r0, omega=omega,\n                                   normalization=normalization.lower(),\n                                   csphase=csphase)\n        else:\n            clm = SHGravRealCoeffs(coeffs, gm=gm, r0=r0, omega=omega,\n                                   errors=_np.zeros((2, lmax + 1, lmax + 1)),\n                                   normalization=normalization.lower(),\n                                   csphase=csphase)\n        return clm", "response": "Initialize the class instance with spherical harmonic coefficients set to zero and set the degree 0 term to 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize a new instance of the class from a file.", "response": "def from_file(self, fname, format='shtools', gm=None, r0=None,\n                  omega=None, lmax=None, normalization='4pi', skip=0,\n                  header=True, errors=False, csphase=1, r0_index=0, gm_index=1,\n                  omega_index=None, header_units='m', **kwargs):\n        \"\"\"\n        Initialize the class with spherical harmonic coefficients from a file.\n\n        Usage\n        -----\n        x = SHGravCoeffs.from_file(filename, [format='shtools', gm, r0, omega,\n                                              lmax, normalization, csphase,\n                                              skip, header, errors, gm_index,\n                                              r0_index, omega_index,\n                                              header_units])\n        x = SHGravCoeffs.from_file(filename, format='npy', gm, r0,\n                                   [omega, normalization, csphase, **kwargs])\n\n        Returns\n        -------\n        x : SHGravCoeffs class instance.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the file, including path.\n        format : str, optional, default = 'shtools'\n            'shtools' format or binary numpy 'npy' format.\n        lmax : int, optional, default = None\n            The maximum spherical harmonic degree to read from 'shtools'\n            formatted files.\n        header : bool, optional, default = True\n            If True, read a list of values from the header line of an 'shtools'\n            formatted file.\n        errors : bool, optional, default = False\n            If True, read errors from the file (for 'shtools' formatted files\n            only).\n        r0_index : int, optional, default = 0\n            For shtools formatted files, if header is True, r0 will be set\n            using the value from the header line with this index.\n        gm_index : int, optional, default = 1\n            For shtools formatted files, if header is True, gm will be set\n            using the value from the header line with this index.\n        omega_index : int, optional, default = None\n            For shtools formatted files, if header is True, omega will be set\n            using the value from the header line with this index.\n        gm : float, optional, default = None\n            The gravitational constant time the mass that is associated with\n            the gravitational potential coefficients.\n        r0 : float, optional, default = None\n            The reference radius of the spherical harmonic coefficients.\n        omega : float, optional, default = None\n            The angular rotation rate of the body.\n        header_units : str, optional, default = 'm'\n            The units used for r0 and gm in the header line of an shtools\n            formatted file: 'm' or 'km'. If 'km', the values of r0 and gm will\n            be converted to meters.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        skip : int, optional, default = 0\n            Number of lines to skip at the beginning of the file when format is\n            'shtools'.\n        **kwargs : keyword argument list, optional for format = 'npy'\n            Keyword arguments of numpy.load() when format is 'npy'.\n\n        Description\n        -----------\n        If format='shtools', spherical harmonic coefficients will be read from\n        a text file. The optional parameter `skip` specifies how many lines\n        should be skipped before attempting to parse the file, the optional\n        parameter `header` specifies whether to read a list of values from a\n        header line, and the optional parameter `lmax` specifies the maximum\n        degree to read from the file. If a header line is read, r0_index,\n        gm_index, and omega_index, are used as the indices to set r0, gm, and\n        omega. If header_unit is specified as 'km', the values of r0 and gm\n        that are read from the header will be converted to meters.\n\n        For shtools formatted files, all lines that do not start with 2\n        integers and that are less than 3 words long will be treated as\n        comments and ignored. For this format, each line of the file must\n        contain\n\n        l, m, coeffs[0, l, m], coeffs[1, l, m]\n\n        where l and m are the spherical harmonic degree and order,\n        respectively. The terms coeffs[1, l, 0] can be neglected as they are\n        zero. For more information, see `shio.shread()`. If errors are read,\n        each line must contain:\n\n        l, m, coeffs[0, l, m], coeffs[1, l, m], error[0, l, m], error[1, l, m]\n\n        If format='npy', a binary numpy 'npy' file will be read using\n        numpy.load().\n\n        If the degree 0 term of the file is zero (or not specified), this will\n        be set to 1.\n        \"\"\"\n        error = None\n\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. '\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The input normalization must be '4pi', 'ortho', 'schmidt', \"\n                \"or 'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        if header_units.lower() not in ('m', 'km'):\n            raise ValueError(\"header_units can be only 'm', or 'km'. Input \"\n                             \"value is {:s}.\".format(repr(header_units)))\n\n        if format == 'shtools':\n            if r0_index is not None and r0 is not None:\n                raise ValueError('Can not specify both r0_index and r0')\n            if gm_index is not None and gm is not None:\n                raise ValueError('Can not specify both gm_index and gm')\n            if omega_index is not None and omega is not None:\n                raise ValueError('Can not specify both omega_index and omega')\n            if header is False and (r0 is None or gm is None):\n                raise ValueError('If header is False, r0 and gm must be '\n                                 'specified.')\n\n        header_list = None\n        if format.lower() == 'shtools':\n            if header is True:\n                if errors is True:\n                    coeffs, error, lmaxout, header_list = _shread(\n                        fname, lmax=lmax, skip=skip, header=True, error=True)\n                else:\n                    coeffs, lmaxout, header_list = _shread(\n                        fname, lmax=lmax, skip=skip, header=True)\n            else:\n                if errors is True:\n                    coeffs, error, lmaxout = _shread(\n                        fname, lmax=lmax, error=True, skip=skip)\n                else:\n                    coeffs, lmaxout = _shread(fname, lmax=lmax, skip=skip)\n\n        elif format.lower() == 'npy':\n            if gm is None or r0 is None:\n                raise ValueError('For binary npy files, gm and r0 must be '\n                                 'specified.')\n            coeffs = _np.load(fname, **kwargs)\n            lmaxout = coeffs.shape[1] - 1\n        else:\n            raise NotImplementedError(\n                'format={:s} not implemented'.format(repr(format)))\n\n        if _np.iscomplexobj(coeffs):\n            raise TypeError('The input coefficients must be real.')\n\n        if normalization.lower() == 'unnorm' and lmaxout > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \"\n                           \"are stable only for degrees less than or equal \"\n                           \"to 85. lmax for the coefficients will be set to \"\n                           \"85. Input value was {:d}.\".format(lmaxout),\n                           category=RuntimeWarning)\n            lmaxout = 85\n\n        if coeffs[0, 0, 0] == 0:\n            warnstr = (\"The degree 0 term of the file was not set. \"\n                       \"This will be set to 1.\")\n            _warnings.warn(warnstr, category=RuntimeWarning)\n            coeffs[0, 0, 0] = 1.0\n\n        if format.lower() == 'shtools' and header is True:\n            if r0_index is not None:\n                r0 = float(header_list[r0_index])\n            if gm_index is not None:\n                gm = float(header_list[gm_index])\n            if omega_index is not None:\n                omega = float(header_list[omega_index])\n            if header_units.lower() == 'km':\n                r0 *= 1.e3\n                gm *= 1.e9\n\n        clm = SHGravRealCoeffs(coeffs, gm=gm, r0=r0, omega=omega,\n                               errors=error,\n                               normalization=normalization.lower(),\n                               csphase=csphase, header=header_list)\n        return clm"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the gravitational potential spherical harmonic coefficients with a given reference radius and angular rotation rate.", "response": "def from_random(self, power, gm, r0, omega=None, function='geoid',\n                    lmax=None, normalization='4pi', csphase=1,\n                    exact_power=False):\n        \"\"\"\n        Initialize the class of gravitational potential spherical harmonic\n        coefficients as random variables with a given spectrum.\n\n        Usage\n        -----\n        x = SHGravCoeffs.from_random(power, gm, r0, [omega, function, lmax,\n                                                     normalization,\n                                                     csphase, exact_power])\n\n        Returns\n        -------\n        x : SHGravCoeffs class instance.\n\n        Parameters\n        ----------\n        power : ndarray, shape (L+1)\n            numpy array of shape (L+1) that specifies the expected power per\n            degree l, where L is the maximum spherical harmonic bandwidth.\n        gm : float\n            The gravitational constant times the mass that is associated with\n            the gravitational potential coefficients.\n        r0 : float\n            The reference radius of the spherical harmonic coefficients.\n        omega : float, optional, default = None\n            The angular rotation rate of the body.\n        function : str, optional, default = 'geoid'\n            The type of input power spectrum: 'potential' for the gravitational\n            potential, 'geoid' for the geoid, 'radial' for the radial gravity,\n            or 'total' for the total gravity field.\n        lmax : int, optional, default = len(power) - 1\n            The maximum spherical harmonic degree l of the output coefficients.\n            The coefficients will be set to zero for degrees greater than L.\n        normalization : str, optional, default = '4pi'\n            '4pi', 'ortho', 'schmidt', or 'unnorm' for geodesy 4pi normalized,\n            orthonormalized, Schmidt semi-normalized, or unnormalized\n            coefficients, respectively.\n        csphase : int, optional, default = 1\n            Condon-Shortley phase convention: 1 to exclude the phase factor,\n            or -1 to include it.\n        exact_power : bool, optional, default = False\n            The total variance of the coefficients is set exactly to the input\n            power. The distribution of power at degree l amongst the angular\n            orders is random, but the total power is fixed.\n\n        Description\n        -----------\n        This routine returns a random realization of spherical harmonic\n        gravitational potential coefficients obtained from a normal\n        distribution. The variance of each coefficient at degree l is equal to\n        the total power at degree l divided by the number of coefficients at\n        that degree (2l+1). These coefficients are then divided by a prefactor\n        that depends upon the function being used to calculate the spectrum:\n        gm/r0 for the gravitiational potential, r0 for the geoid, and\n        (l+1)*gm/(r**2) for the radial gravity. The power spectrum of the\n        random realization can be fixed exactly to the input spectrum by\n        setting exact_power to True.\n\n        Note that the degree 0 term is set to 1, and the degree-1 terms are\n        set to 0.\n        \"\"\"\n        if type(normalization) != str:\n            raise ValueError('normalization must be a string. '\n                             'Input type was {:s}'\n                             .format(str(type(normalization))))\n\n        if function.lower() not in ('potential', 'geoid', 'radial', 'total'):\n            raise ValueError(\n                \"function must be of type 'potential', \"\n                \"'geoid', 'radial', or 'total'. Provided value was {:s}\"\n                .format(repr(function))\n                )\n\n        if normalization.lower() not in ('4pi', 'ortho', 'schmidt', 'unnorm'):\n            raise ValueError(\n                \"The input normalization must be '4pi', 'ortho', 'schmidt', \"\n                \"or 'unnorm'. Provided value was {:s}\"\n                .format(repr(normalization))\n                )\n\n        if csphase != 1 and csphase != -1:\n            raise ValueError(\n                \"csphase must be 1 or -1. Input value was {:s}\"\n                .format(repr(csphase))\n                )\n\n        if lmax is None:\n            nl = len(power)\n            lmax = nl - 1\n        else:\n            if lmax <= len(power) - 1:\n                nl = lmax + 1\n            else:\n                nl = len(power)\n        degrees = _np.arange(nl)\n\n        if normalization.lower() == 'unnorm' and nl - 1 > 85:\n            _warnings.warn(\"Calculations using unnormalized coefficients \"\n                           \"are stable only for degrees less than or equal \"\n                           \"to 85. lmax for the coefficients will be set to \"\n                           \"85. Input value was {:d}.\".format(nl-1),\n                           category=RuntimeWarning)\n            nl = 85 + 1\n            lmax = 85\n\n        # Create coefficients with unit variance, which returns an expected\n        # total power per degree of (2l+1) for 4pi normalized harmonics.\n        coeffs = _np.empty((2, nl, nl))\n        for l in degrees:\n            coeffs[:2, l, :l+1] = _np.random.normal(size=(2, l+1))\n\n        if exact_power:\n            power_per_l = _spectrum(coeffs, normalization='4pi', unit='per_l')\n            coeffs *= _np.sqrt(\n                power[0:nl] / power_per_l)[_np.newaxis, :, _np.newaxis]\n        else:\n            coeffs *= _np.sqrt(\n                power[0:nl] / (2 * degrees + 1))[_np.newaxis, :, _np.newaxis]\n\n        if normalization.lower() == '4pi':\n            pass\n        elif normalization.lower() == 'ortho':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='ortho')\n        elif normalization.lower() == 'schmidt':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='schmidt')\n        elif normalization.lower() == 'unnorm':\n            coeffs = _convert(coeffs, normalization_in='4pi',\n                              normalization_out='unnorm')\n\n        if function.lower() == 'potential':\n            coeffs /= (gm / r0)\n        elif function.lower() == 'geoid':\n            coeffs /= r0\n        elif function.lower() == 'radial':\n            for l in degrees:\n                coeffs[:, l, :l+1] /= (gm * (l + 1) / r0**2)\n        elif function.lower() == 'total':\n            for l in degrees:\n                coeffs[:, l, :l+1] /= (gm / r0**2) * _np.sqrt((l + 1) *\n                                                              (2 * l + 1))\n\n        if lmax > nl - 1:\n            coeffs = _np.pad(coeffs, ((0, 0), (0, lmax - nl + 1),\n                             (0, lmax - nl + 1)), 'constant')\n\n        coeffs[0, 0, 0] = 1.0\n        coeffs[:, 1, :] = 0.0\n\n        clm = SHGravRealCoeffs(coeffs, gm=gm, r0=r0, omega=omega,\n                               normalization=normalization.lower(),\n                               csphase=csphase)\n        return clm"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes a new instance of the class of gravitational potential spherical harmonic coefficients with the given shape and mass.", "response": "def from_shape(self, shape, rho, gm, nmax=7, lmax=None, lmax_grid=None,\n                   lmax_calc=None, omega=None):\n        \"\"\"\n        Initialize a class of gravitational potential spherical harmonic\n        coefficients by calculuting the gravitational potential associatiated\n        with relief along an interface.\n\n        Usage\n        -----\n        x = SHGravCoeffs.from_shape(shape, rho, gm, [nmax, lmax, lmax_grid,\n                                                     lmax_calc, omega])\n\n        Returns\n        -------\n        x : SHGravCoeffs class instance.\n\n        Parameters\n        ----------\n        shape : SHGrid or SHCoeffs class instance\n            The shape of the interface, either as an SHGrid or SHCoeffs class\n            instance. If the input is an SHCoeffs class instance, this will be\n            expaned on a grid using the optional parameters lmax_grid and\n            lmax_calc.\n        rho : int, float, or ndarray, or an SHGrid or SHCoeffs class instance\n            The density contrast associated with the interface in kg / m3. If\n            the input is a scalar, the density contrast is constant. If\n            the input is an SHCoeffs or SHGrid class instance, the density\n            contrast will vary laterally.\n        gm : float\n            The gravitational constant times the mass that is associated with\n            the gravitational potential coefficients.\n        nmax : integer, optional, default = 7\n             The maximum order used in the Taylor-series expansion when\n             calculating the potential coefficients.\n        lmax : int, optional, shape.lmax\n            The maximum spherical harmonic degree of the output spherical\n            harmonic coefficients.\n        lmax_grid : int, optional, default = lmax\n            If shape or rho is of type SHCoeffs, this parameter determines the\n            maximum spherical harmonic degree that is resolvable when expanded\n            onto a grid.\n        lmax_calc : optional, integer, default = lmax\n            If shape or rho is of type SHCoeffs, this parameter determines the\n            maximum spherical harmonic degree that will be used when expanded\n            onto a grid.\n        omega : float, optional, default = None\n            The angular rotation rate of the body.\n\n        Description\n        -----------\n        Initialize an SHGravCoeffs class instance by calculating the spherical\n        harmonic coefficients of the gravitational potential associated with\n        the shape of a density interface. The potential is calculated using the\n        finite-amplitude technique of Wieczorek and Phillips (1998) for a\n        constant density contrast and Wieczorek (2007) for a density contrast\n        that varies laterally. The output coefficients are referenced to the\n        mean radius of shape, and the potential is strictly valid only when it\n        is evaluated at a radius greater than the maximum radius of shape.\n\n        The input shape (and density contrast rho for variable density) can be\n        either an SHGrid or SHCoeffs class instance. The routine makes direct\n        use of gridded versions of these quantities, so if the input is of type\n        SHCoeffs, it will first be expanded onto a grid. This exansion will be\n        performed on a grid that can resolve degrees up to lmax_grid, with only\n        the first lmax_calc coefficients being used. The input shape must\n        correspond to absolute radii as the degree 0 term determines the\n        reference radius of the coefficients.\n\n        As an intermediate step, this routine calculates the spherical harmonic\n        coefficients of the interface raised to the nth power, i.e.,\n        (shape-r0)**n, where r0 is the mean radius of shape. If the input shape\n        is bandlimited to degree L, the resulting function will thus be\n        bandlimited to degree L*nmax. This subroutine assumes implicitly that\n        the maximum spherical harmonic degree of the input shape (when\n        SHCoeffs) or maximum resolvable spherical harmonic degree of shape\n        (when SHGrid) is greater or equal to this value. If this is not the\n        case, aliasing will occur. In practice, for accurate results, the\n        effective bandwidth needs only to be about three times the size of L,\n        though this should be verified for each application. The effective\n        bandwidth of shape (when SHCoeffs) can be increased by preprocessing\n        with the method pad(), or by increaesing the value of lmax_grid (when\n        SHGrid).\n        \"\"\"\n        mass = gm / _G.value\n\n        if type(shape) is not _SHRealCoeffs and type(shape) is not _DHRealGrid:\n            raise ValueError('shape must be of type SHRealCoeffs '\n                             'or DHRealGrid. Input type is {:s}'\n                             .format(repr(type(shape))))\n\n        if (not issubclass(type(rho), float) and type(rho) is not int\n                and type(rho) is not _np.ndarray and\n                type(rho) is not _SHRealCoeffs and\n                type(rho is not _DHRealGrid)):\n            raise ValueError('rho must be of type float, int, ndarray, '\n                             'SHRealCoeffs or DHRealGrid. Input type is {:s}'\n                             .format(repr(type(rho))))\n\n        if type(shape) is _SHRealCoeffs:\n            shape = shape.expand(lmax=lmax_grid, lmax_calc=lmax_calc)\n\n        if type(rho) is _SHRealCoeffs:\n            rho = rho.expand(lmax=lmax_grid, lmax_calc=lmax_calc)\n\n        if type(rho) is _DHRealGrid:\n            if shape.lmax != rho.lmax:\n                raise ValueError('The grids for shape and rho must have the '\n                                 'same size. '\n                                 'lmax of shape = {:d}, lmax of rho = {:d}'\n                                 .format(shape.lmax, rho.lmax))\n            cilm, d = _CilmPlusRhoHDH(shape.data, nmax, mass, rho.data,\n                                      lmax=lmax)\n\n        else:\n            cilm, d = _CilmPlusDH(shape.data, nmax, mass, rho, lmax=lmax)\n\n        clm = SHGravRealCoeffs(cilm, gm=gm, r0=d, omega=omega,\n                               normalization='4pi', csphase=1)\n        return clm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_file(self, filename, format='shtools', header=None, errors=False,\n                **kwargs):\n        \"\"\"\n        Save spherical harmonic coefficients to a file.\n\n        Usage\n        -----\n        x.to_file(filename, [format='shtools', header, errors])\n        x.to_file(filename, [format='npy', **kwargs])\n\n        Parameters\n        ----------\n        filename : str\n            Name of the output file.\n        format : str, optional, default = 'shtools'\n            'shtools' or 'npy'. See method from_file() for more information.\n        header : str, optional, default = None\n            A header string written to an 'shtools'-formatted file directly\n            before the spherical harmonic coefficients.\n        errors : bool, optional, default = False\n            If True, save the errors in the file (for 'shtools' formatted\n            files only).\n        **kwargs : keyword argument list, optional for format = 'npy'\n            Keyword arguments of numpy.save().\n\n        Description\n        -----------\n        If format='shtools', the coefficients and meta-data will be written to\n        an ascii formatted file. The first line is an optional user provided\n        header line, and the following line provides the attributes r0, gm,\n        omega, and lmax. The spherical harmonic coefficients are then listed,\n        with increasing degree and order, with the format\n\n        l, m, coeffs[0, l, m], coeffs[1, l, m]\n\n        where l and m are the spherical harmonic degree and order,\n        respectively. If the errors are to be saved, the format of each line\n        will be\n\n        l, m, coeffs[0, l, m], coeffs[1, l, m], error[0, l, m], error[1, l, m]\n\n        If format='npy', the spherical harmonic coefficients (but not the\n        meta-data nor errors) will be saved to a binary numpy 'npy' file using\n        numpy.save().\n        \"\"\"\n        if format is 'shtools':\n            if errors is True and self.errors is None:\n                raise ValueError('Can not save errors when then have not been '\n                                 'initialized.')\n\n            if self.omega is None:\n                omega = 0.\n            else:\n                omega = self.omega\n\n            with open(filename, mode='w') as file:\n                if header is not None:\n                    file.write(header + '\\n')\n                file.write('{:.16e}, {:.16e}, {:.16e}, {:d}\\n'.format(\n                    self.r0, self.gm, omega, self.lmax))\n                for l in range(self.lmax+1):\n                    for m in range(l+1):\n                        if errors is True:\n                            file.write('{:d}, {:d}, {:.16e}, {:.16e}, '\n                                       '{:.16e}, {:.16e}\\n'\n                                       .format(l, m, self.coeffs[0, l, m],\n                                               self.coeffs[1, l, m],\n                                               self.errors[0, l, m],\n                                               self.errors[1, l, m]))\n                        else:\n                            file.write('{:d}, {:d}, {:.16e}, {:.16e}\\n'\n                                       .format(l, m, self.coeffs[0, l, m],\n                                               self.coeffs[1, l, m]))\n        elif format is 'npy':\n            _np.save(filename, self.coeffs, **kwargs)\n        else:\n            raise NotImplementedError(\n                'format={:s} not implemented'.format(repr(format)))", "response": "Save the spherical harmonic coefficients to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the power spectrum of the class instance.", "response": "def spectrum(self, function='geoid', lmax=None, unit='per_l', base=10.):\n        \"\"\"\n        Return the spectrum as a function of spherical harmonic degree.\n\n        Usage\n        -----\n        spectrum, [error_spectrum] = x.spectrum([function, lmax, unit, base])\n\n        Returns\n        -------\n        spectrum : ndarray, shape (lmax+1)\n            1-D numpy ndarray of the spectrum, where lmax is the maximum\n            spherical harmonic degree.\n        error_spectrum : ndarray, shape (lmax+1)\n            1-D numpy ndarray of the error_spectrum (if the attribute errors\n            is not None).\n\n        Parameters\n        ----------\n        function : str, optional, default = 'geoid'\n            The type of power spectrum to return: 'potential' for the\n            gravitational potential in m2/s2, 'geoid' for the geoid in m,\n            'radial' for the radial gravity in m/s2, or 'total' for the total\n            gravitational field in m/s2.\n        lmax : int, optional, default = x.lmax\n            Maximum spherical harmonic degree of the spectrum to return.\n        unit : str, optional, default = 'per_l'\n            If 'per_l', return the total contribution to the spectrum for each\n            spherical harmonic degree l. If 'per_lm', return the average\n            contribution to the spectrum for each coefficient at spherical\n            harmonic degree l. If 'per_dlogl', return the spectrum per log\n            interval dlog_a(l).\n        base : float, optional, default = 10.\n            The logarithm base when calculating the 'per_dlogl' spectrum.\n\n        Description\n        -----------\n        This method returns the power spectrum of the class instance, where the\n        type of function is defined by the function parameter: 'potential' for\n        the gravitational potential, 'geoid' for the geoid, 'radial' for\n        the radial gravity, or 'total' for the total gravitational field. In\n        all cases, the total power of the function is defined as the integral\n        of the function squared over all space, divided by the area the\n        function spans. If the mean of the function is zero, this is equivalent\n        to the variance of the function.\n\n        The output spectrum can be expresed using one of three units. 'per_l'\n        returns the contribution to the total spectrum from all angular orders\n        at degree l. 'per_lm' returns the average contribution to the total\n        spectrum from a single coefficient at degree l, which is equal to the\n        'per_l' spectrum divided by (2l+1). 'per_dlogl' returns the\n        contribution to the total spectrum from all angular orders over an\n        infinitessimal logarithmic degree band. The contrubution in the band\n        dlog_a(l) is spectrum(l, 'per_dlogl')*dlog_a(l), where a is the base,\n        and where spectrum(l, 'per_dlogl) is equal to\n        spectrum(l, 'per_l')*l*log(a).\n        \"\"\"\n        if function.lower() not in ('potential', 'geoid', 'radial', 'total'):\n            raise ValueError(\n                \"function must be of type 'potential', 'geoid', 'radial', or \"\n                \"'total'. Provided value was {:s}\".format(repr(function))\n                )\n\n        s = _spectrum(self.coeffs, normalization=self.normalization,\n                      convention='power', unit=unit, base=base, lmax=lmax)\n\n        if self.errors is not None:\n            es = _spectrum(self.errors, normalization=self.normalization,\n                           convention='power', unit=unit, base=base, lmax=lmax)\n\n        if function.lower() == 'potential':\n            s *= (self.gm / self.r0)**2\n            if self.errors is not None:\n                es *= (self.gm / self.r0)**2\n        elif function.lower() == 'geoid':\n            s *= self.r0**2\n            if self.errors is not None:\n                es *= self.r0**2\n        elif function.lower() == 'radial':\n            degrees = _np.arange(len(s))\n            s *= (self.gm * (degrees + 1) / self.r0**2)**2\n            if self.errors is not None:\n                es *= (self.gm * (degrees + 1) / self.r0**2)**2\n        elif function.lower() == 'total':\n            degrees = _np.arange(len(s))\n            s *= (self.gm / self.r0**2)**2 * (degrees + 1) * (2 * degrees + 1)\n            if self.errors is not None:\n                es *= (self.gm / self.r0**2)**2 * (degrees + 1) * \\\n                    (2 * degrees + 1)\n\n        if self.errors is not None:\n            return s, es\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_ref(self, gm=None, r0=None, lmax=None):\n        if lmax is None:\n            lmax = self.lmax\n\n        clm = self.pad(lmax)\n\n        if gm is not None and gm != self.gm:\n            clm.coeffs *= self.gm / gm\n            clm.gm = gm\n            if self.errors is not None:\n                clm.errors *= self.gm / gm\n\n        if r0 is not None and r0 != self.r0:\n            for l in _np.arange(lmax+1):\n                clm.coeffs[:, l, :l+1] *= (self.r0 / r0)**l\n                if self.errors is not None:\n                    clm.errors[:, l, :l+1] *= (self.r0 / r0)**l\n            clm.r0 = r0\n\n        return clm", "response": "Returns a new class instance with a different reference gm or r0 or lmax."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate 2D cylindrical maps on a flattened and rotating ellipsoid of all three components of the gravity field, the gravity disturbance, and the gravitational potential, and return as a SHGravGrid class instance. Usage ----- grav = x.expand([a, f, lmax, lmax_calc, normal_gravity, sampling]) Returns ------- grav : SHGravGrid class instance. Parameters ---------- a : optional, float, default = self.r0 The semi-major axis of the flattened ellipsoid on which the field is computed. f : optional, float, default = 0 The flattening of the reference ellipsoid: f=(a-b)/a. lmax : optional, integer, default = self.lmax The maximum spherical harmonic degree, which determines the number of samples of the output grids, n=2lmax+2, and the latitudinal sampling interval, 90/(lmax+1). lmax_calc : optional, integer, default = lmax The maximum spherical harmonic degree used in evaluating the functions. This must be less than or equal to lmax. normal_gravity : optional, bool, default = True If True, the normal gravity (the gravitational acceleration on the ellipsoid) will be subtracted from the total gravity, yielding the \"gravity disturbance.\" This is done using Somigliana's formula (after converting geocentric to geodetic coordinates). sampling : optional, integer, default = 2 If 1 the output grids are equally sampled (n by n). If 2 (default), the grids are equally spaced in degrees (n by 2n). Description ----------- This method will create 2-dimensional cylindrical maps of the three components of the gravity field, the total field, and the gravitational potential, and return these as an SHGravGrid class instance. Each map is stored as an SHGrid class instance using Driscoll and Healy grids that are either equally sampled (n by n) or equally spaced (n by 2n) in latitude and longitude. All grids use geocentric coordinates, the output is in SI units, and the sign of the radial components is positive when directed upwards. If the optional angular rotation rate omega is specified in the SHGravCoeffs instance, the potential and radial gravitational acceleration will be calculated in a body-fixed rotating reference frame. If normal_gravity is set to True, the normal gravity will be removed from the total field, yielding the gravity disturbance. The gravitational potential is given by V = GM/r Sum_{l=0}^lmax (r0/r)^l Sum_{m=-l}^l C_{lm} Y_{lm}, and the gravitational acceleration is B = Grad V. The coefficients are referenced to the radius r0, and the function is computed on a flattened ellipsoid with semi-major axis a (i.e., the mean equatorial radius) and flattening f. To convert m/s^2 to mGals, multiply the gravity grids by 10^5.", "response": "def expand(self, a=None, f=None, lmax=None, lmax_calc=None,\n               normal_gravity=True, sampling=2):\n        \"\"\"\n        Create 2D cylindrical maps on a flattened and rotating ellipsoid of all\n        three components of the gravity field, the gravity disturbance, and the\n        gravitational potential, and return as a SHGravGrid class instance.\n\n        Usage\n        -----\n        grav = x.expand([a, f, lmax, lmax_calc, normal_gravity, sampling])\n\n        Returns\n        -------\n        grav : SHGravGrid class instance.\n\n        Parameters\n        ----------\n        a : optional, float, default = self.r0\n            The semi-major axis of the flattened ellipsoid on which the field\n            is computed.\n        f : optional, float, default = 0\n            The flattening of the reference ellipsoid: f=(a-b)/a.\n        lmax : optional, integer, default = self.lmax\n            The maximum spherical harmonic degree, which determines the number\n            of samples of the output grids, n=2lmax+2, and the latitudinal\n            sampling interval, 90/(lmax+1).\n        lmax_calc : optional, integer, default = lmax\n            The maximum spherical harmonic degree used in evaluating the\n            functions. This must be less than or equal to lmax.\n        normal_gravity : optional, bool, default = True\n            If True, the normal gravity (the gravitational acceleration on the\n            ellipsoid) will be subtracted from the total gravity, yielding the\n            \"gravity disturbance.\" This is done using Somigliana's formula\n            (after converting geocentric to geodetic coordinates).\n        sampling : optional, integer, default = 2\n            If 1 the output grids are equally sampled (n by n). If 2 (default),\n            the grids are equally spaced in degrees (n by 2n).\n\n        Description\n        -----------\n        This method will create 2-dimensional cylindrical maps of the three\n        components of the gravity field, the total field, and the gravitational\n        potential, and return these as an SHGravGrid class instance. Each\n        map is stored as an SHGrid class instance using Driscoll and Healy\n        grids that are either equally sampled (n by n) or equally spaced\n        (n by 2n) in latitude and longitude. All grids use geocentric\n        coordinates, the output is in SI units, and the sign of the radial\n        components is positive when directed upwards. If the optional angular\n        rotation rate omega is specified in the SHGravCoeffs instance, the\n        potential and radial gravitational acceleration will be calculated in a\n        body-fixed rotating reference frame. If normal_gravity is set to True,\n        the normal gravity will be removed from the total field, yielding the\n        gravity disturbance.\n\n        The gravitational potential is given by\n\n            V = GM/r Sum_{l=0}^lmax (r0/r)^l Sum_{m=-l}^l C_{lm} Y_{lm},\n\n        and the gravitational acceleration is\n\n            B = Grad V.\n\n        The coefficients are referenced to the radius r0, and the function is\n        computed on a flattened ellipsoid with semi-major axis a (i.e., the\n        mean equatorial radius) and flattening f. To convert m/s^2 to mGals,\n        multiply the gravity grids by 10^5.\n        \"\"\"\n        if a is None:\n            a = self.r0\n        if f is None:\n            f = 0.\n        if normal_gravity is True:\n            ng = 1\n        else:\n            ng = 0\n        if lmax is None:\n            lmax = self.lmax\n        if lmax_calc is None:\n            lmax_calc = lmax\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization='4pi', csphase=1)\n        else:\n            coeffs = self.to_array(normalization='4pi', csphase=1)\n\n        rad, theta, phi, total, pot = _MakeGravGridDH(\n            coeffs, self.gm, self.r0, a=a, f=f, lmax=lmax,\n            lmax_calc=lmax_calc, sampling=sampling, omega=self.omega,\n            normal_gravity=ng)\n\n        return _SHGravGrid(rad, theta, phi, total, pot, self.gm, a, f,\n                           self.omega, normal_gravity, lmax, lmax_calc)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tensor(self, a=None, f=None, lmax=None, lmax_calc=None, degree0=False,\n               sampling=2):\n        \"\"\"\n        Create 2D cylindrical maps on a flattened ellipsoid of the 9\n        components of the gravity \"gradient\" tensor in a local north-oriented\n        reference frame, and return an SHGravTensor class instance.\n\n        Usage\n        -----\n        tensor = x.tensor([a, f, lmax, lmax_calc, sampling])\n\n        Returns\n        -------\n        tensor : SHGravTensor class instance.\n\n        Parameters\n        ----------\n        a : optional, float, default = self.r0\n            The semi-major axis of the flattened ellipsoid on which the field\n            is computed.\n        f : optional, float, default = 0\n            The flattening of the reference ellipsoid: f=(a-b)/a.\n        lmax : optional, integer, default = self.lmax\n            The maximum spherical harmonic degree that determines the number of\n            samples of the output grids, n=2lmax+2, and the latitudinal\n            sampling interval, 90/(lmax+1).\n        lmax_calc : optional, integer, default = lmax\n            The maximum spherical harmonic degree used in evaluating the\n            functions. This must be less than or equal to lmax.\n        degree0 : optional, default = False\n            If True, include the degree-0 term when calculating the tensor. If\n            False, set the degree-0 term to zero.\n        sampling : optional, integer, default = 2\n            If 1 the output grids are equally sampled (n by n). If 2 (default),\n            the grids are equally spaced in degrees (n by 2n).\n\n        Description\n        -----------\n        This method will create 2-dimensional cylindrical maps for the 9\n        components of the gravity 'gradient' tensor and return an SHGravTensor\n        class instance. The components are\n\n            (Vxx, Vxy, Vxz)\n            (Vyx, Vyy, Vyz)\n            (Vzx, Vzy, Vzz)\n\n        where the reference frame is north-oriented, where x points north, y\n        points west, and z points upward (all tangent or perpendicular to a\n        sphere of radius r, where r is the local radius of the flattened\n        ellipsoid). The gravitational potential is defined as\n\n            V = GM/r Sum_{l=0}^lmax (r0/r)^l Sum_{m=-l}^l C_{lm} Y_{lm},\n\n        where r0 is the reference radius of the spherical harmonic coefficients\n        Clm, and the gravitational acceleration is\n\n            B = Grad V.\n\n        The components of the gravity tensor are calculated according to eq. 1\n        in Petrovskaya and Vershkov (2006), which is based on eq. 3.28 in Reed\n        (1973) (noting that Reed's equations are in terms of latitude and that\n        the y axis points east):\n\n            Vzz = Vrr\n            Vxx = 1/r Vr + 1/r^2 Vtt\n            Vyy = 1/r Vr + 1/r^2 /tan(t) Vt + 1/r^2 /sin(t)^2 Vpp\n            Vxy = 1/r^2 /sin(t) Vtp - cos(t)/sin(t)^2 /r^2 Vp\n            Vxz = 1/r^2 Vt - 1/r Vrt\n            Vyz = 1/r^2 /sin(t) Vp - 1/r /sin(t) Vrp\n\n        where r, t, p stand for radius, theta, and phi, respectively, and\n        subscripts on V denote partial derivatives. The output grids are in\n        units of Eotvos (10^-9 s^-2).\n\n        References\n        ----------\n        Reed, G.B., Application of kinematical geodesy for determining\n        the short wave length components of the gravity field by satellite\n        gradiometry, Ohio State University, Dept. of Geod. Sciences, Rep. No.\n        201, Columbus, Ohio, 1973.\n\n        Petrovskaya, M.S. and A.N. Vershkov, Non-singular expressions for the\n        gravity gradients in the local north-oriented and orbital reference\n        frames, J. Geod., 80, 117-127, 2006.\n        \"\"\"\n        if a is None:\n            a = self.r0\n        if f is None:\n            f = 0.\n        if lmax is None:\n            lmax = self.lmax\n        if lmax_calc is None:\n            lmax_calc = lmax\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization='4pi', csphase=1)\n        else:\n            coeffs = self.to_array(normalization='4pi', csphase=1)\n\n        if degree0 is False:\n            coeffs[0, 0, 0] = 0.\n\n        vxx, vyy, vzz, vxy, vxz, vyz = _MakeGravGradGridDH(\n            coeffs, self.gm, self.r0, a=a, f=f, lmax=lmax,\n            lmax_calc=lmax_calc, sampling=sampling)\n\n        return _SHGravTensor(1.e9*vxx, 1.e9*vyy, 1.e9*vzz, 1.e9*vxy, 1.e9*vxz,\n                             1.e9*vyz, self.gm, a, f, lmax, lmax_calc)", "response": "This method creates a 2D cylindrical tensor for the 9 - dimensional field of the reference frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a global map of the height of the geoid.", "response": "def geoid(self, potref, a=None, f=None, r=None, omega=None, order=2,\n              lmax=None, lmax_calc=None, grid='DH2'):\n        \"\"\"\n        Create a global map of the height of the geoid and return an SHGeoid\n        class instance.\n\n        Usage\n        -----\n        geoid = x.geoid(potref, [a, f, r, omega, order, lmax, lmax_calc, grid])\n\n        Returns\n        -------\n        geoid : SHGeoid class instance.\n\n        Parameters\n        ----------\n        potref : float\n            The value of the potential on the chosen geoid, in m2 / s2.\n        a : optional, float, default = self.r0\n            The semi-major axis of the flattened ellipsoid on which the field\n            is computed.\n        f : optional, float, default = 0\n            The flattening of the reference ellipsoid: f=(a-b)/a.\n        r : optional, float, default = self.r0\n            The radius of the reference sphere that the Taylor expansion of the\n            potential is calculated on.\n        order : optional, integer, default = 2\n            The order of the Taylor series expansion of the potential about the\n            reference radius r. This can be either 1, 2, or 3.\n        omega : optional, float, default = self.omega\n            The angular rotation rate of the planet.\n        lmax : optional, integer, default = self.lmax\n            The maximum spherical harmonic degree that determines the number\n            of samples of the output grid, n=2lmax+2, and the latitudinal\n            sampling interval, 90/(lmax+1).\n        lmax_calc : optional, integer, default = lmax\n            The maximum spherical harmonic degree used in evaluating the\n            functions. This must be less than or equal to lmax.\n        grid : str, optional, default = 'DH2'\n            'DH' or 'DH1' for an equisampled lat/lon grid with nlat=nlon, or\n            'DH2' for an equidistant lat/lon grid with nlon=2*nlat.\n\n        Description\n        -----------\n        This method will create a global map of the geoid height, accurate to\n        either first, second, or third order, using the method described in\n        Wieczorek (2007; equation 19-20). The algorithm expands the potential\n        in a Taylor series on a spherical interface of radius r, and computes\n        the height above this interface to the potential potref exactly from\n        the linear, quadratic, or cubic equation at each grid point. If the\n        optional parameters a and f are specified, the geoid height will be\n        referenced to a flattened ellipsoid with semi-major axis a and\n        flattening f. The pseudo-rotational potential is explicitly accounted\n        for by using the angular rotation rate omega of the planet in the\n        SHGravCoeffs class instance. If omega is explicitly specified for this\n        method, it will override the value present in the class instance.\n\n        Reference\n        ----------\n        Wieczorek, M. A. Gravity and topography of the terrestrial planets,\n        Treatise on Geophysics, 10, 165-206, 2007.\n        \"\"\"\n        if a is None:\n            a = self.r0\n        if f is None:\n            f = 0.\n        if r is None:\n            r = self.r0\n        if lmax is None:\n            lmax = self.lmax\n        if lmax_calc is None:\n            lmax_calc = lmax\n\n        if grid.upper() in ('DH', 'DH1'):\n            sampling = 1\n        elif grid.upper() == 'DH2':\n            sampling = 2\n        else:\n            raise ValueError(\n                    \"grid must be 'DH', 'DH1', or 'DH2'. \"\n                    \"Input value was {:s}\".format(repr(grid)))\n\n        if self.errors is not None:\n            coeffs, errors = self.to_array(normalization='4pi', csphase=1)\n        else:\n            coeffs = self.to_array(normalization='4pi', csphase=1)\n\n        if omega is None:\n            omega = self.omega\n\n        geoid = _MakeGeoidGridDH(coeffs, self.r0, self.gm, potref, lmax=lmax,\n                                 omega=omega, r=r, order=order,\n                                 lmax_calc=lmax_calc, a=a, f=f,\n                                 sampling=sampling)\n\n        return _SHGeoid(geoid, self.gm, potref, a, f, omega, r, order,\n                        lmax, lmax_calc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrotates the coefficients by the Euler angles alpha beta gamma.", "response": "def _rotate(self, angles, dj_matrix, gm=None, r0=None, omega=None):\n        \"\"\"Rotate the coefficients by the Euler angles alpha, beta, gamma.\"\"\"\n        if dj_matrix is None:\n            dj_matrix = _shtools.djpi2(self.lmax + 1)\n\n        # The coefficients need to be 4pi normalized with csphase = 1\n        coeffs = _shtools.SHRotateRealCoef(\n            self.to_array(normalization='4pi', csphase=1), angles, dj_matrix)\n\n        # Convert 4pi normalized coefficients to the same normalization\n        # as the unrotated coefficients.\n        if self.normalization != '4pi' or self.csphase != 1:\n            temp = _convert(coeffs, normalization_in='4pi', csphase=1,\n                            normalization_out=self.normalization,\n                            csphase_out=self.csphase)\n            return SHGravCoeffs.from_array(\n                temp, normalization=self.normalization,\n                csphase=self.csphase, copy=False, gm=gm, r0=r0, omega=omega)\n        else:\n            return SHGravCoeffs.from_array(coeffs, gm=gm, r0=r0, omega=omega,\n                                           copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cluster_application_statistics(self, state_list=None,\n                                       application_type_list=None):\n        \"\"\"\n        With the Application Statistics API, you can obtain a collection of\n        triples, each of which contains the application type, the application\n        state and the number of applications of this type and this state in\n        ResourceManager context.\n\n        This method work in Hadoop > 2.0.0\n\n        :param list state_list: states of the applications, specified as a\n            comma-separated list. If states is not provided, the API will\n            enumerate all application states and return the counts of them.\n        :param list application_type_list: types of the applications,\n            specified as a comma-separated list. If application_types is not\n            provided, the API will count the applications of any application\n            type. In this case, the response shows * to indicate any\n            application type. Note that we only support at most one\n            applicationType temporarily. Otherwise, users will expect\n            an BadRequestException.\n        :returns: API response object with JSON data\n        :rtype: :py:class:`yarn_api_client.base.Response`\n        \"\"\"\n        path = '/ws/v1/cluster/appstatistics'\n\n        # TODO: validate state argument\n        states = ','.join(state_list) if state_list is not None else None\n        if application_type_list is not None:\n            application_types = ','.join(application_type_list)\n        else:\n            application_types = None\n\n        loc_args = (\n            ('states', states),\n            ('applicationTypes', application_types))\n        params = self.construct_parameters(loc_args)\n\n        return self.request(path, **params)", "response": "This method returns the application statistics for the specified state and application types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cluster_application_state(self, application_id):\n        path = '/ws/v1/cluster/apps/{appid}/state'.format(\n            appid=application_id)\n\n        return self.request(path)", "response": "With the application state API you can obtain the current state of an application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cluster_application_kill(self, application_id):\n\n        data = '{\"state\": \"KILLED\"}'\n        path = '/ws/v1/cluster/apps/{appid}/state'.format(\n            appid=application_id)\n\n        return self.update(path, data)", "response": "Kill an application with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cluster_nodes(self, state=None, healthy=None):\n        path = '/ws/v1/cluster/nodes'\n        # TODO: validate state argument\n\n        legal_healthy = ['true', 'false']\n        if healthy is not None and healthy not in legal_healthy:\n            msg = 'Valid Healthy arguments are true, false'\n            raise IllegalArgumentError(msg)\n\n        loc_args = (\n            ('state', state),\n            ('healthy', healthy),\n        )\n        params = self.construct_parameters(loc_args)\n\n        return self.request(path, **params)", "response": "With the Nodes API you can obtain a collection of resources each of\n            which represents a node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cluster_node(self, node_id):\n        path = '/ws/v1/cluster/nodes/{nodeid}'.format(nodeid=node_id)\n\n        return self.request(path)", "response": "A node resource contains information about a node in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef application_information(self, application_id):\n        path = '/proxy/{appid}/ws/v1/mapreduce/info'.format(\n            appid=application_id)\n\n        return self.request(path)", "response": "This method provides overall MapReduce application master information resource provides overall\n        information about that mapreduce application master."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef job(self, application_id, job_id):\n        path = '/proxy/{appid}/ws/v1/mapreduce/jobs/{jobid}'.format(\n            appid=application_id, jobid=job_id)\n\n        return self.request(path)", "response": "This method returns the information about a particular job that was started by this application master."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef job_task(self, application_id, job_id, task_id):\n        path = '/proxy/{appid}/ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}'.format(\n            appid=application_id, jobid=job_id, taskid=task_id)\n\n        return self.request(path)", "response": "A Task resource contains information about a particular\n            task within a job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef job(self, job_id):\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}'.format(jobid=job_id)\n\n        return self.request(path)", "response": "A Job resource contains information about a particular job identified by jobid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef job_attempts(self, job_id):\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}/jobattempts'.format(\n            jobid=job_id)\n\n        return self.request(path)", "response": "This API endpoint returns a collection of resources that represent a job attempt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef job_counters(self, job_id):\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}/counters'.format(\n            jobid=job_id)\n\n        return self.request(path)", "response": "This API endpoint provides a collection of resources that represent al the counters for that job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef task_attempt(self, job_id, task_id, attempt_id):\n        path = '/ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}'.format(\n            jobid=job_id, taskid=task_id, attemptid=attempt_id)\n\n        return self.request(path)", "response": "A Task Attempt resource contains information about a particular task in a particular job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn list of field names mentioned in Q object.", "response": "def q_mentioned_fields(q, model):\n    \"\"\"Returns list of field names mentioned in Q object.\n\n    Q(a__isnull=True, b=F('c')) -> ['a', 'b', 'c']\n    \"\"\"\n    query = Query(model)\n    where = query._add_q(q, used_aliases=set(), allow_joins=False)[0]\n    return list(sorted(set(expression_mentioned_fields(where))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_name_with_model(self, model):\n        table_name = model._meta.db_table\n        column_names = [model._meta.get_field(field_name).column for field_name, order in self.fields_orders]\n        column_names_with_order = [\n            (('-%s' if order else '%s') % column_name)\n            for column_name, (field_name, order) in zip(column_names, self.fields_orders)\n        ]\n        # The length of the parts of the name is based on the default max\n        # length of 30 characters.\n        hash_data = [table_name] + column_names_with_order + [self.suffix] + self.name_hash_extra_data()\n        self.name = '%s_%s_%s' % (\n            table_name[:11],\n            column_names[0][:7],\n            '%s_%s' % (self._hash_generator(*hash_data), self.suffix),\n        )\n        assert len(self.name) <= self.max_name_length, (\n            'Index too long for multiple database support. Is self.suffix '\n            'longer than 3 characters?'\n        )\n        self.check_name()", "response": "Sets the name of the index based on the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking partial unique constraints on the model and raise ValidationError if any failed.", "response": "def validate_partial_unique(self):\n        \"\"\"Check partial unique constraints on the model and raise ValidationError if any failed.\n\n        We want to check if another instance already exists with the fields mentioned in idx.fields, but only if idx.where matches.\n        But can't just check for the fields in idx.fields - idx.where may refer to other fields on the current (or other) models.\n        Also can't check for all fields on the current model - should not include irrelevant fields which may hide duplicates.\n\n        To find potential conflicts, we need to build a queryset which:\n        1. Filters by idx.fields with their current values on this instance,\n        2. Filters on idx.where\n        3. Filters by fields mentioned in idx.where, with their current values on this instance,\n        4. Excludes current object if it does not match the where condition.\n\n        Note that step 2 ensures the lookup only looks for conflicts among rows covered by the PartialIndes,\n        and steps 2+3 ensures that the QuerySet is empty if the PartialIndex does not cover the current object.\n        \"\"\"\n        # Find PartialIndexes with unique=True defined on model.\n        unique_idxs = [idx for idx in self._meta.indexes if isinstance(idx, PartialIndex) and idx.unique]\n\n        if unique_idxs:\n            model_fields = set(f.name for f in self._meta.get_fields(include_parents=True, include_hidden=True))\n\n            for idx in unique_idxs:\n                where = idx.where\n                if not isinstance(where, Q):\n                    raise ImproperlyConfigured(\n                        'ValidatePartialUniqueMixin is not supported for PartialIndexes with a text-based where condition. ' +\n                        'Please upgrade to Q-object based where conditions.'\n                    )\n\n                mentioned_fields = set(idx.fields) | set(query.q_mentioned_fields(where, self.__class__))\n\n                missing_fields = mentioned_fields - model_fields\n                if missing_fields:\n                    raise RuntimeError('Unable to use ValidatePartialUniqueMixin: expecting to find fields %s on model. ' +\n                                       'This is a bug in the PartialIndex definition or the django-partial-index library itself.')\n\n                values = {field_name: getattr(self, field_name) for field_name in mentioned_fields}\n\n                conflict = self.__class__.objects.filter(**values)  # Step 1 and 3\n                conflict = conflict.filter(where)  # Step 2\n                if self.pk:\n                    conflict = conflict.exclude(pk=self.pk)  # Step 4\n\n                if conflict.exists():\n                    raise PartialUniqueValidationError('%s with the same values for %s already exists.' % (\n                        self.__class__.__name__,\n                        ', '.join(sorted(idx.fields)),\n                    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a Wikipedia page with title and ns and language.", "response": "def page(\n            self,\n            title: str,\n            ns: Namespace = Namespace.MAIN,\n            unquote: bool = False,\n    ) -> 'WikipediaPage':\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        Creating `WikipediaPage` object is always the first step for extracting any information.\n\n        Example::\n\n            wiki_wiki = wikipediaapi.Wikipedia('en')\n            page_py = wiki_wiki.page('Python_(programming_language)')\n            print(page_py.title)\n            # Python (programming language)\n\n            wiki_hi = wikipediaapi.Wikipedia('hi')\n\n            page_hi_py = wiki_hi.article(\n                title='%E0%A4%AA%E0%A4%BE%E0%A4%87%E0%A4%A5%E0%A4%A8',\n                unquote=True,\n            )\n            print(page_hi_py.title)\n            # \u092a\u093e\u0907\u0925\u0928\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`Namespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n\n        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            self,\n            title=title,\n            ns=ns,\n            language=self.language\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a Wikipedia page with title title.", "response": "def article(\n            self,\n            title: str,\n            ns: Namespace = Namespace.MAIN,\n            unquote: bool = False\n    ) -> 'WikipediaPage':\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        This function is an alias for :func:`page`\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`Namespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the summary of the page with respect to parameters", "response": "def extracts(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            'action': 'query',\n            'prop': 'extracts',\n            'titles': page.title\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params['explaintext'] = 1\n            params['exsectionformat'] = 'wiki'\n        # elif self.extract_format == ExtractFormat.PLAIN:\n        #    params['explaintext'] = 1\n        #    params['exsectionformat'] = 'plain'\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n        self._common_attributes(raw['query'], page)\n        pages = raw['query']['pages']\n        for k, v in pages.items():\n            if k == '-1':\n                page._attributes['pageid'] = -1\n                return ''\n            else:\n                return self._build_extracts(v, page)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef info(\n            self,\n            page: 'WikipediaPage'\n    ) -> 'WikipediaPage':\n        \"\"\"\n        https://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\n        https://www.mediawiki.org/wiki/API:Info\n        \"\"\"\n        params = {\n            'action': 'query',\n            'prop': 'info',\n            'titles': page.title,\n            'inprop': '|'.join([\n                'protection',\n                'talkid',\n                'watched',\n                'watchers',\n                'visitingwatchers',\n                'notificationtimestamp',\n                'subjectid',\n                'url',\n                'readable',\n                'preload',\n                'displaytitle'\n            ])\n        }\n        raw = self._query(\n            page,\n            params\n        )\n        self._common_attributes(raw['query'], page)\n        pages = raw['query']['pages']\n        for k, v in pages.items():\n            if k == '-1':\n                page._attributes['pageid'] = -1\n                return page\n            else:\n                return self._build_info(v, page)\n        return page", "response": "Returns the info page for the specified object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of langlinks of the page with respect to parameters AttributeAsite.", "response": "def langlinks(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ) -> PagesDict:\n        \"\"\"\n        Returns langlinks of the page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        - https://www.mediawiki.org/wiki/API:Langlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to pages in other languages\n\n        \"\"\"\n\n        params = {\n            'action': 'query',\n            'prop': 'langlinks',\n            'titles': page.title,\n            'lllimit': 500,\n            'llprop': 'url',\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n        self._common_attributes(raw['query'], page)\n        pages = raw['query']['pages']\n        for k, v in pages.items():\n            if k == '-1':\n                page._attributes['pageid'] = -1\n                return {}\n            else:\n                return self._build_langlinks(v, page)\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns backlinks from other pages with respect to parameters", "response": "def backlinks(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ) -> PagesDict:\n        \"\"\"\n        Returns backlinks from other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        - https://www.mediawiki.org/wiki/API:Backlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: backlinks from other pages\n\n        \"\"\"\n\n        params = {\n            'action': 'query',\n            'list': 'backlinks',\n            'bltitle': page.title,\n            'bllimit': 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n\n        self._common_attributes(raw['query'], page)\n        v = raw['query']\n        while 'continue' in raw:\n            params['blcontinue'] = raw['continue']['blcontinue']\n            raw = self._query(\n                page,\n                params\n            )\n            v['backlinks'] += raw['query']['backlinks']\n        return self._build_backlinks(v, page)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of pages in given category with respect to parameters", "response": "def categorymembers(\n            self,\n            page: 'WikipediaPage',\n            **kwargs\n    ) -> PagesDict:\n        \"\"\"\n        Returns pages in given category with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        - https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: pages in given category\n        \"\"\"\n\n        params = {\n            'action': 'query',\n            'list': 'categorymembers',\n            'cmtitle': page.title,\n            'cmlimit': 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(\n            page,\n            used_params\n        )\n\n        self._common_attributes(raw['query'], page)\n        v = raw['query']\n        while 'continue' in raw:\n            params['cmcontinue'] = raw['continue']['cmcontinue']\n            raw = self._query(\n                page,\n                params\n            )\n            v['categorymembers'] += raw['query']['categorymembers']\n\n        return self._build_categorymembers(v, page)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef full_text(self, level: int = 1) -> str:\n        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += \"<h{}>{}</h{}>\".format(level, self.title, level)\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += sec.full_text(level + 1)\n        return res", "response": "Returns the full text of the current section as well as all its subsections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sections(self) -> List[WikipediaPageSection]:\n        if not self._called['extracts']:\n            self._fetch('extracts')\n        return self._section", "response": "Returns all sections of the curent page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef section_by_title(\n            self,\n            title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called['extracts']:\n            self._fetch('extracts')\n        return self._section_mapping.get(title)", "response": "Returns the section of the current page with given title."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the text of the current page.", "response": "def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def permits(self, identity, permission, context=None):\n        # pylint: disable=unused-argument\n        user = self.user_map.get(identity)\n        if not user:\n            return False\n        return permission in user.permissions", "response": "Check if the identity is allowed the permission in the current context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def remember(request, response, identity, **kwargs):\n    assert isinstance(identity, str), identity\n    assert identity\n    identity_policy = request.config_dict.get(IDENTITY_KEY)\n    if identity_policy is None:\n        text = (\"Security subsystem is not initialized, \"\n                \"call aiohttp_security.setup(...) first\")\n        # in order to see meaningful exception message both: on console\n        # output and rendered page we add same message to *reason* and\n        # *text* arguments.\n        raise web.HTTPInternalServerError(reason=text, text=text)\n    await identity_policy.remember(request, response, identity, **kwargs)", "response": "Remember identity into response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def forget(request, response):\n    identity_policy = request.config_dict.get(IDENTITY_KEY)\n    if identity_policy is None:\n        text = (\"Security subsystem is not initialized, \"\n                \"call aiohttp_security.setup(...) first\")\n        # in order to see meaningful exception message both: on console\n        # output and rendered page we add same message to *reason* and\n        # *text* arguments.\n        raise web.HTTPInternalServerError(reason=text, text=text)\n    await identity_policy.forget(request, response)", "response": "Forget previously remembered identity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if user is anonymous.", "response": "async def is_anonymous(request):\n    \"\"\"Check if user is anonymous.\n\n    User is considered anonymous if there is not identity\n    in request.\n    \"\"\"\n    identity_policy = request.config_dict.get(IDENTITY_KEY)\n    if identity_policy is None:\n        return True\n    identity = await identity_policy.identify(request)\n    if identity is None:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef login_required(fn):\n    @wraps(fn)\n    async def wrapped(*args, **kwargs):\n        request = args[-1]\n        if not isinstance(request, web.BaseRequest):\n            msg = (\"Incorrect decorator usage. \"\n                   \"Expecting `def handler(request)` \"\n                   \"or `def handler(self, request)`.\")\n            raise RuntimeError(msg)\n\n        await check_authorized(request)\n        return await fn(*args, **kwargs)\n\n    warnings.warn(\"login_required decorator is deprecated, \"\n                  \"use check_authorized instead\",\n                  DeprecationWarning)\n    return wrapped", "response": "Decorator that restricts access only for authorized users."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def check_permission(request, permission, context=None):\n\n    await check_authorized(request)\n    allowed = await permits(request, permission, context)\n    if not allowed:\n        raise web.HTTPForbidden()", "response": "Checker that passes only to authoraised users with given permission."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_permission(\n    permission,\n    context=None,\n):\n    \"\"\"Decorator that restricts access only for authorized users\n    with correct permissions.\n\n    If user is not authorized - raises HTTPUnauthorized,\n    if user is authorized and does not have permission -\n    raises HTTPForbidden.\n    \"\"\"\n    def wrapper(fn):\n        @wraps(fn)\n        async def wrapped(*args, **kwargs):\n            request = args[-1]\n            if not isinstance(request, web.BaseRequest):\n                msg = (\"Incorrect decorator usage. \"\n                       \"Expecting `def handler(request)` \"\n                       \"or `def handler(self, request)`.\")\n                raise RuntimeError(msg)\n\n            await check_permission(request, permission, context)\n            return await fn(*args, **kwargs)\n\n        return wrapped\n\n    warnings.warn(\"has_permission decorator is deprecated, \"\n                  \"use check_permission instead\",\n                  DeprecationWarning)\n    return wrapper", "response": "Decorator that restricts access only for authorized users\n    with correct permissions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colorful_print(raw):\n    '''print colorful text in terminal.'''\n\n    lines = raw.split('\\n')\n    colorful = True\n    detail = False\n    for line in lines:\n        if line:\n            if colorful:\n                colorful = False\n                print(colored(line, 'white', 'on_green') + '\\n')\n                continue\n            elif line.startswith('\u4f8b'):\n                print(line + '\\n')\n                continue\n            elif line.startswith('\u3010'):\n                print(colored(line, 'white', 'on_green') + '\\n')\n                detail = True\n                continue\n\n            if not detail:\n                print(colored(line + '\\n', 'yellow'))\n            else:\n                print(colored(line, 'cyan') + '\\n')", "response": "print colorful text in terminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_online(word, printer=True):\n    '''search the word or phrase on http://dict.youdao.com.'''\n\n    url = 'http://dict.youdao.com/w/ %s' % word\n\n    expl = get_text(url)\n\n    if printer:\n        colorful_print(expl)\n    return expl", "response": "search the word or phrase on http://dict. youdao. com."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the word or phrase to database.", "response": "def add_word(word, default_pr):\n    '''add the word or phrase to database.'''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n    curs.execute('SELECT expl, pr FROM Word WHERE name = \"%s\"' % word)\n    res = curs.fetchall()\n    if res:\n        print(colored(word + ' \u5728\u6570\u636e\u5e93\u4e2d\u5df2\u5b58\u5728\uff0c\u4e0d\u9700\u8981\u6dfb\u52a0', 'white', 'on_red'))\n        sys.exit()\n\n    try:\n        expl = search_online(word, printer=False)\n        curs.execute('insert into word(name, expl, pr, aset) values (\"%s\", \"%s\", %d, \"%s\")' % (\n            word, expl, default_pr, word[0].upper()))\n    except Exception as e:\n        print(colored('something\\'s wrong, you can\\'t add the word', 'white', 'on_red'))\n        print(e)\n    else:\n        conn.commit()\n        print(colored('%s has been inserted into database' % word, 'green'))\n    finally:\n        curs.close()\n        conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the word or phrase from database.", "response": "def delete_word(word):\n    '''delete the word or phrase from database.'''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n    # search fisrt\n    curs.execute('SELECT expl, pr FROM Word WHERE name = \"%s\"' % word)\n    res = curs.fetchall()\n\n    if res:\n        try:\n            curs.execute('DELETE FROM Word WHERE name = \"%s\"' % word)\n        except Exception as e:\n            print(e)\n        else:\n            print(colored('%s has been deleted from database' % word, 'green'))\n            conn.commit()\n        finally:\n            curs.close()\n            conn.close()\n    else:\n        print(colored('%s not exists in the database' % word, 'white', 'on_red'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist words which have priority pr.", "response": "def list_priority(pr, vb=False, output=False):\n    '''\n    list words by priority, like this:\n    1   : list words which the priority is 1,\n    2+  : list words which the priority is lager than 2,\n    3-4 : list words which the priority is from 3 to 4.\n    '''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n\n    try:\n        if not vb:\n            if len(pr) == 1:\n                curs.execute('SELECT name, pr FROM Word WHERE pr ==  %d ORDER by pr, name' % (int(pr[0])))\n            elif len(pr) == 2 and pr[1] == '+':\n                curs.execute('SELECT name, pr FROM Word WHERE pr >=  %d ORDER by pr, name' % (int(pr[0])))\n            elif len(pr) == 3 and pr[1] == '-':\n                curs.execute('SELECT name, pr FROM Word WHERE pr >=  %d AND pr<=  % d ORDER by pr, name' % (\n                    int(pr[0]), int(pr[2])))\n        else:\n            if len(pr) == 1:\n                curs.execute('SELECT expl, pr FROM Word WHERE pr ==  %d ORDER by pr, name' % (int(pr[0])))\n            elif len(pr) == 2 and pr[1] == '+':\n                curs.execute('SELECT expl, pr FROM Word WHERE pr >=  %d ORDER by pr, name' % (int(pr[0])))\n            elif len(pr) == 3 and pr[1] == '-':\n                curs.execute('SELECT expl, pr FROM Word WHERE pr >=  %d AND pr<=  %d ORDER by pr, name' % (\n                    int(pr[0]), int(pr[2])))\n    except Exception as e:\n        print(colored('something\\'s wrong, priority must be 1-5', 'red'))\n        print(e)\n    else:\n        for line in curs.fetchall():\n            expl = line[0]\n            pr = line[1]\n            print('\\n' + '=' * 40 + '\\n')\n            if not output:\n                print(colored('\u2605 ' * pr, 'red', ), colored('\u2606 ' * (5 - pr), 'yellow'), sep='')\n                colorful_print(expl)\n            else:\n                print('\u2605 ' * pr + '\u2606 ' * (5 - pr))\n                normal_print(expl)\n    finally:\n        curs.close()\n        conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncount the number of words", "response": "def count_word(arg):\n    '''count the number of words'''\n\n    conn = sqlite3.connect(os.path.join(DEFAULT_PATH, 'word.db'))\n    curs = conn.cursor()\n    if arg[0].isdigit():\n        if len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE pr ==  %d' % (int(arg[0])))\n        elif len(arg) == 2 and arg[1] == '+':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d' % (int(arg[0])))\n        elif len(arg) == 3 and arg[1] == '-':\n            curs.execute('SELECT count(*) FROM Word WHERE pr >=  %d AND pr<=  % d' % (int(arg[0]), int(arg[2])))\n    elif arg[0].isalpha():\n        if arg == 'all':\n            curs.execute('SELECT count(*) FROM Word')\n        elif len(arg) == 1:\n            curs.execute('SELECT count(*) FROM Word WHERE aset == \"%s\"' % arg.upper())\n    res = curs.fetchall()\n    print(res[0][0])\n    curs.close()\n    conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a datetime object from the given date which is in a specific format YYYY - MM - ddTHH : mm : ss", "response": "def strptime(date):\n    \"\"\"Returns datetime object from the given date, which is in a specific format: YYYY-MM-ddTHH:mm:ss\"\"\"\n    d = {\n        'year': date[0:4],\n        'month': date[5:7],\n        'day': date[8:10],\n        'hour': date[11:13],\n        'minute': date[14:16],\n        'second': date[17:],\n    }\n\n    d = dict((k, int(v)) for k, v in d.items())\n\n    return datetime(**d)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nredirecting your users to here to authenticate them.", "response": "def authentication_url(self):\n        \"\"\"Redirect your users to here to authenticate them.\"\"\"\n        params = {\n            'client_id': self.client_id,\n            'response_type': self.type,\n            'redirect_uri': self.callback_url\n        }\n        return AUTHENTICATION_URL + \"?\" + urlencode(params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, path, method='GET', params=None, data=None, files=None,\n                headers=None, raw=False, allow_redirects=True, stream=False,\n                timeout=None):\n        \"\"\"\n        Wrapper around requests.request()\n\n        Prepends BASE_URL to path.\n        Adds self.oauth_token to authorization header.\n        Parses response as JSON and returns it.\n\n        \"\"\"\n        if not headers:\n            headers = {}\n\n        if timeout is None:\n            timeout = self.timeout\n\n        # All requests must include oauth_token\n        headers['Authorization'] = 'token %s' % self.access_token\n\n        if path.startswith(('http://', 'https://')):\n            url = path\n        else:\n            url = BASE_URL + path\n        logger.debug('url: %s', url)\n\n        response = self.session.request(\n            method, url, params=params, data=data, files=files,\n            headers=headers, allow_redirects=allow_redirects, stream=stream,\n            timeout=self.timeout)\n        logger.debug('response: %s', response)\n        if raw:\n            return response\n\n        return _process_response(response)", "response": "Wrapper around requests. request that adds self. oauth_token to authorization header and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search(cls, query, page=1):\n        path = '/files/search/{query}/page/{page}'.format(query=query, page=page)\n        result = cls.client.request(path)\n        files = result['files']\n        return [cls(f) for f in files]", "response": "Search for the given keyword in the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets up the _paths attribute.", "response": "def setup(self, paths=None):  # pylint: disable=arguments-differ\n    \"\"\"Sets up the _paths attribute.\n\n    Args:\n      paths: Comma-separated list of strings representing the paths to collect.\n    \"\"\"\n    if not paths:\n      self.state.add_error(\n          'No `paths` argument provided in recipe, bailing', critical=True)\n    else:\n      self._paths = [path.strip() for path in paths.strip().split(',')]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self):\n    for path in self._paths:\n      if os.path.exists(path):\n        self.state.output.append((os.path.basename(path), path))\n      else:\n        self.state.add_error(\n            'Path {0:s} does not exist'.format(str(path)), critical=False)\n    if not self.state.output:\n      self.state.add_error('No valid paths collected, bailing', critical=True)", "response": "Checks whether the paths exist and updates the state accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating specified hunt. Args: name: string containing hunt name. args: proto (*FlowArgs) for type of hunt, as defined in GRR flow proto. Returns: The newly created GRR hunt object. Raises: ValueError: if approval is needed and approvers were not specified.", "response": "def _create_hunt(self, name, args):\n    \"\"\"Create specified hunt.\n\n    Args:\n      name: string containing hunt name.\n      args: proto (*FlowArgs) for type of hunt, as defined in GRR flow proto.\n\n    Returns:\n      The newly created GRR hunt object.\n\n    Raises:\n      ValueError: if approval is needed and approvers were not specified.\n    \"\"\"\n    runner_args = self.grr_api.types.CreateHuntRunnerArgs()\n    runner_args.description = self.reason\n    hunt = self.grr_api.CreateHunt(\n        flow_name=name, flow_args=args, hunt_runner_args=runner_args)\n    print('{0!s}: Hunt created'.format(hunt.hunt_id))\n    self._check_approval_wrapper(hunt, hunt.Start)\n    return hunt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self,\n            artifacts, use_tsk,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR Hunt artifact collector.\n\n    Args:\n      artifacts: str, comma-separated list of GRR-defined artifacts.\n      use_tsk: toggle for use_tsk flag.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: str, comma-separated list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRHuntArtifactCollector, self).setup(\n        reason, grr_server_url, grr_username, grr_password,\n        approvers=approvers, verify=verify)\n\n    self.artifacts = [item.strip() for item in artifacts.strip().split(',')]\n    if not artifacts:\n      self.state.add_error('No artifacts were specified.', critical=True)\n    self.use_tsk = use_tsk", "response": "Initializes a GRR Hunt artifact collector."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self):\n\n    print('Artifacts to be collected: {0!s}'.format(self.artifacts))\n    hunt_args = flows_pb2.ArtifactCollectorFlowArgs(\n        artifact_list=self.artifacts,\n        use_tsk=self.use_tsk,\n        ignore_interpolation_errors=True,\n        apply_parsers=False,)\n    return self._create_hunt('ArtifactCollectorFlow', hunt_args)", "response": "Construct and start a new Artifact Collection hunt."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize a GRR Hunt File Collector.", "response": "def setup(self,\n            file_path_list,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR Hunt file collector.\n\n    Args:\n      file_path_list: comma-separated list of file paths.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: comma-separated list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRHuntFileCollector, self).setup(\n        reason, grr_server_url, grr_username, grr_password,\n        approvers=approvers, verify=verify)\n    self.file_path_list = [item.strip() for item\n                           in file_path_list.strip().split(',')]\n    if not file_path_list:\n      self.state.add_error('Files must be specified for hunts', critical=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs and start a new File hunt.", "response": "def process(self):\n    \"\"\"Construct and start a new File hunt.\n\n    Returns:\n      The newly created GRR hunt object.\n\n    Raises:\n      RuntimeError: if no items specified for collection.\n    \"\"\"\n    print('Hunt to collect {0:d} items'.format(len(self.file_path_list)))\n    print('Files to be collected: {0!s}'.format(self.file_path_list))\n    hunt_action = flows_pb2.FileFinderAction(\n        action_type=flows_pb2.FileFinderAction.DOWNLOAD)\n    hunt_args = flows_pb2.FileFinderArgs(\n        paths=self.file_path_list, action=hunt_action)\n    return self._create_hunt('FileFinder', hunt_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a GRR Hunt file collector.", "response": "def setup(self,\n            hunt_id,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR Hunt file collector.\n\n    Args:\n      hunt_id: Hunt ID to download results from.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: comma-separated list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRHuntDownloader, self).setup(\n        reason, grr_server_url, grr_username, grr_password,\n        approvers=approvers, verify=verify)\n    self.hunt_id = hunt_id\n    self.output_path = tempfile.mkdtemp()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads current set of files in results.", "response": "def collect_hunt_results(self, hunt):\n    \"\"\"Download current set of files in results.\n\n    Args:\n      hunt: The GRR hunt object to download files from.\n\n    Returns:\n      list: tuples containing:\n          str: human-readable description of the source of the collection. For\n              example, the name of the source host.\n          str: path to the collected data.\n    Raises:\n      ValueError: if approval is needed and approvers were not specified.\n    \"\"\"\n    if not os.path.isdir(self.output_path):\n      os.makedirs(self.output_path)\n\n    output_file_path = os.path.join(\n        self.output_path, '.'.join((self.hunt_id, 'zip')))\n\n    if os.path.exists(output_file_path):\n      print('{0:s} already exists: Skipping'.format(output_file_path))\n      return None\n\n    self._check_approval_wrapper(\n        hunt, self._get_and_write_archive, hunt, output_file_path)\n\n    results = self._extract_hunt_results(output_file_path)\n    print('Wrote results of {0:s} to {1:s}'.format(\n        hunt.hunt_id, output_file_path))\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets and writes a hunt archive.", "response": "def _get_and_write_archive(self, hunt, output_file_path):\n    \"\"\"Gets and writes a hunt archive.\n\n    Function is necessary for the _check_approval_wrapper to work.\n\n    Args:\n      hunt: The GRR hunt object.\n      output_file_path: The output path where to write the Hunt Archive.\n    \"\"\"\n    hunt_archive = hunt.GetFilesArchive()\n    hunt_archive.WriteToFile(output_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts a GRR client s FQDN from its client_info. yaml file.", "response": "def _get_client_fqdn(self, client_info_contents):\n    \"\"\"Extracts a GRR client's FQDN from its client_info.yaml file.\n\n    Args:\n      client_info_contents: The contents of the client_info.yaml file.\n\n    Returns:\n      A (str, str) tuple representing client ID and client FQDN.\n    \"\"\"\n    yamldict = yaml.safe_load(client_info_contents)\n    fqdn = yamldict['system_info']['fqdn']\n    client_id = yamldict['client_id'].split('/')[1]\n    return client_id, fqdn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen a hunt output archive and extract files.", "response": "def _extract_hunt_results(self, output_file_path):\n    \"\"\"Open a hunt output archive and extract files.\n\n    Args:\n      output_file_path: The path where the hunt archive is downloaded to.\n\n    Returns:\n      list: tuples containing:\n          str: The name of the client from where the files were downloaded.\n          str: The directory where the files were downloaded to.\n    \"\"\"\n    # Extract items from archive by host for processing\n    collection_paths = []\n    client_ids = set()\n    client_id_to_fqdn = {}\n    hunt_dir = None\n    try:\n      with zipfile.ZipFile(output_file_path) as archive:\n        items = archive.infolist()\n        for f in items:\n\n          if not hunt_dir:\n            hunt_dir = f.filename.split('/')[0]\n\n          # If we're dealing with client_info.yaml, use it to build a client\n          # ID to FQDN correspondence table & skip extraction.\n          if f.filename.split('/')[-1] == 'client_info.yaml':\n            client_id, fqdn = self._get_client_fqdn(archive.read(f))\n            client_id_to_fqdn[client_id] = fqdn\n            continue\n\n          client_id = f.filename.split('/')[1]\n          if client_id.startswith('C.'):\n            if client_id not in client_ids:\n              client_directory = os.path.join(self.output_path,\n                                              hunt_dir, client_id)\n              collection_paths.append((client_id, client_directory))\n              client_ids.add(client_id)\n            try:\n              archive.extract(f, self.output_path)\n            except KeyError as exception:\n              print('Extraction error: {0:s}'.format(exception))\n              return []\n\n    except OSError as exception:\n      msg = 'Error manipulating file {0:s}: {1!s}'.format(\n          output_file_path, exception)\n      self.state.add_error(msg, critical=True)\n      return []\n    except zipfile.BadZipfile as exception:\n      msg = 'Bad zipfile {0:s}: {1!s}'.format(\n          output_file_path, exception)\n      self.state.add_error(msg, critical=True)\n      return []\n\n    try:\n      os.remove(output_file_path)\n    except OSError as exception:\n      print('Output path {0:s} could not be removed: {1:s}'.format(\n          output_file_path, exception))\n\n    # Translate GRR client IDs to FQDNs with the information retrieved\n    # earlier\n    fqdn_collection_paths = []\n    for client_id, path in collection_paths:\n      fqdn = client_id_to_fqdn.get(client_id, client_id)\n      fqdn_collection_paths.append((fqdn, path))\n\n    if not fqdn_collection_paths:\n      self.state.add_error('Nothing was extracted from the hunt archive',\n                           critical=True)\n      return []\n\n    return fqdn_collection_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process(self):\n    hunt = self.grr_api.Hunt(self.hunt_id).Get()\n    self.state.output = self.collect_hunt_results(hunt)", "response": "Construct and start a new hunt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_extra(cls, name=None):\n    if not name:\n      return cls._extra_config\n    return cls._extra_config.get(name, None)", "response": "Gets extra configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_extra(cls, filename):\n    try:\n      with open(filename, 'rb') as configuration_file:\n        cls.load_extra_data(configuration_file.read())\n        sys.stderr.write(\"Config successfully loaded from {0:s}\\n\".format(\n            filename))\n        return True\n    except IOError:\n      return False", "response": "Loads extra JSON configuration parameters from a file on the filesystem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading extra JSON configuration parameters from a data buffer.", "response": "def load_extra_data(cls, data):\n    \"\"\"Loads extra JSON configuration parameters from a data buffer.\n\n    The data buffer must represent a JSON object.\n\n    Args:\n      data: str, the buffer to load the JSON data from.\n    \"\"\"\n    try:\n      cls._extra_config.update(json.loads(data))\n    except ValueError as exception:\n      sys.stderr.write('Could convert to JSON. {0:s}'.format(exception))\n      exit(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_recipe(cls, recipe):\n    recipe_name = recipe.contents['name']\n    cls._recipe_classes[recipe_name] = (\n        recipe.contents, recipe.args, recipe.__doc__)", "response": "Registers a dftimewolf recipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches GRR by hostname and get the latest active client.", "response": "def _get_client_by_hostname(self, hostname):\n    \"\"\"Search GRR by hostname and get the latest active client.\n\n    Args:\n      hostname: hostname to search for.\n\n    Returns:\n      GRR API Client object\n\n    Raises:\n      DFTimewolfError: if no client ID found for hostname.\n    \"\"\"\n    # Search for the hostname in GRR\n    print('Searching for client: {0:s}'.format(hostname))\n    try:\n      search_result = self.grr_api.SearchClients(hostname)\n    except grr_errors.UnknownError as exception:\n      self.state.add_error('Could not search for host {0:s}: {1!s}'.format(\n          hostname, exception\n      ), critical=True)\n      return None\n\n    result = []\n    for client in search_result:\n      if hostname.lower() in client.data.os_info.fqdn.lower():\n        result.append((client.data.last_seen_at, client))\n\n    if not result:\n      self.state.add_error(\n          'Could not get client_id for {0:s}'.format(hostname), critical=True)\n      return None\n\n    last_seen, client = sorted(result, key=lambda x: x[0], reverse=True)[0]\n    # Remove microseconds and create datetime object\n    last_seen_datetime = datetime.datetime.utcfromtimestamp(\n        last_seen / 1000000)\n    # Timedelta between now and when the client was last seen, in minutes.\n    # First, count total seconds. This will return a float.\n    last_seen_seconds = (\n        datetime.datetime.utcnow() - last_seen_datetime).total_seconds()\n    last_seen_minutes = int(round(last_seen_seconds / 60))\n\n    print('{0:s}: Found active client'.format(client.client_id))\n    print('Found active client: {0:s}'.format(client.client_id))\n    print('Client last seen: {0:s} ({1:d} minutes ago)'.format(\n        last_seen_datetime.strftime('%Y-%m-%dT%H:%M:%S+0000'),\n        last_seen_minutes))\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind GRR clients given a list of hosts.", "response": "def find_clients(self, hosts):\n    \"\"\"Finds GRR clients given a list of hosts.\n\n    Args:\n      hosts: List of hostname FQDNs\n\n    Returns:\n      List of GRR client objects.\n    \"\"\"\n    # TODO(tomchop): Thread this\n    clients = []\n    for host in hosts:\n      clients.append(self._get_client_by_hostname(host))\n    return [client for client in clients if client is not None]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget GRR client dictionary and make sure valid approvals exist.", "response": "def _get_client_by_id(self, client_id):\n    \"\"\"Get GRR client dictionary and make sure valid approvals exist.\n\n    Args:\n      client_id: GRR client ID.\n\n    Returns:\n      GRR API Client object\n    \"\"\"\n    client = self.grr_api.Client(client_id)\n    print('Checking for client approval')\n    self._check_approval_wrapper(client, client.ListFlows)\n    print('{0:s}: Client approval is valid'.format(client_id))\n    return client.Get()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate specified flow and set KeepAlive if requested.", "response": "def _launch_flow(self, client, name, args):\n    \"\"\"Create specified flow, setting KeepAlive if requested.\n\n    Args:\n      client: GRR Client object on which to launch the flow.\n      name: string containing flow name.\n      args: proto (*FlowArgs) for type of flow, as defined in GRR flow proto.\n\n    Returns:\n      string containing ID of launched flow\n    \"\"\"\n    # Start the flow and get the flow ID\n    flow = self._check_approval_wrapper(\n        client, client.CreateFlow, name=name, args=args)\n    flow_id = flow.flow_id\n    print('{0:s}: Scheduled'.format(flow_id))\n\n    if self.keepalive:\n      keepalive_flow = client.CreateFlow(\n          name='KeepAlive', args=flows_pb2.KeepAliveArgs())\n      print('KeepAlive Flow:{0:s} scheduled'.format(keepalive_flow.flow_id))\n\n    return flow_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nawait the flow completion.", "response": "def _await_flow(self, client, flow_id):\n    \"\"\"Awaits flow completion.\n\n    Args:\n      client: GRR Client object in which to await the flow.\n      flow_id: string containing ID of flow to await.\n\n    Raises:\n      DFTimewolfError: if flow error encountered.\n    \"\"\"\n    # Wait for the flow to finish\n    print('{0:s}: Waiting to finish'.format(flow_id))\n    while True:\n      try:\n        status = client.Flow(flow_id).Get().data\n      except grr_errors.UnknownError:\n        msg = 'Unable to stat flow {0:s} for host {1:s}'.format(\n            flow_id, client.data.os_info.fqdn.lower())\n        self.state.add_error(msg)\n        raise DFTimewolfError(\n            'Unable to stat flow {0:s} for host {1:s}'.format(\n                flow_id, client.data.os_info.fqdn.lower()))\n\n      if status.state == flows_pb2.FlowContext.ERROR:\n        # TODO(jbn): If one artifact fails, what happens? Test.\n        message = status.context.backtrace\n        if 'ArtifactNotRegisteredError' in status.context.backtrace:\n          message = status.context.backtrace.split('\\n')[-2]\n        raise DFTimewolfError(\n            '{0:s}: FAILED! Message from GRR:\\n{1:s}'.format(\n                flow_id, message))\n\n      if status.state == flows_pb2.FlowContext.TERMINATED:\n        print('{0:s}: Complete'.format(flow_id))\n        break\n      time.sleep(self._CHECK_FLOW_INTERVAL_SEC)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _download_files(self, client, flow_id):\n    output_file_path = os.path.join(\n        self.output_path, '.'.join((flow_id, 'zip')))\n\n    if os.path.exists(output_file_path):\n      print('{0:s} already exists: Skipping'.format(output_file_path))\n      return None\n\n    flow = client.Flow(flow_id)\n    file_archive = flow.GetFilesArchive()\n    file_archive.WriteToFile(output_file_path)\n\n    # Unzip archive for processing and remove redundant zip\n    fqdn = client.data.os_info.fqdn.lower()\n    client_output_file = os.path.join(self.output_path, fqdn)\n    if not os.path.isdir(client_output_file):\n      os.makedirs(client_output_file)\n\n    with zipfile.ZipFile(output_file_path) as archive:\n      archive.extractall(path=client_output_file)\n    os.remove(output_file_path)\n\n    return client_output_file", "response": "Download files from the specified flow."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize a GRR artifact collector.", "response": "def setup(self,\n            hosts, artifacts, extra_artifacts, use_tsk,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR artifact collector.\n\n    Args:\n      hosts: Comma-separated list of hostnames to launch the flow on.\n      artifacts: list of GRR-defined artifacts.\n      extra_artifacts: list of GRR-defined artifacts to append.\n      use_tsk: toggle for use_tsk flag on GRR flow.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRArtifactCollector, self).setup(\n        reason, grr_server_url, grr_username, grr_password, approvers=approvers,\n        verify=verify)\n\n    if artifacts is not None:\n      self.artifacts = [item.strip() for item in artifacts.strip().split(',')]\n\n    if extra_artifacts is not None:\n      self.extra_artifacts = [item.strip() for item\n                              in extra_artifacts.strip().split(',')]\n\n    self.hostnames = [item.strip() for item in hosts.strip().split(',')]\n    self.use_tsk = use_tsk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a single GRR client object.", "response": "def _process_thread(self, client):\n    \"\"\"Process a single GRR client.\n\n    Args:\n      client: a GRR client object.\n    \"\"\"\n    system_type = client.data.os_info.system\n    print('System type: {0:s}'.format(system_type))\n\n    # If the list is supplied by the user via a flag, honor that.\n    artifact_list = []\n    if self.artifacts:\n      print('Artifacts to be collected: {0!s}'.format(self.artifacts))\n      artifact_list = self.artifacts\n    else:\n      default_artifacts = self.artifact_registry.get(system_type, None)\n      if default_artifacts:\n        print('Collecting default artifacts for {0:s}: {1:s}'.format(\n            system_type, ', '.join(default_artifacts)))\n        artifact_list.extend(default_artifacts)\n\n    if self.extra_artifacts:\n      print('Throwing in an extra {0!s}'.format(self.extra_artifacts))\n      artifact_list.extend(self.extra_artifacts)\n      artifact_list = list(set(artifact_list))\n\n    if not artifact_list:\n      return\n\n    flow_args = flows_pb2.ArtifactCollectorFlowArgs(\n        artifact_list=artifact_list,\n        use_tsk=self.use_tsk,\n        ignore_interpolation_errors=True,\n        apply_parsers=False)\n    flow_id = self._launch_flow(client, 'ArtifactCollectorFlow', flow_args)\n    self._await_flow(client, flow_id)\n    collected_flow_data = self._download_files(client, flow_id)\n    if collected_flow_data:\n      print('{0!s}: Downloaded: {1:s}'.format(flow_id, collected_flow_data))\n      fqdn = client.data.os_info.fqdn.lower()\n      self.state.output.append((fqdn, collected_flow_data))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncollecting the artifacts. Raises: DFTimewolfError: if no artifacts specified nor resolved by platform.", "response": "def process(self):\n    \"\"\"Collect the artifacts.\n\n    Raises:\n      DFTimewolfError: if no artifacts specified nor resolved by platform.\n    \"\"\"\n    threads = []\n    for client in self.find_clients(self.hostnames):\n      print(client)\n      thread = threading.Thread(target=self._process_thread, args=(client, ))\n      threads.append(thread)\n      thread.start()\n\n    for thread in threads:\n      thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes a GRR file collector.", "response": "def setup(self,\n            hosts, files, use_tsk,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR file collector.\n\n    Args:\n      hosts: Comma-separated list of hostnames to launch the flow on.\n      files: list of file paths.\n      use_tsk: toggle for use_tsk flag on GRR flow.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRFileCollector, self).setup(\n        reason, grr_server_url, grr_username, grr_password,\n        approvers=approvers, verify=verify)\n\n    if files is not None:\n      self.files = [item.strip() for item in files.strip().split(',')]\n\n    self.hostnames = [item.strip() for item in hosts.strip().split(',')]\n    self.use_tsk = use_tsk"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_thread(self, client):\n    file_list = self.files\n    if not file_list:\n      return\n    print('Filefinder to collect {0:d} items'.format(len(file_list)))\n\n    flow_action = flows_pb2.FileFinderAction(\n        action_type=flows_pb2.FileFinderAction.DOWNLOAD)\n    flow_args = flows_pb2.FileFinderArgs(\n        paths=file_list,\n        action=flow_action,)\n    flow_id = self._launch_flow(client, 'FileFinder', flow_args)\n    self._await_flow(client, flow_id)\n    collected_flow_data = self._download_files(client, flow_id)\n    if collected_flow_data:\n      print('{0!s}: Downloaded: {1:s}'.format(flow_id, collected_flow_data))\n      fqdn = client.data.os_info.fqdn.lower()\n      self.state.output.append((fqdn, collected_flow_data))", "response": "Process a single client."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self,\n            host, flow_id,\n            reason, grr_server_url, grr_username, grr_password, approvers=None,\n            verify=True):\n    \"\"\"Initializes a GRR flow collector.\n\n    Args:\n      host: hostname of machine.\n      flow_id: ID of GRR flow to retrieve.\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    super(GRRFlowCollector, self).setup(\n        reason, grr_server_url, grr_username, grr_password,\n        approvers=approvers, verify=verify)\n    self.flow_id = flow_id\n    self.host = host", "response": "Initializes a GRR flow collector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting the results. Raises: DFTimewolfError: if no files specified", "response": "def process(self):\n    \"\"\"Collect the results.\n\n    Raises:\n      DFTimewolfError: if no files specified\n    \"\"\"\n    client = self._get_client_by_hostname(self.host)\n    self._await_flow(client, self.flow_id)\n    collected_flow_data = self._download_files(client, self.flow_id)\n    if collected_flow_data:\n      print('{0:s}: Downloaded: {1:s}'.format(\n          self.flow_id, collected_flow_data))\n      fqdn = client.data.os_info.fqdn.lower()\n      self.state.output.append((fqdn, collected_flow_data))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying a disk to the analysis project.", "response": "def process(self):\n    \"\"\"Copy a disk to the analysis project.\"\"\"\n    for disk in self.disks_to_copy:\n      print(\"Disk copy of {0:s} started...\".format(disk.name))\n      snapshot = disk.snapshot()\n      new_disk = self.analysis_project.create_disk_from_snapshot(\n          snapshot, disk_name_prefix=\"incident\" + self.incident_id)\n      self.analysis_vm.attach_disk(new_disk)\n      snapshot.delete()\n      print(\"Disk {0:s} successfully copied to {1:s}\".format(\n          disk.name, new_disk.name))\n      self.state.output.append((self.analysis_vm.name, new_disk))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(self,\n            analysis_project_name,\n            remote_project_name,\n            incident_id,\n            zone,\n            boot_disk_size,\n            cpu_cores,\n            remote_instance_name=None,\n            disk_names=None,\n            all_disks=False,\n            image_project=\"ubuntu-os-cloud\",\n            image_family=\"ubuntu-1604-lts\"):\n    \"\"\"Sets up a Google cloud collector.\n\n    This method creates and starts an analysis VM in the analysis project and\n    selects disks to copy from the remote project.\n\n    If disk_names is specified, it will copy the corresponding disks from the\n    project, ignoring disks belonging to any specific instances.\n\n    If remote_instance_name is specified, two behaviors are possible:\n      - If no other parameters are specified, it will select the instance's boot\n        disk\n      - if all_disks is set to True, it will select all disks in the project\n        that are attached to the instance\n\n    disk_names takes precedence over instance_names\n\n    Args:\n      analysis_project_name: The name of the project that contains the analysis\n          VM (string).\n      remote_project_name: The name of the remote project where the disks must\n          be copied from (string).\n      incident_id: The incident ID on which the name of the analysis VM will be\n          based (string).\n      zone: The zone in which new resources should be created (string).\n      boot_disk_size: The size of the analysis VM boot disk (in GB) (float).\n      cpu_cores: The number of CPU cores to create the machine with.\n      remote_instance_name: The name of the instance in the remote project\n          containing the disks to be copied (string).\n      disk_names: Comma separated string with disk names to copy (string).\n      all_disks: Copy all disks attached to the source instance (bool).\n      image_project: Name of the project where the analysis VM image is hosted.\n      image_family: Name of the image to use to create the analysis VM.\n    \"\"\"\n\n    disk_names = disk_names.split(\",\") if disk_names else []\n\n    self.analysis_project = libcloudforensics.GoogleCloudProject(\n        analysis_project_name, default_zone=zone)\n    remote_project = libcloudforensics.GoogleCloudProject(\n        remote_project_name)\n\n    if not (remote_instance_name or disk_names):\n      self.state.add_error(\n          \"You need to specify at least an instance name or disks to copy\",\n          critical=True)\n      return\n\n    self.incident_id = incident_id\n    analysis_vm_name = \"gcp-forensics-vm-{0:s}\".format(incident_id)\n    print(\"Your analysis VM will be: {0:s}\".format(analysis_vm_name))\n    print(\"Complimentary gcloud command:\")\n    print(\"gcloud compute ssh --project {0:s} {1:s} --zone {2:s}\".format(\n        analysis_project_name,\n        analysis_vm_name,\n        zone))\n\n    try:\n      # TODO: Make creating an analysis VM optional\n      # pylint: disable=too-many-function-args\n      self.analysis_vm, _ = libcloudforensics.start_analysis_vm(\n          self.analysis_project.project_id,\n          analysis_vm_name,\n          zone,\n          boot_disk_size,\n          int(cpu_cores),\n          attach_disk=None,\n          image_project=image_project,\n          image_family=image_family)\n\n      if disk_names:\n        for name in disk_names:\n          try:\n            self.disks_to_copy.append(remote_project.get_disk(name))\n          except RuntimeError:\n            self.state.add_error(\n                \"Disk '{0:s}' was not found in project {1:s}\".format(\n                    name, remote_project_name),\n                critical=True)\n            break\n\n      elif remote_instance_name:\n        remote_instance = remote_project.get_instance(\n            remote_instance_name)\n\n        if all_disks:\n          self.disks_to_copy = [\n              remote_project.get_disk(disk_name)\n              for disk_name in remote_instance.list_disks()\n          ]\n        else:\n          self.disks_to_copy = [remote_instance.get_boot_disk()]\n\n        if not self.disks_to_copy:\n          self.state.add_error(\"Could not find any disks to copy\",\n                               critical=True)\n\n    except AccessTokenRefreshError as err:\n      self.state.add_error(\"Something is wrong with your gcloud access token.\")\n      self.state.add_error(err, critical=True)\n\n    except ApplicationDefaultCredentialsError as err:\n      self.state.add_error(\"Something is wrong with your Application Default \"\n                           \"Credentials. Try running:\\n\"\n                           \"  $ gcloud auth application-default login\")\n      self.state.add_error(err, critical=True)\n\n    except HttpError as err:\n      if err.resp.status == 403:\n        self.state.add_error(\n            \"Make sure you have the appropriate permissions on the project\")\n      if err.resp.status == 404:\n        self.state.add_error(\n            \"GCP resource not found. Maybe a typo in the project / instance / \"\n            \"disk name?\")\n      self.state.add_error(err, critical=True)", "response": "This method creates and starts a Google Cloud collector for the analysis VM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the _timezone attribute.", "response": "def setup(self, timezone=None):  # pylint: disable=arguments-differ\n    \"\"\"Sets up the _timezone attribute.\n\n    Args:\n      timezone: Timezone name (optional)\n    \"\"\"\n    self._timezone = timezone\n    self._output_path = tempfile.mkdtemp()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self):\n    for description, path in self.state.input:\n      log_file_path = os.path.join(self._output_path, 'plaso.log')\n      print('Log file: {0:s}'.format(log_file_path))\n\n      # Build the plaso command line.\n      cmd = ['log2timeline.py']\n      # Since we might be running alongside another Module, always disable\n      # the status view.\n      cmd.extend(['-q', '--status_view', 'none'])\n      if self._timezone:\n        cmd.extend(['-z', self._timezone])\n\n      # Analyze all available partitions.\n      cmd.extend(['--partition', 'all'])\n\n      # Setup logging.\n      cmd.extend(['--logfile', log_file_path])\n\n      # And now, the crux of the command.\n      # Generate a new storage file for each plaso run\n      plaso_storage_file_path = os.path.join(\n          self._output_path, '{0:s}.plaso'.format(uuid.uuid4().hex))\n      cmd.extend([plaso_storage_file_path, path])\n\n      # Run the l2t command\n      full_cmd = ' '.join(cmd)\n      print('Running external command: \"{0:s}\"'.format(full_cmd))\n      try:\n        l2t_proc = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        _, error = l2t_proc.communicate()\n        l2t_status = l2t_proc.wait()\n        if l2t_status:\n          # self.console_out.StdErr(errors)\n          message = ('The log2timeline command {0:s} failed: {1:s}.'\n                     ' Check log file for details.').format(full_cmd, error)\n          self.state.add_error(message, critical=True)\n        self.state.output.append((description, plaso_storage_file_path))\n      except OSError as exception:\n        self.state.add_error(exception, critical=True)\n      # Catch all remaining errors since we want to gracefully report them\n      except Exception as exception:  # pylint: disable=broad-except\n        self.state.add_error(exception, critical=True)", "response": "Execute the Plaso process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a hunt result collector.", "response": "def setup(self, reason, grr_server_url, grr_username, grr_password,\n            approvers=None, verify=True):\n    \"\"\"Initializes a GRR hunt result collector.\n\n    Args:\n      reason: justification for GRR access.\n      grr_server_url: GRR server URL.\n      grr_username: GRR username.\n      grr_password: GRR password.\n      approvers: list of GRR approval recipients.\n      verify: boolean, whether to verify the GRR server's x509 certificate.\n    \"\"\"\n    grr_auth = (grr_username, grr_password)\n    self.approvers = []\n    if approvers:\n      self.approvers = [item.strip() for item in approvers.strip().split(',')]\n    self.grr_api = grr_api.InitHttp(api_endpoint=grr_server_url,\n                                    auth=grr_auth,\n                                    verify=verify)\n    self.output_path = tempfile.mkdtemp()\n    self.reason = reason"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a call to GRR functions checking for approval.", "response": "def _check_approval_wrapper(self, grr_object, grr_function, *args, **kwargs):\n    \"\"\"Wraps a call to GRR functions checking for approval.\n\n    Args:\n      grr_object: the GRR object to create the eventual approval on.\n      grr_function: The GRR function requiring approval.\n      *args: Positional arguments that are to be passed to `grr_function`.\n      **kwargs: Keyword arguments that are to be passed to `grr_function`.\n\n    Returns:\n      The return value of the execution of grr_function(*args, **kwargs).\n    \"\"\"\n    approval_sent = False\n\n    while True:\n      try:\n        return grr_function(*args, **kwargs)\n      except grr_errors.AccessForbiddenError as exception:\n        print('No valid approval found: {0!s}'.format(exception))\n        # If approval was already sent, just wait a bit more.\n        if approval_sent:\n          print('Approval not yet granted, waiting {0:d}s'.format(\n              self._CHECK_APPROVAL_INTERVAL_SEC))\n          time.sleep(self._CHECK_APPROVAL_INTERVAL_SEC)\n          continue\n\n        # If no approvers were specified, abort.\n        if not self.approvers:\n          message = ('GRR needs approval but no approvers specified '\n                     '(hint: use --approvers)')\n          self.state.add_error(message, critical=True)\n          return None\n\n        # Otherwise, send a request for approval\n        grr_object.CreateApproval(\n            reason=self.reason, notified_users=self.approvers)\n        approval_sent = True\n        print('{0!s}: approval request sent to: {1!s} (reason: {2:s})'.format(\n            grr_object, self.approvers, self.reason))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_replace(self, node):\n    if isinstance(node, nodes.reference) and 'refuri' in node:\n      r = node['refuri']\n      if r.endswith('.md'):\n        r = r[:-3] + '.html'\n        node['refuri'] = r\n      else:\n        match = anchor_regex.match(r)\n        if match:\n          node['refuri'] = '{0:s}.html#{1:s}'.format(\n              match.group('uri'), match.group('anchor'))\n    return node", "response": "Parses URIs containing. md and replaces them with their HTML page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef traverse(self, node):\n    self.find_replace(node)\n\n    for c in node.children:\n      self.traverse(c)", "response": "Traverse the document tree rooted at node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_session(self, username, password):\n    session = requests.Session()\n    session.verify = False  # Depending on SSL cert is verifiable\n    try:\n      response = session.get(self.host_url)\n    except requests.exceptions.ConnectionError:\n      return False\n    # Get the CSRF token from the response\n    soup = BeautifulSoup(response.text, 'html.parser')\n    csrf_token = soup.find('input', dict(name='csrf_token'))['value']\n    login_data = dict(username=username, password=password)\n    session.headers.update({\n        'x-csrftoken': csrf_token,\n        'referer': self.host_url\n    })\n    _ = session.post('{0:s}/login/'.format(self.host_url), data=login_data)\n    return session", "response": "Create HTTP session.\n\n    Args:\n      username (str): Timesketch username\n      password (str): Timesketch password\n\n    Returns:\n      requests.Session: Session object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_sketch(self, name, description):\n    resource_url = '{0:s}/sketches/'.format(self.api_base_url)\n    form_data = {'name': name, 'description': description}\n    response = self.session.post(resource_url, json=form_data)\n    response_dict = response.json()\n    sketch_id = response_dict['objects'][0]['id']\n    return sketch_id", "response": "Create a new sketch with the specified name and description."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upload_timeline(self, timeline_name, plaso_storage_path):\n    resource_url = '{0:s}/upload/'.format(self.api_base_url)\n    files = {'file': open(plaso_storage_path, 'rb')}\n    data = {'name': timeline_name}\n    response = self.session.post(resource_url, files=files, data=data)\n    try:\n      response_dict = response.json()\n    except ValueError:\n      raise RuntimeError(\n          'Could not decode JSON response from Timesketch'\n          ' (Status {0:d}):\\n{1:s}'.format(\n              response.status_code, response.content))\n\n    index_id = response_dict['objects'][0]['id']\n    return index_id", "response": "Uploads a timeline from the given plaso file to the Timesketch API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload provided artifacts to specified sketch.", "response": "def export_artifacts(self, processed_artifacts, sketch_id):\n    \"\"\"Upload provided artifacts to specified, or new if non-existent, sketch.\n\n    Args:\n      processed_artifacts:  List of (timeline_name, artifact_path) tuples\n      sketch_id: ID of sketch to append the timeline to\n\n    Returns:\n      int: ID of sketch.\n    \"\"\"\n\n    # Export processed timeline(s)\n    for timeline_name, artifact_path in processed_artifacts:\n      print('Uploading {0:s} to timeline {1:s}'.format(\n          artifact_path, timeline_name))\n      new_timeline_id = self.upload_timeline(timeline_name, artifact_path)\n      self.add_timeline_to_sketch(sketch_id, new_timeline_id)\n\n    return sketch_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassociates the specified timeline with the specified sketch.", "response": "def add_timeline_to_sketch(self, sketch_id, index_id):\n    \"\"\"Associate the specified timeline and sketch.\n\n    Args:\n      sketch_id (int): ID of sketch\n      index_id (int): ID of timeline to add to sketch\n    \"\"\"\n    resource_url = '{0:s}/sketches/{1:d}/timelines/'.format(\n        self.api_base_url, sketch_id)\n    form_data = {'timeline': [index_id]}\n    self.session.post(resource_url, json=form_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting information on the specified sketch.", "response": "def get_sketch(self, sketch_id):\n    \"\"\"Get information on the specified sketch.\n\n    Args:\n      sketch_id (int): ID of sketch\n\n    Returns:\n      dict: Dictionary of sketch information\n\n    Raises:\n      ValueError: Sketch is inaccessible\n    \"\"\"\n    resource_url = '{0:s}/sketches/{1:d}/'.format(self.api_base_url, sketch_id)\n    response = self.session.get(resource_url)\n    response_dict = response.json()\n    try:\n      response_dict['objects']\n    except KeyError:\n      raise ValueError('Sketch does not exist or you have no access')\n    return response_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the attribute container from a dictionary.", "response": "def copy_from_dict(self, attributes):\n    \"\"\"Copies the attribute container from a dictionary.\n    Args:\n      attributes (dict[str, object]): attribute values per name.\n    \"\"\"\n    for attribute_name, attribute_value in attributes.items():\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_':\n        continue\n      setattr(self, attribute_name, attribute_value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_attribute_names(self):\n    attribute_names = []\n    for attribute_name in iter(self.__dict__.keys()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_':\n        continue\n      attribute_names.append(attribute_name)\n\n    return attribute_names", "response": "Retrieves the names of all attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_attributes(self):\n    for attribute_name, attribute_value in iter(self.__dict__.items()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_' or attribute_value is None:\n        continue\n\n      yield attribute_name, attribute_value", "response": "Retrieves the attribute names and values of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_attribute_values_string(self):\n    attributes = []\n    for attribute_name, attribute_value in sorted(self.__dict__.items()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_' or attribute_value is None:\n        continue\n\n      if isinstance(attribute_value, dict):\n        attribute_value = sorted(attribute_value.items())\n\n      elif isinstance(attribute_value, six.binary_type):\n        attribute_value = repr(attribute_value)\n\n      attribute_string = '{0:s}: {1!s}'.format(attribute_name, attribute_value)\n      attributes.append(attribute_string)\n\n    return ', '.join(attributes)", "response": "Retrieves a comparable string of the attribute values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset up the _keywords attribute.", "response": "def setup(self, keywords=None):  # pylint: disable=arguments-differ\n    \"\"\"Sets up the _keywords attribute.\n\n    Args:\n      keywords: pipe separated list of keyword to search\n    \"\"\"\n    self._keywords = keywords\n    self._output_path = tempfile.mkdtemp()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting the grep command", "response": "def process(self):\n    \"\"\"Execute the grep command\"\"\"\n\n    for _, path in self.state.input:\n      log_file_path = os.path.join(self._output_path, 'grepper.log')\n      print('Log file: {0:s}'.format(log_file_path))\n\n      print('Walking through dir (absolute) = ' + os.path.abspath(path))\n      try:\n        for root, _, files in os.walk(path):\n          for filename in files:\n            found = set()\n            fullpath = '{0:s}/{1:s}'.format(os.path.abspath(root), filename)\n            if mimetypes.guess_type(filename)[0] == 'application/pdf':\n              found = self.grepPDF(fullpath)\n            else:\n              with open(fullpath, 'r') as fp:\n                for line in fp:\n                  found.update(set(x.lower() for x in re.findall(\n                      self._keywords, line, re.IGNORECASE)))\n            if [item for item in found if item]:\n              output = '{0:s}/{1:s}:{2:s}'.format(path, filename, ','.join(\n                  filter(None, found)))\n              if self._final_output:\n                self._final_output += '\\n' + output\n              else:\n                self._final_output = output\n              print(output)\n      except OSError as exception:\n        self.state.add_error(exception, critical=True)\n        return\n      # Catch all remaining errors since we want to gracefully report them\n      except Exception as exception:  # pylint: disable=broad-except\n        self.state.add_error(exception, critical=True)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grepPDF(self, path):\n    with open(path, 'rb') as pdf_file_obj:\n      match = set()\n      text = ''\n      pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n      pages = pdf_reader.numPages\n      for page in range(pages):\n        page_obj = pdf_reader.getPage(page)\n        text += '\\n' + page_obj.extractText()\n      match.update(set(x.lower() for x in re.findall(\n          self._keywords, text, re.IGNORECASE)))\n    return match", "response": "Parse PDF files text content for keywords."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the internal module pool with modules declared in a recipe.", "response": "def load_recipe(self, recipe):\n    \"\"\"Populates the internal module pool with modules declared in a recipe.\n\n    Args:\n      recipe: Dict, recipe declaring modules to load.\n    \"\"\"\n    self.recipe = recipe\n    for module_description in recipe['modules']:\n      # Combine CLI args with args from the recipe description\n      module_name = module_description['name']\n      module = self.config.get_module(module_name)(self)\n      self._module_pool[module_name] = module"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores the given container in the state s store.", "response": "def store_container(self, container):\n    \"\"\"Thread-safe method to store data in the state's store.\n\n    Args:\n      container (containers.interface.AttributeContainer): The data to store.\n    \"\"\"\n    with self._store_lock:\n      self.store.setdefault(container.CONTAINER_TYPE, []).append(container)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_containers(self, container_class):\n    with self._store_lock:\n      return self.store.get(container_class.CONTAINER_TYPE, [])", "response": "Thread - safe method to retrieve data from the state s store."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform setup tasks for each module in the module pool.", "response": "def setup_modules(self, args):\n    \"\"\"Performs setup tasks for each module in the module pool.\n\n    Threads declared modules' setup() functions. Takes CLI arguments into\n    account when replacing recipe parameters for each module.\n\n    Args:\n      args: Command line arguments that will be used to replace the parameters\n          declared in the recipe.\n    \"\"\"\n\n    def _setup_module_thread(module_description):\n      \"\"\"Calls the module's setup() function and sets an Event object for it.\n\n      Args:\n        module_description (dict): Corresponding recipe module description.\n      \"\"\"\n      new_args = utils.import_args_from_dict(\n          module_description['args'], vars(args), self.config)\n      module = self._module_pool[module_description['name']]\n      try:\n        module.setup(**new_args)\n      except Exception as error:  # pylint: disable=broad-except\n        self.add_error(\n            'An unknown error occurred: {0!s}\\nFull traceback:\\n{1:s}'.format(\n                error, traceback.format_exc()),\n            critical=True)\n\n      self.events[module_description['name']] = threading.Event()\n      self.cleanup()\n\n    threads = []\n    for module_description in self.recipe['modules']:\n      t = threading.Thread(\n          target=_setup_module_thread,\n          args=(module_description, )\n      )\n      threads.append(t)\n      t.start()\n    for t in threads:\n      t.join()\n\n    self.check_errors(is_global=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the actual processing for each module in the module pool.", "response": "def run_modules(self):\n    \"\"\"Performs the actual processing for each module in the module pool.\"\"\"\n\n    def _run_module_thread(module_description):\n      \"\"\"Runs the module's process() function.\n\n      Waits for any blockers to have finished before running process(), then\n      sets an Event flag declaring the module has completed.\n      \"\"\"\n      for blocker in module_description['wants']:\n        self.events[blocker].wait()\n      module = self._module_pool[module_description['name']]\n      try:\n        module.process()\n      except DFTimewolfError as error:\n        self.add_error(error.message, critical=True)\n      except Exception as error:  # pylint: disable=broad-except\n        self.add_error(\n            'An unknown error occurred: {0!s}\\nFull traceback:\\n{1:s}'.format(\n                error, traceback.format_exc()),\n            critical=True)\n      print('Module {0:s} completed'.format(module_description['name']))\n      self.events[module_description['name']].set()\n      self.cleanup()\n\n    threads = []\n    for module_description in self.recipe['modules']:\n      t = threading.Thread(\n          target=_run_module_thread,\n          args=(module_description, )\n      )\n      threads.append(t)\n      t.start()\n    for t in threads:\n      t.join()\n\n    self.check_errors(is_global=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an error to the state.", "response": "def add_error(self, error, critical=False):\n    \"\"\"Adds an error to the state.\n\n    Args:\n      error: The text that will be added to the error list.\n      critical: If set to True and the error is checked with check_errors, will\n          dfTimewolf will abort.\n    \"\"\"\n    self.errors.append((error, critical))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cleanup(self):\n    # Move any existing errors to global errors\n    self.global_errors.extend(self.errors)\n    self.errors = []\n\n    # Make the previous module's output available to the next module\n    self.input = self.output\n    self.output = []", "response": "Basic cleanup after modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for errors and exits if any of them are critical.", "response": "def check_errors(self, is_global=False):\n    \"\"\"Checks for errors and exits if any of them are critical.\n\n    Args:\n      is_global: If True, check the global_errors attribute. If false, check the\n          error attribute.\n    \"\"\"\n    errors = self.global_errors if is_global else self.errors\n    if errors:\n      print('dfTimewolf encountered one or more errors:')\n      for error, critical in errors:\n        print('{0:s}  {1:s}'.format('CRITICAL: ' if critical else '', error))\n        if critical:\n          print('Critical error found. Aborting.')\n          sys.exit(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up the object attributes.", "response": "def setup(self, disk_name, project, turbinia_zone):\n    \"\"\"Sets up the object attributes.\n\n    Args:\n      disk_name (string): Name of the disk to process\n      project (string): The project containing the disk to process\n      turbinia_zone (string): The zone containing the disk to process\n    \"\"\"\n    # TODO: Consider the case when multiple disks are provided by the previous\n    # module or by the CLI.\n\n    if project is None or turbinia_zone is None:\n      self.state.add_error(\n          'project or turbinia_zone are not all specified, bailing out',\n          critical=True)\n      return\n\n    self.disk_name = disk_name\n    self.project = project\n    self.turbinia_zone = turbinia_zone\n\n    try:\n      turbinia_config.LoadConfig()\n      self.turbinia_region = turbinia_config.TURBINIA_REGION\n      self.instance = turbinia_config.PUBSUB_TOPIC\n      if turbinia_config.PROJECT != self.project:\n        self.state.add_error(\n            'Specified project {0:s} does not match Turbinia configured '\n            'project {1:s}. Use gcp_turbinia_import recipe to copy the disk '\n            'into the same project.'.format(\n                self.project, turbinia_config.PROJECT), critical=True)\n        return\n      self._output_path = tempfile.mkdtemp()\n      self.client = turbinia_client.TurbiniaClient()\n    except TurbiniaException as e:\n      self.state.add_error(e, critical=True)\n      return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _print_task_data(self, task):\n    print(' {0:s} ({1:s})'.format(task['name'], task['id']))\n    paths = task.get('saved_paths', [])\n    if not paths:\n      return\n    for path in paths:\n      if path.endswith('worker-log.txt'):\n        continue\n      if path.endswith('{0:s}.log'.format(task.get('id'))):\n        continue\n      if path.startswith('/'):\n        continue\n      print('   ' + path)", "response": "Pretty - prints task data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay the overall progress of tasks in a Turbinia job.", "response": "def display_task_progress(\n      self, instance, project, region, request_id=None, user=None,\n      poll_interval=60):\n    \"\"\"Displays the overall progress of tasks in a Turbinia job.\n\n    Args:\n      instance (string): The name of the Turbinia instance\n      project (string): The project containing the disk to process\n      region (string): Region where turbinia is configured.\n      request_id (string): The request ID provided by Turbinia.\n      user (string): The username to filter tasks by.\n      poll_interval (int): The interval at which to poll for new results.\n    \"\"\"\n    total_completed = 0\n\n    while True:\n      task_results = self.client.get_task_data(\n          instance, project, region, request_id=request_id, user=user)\n      tasks = {task['id']: task for task in task_results}\n      completed_tasks = set()\n      pending_tasks = set()\n\n      for task in tasks.values():\n        if task.get('successful') is not None:\n          completed_tasks.add(task['id'])\n        else:\n          pending_tasks.add(task['id'])\n\n      if len(completed_tasks) > total_completed or not completed_tasks:\n        total_completed = len(completed_tasks)\n\n        print('Task status update (completed: {0:d} | pending: {1:d})'.format(\n            len(completed_tasks), len(pending_tasks)))\n\n        print('Completed tasks:')\n        for task_id in completed_tasks:\n          self._print_task_data(tasks[task_id])\n\n        print('Pending tasks:')\n        for task_id in pending_tasks:\n          self._print_task_data(tasks[task_id])\n\n      if len(completed_tasks) == len(task_results) and completed_tasks:\n        print('All {0:d} Tasks completed'.format(len(task_results)))\n        return\n\n      time.sleep(poll_interval)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process(self):\n    log_file_path = os.path.join(self._output_path, 'turbinia.log')\n    print('Turbinia log file: {0:s}'.format(log_file_path))\n\n    if self.state.input and not self.disk_name:\n      _, disk = self.state.input[0]\n      self.disk_name = disk.name\n      print('Using disk {0:s} from previous collector'.format(self.disk_name))\n\n    evidence_ = evidence.GoogleCloudDisk(\n        disk_name=self.disk_name, project=self.project, zone=self.turbinia_zone)\n    request = TurbiniaRequest()\n    request.evidence.append(evidence_)\n\n    # Get threat intelligence data from any modules that have stored some.\n    # In this case, observables is a list of containers.ThreatIntelligence\n    # objects.\n    threatintel = self.state.get_containers(containers.ThreatIntelligence)\n    if threatintel:\n      print('Sending {0:d} threatintel to Turbinia GrepWorkers...'.format(\n          len(threatintel)))\n      indicators = [item.indicator for item in threatintel]\n      request.recipe['filter_patterns'] = indicators\n\n    request_dict = {\n        'instance': self.instance,\n        'project': self.project,\n        'region': self.turbinia_region,\n        'request_id': request.request_id\n    }\n\n    try:\n      print('Creating Turbinia request {0:s} with Evidence {1!s}'.format(\n          request.request_id, evidence_.name))\n      self.client.send_request(request)\n      print('Waiting for Turbinia request {0:s} to complete'.format(\n          request.request_id))\n      self.client.wait_for_request(**request_dict)\n      task_data = self.client.get_task_data(**request_dict)\n    except TurbiniaException as e:\n      self.state.add_error(e, critical=True)\n      return\n\n    # Turbinia run complete, build a human-readable message of results.\n    message = 'Completed {0:d} Turbinia tasks\\n'.format(len(task_data))\n    for task in task_data:\n      message += '{0!s} ({1!s}): {2!s}\\n'.format(\n          task.get('name'),\n          task.get('id'),\n          task.get('status', 'No task status'))\n      # saved_paths may be set to None\n      for path in task.get('saved_paths') or []:\n        if path.endswith('worker-log.txt'):\n          continue\n        if path.endswith('{0!s}.log'.format(task.get('id'))):\n          continue\n        if path.startswith('/'):\n          continue\n        message += '  {0:s}\\n'.format(path)\n    print(message)\n\n    # Store the message for consumption by any reporting modules.\n    report = containers.Report(module_name='TurbiniaProcessor', text=message)\n    self.state.store_container(report)\n\n    # This finds all .plaso files in the Turbinia output, and determines if they\n    # are local or remote (it's possible this will be running against a local\n    # instance of Turbinia).\n    local_paths = []\n    gs_paths = []\n    timeline_label = '{0:s}-{1:s}'.format(self.project, self.disk_name)\n    for task in task_data:\n      # saved_paths may be set to None\n      for path in task.get('saved_paths') or []:\n        if path.startswith('/') and path.endswith('.plaso'):\n          local_paths.append(path)\n        if path.startswith('gs://') and path.endswith('.plaso'):\n          gs_paths.append(path)\n\n    if not local_paths and not gs_paths:\n      self.state.add_error(\n          'No .plaso files found in Turbinia output.', critical=True)\n      return\n\n    # Any local .plaso files that exist we can add immediately to the output\n    self.state.output = [\n        (timeline_label, p) for p in local_paths if os.path.exists(p)]\n\n    # For files remote in GCS we copy each plaso file back from GCS and then add\n    # to output paths\n    # TODO: Externalize fetching files from GCS buckets to a different module.\n    for path in gs_paths:\n      local_path = None\n      try:\n        output_writer = output_manager.GCSOutputWriter(\n            path, local_output_dir=self._output_path)\n        local_path = output_writer.copy_from(path)\n      except TurbiniaException as e:\n        self.state.add_error(e, critical=True)\n        return\n\n      if local_path:\n        self.state.output.append((timeline_label, local_path))\n\n    if not self.state.output:\n      self.state.add_error('No .plaso files could be found.', critical=True)", "response": "Process files with Turbinia."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate help text with alphabetically sorted recipes.", "response": "def generate_help():\n  \"\"\"Generates help text with alphabetically sorted recipes.\"\"\"\n  help_text = '\\nAvailable recipes:\\n\\n'\n  recipes = config.Config.get_registered_recipes()\n  for contents, _, _ in sorted(recipes, key=lambda k: k[0]['name']):\n    help_text += ' {0:<35s}{1:s}\\n'.format(\n        contents['name'], contents.get('short_description', 'No description'))\n  return help_text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self):\n    # This is not the best way of catching errors, but timesketch_utils will be\n    # deprecated soon.\n    # TODO(tomchop): Consider using the official Timesketch python API.\n    if not self.timesketch_api.session:\n      message = 'Could not connect to Timesketch server'\n      self.state.add_error(message, critical=True)\n\n    named_timelines = []\n    for description, path in self.state.input:\n      if not description:\n        description = 'untitled timeline for '+path\n      named_timelines.append((description, path))\n    try:\n      self.timesketch_api.export_artifacts(named_timelines, self.sketch_id)\n    except RuntimeError as e:\n      self.state.add_error(\n          'Error occurred while working with Timesketch: {0:s}'.format(str(e)),\n          critical=True)\n      return\n    sketch_url = self.timesketch_api.get_sketch_url(self.sketch_id)\n    print('Your Timesketch URL is: {0:s}'.format(sketch_url))\n    self.state.output = sketch_url", "response": "Executes a Timesketch export."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(self, target_directory=None):  # pylint: disable=arguments-differ\n    self._target_directory = target_directory\n    if not target_directory:\n      self._target_directory = tempfile.mkdtemp()\n    elif not os.path.exists(target_directory):\n      try:\n        os.makedirs(target_directory)\n      except OSError as exception:\n        message = 'An unknown error occurred: {0!s}'.format(exception)\n        self.state.add_error(message, critical=True)", "response": "Sets up the _target_directory attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self):\n    for _, path in self.state.input:\n      self._copy_file_or_directory(path, self._target_directory)\n      print('{0:s} -> {1:s}'.format(path, self._target_directory))", "response": "Checks whether the paths exists and updates the state accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing some arguments by those specified by a key-value dictionary. This function will be recursively called on a dictionary looking for any value containing a \"$\" variable. If found, the value will be replaced by the attribute in \"args\" of the same name. It is used to load arguments from the CLI and any extra configuration parameters passed in recipes. Args: value: The value of a {key: value} dictionary. This is passed recursively and may change in nature: string, list, or dict. The top-level variable should be the dictionary that is supposed to be recursively traversed. args: A {key: value} dictionary used to do replacements. config: A dftimewolf.Config class containing configuration information Returns: The first caller of the function will receive a dictionary in which strings starting with \"@\" are replaced by the parameters in args.", "response": "def import_args_from_dict(value, args, config):\n  \"\"\"Replaces some arguments by those specified by a key-value dictionary.\n\n  This function will be recursively called on a dictionary looking for any\n  value containing a \"$\" variable. If found, the value will be replaced\n  by the attribute in \"args\" of the same name.\n\n  It is used to load arguments from the CLI and any extra configuration\n  parameters passed in recipes.\n\n  Args:\n    value: The value of a {key: value} dictionary. This is passed recursively\n        and may change in nature: string, list, or dict. The top-level variable\n        should be the dictionary that is supposed to be recursively traversed.\n    args: A {key: value} dictionary used to do replacements.\n    config: A dftimewolf.Config class containing configuration information\n\n  Returns:\n    The first caller of the function will receive a dictionary in which strings\n    starting with \"@\" are replaced by the parameters in args.\n  \"\"\"\n  if isinstance(value, six.string_types):\n    for match in TOKEN_REGEX.finditer(str(value)):\n      token = match.group(1)\n      if token in args:\n        actual_param = args[token]\n        if isinstance(actual_param, six.string_types):\n          value = value.replace(\"@\"+token, args[token])\n        else:\n          value = actual_param\n  elif isinstance(value, list):\n    return [import_args_from_dict(item, args, config) for item in value]\n  elif isinstance(value, dict):\n    return {\n        key: import_args_from_dict(val, args, config)\n        for key, val in value.items()\n    }\n  elif isinstance(value, tuple):\n    return tuple(import_args_from_dict(val, args, config) for val in value)\n  return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if any values in a given dictionary still contain @ parameters.", "response": "def check_placeholders(value):\n  \"\"\"Checks if any values in a given dictionary still contain @ parameters.\n\n  Args:\n    value: Dictionary, list, or string that will be recursively checked for\n        placeholders\n\n  Raises:\n    ValueError: There still exists a value with an @ parameter.\n\n  Returns:\n    Top-level caller: a modified dict with replaced tokens.\n    Recursive caller: a modified object with replaced tokens.\n  \"\"\"\n  if isinstance(value, six.string_types):\n    if TOKEN_REGEX.search(value):\n      raise ValueError('{0:s} must be replaced in dictionary'.format(value))\n  elif isinstance(value, list):\n    return [check_placeholders(item) for item in value]\n  elif isinstance(value, dict):\n    return {key: check_placeholders(val) for key, val in value.items()}\n  elif isinstance(value, tuple):\n    return tuple(check_placeholders(val) for val in value)\n  return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _negotiate_value(response):\n    if hasattr(_negotiate_value, 'regex'):\n        regex = _negotiate_value.regex\n    else:\n        # There's no need to re-compile this EVERY time it is called. Compile\n        # it once and you won't have the performance hit of the compilation.\n        regex = re.compile('(?:.*,)*\\s*Negotiate\\s*([^,]*),?', re.I)\n        _negotiate_value.regex = regex\n\n    authreq = response.headers.get('www-authenticate', None)\n\n    if authreq:\n        match_obj = regex.search(authreq)\n        if match_obj:\n            return match_obj.group(1)\n\n    return None", "response": "Extracts the gssapi authentication token from the appropriate header"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_channel_bindings_application_data(response):\n\n    application_data = None\n    raw_response = response.raw\n\n    if isinstance(raw_response, HTTPResponse):\n        try:\n            if sys.version_info > (3, 0):\n                socket = raw_response._fp.fp.raw._sock\n            else:\n                socket = raw_response._fp.fp._sock\n        except AttributeError:\n            warnings.warn(\"Failed to get raw socket for CBT; has urllib3 impl changed\",\n                          NoCertificateRetrievedWarning)\n        else:\n            try:\n                server_certificate = socket.getpeercert(True)\n            except AttributeError:\n                pass\n            else:\n                certificate_hash = _get_certificate_hash(server_certificate)\n                application_data = b'tls-server-end-point:' + certificate_hash\n    else:\n        warnings.warn(\n            \"Requests is running with a non urllib3 backend, cannot retrieve server certificate for CBT\",\n            NoCertificateRetrievedWarning)\n\n    return application_data", "response": "Get the application_data field of the Kerberos auth response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_request_header(self, response, host, is_preemptive=False):\n\n        # Flags used by kerberos module.\n        gssflags = kerberos.GSS_C_MUTUAL_FLAG | kerberos.GSS_C_SEQUENCE_FLAG\n        if self.delegate:\n            gssflags |= kerberos.GSS_C_DELEG_FLAG\n\n        try:\n            kerb_stage = \"authGSSClientInit()\"\n            # contexts still need to be stored by host, but hostname_override\n            # allows use of an arbitrary hostname for the kerberos exchange\n            # (eg, in cases of aliased hosts, internal vs external, CNAMEs\n            # w/ name-based HTTP hosting)\n            kerb_host = self.hostname_override if self.hostname_override is not None else host\n            kerb_spn = \"{0}@{1}\".format(self.service, kerb_host)\n\n            result, self.context[host] = kerberos.authGSSClientInit(kerb_spn,\n                gssflags=gssflags, principal=self.principal)\n\n            if result < 1:\n                raise EnvironmentError(result, kerb_stage)\n\n            # if we have a previous response from the server, use it to continue\n            # the auth process, otherwise use an empty value\n            negotiate_resp_value = '' if is_preemptive else _negotiate_value(response)\n\n            kerb_stage = \"authGSSClientStep()\"\n            # If this is set pass along the struct to Kerberos\n            if self.cbt_struct:\n                result = kerberos.authGSSClientStep(self.context[host],\n                                                    negotiate_resp_value,\n                                                    channel_bindings=self.cbt_struct)\n            else:\n                result = kerberos.authGSSClientStep(self.context[host],\n                                                    negotiate_resp_value)\n\n            if result < 0:\n                raise EnvironmentError(result, kerb_stage)\n\n            kerb_stage = \"authGSSClientResponse()\"\n            gss_response = kerberos.authGSSClientResponse(self.context[host])\n\n            return \"Negotiate {0}\".format(gss_response)\n\n        except kerberos.GSSError as error:\n            log.exception(\n                \"generate_request_header(): {0} failed:\".format(kerb_stage))\n            log.exception(error)\n            raise KerberosExchangeError(\"%s failed: %s\" % (kerb_stage, str(error.args)))\n\n        except EnvironmentError as error:\n            # ensure we raised this for translation to KerberosExchangeError\n            # by comparing errno to result, re-raise if not\n            if error.errno != result:\n                raise\n            message = \"{0} failed, result: {1}\".format(kerb_stage, result)\n            log.error(\"generate_request_header(): {0}\".format(message))\n            raise KerberosExchangeError(message)", "response": "Generates the request header for the given response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles user authentication with gssapi / kerberos", "response": "def authenticate_user(self, response, **kwargs):\n        \"\"\"Handles user authentication with gssapi/kerberos\"\"\"\n\n        host = urlparse(response.url).hostname\n\n        try:\n            auth_header = self.generate_request_header(response, host)\n        except KerberosExchangeError:\n            # GSS Failure, return existing response\n            return response\n\n        log.debug(\"authenticate_user(): Authorization header: {0}\".format(\n            auth_header))\n        response.request.headers['Authorization'] = auth_header\n\n        # Consume the content so we can reuse the connection for the next\n        # request.\n        response.content\n        response.raw.release_conn()\n\n        _r = response.connection.send(response.request, **kwargs)\n        _r.history.append(response)\n\n        log.debug(\"authenticate_user(): returning {0}\".format(_r))\n        return _r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_401(self, response, **kwargs):\n\n        log.debug(\"handle_401(): Handling: 401\")\n        if _negotiate_value(response) is not None:\n            _r = self.authenticate_user(response, **kwargs)\n            log.debug(\"handle_401(): returning {0}\".format(_r))\n            return _r\n        else:\n            log.debug(\"handle_401(): Kerberos is not supported\")\n            log.debug(\"handle_401(): returning {0}\".format(response))\n            return response", "response": "Handles 401 s and attempts to use gssapi authentication"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling all responses with the exception of 401s. This is necessary so that we can authenticate the user if required.", "response": "def handle_other(self, response):\n        \"\"\"Handles all responses with the exception of 401s.\n\n        This is necessary so that we can authenticate responses if requested\"\"\"\n\n        log.debug(\"handle_other(): Handling: %d\" % response.status_code)\n\n        if self.mutual_authentication in (REQUIRED, OPTIONAL) and not self.auth_done:\n\n            is_http_error = response.status_code >= 400\n\n            if _negotiate_value(response) is not None:\n                log.debug(\"handle_other(): Authenticating the server\")\n                if not self.authenticate_server(response):\n                    # Mutual authentication failure when mutual auth is wanted,\n                    # raise an exception so the user doesn't use an untrusted\n                    # response.\n                    log.error(\"handle_other(): Mutual authentication failed\")\n                    raise MutualAuthenticationError(\"Unable to authenticate \"\n                                                    \"{0}\".format(response))\n\n                # Authentication successful\n                log.debug(\"handle_other(): returning {0}\".format(response))\n                self.auth_done = True\n                return response\n\n            elif is_http_error or self.mutual_authentication == OPTIONAL:\n                if not response.ok:\n                    log.error(\"handle_other(): Mutual authentication unavailable \"\n                              \"on {0} response\".format(response.status_code))\n\n                if(self.mutual_authentication == REQUIRED and\n                       self.sanitize_mutual_error_response):\n                    return SanitizedResponse(response)\n                else:\n                    return response\n            else:\n                # Unable to attempt mutual authentication when mutual auth is\n                # required, raise an exception so the user doesn't use an\n                # untrusted response.\n                log.error(\"handle_other(): Mutual authentication failed\")\n                raise MutualAuthenticationError(\"Unable to authenticate \"\n                                                \"{0}\".format(response))\n        else:\n            log.debug(\"handle_other(): returning {0}\".format(response))\n            return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate_server(self, response):\n\n        log.debug(\"authenticate_server(): Authenticate header: {0}\".format(\n            _negotiate_value(response)))\n\n        host = urlparse(response.url).hostname\n\n        try:\n            # If this is set pass along the struct to Kerberos\n            if self.cbt_struct:\n                result = kerberos.authGSSClientStep(self.context[host],\n                                                    _negotiate_value(response),\n                                                    channel_bindings=self.cbt_struct)\n            else:\n                result = kerberos.authGSSClientStep(self.context[host],\n                                                    _negotiate_value(response))\n        except kerberos.GSSError:\n            log.exception(\"authenticate_server(): authGSSClientStep() failed:\")\n            return False\n\n        if result < 1:\n            log.error(\"authenticate_server(): authGSSClientStep() failed: \"\n                      \"{0}\".format(result))\n            return False\n\n        log.debug(\"authenticate_server(): returning {0}\".format(response))\n        return True", "response": "Uses GSSAPI to authenticate the server.\n\n        Returns True on success, False on failure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking the given response and tries kerberos - auth as needed.", "response": "def handle_response(self, response, **kwargs):\n        \"\"\"Takes the given response and tries kerberos-auth, as needed.\"\"\"\n        num_401s = kwargs.pop('num_401s', 0)\n\n        # Check if we have already tried to get the CBT data value\n        if not self.cbt_binding_tried and self.send_cbt:\n            # If we haven't tried, try getting it now\n            cbt_application_data = _get_channel_bindings_application_data(response)\n            if cbt_application_data:\n                # Only the latest version of pykerberos has this method available\n                try:\n                    self.cbt_struct = kerberos.channelBindings(application_data=cbt_application_data)\n                except AttributeError:\n                    # Using older version set to None\n                    self.cbt_struct = None\n            # Regardless of the result, set tried to True so we don't waste time next time\n            self.cbt_binding_tried = True\n\n        if self.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            response.request.body.seek(self.pos)\n\n        if response.status_code == 401 and num_401s < 2:\n            # 401 Unauthorized. Handle it, and if it still comes back as 401,\n            # that means authentication failed.\n            _r = self.handle_401(response, **kwargs)\n            log.debug(\"handle_response(): returning %s\", _r)\n            log.debug(\"handle_response() has seen %d 401 responses\", num_401s)\n            num_401s += 1\n            return self.handle_response(_r, num_401s=num_401s, **kwargs)\n        elif response.status_code == 401 and num_401s >= 2:\n            # Still receiving 401 responses after attempting to handle them.\n            # Authentication has failed. Return the 401 response.\n            log.debug(\"handle_response(): returning 401 %s\", response)\n            return response\n        else:\n            _r = self.handle_other(response)\n            log.debug(\"handle_response(): returning %s\", _r)\n            return _r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _quote_query(query):\n    return \"&\".join(\"%s=%s\" % (\n        k, urllib_quote(\n            unicode(query[k]).encode('utf-8'), safe='~'))\n            for k in sorted(query))", "response": "Turn a dictionary into a query string in a URL with keys\nApps and keys\nVers in alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the URL for making the given query against the API.", "response": "def api_url(self, **kwargs):\n        \"\"\"The URL for making the given query against the API.\"\"\"\n        query = {\n            'Operation': self.Operation,\n            'Service': \"AWSECommerceService\",\n            'Timestamp': time.strftime(\n                \"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n            'Version': self.Version,\n        }\n        query.update(kwargs)\n\n        query['AWSAccessKeyId'] = self.AWSAccessKeyId\n        query['Timestamp'] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\",\n                                           time.gmtime())\n\n        if self.AssociateTag:\n            query['AssociateTag'] = self.AssociateTag\n\n        service_domain = SERVICE_DOMAINS[self.Region][0]\n        quoted_strings = _quote_query(query)\n\n        data = \"GET\\n\" + service_domain + \"\\n/onca/xml\\n\" + quoted_strings\n\n        # convert unicode to UTF8 bytes for hmac library\n        if type(self.AWSSecretAccessKey) is unicode:\n            self.AWSSecretAccessKey = self.AWSSecretAccessKey.encode('utf-8')\n\n        if type(data) is unicode:\n            data = data.encode('utf-8')\n\n        # calculate sha256 signature\n        digest = hmac.new(self.AWSSecretAccessKey, data, sha256).digest()\n\n        # base64 encode and urlencode\n        if sys.version_info[0] == 3:\n            signature = urllib.parse.quote(b64encode(digest))\n        else:\n            signature = urllib.quote(b64encode(digest))\n\n        return (\"https://\" + service_domain + \"/onca/xml?\" +\n                quoted_strings + \"&Signature=%s\" % signature)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the API and returns the response.", "response": "def _call_api(self, api_url, err_env):\n        \"\"\"urlopen(), plus error handling and possible retries.\n\n        err_env is a dict of additional info passed to the error handler\n        \"\"\"\n        while True:  # may retry on error\n            api_request = urllib2.Request(\n                api_url, headers={\"Accept-Encoding\": \"gzip\"})\n\n            log.debug(\"Amazon URL: %s\" % api_url)\n\n            try:\n                if self.Timeout and sys.version[:3] in [\"2.4\", \"2.5\"]:\n                    # urllib2.urlopen() doesn't accept timeout until 2.6\n                    old_timeout = socket.getdefaulttimeout()\n                    try:\n                        socket.setdefaulttimeout(self.Timeout)\n                        return urllib2.urlopen(api_request)\n                    finally:\n                        socket.setdefaulttimeout(old_timeout)\n                else:\n                    # the simple way\n                    return urllib2.urlopen(api_request, timeout=self.Timeout)\n            except:\n                if not self.ErrorHandler:\n                    raise\n\n                exception = sys.exc_info()[1]  # works in Python 2 and 3\n                err = {'exception': exception}\n                err.update(err_env)\n                if not self.ErrorHandler(err):\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isSameTypeWith(self, other, matchTags=True, matchConstraints=True):\n        return (self is other or\n                (not matchTags or self.tagSet == other.tagSet) and\n                (not matchConstraints or self.subtypeSpec == other.subtypeSpec))", "response": "Return True if self is the same type with other."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isSuperTypeOf(self, other, matchTags=True, matchConstraints=True):\n        return (not matchTags or\n                (self.tagSet.isSuperTagSetOf(other.tagSet)) and\n                 (not matchConstraints or self.subtypeSpec.isSuperTypeOf(other.subtypeSpec)))", "response": "Return True if this class is a subtype of other."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(self, value=noValue, **kwargs):\n        if value is noValue:\n            if not kwargs:\n                return self\n\n            value = self._value\n\n        initializers = self.readOnly.copy()\n        initializers.update(kwargs)\n\n        return self.__class__(value, **initializers)", "response": "Create a copy of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subtype(self, value=noValue, **kwargs):\n        if value is noValue:\n            if not kwargs:\n                return self\n\n            value = self._value\n\n        initializers = self.readOnly.copy()\n\n        implicitTag = kwargs.pop('implicitTag', None)\n        if implicitTag is not None:\n            initializers['tagSet'] = self.tagSet.tagImplicitly(implicitTag)\n\n        explicitTag = kwargs.pop('explicitTag', None)\n        if explicitTag is not None:\n            initializers['tagSet'] = self.tagSet.tagExplicitly(explicitTag)\n\n        for arg, option in kwargs.items():\n            initializers[arg] += option\n\n        return self.__class__(value, **initializers)", "response": "Create a specialization of |ASN. 1| schema or value object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(self, **kwargs):\n        cloneValueFlag = kwargs.pop('cloneValueFlag', False)\n\n        initializers = self.readOnly.copy()\n        initializers.update(kwargs)\n\n        clone = self.__class__(**initializers)\n\n        if cloneValueFlag:\n            self._cloneComponentValues(clone, cloneValueFlag)\n\n        return clone", "response": "Create a shallow version of this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a specialization of |ASN. 1| schema object.", "response": "def subtype(self, **kwargs):\n        \"\"\"Create a specialization of |ASN.1| schema object.\n\n        The `subtype()` method accepts the same set arguments as |ASN.1|\n        class takes on instantiation except that all parameters\n        of the `subtype()` method are optional.\n\n        With the exception of the arguments described below, the rest of\n        supplied arguments they are used to create a copy of `self` taking\n        precedence over the ones used to instantiate `self`.\n\n        The following arguments to `subtype()` create a ASN.1 subtype out of\n        |ASN.1| type.\n\n        Other Parameters\n        ----------------\n        implicitTag: :py:class:`~pyasn1.type.tag.Tag`\n            Implicitly apply given ASN.1 tag object to `self`'s\n            :py:class:`~pyasn1.type.tag.TagSet`, then use the result as\n            new object's ASN.1 tag(s).\n\n        explicitTag: :py:class:`~pyasn1.type.tag.Tag`\n            Explicitly apply given ASN.1 tag object to `self`'s\n            :py:class:`~pyasn1.type.tag.TagSet`, then use the result as\n            new object's ASN.1 tag(s).\n\n        subtypeSpec: :py:class:`~pyasn1.type.constraint.ConstraintsIntersection`\n            Add ASN.1 constraints object to one of the `self`'s, then\n            use the result as new object's ASN.1 constraints.\n\n\n        Returns\n        -------\n        :\n            new instance of |ASN.1| type/value\n\n        Note\n        ----\n        Due to the immutable nature of the |ASN.1| object, if no arguments\n        are supplied, no new |ASN.1| object will be created and `self` will\n        be returned instead.\n        \"\"\"\n\n        initializers = self.readOnly.copy()\n\n        cloneValueFlag = kwargs.pop('cloneValueFlag', False)\n\n        implicitTag = kwargs.pop('implicitTag', None)\n        if implicitTag is not None:\n            initializers['tagSet'] = self.tagSet.tagImplicitly(implicitTag)\n\n        explicitTag = kwargs.pop('explicitTag', None)\n        if explicitTag is not None:\n            initializers['tagSet'] = self.tagSet.tagExplicitly(explicitTag)\n\n        for arg, option in kwargs.items():\n            initializers[arg] += option\n\n        clone = self.__class__(**initializers)\n\n        if cloneValueFlag:\n            self._cloneComponentValues(clone, cloneValueFlag)\n\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort SET components by tag", "response": "def _componentSortKey(componentAndType):\n        \"\"\"Sort SET components by tag\n\n        Sort regardless of the Choice value (static sort)\n        \"\"\"\n        component, asn1Spec = componentAndType\n\n        if asn1Spec is None:\n            asn1Spec = component\n\n        if asn1Spec.typeId == univ.Choice.typeId and not asn1Spec.tagSet:\n            if asn1Spec.tagSet:\n                return asn1Spec.tagSet\n            else:\n                return asn1Spec.componentType.minTagSet\n        else:\n            return asn1Spec.tagSet"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _componentSortKey(componentAndType):\n        component, asn1Spec = componentAndType\n\n        if asn1Spec is None:\n            compType = component\n        else:\n            compType = asn1Spec\n\n        if compType.typeId == univ.Choice.typeId and not compType.tagSet:\n            if asn1Spec is None:\n                return component.getComponent().tagSet\n            else:\n                # TODO: move out of sorting key function\n                names = [namedType.name for namedType in asn1Spec.componentType.namedTypes\n                         if namedType.name in component]\n                if len(names) != 1:\n                    raise error.PyAsn1Error(\n                        '%s components for Choice at %r' % (len(names) and 'Multiple ' or 'None ', component))\n\n                # TODO: support nested CHOICE ordering\n                return asn1Spec[names[0]].tagSet\n\n        else:\n            return compType.tagSet", "response": "Sort SET components by tag\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a datetime. datetime object from an ASN. 1 object.", "response": "def asDateTime(self):\n        \"\"\"Create :py:class:`datetime.datetime` object from a |ASN.1| object.\n\n        Returns\n        -------\n        :\n            new instance of :py:class:`datetime.datetime` object\n        \"\"\"\n        text = str(self)\n        if text.endswith('Z'):\n            tzinfo = TimeMixIn.UTC\n            text = text[:-1]\n\n        elif '-' in text or '+' in text:\n            if '+' in text:\n                text, plusminus, tz = string.partition(text, '+')\n            else:\n                text, plusminus, tz = string.partition(text, '-')\n\n            if self._shortTZ and len(tz) == 2:\n                tz += '00'\n\n            if len(tz) != 4:\n                raise error.PyAsn1Error('malformed time zone offset %s' % tz)\n\n            try:\n                minutes = int(tz[:2]) * 60 + int(tz[2:])\n                if plusminus == '-':\n                    minutes *= -1\n\n            except ValueError:\n                raise error.PyAsn1Error('unknown time specification %s' % self)\n\n            tzinfo = TimeMixIn.FixedOffset(minutes, '?')\n\n        else:\n            tzinfo = None\n\n        if '.' in text or ',' in text:\n            if '.' in text:\n                text, _, ms = string.partition(text, '.')\n            else:\n                text, _, ms = string.partition(text, ',')\n\n            try:\n                ms = int(ms) * 1000\n\n            except ValueError:\n                raise error.PyAsn1Error('bad sub-second time specification %s' % self)\n\n        else:\n            ms = 0\n\n        if self._optionalMinutes and len(text) - self._yearsDigits == 6:\n            text += '0000'\n        elif len(text) - self._yearsDigits == 8:\n            text += '00'\n\n        try:\n            dt = dateandtime.strptime(text, self._yearsDigits == 4 and '%Y%m%d%H%M%S' or '%y%m%d%H%M%S')\n\n        except ValueError:\n            raise error.PyAsn1Error('malformed datetime format %s' % self)\n\n        return dt.replace(microsecond=ms, tzinfo=tzinfo)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a |ASN. 1| object from a datetime. datetime object.", "response": "def fromDateTime(cls, dt):\n        \"\"\"Create |ASN.1| object from a :py:class:`datetime.datetime` object.\n\n        Parameters\n        ----------\n        dt: :py:class:`datetime.datetime` object\n            The `datetime.datetime` object to initialize the |ASN.1| object\n            from\n\n        Returns\n        -------\n        :\n            new instance of |ASN.1| value\n        \"\"\"\n        text = dt.strftime(cls._yearsDigits == 4 and '%Y%m%d%H%M%S' or '%y%m%d%H%M%S')\n        if cls._hasSubsecond:\n            text += '.%d' % (dt.microsecond // 1000)\n\n        if dt.utcoffset():\n            seconds = dt.utcoffset().seconds\n            if seconds < 0:\n                text += '-'\n            else:\n                text += '+'\n            text += '%.2d%.2d' % (seconds // 3600, seconds % 3600)\n        else:\n            text += 'Z'\n\n        return cls(text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning ASN. 1 type object by its position in fields set.", "response": "def getTypeByPosition(self, idx):\n        \"\"\"Return ASN.1 type object by its position in fields set.\n\n        Parameters\n        ----------\n        idx: :py:class:`int`\n            Field index\n\n        Returns\n        -------\n        :\n            ASN.1 type\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If given position is out of fields range\n        \"\"\"\n        try:\n            return self.__namedTypes[idx].asn1Object\n\n        except IndexError:\n            raise error.PyAsn1Error('Type position out of range')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the field position by its ASN. 1 type.", "response": "def getPositionByType(self, tagSet):\n        \"\"\"Return field position by its ASN.1 type.\n\n        Parameters\n        ----------\n        tagSet: :class:`~pysnmp.type.tag.TagSet`\n            ASN.1 tag set distinguishing one ASN.1 type from others.\n\n        Returns\n        -------\n        : :py:class:`int`\n            ASN.1 type position in fields set\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If *tagSet* is not present or ASN.1 types are not unique within callee *NamedTypes*\n        \"\"\"\n        try:\n            return self.__tagToPosMap[tagSet]\n\n        except KeyError:\n            raise error.PyAsn1Error('Type %s not found' % (tagSet,))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning field name by its position in fields set.", "response": "def getNameByPosition(self, idx):\n        \"\"\"Return field name by its position in fields set.\n\n        Parameters\n        ----------\n        idx: :py:class:`idx`\n            Field index\n\n        Returns\n        -------\n        : :py:class:`str`\n            Field name\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If given field name is not present in callee *NamedTypes*\n        \"\"\"\n        try:\n            return self.__namedTypes[idx].name\n\n        except IndexError:\n            raise error.PyAsn1Error('Type position out of range')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the field position by filed name.", "response": "def getPositionByName(self, name):\n        \"\"\"Return field position by filed name.\n\n        Parameters\n        ----------\n        name: :py:class:`str`\n            Field name\n\n        Returns\n        -------\n        : :py:class:`int`\n            Field position in fields set\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If *name* is not present or not unique within callee *NamedTypes*\n        \"\"\"\n        try:\n            return self.__nameToPosMap[name]\n\n        except KeyError:\n            raise error.PyAsn1Error('Name %s not found' % (name,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ASN. 1 types that are allowed at or past given field position.", "response": "def getTagMapNearPosition(self, idx):\n        \"\"\"Return ASN.1 types that are allowed at or past given field position.\n\n        Some ASN.1 serialisation allow for skipping optional and defaulted fields.\n        Some constructed ASN.1 types allow reordering of the fields. When recovering\n        such objects it may be important to know which types can possibly be\n        present at any given position in the field sets.\n\n        Parameters\n        ----------\n        idx: :py:class:`int`\n            Field index\n\n        Returns\n        -------\n        : :class:`~pyasn1.type.tagmap.TagMap`\n            Map if ASN.1 types allowed at given field position\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If given position is out of fields range\n        \"\"\"\n        try:\n            return self.__ambiguousTypes[idx].tagMap\n\n        except KeyError:\n            raise error.PyAsn1Error('Type position out of range')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the closest field position where given ASN. 1 type is allowed.", "response": "def getPositionNearType(self, tagSet, idx):\n        \"\"\"Return the closest field position where given ASN.1 type is allowed.\n\n        Some ASN.1 serialisation allow for skipping optional and defaulted fields.\n        Some constructed ASN.1 types allow reordering of the fields. When recovering\n        such objects it may be important to know at which field position, in field set,\n        given *tagSet* is allowed at or past *idx* position.\n\n        Parameters\n        ----------\n        tagSet: :class:`~pyasn1.type.tag.TagSet`\n           ASN.1 type which field position to look up\n\n        idx: :py:class:`int`\n            Field position at or past which to perform ASN.1 type look up\n\n        Returns\n        -------\n        : :py:class:`int`\n            Field position in fields set\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If *tagSet* is not present or not unique within callee *NamedTypes*\n            or *idx* is out of fields range\n        \"\"\"\n        try:\n            return idx + self.__ambiguousTypes[idx].getPositionByType(tagSet)\n\n        except KeyError:\n            raise error.PyAsn1Error('Type position out of range')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tagExplicitly(self, superTag):\n        if superTag.tagClass == tagClassUniversal:\n            raise error.PyAsn1Error(\"Can't tag with UNIVERSAL class tag\")\n        if superTag.tagFormat != tagFormatConstructed:\n            superTag = Tag(superTag.tagClass, tagFormatConstructed, superTag.tagId)\n        return self + superTag", "response": "Return a new TagSet representing this object with the specified tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns implicitly tagged TagSet with passed tag.", "response": "def tagImplicitly(self, superTag):\n        \"\"\"Return implicitly tagged *TagSet*\n\n        Create a new *TagSet* representing callee *TagSet* implicitly tagged\n        with passed tag(s). With implicit tagging mode, new tag(s) replace the\n        last existing tag.\n\n        Parameters\n        ----------\n        superTag: :class:`~pyasn1.type.tag.Tag`\n            *Tag* object to tag this *TagSet*\n\n        Returns\n        -------\n        : :class:`~pyasn1.type.tag.TagSet`\n            New *TagSet* object\n        \"\"\"\n        if self.__superTags:\n            superTag = Tag(superTag.tagClass, self.__superTags[-1].tagFormat, superTag.tagId)\n        return self[:-1] + superTag"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntest type relationship against given TagSet.", "response": "def isSuperTagSetOf(self, tagSet):\n        \"\"\"Test type relationship against given *TagSet*\n\n        The callee is considered to be a supertype of given *TagSet*\n        tag-wise if all tags in *TagSet* are present in the callee and\n        they are in the same order.\n\n        Parameters\n        ----------\n        tagSet: :class:`~pyasn1.type.tag.TagSet`\n            *TagSet* object to evaluate against the callee\n\n        Returns\n        -------\n        : :py:class:`bool`\n            `True` if callee is a supertype of *tagSet*\n        \"\"\"\n        if len(tagSet) < self.__lenOfSuperTags:\n            return False\n        return self.__superTags == tagSet[:self.__lenOfSuperTags]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef asBinary(self):\n        binString = binary.bin(self._value)[2:]\n        return '0' * (len(self._value) - len(binString)) + binString", "response": "Get |ASN. 1| value as a text string of bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a |ASN. 1| object initialized from the hex string.", "response": "def fromHexString(cls, value, internalFormat=False, prepend=None):\n        \"\"\"Create a |ASN.1| object initialized from the hex string.\n\n        Parameters\n        ----------\n        value: :class:`str`\n            Text string like 'DEADBEEF'\n        \"\"\"\n        try:\n            value = SizedInteger(value, 16).setBitLength(len(value) * 4)\n\n        except ValueError:\n            raise error.PyAsn1Error('%s.fromHexString() error: %s' % (cls.__name__, sys.exc_info()[1]))\n\n        if prepend is not None:\n            value = SizedInteger(\n                (SizedInteger(prepend) << len(value)) | value\n            ).setBitLength(len(prepend) + len(value))\n\n        if not internalFormat:\n            value = cls(value)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a |ASN. 1| object initialized from a string.", "response": "def fromOctetString(cls, value, internalFormat=False, prepend=None, padding=0):\n        \"\"\"Create a |ASN.1| object initialized from a string.\n\n        Parameters\n        ----------\n        value: :class:`str` (Py2) or :class:`bytes` (Py3)\n            Text string like '\\\\\\\\x01\\\\\\\\xff' (Py2) or b'\\\\\\\\x01\\\\\\\\xff' (Py3)\n        \"\"\"\n        value = SizedInteger(integer.from_bytes(value) >> padding).setBitLength(len(value) * 8 - padding)\n\n        if prepend is not None:\n            value = SizedInteger(\n                (SizedInteger(prepend) << len(value)) | value\n            ).setBitLength(len(prepend) + len(value))\n\n        if not internalFormat:\n            value = cls(value)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromBinaryString(value):\n        bitNo = 8\n        byte = 0\n        r = []\n        for v in value:\n            if bitNo:\n                bitNo -= 1\n            else:\n                bitNo = 7\n                r.append(byte)\n                byte = 0\n            if v in ('0', '1'):\n                v = int(v)\n            else:\n                raise error.PyAsn1Error(\n                    'Non-binary OCTET STRING initializer %s' % (v,)\n                )\n            byte |= v << bitNo\n\n        r.append(byte)\n\n        return octets.ints2octs(r)", "response": "Create a |ASN. 1| object initialized from a binary string like 1010111."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a |ASN. 1| object initialized from the hex string.", "response": "def fromHexString(value):\n        \"\"\"Create a |ASN.1| object initialized from the hex string.\n\n        Parameters\n        ----------\n        value: :class:`str`\n            Text string like 'DEADBEEF'\n        \"\"\"\n        r = []\n        p = []\n        for v in value:\n            if p:\n                r.append(int(p + v, 16))\n                p = None\n            else:\n                p = v\n        if p:\n            r.append(int(p + '0', 16))\n\n        return octets.ints2octs(r)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isPrefixOf(self, other):\n        l = len(self)\n        if l <= len(other):\n            if self._value[:l] == other[:l]:\n                return True\n        return False", "response": "Indicate if this object is a prefix of other object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getComponentByPosition(self, idx, default=noValue, instantiate=True):\n        try:\n            componentValue = self._componentValues[idx]\n\n        except IndexError:\n            if not instantiate:\n                return default\n\n            self.setComponentByPosition(idx)\n\n            componentValue = self._componentValues[idx]\n\n        if default is noValue or componentValue.isValue:\n            return componentValue\n        else:\n            return default", "response": "Return |ASN. 1| type component value by position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign a value to a type component by position.", "response": "def setComponentByPosition(self, idx, value=noValue,\n                               verifyConstraints=True,\n                               matchTags=True,\n                               matchConstraints=True):\n        \"\"\"Assign |ASN.1| type component by position.\n\n        Equivalent to Python sequence item assignment operation (e.g. `[]`)\n        or list.append() (when idx == len(self)).\n\n        Parameters\n        ----------\n        idx: :class:`int`\n            Component index (zero-based). Must either refer to existing\n            component or to N+1 component. In the latter case a new component\n            type gets instantiated (if *componentType* is set, or given ASN.1\n            object is taken otherwise) and appended to the |ASN.1| sequence.\n\n        Keyword Args\n        ------------\n        value: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n            A Python value to initialize |ASN.1| component with (if *componentType* is set)\n            or ASN.1 value object to assign to |ASN.1| component.\n\n        verifyConstraints: :class:`bool`\n             If `False`, skip constraints validation\n\n        matchTags: :class:`bool`\n             If `False`, skip component tags matching\n\n        matchConstraints: :class:`bool`\n             If `False`, skip component constraints matching\n\n        Returns\n        -------\n        self\n\n        Raises\n        ------\n        IndexError:\n            When idx > len(self)\n        \"\"\"\n        componentType = self.componentType\n\n        try:\n            currentValue = self._componentValues[idx]\n        except IndexError:\n            currentValue = noValue\n\n            if len(self._componentValues) < idx:\n                raise error.PyAsn1Error('Component index out of range')\n\n        if value is noValue:\n            if componentType is not None:\n                value = componentType.clone()\n            elif currentValue is noValue:\n                raise error.PyAsn1Error('Component type not defined')\n        elif not isinstance(value, base.Asn1Item):\n            if componentType is not None and isinstance(componentType, base.AbstractSimpleAsn1Item):\n                value = componentType.clone(value=value)\n            elif currentValue is not noValue and isinstance(currentValue, base.AbstractSimpleAsn1Item):\n                value = currentValue.clone(value=value)\n            else:\n                raise error.PyAsn1Error('Non-ASN.1 value %r and undefined component type at %r' % (value, self))\n        elif componentType is not None:\n            if self.strictConstraints:\n                if not componentType.isSameTypeWith(value, matchTags, matchConstraints):\n                    raise error.PyAsn1Error('Component value is tag-incompatible: %r vs %r' % (value, componentType))\n            else:\n                if not componentType.isSuperTypeOf(value, matchTags, matchConstraints):\n                    raise error.PyAsn1Error('Component value is tag-incompatible: %r vs %r' % (value, componentType))\n\n        if verifyConstraints and value.isValue:\n            try:\n                self.subtypeSpec(value, idx)\n\n            except error.PyAsn1Error:\n                exType, exValue, exTb = sys.exc_info()\n                raise exType('%s at %s' % (exValue, self.__class__.__name__))\n\n        if currentValue is noValue:\n            self._componentValues.append(value)\n        else:\n            self._componentValues[idx] = value\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isValue(self):\n        for componentValue in self._componentValues:\n            if componentValue is noValue or not componentValue.isValue:\n                return False\n\n        return True", "response": "Indicate that |ASN. 1| object represents a value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getComponentByName(self, name, default=noValue, instantiate=True):\n        if self._componentTypeLen:\n            idx = self.componentType.getPositionByName(name)\n        else:\n            try:\n                idx = self._dynamicNames.getPositionByName(name)\n\n            except KeyError:\n                raise error.PyAsn1Error('Name %s not found' % (name,))\n\n        return self.getComponentByPosition(idx, default=default, instantiate=instantiate)", "response": "Returns |ASN. 1| type component by name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign |ASN. 1| type component by name.", "response": "def setComponentByName(self, name, value=noValue,\n                           verifyConstraints=True,\n                           matchTags=True,\n                           matchConstraints=True):\n        \"\"\"Assign |ASN.1| type component by name.\n\n        Equivalent to Python :class:`dict` item assignment operation (e.g. `[]`).\n\n        Parameters\n        ----------\n        name: :class:`str`\n            |ASN.1| type component name\n\n        Keyword Args\n        ------------\n        value: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n            A Python value to initialize |ASN.1| component with (if *componentType* is set)\n            or ASN.1 value object to assign to |ASN.1| component.\n\n        verifyConstraints: :class:`bool`\n             If `False`, skip constraints validation\n\n        matchTags: :class:`bool`\n             If `False`, skip component tags matching\n\n        matchConstraints: :class:`bool`\n             If `False`, skip component constraints matching\n\n        Returns\n        -------\n        self\n        \"\"\"\n        if self._componentTypeLen:\n            idx = self.componentType.getPositionByName(name)\n        else:\n            try:\n                idx = self._dynamicNames.getPositionByName(name)\n\n            except KeyError:\n                raise error.PyAsn1Error('Name %s not found' % (name,))\n\n        return self.setComponentByPosition(\n            idx, value, verifyConstraints, matchTags, matchConstraints\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign |ASN. 1| type component by position.", "response": "def setComponentByPosition(self, idx, value=noValue,\n                               verifyConstraints=True,\n                               matchTags=True,\n                               matchConstraints=True):\n        \"\"\"Assign |ASN.1| type component by position.\n\n        Equivalent to Python sequence item assignment operation (e.g. `[]`).\n\n        Parameters\n        ----------\n        idx : :class:`int`\n            Component index (zero-based). Must either refer to existing\n            component (if *componentType* is set) or to N+1 component\n            otherwise. In the latter case a new component of given ASN.1\n            type gets instantiated and appended to |ASN.1| sequence.\n\n        Keyword Args\n        ------------\n        value: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n            A Python value to initialize |ASN.1| component with (if *componentType* is set)\n            or ASN.1 value object to assign to |ASN.1| component.\n\n        verifyConstraints : :class:`bool`\n             If `False`, skip constraints validation\n\n        matchTags: :class:`bool`\n             If `False`, skip component tags matching\n\n        matchConstraints: :class:`bool`\n             If `False`, skip component constraints matching\n\n        Returns\n        -------\n        self\n        \"\"\"\n        componentType = self.componentType\n        componentTypeLen = self._componentTypeLen\n\n        try:\n            currentValue = self._componentValues[idx]\n\n        except IndexError:\n            currentValue = noValue\n            if componentTypeLen:\n                if componentTypeLen < idx:\n                    raise error.PyAsn1Error('component index out of range')\n\n                self._componentValues = [noValue] * componentTypeLen\n\n        if value is noValue:\n            if componentTypeLen:\n                value = componentType.getTypeByPosition(idx)\n                if isinstance(value, base.AbstractConstructedAsn1Item):\n                    value = value.clone(cloneValueFlag=componentType[idx].isDefaulted)\n\n            elif currentValue is noValue:\n                raise error.PyAsn1Error('Component type not defined')\n\n        elif not isinstance(value, base.Asn1Item):\n            if componentTypeLen:\n                subComponentType = componentType.getTypeByPosition(idx)\n                if isinstance(subComponentType, base.AbstractSimpleAsn1Item):\n                    value = subComponentType.clone(value=value)\n\n                else:\n                    raise error.PyAsn1Error('%s can cast only scalar values' % componentType.__class__.__name__)\n\n            elif currentValue is not noValue and isinstance(currentValue, base.AbstractSimpleAsn1Item):\n                value = currentValue.clone(value=value)\n\n            else:\n                raise error.PyAsn1Error('%s undefined component type' % componentType.__class__.__name__)\n\n        elif (matchTags or matchConstraints) and componentTypeLen:\n            subComponentType = componentType.getTypeByPosition(idx)\n            if subComponentType is not noValue:\n                subtypeChecker = (self.strictConstraints and\n                                  subComponentType.isSameTypeWith or\n                                  subComponentType.isSuperTypeOf)\n\n                if not subtypeChecker(value, matchTags, matchConstraints):\n                    if not componentType[idx].openType:\n                        raise error.PyAsn1Error('Component value is tag-incompatible: %r vs %r' % (value, componentType))\n\n        if verifyConstraints and value.isValue:\n            try:\n                self.subtypeSpec(value, idx)\n\n            except error.PyAsn1Error:\n                exType, exValue, exTb = sys.exc_info()\n                raise exType('%s at %s' % (exValue, self.__class__.__name__))\n\n        if componentTypeLen or idx in self._dynamicNames:\n            self._componentValues[idx] = value\n\n        elif len(self._componentValues) == idx:\n            self._componentValues.append(value)\n            self._dynamicNames.addField(idx)\n\n        else:\n            raise error.PyAsn1Error('Component index out of range')\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nindicate that |ASN. 1| object represents a value.", "response": "def isValue(self):\n        \"\"\"Indicate that |ASN.1| object represents ASN.1 value.\n\n        If *isValue* is `False` then this object represents just ASN.1 schema.\n\n        If *isValue* is `True` then, in addition to its ASN.1 schema features,\n        this object can also be used like a Python built-in object (e.g. `int`,\n        `str`, `dict` etc.).\n\n        Returns\n        -------\n        : :class:`bool`\n            :class:`False` if object represents just ASN.1 schema.\n            :class:`True` if object represents ASN.1 schema and can be used as a normal value.\n\n        Note\n        ----\n        There is an important distinction between PyASN1 schema and value objects.\n        The PyASN1 schema objects can only participate in ASN.1 schema-related\n        operations (e.g. defining or testing the structure of the data). Most\n        obvious uses of ASN.1 schema is to guide serialisation codecs whilst\n        encoding/decoding serialised ASN.1 contents.\n\n        The PyASN1 value objects can **additionally** participate in many operations\n        involving regular Python objects (e.g. arithmetic, comprehension etc).\n        \"\"\"\n        componentType = self.componentType\n\n        if componentType:\n            for idx, subComponentType in enumerate(componentType.namedTypes):\n                if subComponentType.isDefaulted or subComponentType.isOptional:\n                    continue\n\n                if not self._componentValues:\n                    return False\n\n                componentValue = self._componentValues[idx]\n                if componentValue is noValue or not componentValue.isValue:\n                    return False\n\n        else:\n            for componentValue in self._componentValues:\n                if componentValue is noValue or not componentValue.isValue:\n                    return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prettyPrint(self, scope=0):\n        scope += 1\n        representation = self.__class__.__name__ + ':\\n'\n        for idx, componentValue in enumerate(self._componentValues):\n            if componentValue is not noValue and componentValue.isValue:\n                representation += ' ' * scope\n                if self.componentType:\n                    representation += self.componentType.getNameByPosition(idx)\n                else:\n                    representation += self._dynamicNames.getNameByPosition(idx)\n                representation = '%s=%s\\n' % (\n                    representation, componentValue.prettyPrint(scope)\n                )\n        return representation", "response": "Return an object representation string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getComponentByType(self, tagSet, default=noValue,\n                           instantiate=True, innerFlag=False):\n        \"\"\"Returns |ASN.1| type component by ASN.1 tag.\n\n        Parameters\n        ----------\n        tagSet : :py:class:`~pyasn1.type.tag.TagSet`\n            Object representing ASN.1 tags to identify one of\n            |ASN.1| object component\n\n        Keyword Args\n        ------------\n        default: :class:`object`\n            If set and requested component is a schema object, return the `default`\n            object instead of the requested component.\n\n        instantiate: :class:`bool`\n            If `True` (default), inner component will be automatically instantiated.\n            If 'False' either existing component or the `noValue` object will be\n            returned.\n\n        Returns\n        -------\n        : :py:class:`~pyasn1.type.base.PyAsn1Item`\n            a pyasn1 object\n        \"\"\"\n        componentValue = self.getComponentByPosition(\n            self.componentType.getPositionByType(tagSet),\n            default=default, instantiate=instantiate\n        )\n        if innerFlag and isinstance(componentValue, Set):\n            # get inner component by inner tagSet\n            return componentValue.getComponent(innerFlag=True)\n        else:\n            # get outer component by inner tagSet\n            return componentValue", "response": "Returns |ASN. 1| type component by tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassigning |ASN. 1| type component by tagSet.", "response": "def setComponentByType(self, tagSet, value=noValue,\n                           verifyConstraints=True,\n                           matchTags=True,\n                           matchConstraints=True,\n                           innerFlag=False):\n        \"\"\"Assign |ASN.1| type component by ASN.1 tag.\n\n        Parameters\n        ----------\n        tagSet : :py:class:`~pyasn1.type.tag.TagSet`\n            Object representing ASN.1 tags to identify one of\n            |ASN.1| object component\n\n        Keyword Args\n        ------------\n        value: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n            A Python value to initialize |ASN.1| component with (if *componentType* is set)\n            or ASN.1 value object to assign to |ASN.1| component.\n\n        verifyConstraints : :class:`bool`\n            If `False`, skip constraints validation\n\n        matchTags: :class:`bool`\n            If `False`, skip component tags matching\n\n        matchConstraints: :class:`bool`\n            If `False`, skip component constraints matching\n\n        innerFlag: :class:`bool`\n            If `True`, search for matching *tagSet* recursively.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        idx = self.componentType.getPositionByType(tagSet)\n\n        if innerFlag:  # set inner component by inner tagSet\n            componentType = self.componentType.getTypeByPosition(idx)\n\n            if componentType.tagSet:\n                return self.setComponentByPosition(\n                    idx, value, verifyConstraints, matchTags, matchConstraints\n                )\n            else:\n                componentType = self.getComponentByPosition(idx)\n                return componentType.setComponentByType(\n                    tagSet, value, verifyConstraints, matchTags, matchConstraints, innerFlag=innerFlag\n                )\n        else:  # set outer component by inner tagSet\n            return self.setComponentByPosition(\n                idx, value, verifyConstraints, matchTags, matchConstraints\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setComponentByPosition(self, idx, value=noValue,\n                               verifyConstraints=True,\n                               matchTags=True,\n                               matchConstraints=True):\n        \"\"\"Assign |ASN.1| type component by position.\n\n        Equivalent to Python sequence item assignment operation (e.g. `[]`).\n\n        Parameters\n        ----------\n        idx: :class:`int`\n            Component index (zero-based). Must either refer to existing\n            component or to N+1 component. In the latter case a new component\n            type gets instantiated (if *componentType* is set, or given ASN.1\n            object is taken otherwise) and appended to the |ASN.1| sequence.\n\n        Keyword Args\n        ------------\n        value: :class:`object` or :py:class:`~pyasn1.type.base.PyAsn1Item` derivative\n            A Python value to initialize |ASN.1| component with (if *componentType* is set)\n            or ASN.1 value object to assign to |ASN.1| component. Once a new value is\n            set to *idx* component, previous value is dropped.\n\n        verifyConstraints : :class:`bool`\n            If `False`, skip constraints validation\n\n        matchTags: :class:`bool`\n            If `False`, skip component tags matching\n\n        matchConstraints: :class:`bool`\n            If `False`, skip component constraints matching\n\n        Returns\n        -------\n        self\n        \"\"\"\n        oldIdx = self._currentIdx\n        Set.setComponentByPosition(self, idx, value, verifyConstraints, matchTags, matchConstraints)\n        self._currentIdx = idx\n        if oldIdx is not None and oldIdx != idx:\n            self._componentValues[oldIdx] = noValue\n        return self", "response": "Assign a value to a type component by position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a : class : ~pyasn1. type. tag. TagSet object of the currently initialized component or self.", "response": "def effectiveTagSet(self):\n        \"\"\"Return a :class:`~pyasn1.type.tag.TagSet` object of the currently initialized component or self (if |ASN.1| is tagged).\"\"\"\n        if self.tagSet:\n            return self.tagSet\n        else:\n            component = self.getComponent()\n            return component.effectiveTagSet"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a : class : ~pyasn1. type. tagmap. TagMap object mapping ASN. 1 tags to ASN. 1 objects contained within callee.", "response": "def tagMap(self):\n        \"\"\"\"Return a :class:`~pyasn1.type.tagmap.TagMap` object mapping\n            ASN.1 tags to ASN.1 objects contained within callee.\n        \"\"\"\n        if self.tagSet:\n            return Set.tagMap.fget(self)\n        else:\n            return self.componentType.tagMapUnique"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getComponent(self, innerFlag=False):\n        if self._currentIdx is None:\n            raise error.PyAsn1Error('Component not chosen')\n        else:\n            c = self._componentValues[self._currentIdx]\n            if innerFlag and isinstance(c, Choice):\n                return c.getComponent(innerFlag)\n            else:\n                return c", "response": "Return currently assigned component of the |ASN. 1| object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getName(self, innerFlag=False):\n        if self._currentIdx is None:\n            raise error.PyAsn1Error('Component not chosen')\n        else:\n            if innerFlag:\n                c = self._componentValues[self._currentIdx]\n                if isinstance(c, Choice):\n                    return c.getName(innerFlag)\n            return self.componentType.getNameByPosition(self._currentIdx)", "response": "Return the name of currently assigned component of the |ASN. 1| object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isValue(self):\n        if self._currentIdx is None:\n            return False\n\n        componentValue = self._componentValues[self._currentIdx]\n\n        return componentValue is not noValue and componentValue.isValue", "response": "Indicate that |ASN. 1| object represents a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tagMap(self):\n        try:\n            return self._tagMap\n\n        except AttributeError:\n            self._tagMap = tagmap.TagMap(\n                {self.tagSet: self},\n                {eoo.endOfOctets.tagSet: eoo.endOfOctets},\n                self\n            )\n\n            return self._tagMap", "response": "Return a : class : ~pyasn1. type. tagmap. TagMap object mapping\n            ASN. 1 tags to ASN. 1 objects contained within callee."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a backtrace of the given node.", "response": "def backtrace(node):\n    \"\"\"\n    Backtrace according to the parent records and return the path.\n    (including both start and end nodes)\n    \"\"\"\n    path = [(node.x, node.y)]\n    while node.parent:\n        node = node.parent\n        path.append((node.x, node.y))\n    path.reverse()\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path from start and end node", "response": "def bi_backtrace(node_a, node_b):\n    \"\"\"\n    Backtrace from start and end node, returns the path for bi-directional A*\n    (including both start and end nodes)\n    \"\"\"\n    path_a = backtrace(node_a)\n    path_b = backtrace(node_b)\n    path_b.reverse()\n    return path_a + path_b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving the start and end coordinates return all the coordinates lying on the line formed by these coordinates.", "response": "def bresenham(coords_a, coords_b):\n    '''\n    Given the start and end coordinates, return all the coordinates lying\n    on the line formed by these coordinates, based on Bresenham's algorithm.\n    http://en.wikipedia.org/wiki/Bresenham's_line_algorithm#Simplification\n    '''\n    line = []\n    x0, y0 = coords_a\n    x1, y1 = coords_b\n    dx = abs(x1 - x0)\n    dy = abs(y1 - y0)\n    sx = 1 if x0 < x1 else -1\n    sy = 1 if y0 < y1 else -1\n    err = dx - dy\n\n    while True:\n        line += [[x0, y0]]\n        if x0 == x1 and y0 == y1:\n            break\n        e2 = err * 2\n        if e2 > -dy:\n            err = err - dy\n            x0 = x0 + sx\n        if e2 < dx:\n            err = err + dx\n            y0 = y0 + sy\n\n    return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a compressed path return a new path that has all the segments in it interpolated.", "response": "def expand_path(path):\n    '''\n    Given a compressed path, return a new path that has all the segments\n    in it interpolated.\n    '''\n    expanded = []\n    if len(path) < 2:\n        return expanded\n    for i in range(len(path)-1):\n        expanded += bresenham(path[i], path[i + 1])\n    expanded += [path[:-1]]\n    return expanded"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the cost between two nodes.", "response": "def calc_cost(self, node_a, node_b):\n        \"\"\"\n        get the distance between current node and the neighbor (cost)\n        \"\"\"\n        if node_b.x - node_a.x == 0 or node_b.y - node_a.y == 0:\n            # direct neighbor - distance is 1\n            ng = 1\n        else:\n            # not a direct neighbor - diagonal movement\n            ng = SQRT2\n\n        # weight for weighted algorithms\n        if self.weighted:\n            ng *= node_b.weight\n\n        return node_a.g + ng"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_heuristic(self, node_a, node_b, heuristic=None):\n        if not heuristic:\n            heuristic = self.heuristic\n        return heuristic(\n            abs(node_a.x - node_b.x),\n            abs(node_a.y - node_b.y))", "response": "helper function to apply heuristic to the internal data structure"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_neighbors(self, grid, node, diagonal_movement=None):\n        '''\n        find neighbor, same for Djikstra, A*, Bi-A*, IDA*\n        '''\n        if not diagonal_movement:\n            diagonal_movement = self.diagonal_movement\n        return grid.neighbors(node, diagonal_movement=diagonal_movement)", "response": "find neighbors of a node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if we keep running", "response": "def keep_running(self):\n        \"\"\"\n        check, if we run into time or iteration constrains.\n        :returns: True if we keep running and False if we run into a constraint\n        \"\"\"\n        if self.runs >= self.max_runs:\n            raise ExecutionRunsException(\n                '{} run into barrier of {} iterations without '\n                'finding the destination'.format(\n                    self.__class__.__name__, self.max_runs))\n\n        if time.time() - self.start_time >= self.time_limit:\n            raise ExecutionTimeException(\n                '{} took longer than {} seconds, aborting!'.format(\n                    self.__class__.__name__, self.time_limit))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a path from start to end node on grid by iterating over all neighbors of a node and checking if all possible steps are available", "response": "def find_path(self, start, end, grid):\n        \"\"\"\n        find a path from start to end node on grid by iterating over\n        all neighbors of a node (see check_neighbors)\n        :param start: start node\n        :param end: end node\n        :param grid: grid that stores all possible steps/tiles as 2D-list\n        :return:\n        \"\"\"\n        self.start_time = time.time()  # execution time limitation\n        self.runs = 0  # count number of iterations\n        start.opened = True\n\n        open_list = [start]\n\n        while len(open_list) > 0:\n            self.runs += 1\n            self.keep_running()\n\n            path = self.check_neighbors(start, end, grid, open_list)\n            if path:\n                return path, self.runs\n\n        # failed to find path\n        return [], self.runs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_path(self, start, end, grid):\n        self.start_time = time.time()  # execution time limitation\n        self.runs = 0  # count number of iterations\n\n        start_open_list = [start]\n        start.g = 0\n        start.f = 0\n        start.opened = BY_START\n\n        end_open_list = [end]\n        end.g = 0\n        end.f = 0\n        end.opened = BY_END\n\n        while len(start_open_list) > 0 and len(end_open_list) > 0:\n            self.runs += 1\n            self.keep_running()\n            path = self.check_neighbors(start, end, grid, start_open_list,\n                                        open_value=BY_START,\n                                        backtrace_by=BY_END)\n            if path:\n                return path, self.runs\n\n            self.runs += 1\n            self.keep_running()\n            path = self.check_neighbors(end, start, grid, end_open_list,\n                                        open_value=BY_END,\n                                        backtrace_by=BY_START)\n            if path:\n                return path, self.runs\n\n        # failed to find path\n        return [], self.runs", "response": "find a path from start to end node on grid using the A* algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a list of nodes according to grid size.", "response": "def build_nodes(width, height, matrix=None, inverse=False):\n    \"\"\"\n    create nodes according to grid size. If a matrix is given it\n    will be used to determine what nodes are walkable.\n    :rtype : list\n    \"\"\"\n    nodes = []\n    use_matrix = (isinstance(matrix, (tuple, list))) or \\\n        (USE_NUMPY and isinstance(matrix, np.ndarray) and matrix.size > 0)\n\n    for y in range(height):\n        nodes.append([])\n        for x in range(width):\n            # 1, '1', True will be walkable\n            # while others will be obstacles\n            # if inverse is False, otherwise\n            # it changes\n            weight = int(matrix[y][x]) if use_matrix else 1\n            walkable = weight <= 0 if inverse else weight >= 1\n\n            nodes[y].append(Node(x=x, y=y, walkable=walkable, weight=weight))\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if field position is inside map", "response": "def inside(self, x, y):\n        \"\"\"\n        check, if field position is inside map\n        :param x: x pos\n        :param y: y pos\n        :return:\n        \"\"\"\n        return 0 <= x < self.width and 0 <= y < self.height"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if tile is inside grid and if it is set as walkable", "response": "def walkable(self, x, y):\n        \"\"\"\n        check, if the tile is inside grid and if it is set as walkable\n        \"\"\"\n        return self.inside(x, y) and self.nodes[y][x].walkable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all neighbors of one node", "response": "def neighbors(self, node, diagonal_movement=DiagonalMovement.never):\n        \"\"\"\n        get all neighbors of one node\n        :param node: node\n        \"\"\"\n        x = node.x\n        y = node.y\n        neighbors = []\n        s0 = d0 = s1 = d1 = s2 = d2 = s3 = d3 = False\n\n        # \u2191\n        if self.walkable(x, y - 1):\n            neighbors.append(self.nodes[y - 1][x])\n            s0 = True\n        # \u2192\n        if self.walkable(x + 1, y):\n            neighbors.append(self.nodes[y][x + 1])\n            s1 = True\n        # \u2193\n        if self.walkable(x, y + 1):\n            neighbors.append(self.nodes[y + 1][x])\n            s2 = True\n        # \u2190\n        if self.walkable(x - 1, y):\n            neighbors.append(self.nodes[y][x - 1])\n            s3 = True\n\n        if diagonal_movement == DiagonalMovement.never:\n            return neighbors\n\n        if diagonal_movement == DiagonalMovement.only_when_no_obstacle:\n            d0 = s3 and s0\n            d1 = s0 and s1\n            d2 = s1 and s2\n            d3 = s2 and s3\n        elif diagonal_movement == DiagonalMovement.if_at_most_one_obstacle:\n            d0 = s3 or s0\n            d1 = s0 or s1\n            d2 = s1 or s2\n            d3 = s2 or s3\n        elif diagonal_movement == DiagonalMovement.always:\n            d0 = d1 = d2 = d3 = True\n\n        # \u2196\n        if d0 and self.walkable(x - 1, y - 1):\n            neighbors.append(self.nodes[y - 1][x - 1])\n\n        # \u2197\n        if d1 and self.walkable(x + 1, y - 1):\n            neighbors.append(self.nodes[y - 1][x + 1])\n\n        # \u2198\n        if d2 and self.walkable(x + 1, y + 1):\n            neighbors.append(self.nodes[y + 1][x + 1])\n\n        # \u2199\n        if d3 and self.walkable(x - 1, y + 1):\n            neighbors.append(self.nodes[y + 1][x - 1])\n\n        return neighbors"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a printable string from the grid", "response": "def grid_str(self, path=None, start=None, end=None,\n                 border=True, start_chr='s', end_chr='e',\n                 path_chr='x', empty_chr=' ', block_chr='#',\n                 show_weight=False):\n        \"\"\"\n        create a printable string from the grid using ASCII characters\n\n        :param path: list of nodes that show the path\n        :param start: start node\n        :param end: end node\n        :param border: create a border around the grid\n        :param start_chr: character for the start (default \"s\")\n        :param end_chr: character for the destination (default \"e\")\n        :param path_chr: character to show the path (default \"x\")\n        :param empty_chr: character for empty fields (default \" \")\n        :param block_chr: character for blocking elements (default \"#\")\n        :param show_weight: instead of empty_chr show the cost of each empty\n                            field (shows a + if the value of weight is > 10)\n        :return:\n        \"\"\"\n        data = ''\n        if border:\n            data = '+{}+'.format('-'*len(self.nodes[0]))\n        for y in range(len(self.nodes)):\n            line = ''\n            for x in range(len(self.nodes[y])):\n                node = self.nodes[y][x]\n                if node == start:\n                    line += start_chr\n                elif node == end:\n                    line += end_chr\n                elif path and ((node.x, node.y) in path or node in path):\n                    line += path_chr\n                elif node.walkable:\n                    # empty field\n                    weight = str(node.weight) if node.weight < 10 else '+'\n                    line += weight if show_weight else empty_chr\n                else:\n                    line += block_chr  # blocked field\n            if border:\n                line = '|'+line+'|'\n            if data:\n                data += '\\n'\n            data += line\n        if border:\n            data += '\\n+{}+'.format('-'*len(self.nodes[0]))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_neighbors(self, start, end, grid, open_list,\n                        open_value=True, backtrace_by=None):\n        \"\"\"\n        find next path segment based on given node\n        (or return path if we found the end)\n        \"\"\"\n        # pop node with minimum 'f' value\n        node = heapq.nsmallest(1, open_list)[0]\n        open_list.remove(node)\n        node.closed = True\n\n        # if reached the end position, construct the path and return it\n        # (ignored for bi-directional a*, there we look for a neighbor that is\n        #  part of the oncoming path)\n        if not backtrace_by and node == end:\n            return backtrace(end)\n\n        # get neighbors of the current node\n        neighbors = self.find_neighbors(grid, node)\n        for neighbor in neighbors:\n            if neighbor.closed:\n                # already visited last minimum f value\n                continue\n            if backtrace_by and neighbor.opened == backtrace_by:\n                # found the oncoming path\n                if backtrace_by == BY_END:\n                    return bi_backtrace(node, neighbor)\n                else:\n                    return bi_backtrace(neighbor, node)\n\n            # check if the neighbor has not been inspected yet, or\n            # can be reached with smaller cost from the current node\n            self.process_node(neighbor, node, end, open_list, open_value)\n\n        # the end has not been reached (yet) keep the find_path loop running\n        return None", "response": "check if a given node is in the given grid and return the path segment that is found"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_path(self, start, end, grid):\n        start.g = 0\n        start.f = 0\n        return super(AStarFinder, self).find_path(start, end, grid)", "response": "find a path from start to end node on grid using the A * algorithm\n        is used to find all possible steps and tiles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the libzar shared library and its dependencies.", "response": "def load():\n    \"\"\"Loads the libzar shared library and its dependencies.\n    \"\"\"\n    if 'Windows' == platform.system():\n        # Possible scenarios here\n        #   1. Run from source, DLLs are in pyzbar directory\n        #       cdll.LoadLibrary() imports DLLs in repo root directory\n        #   2. Wheel install into CPython installation\n        #       cdll.LoadLibrary() imports DLLs in package directory\n        #   3. Wheel install into virtualenv\n        #       cdll.LoadLibrary() imports DLLs in package directory\n        #   4. Frozen\n        #       cdll.LoadLibrary() imports DLLs alongside executable\n        fname, dependencies = _windows_fnames()\n\n        def load_objects(directory):\n            # Load dependencies before loading libzbar dll\n            deps = [\n                cdll.LoadLibrary(str(directory.joinpath(dep)))\n                for dep in dependencies\n            ]\n            libzbar = cdll.LoadLibrary(str(directory.joinpath(fname)))\n            return deps, libzbar\n\n        try:\n            dependencies, libzbar = load_objects(Path(''))\n        except OSError:\n            dependencies, libzbar = load_objects(Path(__file__).parent)\n    else:\n        # Assume a shared library on the path\n        path = find_library('zbar')\n        if not path:\n            raise ImportError('Unable to find zbar shared library')\n        libzbar = cdll.LoadLibrary(path)\n        dependencies = []\n\n    return libzbar, dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_libzbar():\n    global LIBZBAR\n    global EXTERNAL_DEPENDENCIES\n    if not LIBZBAR:\n        libzbar, dependencies = zbar_library.load()\n        LIBZBAR = libzbar\n        EXTERNAL_DEPENDENCIES = [LIBZBAR] + dependencies\n\n    return LIBZBAR", "response": "Loads the zbar shared library and its dependencies. Returns the global LIBZBAR and EXTERNAL_DEPENDENCIES."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a foreign function exported by zbar.", "response": "def zbar_function(fname, restype, *args):\n    \"\"\"Returns a foreign function exported by `zbar`.\n\n    Args:\n        fname (:obj:`str`): Name of the exported function as string.\n        restype (:obj:): Return type - one of the `ctypes` primitive C data\n        types.\n        *args: Arguments - a sequence of `ctypes` primitive C data types.\n\n    Returns:\n        cddl.CFunctionType: A wrapper around the function.\n    \"\"\"\n    prototype = CFUNCTYPE(restype, *args)\n    return prototype((fname, load_libzbar()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bounding_box(locations):\n    x_values = list(map(itemgetter(0), locations))\n    x_min, x_max = min(x_values), max(x_values)\n    y_values = list(map(itemgetter(1), locations))\n    y_min, y_max = min(y_values), max(y_values)\n    return Rect(x_min, y_min, x_max - x_min, y_max - y_min)", "response": "Computes the bounding box of an iterable of x y tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convex_hull(points):\n\n    def is_not_clockwise(p0, p1, p2):\n        return 0 <= (\n            (p1[0] - p0[0]) * (p2[1] - p0[1]) -\n            (p1[1] - p0[1]) * (p2[0] - p0[0])\n        )\n\n    def go(points_):\n        res = []\n        for p in points_:\n            while 1 < len(res) and is_not_clockwise(res[-2], res[-1], p):\n                res.pop()\n            res.append(p)\n\n        # The last point in each list is the first point in the other list\n        res.pop()\n\n        return res\n\n    # Discard duplicates and sort by x then y\n    points = sorted(set(points))\n\n    # Algorithm needs at least two points\n    hull = (\n        points if len(points) < 2 else chain(go(points), go(reversed(points)))\n    )\n\n    return list(map(Point._make, hull))", "response": "Computes the convex hull of an iterable of x y tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _decode_symbols(symbols):\n    for symbol in symbols:\n        data = string_at(zbar_symbol_get_data(symbol))\n        # The 'type' int in a value in the ZBarSymbol enumeration\n        symbol_type = ZBarSymbol(symbol.contents.type).name\n        polygon = convex_hull(\n            (\n                zbar_symbol_get_loc_x(symbol, index),\n                zbar_symbol_get_loc_y(symbol, index)\n            )\n            for index in _RANGEFN(zbar_symbol_get_loc_size(symbol))\n        )\n\n        yield Decoded(\n            data=data,\n            type=symbol_type,\n            rect=bounding_box(polygon),\n            polygon=polygon\n        )", "response": "Generator of decoded symbol information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the image data in a tuple.", "response": "def _pixel_data(image):\n    \"\"\"Returns (pixels, width, height)\n\n    Returns:\n        :obj: `tuple` (pixels, width, height)\n    \"\"\"\n    # Test for PIL.Image and numpy.ndarray without requiring that cv2 or PIL\n    # are installed.\n    if 'PIL.' in str(type(image)):\n        if 'L' != image.mode:\n            image = image.convert('L')\n        pixels = image.tobytes()\n        width, height = image.size\n    elif 'numpy.ndarray' in str(type(image)):\n        if 3 == len(image.shape):\n            # Take just the first channel\n            image = image[:, :, 0]\n        if 'uint8' != str(image.dtype):\n            image = image.astype('uint8')\n        try:\n            pixels = image.tobytes()\n        except AttributeError:\n            # `numpy.ndarray.tobytes()` introduced in `numpy` 1.9.0 - use the\n            # older `tostring` method.\n            pixels = image.tostring()\n        height, width = image.shape[:2]\n    else:\n        # image should be a tuple (pixels, width, height)\n        pixels, width, height = image\n\n        # Check dimensions\n        if 0 != len(pixels) % (width * height):\n            raise PyZbarError(\n                (\n                    'Inconsistent dimensions: image data of {0} bytes is not '\n                    'divisible by (width x height = {1})'\n                ).format(len(pixels), (width * height))\n            )\n\n    # Compute bits-per-pixel\n    bpp = 8 * len(pixels) // (width * height)\n    if 8 != bpp:\n        raise PyZbarError(\n            'Unsupported bits-per-pixel [{0}]. Only [8] is supported.'.format(\n                bpp\n            )\n        )\n\n    return pixels, width, height"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(image, symbols=None):\n    pixels, width, height = _pixel_data(image)\n\n    results = []\n    with _image_scanner() as scanner:\n        if symbols:\n            # Disable all but the symbols of interest\n            disable = set(ZBarSymbol).difference(symbols)\n            for symbol in disable:\n                zbar_image_scanner_set_config(\n                    scanner, symbol, ZBarConfig.CFG_ENABLE, 0\n                )\n            # I think it likely that zbar will detect all symbol types by\n            # default, in which case enabling the types of interest is\n            # redundant but it seems sensible to be over-cautious and enable\n            # them.\n            for symbol in symbols:\n                zbar_image_scanner_set_config(\n                    scanner, symbol, ZBarConfig.CFG_ENABLE, 1\n                )\n        with _image() as img:\n            zbar_image_set_format(img, _FOURCC['L800'])\n            zbar_image_set_size(img, width, height)\n            zbar_image_set_data(img, cast(pixels, c_void_p), len(pixels), None)\n            decoded = zbar_scan_image(scanner, img)\n            if decoded < 0:\n                raise PyZbarError('Unsupported image format')\n            else:\n                results.extend(_decode_symbols(_symbols_for_image(img)))\n\n    return results", "response": "Decodes datamatrix barcodes in image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay a language selector dropdown in the admin.", "response": "def language_selector(context):\n    \"\"\" displays a language selector dropdown in the admin, based on Django \"LANGUAGES\" context.\n        requires:\n            * USE_I18N = True / settings.py\n            * LANGUAGES specified / settings.py (otherwise all Django locales will be displayed)\n            * \"set_language\" url configured (see https://docs.djangoproject.com/en/dev/topics/i18n/translation/#the-set-language-redirect-view)\n    \"\"\"\n    output = \"\"\n    i18 = getattr(settings, 'USE_I18N', False)\n    if i18:\n        template = \"admin/language_selector.html\"\n        context['i18n_is_set'] = True\n        try:\n            output = render_to_string(template, context)\n        except:\n            pass\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_app_name(context, app, template=\"/admin_app_name.html\"):\n    try:\n        template = app['app_label'] + template\n        text = render_to_string(template, context)\n    except:\n        text = app['name']\n    return text", "response": "Render the application name using the default template name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_app_label(context, app, fallback=\"\"):\n    try:\n        text = app['app_label']\n    except KeyError:\n        text = fallback\n    except TypeError:\n        text = app\n    return text", "response": "Render the application label."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render_app_description(context, app, fallback=\"\", template=\"/admin_app_description.html\"):\n    try:\n        template = app['app_label'] + template\n        text = render_to_string(template, context)\n    except:\n        text = fallback\n    return text", "response": "Render the application description using the default template name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap for rendering the field via an external renderer", "response": "def custom_field_rendering(context, field, *args, **kwargs):\n    \"\"\" Wrapper for rendering the field via an external renderer \"\"\"\n    if CUSTOM_FIELD_RENDERER:\n        mod, cls = CUSTOM_FIELD_RENDERER.rsplit(\".\", 1)\n        field_renderer = getattr(import_module(mod), cls)\n        if field_renderer:\n            return field_renderer(field, **kwargs).render()\n    return field"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filenames(self):\n        if self._is_reader:\n            assert self._filenames is not None\n            return self._filenames\n        else:\n            return self.data_producer.filenames", "response": "list of file names the data was originally being read from."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of all elements in the data flow graph.", "response": "def _data_flow_chain(self):\n        \"\"\"\n        Get a list of all elements in the data flow graph.\n        The first element is the original source, the next one reads from the prior and so on and so forth.\n\n        Returns\n        -------\n        list: list of data sources\n\n        \"\"\"\n        if self.data_producer is None:\n            return []\n\n        res = []\n        ds = self.data_producer\n        while not ds.is_reader:\n            res.append(ds)\n            ds = ds.data_producer\n        res.append(ds)\n        res = res[::-1]\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef number_of_trajectories(self, stride=None):\n        if not IteratorState.is_uniform_stride(stride):\n            n = len(np.unique(stride[:, 0]))\n        else:\n            n = self.ntraj\n        return n", "response": "r Returns the number of trajectories in the current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef n_chunks(self, chunksize, stride=1, skip=0):\n        if chunksize != 0:\n            chunksize = float(chunksize)\n            chunks = int(sum((ceil(l / chunksize) for l in self.trajectory_lengths(stride=stride, skip=skip))))\n        else:\n            chunks = self.number_of_trajectories(stride)\n        return chunks", "response": "how many chunks an iterator of this sourcde will output starting at the beginning"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps all input data of this transformer and returns it as a list of arrays or lists of arrays.", "response": "def get_output(self, dimensions=slice(0, None), stride=1, skip=0, chunk=None):\n        \"\"\"Maps all input data of this transformer and returns it as an array or list of arrays\n\n        Parameters\n        ----------\n        dimensions : list-like of indexes or slice, default=all\n           indices of dimensions you like to keep.\n        stride : int, default=1\n           only take every n'th frame.\n        skip : int, default=0\n            initially skip n frames of each file.\n        chunk: int, default=None\n            How many frames to process at once. If not given obtain the chunk size\n            from the source.\n\n        Returns\n        -------\n        output : list of ndarray(T_i, d)\n           the mapped data, where T is the number of time steps of the input data, or if stride > 1,\n           floor(T_in / stride). d is the output dimension of this transformer.\n           If the input consists of a list of trajectories, Y will also be a corresponding list of trajectories\n\n        \"\"\"\n        if isinstance(dimensions, int):\n            ndim = 1\n            dimensions = slice(dimensions, dimensions + 1)\n        elif isinstance(dimensions, (list, np.ndarray, tuple, slice)):\n            if hasattr(dimensions, 'ndim') and dimensions.ndim > 1:\n                raise ValueError('dimension indices can\\'t have more than one dimension')\n            ndim = len(np.zeros(self.ndim)[dimensions])\n        else:\n            raise ValueError('unsupported type (%s) of \"dimensions\"' % type(dimensions))\n\n        assert ndim > 0, \"ndim was zero in %s\" % self.__class__.__name__\n\n        if chunk is None:\n            chunk = self.chunksize\n\n        # create iterator\n        if self.in_memory and not self._mapping_to_mem_active:\n            from pyemma.coordinates.data.data_in_memory import DataInMemory\n            assert self._Y is not None\n            it = DataInMemory(self._Y)._create_iterator(skip=skip, chunk=chunk,\n                                                        stride=stride, return_trajindex=True)\n        else:\n            it = self._create_iterator(skip=skip, chunk=chunk, stride=stride, return_trajindex=True)\n\n        with it:\n            # allocate memory\n            try:\n                from pyemma import config\n                if config.coordinates_check_output:\n                    trajs = [np.full((l, ndim), np.nan, dtype=self.output_type()) for l in it.trajectory_lengths()]\n                else:\n                    # TODO: avoid having a copy here, if Y is already filled\n                    trajs = [np.empty((l, ndim), dtype=self.output_type())\n                             for l in it.trajectory_lengths()]\n            except MemoryError:\n                self.logger.exception(\"Could not allocate enough memory to map all data.\"\n                                      \" Consider using a larger stride.\")\n                return\n\n            if self._logger_is_active(self._loglevel_DEBUG):\n                self.logger.debug(\"get_output(): dimensions=%s\" % str(dimensions))\n                self.logger.debug(\"get_output(): created output trajs with shapes: %s\"\n                                   % [x.shape for x in trajs])\n                self.logger.debug(\"nchunks :%s, chunksize=%s\" % (it.n_chunks, it.chunksize))\n            # fetch data\n            from pyemma._base.progress import ProgressReporter\n            pg = ProgressReporter()\n            pg.register(it.n_chunks, description='getting output of %s' % self.__class__.__name__)\n            with pg.context(), it:\n                for itraj, chunk in it:\n                    i = slice(it.pos, it.pos + len(chunk))\n                    assert i.stop - i.start > 0\n                    trajs[itraj][i, :] = chunk[:, dimensions]\n                    pg.update(1)\n\n        if config.coordinates_check_output:\n            for i, t in enumerate(trajs):\n                finite = self._chunk_finite(t)\n                if not np.all(finite):\n                    # determine position\n                    frames = np.where(np.logical_not(finite))\n                    if not len(frames):\n                        raise RuntimeError('nothing got assigned for traj {}'.format(i))\n                    raise RuntimeError('unassigned sections in traj {i} in range [{frames}]'.format(frames=frames, i=i))\n\n        return trajs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_to_hdf5(self, filename, group='/', data_set_prefix='', overwrite=False,\n                      stride=1, chunksize=None, h5_opt=None):\n        \"\"\" writes all data of this Iterable to a given HDF5 file.\n        This is equivalent of writing the result of func:`pyemma.coordinates.data._base.DataSource.get_output` to a file.\n\n        Parameters\n        ----------\n        filename: str\n            file name of output HDF5 file\n        group: str, default='/'\n            write all trajectories to this HDF5 group. The group name may not already exist in the file.\n        data_set_prefix: str, default=None\n            data set name prefix, will postfixed with the index of the trajectory.\n        overwrite: bool, default=False\n            if group and data sets already exist, shall we overwrite data?\n        stride: int, default=1\n            stride argument to iterator\n        chunksize: int, default=None\n            how many frames to process at once\n        h5_opt: dict\n            optional parameters for h5py.create_dataset\n\n        Notes\n        -----\n        You can pass the following via h5_opt to enable compression/filters/shuffling etc:\n\n        chunks\n            (Tuple) Chunk shape, or True to enable auto-chunking.\n        maxshape\n            (Tuple) Make the dataset resizable up to this shape.  Use None for\n            axes you want to be unlimited.\n        compression\n            (String or int) Compression strategy.  Legal values are 'gzip',\n            'szip', 'lzf'.  If an integer in range(10), this indicates gzip\n            compression level. Otherwise, an integer indicates the number of a\n            dynamically loaded compression filter.\n        compression_opts\n            Compression settings.  This is an integer for gzip, 2-tuple for\n            szip, etc. If specifying a dynamically loaded compression filter\n            number, this must be a tuple of values.\n        scaleoffset\n            (Integer) Enable scale/offset filter for (usually) lossy\n            compression of integer or floating-point data. For integer\n            data, the value of scaleoffset is the number of bits to\n            retain (pass 0 to let HDF5 determine the minimum number of\n            bits necessary for lossless compression). For floating point\n            data, scaleoffset is the number of digits after the decimal\n            place to retain; stored values thus have absolute error\n            less than 0.5*10**(-scaleoffset).\n        shuffle\n            (T/F) Enable shuffle filter. Only effective in combination with chunks.\n        fletcher32\n            (T/F) Enable fletcher32 error detection. Not permitted in\n            conjunction with the scale/offset filter.\n        fillvalue\n            (Scalar) Use this value for uninitialized parts of the dataset.\n        track_times\n            (T/F) Enable dataset creation timestamps.\n        \"\"\"\n        if h5_opt is None:\n            h5_opt = {}\n        import h5py\n        from pyemma._base.progress import ProgressReporter\n        pg = ProgressReporter()\n        it = self.iterator(stride=stride, chunk=chunksize, return_trajindex=True)\n        pg.register(it.n_chunks, 'writing output')\n        with h5py.File(filename) as f, it, pg.context():\n            if group not in f:\n                g = f.create_group(group)\n            elif group == '/':  # root always exists.\n                g = f[group]\n            elif group in f and overwrite:\n                self.logger.info('overwriting group \"{}\"'.format(group))\n                del f[group]\n                g = f.create_group(group)\n            else:\n                raise ValueError('Given group \"{}\" already exists. Choose another one.'.format(group))\n\n            # check output data sets\n            data_sets = {}\n            for itraj in np.arange(self.ntraj):\n                template = '{prefix}_{index}' if data_set_prefix else '{index}'\n                ds_name = template.format(prefix=data_set_prefix, index='{:04d}'.format(itraj))\n                # group can be reused, eg. was empty before now check if we will overwrite something\n                if ds_name in g:\n                    if not overwrite:\n                        raise ValueError('Refusing to overwrite data in group \"{}\".'.format(group))\n                else:\n                    data_sets[itraj] = g.require_dataset(ds_name, shape=(self.trajectory_length(itraj=itraj, stride=stride),\n                                                                         self.ndim), dtype=self.output_type(), **h5_opt)\n            for itraj, X in it:\n                ds = data_sets[itraj]\n                ds[it.pos:it.pos + len(X)] = X\n                pg.update(1)", "response": "Writes all data of this Iterable to a given HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite all data to a CSV file.", "response": "def write_to_csv(self, filename=None, extension='.dat', overwrite=False,\n                     stride=1, chunksize=None, **kw):\n        \"\"\" write all data to csv with numpy.savetxt\n\n        Parameters\n        ----------\n        filename : str, optional\n            filename string, which may contain placeholders {itraj} and {stride}:\n\n            * itraj will be replaced by trajetory index\n            * stride is stride argument of this method\n\n            If filename is not given, it is being tried to obtain the filenames\n            from the data source of this iterator.\n        extension : str, optional, default='.dat'\n            filename extension of created files\n        overwrite : bool, optional, default=False\n            shall existing files be overwritten? If a file exists, this method will raise.\n        stride : int\n            omit every n'th frame\n        chunksize: int, default=None\n            how many frames to process at once\n        kw : dict, optional\n            named arguments passed into numpy.savetxt (header, seperator etc.)\n\n        Example\n        -------\n        Assume you want to save features calculated by some FeatureReader to ASCII:\n\n        >>> import numpy as np, pyemma\n        >>> import os\n        >>> from pyemma.util.files import TemporaryDirectory\n        >>> from pyemma.util.contexts import settings\n        >>> data = [np.random.random((10,3))] * 3\n        >>> reader = pyemma.coordinates.source(data)\n        >>> filename = \"distances_{itraj}.dat\"\n        >>> with TemporaryDirectory() as td, settings(show_progress_bars=False):\n        ...    out = os.path.join(td, filename)\n        ...    reader.write_to_csv(out, header='', delimiter=';')\n        ...    print(sorted(os.listdir(td)))\n        ['distances_0.dat', 'distances_1.dat', 'distances_2.dat']\n        \"\"\"\n        import os\n        if not filename:\n            assert hasattr(self, 'filenames')\n            #    raise RuntimeError(\"could not determine filenames\")\n            filenames = []\n            for f in self.filenames:\n                base, _ = os.path.splitext(f)\n                filenames.append(base + extension)\n        elif isinstance(filename, str):\n            filename = filename.replace('{stride}', str(stride))\n            filenames = [filename.replace('{itraj}', str(itraj)) for itraj\n                         in range(self.number_of_trajectories())]\n        else:\n            raise TypeError(\"filename should be str or None\")\n        self.logger.debug(\"write_to_csv, filenames=%s\" % filenames)\n        # check files before starting to write\n        import errno\n        for f in filenames:\n            try:\n                st = os.stat(f)\n                raise OSError(errno.EEXIST)\n            except OSError as e:\n                if e.errno == errno.EEXIST:\n                    if overwrite:\n                        continue\n                elif e.errno == errno.ENOENT:\n                    continue\n                raise\n        f = None\n        from pyemma._base.progress import ProgressReporter\n        pg = ProgressReporter()\n        it = self.iterator(stride, chunk=chunksize, return_trajindex=False)\n        pg.register(it.n_chunks, \"saving to csv\")\n        with it, pg.context():\n            oldtraj = -1\n            for X in it:\n                if oldtraj != it.current_trajindex:\n                    if f is not None:\n                        f.close()\n                    fn = filenames[it.current_trajindex]\n                    self.logger.debug(\"opening file %s for writing csv.\" % fn)\n                    f = open(fn, 'wb')\n                    oldtraj = it.current_trajindex\n                np.savetxt(f, X, **kw)\n                f.flush()\n                pg.update(1, 0)\n        if f is not None:\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the indices for a trajectory file index.", "response": "def ra_indices_for_traj(self, traj):\n        \"\"\"\n        Gives the indices for a trajectory file index (without changing the order within the trajectory itself).\n        :param traj: a trajectory file index\n        :return: a Nx1 - np.array of the indices corresponding to the trajectory index\n        \"\"\"\n        assert not self.uniform_stride, \"requested random access indices, but is in uniform stride mode\"\n        if traj in self.traj_keys:\n            return self.ra_indices_for_traj_dict[traj]\n        else:\n            return np.array([])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef n_chunks(self):\n        return self._data_source.n_chunks(self.chunksize, stride=self.stride, skip=self.skip)", "response": "approximates how many chunks will be processed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _itraj(self, value):\n        if value != self._selected_itraj:\n            self.state.itraj = value\n            # TODO: this side effect is unexpected.\n            self.state.t = 0", "response": "Internal method that tracks the upcoming trajectory index. Should not be used within iterator loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_model_param_names(cls):\n        # fetch model parameters\n        if hasattr(cls, 'set_model_params'):\n            # introspect the constructor arguments to find the model parameters\n            # to represent\n            args, varargs, kw, default = getargspec_no_self(cls.set_model_params)\n            if varargs is not None:\n                raise RuntimeError(\"PyEMMA models should always specify their parameters in the signature\"\n                                   \" of their set_model_params (no varargs). %s doesn't follow this convention.\"\n                                   % (cls,))\n            return args\n        else:\n            # No parameters known\n            return []", "response": "r Get parameter names for the model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_model_params(self, **params):\n        for key, value in params.items():\n            if not hasattr(self, key):\n                setattr(self, key, value)  # set parameter for the first time.\n            elif getattr(self, key) is None:\n                setattr(self, key, value)  # update because this parameter is still None.\n            elif value is not None:\n                setattr(self, key, value)", "response": "r Update given model parameter if they are set to specific values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_model_params(self, deep=True):\n        out = dict()\n        for key in self._get_model_param_names():\n            # We need deprecation warnings to always be on in order to\n            # catch deprecated param values.\n            # This is set in utils/__init__.py but it gets overwritten\n            # when running under python3 somehow.\n            from pyemma.util.exceptions import PyEMMA_DeprecationWarning\n            warnings.simplefilter(\"always\", DeprecationWarning)\n            warnings.simplefilter(\"always\", PyEMMA_DeprecationWarning)\n            try:\n                with warnings.catch_warnings(record=True) as w:\n                    value = getattr(self, key, None)\n                if len(w) and w[0].category in(DeprecationWarning, PyEMMA_DeprecationWarning):\n                    # if the parameter is deprecated, don't show it\n                    continue\n            finally:\n                warnings.filters.pop(0)\n                warnings.filters.pop(0)\n\n            # XXX: should we rather test if instance of estimator?\n            if deep and hasattr(value, 'get_params'):\n                deep_items = list(value.get_params().items())\n                out.update((key + '__' + k, val) for k, val in deep_items)\n            out[key] = value\n        return out", "response": "r Returns a dictionary of parameters for this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_mean(self, f, *args, **kwargs):\n        vals = self.sample_f(f, *args, **kwargs)\n        return _np.mean(vals, axis=0)", "response": "r Sample mean of numerical method f over all samples and computes the mean."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_std(self, f, *args, **kwargs):\n        vals = self.sample_f(f, *args, **kwargs)\n        return _np.std(vals, axis=0)", "response": "r Sample standard deviation of numerical method f over all samples and computes the standard deviation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of strings one for each feature selected and one for each active Returns a list of strings one for each feature selected and the list of str with human - readable descriptions of the features.", "response": "def describe(self):\n        \"\"\"\n        Returns a list of strings, one for each feature selected,\n        with human-readable descriptions of the features.\n\n        Returns\n        -------\n        labels : list of str\n            An ordered list of strings, one for each feature selected,\n            with human-readable descriptions of the features.\n\n        \"\"\"\n        all_labels = []\n        for f in self.active_features:\n            all_labels += f.describe()\n        return all_labels"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the indexes of all heavy atoms in the current species.", "response": "def select_Heavy(self, exclude_symmetry_related=False):\n        \"\"\"\n        Returns the indexes of all heavy atoms (Mass >= 2),\n        optionally excluding symmetry-related heavy atoms.\n\n        Parameters\n        ----------\n        exclude_symmetry_related : boolean, default=False\n            if True, exclude symmetry-related heavy atoms.\n\n        Returns\n        -------\n        indexes : ndarray((n), dtype=int)\n            array with selected atom indexes\n\n        \"\"\"\n        if exclude_symmetry_related:\n            exclusions = []\n\n            exclusions.append(\"mass < 2\")\n            exclusions.append(\"(resname == VAL and name == CG)\")\n            exclusions.append(\"(resname == LEU and name == CD)\")\n            exclusions.append(\"(resname == PHE and name == CD) or (resname == PHE and name == CE)\")\n            exclusions.append(\"(resname == TYR and name == CD) or (resname == TYR and name == CE)\")\n            exclusions.append(\"(resname == GLU and name == OD1) or (resname == GLU and name == OD2)\")\n            exclusions.append(\"(resname == ASP and name == OG1) or (resname == ASP and name == OG2)\")\n            exclusions.append(\"(resname == HIS and name == ND1) or (resname == HIS and name == NE2)\")\n            exclusions.append(\"(resname == ARG and name == NH1) or (resname == ARG and name == NH2)\")\n\n            exclusion_string = ' or '.join(exclusions)\n            selection_string = 'not (' + exclusion_string + ')'\n\n            return self.topology.select(selection_string)\n        else:\n            return self.topology.select(\"mass >= 2\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating all pairs between indexes. Will exclude closest neighbors up to excluded_neighbors", "response": "def pairs(sel, excluded_neighbors=0):\n        \"\"\"\n        Creates all pairs between indexes. Will exclude closest neighbors up to :py:obj:`excluded_neighbors`\n        The self-pair (i,i) is always excluded\n\n        Parameters\n        ----------\n        sel : ndarray((n), dtype=int)\n            array with selected atom indexes\n\n        excluded_neighbors: int, default = 0\n            number of neighbors that will be excluded when creating the pairs\n\n        Returns\n        -------\n        sel : ndarray((m,2), dtype=int)\n            m x 2 array with all pair indexes between different atoms that are at least :obj:`excluded_neighbors`\n            indexes apart, i.e. if i is the index of an atom, the pairs [i,i-2], [i,i-1], [i,i], [i,i+1], [i,i+2], will\n            not be in :py:obj:`sel` (n=excluded_neighbors) if :py:obj:`excluded_neighbors` = 2.\n            Moreover, the list is non-redundant,i.e. if [i,j] is in sel, then [j,i] is not.\n\n        \"\"\"\n\n        assert isinstance(excluded_neighbors,int)\n\n        p = []\n        for i in range(len(sel)):\n            for j in range(i + 1, len(sel)):\n                # get ordered pair\n                I = sel[i]\n                J = sel[j]\n                if (I > J):\n                    I = sel[j]\n                    J = sel[i]\n                # exclude 1 and 2 neighbors\n                if (J > I + excluded_neighbors):\n                    p.append([I, J])\n        return np.array(p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring pairs are valid", "response": "def _check_indices(self, pair_inds, pair_n=2):\n        \"\"\"ensure pairs are valid (shapes, all atom indices available?, etc.)\n        \"\"\"\n\n        pair_inds = np.array(pair_inds).astype(dtype=np.int, casting='safe')\n\n        if pair_inds.ndim != 2:\n            raise ValueError(\"pair indices has to be a matrix.\")\n\n        if pair_inds.shape[1] != pair_n:\n            raise ValueError(\"pair indices shape has to be (x, %i).\" % pair_n)\n\n        if pair_inds.max() > self.topology.n_atoms:\n            raise ValueError(\"index out of bounds: %i.\"\n                             \" Maximum atom index available: %i\"\n                             % (pair_inds.max(), self.topology.n_atoms))\n\n        return pair_inds"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds all atom coordinates to the feature list.", "response": "def add_all(self, reference=None, atom_indices=None, ref_atom_indices=None):\n        \"\"\"\n        Adds all atom coordinates to the feature list.\n        The coordinates are flattened as follows: [x1, y1, z1, x2, y2, z2, ...]\n\n        Parameters\n        ----------\n        reference: mdtraj.Trajectory or None, default=None\n            if given, all data is being aligned to the given reference with Trajectory.superpose\n        atom_indices : array_like, or None\n            The indices of the atoms to superpose. If not\n            supplied, all atoms will be used.\n        ref_atom_indices : array_like, or None\n            Use these atoms on the reference structure. If not supplied,\n            the same atom indices will be used for this trajectory and the\n            reference one.\n        \"\"\"\n        self.add_selection(list(range(self.topology.n_atoms)), reference=reference,\n                           atom_indices=atom_indices, ref_atom_indices=ref_atom_indices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_selection(self, indexes, reference=None, atom_indices=None, ref_atom_indices=None):\n        from .misc import SelectionFeature, AlignFeature\n        if reference is None:\n            f = SelectionFeature(self.topology, indexes)\n        else:\n            if not isinstance(reference, mdtraj.Trajectory):\n                raise ValueError('reference is not a mdtraj.Trajectory object, but {}'.format(reference))\n            f = AlignFeature(reference=reference, indexes=indexes,\n                             atom_indices=atom_indices, ref_atom_indices=ref_atom_indices)\n        self.__add_feature(f)", "response": "Adds the coordinates of the selected atom indexes to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_distances_ca(self, periodic=True, excluded_neighbors=2):\n\n        # Atom indices for CAs\n        at_idxs_ca = self.select_Ca()\n        # Residue indices for residues contatinig CAs\n        res_idxs_ca = [self.topology.atom(ca).residue.index for ca in at_idxs_ca]\n        # Pairs of those residues, with possibility to exclude neighbors\n        res_idxs_ca_pairs = self.pairs(res_idxs_ca, excluded_neighbors=excluded_neighbors)\n        # Mapping back pairs of residue indices to pairs of CA indices\n        distance_indexes = []\n        for ri, rj in res_idxs_ca_pairs:\n            distance_indexes.append([self.topology.residue(ri).atom('CA').index,\n                                     self.topology.residue(rj).atom('CA').index\n                                     ])\n        distance_indexes = np.array(distance_indexes)\n\n        self.add_distances(distance_indexes, periodic=periodic)", "response": "Adds the distances between all Ca s to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_inverse_distances(self, indices, periodic=True, indices2=None):\n        from .distances import InverseDistanceFeature\n        atom_pairs = _parse_pairwise_input(\n            indices, indices2, self.logger, fname='add_inverse_distances()')\n\n        atom_pairs = self._check_indices(atom_pairs)\n        f = InverseDistanceFeature(self.topology, atom_pairs, periodic=periodic)\n        self.__add_feature(f)", "response": "Adds the inverse distances between atoms between which the inverse distances shall be computed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_contacts(self, indices, indices2=None, threshold=0.3, periodic=True, count_contacts=False):\n        from .distances import ContactFeature\n        atom_pairs = _parse_pairwise_input(\n            indices, indices2, self.logger, fname='add_contacts()')\n\n        atom_pairs = self._check_indices(atom_pairs)\n        f = ContactFeature(self.topology, atom_pairs, threshold, periodic, count_contacts)\n        self.__add_feature(f)", "response": "r Adds contacts to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_residue_COM(self, residue_indices, scheme='all', ref_geom=None, image_molecules=False, mass_weighted=True,):\n\n        from .misc import ResidueCOMFeature\n        from pyemma.coordinates.data.featurization.util import _atoms_in_residues\n        assert scheme in ['all', 'backbone', 'sidechain']\n\n        residue_atoms = _atoms_in_residues(self.topology, residue_indices, subset_of_atom_idxs=self.topology.select(scheme), MDlogger=self.logger)\n\n        f = ResidueCOMFeature(self.topology, np.asarray(residue_indices), residue_atoms, scheme, ref_geom=ref_geom, image_molecules=image_molecules, mass_weighted=mass_weighted)\n\n        self.__add_feature(f)", "response": "r Adds a per - residue COM in cartesian coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_angles(self, indexes, deg=False, cossin=False, periodic=True):\n        from .angles import AngleFeature\n        indexes = self._check_indices(indexes, pair_n=3)\n        f = AngleFeature(self.topology, indexes, deg=deg, cossin=cossin,\n                         periodic=periodic)\n        self.__add_feature(f)", "response": "Adds the list of angles to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the list of dihedrals to the feature list.", "response": "def add_dihedrals(self, indexes, deg=False, cossin=False, periodic=True):\n        \"\"\"\n        Adds the list of dihedrals to the feature list\n\n        Parameters\n        ----------\n        indexes : np.ndarray, shape=(num_pairs, 4), dtype=int\n            an array with quadruplets of atom indices\n        deg : bool, optional, default = False\n            If False (default), angles will be computed in radians.\n            If True, angles will be computed in degrees.\n        cossin : bool, optional, default = False\n            If True, each angle will be returned as a pair of (sin(x), cos(x)).\n            This is useful, if you calculate the mean (e.g TICA/PCA, clustering)\n            in that space.\n        periodic : bool, optional, default = True\n            If `periodic` is True and the trajectory contains unitcell\n            information, we will treat dihedrals that cross periodic images\n            using the minimum image convention.\n\n        \"\"\"\n        from .angles import DihedralFeature\n        indexes = self._check_indices(indexes, pair_n=4)\n        f = DihedralFeature(self.topology, indexes, deg=deg, cossin=cossin,\n                            periodic=periodic)\n        self.__add_feature(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_backbone_torsions(self, selstr=None, deg=False, cossin=False, periodic=True):\n        from .angles import BackboneTorsionFeature\n        f = BackboneTorsionFeature(\n            self.topology, selstr=selstr, deg=deg, cossin=cossin, periodic=periodic)\n        self.__add_feature(f)", "response": "Adds all backbone torsions to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_chi1_torsions(self, selstr=\"\", deg=False, cossin=False, periodic=True):\n        from .angles import SideChainTorsions\n        f = SideChainTorsions(\n            self.topology, selstr=selstr, deg=deg, cossin=cossin, periodic=periodic, which=['chi1'])\n        self.__add_feature(f)", "response": "Adds all chi1 torsions to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_custom_feature(self, feature):\n        if feature.dimension <= 0:\n            raise ValueError(\"Dimension has to be positive. \"\n                             \"Please override dimension attribute in feature!\")\n\n        if not hasattr(feature, 'transform'):\n            raise ValueError(\"no 'transform' method in given feature\")\n        elif not callable(getattr(feature, 'transform')):\n            raise ValueError(\"'transform' attribute exists but is not a method\")\n\n        self.__add_feature(feature)", "response": "Adds a custom feature to the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_minrmsd_to_ref(self, ref, ref_frame=0, atom_indices=None, precentered=False):\n        from .misc import MinRmsdFeature\n        f = MinRmsdFeature(ref, ref_frame=ref_frame, atom_indices=atom_indices, topology=self.topology,\n                           precentered=precentered)\n        self.__add_feature(f)", "response": "r Adds the minimum root - mean - square - deviation to the feature list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_custom_func(self, func, dim, *args, **kwargs):\n        description = kwargs.pop('description', None)\n        f = CustomFeature(func, dim=dim, description=description, fun_args=args, fun_kwargs=kwargs)\n        self.add_custom_feature(f)", "response": "Adds a custom function to extract features from the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_all_custom_funcs(self):\n        custom_feats = [f for f in self.active_features if isinstance(f, CustomFeature)]\n        for f in custom_feats:\n            self.active_features.remove(f)", "response": "Removes all instances of CustomFeature from the active feature list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current dimension due to selected features", "response": "def dimension(self):\n        \"\"\" current dimension due to selected features\n\n        Returns\n        -------\n        dim : int\n            total dimension due to all selection features\n\n        \"\"\"\n        dim = sum(f.dimension for f in self.active_features)\n        return dim"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transform(self, traj):\n        # if there are no features selected, return given trajectory\n        if not self.active_features:\n            self.add_selection(np.arange(self.topology.n_atoms))\n            warnings.warn(\"You have not selected any features. Returning plain coordinates.\")\n\n        # otherwise build feature vector.\n        feature_vec = []\n\n        # TODO: consider parallel evaluation computation here, this effort is\n        # only worth it, if computation time dominates memory transfers\n        for f in self.active_features:\n            # perform sanity checks for custom feature input\n            if isinstance(f, CustomFeature):\n                # NOTE: casting=safe raises in numpy>=1.9\n                vec = f.transform(traj).astype(np.float32, casting='safe')\n                if vec.shape[0] == 0:\n                    vec = np.empty((0, f.dimension))\n\n                if not isinstance(vec, np.ndarray):\n                    raise ValueError('Your custom feature %s did not return'\n                                     ' a numpy.ndarray!' % str(f.describe()))\n                if not vec.ndim == 2:\n                    raise ValueError('Your custom feature %s did not return'\n                                     ' a 2d array. Shape was %s'\n                                     % (str(f.describe()),\n                                        str(vec.shape)))\n                if not vec.shape[0] == traj.xyz.shape[0]:\n                    raise ValueError('Your custom feature %s did not return'\n                                     ' as many frames as it received!'\n                                     'Input was %i, output was %i'\n                                     % (str(f.describe()),\n                                        traj.xyz.shape[0],\n                                        vec.shape[0]))\n            else:\n                vec = f.transform(traj).astype(np.float32)\n            feature_vec.append(vec)\n\n        if len(feature_vec) > 1:\n            res = np.hstack(feature_vec)\n        else:\n            res = feature_vec[0]\n\n        return res", "response": "Maps an mdtraj Trajectory object to the selected output features."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bootstrapping_dtrajs(dtrajs, lag, N_full, nbs=10000, active_set=None):\n\n    # Get the number of simulations:\n    Q = len(dtrajs)\n    # Get the number of states in the active set:\n    if active_set is not None:\n        N = active_set.size\n    else:\n        N = N_full\n    # Build up a matrix of count matrices for each simulation. Size is Q*N^2:\n    traj_ind = []\n    state1 = []\n    state2 = []\n    q = 0\n    for traj in dtrajs:\n        traj_ind.append(q*np.ones(traj[:-lag].size))\n        state1.append(traj[:-lag])\n        state2.append(traj[lag:])\n        q += 1\n    traj_inds = np.concatenate(traj_ind)\n    pairs = N_full * np.concatenate(state1) + np.concatenate(state2)\n    data = np.ones(pairs.size)\n    Ct_traj = scipy.sparse.coo_matrix((data, (traj_inds, pairs)), shape=(Q, N_full*N_full))\n    Ct_traj = Ct_traj.tocsr()\n\n    # Perform re-sampling:\n    svals = np.zeros((nbs, N))\n    for s in range(nbs):\n        # Choose selection:\n        sel = np.random.choice(Q, Q, replace=True)\n        # Compute count matrix for selection:\n        Ct_sel = Ct_traj[sel, :].sum(axis=0)\n        Ct_sel = np.asarray(Ct_sel).reshape((N_full, N_full))\n        if active_set is not None:\n            from pyemma.util.linalg import submatrix\n            Ct_sel = submatrix(Ct_sel, active_set)\n        svals[s, :] = scl.svdvals(Ct_sel)\n    # Compute mean and uncertainties:\n    smean = np.mean(svals, axis=0)\n    sdev = np.std(svals, axis=0)\n\n    return smean, sdev", "response": "Perform trajectory based re - sampling."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms bootstrapping on trajectories to estimate uncertainties for singular values of count matrices.", "response": "def bootstrapping_count_matrix(Ct, nbs=10000):\n    \"\"\"\n    Perform bootstrapping on trajectories to estimate uncertainties for singular values of count matrices.\n\n    Parameters\n    ----------\n    Ct : csr-matrix\n        count matrix of the data.\n\n    nbs : int, optional\n        the number of re-samplings to be drawn from dtrajs\n\n    Returns\n    -------\n    smean : ndarray(N,)\n        mean values of singular values\n    sdev : ndarray(N,)\n        standard deviations of singular values\n    \"\"\"\n    # Get the number of states:\n    N = Ct.shape[0]\n    # Get the number of transition pairs:\n    T = Ct.sum()\n    # Reshape and normalize the count matrix:\n    p = Ct.toarray()\n    p = np.reshape(p, (N*N,)).astype(np.float)\n    p = p / T\n    # Perform the bootstrapping:\n    svals = np.zeros((nbs, N))\n    for s in range(nbs):\n        # Draw sample:\n        sel = np.random.multinomial(T, p)\n        # Compute the count-matrix:\n        sC = np.reshape(sel, (N, N))\n        # Compute singular values:\n        svals[s, :] = scl.svdvals(sC)\n    # Compute mean and uncertainties:\n    smean = np.mean(svals, axis=0)\n    sdev = np.std(svals, axis=0)\n\n    return smean, sdev"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef twostep_count_matrix(dtrajs, lag, N):\n    # List all transition triples:\n    rows = []\n    cols = []\n    states = []\n    for dtraj in dtrajs:\n        if dtraj.size > 2*lag:\n            rows.append(dtraj[0:-2*lag])\n            states.append(dtraj[lag:-lag])\n            cols.append(dtraj[2*lag:])\n    row = np.concatenate(rows)\n    col = np.concatenate(cols)\n    state = np.concatenate(states)\n    data = np.ones(row.size)\n    # Transform the rows and cols into a single list with N*+2 possible values:\n    pair = N * row + col\n    # Estimate sparse matrix:\n    C2t = scipy.sparse.coo_matrix((data, (pair, state)), shape=(N*N, N))\n\n    return C2t.tocsc()", "response": "Compute all two - step count matrices for all states in a discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing OOM components and eigenvalues from count matrices.", "response": "def oom_components(Ct, C2t, rank_ind=None, lcc=None, tol_one=1e-2):\n    \"\"\"\n    Compute OOM components and eigenvalues from count matrices:\n\n    Parameters\n    ----------\n    Ct : ndarray(N, N)\n        count matrix from data\n    C2t : sparse csc-matrix (N*N, N)\n        two-step count matrix from data for all states, columns enumerate\n        intermediate steps.\n    rank_ind : ndarray(N, dtype=bool), optional, default=None\n        indicates which singular values are accepted. By default, all non-\n        zero singular values are accepted.\n    lcc : ndarray(N,), optional, default=None\n        largest connected set of the count-matrix. Two step count matrix\n        will be reduced to this set.\n    tol_one : float, optional, default=1e-2\n        keep eigenvalues of absolute value less or equal 1+tol_one.\n\n    Returns\n    -------\n    Xi : ndarray(M, N, M)\n        matrix of set-observable operators\n    omega: ndarray(M,)\n        information state vector of OOM\n    sigma : ndarray(M,)\n        evaluator of OOM\n    l : ndarray(M,)\n        eigenvalues from OOM\n    \"\"\"\n    import msmtools.estimation as me\n    # Decompose count matrix by SVD:\n    if lcc is not None:\n        Ct_svd = me.largest_connected_submatrix(Ct, lcc=lcc)\n        N1 = Ct.shape[0]\n    else:\n        Ct_svd = Ct\n    V, s, W = scl.svd(Ct_svd, full_matrices=False)\n    # Make rank decision:\n    if rank_ind is None:\n        ind = (s >= np.finfo(float).eps)\n    V = V[:, rank_ind]\n    s = s[rank_ind]\n    W = W[rank_ind, :].T\n\n    # Compute transformations:\n    F1 = np.dot(V, np.diag(s**-0.5))\n    F2 = np.dot(W, np.diag(s**-0.5))\n\n    # Apply the transformations to C2t:\n    N = Ct_svd.shape[0]\n    M = F1.shape[1]\n    Xi = np.zeros((M, N, M))\n    for n in range(N):\n        if lcc is not None:\n            C2t_n = C2t[:, lcc[n]]\n            C2t_n = _reshape_sparse(C2t_n, (N1, N1))\n            C2t_n = me.largest_connected_submatrix(C2t_n, lcc=lcc)\n        else:\n            C2t_n = C2t[:, n]\n            C2t_n = _reshape_sparse(C2t_n, (N, N))\n        Xi[:, n, :] = np.dot(F1.T, C2t_n.dot(F2))\n\n    # Compute sigma:\n    c = np.sum(Ct_svd, axis=1)\n    sigma = np.dot(F1.T, c)\n    # Compute eigenvalues:\n    Xi_S = np.sum(Xi, axis=1)\n    l, R = scl.eig(Xi_S.T)\n    # Restrict eigenvalues to reasonable range:\n    ind = np.where(np.logical_and(np.abs(l) <= (1+tol_one), np.real(l) >= 0.0))[0]\n    l = l[ind]\n    R = R[:, ind]\n    # Sort and extract omega\n    l, R = _sort_by_norm(l, R)\n    omega = np.real(R[:, 0])\n    omega = omega / np.dot(omega, sigma)\n\n    return Xi, omega, sigma, l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the equilibrium transition matrix from the given information state vector and the given sigma.", "response": "def equilibrium_transition_matrix(Xi, omega, sigma, reversible=True, return_lcc=True):\n    \"\"\"\n    Compute equilibrium transition matrix from OOM components:\n\n    Parameters\n    ----------\n    Xi : ndarray(M, N, M)\n        matrix of set-observable operators\n    omega: ndarray(M,)\n        information state vector of OOM\n    sigma : ndarray(M,)\n        evaluator of OOM\n    reversible : bool, optional, default=True\n        symmetrize corrected count matrix in order to obtain\n        a reversible transition matrix.\n    return_lcc: bool, optional, default=True\n        return indices of largest connected set.\n\n    Returns\n    -------\n    Tt_Eq : ndarray(N, N)\n        equilibrium transition matrix\n    lcc : ndarray(M,)\n        the largest connected set of the transition matrix.\n    \"\"\"\n    import msmtools.estimation as me\n\n    # Compute equilibrium transition matrix:\n    Ct_Eq = np.einsum('j,jkl,lmn,n->km', omega, Xi, Xi, sigma)\n    # Remove negative entries:\n    Ct_Eq[Ct_Eq < 0.0] = 0.0\n    # Compute transition matrix after symmetrization:\n    pi_r = np.sum(Ct_Eq, axis=1)\n    if reversible:\n        pi_c = np.sum(Ct_Eq, axis=0)\n        pi_sym = pi_r + pi_c\n        # Avoid zero row-sums. States with zero row-sums will be eliminated by active set update.\n        ind0 = np.where(pi_sym == 0.0)[0]\n        pi_sym[ind0] = 1.0\n        Tt_Eq = (Ct_Eq + Ct_Eq.T) / pi_sym[:, None]\n    else:\n        # Avoid zero row-sums. States with zero row-sums will be eliminated by active set update.\n        ind0 = np.where(pi_r == 0.0)[0]\n        pi_r[ind0] = 1.0\n        Tt_Eq = Ct_Eq / pi_r[:, None]\n\n    # Perform active set update:\n    lcc = me.largest_connected_set(Tt_Eq)\n    Tt_Eq = me.largest_connected_submatrix(Tt_Eq, lcc=lcc)\n\n    if return_lcc:\n        return Tt_Eq, lcc\n    else:\n        return Tt_Eq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef featurizer(topfile):\n    from pyemma.coordinates.data.featurization.featurizer import MDFeaturizer\n    return MDFeaturizer(topfile)", "response": "r Returns a new featurizer that can select features from MD data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef source(inp, features=None, top=None, chunksize=None, **kw):\n    from pyemma.coordinates.data._base.iterable import Iterable\n    from pyemma.coordinates.data.util.reader_utils import create_file_reader\n\n    from pyemma.util.reflection import get_default_args\n    cs = _check_old_chunksize_arg(chunksize, get_default_args(source)['chunksize'], **kw)\n\n    # CASE 1: input is a string or list of strings\n    # check: if single string create a one-element list\n    if isinstance(inp, _string_types) or (\n            isinstance(inp, (list, tuple))\n            and (any(isinstance(item, (list, tuple, _string_types)) for item in inp) or len(inp) is 0)):\n        reader = create_file_reader(inp, top, features, chunksize=cs, **kw)\n\n    elif isinstance(inp, _np.ndarray) or (isinstance(inp, (list, tuple))\n                                          and (any(isinstance(item, _np.ndarray) for item in inp) or len(inp) is 0)):\n        # CASE 2: input is a (T, N, 3) array or list of (T_i, N, 3) arrays\n        # check: if single array, create a one-element list\n        # check: do all arrays have compatible dimensions (*, N, 3)? If not: raise ValueError.\n        # check: if single array, create a one-element list\n        # check: do all arrays have compatible dimensions (*, N)? If not: raise ValueError.\n        # create MemoryReader\n        from pyemma.coordinates.data.data_in_memory import DataInMemory as _DataInMemory\n        reader = _DataInMemory(inp, chunksize=cs, **kw)\n    elif isinstance(inp, Iterable):\n        inp.chunksize = cs\n        return inp\n    else:\n        raise ValueError('unsupported type (%s) of input' % type(inp))\n\n    return reader", "response": "r A function that returns a sequence of trajectory data from a file or list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef combine_sources(sources, chunksize=None):\n    from pyemma.coordinates.data.sources_merger import SourcesMerger\n    return SourcesMerger(sources, chunk=chunksize)", "response": "r Returns a new un - streaming source tree for the given list of data sources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_traj(traj_inp, indexes, outfile, top=None, stride = 1, chunksize=None, image_molecules=False, verbose=True):\n    from mdtraj import Topology, Trajectory\n\n    from pyemma.coordinates.data.feature_reader import FeatureReader\n    from pyemma.coordinates.data.fragmented_trajectory_reader import FragmentedTrajectoryReader\n    from pyemma.coordinates.data.util.frames_from_file import frames_from_files\n    from pyemma.coordinates.data.util.reader_utils import enforce_top\n    import itertools\n\n    # Determine the type of input and extract necessary parameters\n    if isinstance(traj_inp, (FeatureReader, FragmentedTrajectoryReader)):\n        if isinstance(traj_inp, FragmentedTrajectoryReader):\n            # lengths array per reader\n            if not all(isinstance(reader, FeatureReader)\n                                     for reader in itertools.chain.from_iterable(traj_inp._readers)):\n                raise ValueError(\"Only FeatureReaders (MD-data) are supported for fragmented trajectories.\")\n            trajfiles = traj_inp.filenames_flat\n            top = traj_inp._readers[0][0].featurizer.topology\n        else:\n            top = traj_inp.featurizer.topology\n            trajfiles = traj_inp.filenames\n        chunksize = traj_inp.chunksize\n        reader = traj_inp\n    else:\n        # Do we have what we need?\n        if not isinstance(traj_inp, (list, tuple)):\n            raise TypeError(\"traj_inp has to be of type list, not %s\" % type(traj_inp))\n        if not isinstance(top, (_string_types, Topology, Trajectory)):\n            raise TypeError(\"traj_inp cannot be a list of files without an input \"\n                            \"top of type str (eg filename.pdb), mdtraj.Trajectory or mdtraj.Topology. \"\n                            \"Got type %s instead\" % type(top))\n        trajfiles = traj_inp\n        reader = None\n\n    # Enforce the input topology to actually be an md.Topology object\n    top = enforce_top(top)\n\n    # Convert to index (T,2) array if parsed a list or a list of arrays\n    indexes = _np.vstack(indexes)\n\n    # Check that we've been given enough filenames\n    if len(trajfiles) < indexes[:, 0].max():\n        raise ValueError(\"traj_inp contains %u trajfiles, \"\n                         \"but indexes will ask for file nr. %u\"\n                         % (len(trajfiles), indexes[:,0].max()))\n\n    traj = frames_from_files(trajfiles, top, indexes, chunksize, stride, reader=reader)\n\n    # Avoid broken molecules\n    if image_molecules:\n        traj.image_molecules(inplace=True)\n\n    # Return to memory as an mdtraj trajectory object\n    if outfile is None:\n        return traj\n    # or to disk as a molecular trajectory file\n    else:\n        traj.save(outfile)\n    if verbose:\n        _logger.info(\"Created file %s\" % outfile)", "response": "r Saves a sequence of frames as a single molecular dynamics trajectory file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_trajs(traj_inp, indexes, prefix='set_', fmt=None, outfiles=None,\n               inmemory=False, stride=1, verbose=False):\n    r\"\"\" Saves sequences of frames as multiple trajectories.\n\n    Extracts a number of specified sequences of time/trajectory indexes from the\n    input loader and saves them in a set of molecular dynamics trajectories.\n    The output filenames are obtained by prefix + str(n) + .fmt, where n counts\n    the output trajectory and extension is either set by the user, or else\n    determined from the input. Example: When the input is in dcd format, and\n    indexes is a list of length 3, the output will by default go to files\n    \"set_1.dcd\", \"set_2.dcd\", \"set_3.dcd\". If you want files to be stored\n    in a specific subfolder, simply specify the relative path in the prefix,\n    e.g. prefix='~/macrostates/\\pcca_'\n\n    Parameters\n    ----------\n    traj_inp : :py:class:`pyemma.coordinates.data.feature_reader.FeatureReader`\n        A data source as provided by Please use :py:func:`pyemma.coordinates.source` to construct it.\n\n    indexes : list of ndarray(T_i, 2)\n        A list of N arrays, each of size (T_n x 2) for writing N trajectories\n        of T_i time steps. Each row contains two indexes (i, t), where i is the\n        index of the trajectory from the input and t is the index of the time\n        step within the trajectory.\n\n    prefix : str, optional, default = `set_`\n        output filename prefix. Can include an absolute or relative path name.\n\n    fmt : str, optional, default = None\n        Outpuf file format. By default, the file extension and format. It will\n        be determined from the input. If a different format is desired, specify\n        the corresponding file extension here without a dot, e.g. \"dcd\" or \"xtc\".\n\n    outfiles : list of str, optional, default = None\n        A list of output filenames. When given, this will override the settings\n        of prefix and fmt, and output will be written to these files.\n\n    inmemory : Boolean, default = False (untested for large files)\n        Instead of internally calling traj_save for every (T_i,2) array in\n        \"indexes\", only one call is made. Internally, this generates a\n        potentially large molecular trajectory object in memory that is\n        subsequently sliced into the files of \"outfiles\". Should be faster for\n        large \"indexes\" arrays and  large files, though it is quite memory\n        intensive. The optimal situation is to avoid streaming two times\n        through a huge file for \"indexes\" of type: indexes = [[1 4000000],[1 4000001]]\n\n    stride  : integer, default is 1\n        This parameter informs :py:func:`save_trajs` about the stride used in\n        the indexes variable. Typically, the variable indexes contains frame\n        indexes that match exactly the frames of the files contained in\n        traj_inp.trajfiles. However, in certain situations, that might not be\n        the case. Examples of these situations are cases in which stride\n        value != 1 was used when reading/featurizing/transforming/discretizing\n        the files contained in traj_inp.trajfiles.\n\n    verbose : boolean, default is False\n        Verbose output while looking for \"indexes\" in the \"traj_inp.trajfiles\"\n\n    Returns\n    -------\n    outfiles : list of str\n        The list of absolute paths that the output files have been written to.\n\n    \"\"\"\n    # Make sure indexes is iterable\n    assert _types.is_iterable(indexes), \"Indexes must be an iterable of matrices.\"\n    # only if 2d-array, convert into a list\n    if isinstance(indexes, _np.ndarray):\n        if indexes.ndim == 2:\n            indexes = [indexes]\n\n    # Make sure the elements of that lists are arrays, and that they are shaped properly\n    for i_indexes in indexes:\n        assert isinstance(i_indexes, _np.ndarray), \"The elements in the 'indexes' variable must be numpy.ndarrays\"\n        assert i_indexes.ndim == 2, \\\n            \"The elements in the 'indexes' variable must have ndim = 2, and not %u\" % i_indexes.ndim\n        assert i_indexes.shape[1] == 2, \\\n            \"The elements in the 'indexes' variable must be of shape (T_i,2), and not (%u,%u)\" % i_indexes.shape\n\n    # Determine output format of the molecular trajectory file\n    if fmt is None:\n        import os\n\n        _, fmt = os.path.splitext(traj_inp.filenames[0])\n    else:\n        fmt = '.' + fmt\n\n    # Prepare the list of outfiles before the loop\n    if outfiles is None:\n        outfiles = []\n        for ii in range(len(indexes)):\n            outfiles.append(prefix + '%06u' % ii + fmt)\n\n    # Check that we have the same name of outfiles as (T, 2)-indexes arrays\n    if len(indexes) != len(outfiles):\n        raise Exception('len(indexes) (%s) does not match len(outfiles) (%s)' % (len(indexes), len(outfiles)))\n\n    # This implementation looks for \"i_indexes\" separately, and thus one traj_inp.trajfile\n    # might be accessed more than once (less memory intensive)\n    if not inmemory:\n        for i_indexes, outfile in zip(indexes, outfiles):\n            # TODO: use **kwargs to parse to save_traj\n            save_traj(traj_inp, i_indexes, outfile, stride=stride, verbose=verbose)\n\n    # This implementation is \"one file - one pass\" but might temporally create huge memory objects\n    else:\n        traj = save_traj(traj_inp, indexes, outfile=None, stride=stride, verbose=verbose)\n        i_idx = 0\n        for i_indexes, outfile in zip(indexes, outfiles):\n            # Create indices for slicing the mdtraj trajectory object\n            f_idx = i_idx + len(i_indexes)\n            # print i_idx, f_idx\n            traj[i_idx:f_idx].save(outfile)\n            _logger.info(\"Created file %s\" % outfile)\n            # update the initial frame index\n            i_idx = f_idx\n\n    return outfiles", "response": "r Saves a sequence of time - to - time trajectories from a single input loader and optionally a list of time - to - time trajectories to a single output file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tica(data=None, lag=10, dim=-1, var_cutoff=0.95, kinetic_map=True, commute_map=False, weights='empirical',\n         stride=1, remove_mean=True, skip=0, reversible=True, ncov_max=float('inf'), chunksize=None, **kwargs):\n    r\"\"\" Time-lagged independent component analysis (TICA).\n\n    TICA is a linear transformation method. In contrast to PCA, which finds\n    coordinates of maximal variance, TICA finds coordinates of maximal\n    autocorrelation at the given lag time. Therefore, TICA is useful in order\n    to find the *slow* components in a dataset and thus an excellent choice to\n    transform molecular dynamics data before clustering data for the\n    construction of a Markov model. When the input data is the result of a\n    Markov process (such as thermostatted molecular dynamics), TICA finds in\n    fact an approximation to the eigenfunctions and eigenvalues of the\n    underlying Markov operator [1]_.\n\n    It estimates a TICA transformation from *data*. When input data is given as\n    an argument, the estimation will be carried out straight away, and the\n    resulting object can be used to obtain eigenvalues, eigenvectors or project\n    input data onto the slowest TICA components. If no data is given, this\n    object is an empty estimator and can be put into a :func:`pipeline` in\n    order to use TICA in the streaming mode.\n\n    Parameters\n    ----------\n    data : ndarray (T, d) or list of ndarray (T_i, d) or a reader created by\n        source function array with the data, if available. When given, the TICA\n        transformation is immediately computed and can be used to transform data.\n\n    lag : int, optional, default = 10\n        the lag time, in multiples of the input time step\n\n    dim : int, optional, default -1\n        the number of dimensions (independent components) to project onto. A\n        call to the :func:`map <pyemma.coordinates.transform.TICA.map>` function\n        reduces the d-dimensional input to only dim dimensions such that the\n        data preserves the maximum possible autocorrelation amongst\n        dim-dimensional linear projections. -1 means all numerically available\n        dimensions will be used unless reduced by var_cutoff.\n        Setting dim to a positive value is exclusive with var_cutoff.\n\n    var_cutoff : float in the range [0,1], optional, default 0.95\n        Determines the number of output dimensions by including dimensions\n        until their cumulative kinetic variance exceeds the fraction\n        subspace_variance. var_cutoff=1.0 means all numerically available\n        dimensions (see epsilon) will be used, unless set by dim. Setting\n        var_cutoff smaller than 1.0 is exclusive with dim\n\n    kinetic_map : bool, optional, default True\n        Eigenvectors will be scaled by eigenvalues. As a result, Euclidean\n        distances in the transformed data approximate kinetic distances [4]_.\n        This is a good choice when the data is further processed by clustering.\n\n    commute_map : bool, optional, default False\n        Eigenvector_i will be scaled by sqrt(timescale_i / 2). As a result, Euclidean distances in the transformed\n        data will approximate commute distances [5]_.\n\n    stride : int, optional, default = 1\n        If set to 1, all input data will be used for estimation. Note that this\n        could cause this calculation to be very slow for large data sets. Since\n        molecular dynamics data is usually correlated at short timescales, it is\n        often sufficient to estimate transformations at a longer stride. Note\n        that the stride option in the get_output() function of the returned\n        object is independent, so you can parametrize at a long stride, and\n        still map all frames through the transformer.\n\n    weights : optional, default=\"empirical\"\n             Re-weighting strategy to be used in order to compute equilibrium covariances from non-equilibrium data.\n                * \"empirical\":  no re-weighting\n                * \"koopman\":    use re-weighting procedure from [6]_\n                * weights:      An object that allows to compute re-weighting factors. It must possess a method\n                                weights(X) that accepts a trajectory X (np.ndarray(T, n)) and returns a vector of\n                                re-weighting factors (np.ndarray(T,)).\n\n    remove_mean: bool, optional, default True\n        remove mean during covariance estimation. Should not be turned off.\n\n    skip : int, default=0\n        skip the first initial n frames per trajectory.\n\n    reversible: bool, default=True\n            symmetrize correlation matrices C_0, C_{\\tau}.\n\n    ncov_max : int, default=infinity\n        limit the memory usage of the algorithm from [7]_ to an amount that corresponds\n        to ncov_max additional copies of each correlation matrix\n\n    chunksize: int, default=None\n        Number of data frames to process at once. Choose a higher value here,\n        to optimize thread usage and gain processing speed. If None is passed,\n        use the default value of the underlying reader/data source. Choose zero to\n        disable chunking at all.\n\n    Returns\n    -------\n    tica : a :class:`TICA <pyemma.coordinates.transform.TICA>` transformation object\n        Object for time-lagged independent component (TICA) analysis.\n        it contains TICA eigenvalues and eigenvectors, and the projection of\n        input data to the dominant TICA\n\n\n    Notes\n    -----\n    Given a sequence of multivariate data :math:`X_t`, it computes the\n    mean-free covariance and time-lagged covariance matrix:\n\n    .. math::\n\n        C_0 &=      (X_t - \\mu)^T \\mathrm{diag}(w) (X_t - \\mu) \\\\\n        C_{\\tau} &= (X_t - \\mu)^T \\mathrm{diag}(w) (X_t + \\tau - \\mu)\n\n    where w is a vector of weights for each time step. By default, these weights\n    are all equal to one, but different weights are possible, like the re-weighting\n    to equilibrium described in [6]_. Subsequently, the eigenvalue problem\n\n    .. math:: C_{\\tau} r_i = C_0 \\lambda_i r_i,\n\n    is solved,where :math:`r_i` are the independent components and :math:`\\lambda_i` are\n    their respective normalized time-autocorrelations. The eigenvalues are\n    related to the relaxation timescale by\n\n    .. math::\n\n        t_i = -\\frac{\\tau}{\\ln |\\lambda_i|}.\n\n    When used as a dimension reduction method, the input data is projected\n    onto the dominant independent components.\n\n    TICA was originally introduced for signal processing in [2]_. It was\n    introduced to molecular dynamics and as a method for the construction\n    of Markov models in [1]_ and [3]_. It was shown in [1]_ that when applied\n    to molecular dynamics data, TICA is an approximation to the eigenvalues\n    and eigenvectors of the true underlying dynamics.\n\n    Examples\n    --------\n    Invoke TICA transformation with a given lag time and output dimension:\n\n    >>> import numpy as np\n    >>> from pyemma.coordinates import tica\n    >>> data = np.random.random((100,3))\n    >>> projected_data = tica(data, lag=2, dim=1).get_output()[0]\n\n    For a brief explaination why TICA outperforms PCA to extract a good reaction\n    coordinate have a look `here\n    <http://docs.markovmodel.org/lecture_tica.html#Example:-TICA-versus-PCA-in-a-stretched-double-well-potential>`_.\n\n    See also\n    --------\n    :class:`TICA <pyemma.coordinates.transform.TICA>` : tica object\n\n    :func:`pca <pyemma.coordinates.pca>` : for principal component analysis\n\n\n    .. autoclass:: pyemma.coordinates.transform.tica.TICA\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.coordinates.transform.tica.TICA\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.coordinates.transform.tica.TICA\n            :attributes:\n\n    References\n    ----------\n\n    .. [1] Perez-Hernandez G, F Paul, T Giorgino, G De Fabritiis and F Noe. 2013.\n       Identification of slow molecular order parameters for Markov model construction\n       J. Chem. Phys. 139, 015102. doi:10.1063/1.4811489\n\n    .. [2] L. Molgedey and H. G. Schuster. 1994.\n       Separation of a mixture of independent signals using time delayed correlations\n       Phys. Rev. Lett. 72, 3634.\n\n    .. [3] Schwantes C, V S Pande. 2013.\n       Improvements in Markov State Model Construction Reveal Many Non-Native Interactions in the Folding of NTL9\n       J. Chem. Theory. Comput. 9, 2000-2009. doi:10.1021/ct300878a\n\n    .. [4] Noe, F. and Clementi, C. 2015. Kinetic distance and kinetic maps from molecular dynamics simulation.\n        J. Chem. Theory. Comput. doi:10.1021/acs.jctc.5b00553\n\n    .. [5] Noe, F., Banisch, R., Clementi, C. 2016. Commute maps: separating slowly-mixing molecular configurations\n       for kinetic modeling. J. Chem. Theory. Comput. doi:10.1021/acs.jctc.6b00762\n\n    .. [6] Wu, H., Nueske, F., Paul, F., Klus, S., Koltai, P., and Noe, F. 2016. Bias reduced variational\n        approximation of molecular kinetics from short off-equilibrium simulations. J. Chem. Phys. (submitted),\n        https://arxiv.org/abs/1610.06773.\n\n    .. [7] Chan, T. F., Golub G. H., LeVeque R. J. 1979. Updating formulae and pairwiese algorithms for\n        computing sample variances. Technical Report STAN-CS-79-773, Department of Computer Science, Stanford University.\n\n    \"\"\"\n    from pyemma.coordinates.transform.tica import TICA\n    from pyemma.coordinates.estimation.koopman import _KoopmanEstimator\n    import types\n    from pyemma.util.reflection import get_default_args\n    cs = _check_old_chunksize_arg(chunksize, get_default_args(tica)['chunksize'], **kwargs)\n\n    if isinstance(weights, _string_types):\n        if weights == \"koopman\":\n            if data is None:\n                raise ValueError(\"Data must be supplied for reweighting='koopman'\")\n            if not reversible:\n                raise ValueError(\"Koopman re-weighting is designed for reversible processes, set reversible=True\")\n            koop = _KoopmanEstimator(lag=lag, stride=stride, skip=skip, ncov_max=ncov_max)\n            koop.estimate(data, chunksize=cs)\n            weights = koop.weights\n        elif weights == \"empirical\":\n            weights = None\n        else:\n            raise ValueError(\"reweighting must be either 'empirical', 'koopman' \"\n                             \"or an object with a weights(data) method.\")\n    elif hasattr(weights, 'weights') and type(getattr(weights, 'weights')) == types.MethodType:\n        weights = weights\n    elif isinstance(weights, (list, tuple)) and all(isinstance(w, _np.ndarray) for w in weights):\n        if data is not None and len(data) != len(weights):\n            raise ValueError(\"len of weights({}) must match len of data({}).\".format(len(weights), len(data)))\n    else:\n        raise ValueError(\"reweighting must be either 'empirical', 'koopman' or an object with a weights(data) method.\")\n\n    if not remove_mean:\n        import warnings\n        user_msg = 'remove_mean option is deprecated. The mean is removed from the data by default, otherwise it' \\\n                   'cannot be guaranteed that all eigenvalues will be smaller than one. Some functionalities might' \\\n                   'become useless in this case (e.g. commute_maps). Also, not removing the mean will not result in' \\\n                   'a significant speed up of calculations.'\n        warnings.warn(\n            user_msg,\n            category=_PyEMMA_DeprecationWarning)\n\n    res = TICA(lag, dim=dim, var_cutoff=var_cutoff, kinetic_map=kinetic_map, commute_map=commute_map, skip=skip, stride=stride,\n               weights=weights, reversible=reversible, ncov_max=ncov_max)\n    if data is not None:\n        res.estimate(data, chunksize=cs)\n    else:\n        res.chunksize = cs\n    return res", "response": "r Time - lagged independent component analysis of a single cluster of molecular dynamics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vamp(data=None, lag=10, dim=None, scaling=None, right=False, ncov_max=float('inf'),\n         stride=1, skip=0, chunksize=None):\n    r\"\"\" Variational approach for Markov processes (VAMP) [1]_.\n\n      Parameters\n      ----------\n      lag : int\n          lag time\n      dim : float or int, default=None\n          Number of dimensions to keep:\n\n          * if dim is not set (None) all available ranks are kept:\n              `n_components == min(n_samples, n_uncorrelated_features)`\n          * if dim is an integer >= 1, this number specifies the number\n            of dimensions to keep.\n          * if dim is a float with ``0 < dim < 1``, select the number\n            of dimensions such that the amount of kinetic variance\n            that needs to be explained is greater than the percentage\n            specified by dim.\n      scaling : None or string\n          Scaling to be applied to the VAMP order parameters upon transformation\n\n          * None: no scaling will be applied, variance of the order parameters is 1\n          * 'kinetic map' or 'km': order parameters are scaled by singular value.\n            Only the left singular functions induce a kinetic map wrt the\n            conventional forward propagator. The right singular functions induce\n            a kinetic map wrt the backward propagator.      right : boolean\n          Whether to compute the right singular functions.\n          If `right==True`, `get_output()` will return the right singular\n          functions. Otherwise, `get_output()` will return the left singular\n          functions.\n          Beware that only `frames[tau:, :]` of each trajectory returned\n          by `get_output()` contain valid values of the right singular\n          functions. Conversely, only `frames[0:-tau, :]` of each\n          trajectory returned by `get_output()` contain valid values of\n          the left singular functions. The remaining frames might\n          possibly be interpreted as some extrapolation.\n      epsilon : float\n          eigenvalue cutoff. Eigenvalues of :math:`C_{00}` and :math:`C_{11}`\n          with norms <= epsilon will be cut off. The remaining number of\n          eigenvalues together with the value of `dim` define the size of the output.\n      stride: int, optional, default = 1\n          Use only every stride-th time step. By default, every time step is used.\n      skip : int, default=0\n          skip the first initial n frames per trajectory.\n      ncov_max : int, default=infinity\n          limit the memory usage of the algorithm from [3]_ to an amount that corresponds\n          to ncov_max additional copies of each correlation matrix\n\n      Returns\n      -------\n      vamp : a :class:`VAMP <pyemma.coordinates.transform.VAMP>` transformation object\n         It contains the definitions of singular functions and singular values and\n         can be used to project input data to the dominant VAMP components, predict\n         expectations and time-lagged covariances and perform a Chapman-Kolmogorov\n         test.\n\n      Notes\n      -----\n      VAMP is a method for dimensionality reduction of Markov processes.\n\n      The Koopman operator :math:`\\mathcal{K}` is an integral operator\n      that describes conditional future expectation values. Let\n      :math:`p(\\mathbf{x},\\,\\mathbf{y})` be the conditional probability\n      density of visiting an infinitesimal phase space volume around\n      point :math:`\\mathbf{y}` at time :math:`t+\\tau` given that the phase\n      space point :math:`\\mathbf{x}` was visited at the earlier time\n      :math:`t`. Then the action of the Koopman operator on a function\n      :math:`f` can be written as follows:\n\n      .. math::\n\n          \\mathcal{K}f=\\int p(\\mathbf{x},\\,\\mathbf{y})f(\\mathbf{y})\\,\\mathrm{dy}=\\mathbb{E}\\left[f(\\mathbf{x}_{t+\\tau}\\mid\\mathbf{x}_{t}=\\mathbf{x})\\right]\n\n      The Koopman operator is defined without any reference to an\n      equilibrium distribution. Therefore it is well-defined in\n      situations where the dynamics is irreversible or/and non-stationary\n      such that no equilibrium distribution exists.\n\n      If we approximate :math:`f` by a linear superposition of ansatz\n      functions :math:`\\boldsymbol{\\chi}` of the conformational\n      degrees of freedom (features), the operator :math:`\\mathcal{K}`\n      can be approximated by a (finite-dimensional) matrix :math:`\\mathbf{K}`.\n\n      The approximation is computed as follows: From the time-dependent\n      input features :math:`\\boldsymbol{\\chi}(t)`, we compute the mean\n      :math:`\\boldsymbol{\\mu}_{0}` (:math:`\\boldsymbol{\\mu}_{1}`) from\n      all data excluding the last (first) :math:`\\tau` steps of every\n      trajectory as follows:\n\n      .. math::\n\n          \\boldsymbol{\\mu}_{0}\t:=\\frac{1}{T-\\tau}\\sum_{t=0}^{T-\\tau}\\boldsymbol{\\chi}(t)\n\n          \\boldsymbol{\\mu}_{1}\t:=\\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T}\\boldsymbol{\\chi}(t)\n\n      Next, we compute the instantaneous covariance matrices\n      :math:`\\mathbf{C}_{00}` and :math:`\\mathbf{C}_{11}` and the\n      time-lagged covariance matrix :math:`\\mathbf{C}_{01}` as follows:\n\n      .. math::\n\n          \\mathbf{C}_{00}\t:=\\frac{1}{T-\\tau}\\sum_{t=0}^{T-\\tau}\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{0}\\right]\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{0}\\right]\n\n          \\mathbf{C}_{11}\t:=\\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T}\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{1}\\right]\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{1}\\right]\n\n          \\mathbf{C}_{01}\t:=\\frac{1}{T-\\tau}\\sum_{t=0}^{T-\\tau}\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{0}\\right]\\left[\\boldsymbol{\\chi}(t+\\tau)-\\boldsymbol{\\mu}_{1}\\right]\n\n      The Koopman matrix is then computed as follows:\n\n      .. math::\n\n          \\mathbf{K}=\\mathbf{C}_{00}^{-1}\\mathbf{C}_{01}\n\n      It can be shown [1]_ that the leading singular functions of the\n      half-weighted Koopman matrix\n\n      .. math::\n\n          \\bar{\\mathbf{K}}:=\\mathbf{C}_{00}^{-\\frac{1}{2}}\\mathbf{C}_{01}\\mathbf{C}_{11}^{-\\frac{1}{2}}\n\n      encode the best reduced dynamical model for the time series.\n\n      The singular functions can be computed by first performing the\n      singular value decomposition\n\n      .. math::\n\n          \\bar{\\mathbf{K}}=\\mathbf{U}^{\\prime}\\mathbf{S}\\mathbf{V}^{\\prime}\n\n      and then mapping the input conformation to the left singular\n      functions :math:`\\boldsymbol{\\psi}` and right singular\n      functions :math:`\\boldsymbol{\\phi}` as follows:\n\n      .. math::\n\n          \\boldsymbol{\\psi}(t):=\\mathbf{U}^{\\prime\\top}\\mathbf{C}_{00}^{-\\frac{1}{2}}\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{0}\\right]\n\n          \\boldsymbol{\\phi}(t):=\\mathbf{V}^{\\prime\\top}\\mathbf{C}_{11}^{-\\frac{1}{2}}\\left[\\boldsymbol{\\chi}(t)-\\boldsymbol{\\mu}_{1}\\right]\n\n\n      References\n      ----------\n      .. [1] Wu, H. and Noe, F. 2017. Variational approach for learning Markov processes from time series data.\n          arXiv:1707.04659v1\n      .. [2] Noe, F. and Clementi, C. 2015. Kinetic distance and kinetic maps from molecular dynamics simulation.\n          J. Chem. Theory. Comput. doi:10.1021/acs.jctc.5b00553\n      .. [3] Chan, T. F., Golub G. H., LeVeque R. J. 1979. Updating formulae and pairwiese algorithms for\n         computing sample variances. Technical Report STAN-CS-79-773, Department of Computer Science, Stanford University.\n    \"\"\"\n    from pyemma.coordinates.transform.vamp import VAMP\n    res = VAMP(lag, dim=dim, scaling=scaling, right=right, skip=skip, ncov_max=ncov_max)\n    if data is not None:\n        res.estimate(data, stride=stride, chunksize=chunksize)\n    else:\n        res.chunksize = chunksize\n    return res", "response": "r Variational approach for Markov processes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric='euclidean',\n                              init_strategy='kmeans++', n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs):\n    r\"\"\"k-means clustering with mini-batch strategy\n\n    Mini-batch k-means is an approximation to k-means which picks a randomly\n    selected subset of data points to be updated in each iteration. Usually\n    much faster than k-means but will likely deliver a less optimal result.\n\n    Returns\n    -------\n    kmeans_mini : a :class:`MiniBatchKmeansClustering <pyemma.coordinates.clustering.MiniBatchKmeansClustering>` clustering object\n        Object for mini-batch kmeans clustering.\n        It holds discrete trajectories and cluster center information.\n\n    See also\n    --------\n    :func:`kmeans <pyemma.coordinates.kmeans>` : for full k-means clustering\n\n\n    .. autoclass:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering\n            :attributes:\n\n    References\n    ----------\n    .. [1] http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n    \"\"\"\n    from pyemma.coordinates.clustering.kmeans import MiniBatchKmeansClustering\n    res = MiniBatchKmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, init_strategy=init_strategy,\n                                    batch_size=batch_size, n_jobs=n_jobs, skip=skip, clustercenters=clustercenters)\n    from pyemma.util.reflection import get_default_args\n    cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_mini_batch_kmeans)['chunksize'], **kwargs)\n    if data is not None:\n        res.estimate(data, chunksize=cs)\n    else:\n        res.chunksize = chunksize\n    return res", "response": "r K - Means clustering with mini - batch strategy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cluster_uniform_time(data=None, k=None, stride=1, metric='euclidean',\n                         n_jobs=None, chunksize=None, skip=0, **kwargs):\n    r\"\"\"Uniform time clustering\n\n    If given data, performs a clustering that selects data points uniformly in\n    time and then assigns the data using a Voronoi discretization. Returns a\n    :class:`UniformTimeClustering <pyemma.coordinates.clustering.UniformTimeClustering>` object\n    that can be used to extract the discretized data sequences, or to assign\n    other data points to the same partition. If data is not given, an empty\n    :class:`UniformTimeClustering <pyemma.coordinates.clustering.UniformTimeClustering>` will be created that\n    still needs to be parametrized, e.g. in a :func:`pipeline`.\n\n    Parameters\n    ----------\n    data : ndarray (T, d) or list of ndarray (T_i, d) or a reader created\n        by source function input data, if available in memory\n\n    k : int\n        the number of cluster centers. When not specified (None), min(sqrt(N), 5000) is chosen as default value,\n        where N denotes the number of data points\n\n    stride : int, optional, default = 1\n        If set to 1, all input data will be used for estimation. Note that this\n        could cause this calculation to be very slow for large data sets. Since\n        molecular dynamics data is usually correlated at short timescales, it is\n        often sufficient to estimate transformations at a longer stride.\n        Note that the stride option in the get_output() function of the returned\n        object is independent, so you can parametrize at a long stride, and\n        still map all frames through the transformer.\n\n    metric : str\n        metric to use during clustering ('euclidean', 'minRMSD')\n\n    n_jobs : int or None, default None\n        Number of threads to use during assignment of the data.\n        If None, all available CPUs will be used.\n\n    chunksize: int, default=None\n        Number of data frames to process at once. Choose a higher value here,\n        to optimize thread usage and gain processing speed. If None is passed,\n        use the default value of the underlying reader/data source. Choose zero to\n        disable chunking at all.\n\n    skip : int, default=0\n        skip the first initial n frames per trajectory.\n\n    Returns\n    -------\n    uniformTime : a :class:`UniformTimeClustering <pyemma.coordinates.clustering.UniformTimeClustering>` clustering object\n        Object for uniform time clustering.\n        It holds discrete trajectories and cluster center information.\n\n\n    .. autoclass:: pyemma.coordinates.clustering.uniform_time.UniformTimeClustering\n         :members:\n         :undoc-members:\n\n         .. rubric:: Methods\n\n         .. autoautosummary:: pyemma.coordinates.clustering.uniform_time.UniformTimeClustering\n            :methods:\n\n         .. rubric:: Attributes\n\n         .. autoautosummary:: pyemma.coordinates.clustering.uniform_time.UniformTimeClustering\n             :attributes:\n\n    \"\"\"\n    from pyemma.coordinates.clustering.uniform_time import UniformTimeClustering\n    res = UniformTimeClustering(k, metric=metric, n_jobs=n_jobs, skip=skip, stride=stride)\n    from pyemma.util.reflection import get_default_args\n    cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_uniform_time)['chunksize'], **kwargs)\n    if data is not None:\n        res.estimate(data, chunksize=cs)\n    else:\n        res.chunksize = cs\n    return res", "response": "r Uniform time clustering."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assign_to_centers(data=None, centers=None, stride=1, return_dtrajs=True,\n                      metric='euclidean', n_jobs=None, chunksize=None, skip=0, **kwargs):\n    r\"\"\"Assigns data to the nearest cluster centers\n\n    Creates a Voronoi partition with the given cluster centers. If given\n    trajectories as data, this function will by default discretize the\n    trajectories and return discrete trajectories of corresponding lengths.\n    Otherwise, an assignment object will be returned that can be used to\n    assign data later or can serve as a pipeline stage.\n\n    Parameters\n    ----------\n    data : ndarray or list of arrays or reader created by source function\n        data to be assigned\n\n    centers : path to file or ndarray or a reader created by source function\n        cluster centers to use in assignment of data\n\n    stride : int, optional, default = 1\n        assign only every n'th frame to the centers. Usually you want to assign\n        all the data and only use a stride during calculation the centers.\n\n    return_dtrajs : bool, optional, default = True\n        If True, it will return the discretized trajectories obtained from\n        assigning the coordinates in the data input. This will only have effect\n        if data is given. When data is not given or return_dtrajs is False,\n        the :class:'AssignCenters <_AssignCenters>' object will be returned.\n\n    metric : str\n        metric to use during clustering ('euclidean', 'minRMSD')\n\n    n_jobs : int or None, default None\n        Number of threads to use during assignment of the data.\n        If None, all available CPUs will be used.\n\n    chunksize: int, default=None\n        Number of data frames to process at once. Choose a higher value here,\n        to optimize thread usage and gain processing speed. If None is passed,\n        use the default value of the underlying reader/data source. Choose zero to\n        disable chunking at all.\n\n    Returns\n    -------\n    assignment : list of integer arrays or an :class:`AssignCenters <pyemma.coordinates.clustering.AssignCenters>` object\n        assigned data\n\n    Examples\n    --------\n    Load data to assign to clusters from 'my_data.csv' by using the cluster\n    centers from file 'my_centers.csv'\n\n    >>> import numpy as np\n\n    Generate some random data and choose 10 random centers:\n\n    >>> data = np.random.random((100, 3))\n    >>> cluster_centers = data[np.random.randint(0, 99, size=10)]\n    >>> dtrajs = assign_to_centers(data, cluster_centers)\n    >>> print(dtrajs) # doctest: +ELLIPSIS\n    [array([...\n\n\n    .. autoclass:: pyemma.coordinates.clustering.assign.AssignCenters\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.coordinates.clustering.assign.AssignCenters\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.coordinates.clustering.assign.AssignCenters\n            :attributes:\n\n    \"\"\"\n    if centers is None:\n        raise ValueError('You have to provide centers in form of a filename'\n                         ' or NumPy array or a reader created by source function')\n    from pyemma.coordinates.clustering.assign import AssignCenters\n    res = AssignCenters(centers, metric=metric, n_jobs=n_jobs, skip=skip, stride=stride)\n    from pyemma.util.reflection import get_default_args\n    cs = _check_old_chunksize_arg(chunksize, get_default_args(assign_to_centers)['chunksize'], **kwargs)\n    if data is not None:\n        res.estimate(data, chunksize=cs)\n        if return_dtrajs:\n            return res.dtrajs\n    else:\n        res.chunksize = cs\n\n    return res", "response": "r Returns a Voronoi partition with the given data and cluster centers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dtraj_T100K_dt10_n(self, divides):\n        disc = np.zeros(100, dtype=int)\n        divides = np.concatenate([divides, [100]])\n        for i in range(len(divides)-1):\n            disc[divides[i]:divides[i+1]] = i+1\n        return disc[self.dtraj_T100K_dt10]", "response": "100K frames trajectory at timestep 10 arbitrary n - state discretization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_traj(self, N, start=None, stop=None, dt=1):\n        from msmtools.generation import generate_traj\n        return generate_traj(self._P, N, start=start, stop=stop, dt=dt)", "response": "Generates a random trajectory of length N with time step dt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating M random trajectories of length N each with time step dt", "response": "def generate_trajs(self, M, N, start=None, stop=None, dt=1):\n        \"\"\" Generates M random trajectories of length N each with time step dt \"\"\"\n        from msmtools.generation import generate_trajs\n        return generate_trajs(self._P, M, N, start=start, stop=stop, dt=dt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frames_from_files(files, top, frames, chunksize=1000, stride=1, verbose=False, copy_not_join=None, reader=None):\n    # Enforce topology to be a md.Topology object\n    if reader is None:\n        top = _enforce_top(top)\n        reader_given = False\n    else:\n        if not reader.number_of_trajectories():\n            raise ValueError(\"need at least one trajectory file in reader.\")\n        if isinstance(reader, FragmentedTrajectoryReader):\n            top = reader._readers[0][0].featurizer.topology\n        elif isinstance(reader, FeatureReader):\n            top = reader.featurizer.topology\n        else:\n            raise ValueError(\"unsupported reader (only md readers).\")\n        reader_given = True\n\n    stride = int(stride)\n    frames = np.array(frames)\n\n    # only one file, so we expect frames to be a one dimensional array\n    if isinstance(files, str):\n        files = [files]\n        if frames.ndim == 1:\n            # insert a constant column for file index\n            frames = np.insert(np.atleast_2d(frames), 0, np.zeros_like(frames), axis=0).T\n\n    if stride != 1:\n        frames[:, 1] *= stride\n        if verbose:\n            log.info('A stride value of = %u was parsed, '\n                     'interpreting \"indexes\" accordingly.' % stride)\n\n    # sort by file and frame index\n    sort_inds = np.lexsort((frames[:, 1], frames[:, 0]))\n    sorted_inds = frames[sort_inds]\n    assert len(sorted_inds) == len(frames)\n\n    file_inds_unique = np.unique(sorted_inds[:, 0])\n    # construct reader\n    if reader is None:\n        # filter out files, we would never read, because no indices are pointing to them\n        reader = source(np.array(files)[file_inds_unique].tolist(), top=top)\n        # re-map indices to reflect filtered files:\n        for itraj, c in zip(file_inds_unique, itertools.count(0)):\n            mask = sorted_inds[:, 0] == itraj\n            sorted_inds[mask, 0] = c\n\n        inds_to_check = np.arange(len(file_inds_unique))\n    else:\n        inds_to_check = file_inds_unique\n\n    # sanity check of indices\n    for itraj in inds_to_check:\n        inds_by_traj = sorted_inds[sorted_inds[:, 0] == itraj][:, 1]\n        assert inds_by_traj.ndim == 1\n        largest_ind_in_traj = np.max(inds_by_traj)\n        length = reader.trajectory_length(itraj)\n        if largest_ind_in_traj >= length:\n            raise ValueError(\"largest specified index ({largest_without_stride} * stride=\"\n                             \"{largest_without_stride} * {stride}={largest}) \"\n                             \"is larger than trajectory length '{filename}' = {length}\".format(\n                                largest_without_stride=largest_ind_in_traj / stride,\n                                stride=stride,\n                                largest=largest_ind_in_traj,\n                                filename=reader.filenames[itraj],\n                                length=length))\n\n    def set_reader_return_traj_objects(reader, flag):\n        if isinstance(reader, FeatureReader):\n            reader._return_traj_obj = flag\n        elif isinstance(reader, FragmentedTrajectoryReader):\n            for file in reader.filenames_flat:\n                r = reader.reader_by_filename(file)\n                if isinstance(r, FeatureReader):\n                    r = [r]\n                for _r in r:\n                    _r._return_traj_obj = flag\n\n    try:\n        # If the reader got passed in, it could have the data already mapped to memory.\n        # In this case, we cannot force it to return trajectory objects, so we have to re-create it.\n        if reader.in_memory:\n            reader = source(reader.filenames, top=top, chunksize=chunksize)\n        # we want the FeatureReader to return mdtraj.Trajectory objects\n        set_reader_return_traj_objects(reader, True)\n\n        it = reader.iterator(chunk=chunksize, stride=sorted_inds, return_trajindex=False)\n\n        with it:\n            collected_frames = [f for f in it]\n        dest = _preallocate_empty_trajectory(top, len(frames))\n        t = 0\n        for chunk in collected_frames:\n            _copy_traj_attributes(dest, chunk, t)\n            t += len(chunk)\n        # reshuffle the indices of the final trajectory object to obtain the desired order\n        dest = dest.slice(sort_inds.argsort(), copy=False)\n    finally:\n        # in any case we want to reset the reader to its previous state (return features, instead of md.Trajectory)\n        if reader_given:\n            set_reader_return_traj_objects(reader, False)\n    return dest", "response": "Construct a trajectory object from a list of files and a list of frames."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_by_norm(evals, evecs):\n    # norms\n    evnorms = _np.abs(evals)\n    # sort\n    I = _np.argsort(evnorms)[::-1]\n    # permute\n    evals2 = evals[I]\n    evecs2 = evecs[:, I]\n    # done\n    return evals2, evecs2", "response": "Sorts the eigenvalues and eigenvectors by descending norm of the eigenvalues."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spd_eig(W, epsilon=1e-10, method='QR', canonical_signs=False):\n    # check input\n    assert _np.allclose(W.T, W), 'W is not a symmetric matrix'\n\n    if method.lower() == 'qr':\n        from .eig_qr.eig_qr import eig_qr\n        s, V = eig_qr(W)\n    # compute the Eigenvalues of C0 using Schur factorization\n    elif method.lower() == 'schur':\n        from scipy.linalg import schur\n        S, V = schur(W)\n        s = _np.diag(S)\n    else:\n        raise ValueError('method not implemented: ' + method)\n\n    s, V = sort_by_norm(s, V) # sort them\n\n    # determine the cutoff. We know that C0 is an spd matrix,\n    # so we select the truncation threshold such that everything that is negative vanishes\n    evmin = _np.min(s)\n    if evmin < 0:\n        epsilon = max(epsilon, -evmin + 1e-16)\n\n    # determine effective rank m and perform low-rank approximations.\n    evnorms = _np.abs(s)\n    n = _np.shape(evnorms)[0]\n    m = n - _np.searchsorted(evnorms[::-1], epsilon)\n    if m == 0:\n        raise _ZeroRankError('All eigenvalues are smaller than %g, rank reduction would discard all dimensions.'%epsilon)\n    Vm = V[:, 0:m]\n    sm = s[0:m]\n\n    if canonical_signs:\n        # enforce canonical eigenvector signs\n        for j in range(m):\n            jj = _np.argmax(_np.abs(Vm[:, j]))\n            Vm[:, j] *= _np.sign(Vm[jj, j])\n\n    return sm, Vm", "response": "Rank - reduced eigenvalue decomposition of a symmetric positive definite matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute matrix inverse of symmetric positive - definite matrix W.", "response": "def spd_inv(W, epsilon=1e-10, method='QR'):\n    \"\"\"\n    Compute matrix inverse of symmetric positive-definite matrix :math:`W`.\n\n    by first reducing W to a low-rank approximation that is truly spd\n    (Moore-Penrose inverse).\n\n    Parameters\n    ----------\n    W : ndarray((m,m), dtype=float)\n        Symmetric positive-definite (spd) matrix.\n    epsilon : float\n        Truncation parameter. Eigenvalues with norms smaller than this cutoff will\n        be removed.\n    method : str\n        Method to perform the decomposition of :math:`W` before inverting. Options are:\n\n        * 'QR': QR-based robust eigenvalue decomposition of W\n        * 'schur': Schur decomposition of W\n\n    Returns\n    -------\n    L : ndarray((n, r))\n        the Moore-Penrose inverse of the symmetric positive-definite matrix :math:`W`\n\n    \"\"\"\n    if (_np.shape(W)[0] == 1):\n        if W[0,0] < epsilon:\n            raise _ZeroRankError(\n                'All eigenvalues are smaller than %g, rank reduction would discard all dimensions.' % epsilon)\n        Winv = 1./W[0,0]\n    else:\n        sm, Vm = spd_eig(W, epsilon=epsilon, method=method)\n        Winv = _np.dot(Vm, _np.diag(1.0 / sm)).dot(Vm.T)\n\n    # return split\n    return Winv"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the inverse square root of a symmetric positive - definite matrix W.", "response": "def spd_inv_sqrt(W, epsilon=1e-10, method='QR', return_rank=False):\n    \"\"\"\n    Computes :math:`W^{-1/2}` of symmetric positive-definite matrix :math:`W`.\n\n    by first reducing W to a low-rank approximation that is truly spd.\n\n    Parameters\n    ----------\n    W : ndarray((m,m), dtype=float)\n        Symmetric positive-definite (spd) matrix.\n    epsilon : float\n        Truncation parameter. Eigenvalues with norms smaller than this cutoff will\n        be removed.\n    method : str\n        Method to perform the decomposition of :math:`W` before inverting. Options are:\n\n        * 'QR': QR-based robust eigenvalue decomposition of W\n        * 'schur': Schur decomposition of W\n\n    Returns\n    -------\n    L : ndarray((n, r))\n        :math:`W^{-1/2}` after reduction of W to a low-rank spd matrix\n\n    \"\"\"\n    if _np.shape(W)[0] == 1:\n        if W[0,0] < epsilon:\n            raise _ZeroRankError(\n                'All eigenvalues are smaller than %g, rank reduction would discard all dimensions.' % epsilon)\n        Winv = 1./_np.sqrt(W[0, 0])\n        sm = _np.ones(1)\n    else:\n        sm, Vm = spd_eig(W, epsilon=epsilon, method=method)\n        Winv = _np.dot(Vm, _np.diag(1.0 / _np.sqrt(sm))).dot(Vm.T)\n\n    # return split\n    if return_rank:\n        return Winv, sm.shape[0]\n    else:\n        return Winv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes :math:`W^{-1} = L L^T` of the symmetric positive-definite matrix :math:`W`. by first reducing W to a low-rank approximation that is truly spd. Parameters ---------- W : ndarray((m,m), dtype=float) Symmetric positive-definite (spd) matrix. epsilon : float Truncation parameter. Eigenvalues with norms smaller than this cutoff will be removed. method : str Method to perform the decomposition of :math:`W` before inverting. Options are: * 'QR': QR-based robust eigenvalue decomposition of W * 'schur': Schur decomposition of W canonical_signs : boolean, default = False Fix signs in L, s. t. the largest element of in every row of L is positive. Returns ------- L : ndarray((n, r)) Matrix :math:`L` from the decomposition :math:`W^{-1} = L L^T`.", "response": "def spd_inv_split(W, epsilon=1e-10, method='QR', canonical_signs=False):\n    \"\"\"\n    Compute :math:`W^{-1} = L L^T` of the symmetric positive-definite matrix :math:`W`.\n\n    by first reducing W to a low-rank approximation that is truly spd.\n\n    Parameters\n    ----------\n    W : ndarray((m,m), dtype=float)\n        Symmetric positive-definite (spd) matrix.\n    epsilon : float\n        Truncation parameter. Eigenvalues with norms smaller than this cutoff will\n        be removed.\n    method : str\n        Method to perform the decomposition of :math:`W` before inverting. Options are:\n\n        * 'QR': QR-based robust eigenvalue decomposition of W\n        * 'schur': Schur decomposition of W\n\n     canonical_signs : boolean, default = False\n        Fix signs in L, s. t. the largest element of in every row of L is positive.\n\n    Returns\n    -------\n    L : ndarray((n, r))\n        Matrix :math:`L` from the decomposition :math:`W^{-1} = L L^T`.\n\n    \"\"\"\n    if (_np.shape(W)[0] == 1):\n        if W[0,0] < epsilon:\n            raise _ZeroRankError(\n                'All eigenvalues are smaller than %g, rank reduction would discard all dimensions.' % epsilon)\n        L = 1./_np.sqrt(W[0,0])\n    else:\n        sm, Vm = spd_eig(W, epsilon=epsilon, method=method, canonical_signs=canonical_signs)\n        L = _np.dot(Vm, _np.diag(1.0/_np.sqrt(sm)))\n\n    # return split\n    return L"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mdot(*args):\n    if len(args) < 1:\n        raise ValueError('need at least one argument')\n    elif len(args) == 1:\n        return args[0]\n    elif len(args) == 2:\n        return np.dot(args[0], args[1])\n    else:\n        return np.dot(args[0], mdot(*args[1:]))", "response": "Computes a matrix product of multiple ndarrays"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a submatrix of the quadratic matrix M given by the selected columns and row indices.", "response": "def submatrix(M, sel):\n    \"\"\"Returns a submatrix of the quadratic matrix M, given by the selected columns and row\n\n    Parameters\n    ----------\n    M : ndarray(n,n)\n        symmetric matrix\n    sel : int-array\n        selection of rows and columns. Element i,j will be selected if both are in sel.\n\n    Returns\n    -------\n    S : ndarray(m,m)\n        submatrix with m=len(sel)\n\n    \"\"\"\n    assert len(M.shape) == 2, 'M is not a matrix'\n    assert M.shape[0] == M.shape[1], 'M is not quadratic'\n\n    \"\"\"Row slicing\"\"\"\n    if scipy.sparse.issparse(M):\n        C_cc = M.tocsr()\n    else:\n        C_cc = M\n    C_cc = C_cc[sel, :]\n\n    \"\"\"Column slicing\"\"\"\n    if scipy.sparse.issparse(M):\n        C_cc = C_cc.tocsc()\n    C_cc = C_cc[:, sel]\n\n    if scipy.sparse.issparse(M):\n        return C_cc.tocoo()\n    else:\n        return C_cc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sort_by_norm(evals, evecs):\n    # norms\n    evnorms = np.abs(evals)\n    # sort\n    I = np.argsort(evnorms)[::-1]\n    # permute\n    evals2 = evals[I]\n    evecs2 = evecs[:, I]\n    # done\n    return evals2, evecs2", "response": "Sorts the eigenvalues and eigenvectors by descending norm of the eigenvalues."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef meval(self, f, *args, **kw):\n        # !! PART OF ORIGINAL DOCSTRING INCOMPATIBLE WITH CLASS INTERFACE !!\n        # Example\n        # -------\n        # We set up multiple stationary models, one for a reference (ground)\n        # state, and two for biased states, and group them in a\n        # MultiStationaryModel.\n        # >>> from pyemma.thermo import StationaryModel, MEMM\n        # >>> m_1 = StationaryModel(f=[1.0, 0], label='biased 1')\n        # >>> m_2 = StationaryModel(f=[2.0, 0], label='biased 2')\n        # >>> m_mult = MEMM([m_1, m_2], [0, 0], label='unbiased')\n        # Compute the stationary distribution for the two biased models\n        # >>> m_mult.meval('stationary_distribution')\n        # [array([ 0.73105858,  0.26894142]), array([ 0.88079708,  0.11920292])]\n        # We set up multiple Markov state models for different temperatures\n        # and group them in a MultiStationaryModel.\n        # >>> import numpy as np\n        # >>> from pyemma.msm import MSM\n        # >>> from pyemma.thermo import MEMM\n        # >>> b = 20  # transition barrier in kJ / mol\n        # >>> temps = np.arange(300, 500, 25)  # temperatures 300 to 500 K\n        # >>> p_trans = [np.exp(- b / kT) for kT in 0.00831*temps ]\n        # >>> # build MSMs for different temperatures\n        # >>> msms = [MSM(P=np.array([[1.0-p, p], [p, 1.0-p]])) for p in p_trans]\n        # >>> # build Multi-MSM\n        # >>> msm_mult = MEMM(pi=msms[0].stationary_distribution, label='300 K', models=msms)\n        # Compute the timescales and see how they decay with temperature\n        # Greetings to Arrhenius.\n        # >>> np.hstack(msm_mult.meval('timescales'))\n        # array([ 1523.83827932,   821.88040004,   484.06386176,   305.87880068,\n        #          204.64109413,   143.49286817,   104.62539128,    78.83331598])\n        # !! END OF INCOMPATIBLE PART !!\n        return [_call_member(M, f, *args, **kw) for M in self.models]", "response": "Evaluates the given function call for all models returning the results of the calls in a list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_u(K):\n    M = K.shape[0] - 1\n    # Compute right and left eigenvectors:\n    l, U = scl.eig(K.T)\n    l, U = sort_by_norm(l, U)\n    # Extract the eigenvector for eigenvalue one and normalize:\n    u = np.real(U[:, 0])\n    v = np.zeros(M+1)\n    v[M] = 1.0\n    u = u / np.dot(u, v)\n    return u", "response": "Compute the approximation of the ratio stationary over empirical distribution from the whitened and expanded data set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef u(self):\n        'weights in the input basis'\n        self._check_estimated()\n        u_mod = self.u_pc_1\n        N = self._R.shape[0]\n        u_input = np.zeros(N+1)\n        u_input[0:N] = self._R.dot(u_mod[0:-1])  # in input basis\n        u_input[N] = u_mod[-1] - self.mean.dot(self._R.dot(u_mod[0:-1]))\n        return u_input", "response": "weights in the input basis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nweighting in the input basis ( encapsulated in an object )", "response": "def weights(self):\n        'weights in the input basis (encapsulated in an object)'\n        self._check_estimated()\n        u_input = self.u\n        return _KoopmanWeights(u_input[0:-1], u_input[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls the specified method property or attribute of the object.", "response": "def _call_member(obj, name, failfast=True, *args, **kwargs):\n    \"\"\" Calls the specified method, property or attribute of the given object\n\n    Parameters\n    ----------\n    obj : object\n        The object that will be used\n    name : str\n        Name of method, property or attribute\n    failfast : bool\n        If True, will raise an exception when trying a method that doesn't exist. If False, will simply return None\n        in that case\n    args : list, optional, default=[]\n        Arguments to be passed to the method (if any)\n\n    kwargs: dict\n    \"\"\"\n    try:\n        attr = getattr(obj, name)\n    except AttributeError as e:\n        if failfast:\n            raise e\n        else:\n            return None\n    try:\n        if inspect.ismethod(attr):  # call function\n            return attr(*args, **kwargs)\n        elif isinstance(attr, property):  # call property\n                return obj.attr\n        else:  # now it's an Attribute, so we can just return its value\n            return attr\n    except Exception as e:\n        if failfast:\n            raise e\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _estimate_param_scan_worker(estimator, params, X, evaluate, evaluate_args,\n                                failfast, return_exceptions):\n    \"\"\" Method that runs estimation for several parameter settings.\n\n    Defined as a worker for parallelization\n\n    \"\"\"\n    # run estimation\n    model = None\n    try:  # catch any exception\n        estimator.estimate(X, **params)\n        model = estimator.model\n    except KeyboardInterrupt:\n        # we want to be able to interactively interrupt the worker, no matter of failfast=False.\n        raise\n    except:\n        e = sys.exc_info()[1]\n        if isinstance(estimator, Loggable):\n            estimator.logger.warning(\"Ignored error during estimation: %s\" % e)\n        if failfast:\n            raise  # re-raise\n        elif return_exceptions:\n            model = e\n        else:\n            pass  # just return model=None\n\n    # deal with results\n    res = []\n\n    # deal with result\n    if evaluate is None:  # we want full models\n        res.append(model)\n    # we want to evaluate function(s) of the model\n    elif _types.is_iterable(evaluate):\n        values = []  # the function values the model\n        for ieval, name in enumerate(evaluate):\n            # get method/attribute name and arguments to be evaluated\n            #name = evaluate[ieval]\n            args = ()\n            if evaluate_args is not None:\n                args = evaluate_args[ieval]\n                # wrap single arguments in an iterable again to pass them.\n                if _types.is_string(args):\n                    args = (args, )\n            # evaluate\n            try:\n                # try calling method/property/attribute\n                value = _call_member(estimator.model, name, failfast, *args)\n            # couldn't find method/property/attribute\n            except AttributeError as e:\n                if failfast:\n                    raise e  # raise an AttributeError\n                else:\n                    value = None  # we just ignore it and return None\n            values.append(value)\n        # if we only have one value, unpack it\n        if len(values) == 1:\n            values = values[0]\n        res.append(values)\n    else:\n        raise ValueError('Invalid setting for evaluate: ' + str(evaluate))\n\n    if len(res) == 1:\n        res = res[0]\n    return res", "response": "Method that runs estimation for several parameter settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef estimate_param_scan(estimator, X, param_sets, evaluate=None, evaluate_args=None, failfast=True,\n                        return_estimators=False, n_jobs=1, progress_reporter=None, show_progress=True,\n                        return_exceptions=False):\n    \"\"\" Runs multiple estimations using a list of parameter settings\n\n    Parameters\n    ----------\n    estimator : Estimator object or class\n        An estimator object that provides an estimate(X, **params) function.\n        If only a class is provided here, the Estimator objects will be\n        constructed with default parameter settings, and the parameter settings\n        from param_sets for each estimation. If you want to specify other\n        parameter settings for those parameters not specified in param_sets,\n        construct an Estimator before and pass the object.\n\n    param_sets : iterable over dictionaries\n        An iterable that provides parameter settings. Each element defines a\n        parameter set, for which an estimation will be run using these\n        parameters in estimate(X, **params). All other parameter settings will\n        be taken from the default settings in the estimator object.\n\n    evaluate : str or list of str, optional\n        The given methods or properties will be called on the estimated\n        models, and their results will be returned instead of the full models.\n        This may be useful for reducing memory overhead.\n\n    evaluate_args: iterable of iterable, optional\n        Arguments to be passed to evaluated methods. Note, that size has to match to the size of evaluate.\n\n    failfast : bool\n        If True, will raise an exception when estimation failed with an exception\n        or trying to calls a method that doesn't exist. If False, will simply\n        return None in these cases.\n\n    return_estimators: bool\n        If True, return a list estimators in addition to the models.\n\n    show_progress: bool\n        if the given estimator supports show_progress interface, we set the flag\n        prior doing estimations.\n\n    return_exceptions: bool, default=False\n        if failfast is False while this setting is True, returns the exception thrown at the actual grid element,\n        instead of None.\n\n    Returns\n    -------\n    models : list of model objects or evaluated function values\n        A list of estimated models in the same order as param_sets. If evaluate\n        is given, each element will contain the results from these method\n        evaluations.\n\n    estimators (optional) : list of estimator objects. These are returned only\n        if return_estimators=True\n\n    Examples\n    --------\n\n    Estimate a maximum likelihood Markov model at lag times 1, 2, 3.\n\n    >>> from pyemma.msm.estimators import MaximumLikelihoodMSM, BayesianMSM\n    >>>\n    >>> dtraj = [0,0,1,2,1,0,1,0,1,2,2,0,0,0,1,1,2,1,0,0,1,2,1,0,0,0,1,1,0,1,2]  # mini-trajectory\n    >>> param_sets=param_grid({'lag': [1,2,3]})\n    >>>\n    >>> estimate_param_scan(MaximumLikelihoodMSM, dtraj, param_sets, evaluate='timescales')\n    [array([ 1.24113168,  0.77454377]), array([ 2.65266698,  1.42909842]), array([ 5.34810405,  1.14784446])]\n\n    Now we also want to get samples of the timescales using the BayesianMSM.\n    >>> estimate_param_scan(MaximumLikelihoodMSM, dtraj, param_sets, failfast=False,\n    ...     evaluate=['timescales', 'timescales_samples']) # doctest: +SKIP\n    [[array([ 1.24113168,  0.77454377]), None], [array([ 2.48226337,  1.54908754]), None], [array([ 3.72339505,  2.32363131]), None]]\n\n    We get Nones because the MaximumLikelihoodMSM estimator doesn't provide timescales_samples. Use for example\n    a Bayesian estimator for that.\n\n    Now we also want to get samples of the timescales using the BayesianMSM.\n    >>> estimate_param_scan(BayesianMSM, dtraj, param_sets, show_progress=False,\n    ...     evaluate=['timescales', 'sample_f'], evaluate_args=((), ('timescales', ))) # doctest: +SKIP\n    [[array([ 1.24357685,  0.77609028]), [array([ 1.5963252 ,  0.73877883]), array([ 1.29915847,  0.49004912]), array([ 0.90058583,  0.73841786]), ... ]]\n\n    \"\"\"\n    # make sure we have an estimator object\n    estimator = get_estimator(estimator)\n    if hasattr(estimator, 'show_progress'):\n        estimator.show_progress = show_progress\n\n    if n_jobs is None:\n        from pyemma._base.parallel import get_n_jobs\n        n_jobs = get_n_jobs(logger=getattr(estimator, 'logger', None))\n\n    # if we want to return estimators, make clones. Otherwise just copy references.\n    # For parallel processing we always need clones.\n    # Also if the Estimator is its own Model, we have to clone.\n    from pyemma._base.model import Model\n    if (return_estimators or\n        n_jobs > 1 or n_jobs is None or\n        isinstance(estimator, Model)):\n        estimators = [clone_estimator(estimator) for _ in param_sets]\n    else:\n        estimators = [estimator for _ in param_sets]\n\n    # only show progress of parameter study.\n    if hasattr(estimators[0], 'show_progress'):\n        for e in estimators:\n            e.show_progress = False\n\n    # if we evaluate, make sure we have a list of functions to evaluate\n    if _types.is_string(evaluate):\n        evaluate = [evaluate]\n    if _types.is_string(evaluate_args):\n        evaluate_args = [evaluate_args]\n\n    if evaluate is not None and evaluate_args is not None and len(evaluate) != len(evaluate_args):\n        raise ValueError(\"length mismatch: evaluate ({}) and evaluate_args ({})\".format(len(evaluate), len(evaluate_args)))\n\n    logger_available = hasattr(estimators[0], 'logger')\n    if logger_available:\n        logger = estimators[0].logger\n    if progress_reporter is None:\n        from unittest.mock import MagicMock\n        ctx = progress_reporter = MagicMock()\n        callback = None\n    else:\n        ctx = progress_reporter._progress_context('param-scan')\n        callback = lambda _: progress_reporter._progress_update(1, stage='param-scan')\n\n        progress_reporter._progress_register(len(estimators), stage='param-scan',\n                                             description=\"estimating %s\" % str(estimator.__class__.__name__))\n\n    # TODO: test on win, osx\n    if n_jobs > 1 and os.name == 'posix':\n        if logger_available:\n            logger.debug('estimating %s with n_jobs=%s', estimator, n_jobs)\n        # iterate over parameter settings\n        task_iter = ((estimator,\n                      param_set, X,\n                      evaluate,\n                      evaluate_args,\n                      failfast, return_exceptions)\n                     for estimator, param_set in zip(estimators, param_sets))\n\n        from pathos.multiprocessing import Pool\n        pool = Pool(processes=n_jobs)\n        args = list(task_iter)\n\n        from contextlib import closing\n\n        def error_callback(*args, **kw):\n            if failfast:\n                # TODO: can we be specific here? eg. obtain the stack of the actual process or is this the master proc?\n                raise Exception('something failed')\n\n        with closing(pool), ctx:\n            res_async = [pool.apply_async(_estimate_param_scan_worker, a, callback=callback,\n                                          error_callback=error_callback) for a in args]\n            res = [x.get() for x in res_async]\n\n    # if n_jobs=1 don't invoke the pool, but directly dispatch the iterator\n    else:\n        if logger_available:\n            logger.debug('estimating %s with n_jobs=1 because of the setting or '\n                         'you not have a POSIX system', estimator)\n        res = []\n        with ctx:\n            for estimator, param_set in zip(estimators, param_sets):\n                res.append(_estimate_param_scan_worker(estimator, param_set, X,\n                                                       evaluate, evaluate_args, failfast, return_exceptions))\n                if progress_reporter is not None:\n                    progress_reporter._progress_update(1, stage='param-scan')\n\n    # done\n    if return_estimators:\n        return res, estimators\n    else:\n        return res", "response": "Runs multiple estimations using a list of parameter sets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef estimate(self, X, **params):\n        # set params\n        if params:\n            self.set_params(**params)\n        self._model = self._estimate(X)\n        # ensure _estimate returned something\n        assert self._model is not None\n        self._estimated = True\n        return self", "response": "Estimates the model given the data X and returns the model object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unregister_signal_handlers():\n    signal.signal(SIGNAL_STACKTRACE, signal.SIG_IGN)\n    signal.signal(SIGNAL_PDB, signal.SIG_IGN)", "response": "set signal handlers to default"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef selection_strategy(oasis_obj, strategy='spectral-oasis', nsel=1, neig=None):\n    strategy = strategy.lower()\n    if strategy == 'random':\n        return SelectionStrategyRandom(oasis_obj, strategy, nsel=nsel, neig=neig)\n    elif strategy == 'oasis':\n        return SelectionStrategyOasis(oasis_obj, strategy, nsel=nsel, neig=neig)\n    elif strategy == 'spectral-oasis':\n        return SelectionStrategySpectralOasis(oasis_obj, strategy, nsel=nsel, neig=neig)\n    else:\n        raise ValueError('Selected strategy is unknown: '+str(strategy))", "response": "Factory for selection strategy object\n    Returns ------- selstr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the absolute error of the Nystroem approximation for each column", "response": "def _compute_error(self):\n        \"\"\" Evaluate the absolute error of the Nystroem approximation for each column \"\"\"\n        # err_i = sum_j R_{k,ij} A_{k,ji} - d_i\n        self._err = np.sum(np.multiply(self._R_k, self._C_k.T), axis=0) - self._d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_selection_strategy(self, strategy='spectral-oasis', nsel=1, neig=None):\n        self._selection_strategy = selection_strategy(self, strategy, nsel, neig)", "response": "Defines the column selection strategy for the given set of unique entries in the log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a column to the Nystroem approximation and updates the local matrices.", "response": "def add_column(self, col, icol, update_error=True):\n        \"\"\" Attempts to add a single column of :math:`A` to the Nystroem approximation and updates the local matrices\n\n        Parameters\n        ----------\n        col : ndarray((N,), dtype=float)\n            new column of :math:`A`\n        icol : int\n            index of new column within :math:`A`\n        update_error : bool, optional, default = True\n            If True, the absolute and relative approximation error will be updated after adding the column.\n            If False, then not.\n\n        Return\n        ------\n        success : bool\n            True if the new column was added to the approximation. False if not.\n\n        \"\"\"\n        # convenience access\n        k = self._k\n        d = self._d\n        R = self._R_k\n        Winv = self._W_k_inv\n\n        b_new = col[self._columns][:, None]\n        d_new = d[icol]\n        q_new = R[:, icol][:, None]\n\n        # calculate R_new\n        schur_complement = d_new - np.dot(b_new.T, q_new)  # Schur complement\n        if np.isclose(schur_complement, 0):\n            return False\n\n        # otherwise complete the update\n        s_new = 1./schur_complement\n        qC = np.dot(b_new.T, R)\n\n        # update Winv\n        Winv_new = np.zeros((k+1, k+1))\n        Winv_new[0:k, 0:k] = Winv+s_new*np.dot(q_new, q_new.T)\n        Winv_new[0:k, k] = -s_new*q_new[0:k, 0]\n        Winv_new[k, 0:k] = -s_new*q_new[0:k, 0].T\n        Winv_new[k, k] = s_new\n\n        R_new = np.vstack((R + s_new * np.dot(q_new, (qC - col.T)), s_new*(-qC + col.T)))\n\n        # forcing known structure on R_new\n        sel_new = np.append(self._columns, icol)\n        R_new[:, sel_new] = np.eye(k+1)\n\n        # update Winv\n        self._W_k_inv = Winv_new\n        # update R\n        self._R_k = R_new\n        # update C0_k\n        self._C_k = np.hstack((self._C_k, col[:, None]))\n        # update number of selected columns\n        self._k += 1\n        # add column to present selection\n        self._columns = np.append(self._columns, icol)\n\n        # update error\n        if update_error:\n            self._compute_error()\n\n        # exit with success\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef approximate_column(self, i):\n        return np.dot(self._C_k, self._R_k[:, i])", "response": "Computes the Nystroem approximation of column i of matrix A."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef approximate_eig(self, epsilon=1e-6):\n        L = self.approximate_cholesky(epsilon=epsilon)\n        LL = np.dot(L.T, L)\n        s, V = np.linalg.eigh(LL)\n        # sort\n        s, V = sort_by_norm(s, V)\n\n        # back-transform eigenvectors\n        Linv = np.linalg.pinv(L.T)\n        V = np.dot(Linv, V)\n\n        # normalize eigenvectors\n        ncol = V.shape[1]\n        for i in range(ncol):\n            if not np.allclose(V[:, i], 0):\n                V[:, i] /= np.sqrt(np.dot(V[:, i], V[:, i]))\n\n        return s, V", "response": "Compute the low - rank approximation of the eigenvalue decomposition of the target matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects next column indexes according to defined strategy Returns a new array of column indexes.", "response": "def select(self):\n        \"\"\" Selects next column indexes according to defined strategy\n\n        Returns\n        -------\n        cols : ndarray((nsel,), dtype=int)\n            selected columns\n\n        \"\"\"\n        err = self._oasis_obj.error\n        if np.allclose(err, 0):\n            return None\n        nsel = self._check_nsel()\n        if nsel is None:\n            return None\n        return self._select(nsel, err)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of jobs and threads that are used during assignment of data.", "response": "def n_jobs(self):\n        \"\"\" Returns number of jobs/threads to use during assignment of data.\n\n        Returns\n        -------\n        If None it will return the setting of 'PYEMMA_NJOBS' or\n        'SLURM_CPUS_ON_NODE' environment variable. If none of these environment variables exist,\n        the number of processors /or cores is returned.\n\n        Notes\n        -----\n        This setting will effectively be multiplied by the the number of threads used by NumPy for\n        algorithms which use multiple processes. So take care if you choose this manually.\n        \"\"\"\n        if not hasattr(self, '_n_jobs'):\n            self._n_jobs = get_n_jobs(logger=getattr(self, 'logger'))\n        return self._n_jobs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the model with given name.", "response": "def delete(self, name):\n        \"\"\" deletes model with given name \"\"\"\n        if name not in self._parent:\n            raise KeyError('model \"{}\" not present'.format(name))\n        del self._parent[name]\n        if self._current_model_group == name:\n            self._current_model_group = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nselecting a model from the parent", "response": "def select_model(self, name):\n        \"\"\" choose an existing model \"\"\"\n        if name not in self._parent:\n            raise KeyError('model \"{}\" not present'.format(name))\n        self._current_model_group = name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all stored models in given file.", "response": "def models_descriptive(self):\n        \"\"\" list all stored models in given file.\n\n        Returns\n        -------\n        dict: {model_name: {'repr' : 'string representation, 'created': 'human readable date', ...}\n\n        \"\"\"\n        f = self._parent\n        return {name: {a: f[name].attrs[a]\n                       for a in H5File.stored_attributes}\n                for name in f.keys()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _progress_register(self, amount_of_work, description='', stage=0, tqdm_args=None):\n        if not self.show_progress:\n            return\n\n        if tqdm_args is None:\n            tqdm_args = {}\n\n        if not isinstance(amount_of_work, Integral):\n            raise ValueError('amount_of_work has to be of integer type. But is {}'.format(type(amount_of_work)))\n\n        # if we do not have enough work to do for the overhead of a progress bar just dont create a bar.\n        if amount_of_work <= ProgressReporterMixin._pg_threshold:\n            pg = None\n        else:\n            args = dict(total=amount_of_work, desc=description, dynamic_ncols=True, **tqdm_args)\n            if _attached_to_ipy_notebook_with_widgets():\n                from .notebook import my_tqdm_notebook\n                pg = my_tqdm_notebook(leave=False, **args)\n            else:\n                import tqdm\n                pg = tqdm.tqdm(leave=True, **args)\n\n        self._prog_rep_progressbars[stage] = pg\n        self._prog_rep_descriptions[stage] = description\n        assert stage in self._prog_rep_progressbars\n        assert stage in self._prog_rep_descriptions", "response": "Registers a progress which can be reported via a progress bar widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _progress_set_description(self, stage, description):\n        self.__check_stage_registered(stage)\n        self._prog_rep_descriptions[stage] = description\n        if self._prog_rep_progressbars[stage]:\n            self._prog_rep_progressbars[stage].set_description(description, refresh=False)", "response": "set description of an already existing progress"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _progress_update(self, numerator_increment, stage=0, show_eta=True, **kw):\n        if not self.show_progress:\n            return\n\n        self.__check_stage_registered(stage)\n\n        if not self._prog_rep_progressbars[stage]:\n            return\n\n        pg = self._prog_rep_progressbars[stage]\n        pg.update(int(numerator_increment))", "response": "Updates the progress bars. Will update progress bars or other progress output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _map_to_memory(self, stride=1):\n        self._mapping_to_mem_active = True\n        try:\n            self._Y = self.get_output(stride=stride)\n            from pyemma.coordinates.data import DataInMemory\n            self._Y_source = DataInMemory(self._Y)\n        finally:\n            self._mapping_to_mem_active = False\n\n        self._in_memory = True", "response": "Maps results to memory. Will be stored in attribute _Y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the dimension of the user - input", "response": "def dimension(self):\n        \"\"\" output dimension \"\"\"\n        if self.C00 is None:  # no data yet\n            if isinstance(self.dim, int):  # return user choice\n                warnings.warn('Returning user-input for dimension, since this model has not yet been estimated.')\n                return self.dim\n            raise RuntimeError('Please call set_model_params prior using this method.')\n\n        if not self._svd_performed:\n            self._diagonalize()\n        return self._dimension(self._rank0, self._rankt, self.dim, self.singular_values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming SVD on the covariance matrices and saves left right singular vectors and values in the model.", "response": "def _diagonalize(self):\n        \"\"\"Performs SVD on covariance matrices and save left, right singular vectors and values in the model.\n\n        Parameters\n        ----------\n        scaling : None or string, default=None\n            Scaling to be applied to the VAMP modes upon transformation\n            * None: no scaling will be applied, variance of the singular\n              functions is 1\n            * 'kinetic map' or 'km': singular functions are scaled by\n              singular value. Note that only the left singular functions\n              induce a kinetic map.\n        \"\"\"\n        L0 = spd_inv_split(self.C00, epsilon=self.epsilon)\n        self._rank0 = L0.shape[1] if L0.ndim == 2 else 1\n        Lt = spd_inv_split(self.Ctt, epsilon=self.epsilon)\n        self._rankt = Lt.shape[1] if Lt.ndim == 2 else 1\n\n        W = np.dot(L0.T, self.C0t).dot(Lt)\n        from scipy.linalg import svd\n        A, s, BT = svd(W, compute_uv=True, lapack_driver='gesvd')\n\n        self._singular_values = s\n\n        # don't pass any values in the argument list that call _diagonalize again!!!\n        m = VAMPModel._dimension(self._rank0, self._rankt, self.dim, self._singular_values)\n\n        U = np.dot(L0, A[:, :m])\n        V = np.dot(Lt, BT[:m, :].T)\n\n        # scale vectors\n        if self.scaling is not None:\n            U *= s[np.newaxis, 0:m]  # scaled left singular functions induce a kinetic map\n            V *= s[np.newaxis, 0:m]  # scaled right singular functions induce a kinetic map wrt. backward propagator\n\n        self._U = U\n        self._V = V\n        self._svd_performed = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the VAMP score for this model or the cross - validation score between self and test model.", "response": "def score(self, test_model=None, score_method='VAMP2'):\n        \"\"\"Compute the VAMP score for this model or the cross-validation score between self and a second model.\n\n        Parameters\n        ----------\n        test_model : VAMPModel, optional, default=None\n\n            If `test_model` is not None, this method computes the cross-validation score\n            between self and `test_model`. It is assumed that self was estimated from\n            the \"training\" data and `test_model` was estimated from the \"test\" data. The\n            score is computed for one realization of self and `test_model`. Estimation\n            of the average cross-validation score and partitioning of data into test and\n            training part is not performed by this method.\n\n            If `test_model` is None, this method computes the VAMP score for the model\n            contained in self.\n\n        score_method : str, optional, default='VAMP2'\n            Available scores are based on the variational approach for Markov processes [1]_:\n\n            *  'VAMP1'  Sum of singular values of the half-weighted Koopman matrix [1]_ .\n                        If the model is reversible, this is equal to the sum of\n                        Koopman matrix eigenvalues, also called Rayleigh quotient [1]_.\n            *  'VAMP2'  Sum of squared singular values of the half-weighted Koopman matrix [1]_ .\n                        If the model is reversible, this is equal to the kinetic variance [2]_ .\n            *  'VAMPE'  Approximation error of the estimated Koopman operator with respect to\n                        the true Koopman operator up to an additive constant [1]_ .\n\n        Returns\n        -------\n        score : float\n            If `test_model` is not None, returns the cross-validation VAMP score between\n            self and `test_model`. Otherwise return the selected VAMP-score of self.\n\n        References\n        ----------\n        .. [1] Wu, H. and Noe, F. 2017. Variational approach for learning Markov processes from time series data.\n            arXiv:1707.04659v1\n        .. [2] Noe, F. and Clementi, C. 2015. Kinetic distance and kinetic maps from molecular dynamics simulation.\n            J. Chem. Theory. Comput. doi:10.1021/acs.jctc.5b00553\n        \"\"\"\n        # TODO: implement for TICA too\n        if test_model is None:\n            test_model = self\n        Uk = self.U[:, 0:self.dimension()]\n        Vk = self.V[:, 0:self.dimension()]\n        res = None\n        if score_method == 'VAMP1' or score_method == 'VAMP2':\n            A = spd_inv_sqrt(Uk.T.dot(test_model.C00).dot(Uk))\n            B = Uk.T.dot(test_model.C0t).dot(Vk)\n            C = spd_inv_sqrt(Vk.T.dot(test_model.Ctt).dot(Vk))\n            ABC = mdot(A, B, C)\n            if score_method == 'VAMP1':\n                res = np.linalg.norm(ABC, ord='nuc')\n            elif score_method == 'VAMP2':\n                res = np.linalg.norm(ABC, ord='fro')**2\n        elif score_method == 'VAMPE':\n            Sk = np.diag(self.singular_values[0:self.dimension()])\n            res = np.trace(2.0 * mdot(Vk, Sk, Uk.T, test_model.C0t) - mdot(Vk, Sk, Uk.T, test_model.C00, Uk, Sk, Vk.T, test_model.Ctt))\n        else:\n            raise ValueError('\"score\" should be one of VAMP1, VAMP2 or VAMPE')\n        # add the contribution (+1) of the constant singular functions to the result\n        assert res\n        return res + 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _reshape(self, array, dry=False):\n        import functools, numpy as np\n        if array.ndim == 1:\n            shape = (array.shape[0], 1)\n        else:\n            # hold first dimension, multiply the rest\n            shape = (array.shape[0], functools.reduce(lambda x, y: x * y, array.shape[1:]))\n        if not dry:\n            array = np.reshape(array, shape)\n        return array, shape", "response": "reshape given array to 2d."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_umbrella_sampling_data(ntherm=11, us_fc=20.0, us_length=500, md_length=1000, nmd=20):\n    dws = _DWS()\n    us_data = dws.us_sample(\n        ntherm=ntherm, us_fc=us_fc, us_length=us_length, md_length=md_length, nmd=nmd)\n    us_data.update(centers=dws.centers)\n    return us_data", "response": "This function returns the umbrella sampling data for a given set of umbrella trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the multi - temperature data for a single MCMC process in an asymmetric double well potential at multiple temperatures.", "response": "def get_multi_temperature_data(kt0=1.0, kt1=5.0, length0=10000, length1=10000, n0=10, n1=10):\n    \"\"\"\n    Continuous MCMC process in an asymmetric double well potential at multiple temperatures.\n\n    Parameters\n    ----------\n    kt0: double, optional, default=1.0\n        Temperature in kT for the first thermodynamic state.\n    kt1: double, optional, default=5.0\n        Temperature in kT for the second thermodynamic state.\n    length0: int, optional, default=10000\n        Trajectory length in steps for the first thermodynamic state.\n    length1: int, optional, default=10000\n        Trajectory length in steps for the second thermodynamic state.\n    n0: int, optional, default=10\n        Number of trajectories in the first thermodynamic state.\n    n1: int, optional, default=10\n        Number of trajectories in the second thermodynamic state.\n\n    Returns\n    -------\n    dict - keys shown below in brackets\n        Trajectory (trajs), energy (energy_trajs), and temperature (temp_trajs) data from the MCMC\n        runs as well as the discretised version (dtrajs + centers). Energies and temperatures are\n        given in kT, lengths in arbitrary units.\n    \"\"\"\n    dws = _DWS()\n    mt_data = dws.mt_sample(\n        kt0=kt0, kt1=kt1, length0=length0, length1=length1, n0=n0, n1=n1)\n    mt_data.update(centers=dws.centers)\n    return mt_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the size of num in a human readable form up to Yottabytes.", "response": "def bytes_to_string(num, suffix='B'):\n    \"\"\"\n    Returns the size of num (bytes) in a human readable form up to Yottabytes (YB).\n    :param num: The size of interest in bytes.\n    :param suffix: A suffix, default 'B' for 'bytes'.\n    :return: a human readable representation of a size in bytes\n    \"\"\"\n    extensions = [\"%s%s\" % (x, suffix) for x in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']]\n    if num == 0:\n        return \"0%s\" % extensions[0]\n    else:\n        n_bytes = float(abs(num))\n        place = int(math.floor(math.log(n_bytes, 1024)))\n        return \"%.1f%s\" % (np.sign(num) * (n_bytes / 1024** place), extensions[place])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef string_to_bytes(string):\n    if string == '0':\n        return 0\n    import re\n    match = re.match('(\\d+\\.?\\d?)\\s?([bBkKmMgGtTpPeEzZyY])?(\\D?)', string)\n    if not match:\n        raise RuntimeError('\"{}\" does not match \"[integer] [suffix]\"'.format(string))\n    if match.group(3):\n        raise RuntimeError('unknown suffix: \"{}\"'.format(match.group(3)))\n    value = float(match.group(1))\n    if match.group(2) is None:\n        return int(value)\n    suffix = match.group(2).upper()\n    extensions = ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']\n    x = extensions.index(suffix)\n    value *= 1024**x\n    return int(value)", "response": "Converts a string representation of a number to a number of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a new time unit scaled by the given factor", "response": "def get_scaled(self, factor):\n        \"\"\" Get a new time unit, scaled by the given factor \"\"\"\n        res = TimeUnit(self)\n        res._factor = self._factor * factor\n        res._unit = self._unit\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsuggest a rescaling factor and new physical time unit to balance the given time multiples around 1.", "response": "def rescale_around1(self, times):\n        \"\"\"\n        Suggests a rescaling factor and new physical time unit to balance the given time multiples around 1.\n\n        Parameters\n        ----------\n        times : float array\n            array of times in multiple of the present elementary unit\n\n        \"\"\"\n        if self._unit == self._UNIT_STEP:\n            return times, 'step' # nothing to do\n\n        m = np.mean(times)\n        mult = 1.0\n        cur_unit = self._unit\n\n        # numbers are too small. Making them larger and reducing the unit:\n        if (m < 0.001):\n            while mult*m < 0.001 and cur_unit >= 0:\n                mult *= 1000\n                cur_unit -= 1\n            return mult*times, self._unit_names[cur_unit]\n\n        # numbers are too large. Making them smaller and increasing the unit:\n        if (m > 1000):\n            while mult*m > 1000 and cur_unit <= 5:\n                mult /= 1000\n                cur_unit += 1\n            return mult*times, self._unit_names[cur_unit]\n\n        # nothing to do\n        return times, self._unit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all models in given filename.", "response": "def list_models(filename):\n    \"\"\" Lists all models in given filename.\n\n    Parameters\n    ----------\n    filename: str\n        path to filename, where the model has been stored.\n\n    Returns\n    -------\n    obj: dict\n        A mapping by name and a comprehensive description like this:\n        {model_name: {'repr' : 'string representation, 'created': 'human readable date', ...}\n    \"\"\"\n    from .h5file import H5File\n    with H5File(filename, mode='r') as f:\n        return f.models_descriptive"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_iterable_of_int(l):\n    if not is_iterable(l):\n        return False\n\n    return all(is_int(value) for value in l)", "response": "r Checks if l is iterable and contains only integral types"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_int_vector(l):\n    if isinstance(l, np.ndarray):\n        if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'):\n            return True\n    return False", "response": "r Checks if l is a numpy array of integers\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_bool_matrix(l):\n    if isinstance(l, np.ndarray):\n        if l.ndim == 2 and (l.dtype == bool):\n            return True\n    return False", "response": "r Checks if l is a 2D numpy array of bools\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_dtraj_list(dtrajs):\n    if isinstance(dtrajs, list):\n        # elements are ints? then wrap into a list\n        if is_list_of_int(dtrajs):\n            return [np.array(dtrajs, dtype=int)]\n        else:\n            for i, dtraj in enumerate(dtrajs):\n                dtrajs[i] = ensure_dtraj(dtraj)\n            return dtrajs\n    else:\n        return [ensure_dtraj(dtrajs)]", "response": "r Make sure that dtrajs is a list of discrete trajectories"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ensure_int_vector(I, require_order = False):\n    if is_int_vector(I):\n        return I\n    elif is_int(I):\n        return np.array([I])\n    elif is_list_of_int(I):\n        return np.array(I)\n    elif is_tuple_of_int(I):\n        return np.array(I)\n    elif isinstance(I, set):\n        if require_order:\n            raise TypeError('Argument is an unordered set, but I require an ordered array of integers')\n        else:\n            lI = list(I)\n            if is_list_of_int(lI):\n                return np.array(lI)\n    else:\n        raise TypeError('Argument is not of a type that is convertible to an array of integers.')", "response": "Checks if the argument can be converted to an array of ints and does that."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_int_vector_or_None(F, require_order = False):\n    if F is None:\n        return F\n    else:\n        return ensure_int_vector(F, require_order = require_order)", "response": "Ensures that F is either None or a numpy array of floats."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_float_vector(F, require_order = False):\n    if is_float_vector(F):\n        return F\n    elif is_float(F):\n        return np.array([F])\n    elif is_iterable_of_float(F):\n        return np.array(F)\n    elif isinstance(F, set):\n        if require_order:\n            raise TypeError('Argument is an unordered set, but I require an ordered array of floats')\n        else:\n            lF = list(F)\n            if is_list_of_float(lF):\n                return np.array(lF)\n    else:\n        raise TypeError('Argument is not of a type that is convertible to an array of floats.')", "response": "Ensures that F is a numpy array of floats and returns F if it is not already."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that F is either None or a numpy array of floats.", "response": "def ensure_float_vector_or_None(F, require_order = False):\n    \"\"\"Ensures that F is either None, or a numpy array of floats\n\n    If F is already either None or a numpy array of floats, F is returned (no copied!)\n    Otherwise, checks if the argument can be converted to an array of floats and does that.\n\n    Parameters\n    ----------\n    F: float, list of float or 1D-ndarray of float\n\n    Returns\n    -------\n    arr : ndarray(n)\n        numpy array with the floats contained in the argument\n\n    \"\"\"\n    if F is None:\n        return F\n    else:\n        return ensure_float_vector(F, require_order = require_order)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assert_array(A, shape=None, uniform=None, ndim=None, size=None, dtype=None, kind=None):\n    try:\n        if shape is not None:\n            if not np.array_equal(np.shape(A), shape):\n                raise AssertionError('Expected shape '+str(shape)+' but given array has shape '+str(np.shape(A)))\n        if uniform is not None:\n            shapearr = np.array(np.shape(A))\n            is_uniform = np.count_nonzero(shapearr-shapearr[0]) == 0\n            if uniform and not is_uniform:\n                raise AssertionError('Given array is not uniform \\n'+str(shapearr))\n            elif not uniform and is_uniform:\n                raise AssertionError('Given array is not nonuniform: \\n'+str(shapearr))\n        if size is not None:\n            if not np.size(A) == size:\n                raise AssertionError('Expected size '+str(size)+' but given array has size '+str(np.size(A)))\n        if ndim is not None:\n            if not ndim == np.ndim(A):\n                raise AssertionError('Expected shape '+str(ndim)+' but given array has shape '+str(np.ndim(A)))\n        if dtype is not None:\n            # now we must create an array if we don't have one yet\n            if not isinstance(A, (np.ndarray)) and not scisp.issparse(A):\n                A = np.array(A)\n            if not np.dtype(dtype) == A.dtype:\n                raise AssertionError('Expected data type '+str(dtype)+' but given array has data type '+str(A.dtype))\n        if kind is not None:\n            # now we must create an array if we don't have one yet\n            if not isinstance(A, (np.ndarray)) and not scisp.issparse(A):\n                A = np.array(A)\n            if kind == 'numeric':\n                if not (A.dtype.kind == 'i' or A.dtype.kind == 'f'):\n                    raise AssertionError('Expected numerical data, but given array has data kind '+str(A.dtype.kind))\n            elif not A.dtype.kind == kind:\n                raise AssertionError('Expected data kind '+str(kind)\n                                     +' but given array has data kind '+str(A.dtype.kind))\n    except Exception as ex:\n        if isinstance(ex, AssertionError):\n            raise ex\n        else:  # other exception raised in the test code above\n            print('Found exception: ',ex)\n            raise AssertionError('Given argument is not an array of the expected shape or type:\\n'+\n                                 'arg = '+str(A)+'\\ntype = '+str(type(A)))", "response": "r Asserts whether the given array or sparse matrix A has the given properties."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_traj(traj):\n    if is_float_matrix(traj) or is_bool_matrix(traj):\n        return traj\n    elif is_float_vector(traj):\n        return traj[:,None]\n    else:\n        try:\n            arr = np.array(traj)\n            arr = ensure_dtype_float(arr)\n            if is_float_matrix(arr):\n                return arr\n            if is_float_vector(arr):\n                return arr[:,None]\n            else:\n                raise TypeError('Argument traj cannot be cast into a two-dimensional array. Check type.')\n        except:\n            raise TypeError('Argument traj is not a trajectory - only float-arrays or list of float-arrays are allowed. Types is %s' % type(traj))", "response": "rMakes sure that trajectory is a trajectory ( array of float )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _describe_atom(topology, index):\n    at = topology.atom(index)\n    if topology.n_chains > 1:\n        return \"%s %i %s %i %i\" % (at.residue.name, at.residue.resSeq, at.name, at.index, at.residue.chain.index )\n    else:\n        return \"%s %i %s %i\"    % (at.residue.name, at.residue.resSeq, at.name, at.index)", "response": "Returns a string describing the given atom"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two mdtraj. Trajectory objects.", "response": "def cmp_traj(traj_a, traj_b):\n    \"\"\"\n    Parameters\n    ----------\n    traj_a, traj_b: mdtraj.Trajectory\n    \"\"\"\n    if traj_a is None and traj_b is None:\n        return True\n    if traj_a is None and traj_b is not None:\n        return False\n    if traj_a is not None and traj_b is None:\n        return False\n    equal_top = traj_a.top == traj_b.top\n    xyz_close = np.allclose(traj_a.xyz, traj_b.xyz)\n    equal_time = np.all(traj_a.time == traj_b.time)\n    equal_unitcell_angles = np.array_equal(traj_a.unitcell_angles, traj_b.unitcell_angles)\n    equal_unitcell_lengths = np.array_equal(traj_a.unitcell_lengths, traj_b.unitcell_lengths)\n    return np.all([equal_top, equal_time, xyz_close, equal_time, equal_unitcell_angles, equal_unitcell_lengths])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_pairwise_input(indices1, indices2, MDlogger, fname=''):\n\n    if is_iterable_of_int(indices1):\n        MDlogger.warning('The 1D arrays input for %s have been sorted, and '\n                         'index duplicates have been eliminated.\\n'\n                         'Check the output of describe() to see the actual order of the features' % fname)\n\n        # Eliminate duplicates and sort\n        indices1 = np.unique(indices1)\n\n        # Intra-group distances\n        if indices2 is None:\n            atom_pairs = combinations(indices1, 2)\n\n        # Inter-group distances\n        elif is_iterable_of_int(indices2):\n\n            # Eliminate duplicates and sort\n            indices2 = np.unique(indices2)\n\n            # Eliminate duplicates between indices1 and indices1\n            uniqs = np.in1d(indices2, indices1, invert=True)\n            indices2 = indices2[uniqs]\n            atom_pairs = product(indices1, indices2)\n\n    else:\n        atom_pairs = indices1\n\n    return atom_pairs", "response": "r Returns a list of pairs of indices in indices1 and indices2."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _atoms_in_residues(top, residue_idxs, subset_of_atom_idxs=None, fallback_to_full_residue=True, MDlogger=None):\n    atoms_in_residues = []\n    if subset_of_atom_idxs is None:\n        subset_of_atom_idxs = np.arange(top.n_atoms)\n    special_residues = []\n    for rr in top.residues:\n        if rr.index in residue_idxs:\n            toappend = np.array([aa.index for aa in rr.atoms if aa.index in subset_of_atom_idxs])\n            if len(toappend) == 0:\n                special_residues.append(rr)\n                if fallback_to_full_residue:\n                    toappend = np.array([aa.index for aa in rr.atoms])\n\n            atoms_in_residues.append(toappend)\n\n    # Any special cases?\n    if len(special_residues) != 0 and hasattr(MDlogger, 'warning'):\n        if fallback_to_full_residue:\n            msg = 'the full residue'\n        else:\n            msg = 'emtpy lists'\n        MDlogger.warning(\"These residues yielded no atoms in the subset and were returned as %s: %s \" % (\n        msg, ''.join(['%s, ' % rr for rr in special_residues])[:-2]))\n\n    return atoms_in_residues", "response": "r Returns a list of ndarrays containing the atom indices in each residue of residue_idxs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _first_and_last_element(arr):\n    if isinstance(arr, np.ndarray) or hasattr(arr, 'data'):\n        # numpy array or sparse matrix with .data attribute\n        data = arr.data if sparse.issparse(arr) else arr\n        return data.flat[0], data.flat[-1]\n    else:\n        # Sparse matrices without .data attribute. Only dok_matrix at\n        # the time of writing, in this case indexing is fast\n        return arr[0, 0], arr[-1, -1]", "response": "Returns first and last element of numpy array or sparse matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone(estimator, safe=True):\n    estimator_type = type(estimator)\n    # XXX: not handling dictionaries\n    if estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, 'get_params'):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            raise TypeError(\"Cannot clone object '%s' (type %s): \"\n                            \"it does not seem to be a scikit-learn estimator \"\n                            \"as it does not implement a 'get_params' methods.\"\n                            % (repr(estimator), type(estimator)))\n    # TODO: this is a brute force method to make things work for parameter studies. #1135\n    # But this can potentially use a lot of memory in case of large input data, which is also copied then.\n    # we need a way to distinguish input parameters from derived model parameters, which is currently only ensured for\n    # estimators in the coordinates package.\n    if hasattr(estimator, '_estimated') and estimator._estimated:\n        return copy.deepcopy(estimator)\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n    new_object = klass(**new_object_params)\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is param2:\n            # this should always happen\n            continue\n        if isinstance(param1, np.ndarray):\n            # For most ndarrays, we do not test for complete equality\n            if not isinstance(param2, type(param1)):\n                equality_test = False\n            elif (param1.ndim > 0\n                    and param1.shape[0] > 0\n                    and isinstance(param2, np.ndarray)\n                    and param2.ndim > 0\n                    and param2.shape[0] > 0):\n                equality_test = (\n                    param1.shape == param2.shape\n                    and param1.dtype == param2.dtype\n                    and (_first_and_last_element(param1) ==\n                         _first_and_last_element(param2))\n                )\n            else:\n                equality_test = np.all(param1 == param2)\n        elif sparse.issparse(param1):\n            # For sparse matrices equality doesn't work\n            if not sparse.issparse(param2):\n                equality_test = False\n            elif param1.size == 0 or param2.size == 0:\n                equality_test = (\n                    param1.__class__ == param2.__class__\n                    and param1.size == 0\n                    and param2.size == 0\n                )\n            else:\n                equality_test = (\n                    param1.__class__ == param2.__class__\n                    and (_first_and_last_element(param1) ==\n                         _first_and_last_element(param2))\n                    and param1.nnz == param2.nnz\n                    and param1.shape == param2.shape\n                )\n        else:\n            # fall back on standard equality\n            equality_test = param1 == param2\n        if equality_test:\n            warnings.warn(\"Estimator %s modifies parameters in __init__.\"\n                          \" This behavior is deprecated as of 0.18 and \"\n                          \"support for this behavior will be removed in 0.20.\"\n                          % type(estimator).__name__, DeprecationWarning)\n        else:\n            raise RuntimeError('Cannot clone object %s, as the constructor '\n                               'does not seem to set parameter %s' %\n                               (estimator, name))\n\n    return new_object", "response": "Returns a deep copy of the estimator with the same parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the parameter names for the estimator", "response": "def _get_param_names(cls):\n        \"\"\"Get parameter names for the estimator\"\"\"\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n        if init is object.__init__:\n            # No explicit constructor to introspect\n            return []\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        args, varargs, kw, default = getargspec_no_self(init)\n        if varargs is not None:\n            raise RuntimeError(\"scikit-learn estimators should always \"\n                               \"specify their parameters in the signature\"\n                               \" of their __init__ (no varargs).\"\n                               \" %s doesn't follow this convention.\"\n                               % (cls, ))\n        args.sort()\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef running_covar(xx=True, xy=False, yy=False, remove_mean=False, symmetrize=False, sparse_mode='auto',\n                  modify_data=False, column_selection=None, diag_only=False, nsave=5):\n    \"\"\" Returns a running covariance estimator\n\n    Returns an estimator object that can be fed chunks of X and Y data, and\n    that can generate on-the-fly estimates of mean, covariance, running sum\n    and second moment matrix.\n\n    Parameters\n    ----------\n    xx : bool\n        Estimate the covariance of X\n    xy : bool\n        Estimate the cross-covariance of X and Y\n    yy : bool\n        Estimate the covariance of Y\n    remove_mean : bool\n        Remove the data mean in the covariance estimation\n    symmetrize : bool\n        Use symmetric estimates with sum defined by sum_t x_t + y_t and\n        second moment matrices defined by X'X + Y'Y and Y'X + X'Y.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the input data,\n        without creating an independent copy. This option is faster but should\n        only be selected if the input data is not used elsewhere.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    column_selection: ndarray(k, dtype=int) or None\n        Indices of those columns that are to be computed. If None, all columns are computed.\n    diag_only: bool\n        If True, the computation is restricted to the diagonal entries (autocorrelations) only.\n    nsave : int\n        Depth of Moment storage. Moments computed from each chunk will be\n        combined with Moments of similar statistical weight using the pairwise\n        combination algorithm described in [1]_.\n\n    References\n    ----------\n    .. [1] http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\n    \"\"\"\n    return RunningCovar(compute_XX=xx, compute_XY=xy, compute_YY=yy, sparse_mode=sparse_mode, modify_data=modify_data,\n                        remove_mean=remove_mean, symmetrize=symmetrize, column_selection=column_selection,\n                        diag_only=diag_only, nsave=nsave)", "response": "Returns a running covariance estimator for a single entry in the set of possible entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncombines two sets of species entries.", "response": "def combine(self, other, mean_free=False):\n        \"\"\"\n        References\n        ----------\n        [1] http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n        \"\"\"\n        w1 = self.w\n        w2 = other.w\n        w = w1 + w2\n        # TODO: fix this div by zero error\n        q = w2 / w1\n        dsx = q * self.sx - other.sx\n        dsy = q * self.sy - other.sy\n        # update\n        self.w = w1 + w2\n        self.sx = self.sx + other.sx\n        self.sy = self.sy + other.sy\n        #\n        if mean_free:\n            if len(self.Mxy.shape) == 1:  # diagonal only\n                d = dsx*dsy\n            else:\n                d = np.outer(dsx, dsy)\n            self.Mxy += other.Mxy + (w1 / (w2 * w)) * d\n        else:\n            self.Mxy += other.Mxy\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef covar(self, bessel=True):\n        if bessel:\n            return self.Mxy/ (self.w-1)\n        else:\n            return self.Mxy / self.w", "response": "Return the covariance matrix of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the two last list elements can be merged into one.", "response": "def _can_merge_tail(self):\n        \"\"\" Checks if the two last list elements can be merged\n        \"\"\"\n        if len(self.storage) < 2:\n            return False\n        return self.storage[-2].w <= self.storage[-1].w * self.rtol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring object X with weight w .", "response": "def store(self, moments):\n        \"\"\" Store object X with weight w\n        \"\"\"\n        if len(self.storage) == self.nsave:  # merge if we must\n            # print 'must merge'\n            self.storage[-1].combine(moments, mean_free=self.remove_mean)\n        else:  # append otherwise\n            # print 'append'\n            self.storage.append(moments)\n        # merge if possible\n        while self._can_merge_tail():\n            # print 'merge: ',self.storage\n            M = self.storage.pop()\n            # print 'pop last: ',self.storage\n            self.storage[-1].combine(M, mean_free=self.remove_mean)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding trajectory to estimate.", "response": "def add(self, X, Y=None, weights=None):\n        \"\"\"\n        Add trajectory to estimate.\n\n        Parameters\n        ----------\n        X : ndarray(T, N)\n            array of N time series.\n        Y : ndarray(T, N)\n            array of N time series, usually time shifted version of X.\n        weights : None or float or ndarray(T, ):\n            weights assigned to each trajectory point. If None, all data points have weight one. If float,\n            the same weight will be given to all data points. If ndarray, each data point is assigned a separate\n            weight.\n\n        \"\"\"\n\n        # check input\n        T = X.shape[0]\n        if Y is not None:\n            assert Y.shape[0] == T, 'X and Y must have equal length'\n        # Weights cannot be used for compute_YY:\n        if weights is not None and self.compute_YY:\n            raise ValueError('Use of weights is not implemented for compute_YY==True')\n        if weights is not None:\n            # Convert to array of length T if weights is a single number:\n            if isinstance(weights, numbers.Real):\n                weights = weights * np.ones(T, dtype=float)\n            # Check appropriate length if weights is an array:\n            elif isinstance(weights, np.ndarray):\n                if len(weights) != T:\n                    raise ValueError('weights and X must have equal length. Was {} and {} respectively.'.format(len(weights), len(X)))\n            else:\n                raise TypeError('weights is of type %s, must be a number or ndarray' % (type(weights)))\n        # estimate and add to storage\n        if self.compute_XX and not self.compute_XY and not self.compute_YY:\n            w, s_X, C_XX = moments_XX(X, remove_mean=self.remove_mean, weights=weights, sparse_mode=self.sparse_mode,\n                                      modify_data=self.modify_data, column_selection=self.column_selection,\n                                      diag_only=self.diag_only)\n            if self.column_selection is not None:\n                s_Xk = s_X[self.column_selection]\n            else:\n                s_Xk = s_X\n            self.storage_XX.store(Moments(w, s_X, s_Xk, C_XX))\n        elif self.compute_XX and self.compute_XY and not self.compute_YY:\n            assert Y is not None\n            w, s_X, s_Y, C_XX, C_XY = moments_XXXY(X, Y, remove_mean=self.remove_mean, symmetrize=self.symmetrize,\n                                                   weights=weights, sparse_mode=self.sparse_mode, modify_data=self.modify_data,\n                                                   column_selection=self.column_selection, diag_only=self.diag_only)\n            # make copy in order to get independently mergeable moments\n            if self.column_selection is not None:\n                s_Xk = s_X[self.column_selection]\n                s_Yk = s_Y[self.column_selection]\n            else:\n                s_Xk = s_X\n                s_Yk = s_Y\n            self.storage_XX.store(Moments(w, s_X, s_Xk, C_XX))\n            self.storage_XY.store(Moments(w, s_X, s_Yk, C_XY))\n        else:  # compute block\n            assert Y is not None\n            assert not self.symmetrize\n            w, s, C = moments_block(X, Y, remove_mean=self.remove_mean,\n                                    sparse_mode=self.sparse_mode, modify_data=self.modify_data,\n                                    column_selection=self.column_selection, diag_only=self.diag_only)\n            # make copy in order to get independently mergeable moments\n            if self.column_selection is not None:\n                s0k = s[0][self.column_selection]\n                s1k = s[1][self.column_selection]\n            else:\n                s0k = s[0]\n                s1k = s[1]\n            if self.compute_XX:\n                self.storage_XX.store(Moments(w, s[0], s0k, C[0][0]))\n            if self.compute_XY:\n                self.storage_XY.store(Moments(w, s[0], s1k, C[0][1]))\n            self.storage_YY.store(Moments(w, s[1], s1k, C[1][1]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submodel(self, states=None, obs=None):\n        # get the reference HMM submodel\n        ref = super(SampledHMSM, self).submodel(states=states, obs=obs)\n        # get the sample submodels\n        samples_sub = [sample.submodel(states=states, obs=obs) for sample in self.samples]\n        # new model\n        return SampledHMSM(samples_sub, ref=ref, conf=self.conf)", "response": "Returns a new HMM with restricted state space and restricted state space."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns j length subsequences of elements from the input iterable.", "response": "def combinations(seq, k):\n    \"\"\" Return j length subsequences of elements from the input iterable.\n\n    This version uses Numpy/Scipy and should be preferred over itertools. It avoids\n    the creation of all intermediate Python objects.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from itertools import combinations as iter_comb\n    >>> x = np.arange(3)\n    >>> c1 = combinations(x, 2)\n    >>> print(c1)\n    [[0 1]\n     [0 2]\n     [1 2]]\n    >>> c2 = np.array(tuple(iter_comb(x, 2)))\n    >>> print(c2)\n    [[0 1]\n     [0 2]\n     [1 2]]\n    \"\"\"\n    from itertools import combinations as _combinations, chain\n    from scipy.special import comb\n\n    count = comb(len(seq), k, exact=True)\n    res = np.fromiter(chain.from_iterable(_combinations(seq, k)),\n                      int, count=count*k)\n    return res.reshape(-1, k)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a cartesian product of the input arrays.", "response": "def product(*arrays):\n    \"\"\" Generate a cartesian product of input arrays.\n\n    Parameters\n    ----------\n    arrays : list of array-like\n        1-D arrays to form the cartesian product of.\n\n    Returns\n    -------\n    out : ndarray\n        2-D array of shape (M, len(arrays)) containing cartesian products\n        formed of input arrays.\n\n    \"\"\"\n    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    dtype = arrays[0].dtype\n\n    ix = np.indices(shape)\n    ix = ix.reshape(len(arrays), -1).T\n\n    out = np.empty_like(ix, dtype=dtype)\n\n    for n, _ in enumerate(arrays):\n        out[:, n] = arrays[n][ix[:, n]]\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample(self, x0, nsteps, nskip=1):\n        x = np.zeros(shape=(nsteps + 1,))\n        x[0] = x0\n        for t in range(nsteps):\n            q = x[t]\n            for s in range(nskip):\n                q = self.step(q)\n            x[t + 1] = q\n        return x", "response": "r generate nsteps sample points"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_markov_model(\n    P, pos=None, state_sizes=None, state_scale=1.0, state_colors='#ff5500', state_labels='auto',\n    minflux=1e-6, arrow_scale=1.0, arrow_curvature=1.0, arrow_labels='weights',\n    arrow_label_format='%2.e', max_width=12, max_height=12, figpadding=0.2, show_frame=False,\n    ax=None, **textkwargs):\n    r\"\"\"Network representation of MSM transition matrix\n\n    This visualization is not optimized for large matrices. It is meant to be\n    used for the visualization of small models with up to 10-20 states, e.g.\n    obtained by a HMM coarse-graining. If used with large network, the automatic\n    node positioning will be very slow and may still look ugly.\n\n    Parameters\n    ----------\n    P : ndarray(n,n) or MSM object with attribute 'transition matrix'\n        Transition matrix or MSM object\n    pos : ndarray(n,2), optional, default=None\n        User-defined positions to draw the states on. If not given, will try\n        to place them automatically.\n    state_sizes : ndarray(n), optional, default=None\n        User-defined areas of the discs drawn for each state. If not given,\n        the stationary probability of P will be used.\n    state_colors : string, ndarray(n), or list, optional, default='#ff5500' (orange)\n        string :\n            a Hex code for a single color used for all states\n        array :\n            n values in [0,1] which will result in a grayscale plot\n        list :\n            of len = nstates, with a color for each state. The list can mix strings, RGB values and\n            hex codes, e.g. :py:obj:`state_colors` = ['g', 'red', [.23, .34, .35], '#ff5500'] is\n            possible.\n    state_labels : list of strings, optional, default is 'auto'\n        A list with a label for each state, to be displayed at the center\n        of each node/state. If left to 'auto', the labels are automatically set to the state\n        indices.\n    minflux : float, optional, default=1e-6\n        The minimal flux (p_i * p_ij) for a transition to be drawn\n    arrow_scale : float, optional, default=1.0\n        Relative arrow scale. Set to a value different from 1 to increase\n        or decrease the arrow width.\n    arrow_curvature : float, optional, default=1.0\n        Relative arrow curvature. Set to a value different from 1 to make\n        arrows more or less curved.\n    arrow_labels : 'weights', None or a ndarray(n,n) with label strings. Optional, default='weights'\n        Strings to be placed upon arrows. If None, no labels will be used.\n        If 'weights', the elements of P will be used. If a matrix of strings is\n        given by the user these will be used.\n    arrow_label_format : str, optional, default='%10.2f'\n        The numeric format to print the arrow labels\n    max_width = 12\n        The maximum figure width\n    max_height = 12\n        The maximum figure height\n    figpadding = 0.2\n        The relative figure size used for the padding\n    show_frame: boolean (default=False)\n        Draw a frame around the network.\n    ax : matplotlib Axes object, optional, default=None\n        The axes to plot to. When set to None a new Axes (and Figure) object will be used.\n    textkwargs : optional argument for the text of the state and arrow labels.\n        See http://matplotlib.org/api/text_api.html#matplotlib.text.Text for more info. The\n        parameter 'size' refers to the size of the state and arrow labels and overwrites the\n        matplotlib default. The parameter 'arrow_label_size' is only used for the arrow labels;\n        please note that 'arrow_label_size' is not part of matplotlib.text.Text's set of parameters\n        and will raise an exception when passed to matplotlib.text.Text directly.\n\n    Returns\n    -------\n    fig, pos : matplotlib.Figure, ndarray(n,2)\n    a Figure object containing the plot and the positions of states.\n    Can be used later to plot a different network representation (e.g. the flux)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> P = np.array([[0.8,  0.15, 0.05,  0.0,  0.0],\n    ...              [0.1,  0.75, 0.05, 0.05, 0.05],\n    ...              [0.05,  0.1,  0.8,  0.0,  0.05],\n    ...              [0.0,  0.2, 0.0,  0.8,  0.0],\n    ...              [0.0,  0.02, 0.02, 0.0,  0.96]])\n    >>> plot_markov_model(P) # doctest:+ELLIPSIS\n    (<...Figure..., array...)\n\n    \"\"\"\n    from msmtools import analysis as msmana\n    if isinstance(P, _np.ndarray):\n        P = P.copy()\n    else:\n        # MSM object? then get transition matrix first\n        P = P.transition_matrix.copy()\n    if state_sizes is None:\n        state_sizes = msmana.stationary_distribution(P)\n    if minflux > 0:\n        F = _np.dot(_np.diag(msmana.stationary_distribution(P)), P)\n        I, J = _np.where(F < minflux)\n        P[I, J] = 0.0\n    plot = NetworkPlot(P, pos=pos, ax=ax)\n    fig = plot.plot_network(\n        state_sizes=state_sizes, state_scale=state_scale, state_colors=state_colors,\n        state_labels=state_labels, arrow_scale=arrow_scale, arrow_curvature=arrow_curvature,\n        arrow_labels=arrow_labels, arrow_label_format=arrow_label_format, max_width=max_width,\n        max_height=max_height, figpadding=figpadding, xticks=False, yticks=False,\n        show_frame=show_frame, **textkwargs)\n    return fig, plot.pos", "response": "r This function returns a network representation of the MSM transition matrix P."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_network(\n    weights, pos=None, xpos=None, ypos=None, state_sizes=None, state_scale=1.0,\n    state_colors='#ff5500', state_labels='auto', arrow_scale=1.0, arrow_curvature=1.0,\n    arrow_labels='weights', arrow_label_format='%2.e', max_width=12, max_height=12, figpadding=0.2,\n    attribute_to_plot='net_flux', show_frame=False, xticks=False, yticks=False, ax=None,\n    **textkwargs):\n    r\"\"\"Network representation of given matrix\n\n    This visualization is not optimized for large networks. It is meant to be\n    used for the visualization of small models with up to 10-20 states. If used\n    with large network, the automatic node positioning will be very slow and\n    may still look ugly.\n\n    Parameters\n    ----------\n    weights : ndarray(n, n)\n        weight matrix\n    pos : ndarray(n,2), optional, default=None\n        User-defined positions to draw the states on.\n    xpos : ndarray(n,), optional, default=None\n        Fixes the x positions while the y positions are optimized\n    ypos : ndarray(n,), optional, default=None\n        Fixes the y positions while the x positions are optimized\n    state_sizes : ndarray(n), optional, default=None\n        User-defined areas of the discs drawn for each state. If not given, the\n        stationary probability of P will be used\n    state_colors : string, ndarray(n), or list, optional, default='#ff5500' (orange)\n        string :\n            a Hex code for a single color used for all states\n        array :\n            n values in [0,1] which will result in a grayscale plot\n        list :\n            of len = nstates, with a color for each state. The list can mix strings, RGB values and\n            hex codes, e.g. :py:obj:`state_colors` = ['g', 'red', [.23, .34, .35], '#ff5500'] is\n            possible.\n    state_labels : list of strings, optional, default is 'auto'\n        A list with a label for each state, to be displayed at the center\n        of each node/state. If left to 'auto', the labels are automatically set to the state\n        indices.\n    arrow_scale : float, optional, default=1.0\n        Relative arrow scale. Set to a value different from 1 to increase or\n        decrease the arrow width.\n    arrow_curvature : float, optional, default=1.0\n        Relative arrow curvature. Set to a value different from 1 to make arrows\n        more or less curved.\n    arrow_labels : 'weights', None or a ndarray(n,n) with label strings. Optional, default='weights'\n        Strings to be placed upon arrows. If None, no labels will be used. If\n        'weights', the elements of P will be used. If a matrix of strings is\n        given by the user these will be used.\n    arrow_label_format : str, optional, default='%10.2f'\n        The numeric format to print the arrow labels\n    max_width : int (default = 12)\n        The maximum figure width\n    max_height: int (default = 12)\n        The maximum figure height\n    figpadding: float (default = 0.2)\n        The relative figure size used for the padding\n    show_frame: boolean (default=False)\n        Draw a frame around the network.\n    xticks: boolean (default=False)\n        Show x ticks\n    yticks: boolean (default=False)\n        Show y ticks\n    ax : matplotlib Axes object, optional, default=None\n        The axes to plot to. When set to None a new Axes (and Figure) object will be used.\n    textkwargs : optional argument for the text of the state and arrow labels.\n        See http://matplotlib.org/api/text_api.html#matplotlib.text.Text for more info. The\n        parameter 'size' refers to the size of the state and arrow labels and overwrites the\n        matplotlib default. The parameter 'arrow_label_size' is only used for the arrow labels;\n        please note that 'arrow_label_size' is not part of matplotlib.text.Text's set of parameters\n        and will raise an exception when passed to matplotlib.text.Text directly.\n\n    Returns\n    -------\n    (fig, pos) : matpotlib.Figure instance, ndarray\n        Axes instances containing the plot. Use pyplot.show() to display it.\n        The positions of states. Can be used later to plot a different network\n        representation (e.g. the flux).\n\n    Examples\n    --------\n    We define first define a reactive flux by taking the following transition\n    matrix and computing TPT from state 2 to 3\n\n    >>> import numpy as np\n    >>> P = np.array([[0.8,  0.15, 0.05,  0.0,  0.0],\n    ...               [0.1,  0.75, 0.05, 0.05, 0.05],\n    ...               [0.05,  0.1,  0.8,  0.0,  0.05],\n    ...               [0.0,  0.2, 0.0,  0.8,  0.0],\n    ...               [0.0,  0.02, 0.02, 0.0,  0.96]])\n\n    Scale the flux by 100 is basically a change of units to get numbers close\n    to 1 (avoid printing many zeros). Now we visualize the flux:\n\n    >>> plot_network(P) # doctest:+ELLIPSIS\n    (<...Figure..., array...)\n\n    \"\"\"\n    plot = NetworkPlot(weights, pos=pos, xpos=xpos, ypos=ypos, ax=ax)\n    fig = plot.plot_network(\n        state_sizes=state_sizes, state_scale=state_scale, state_colors=state_colors,\n        state_labels=state_labels, arrow_scale=arrow_scale, arrow_curvature=arrow_curvature,\n        arrow_labels=arrow_labels, arrow_label_format=arrow_label_format, max_width=max_width,\n        max_height=max_height, figpadding=figpadding, xticks=xticks, yticks=yticks,\n        show_frame=show_frame, **textkwargs)\n    return fig, plot.pos", "response": "r Plot the network representation of the given matrix\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing a slightly curved arrow from x1 y1 to x2 y2.", "response": "def _draw_arrow(\n        self, x1, y1, x2, y2, Dx, Dy, label=\"\", width=1.0, arrow_curvature=1.0, color=\"grey\",\n        patchA=None, patchB=None, shrinkA=0, shrinkB=0, arrow_label_size=None):\n        \"\"\"\n        Draws a slightly curved arrow from (x1,y1) to (x2,y2).\n        Will allow the given patches at start end end.\n\n        \"\"\"\n        # set arrow properties\n        dist = _sqrt(\n            ((x2 - x1) / float(Dx))**2 + ((y2 - y1) / float(Dy))**2)\n        arrow_curvature *= 0.075  # standard scale\n        rad = arrow_curvature / (dist)\n        tail_width = width\n        head_width = max(0.5, 2 * width)\n        head_length = head_width\n        self.ax.annotate(\n            \"\", xy=(x2, y2), xycoords='data', xytext=(x1, y1), textcoords='data',\n            arrowprops=dict(\n                arrowstyle='simple,head_length=%f,head_width=%f,tail_width=%f' % (\n                    head_length, head_width, tail_width),\n                color=color, shrinkA=shrinkA, shrinkB=shrinkB, patchA=patchA, patchB=patchB,\n                connectionstyle=\"arc3,rad=%f\" % -rad),\n            zorder=0)\n        # weighted center position\n        center = _np.array([0.55 * x1 + 0.45 * x2, 0.55 * y1 + 0.45 * y2])\n        v = _np.array([x2 - x1, y2 - y1])  # 1->2 vector\n        vabs = _np.abs(v)\n        vnorm = _np.array([v[1], -v[0]])  # orthogonal vector\n        vnorm = _np.divide(vnorm, _np.linalg.norm(vnorm))  # normalize\n        # cross product to determine the direction into which vnorm points\n        z = _np.cross(v, vnorm)\n        if z < 0:\n            vnorm *= -1\n        offset = 0.5 * arrow_curvature * \\\n            ((vabs[0] / (vabs[0] + vabs[1]))\n             * Dx + (vabs[1] / (vabs[0] + vabs[1])) * Dy)\n        ptext = center + offset * vnorm\n        self.ax.text(\n            ptext[0], ptext[1], label, size=arrow_label_size,\n            horizontalalignment='center', verticalalignment='center', zorder=1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting a network of the state of the temporary key in the dictionary.", "response": "def plot_network(\n        self, state_sizes=None, state_scale=1.0, state_colors='#ff5500', state_labels='auto',\n        arrow_scale=1.0, arrow_curvature=1.0, arrow_labels='weights', arrow_label_format='%10.2f',\n        max_width=12, max_height=12, figpadding=0.2, xticks=False, yticks=False, show_frame=False,\n        **textkwargs):\n        \"\"\"\n        Draws a network using discs and curved arrows.\n\n        The thicknesses and labels of the arrows are taken from the off-diagonal matrix elements\n        in A.\n\n        \"\"\"\n\n        # Set the default values for the text dictionary\n        from matplotlib import pyplot as _plt\n        textkwargs.setdefault('size', None)\n        textkwargs.setdefault('horizontalalignment', 'center')\n        textkwargs.setdefault('verticalalignment', 'center')\n        textkwargs.setdefault('color', 'black')\n        # remove the temporary key 'arrow_label_size' as it cannot be parsed by plt.text!\n        arrow_label_size = textkwargs.pop('arrow_label_size', textkwargs['size'])\n        if self.pos is None:\n            self.layout_automatic()\n        # number of nodes\n        n = len(self.pos)\n        # get bounds and pad figure\n        xmin = _np.min(self.pos[:, 0])\n        xmax = _np.max(self.pos[:, 0])\n        Dx = xmax - xmin\n        xmin -= Dx * figpadding\n        xmax += Dx * figpadding\n        Dx *= 1 + figpadding\n        ymin = _np.min(self.pos[:, 1])\n        ymax = _np.max(self.pos[:, 1])\n        Dy = ymax - ymin\n        ymin -= Dy * figpadding\n        ymax += Dy * figpadding\n        Dy *= 1 + figpadding\n        # sizes of nodes\n        if state_sizes is None:\n            state_sizes = 0.5 * state_scale * \\\n                min(Dx, Dy)**2 * _np.ones(n) / float(n)\n        else:\n            state_sizes = 0.5 * state_scale * \\\n                min(Dx, Dy)**2 * state_sizes / (_np.max(state_sizes) * float(n))\n        # automatic arrow rescaling\n        arrow_scale *= 1.0 / \\\n            (_np.max(self.A - _np.diag(_np.diag(self.A))) * _sqrt(n))\n        # size figure\n        if (Dx / max_width > Dy / max_height):\n            figsize = (max_width, Dy * (max_width / Dx))\n        else:\n            figsize = (Dx / Dy * max_height, max_height)\n        if self.ax is None:\n            logger.debug(\"creating new figure\")\n            fig = _plt.figure(None, figsize=figsize)\n            self.ax = fig.add_subplot(111)\n        else:\n            fig = self.ax.figure\n            window_extend = self.ax.get_window_extent()\n            axes_ratio = window_extend.height / window_extend.width\n            data_ratio = (ymax - ymin) / (xmax - xmin)\n            q = axes_ratio / data_ratio\n            if q > 1.0:\n                ymin *= q\n                ymax *= q\n            else:\n                xmin /= q\n                xmax /= q\n        if not xticks:\n            self.ax.get_xaxis().set_ticks([])\n        if not yticks:\n            self.ax.get_yaxis().set_ticks([])\n        # show or suppress frame\n        self.ax.set_frame_on(show_frame)\n        # set node labels\n        if state_labels is None:\n            pass\n        elif isinstance(state_labels, str) and state_labels == 'auto':\n            state_labels = [str(i) for i in _np.arange(n)]\n        else:\n            if len(state_labels) != n:\n                raise ValueError(\"length of state_labels({}) has to match length of states({}).\"\n                                 .format(len(state_labels), n))\n        # set node colors\n        if state_colors is None:\n            state_colors = '#ff5500'  # None is not acceptable\n        if isinstance(state_colors, str):\n            state_colors = [state_colors] * n\n        if isinstance(state_colors, list) and not len(state_colors) == n:\n            raise ValueError(\"Mistmatch between nstates and nr. state_colors (%u vs %u)\" % (n, len(state_colors)))\n        try:\n            colorscales = _types.ensure_ndarray(state_colors, ndim=1, kind='numeric')\n            colorscales /= colorscales.max()\n            state_colors = [_plt.cm.binary(int(256.0 * colorscales[i])) for i in range(n)]\n        except AssertionError:\n            # assume we have a list of strings now.\n            logger.debug(\"could not cast 'state_colors' to numeric values.\")\n\n        # set arrow labels\n        if isinstance(arrow_labels, _np.ndarray):\n            L = arrow_labels\n            if isinstance(arrow_labels[0,0], str):\n                arrow_label_format = '%s'\n        elif isinstance(arrow_labels, str) and arrow_labels.lower() == 'weights':\n            L = self.A[:, :]\n        elif arrow_labels is None:\n            L = _np.empty(_np.shape(self.A), dtype=object)\n            L[:, :] = ''\n            arrow_label_format = '%s'\n        else:\n            raise ValueError('invalid arrow labels')\n\n        # draw circles\n        circles = []\n        for i in range(n):\n            # choose color\n            c = _plt.Circle(\n                self.pos[i], radius=_sqrt(\n                    0.5 * state_sizes[i]) / 2.0,\n                color=state_colors[i], zorder=2)\n            circles.append(c)\n            self.ax.add_artist(c)\n            # add annotation\n            if state_labels is not None:\n                self.ax.text(self.pos[i][0], self.pos[i][1], state_labels[i], zorder=3, **textkwargs)\n\n        assert len(circles) == n, \"%i != %i\" % (len(circles), n)\n\n        # draw arrows\n        for i in range(n):\n            for j in range(i + 1, n):\n                if (abs(self.A[i, j]) > 0):\n                    self._draw_arrow(\n                        self.pos[i, 0], self.pos[i, 1], self.pos[j, 0], self.pos[j, 1], Dx, Dy,\n                        label=arrow_label_format%L[i, j], width=arrow_scale * self.A[i, j],\n                        arrow_curvature=arrow_curvature, patchA=circles[i], patchB=circles[j],\n                        shrinkA=3, shrinkB=0, arrow_label_size=arrow_label_size)\n                if (abs(self.A[j, i]) > 0):\n                    self._draw_arrow(\n                        self.pos[j, 0], self.pos[j, 1], self.pos[i, 0], self.pos[i, 1], Dx, Dy,\n                        label=arrow_label_format%L[j, i], width=arrow_scale * self.A[j, i],\n                        arrow_curvature=arrow_curvature, patchA=circles[j], patchB=circles[i],\n                        shrinkA=3, shrinkB=0, arrow_label_size=arrow_label_size)\n\n        # plot\n        self.ax.set_xlim(xmin, xmax)\n        self.ax.set_ylim(ymin, ymax)\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_best_positions(self, G):\n        initpos = None\n        holddim = None\n        if self.xpos is not None:\n            y = _np.random.random(len(self.xpos))\n            initpos = _np.vstack((self.xpos, y)).T\n            holddim = 0\n        elif self.ypos is not None:\n            x = _np.zeros_like(self.xpos)\n            initpos = _np.vstack((x, self.ypos)).T\n            holddim = 1\n        # nothing to do\n        elif self.xpos is not None and self.ypos is not None:\n            return _np.array([self.xpos, self.ypos]), 0\n        from pyemma.plots._ext.fruchterman_reingold import _fruchterman_reingold\n        best_pos = _fruchterman_reingold(G, pos=initpos, dim=2, hold_dim=holddim)\n\n        # rescale fixed to user settings and balance the other coordinate\n        if self.xpos is not None:\n            # rescale x to fixed value\n            best_pos[:, 0] *= (_np.max(self.xpos) - _np.min(self.xpos)\n                               ) / (_np.max(best_pos[:, 0]) - _np.min(best_pos[:, 0]))\n            best_pos[:, 0] += _np.min(self.xpos) - _np.min(best_pos[:, 0])\n            # rescale y to balance\n            if _np.max(best_pos[:, 1]) - _np.min(best_pos[:, 1]) > 0.01:\n                best_pos[:, 1] *= (_np.max(self.xpos) - _np.min(self.xpos)\n                                   ) / (_np.max(best_pos[:, 1]) - _np.min(best_pos[:, 1]))\n        if self.ypos is not None:\n            best_pos[:, 1] *= (_np.max(self.ypos) - _np.min(self.ypos)\n                               ) / (_np.max(best_pos[:, 1]) - _np.min(best_pos[:, 1]))\n            best_pos[:, 1] += _np.min(self.ypos) - _np.min(best_pos[:, 1])\n            # rescale x to balance\n            if _np.max(best_pos[:, 0]) - _np.min(best_pos[:, 0]) > 0.01:\n                best_pos[:, 0] *= (_np.max(self.ypos) - _np.min(self.ypos)\n                                   ) / (_np.max(best_pos[:, 0]) - _np.min(best_pos[:, 0]))\n\n        return best_pos", "response": "Find the best positions for the given graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclass decorator to create \"_full_state\" methods/properties on the class (so they are valid for all instances created from this class). Parameters ---------- class_with_globalize_methods", "response": "def add_full_state_methods(class_with_globalize_methods):\n    \"\"\"\n    class decorator to create \"_full_state\" methods/properties on the class (so they\n    are valid for all instances created from this class).\n\n    Parameters\n    ----------\n    class_with_globalize_methods\n\n    \"\"\"\n    assert hasattr(class_with_globalize_methods, 'active_set')\n    assert hasattr(class_with_globalize_methods, 'nstates_full')\n\n    for name, method in class_with_globalize_methods.__dict__.copy().items():\n        if isinstance(method, property) and hasattr(method.fget, '_map_to_full_state_def_arg'):\n            default_value = method.fget._map_to_full_state_def_arg\n            axis = method.fget._map_to_full_state_along_axis\n            new_getter = _wrap_to_full_state(name, default_value, axis)\n            alias_to_full_state_inst = property(new_getter)\n        elif hasattr(method, '_map_to_full_state_def_arg'):\n            default_value = method._map_to_full_state_def_arg\n            axis = method._map_to_full_state_along_axis\n            alias_to_full_state_inst = _wrap_to_full_state(name, default_value, axis)\n        else:\n            continue\n\n        name += \"_full_state\"\n        setattr(class_with_globalize_methods, name, alias_to_full_state_inst)\n\n    return class_with_globalize_methods"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating which columns are constant or variable.", "response": "def variable_cols(X, tol=0.0, min_constant=0):\n    \"\"\" Evaluates which columns are constant (0) or variable (1)\n\n    Parameters\n    ----------\n    X : ndarray\n        Matrix whose columns will be checked for constant or variable.\n    tol : float\n        Tolerance for float-matrices. When set to 0 only equal columns with\n        values will be considered constant. When set to a positive value,\n        columns where all elements have absolute differences to the first\n        element of that column are considered constant.\n    min_constant : int\n        Minimal number of constant columns to resume operation. If at one\n        point the number of constant columns drops below min_constant, the\n        computation will stop and all columns will be assumed to be variable.\n        In this case, an all-True array will be returned.\n\n    Returns\n    -------\n    variable : bool-array\n        Array with number of elements equal to the columns. True: column is\n        variable / non-constant. False: column is constant.\n\n    \"\"\"\n    if X is None:\n        return None\n    from pyemma._ext.variational.estimators.covar_c._covartools import (variable_cols_double,\n                                                                        variable_cols_float,\n                                                                        variable_cols_int,\n                                                                        variable_cols_long,\n                                                                        variable_cols_char)\n    # prepare column array\n    cols = numpy.zeros(X.shape[1], dtype=numpy.bool, order='C')\n\n    if X.dtype == numpy.float64:\n        completed = variable_cols_double(cols, X, tol, min_constant)\n    elif X.dtype == numpy.float32:\n        completed = variable_cols_float(cols, X, tol, min_constant)\n    elif X.dtype == numpy.int32:\n        completed = variable_cols_int(cols, X, 0, min_constant)\n    elif X.dtype == numpy.int64:\n        completed = variable_cols_long(cols, X, 0, min_constant)\n    elif X.dtype == numpy.bool:\n        completed = variable_cols_char(cols, X, 0, min_constant)\n    else:\n        raise TypeError('unsupported type of X: %s' % X.dtype)\n\n    # if interrupted, return all ones. Otherwise return the variable columns as bool array\n    if completed == 0:\n        return numpy.ones_like(cols, dtype=numpy.bool)\n\n    return cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_csets_TRAM(\n    connectivity, state_counts, count_matrices, equilibrium_state_counts=None,\n    ttrajs=None, dtrajs=None, bias_trajs=None, nn=None, factor=1.0, callback=None):\n    r\"\"\"\n    Computes the largest connected sets in the produce space of Markov state and\n    thermodynamic states for TRAM data.\n\n    Parameters\n    ----------\n    connectivity : string\n        one of None, 'reversible_pathways', 'post_hoc_RE' or 'BAR_variance',\n        'neighbors', 'summed_count_matrix' or None.\n        Selects the algorithm for measuring overlap between thermodynamic\n        and Markov states.\n\n        * 'reversible_pathways' : requires that every state in the connected set\n          can be reached by following a pathway of reversible transitions. A\n          reversible transition between two Markov states (within the same\n          thermodynamic state k) is a pair of Markov states that belong to the\n          same strongly connected component of the count matrix (from\n          thermodynamic state k). A pathway of reversible transitions is a list of\n          reversible transitions [(i_1, i_2), (i_2, i_3),..., (i_(N-2), i_(N-1)),\n          (i_(N-1), i_N)]. The thermodynamic state where the reversible\n          transitions happen, is ignored in constructing the reversible pathways.\n          This is equivalent to assuming that two ensembles overlap at some Markov\n          state whenever there exist frames from both ensembles in that Markov\n          state.\n\n        * 'largest' : alias for reversible_pathways\n\n        * 'post_hoc_RE' : similar to 'reversible_pathways' but with a more strict\n          requirement for the overlap between thermodynamic states. It is required\n          that every state in the connected set can be reached by following a\n          pathway of reversible transitions or jumping between overlapping\n          thermodynamic states while staying in the same Markov state. A reversible\n          transition between two Markov states (within the same thermodynamic\n          state k) is a pair of Markov states that belong to the same strongly\n          connected component of the count matrix (from thermodynamic state k).\n          Two thermodynamic states k and l are defined to overlap at Markov state\n          n if a replica exchange simulation [2]_ restricted to state n would show\n          at least one transition from k to l or one transition from from l to k.\n          The expected number of replica exchanges is estimated from the\n          simulation data. The minimal number required of replica exchanges\n          per Markov state can be increased by decreasing `connectivity_factor`.\n\n        * 'BAR_variance' : like 'post_hoc_RE' but with a different condition to\n          define the thermodynamic overlap based on the variance of the BAR\n          estimator [3]_. Two thermodynamic states k and l are defined to overlap\n          at Markov state n if the variance of the free energy difference Delta\n          f_{kl} computed with BAR (and restricted to conformations form Markov\n          state n) is less or equal than one. The minimally required variance\n          can be controlled with `connectivity_factor`.\n\n        * 'neighbors' : like 'post_hoc_RE' or 'BAR_variance' but assume a\n          overlap between \"neighboring\" thermodynamic states. It is assumed that\n          the data comes from an Umbrella sampling simulation and the number of\n          the thermodynamic state matches the position of the Umbrella along the\n          order parameter. The overlap of thermodynamic states k and l within\n          Markov state n is set according to the value of nn; if there are\n          samples in both product-space states (k,n) and (l,n) and |l-n|<=nn,\n          the states are overlapping.\n\n        * 'summed_count_matrix' : all thermodynamic states are assumed to overlap.\n          The connected set is then computed by summing the count matrices over\n          all thermodynamic states and taking it's largest strongly connected set.\n          Not recommended!\n\n        * None : assume that everything is connected. For debugging.\n\n    state_counts : numpy.ndarray((T, M), dtype=numpy.intc)\n        Number of visits to the combinations of thermodynamic state t\n        and Markov state m\n    count_matrices : numpy.ndarray((T, M, M), dtype=numpy.intc)\n        Count matrices for all T thermodynamic states.\n    equilibrium_state_counts : numpy.dnarray((T, M)), optional\n        Number of visits to the combinations of thermodynamic state t\n        and Markov state m in the equilibrium data (for use with TRAMMBAR).\n    ttrajs : list of numpy.ndarray(X_i, dtype=numpy.intc), optional\n        List of generating thermodynamic state trajectories.\n    dtrajs : list of numpy.ndarray(X_i, dtype=numpy.intc), optional\n        List of configurational state trajectories (disctrajs).\n    bias_trajs : list of numpy.ndarray((X_i, T), dtype=numpy.float64), optional\n        List of bias energy trajectories.\n        The last three parameters are only required for\n        connectivity = 'post_hoc_RE' or connectivity = 'BAR_variance'.\n    nn : int, optional\n        Number of neighbors that are assumed to overlap when\n        connectivity='neighbors'\n    factor : int, default=1.0\n        scaling factor used for connectivity = 'post_hoc_RE' or\n        'BAR_variance'. Values greater than 1.0 weaken the connectivity\n        conditions. For 'post_hoc_RE' this multiplies the number of\n        hypothetically observed transitions. For 'BAR_variance' this\n        scales the threshold for the minimal allowed variance of free\n        energy differences.\n\n    Returns\n    -------\n    csets, projected_cset\n    csets : list of ndarrays((X_i,), dtype=int)\n        List indexed by thermodynamic state. Every element csets[k] is\n        the largest connected set at thermodynamic state k.\n    projected_cset : ndarray(M, dtype=int)\n        The overall connected set. This is the union of the individual\n        connected sets of the thermodynamic states.\n\n    References:\n    -----------\n    [1]_ Hukushima et al, Exchange Monte Carlo method and application to spin\n    glass simulations, J. Phys. Soc. Jan. 65, 1604 (1996)\n    [2]_ Shirts and Chodera, Statistically optimal analysis of samples\n    from multiple equilibrium states, J. Chem. Phys. 129, 124105 (2008)\n    \"\"\"\n    return _compute_csets(\n        connectivity, state_counts, count_matrices, ttrajs, dtrajs, bias_trajs,\n        nn=nn, equilibrium_state_counts=equilibrium_state_counts,\n        factor=factor, callback=callback)", "response": "r Computes the largest connected sets in the produce space of Markov state and the count matrix of the count matrix of the state and count matrix of the TRAM data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_csets_dTRAM(connectivity, count_matrices, nn=None, callback=None):\n    if connectivity=='post_hoc_RE' or connectivity=='BAR_variance':\n        raise Exception('Connectivity type %s not supported for dTRAM data.'%connectivity)\n    state_counts =  _np.maximum(count_matrices.sum(axis=1), count_matrices.sum(axis=2))\n    return _compute_csets(\n        connectivity, state_counts, count_matrices, None, None, None, nn=nn, callback=callback)", "response": "r Computes the largest connected sets for the given connectivity and count matrices."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restrict_to_csets(\n    csets, state_counts=None, count_matrices=None, ttrajs=None, dtrajs=None, bias_trajs=None):\n    r\"\"\"\n    Delete or deactivate elements that are not in the connected sets.\n\n    Parameters\n    ----------\n    csets : list of numpy.ndarray((M_i), dtype=int), length=T\n        List of connected sets for every thermodynamic state t.\n    state_counts : numpy.ndarray((T, M)), optional\n        Number of visits to the combinations of thermodynamic state\n        t and Markov state m.\n    count_matrices : numpy.ndarray((T, M, M)), optional\n        Count matrices for all T thermodynamic states.\n    ttrajs : list of numpy.ndarray(X_i), optional\n        List of generating thermodynamic state trajectories.\n        Only needed if dtrajs or bias_trajs are given as well.\n        Is used to determine which frames are in the connected sets.\n    dtrajs : list of ndarray(X_i), optional\n        List of configurational state trajectories (disctrajs).\n        If given, ttrajs must be set as well.\n    bias_trajs : list of ndarray((X_i, T)), optional\n        List of bias energy trajectories for all T thermodynamic states.\n        If given, ttrajs and dtrajs must be given as well.\n\n    Returns\n    -------\n    Modified copies of:\n    state_counts, count_matrices, dtrajs, bias_trajs\n\n    state_counts, count_matrices and dtrajs are in the same format\n    as the input parameters. Elements of state_counts and count_matrices\n    not in the connected sets are zero. Elements of dtrajs not in the\n    connected sets are negative.\n\n    bias_trajs : list of ndarray((Y_i, T))\n    Same as input but with frames removed where the combination\n    of thermodynamic state and Markov state as given in ttrajs and\n    dtrajs is not in the connected sets.\n    \"\"\"\n    if state_counts is not None:\n        new_state_counts = _np.zeros_like(state_counts, order='C', dtype=_np.intc)\n        for k,cset in enumerate(csets):\n            if len(cset)>0:\n                new_state_counts[k, cset] = state_counts[k, cset]\n    else:\n        new_state_counts = None\n    if count_matrices is not None:\n        new_count_matrices = _np.zeros_like(count_matrices, order='C', dtype=_np.intc)\n        for k,cset in enumerate(csets):\n            if len(cset)>0:\n                csetT = cset[:, _np.newaxis]\n                new_count_matrices[k, csetT, cset] = count_matrices[k, csetT, cset]\n    else:\n        new_count_matrices = None\n    if dtrajs is not None:\n        assert ttrajs is not None, 'ttrajs can\\'t be None, when dtrajs are given.'\n        n_therm_states, n_conf_states = state_counts.shape\n        invalid = _np.ones((n_therm_states, n_conf_states), dtype=bool)\n        for k, cset in enumerate(csets):\n            if len(cset) > 0:\n                invalid[k, cset] = False\n        new_dtrajs = []\n        assert len(ttrajs) == len(dtrajs)\n        for t, d in zip(ttrajs, dtrajs):\n            assert len(t) == len(d)\n            new_d = _np.array(d, dtype=_np.intc, copy=True, order='C', ndmin=1)\n            bad = invalid[t, d]\n            new_d[bad] = new_d[bad] - n_conf_states # 'numpy equivalent' indices as in x[i]==x[i+len(x)]\n            assert _np.all(new_d[bad] < 0)\n            new_dtrajs.append(new_d)\n    else:\n        new_dtrajs = None\n    if bias_trajs is not None:\n        assert ttrajs is not None, 'ttrajs can\\'t be None, when bias_trajs are given.'\n        assert dtrajs is not None, 'dtrajs can\\'t be None, when bias_trajs are given.'\n        n_therm_states, n_conf_states = state_counts.shape\n        valid = _np.zeros((n_therm_states, n_conf_states), dtype=bool)\n        for k, cset in enumerate(csets):\n            if len(cset) > 0:\n                valid[k, cset] = True\n        new_bias_trajs = []\n        assert len(ttrajs) == len(dtrajs) == len(bias_trajs)\n        for t, d, b in zip(ttrajs, dtrajs, bias_trajs):\n            assert len(t) == len(d) == len(b)\n            ok_traj = valid[t, d]\n            new_b = _np.zeros((_np.count_nonzero(ok_traj), b.shape[1]), dtype=_np.float64)\n            new_b[:] = b[ok_traj, :]\n            new_bias_trajs.append(new_b)\n    else:\n        new_bias_trajs = None\n    return new_state_counts, new_count_matrices, new_dtrajs, new_bias_trajs", "response": "r Restricts the given set of state_counts count_matrices ttrajs and bias trajectories to the given set of state_counts and count_matrices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of all indexes of the given array.", "response": "def _indexes(arr):\n    \"\"\" Returns the list of all indexes of the given array.\n\n    Currently works for one and two-dimensional arrays\n\n    \"\"\"\n    myarr = np.array(arr)\n    if myarr.ndim == 1:\n        return list(range(len(myarr)))\n    elif myarr.ndim == 2:\n        return tuple(itertools.product(list(range(arr.shape[0])),\n                                       list(range(arr.shape[1]))))\n    else:\n        raise NotImplementedError('Only supporting arrays of dimension 1 and 2 as yet.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a column with given indexes from a deep array.", "response": "def _column(arr, indexes):\n    \"\"\" Returns a column with given indexes from a deep array\n\n    For example, if the array is a matrix and indexes is a single int, will\n    return arr[:,indexes]. If the array is an order 3 tensor and indexes is a\n    pair of ints, will return arr[:,indexes[0],indexes[1]], etc.\n\n    \"\"\"\n    if arr.ndim == 2 and types.is_int(indexes):\n        return arr[:, indexes]\n    elif arr.ndim == 3 and len(indexes) == 2:\n        return arr[:, indexes[0], indexes[1]]\n    else:\n        raise NotImplementedError('Only supporting arrays of dimension 2 and 3 as yet.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef confidence_interval(data, conf=0.95):\n    if conf < 0 or conf > 1:\n        raise ValueError('Not a meaningful confidence level: '+str(conf))\n\n    try:\n        data = types.ensure_ndarray(data, kind='numeric')\n    except:\n        # if 1D array of arrays try to fuse it\n        if isinstance(data, np.ndarray) and np.ndim(data) == 1:\n            newshape = tuple([len(data)] + list(data[0].shape))\n            newdata = np.zeros(newshape)\n            for i in range(len(data)):\n                newdata[i, :] = data[i]\n            data = newdata\n\n    types.assert_array(data, kind='numeric')\n\n    if np.ndim(data) == 1:\n        m, lower, upper = _confidence_interval_1d(data, conf)\n        return lower, upper\n    else:\n        I = _indexes(data[0])\n        lower = np.zeros(data[0].shape)\n        upper = np.zeros(data[0].shape)\n        for i in I:\n            col = _column(data, i)\n            m, lower[i], upper[i] = _confidence_interval_1d(col, conf)\n        # return\n        return lower, upper", "response": "r Computes element - wise confidence intervals from a sample of arbitrarily shaped ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _maxlength(X):\n    N = 0\n    for x in X:\n        if len(x) > N:\n            N = len(x)\n    return N", "response": "Returns the maximum length of signal trajectories X"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nestimate the statistical inefficiency from univariate time series X.", "response": "def statistical_inefficiency(X, truncate_acf=True):\n    \"\"\" Estimates the statistical inefficiency from univariate time series X\n\n    The statistical inefficiency [1]_ is a measure of the correlatedness of samples in a signal.\n    Given a signal :math:`{x_t}` with :math:`N` samples and statistical inefficiency :math:`I \\in (0,1]`, there are\n    only :math:`I \\cdot N` effective or uncorrelated samples in the signal. This means that :math:`I \\cdot N` should\n    be used in order to compute statistical uncertainties. See [2]_ for a review.\n\n    The statistical inefficiency is computed as :math:`I = (2 \\tau)^{-1}` using the damped autocorrelation time\n\n    ..1:        \\tau = \\frac{1}{2}+\\sum_{K=1}^{N} A(k) \\left(1-\\frac{k}{N}\\right)\n\n    where\n\n    ..1:        A(k) = \\frac{\\langle x_t x_{t+k} \\rangle_t - \\langle x^2 \\rangle_t}{\\mathrm{var}(x)}\n\n    is the autocorrelation function of the signal :math:`{x_t}`, which is computed either for a single or multiple\n    trajectories.\n\n    Parameters\n    ----------\n    X : float array or list of float arrays\n        Univariate time series (single or multiple trajectories)\n    truncate_acf : bool, optional, default=True\n        When the normalized autocorrelation function passes through 0, it is truncated in order to avoid integrating\n        random noise\n\n    References\n    ----------\n    .. [1] Anderson, T. W.: The Statistical Analysis of Time Series (Wiley, New York, 1971)\n\n    .. [2] Janke, W: Statistical Analysis of Simulations: Data Correlations and Error Estimation\n        Quantum Simulations of Complex Many-Body Systems: From Theory to Algorithms, Lecture Notes,\n        J. Grotendorst, D. Marx, A. Muramatsu (Eds.), John von Neumann Institute for Computing, Juelich\n        NIC Series 10, pp. 423-445, 2002.\n\n    \"\"\"\n    # check input\n    assert np.ndim(X[0]) == 1, 'Data must be 1-dimensional'\n    N = _maxlength(X)  # length\n    # mean-free data\n    xflat = np.concatenate(X)\n    Xmean  = np.mean(xflat)\n    X0 = [x-Xmean for x in X]\n    # moments\n    x2m = np.mean(xflat ** 2)\n    # integrate damped autocorrelation\n    corrsum = 0.0\n    for lag in range(N):\n        acf = 0.0\n        n = 0.0\n        for x in X0:\n            Nx = len(x)  # length of this trajectory\n            if (Nx > lag):  # only use trajectories that are long enough\n                acf += np.sum(x[0:Nx-lag] * x[lag:Nx])\n                n += float(Nx-lag)\n        acf /= n\n        if acf <= 0 and truncate_acf:  # zero autocorrelation. Exit\n            break\n        elif lag > 0:  # start integrating at lag 1 (effect of lag 0 is contained in the 0.5 below\n            corrsum += acf * (1.0 - (float(lag)/float(N)))\n    # compute damped correlation time\n    corrtime = 0.5 + corrsum / x2m\n    # return statistical inefficiency\n    return 1.0 / (2 * corrtime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the database name for the given key.", "response": "def _database_from_key(self, key):\n        \"\"\"\n        gets the database name for the given key. Should ensure a uniform spread\n        of keys over the databases in order to minimize waiting times. Since the\n        database has to be locked for updates and multiple processes want to write,\n        each process has to wait until the lock has been released.\n\n        By default the LRU databases will be stored in a sub directory \"traj_info_usage\"\n        lying next to the main database.\n\n        :param key: hash of the TrajInfo instance\n        :return: str, database path\n        \"\"\"\n        if not self.filename:\n            return None\n\n        from pyemma.util.files import mkdir_p\n        hash_value_long = int(key, 16)\n        # bin hash to one of either 10 different databases\n        # TODO: make a configuration parameter out of this number\n        db_name = str(hash_value_long)[-1] + '.db'\n        directory = os.path.dirname(self.filename) + os.path.sep + 'traj_info_usage'\n        mkdir_p(directory)\n        return os.path.join(directory, db_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_time_stamp(self, hash_value):\n        db_name = self._database_from_key(hash_value)\n        if not db_name:\n            db_name=':memory:'\n\n        def _update():\n            import sqlite3\n            try:\n                with sqlite3.connect(db_name, timeout=self.lru_timeout) as conn:\n                    \"\"\" last_read is a result of time.time()\"\"\"\n                    conn.execute('CREATE TABLE IF NOT EXISTS usage '\n                                 '(hash VARCHAR(32), last_read FLOAT)')\n                    conn.commit()\n                    cur = conn.execute('select * from usage where hash=?', (hash_value,))\n                    row = cur.fetchone()\n                    if not row:\n                        conn.execute(\"insert into usage(hash, last_read) values(?, ?)\", (hash_value, time.time()))\n                    else:\n                        conn.execute(\"update usage set last_read=? where hash=?\", (time.time(), hash_value))\n                    conn.commit()\n            except sqlite3.OperationalError:\n                # if there are many jobs to write to same database at same time, the timeout could be hit\n                logger.debug('could not update LRU info for db %s', db_name)\n\n        # this could lead to another (rare) race condition during cleaning...\n        #import threading\n        #threading.Thread(target=_update).start()\n        _update()", "response": "Update the time stamp of the entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete n% oldest entries from the database and the associated LRU dbs", "response": "def _clean(self, n):\n        \"\"\"\n        obtain n% oldest entries by looking into the usage databases. Then these entries\n        are deleted first from the traj_info db and afterwards from the associated LRU dbs.\n\n        :param n: delete n% entries in traj_info db [and associated LRU (usage) dbs].\n        \"\"\"\n        # delete the n % oldest entries in the database\n        import sqlite3\n        num_delete = int(self.num_entries / 100.0 * n)\n        logger.debug(\"removing %i entries from db\" % num_delete)\n        lru_dbs = self._database.execute(\"select hash, lru_db from traj_info\").fetchall()\n        lru_dbs.sort(key=itemgetter(1))\n        hashs_by_db = {}\n        age_by_hash = []\n        for k, v in itertools.groupby(lru_dbs, key=itemgetter(1)):\n            hashs_by_db[k] = list(x[0] for x in v)\n\n        # debug: distribution\n        len_by_db = {os.path.basename(db): len(hashs_by_db[db]) for db in hashs_by_db.keys()}\n        logger.debug(\"distribution of lru: %s\", str(len_by_db))\n        ### end dbg\n\n        # collect timestamps from databases\n        for db in hashs_by_db.keys():\n            with sqlite3.connect(db, timeout=self.lru_timeout) as conn:\n                rows = conn.execute(\"select hash, last_read from usage\").fetchall()\n                for r in rows:\n                    age_by_hash.append((r[0], float(r[1]), db))\n\n        # sort by age\n        age_by_hash.sort(key=itemgetter(1))\n        if len(age_by_hash)>=2:\n            assert[age_by_hash[-1] > age_by_hash[-2]]\n        ids = map(itemgetter(0), age_by_hash[:num_delete])\n        ids = tuple(map(str, ids))\n\n        sql_compatible_ids = SqliteDB._format_tuple_for_sql(ids)\n\n        with self._database as c:\n            c.execute(\"DELETE FROM traj_info WHERE hash in (%s)\" % sql_compatible_ids)\n\n            # iterate over all LRU databases and delete those ids, we've just deleted from the main db.\n            # Do this within the same execution block of the main database, because we do not want the entry to be deleted,\n            # in case of a subsequent failure.\n            age_by_hash.sort(key=itemgetter(2))\n            for db, values in itertools.groupby(age_by_hash, key=itemgetter(2)):\n                values = tuple(v[0] for v in values)\n                with sqlite3.connect(db, timeout=self.lru_timeout) as conn:\n                        stmnt = \"DELETE FROM usage WHERE hash IN (%s)\" \\\n                                % SqliteDB._format_tuple_for_sql(values)\n                        curr = conn.execute(stmnt)\n                        assert curr.rowcount == len(values), curr.rowcount"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef estimate(self, X, **params):\n        return super(TRAM, self).estimate(X, **params)", "response": "Estimate the thermodynamic state and configurational states for a given set of time series X."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef log_likelihood(self):\n        # TODO: check that we are estimated...\n        return _tram.log_likelihood_lower_bound(\n            self.log_lagrangian_mult, self.biased_conf_energies,\n            self.count_matrices, self.btrajs, self.dtrajs, self.state_counts,\n            None, None, None, None, None)", "response": "r Returns the log - likelihood of the converged TRAM estimate."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if random access is enabled.", "response": "def is_random_accessible(self):\n        \"\"\"\n        Check if self._is_random_accessible is set to true and if all the random access strategies are implemented.\n        Returns\n        -------\n        bool : Returns True if random accessible via strategies and False otherwise.\n        \"\"\"\n        return self._is_random_accessible and \\\n               not isinstance(self.ra_itraj_cuboid, NotImplementedRandomAccessStrategy) and \\\n               not isinstance(self.ra_linear, NotImplementedRandomAccessStrategy) and \\\n               not isinstance(self.ra_itraj_jagged, NotImplementedRandomAccessStrategy) and \\\n               not isinstance(self.ra_itraj_linear, NotImplementedRandomAccessStrategy)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the N - dimensional histogram of the transformed data.", "response": "def histogram(transform, dimensions, nbins):\n    '''Computes the N-dimensional histogram of the transformed data.\n\n    Parameters\n    ----------\n    transform : pyemma.coordinates.transfrom.Transformer object\n        transform that provides the input data\n    dimensions : tuple of indices\n        indices of the dimensions you want to examine\n    nbins : tuple of ints\n        number of bins along each dimension\n\n    Returns\n    -------\n    counts : (bins[0],bins[1],...) ndarray of ints\n        counts compatible with pyplot.pcolormesh and pyplot.bar\n    edges : list of (bins[i]) ndarrays\n        bin edges compatible with pyplot.pcolormesh and pyplot.bar,\n        see below.\n\n    Examples\n    --------\n\n    >>> import matplotlib.pyplot as plt # doctest: +SKIP\n\n    Only for ipython notebook\n    >> %matplotlib inline  # doctest: +SKIP\n\n    >>> counts, edges=histogram(transform, dimensions=(0,1), nbins=(20, 30)) # doctest: +SKIP\n    >>> plt.pcolormesh(edges[0], edges[1], counts.T) # doctest: +SKIP\n\n    >>> counts, edges=histogram(transform, dimensions=(1,), nbins=(50,)) # doctest: +SKIP\n    >>> plt.bar(edges[0][:-1], counts, width=edges[0][1:]-edges[0][:-1]) # doctest: +SKIP\n    '''\n    maximum = np.ones(len(dimensions)) * (-np.inf)\n    minimum = np.ones(len(dimensions)) * np.inf\n    # compute min and max\n    for _, chunk in transform:\n        maximum = np.max(\n            np.vstack((\n                maximum,\n                np.max(chunk[:, dimensions], axis=0))),\n            axis=0)\n        minimum = np.min(\n            np.vstack((\n                minimum,\n                np.min(chunk[:, dimensions], axis=0))),\n            axis=0)\n    # define bins\n    bins = [np.linspace(m, M, num=n)\n            for m, M, n in zip(minimum, maximum, nbins)]\n    res = np.zeros(np.array(nbins) - 1)\n    # compute actual histogram\n    for _, chunk in transform:\n        part, _ = np.histogramdd(chunk[:, dimensions], bins=bins)\n        res += part\n    return res, bins"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking latest version online from http://emma - project. org", "response": "def _version_check(current, testing=False):\n    \"\"\" checks latest version online from http://emma-project.org.\n\n    Can be disabled by setting config.check_version = False.\n\n    >>> from unittest.mock import patch\n    >>> import warnings, pyemma\n    >>> with warnings.catch_warnings(record=True) as cw, patch('pyemma.version', '0.1'):\n    ...     warnings.simplefilter('always', UserWarning)\n    ...     v = pyemma.version\n    ...     t = pyemma._version_check(v, testing=True)\n    ...     t.start()\n    ...     t.join()\n    ...     assert cw, \"no warning captured\"\n    ...     assert \"latest release\" in str(cw[0].message), \"wrong msg\"\n    \"\"\"\n    if not config.check_version:\n        class _dummy:\n            def start(self): pass\n        return _dummy()\n    import json\n    import platform\n    import os\n\n    from distutils.version import LooseVersion as parse\n    from contextlib import closing\n    import threading\n    import uuid\n\n    import sys\n    if 'pytest' in sys.modules or os.getenv('CI', False):\n        testing = True\n\n    def _impl():\n        import warnings\n        from urllib.request import Request, urlopen\n\n        try:\n            r = Request('http://emma-project.org/versions.json',\n                        headers={'User-Agent': 'PyEMMA-{emma_version}-Py-{python_version}-{platform}-{addr}'\n                        .format(emma_version=current, python_version=platform.python_version(),\n                                platform=platform.platform(terse=True), addr=uuid.getnode())} if not testing else {})\n            with closing(urlopen(r, timeout=30)) as response:\n                payload = str(response.read(), encoding='ascii')\n            versions = json.loads(payload)\n            latest_json = tuple(filter(lambda x: x['latest'], versions))[0]['version']\n            latest = parse(latest_json)\n            if parse(current) < latest:\n                warnings.warn(\"You are not using the latest release of PyEMMA.\"\n                              \" Latest is {latest}, you have {current}.\"\n                              .format(latest=latest, current=current), category=UserWarning)\n            if sys.version_info[0] < 3:\n                warnings.warn(\"Python 2.7 usage is deprecated. \"\n                              \"Future versions of PyEMMA will not support it. \"\n                              \"Please upgrade your Python installation.\", category=UserWarning)\n        except Exception:\n            import logging\n            logging.getLogger('pyemma').debug(\"error during version check\", exc_info=True)\n    return threading.Thread(target=_impl)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading runtime configuration from given filename.", "response": "def load(self, filename=None):\n        \"\"\" load runtime configuration from given filename.\n        If filename is None try to read from default file from\n        default location. \"\"\"\n        if not filename:\n            filename = self.default_config_file\n\n        files = self._cfgs_to_read()\n        # insert last, so it will override all values,\n        # which have already been set in previous files.\n        files.insert(-1, filename)\n\n        try:\n            config = self.__read_cfg(files)\n        except ReadConfigException as e:\n            print(Config._format_msg('config.load(\"{file}\") failed with {error}'.format(file=filename, error=e)))\n        else:\n            self._conf_values = config\n\n        # notice user?\n        if self.show_config_notification and not self.cfg_dir:\n            print(Config._format_msg(\"no configuration directory set or usable.\"\n                                      \" Falling back to defaults.\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, filename=None):\n        if not filename:\n            filename = self.DEFAULT_CONFIG_FILE_NAME\n        else:\n            filename = str(filename)\n            # try to extract the path from filename and use is as cfg_dir\n            head, tail = os.path.split(filename)\n            if head:\n                self._cfg_dir = head\n\n            # we are search for .cfg files in cfg_dir so make sure it contains the proper extension.\n            base, ext = os.path.splitext(tail)\n            if ext != \".cfg\":\n                filename += \".cfg\"\n\n        # if we have no cfg dir, try to create it first. Return if it failed.\n        if not self.cfg_dir or not os.path.isdir(self.cfg_dir) or not os.stat(self.cfg_dir) != os.W_OK:\n            try:\n                self.cfg_dir = self.DEFAULT_CONFIG_DIR\n            except ConfigDirectoryException as cde:\n\n                print(Config._format_msg('Could not create configuration directory \"{dir}\"! config.save() failed.'\n                                          ' Please set a writeable location with config.cfg_dir = val. Error was {exc}'\n                                          .format(dir=self.cfg_dir, exc=cde)))\n                return\n\n        filename = os.path.join(self.cfg_dir, filename)\n\n        try:\n            with open(filename, 'w') as fh:\n                self._conf_values.write(fh)\n        except IOError as ioe:\n            print(Config._format_msg(\"Save failed with error %s\" % ioe))", "response": "Saves the runtime configuration to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefaults config file living in PyEMMA package", "response": "def default_config_file(self):\n        \"\"\" default config file living in PyEMMA package \"\"\"\n        import os.path as p\n        import pyemma\n        return p.join(pyemma.__path__[0], Config.DEFAULT_CONFIG_FILE_NAME)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the PyEMMAs configuration directory.", "response": "def cfg_dir(self, pyemma_cfg_dir):\n        \"\"\" Sets PyEMMAs configuration directory.\n        Also creates it with some default files, if does not exists. \"\"\"\n        if not os.path.exists(pyemma_cfg_dir):\n            try:\n                mkdir_p(pyemma_cfg_dir)\n            except NotADirectoryError:  # on Python 3\n                raise ConfigDirectoryException(\"pyemma cfg dir (%s) is not a directory\" % pyemma_cfg_dir)\n            except EnvironmentError:\n                raise ConfigDirectoryException(\"could not create configuration directory '%s'\" % pyemma_cfg_dir)\n\n        if not os.path.isdir(pyemma_cfg_dir):\n            raise ConfigDirectoryException(\"%s is no valid directory\" % pyemma_cfg_dir)\n        if not os.access(pyemma_cfg_dir, os.W_OK):\n            raise ConfigDirectoryException(\"%s is not writeable\" % pyemma_cfg_dir)\n\n        # give user the default cfg file, if its not there\n        self.__copy_default_files_to_cfg_dir(pyemma_cfg_dir)\n        self._cfg_dir = pyemma_cfg_dir\n\n        if self.show_config_notification:\n            stars = '*' * 80\n            print(stars, '\\n',\n                  'Changed PyEMMAs config directory to \"{dir}\".\\n'\n                  'To make this change permanent, export the environment variable'\n                  ' \"PYEMMA_CFG_DIR\" \\nto point to this location. Eg. edit your .bashrc file!'\n                  .format(dir=pyemma_cfg_dir), '\\n', stars, sep='')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads config files from various locations to build final config.", "response": "def _cfgs_to_read(self):\n        \"\"\"\n        reads config files from various locations to build final config.\n        \"\"\"\n        # use these files to extend/overwrite the conf_values.\n        # Last red file always overwrites existing values!\n        cfg = Config.DEFAULT_CONFIG_FILE_NAME\n        filenames = [\n            self.default_config_file,\n            cfg,  # conf_values in current directory\n            os.path.join(os.path.expanduser('~' + os.path.sep), cfg),  # config in user dir\n            '.pyemma.cfg',\n        ]\n\n        # look for user defined files\n        if self.cfg_dir:\n            from glob import glob\n            filenames.extend(glob(self.cfg_dir + os.path.sep + \"*.cfg\"))\n        return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_ra_index_indices(self):\n        fragment_indices = []\n        for idx, cumlen in enumerate(self._cumulative_lengths):\n            cumlen_prev = self._cumulative_lengths[idx - 1] if idx > 0 else 0\n            fragment_indices.append([np.argwhere(\n                np.logical_and(self.ra_indices >= cumlen_prev, self.ra_indices < cumlen)\n            )])\n        return fragment_indices", "response": "Returns a list containing indices of the ra_index array which correspond to the separate trajectory fragments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving two trajectories T_1 and T_2, this function calculates for the first trajectory an overlap, i.e., a skip parameter for T_2 such that the trajectory fragments T_1 and T_2 appear as one under the given stride. Idea for deriving the formula: It is K = ((traj_len - skip - 1) // stride + 1) = #(data points in trajectory of length (traj_len - skip)). Therefore, the first point's position that is not contained in T_1 anymore is given by pos = skip + s * K. Thus the needed skip of T_2 such that the same stride parameter makes T_1 and T_2 \"look as one\" is overlap = pos - traj_len. :param stride: the (global) stride parameter :param traj_len: length of T_1 :param skip: skip of T_1 :return: skip of T_2", "response": "def _calculate_new_overlap(stride, traj_len, skip):\n        \"\"\"\n        Given two trajectories T_1 and T_2, this function calculates for the first trajectory an overlap, i.e.,\n        a skip parameter for T_2 such that the trajectory fragments T_1 and T_2 appear as one under the given stride.\n\n        Idea for deriving the formula: It is\n\n        K = ((traj_len - skip - 1) // stride + 1) = #(data points in trajectory of length (traj_len - skip)).\n\n        Therefore, the first point's position that is not contained in T_1 anymore is given by\n\n        pos = skip + s * K.\n\n        Thus the needed skip of T_2 such that the same stride parameter makes T_1 and T_2 \"look as one\" is\n\n        overlap = pos - traj_len.\n\n        :param stride: the (global) stride parameter\n        :param traj_len: length of T_1\n        :param skip: skip of T_1\n        :return: skip of T_2\n        \"\"\"\n        overlap = stride * ((traj_len - skip - 1) // stride + 1) - traj_len + skip\n        return overlap"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the singleton instance of the TrajectoryInfoCache class", "response": "def instance():\n        \"\"\" :returns the TrajectoryInfoCache singleton instance\"\"\"\n        if TrajectoryInfoCache._instance is None:\n            # if we do not have a configuration director yet, we do not want to store\n            if not config.cfg_dir:\n                filename = None\n            else:\n                filename = os.path.join(config.cfg_dir, \"traj_info.sqlite3\")\n            TrajectoryInfoCache._instance = TrajectoryInfoCache(filename)\n\n        return TrajectoryInfoCache._instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_allclose(actual, desired, rtol=1.e-5, atol=1.e-8,\n                    err_msg='', verbose=True):\n    r\"\"\"wrapper for numpy.testing.allclose with default tolerances of\n    numpy.allclose. Needed since testing method has different values.\"\"\"\n    return assert_allclose_np(actual, desired, rtol=rtol, atol=atol,\n                              err_msg=err_msg, verbose=verbose)", "response": "r wrapper for numpy. allclose with default tolerances of\n    numpy. allclose. Used for testing."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef topology_to_numpy(top):\n    data = [(atom.serial, atom.name, atom.element.symbol,\n             atom.residue.resSeq, atom.residue.name,\n             atom.residue.chain.index, atom.segment_id) for atom in top.atoms]\n    atoms = np.array(data,\n                     dtype=[(\"serial\", 'i4'), (\"name\", 'S4'), (\"element\", 'S3'),\n                            (\"resSeq\", 'i4'), (\"resName\", 'S4'), (\"chainID\", 'i4'), (\"segmentID\", 'S4')])\n\n    bonds = np.fromiter(((a.index, b.index) for (a, b) in top.bonds), dtype='i4,i4', count=top.n_bonds)\n    return atoms, bonds", "response": "Convert this topology into a pandas dataframe\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef topology_from_numpy(atoms, bonds=None):\n    if bonds is None:\n        bonds = np.zeros((0, 2))\n\n    for col in [\"name\", \"element\", \"resSeq\",\n                \"resName\", \"chainID\", \"serial\"]:\n        if col not in atoms.dtype.names:\n            raise ValueError('dataframe must have column %s' % col)\n\n    if \"segmentID\" not in atoms.dtype.names:\n        atoms[\"segmentID\"] = \"\"\n\n    from mdtraj.core.topology import Atom\n    from mdtraj.core import element as elem\n    out = mdtraj.Topology()\n\n    # TODO: allow for h5py data sets here, is there a way to check generic ndarray interface?\n    #if not isinstance(bonds, np.ndarray):\n    #    raise TypeError('bonds must be an instance of numpy.ndarray. '\n    #                    'You supplied a %s' % type(bonds))\n\n    out._atoms = [None for _ in range(len(atoms))]\n\n    N = np.arange(0, len(atoms))\n\n    for ci in np.unique(atoms['chainID']):\n        chain_atoms = atoms[atoms['chainID'] == ci]\n        subN = N[atoms['chainID'] == ci]\n        c = out.add_chain()\n\n        for ri in np.unique(chain_atoms['resSeq']):\n            residue_atoms = chain_atoms[chain_atoms['resSeq'] == ri]\n            mask = subN[chain_atoms['resSeq'] == ri]\n            indices = N[mask]\n            rnames = residue_atoms['resName']\n            residue_name = np.array(rnames)[0]\n            segids = residue_atoms['segmentID']\n            segment_id = np.array(segids)[0]\n            if not np.all(rnames == residue_name):\n                raise ValueError('All of the atoms with residue index %d '\n                                 'do not share the same residue name' % ri)\n            r = out.add_residue(residue_name.decode('ascii'), c, ri, segment_id.decode('ascii'))\n\n            for ix, atom in enumerate(residue_atoms):\n                e = atom['element'].decode('ascii')\n                a = Atom(atom['name'].decode('ascii'), elem.get_by_symbol(e),\n                         int(indices[ix]), r, serial=atom['serial'])\n                out._atoms[indices[ix]] = a\n                r._atoms.append(a)\n\n    for ai1, ai2 in bonds:\n        out.add_bond(out.atom(ai1), out.atom(ai2))\n\n    out._numAtoms = out.n_atoms\n    return out", "response": "Create a mdtraj. Topology object from numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading discrete state trajectory from ascii file.", "response": "def read_discrete_trajectory(filename):\n    \"\"\"Read discrete trajectory from ascii file.\n\n    The ascii file containing a single column with integer entries is\n    read into an array of integers.\n\n    Parameters\n    ----------\n    filename : str\n        The filename of the discrete state trajectory file.\n        The filename can either contain the full or the\n        relative path to the file.\n\n    Returns\n    -------\n    dtraj : (M, ) ndarray\n        Discrete state trajectory.\n\n    \"\"\"\n    with open(filename, \"r\") as f:\n        lines=f.read()\n        dtraj=np.fromstring(lines, dtype=int, sep=\"\\n\")\n        return dtraj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_discrete_trajectory(filename, dtraj):\n    dtraj=np.asarray(dtraj)\n    with open(filename, 'w') as f:\n        dtraj.tofile(f, sep='\\n', format='%d')", "response": "r Write discrete trajectory to ascii file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count_states(dtrajs, ignore_negative=False):\n    # format input\n    dtrajs = _ensure_dtraj_list(dtrajs)\n    # make bincounts for each input trajectory\n    nmax = 0\n    bcs = []\n    for dtraj in dtrajs:\n        if ignore_negative:\n            dtraj = dtraj[np.where(dtraj >= 0)]\n        bc = np.bincount(dtraj)\n        nmax = max(nmax, bc.shape[0])\n        bcs.append(bc)\n    # construct total bincount\n    res = np.zeros(nmax, dtype=int)\n    # add up individual bincounts\n    for i, bc in enumerate(bcs):\n        res[:bc.shape[0]] += bc\n    return res", "response": "r returns a histogram count of the states in a trajectory or list of discretized trajectories."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample trajectory and time indexes according to the given sequence of discrete states.", "response": "def sample_indexes_by_sequence(indexes, sequence):\n    \"\"\"Samples trajectory/time indexes according to the given sequence of states\n\n    Parameters\n    ----------\n    indexes : list of ndarray( (N_i, 2) )\n        For each state, all trajectory and time indexes where this state occurs.\n        Each matrix has a number of rows equal to the number of occurrences of the corresponding state,\n        with rows consisting of a tuple (i, t), where i is the index of the trajectory and t is the time index\n        within the trajectory.\n    sequence : array of integers\n        A sequence of discrete states. For each state, a trajectory/time index will be sampled at which dtrajs\n        have an occurrences of this state\n\n    Returns\n    -------\n    indexes : ndarray( (N, 2) )\n        The sampled index sequence.\n        Index array with a number of rows equal to N=len(sequence), with rows consisting of a tuple (i, t),\n        where i is the index of the trajectory and t is the time index within the trajectory.\n\n    \"\"\"\n    N = len(sequence)\n    res = np.zeros((N,2), dtype=int)\n    for t in range(N):\n        s = sequence[t]\n        i = np.random.randint(indexes[s].shape[0])\n        res[t,:] = indexes[s][i,:]\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stdchannel_redirected(stdchannel, dest_filename, fake=False):\n    if fake:\n        yield\n        return\n    oldstdchannel = dest_file = None\n    try:\n        oldstdchannel = os.dup(stdchannel.fileno())\n        dest_file = open(dest_filename, 'w')\n        os.dup2(dest_file.fileno(), stdchannel.fileno())\n\n        yield\n    finally:\n        if oldstdchannel is not None:\n            os.dup2(oldstdchannel, stdchannel.fileno())\n        if dest_file is not None:\n            dest_file.close()", "response": "A context manager that temporarily redirects stdout or stderr to dest_filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_flag(compiler, flagname):\n    with TemporaryDirectory() as tmpdir, \\\n            stdchannel_redirected(sys.stderr, os.devnull), \\\n            stdchannel_redirected(sys.stdout, os.devnull):\n        f = tempfile.mktemp(suffix='.cpp', dir=tmpdir)\n        with open(f, 'w') as fh:\n            fh.write('int main (int argc, char **argv) { return 0; }')\n        try:\n            compiler.compile([f], extra_postargs=[flagname], output_dir=tmpdir)\n        except setuptools.distutils.errors.CompileError:\n            return False\n    return True", "response": "Return a boolean indicating whether a flag name is supported on\n    the specified compiler."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the dimension of the object", "response": "def dimension(self):\n        \"\"\" output dimension \"\"\"\n        if self.dim > -1:\n            return self.dim\n        d = None\n        if self.dim != -1 and not self._estimated:  # fixed parametrization\n            d = self.dim\n        elif self._estimated:  # parametrization finished. Dimension is known\n            dim = len(self.eigenvalues)\n            if self.var_cutoff < 1.0:  # if subspace_variance, reduce the output dimension if needed\n                dim = min(dim, np.searchsorted(self.cumvar, self.var_cutoff) + 1)\n            d = dim\n        elif self.var_cutoff == 1.0:  # We only know that all dimensions are wanted, so return input dim\n            d = self.data_producer.dimension()\n        else:  # We know nothing. Give up\n            raise RuntimeError('Requested dimension, but the dimension depends on the cumulative variance and the '\n                               'transformer has not yet been estimated. Call estimate() before.')\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _transform_array(self, X):\n        X_meanfree = X - self.mean\n        Y = np.dot(X_meanfree, self.eigenvectors[:, 0:self.dimension()])\n\n        return Y.astype(self.output_type())", "response": "r Projects the data onto the dominant independent components."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the implied timescales of the TICA transformation.", "response": "def timescales(self):\n        r\"\"\"Implied timescales of the TICA transformation\n\n        For each :math:`i`-th eigenvalue, this returns\n\n        .. math::\n\n            t_i = -\\frac{\\tau}{\\log(|\\lambda_i|)}\n\n        where :math:`\\tau` is the :py:obj:`lag` of the TICA object and :math:`\\lambda_i` is the `i`-th\n        :py:obj:`eigenvalue <eigenvalues>` of the TICA object.\n\n        Returns\n        -------\n        timescales: 1D np.array\n            numpy array with the implied timescales. In principle, one should expect as many timescales as\n            input coordinates were available. However, less eigenvalues will be returned if the TICA matrices\n            were not full rank or :py:obj:`var_cutoff` was parsed\n        \"\"\"\n        return -self.lag / np.log(np.abs(self.eigenvalues))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef feature_TIC_correlation(self):\n        feature_sigma = np.sqrt(np.diag(self.cov))\n        return np.dot(self.cov, self.eigenvectors[:, : self.dimension()]) / feature_sigma[:, np.newaxis]", "response": "Instantaneous correlation matrix between mean - free input features and TICs and TICs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _svd_sym_koopman(K, C00_train, Ctt_train):\n    from pyemma._ext.variational.solvers.direct import spd_inv_sqrt\n    # reweight operator to empirical distribution\n    C0t_re = mdot(C00_train, K)\n    # symmetrized operator and SVD\n    K_sym = mdot(spd_inv_sqrt(C00_train), C0t_re, spd_inv_sqrt(Ctt_train))\n    U, S, Vt = np.linalg.svd(K_sym, compute_uv=True, full_matrices=False)\n    # projects back to singular functions of K\n    U = mdot(spd_inv_sqrt(C00_train), U)\n    Vt = mdot(Vt,spd_inv_sqrt(Ctt_train))\n    return U, S, Vt.T", "response": "Computes the SVD of the symmetrized Koopman operator in the empirical distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vamp_1_score(K, C00_train, C0t_train, Ctt_train, C00_test, C0t_test, Ctt_test, k=None):\n    from pyemma._ext.variational.solvers.direct import spd_inv_sqrt\n\n    # SVD of symmetrized operator in empirical distribution\n    U, S, V = _svd_sym_koopman(K, C00_train, Ctt_train)\n    if k is not None:\n        U = U[:, :k]\n        # S = S[:k][:, :k]\n        V = V[:, :k]\n    A = spd_inv_sqrt(mdot(U.T, C00_test, U))\n    B = mdot(U.T, C0t_test, V)\n    C = spd_inv_sqrt(mdot(V.T, Ctt_test, V))\n\n    # compute trace norm (nuclear norm), equal to the sum of singular values\n    score = np.linalg.norm(mdot(A, B, C), ord='nuc')\n    return score", "response": "Computes the VAMP - 1 score of a kinetic model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vamp_e_score(K, C00_train, C0t_train, Ctt_train, C00_test, C0t_test, Ctt_test, k=None):\n    # SVD of symmetrized operator in empirical distribution\n    U, s, V = _svd_sym_koopman(K, C00_train, Ctt_train)\n    if k is not None:\n        U = U[:, :k]\n        S = np.diag(s[:k])\n        V = V[:, :k]\n    score = np.trace(2.0 * mdot(V, S, U.T, C0t_test) - mdot(V, S, U.T, C00_test, U, S, V.T, Ctt_test))\n    return score", "response": "Computes the VAMP - E score of a kinetic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies docstrings of derived attributes ( methods properties attrs from parent classes", "response": "def fix_docs(cls):\n    \"\"\" copies docstrings of derived attributes (methods, properties, attrs) from parent classes.\"\"\"\n    import inspect\n    public_undocumented_members = {name: func for name, func in inspect.getmembers(cls)\n                                   if not name.startswith('_') and func.__doc__ is None}\n\n    for name, func in public_undocumented_members.items():\n        for parent in cls.__mro__[1:]:\n            parfunc = getattr(parent, name, None)\n            if parfunc and getattr(parfunc, '__doc__', None):\n                if isinstance(func, property):\n                    # copy property, since its doc attribute is read-only\n                    new_prop = property(fget=func.fget, fset=func.fset,\n                                        fdel=func.fdel, doc=parfunc.__doc__)\n                    setattr(cls, name, new_prop)\n                else:\n                    if hasattr(func, '__func__'):  # handle instancemethods\n                        func.__func__.__doc__ = parfunc.__doc__\n                    else:\n                        func.__doc__ = parfunc.__doc__\n                break\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aliased(aliased_class):\n    original_methods = aliased_class.__dict__.copy()\n    original_methods_set = set(original_methods)\n    for name, method in original_methods.items():\n        aliases = None\n        if isinstance(method, property) and hasattr(method.fget, '_aliases'):\n            aliases = method.fget._aliases\n        elif hasattr(method, '_aliases'):\n            aliases = method._aliases\n\n        if aliases:\n            # Add the aliases for 'method', but don't override any\n            # previously-defined attribute of 'aliased_class'\n            for alias in aliases - original_methods_set:\n                setattr(aliased_class, alias, method)\n    return aliased_class", "response": "A decorator that can be used to define aliased methods in a class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an shortcut (alias) to a decorated function, but not to class methods! Use aliased/alias decorators for class members! Calling the shortcut (alias) will call the decorated function. The shortcut name will be appended to the module's __all__ variable and the shortcut function will inherit the function's docstring Examples -------- In some module you have defined a function >>> @shortcut('is_tmatrix') # doctest: +SKIP >>> def is_transition_matrix(args): # doctest: +SKIP ... pass # doctest: +SKIP Now you are able to call the function under its short name >>> is_tmatrix(args) # doctest: +SKIP", "response": "def shortcut(*names):\n    \"\"\"Add an shortcut (alias) to a decorated function, but not to class methods!\n\n    Use aliased/alias decorators for class members!\n\n    Calling the shortcut (alias) will call the decorated function. The shortcut name will be appended\n    to the module's __all__ variable and the shortcut function will inherit the function's docstring\n\n    Examples\n    --------\n    In some module you have defined a function\n    >>> @shortcut('is_tmatrix') # doctest: +SKIP\n    >>> def is_transition_matrix(args): # doctest: +SKIP\n    ...     pass # doctest: +SKIP\n    Now you are able to call the function under its short name\n    >>> is_tmatrix(args) # doctest: +SKIP\n\n    \"\"\"\n    def wrap(f):\n        globals_ = f.__globals__\n        for name in names:\n            globals_[name] = f\n            if '__all__' in globals_ and name not in globals_['__all__']:\n                globals_['__all__'].append(name)\n        return f\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the filename and line number calling this.", "response": "def get_culprit(omit_top_frames=1):\n    \"\"\"get the filename and line number calling this.\n\n    Parameters\n    ----------\n    omit_top_frames: int, default=1\n        omit n frames from top of stack stack. Purpose is to get the real\n        culprit and not intermediate functions on the stack.\n    Returns\n    -------\n    (filename: str, fileno: int)\n    filename and line number of the culprit.\n    \"\"\"\n    try:\n        caller_stack = stack()[omit_top_frames:]\n        while len(caller_stack) > 0:\n            frame = caller_stack.pop(0)\n            filename = frame[1]\n            # skip callee frames if they are other decorators or this file(func)\n            if '<decorator' in filename or __file__ in filename:\n                continue\n            else:\n                break\n        lineno = frame[2]\n        # avoid cyclic references!\n        del caller_stack, frame\n    except OSError:  # eg. os.getcwd() fails in conda-test, since cwd gets deleted.\n        filename = 'unknown'\n        lineno = -1\n    return filename, lineno"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deprecated(*optional_message):\n    def _deprecated(func, *args, **kw):\n        filename, lineno = get_culprit()\n        user_msg = 'Call to deprecated function \"%s\". Called from %s line %i. %s' \\\n                   % (func.__name__, filename, lineno, msg)\n\n        warnings.warn_explicit(\n            user_msg,\n            category=PyEMMA_DeprecationWarning,\n            filename=filename,\n            lineno=lineno\n        )\n        return func(*args, **kw)\n\n    # add deprecation notice to func docstring:\n    if len(optional_message) == 1 and callable(optional_message[0]):\n        # this is the function itself, decorate!\n        msg = \"\"\n        return decorate(optional_message[0], _deprecated)\n    else:\n        # actually got a message (or empty parenthesis)\n        msg = optional_message[0] if len(optional_message) > 0 else \"\"\n        return decorator(_deprecated)", "response": "This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted when the function is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_umbrella_sampling_data(\n    us_trajs, us_centers, us_force_constants, md_trajs=None, kT=None, width=None):\n    r\"\"\"\n    Wraps umbrella sampling data or a mix of umbrella sampling and and direct molecular dynamics.\n\n    Parameters\n    ----------\n    us_trajs : list of N arrays, each of shape (T_i, d)\n        List of arrays, each having T_i rows, one for each time step, and d columns where d is the\n        dimension in which umbrella sampling was applied. Often d=1, and thus us_trajs will\n        be a list of 1d-arrays.\n    us_centers : array-like of size N\n        List or array of N center positions. Each position must be a d-dimensional vector. For 1d\n        umbrella sampling, one can simply pass a list of centers, e.g. [-5.0, -4.0, -3.0, ... ].\n    us_force_constants : float or array-like of float\n        The force constants used in the umbrellas, unit-less (e.g. kT per length unit). If different\n        force constants were used for different umbrellas, a list or array of N force constants\n        can be given. For multidimensional umbrella sampling, the force matrix must be used.\n    md_trajs : list of M arrays, each of shape (T_i, d), optional, default=None\n        Unbiased molecular dynamics simulations. Format like umbrella_trajs.\n    kT : float (optinal)\n        Use this attribute if the supplied force constants are NOT unit-less.\n    width : array-like of float, optional, default=None\n        Specify periodicity for individual us_traj dimensions. Each positive entry will make the\n        corresponding feature periodic and use the given value as width. None/zero values will be\n        treated as non-periodic.\n\n    Returns\n    -------\n    ttrajs : list of N+M int arrays, each of shape (T_i,)\n        The integers are indexes in 0,...,K-1 enumerating the thermodynamic states the trajectories\n        are in at any time.\n    btrajs : list of N+M float arrays, each of shape (T_i, K)\n        The floats are the reduced bias energies for each thermodynamic state and configuration.\n    umbrella_centers : float array of shape (K, d)\n        The individual umbrella centers labelled accordingly to ttrajs.\n    force_constants : float array of shape (K, d, d)\n        The individual force matrices labelled accordingly to ttrajs.\n    unbiased_state : int or None\n        Index of the unbiased thermodynamic state (if present).\n    \"\"\"\n    ttrajs, umbrella_centers, force_constants, unbiased_state = _get_umbrella_sampling_parameters(\n        us_trajs, us_centers, us_force_constants, md_trajs=md_trajs, kT=kT)\n    if md_trajs is None:\n        md_trajs = []\n    if width is None:\n        width = _np.zeros(shape=(umbrella_centers.shape[1],), dtype=_np.float64)\n    else:\n        width = _np.asarray(\n            map(lambda w: w if w is not None and w > 0.0 else 0.0, width),\n            dtype=_np.float64)\n    if width.shape[0] != umbrella_centers.shape[1]:\n        raise ValueError('Unmatching number of width components.')\n    btrajs = _get_umbrella_bias_sequences(\n        us_trajs + md_trajs, umbrella_centers, force_constants, width)\n    return ttrajs, btrajs, umbrella_centers, force_constants, unbiased_state", "response": "r Returns a list of data structures that can be used to sample the umbrella trajectories for a given set of molecular dynamics."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timescales_msm(dtrajs, lags=None, nits=None, reversible=True, connected=True, weights='empirical',\n                   errors=None, nsamples=50, n_jobs=None, show_progress=True, mincount_connectivity='1/n',\n                   only_timescales=False):\n    # format data\n    r\"\"\" Implied timescales from Markov state models estimated at a series of lag times.\n\n    Parameters\n    ----------\n    dtrajs : array-like or list of array-likes\n        discrete trajectories\n\n    lags : int, array-like with integers or None, optional\n        integer lag times at which the implied timescales will be calculated. If set to None (default)\n        as list of lag times will be automatically generated. For a single int, generate a set of lag times starting\n        from 1 to lags, using a multiplier of 1.5 between successive lags.\n\n    nits : int, optional\n        number of implied timescales to be computed. Will compute less\n        if the number of states are smaller. If None, the number of timescales\n        will be automatically determined.\n\n    reversible : boolean, optional\n        Estimate transition matrix reversibly (True) or nonreversibly (False)\n\n    connected : boolean, optional\n        If true compute the connected set before transition matrix estimation\n        at each lag separately\n\n    weights : str, optional\n        can be used to re-weight non-equilibrium data to equilibrium.\n        Must be one of the following:\n\n        * 'empirical': Each trajectory frame counts as one. (default)\n\n        * 'oom': Each transition is re-weighted using OOM theory, see [5]_.\n\n    errors : None | 'bayes', optional\n        Specifies whether to compute statistical uncertainties (by default\n        not), an which algorithm to use if yes. Currently the only option is:\n\n        * 'bayes' for Bayesian sampling of the posterior\n\n        Attention:\n        * The Bayes mode will use an estimate for the effective count matrix\n          that may produce somewhat different estimates than the\n          'sliding window' estimate used with ``errors=None`` by default.\n        * Computing errors can be// slow if the MSM has many states.\n        * There are still unsolved theoretical problems in the computation\n          of effective count matrices, and therefore the uncertainty interval\n          and the maximum likelihood estimator can be inconsistent. Use this\n          as a rough guess for statistical uncertainties.\n\n    nsamples : int, optional\n        The number of approximately independent transition matrix samples\n        generated for each lag time for uncertainty quantification.\n        Only used if errors is not None.\n\n    n_jobs : int, optional\n        how many subprocesses to start to estimate the models for each lag time.\n\n    show_progress : bool, default=True\n        whether to show progress of estimation.\n\n    mincount_connectivity : float or '1/n'\n        minimum number of counts to consider a connection between two states.\n        Counts lower than that will count zero in the connectivity check and\n        may thus separate the resulting transition matrix. The default\n        evaluates to 1/nstates.\n\n    only_timescales: bool, default=False\n        If you are only interested in the timescales and its samples,\n        you can consider turning this on in order to save memory. This can be\n        useful to avoid blowing up memory with BayesianMSM and lots of samples.\n\n    Returns\n    -------\n    itsobj : :class:`ImpliedTimescales <pyemma.msm.estimators.implied_timescales.ImpliedTimescales>` object\n\n    Example\n    -------\n    >>> from pyemma import msm\n    >>> dtraj = [0,1,1,2,2,2,1,2,2,2,1,0,0,1,1,1,2,2,1,1,2,1,1,0,0,0,1,1,2,2,1]   # mini-trajectory\n    >>> ts = msm.its(dtraj, [1,2,3,4,5], show_progress=False)\n    >>> print(ts.timescales)  # doctest: +ELLIPSIS\n    [[ 1.5...  0.2...]\n     [ 3.1...  1.0...]\n     [ 2.03...  1.02...]\n     [ 4.63...  3.42...]\n     [ 5.13...  2.59...]]\n\n    See also\n    --------\n    ImpliedTimescales\n        The object returned by this function.\n    pyemma.plots.plot_implied_timescales\n        Implied timescales plotting function. Just call it with the\n        :class:`ImpliedTimescales <pyemma.msm.estimators.ImpliedTimescales>`\n        object produced by this function as an argument.\n\n\n    .. autoclass:: pyemma.msm.estimators.implied_timescales.ImpliedTimescales\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.msm.estimators.implied_timescales.ImpliedTimescales\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.msm.estimators.implied_timescales.ImpliedTimescales\n            :attributes:\n\n    References\n    ----------\n    Implied timescales as a lagtime-selection and MSM-validation approach were\n    suggested in [1]_. Error estimation is done either using moving block\n    bootstrapping [2]_ or a Bayesian analysis using Metropolis-Hastings Monte\n    Carlo sampling of the posterior. Nonreversible Bayesian sampling is done\n    by independently sampling Dirichtlet distributions of the transition matrix\n    rows. A Monte Carlo method for sampling reversible MSMs was introduced\n    in [3]_. Here we employ a much more efficient algorithm introduced in [4]_.\n\n    .. [1] Swope, W. C. and J. W. Pitera and F. Suits: Describing protein\n        folding kinetics by molecular dynamics simulations: 1. Theory.\n        J. Phys. Chem. B 108: 6571-6581 (2004)\n    .. [2] Kuensch, H. R.: The jackknife and the bootstrap for general\n        stationary observations. Ann. Stat. 17, 1217-1241 (1989)\n    .. [3] Noe, F.: Probability Distributions of Molecular Observables computed\n        from Markov Models. J. Chem. Phys. 128, 244103 (2008)\n    .. [4] Trendelkamp-Schroer, B, H. Wu, F. Paul and F. Noe:\n        Estimation and uncertainty of reversible Markov models.\n        http://arxiv.org/abs/1507.05990\n    .. [5] Nueske, F., Wu, H., Prinz, J.-H., Wehmeyer, C., Clementi, C. and Noe, F.:\n        Markov State Models from short non-Equilibrium Simulations - Analysis and\n         Correction of Estimation Bias J. Chem. Phys. (submitted) (2017)\n\n    \"\"\"\n    # Catch invalid inputs for weights:\n    if isinstance(weights, str):\n        if weights not in ['empirical', 'oom']:\n            raise ValueError(\"Weights must be either \\'empirical\\' or \\'oom\\'\")\n    else:\n        raise ValueError(\"Weights must be either \\'empirical\\' or \\'oom\\'\")\n    # Set errors to None if weights==oom:\n    if weights == 'oom' and (errors is not None):\n        errors = None\n\n    # format data\n    dtrajs = _types.ensure_dtraj_list(dtrajs)\n\n    if connected:\n        connectivity = 'largest'\n    else:\n        connectivity = 'none'\n\n    # Choose estimator:\n    if errors is None:\n        if weights == 'empirical':\n            estimator = _ML_MSM(reversible=reversible, connectivity=connectivity)\n        else:\n            estimator = _OOM_MSM(reversible=reversible, connectivity=connectivity)\n    elif errors == 'bayes':\n        estimator = _Bayes_MSM(reversible=reversible, connectivity=connectivity,\n                               nsamples=nsamples, show_progress=show_progress)\n    else:\n        raise NotImplementedError('Error estimation method {errors} currently not implemented'.format(errors=errors))\n\n    if hasattr(estimator, 'mincount_connectivity'):\n        estimator.mincount_connectivity = mincount_connectivity\n    # go\n    itsobj = _ImpliedTimescales(estimator, lags=lags, nits=nits, n_jobs=n_jobs,\n                                show_progress=show_progress, only_timescales=only_timescales)\n    itsobj.estimate(dtrajs)\n    return itsobj", "response": "r Returns a list of timescales for the given Markov state models."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bayesian_markov_model(dtrajs, lag, reversible=True, statdist=None,\n                          sparse=False, connectivity='largest',\n                          count_mode='effective',\n                          nsamples=100, conf=0.95, dt_traj='1 step',\n                          show_progress=True, mincount_connectivity='1/n'):\n    r\"\"\" Bayesian Markov model estimate using Gibbs sampling of the posterior\n\n    Returns a :class:`BayesianMSM` that contains the\n    estimated transition matrix and allows to compute a large number of\n    quantities related to Markov models as well as their statistical\n    uncertainties.\n\n    Parameters\n    ----------\n    dtrajs : list containing ndarrays(dtype=int) or ndarray(n, dtype=int)\n        discrete trajectories, stored as integer ndarrays (arbitrary size)\n        or a single ndarray for only one trajectory.\n    lag : int\n        lagtime for the MSM estimation in multiples of trajectory steps\n    reversible : bool, optional, default = True\n        If true compute reversible MSM, else non-reversible MSM\n    sparse : bool, optional, default = False\n        If true compute count matrix, transition matrix and all derived\n        quantities using sparse matrix algebra. In this case python sparse\n        matrices will be returned by the corresponding functions instead of\n        numpy arrays. This behavior is suggested for very large numbers of\n        states (e.g. > 4000) because it is likely to be much more efficient.\n    statdist : (M,) ndarray, optional\n        Stationary vector on the full state-space. Transition matrix\n        will be estimated such that statdist is its equilibrium\n        distribution.\n    count_mode : str, optional, default='sliding'\n        mode to obtain count matrices from discrete trajectories. Should be\n        one of:\n\n        * 'sliding' : A trajectory of length T will have :math:`T-tau` counts\n          at time indexes\n\n              .. math::\n\n                 (0 \\rightarrow \\tau), (1 \\rightarrow \\tau+1), ..., (T-\\tau-1 \\rightarrow T-1)\n\n        * 'effective' : Uses an estimate of the transition counts that are\n          statistically uncorrelated. Recommended when used with a\n          Bayesian MSM.\n        * 'sample' : A trajectory of length T will have :math:`T/tau` counts\n          at time indexes\n\n              .. math::\n\n                    (0 \\rightarrow \\tau), (\\tau \\rightarrow 2 \\tau), ..., (((T/tau)-1) \\tau \\rightarrow T)\n    connectivity : str, optional, default = None\n        Defines if the resulting HMM will be defined on all hidden states or on\n        a connected subset. Connectivity is defined by counting only\n        transitions with at least mincount_connectivity counts.\n        If a subset of states is used, all estimated quantities (transition\n        matrix, stationary distribution, etc) are only defined on this subset\n        and are correspondingly smaller than nstates.\n        Following modes are available:\n        * None or 'all' : The active set is the full set of states.\n          Estimation is done on all weakly connected subsets separately. The\n          resulting transition matrix may be disconnected.\n        * 'largest' : The active set is the largest reversibly connected set.\n        * 'populous' : The active set is the reversibly connected set with\n           most counts.\n    nsample : int, optional, default=100\n        number of transition matrix samples to compute and store\n    conf : float, optional, default=0.95\n        size of confidence intervals\n    dt_traj : str, optional, default='1 step'\n        Description of the physical time corresponding to the trajectory time\n        step. May be used by analysis algorithms such as plotting tools to\n        pretty-print the axes. By default '1 step', i.e.  there is no physical\n        time unit. Specify by a number, whitespace and unit. Permitted units\n        are (* is an arbitrary string):\n\n        |  'fs',  'femtosecond*'\n        |  'ps',  'picosecond*'\n        |  'ns',  'nanosecond*'\n        |  'us',  'microsecond*'\n        |  'ms',  'millisecond*'\n        |  's',   'second*'\n    show_progress : bool, default=True\n        Show progressbars for calculation\n\n    mincount_connectivity : float or '1/n'\n        minimum number of counts to consider a connection between two states.\n        Counts lower than that will count zero in the connectivity check and\n        may thus separate the resulting transition matrix. The default\n        evaluates to 1/nstates.\n\n    Returns\n    -------\n    An :class:`BayesianMSM` object containing the Bayesian MSM estimator\n    and the model.\n\n    Example\n    -------\n    Note that the following example is only qualitatively and not\n    quantitatively reproducible because it involves random numbers.\n\n    We build a Bayesian Markov model for the following two trajectories at lag\n    time 2:\n\n    >>> from pyemma import msm\n    >>> dtrajs = [[0,1,2,2,2,2,1,2,2,2,1,0,0,0,0,0,0,0], [0,0,0,0,1,1,2,2,2,2,2,2,2,1,0,0]]\n    >>> mm = msm.bayesian_markov_model(dtrajs, 2, show_progress=False)\n\n    The resulting Model is an MSM just like you get with estimate_markov_model\n    Its transition matrix does also come from a maximum likelihood estimation,\n    but it's slightly different from the estimate_markov_mode result because\n    bayesian_markov_model uses an effective count matrix with statistically\n    uncorrelated counts:\n\n    >>> print(mm.transition_matrix)  # doctest: +SKIP\n    [[ 0.70000001  0.16463699  0.135363  ]\n     [ 0.38169055  0.          0.61830945]\n     [ 0.12023989  0.23690297  0.64285714]]\n\n    However bayesian_markov_model returns a SampledMSM object which is able to\n    compute the probability distribution and statistical models of all methods\n    that are offered by the MSM object. This works as follows. You can ask for\n    the sample mean and specify the method you wanna evaluate as a string:\n\n    >>> print(mm.sample_mean('transition_matrix'))  # doctest: +SKIP\n    [[ 0.71108663  0.15947371  0.12943966]\n     [ 0.41076105  0.          0.58923895]\n     [ 0.13079372  0.23005443  0.63915185]]\n\n    Likewise, the standard deviation by element:\n\n    >>> print(mm.sample_std('transition_matrix'))  # doctest: +SKIP\n    [[ 0.13707029  0.09479627  0.09200214]\n     [ 0.15247454  0.          0.15247454]\n     [ 0.07701315  0.09385258  0.1119089 ]]\n\n    And this is the 95% (2 sigma) confidence interval. You can control the\n    percentile using the conf argument in this function:\n\n    >>> L, R = mm.sample_conf('transition_matrix')\n    >>> print(L) # doctest: +SKIP\n    >>> print(R)  # doctest: +SKIP\n    [[ 0.44083423  0.03926518  0.0242113 ]\n     [ 0.14102544  0.          0.30729828]\n     [ 0.02440188  0.07629456  0.43682481]]\n    [[ 0.93571706  0.37522581  0.40180041]\n     [ 0.69307665  0.          0.8649215 ]\n     [ 0.31029752  0.44035732  0.85994006]]\n\n    If you wanna compute expectations of functions that require arguments,\n    just pass these arguments as well:\n\n    >>> print(mm.sample_std('mfpt', 0, 2)) # doctest: +SKIP\n    12.9049811296\n\n    And if you want to histogram the distribution or compute more complex\n    statistical moment such as the covariance between different quantities,\n    just get the full sample of your quantity of interest and evaluate it\n    at will:\n\n    >>> samples = mm.sample_f('mfpt', 0, 2)\n    >>> print(samples[:4]) # doctest: +SKIP\n    [7.9763615793248155, 8.6540958274695701, 26.295326015231058, 17.909895469938899]\n\n    Internally, the SampledMSM object has 100 transition matrices (the number\n    can be controlled by nsamples), that were computed by the transition matrix\n    sampling method. All of the above sample functions iterate over these 100\n    transition matrices and evaluate the requested function with the given\n    parameters on each of them.\n\n\n    .. autoclass:: pyemma.msm.estimators.bayesian_msm.BayesianMSM\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.msm.estimators.bayesian_msm.BayesianMSM\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.msm.estimators.bayesian_msm.BayesianMSM\n            :attributes:\n\n    References\n    ----------\n    .. [1] Trendelkamp-Schroer, B, H. Wu, F. Paul and F. Noe:\n        Estimation and uncertainty of reversible Markov models.\n        http://arxiv.org/abs/1507.05990\n\n    \"\"\"\n    # TODO: store_data=True\n    bmsm_estimator = _Bayes_MSM(lag=lag, reversible=reversible, statdist_constraint=statdist,\n                                count_mode=count_mode, sparse=sparse, connectivity=connectivity,\n                                dt_traj=dt_traj, nsamples=nsamples, conf=conf, show_progress=show_progress,\n                                mincount_connectivity=mincount_connectivity)\n    return bmsm_estimator.estimate(dtrajs)", "response": "r Estimates the Bayesian Markov model for a set of discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if x is numerically 0 or an array with 0 s.", "response": "def _is_zero(x):\n    \"\"\" Returns True if x is numerically 0 or an array with 0's. \"\"\"\n    if x is None:\n        return True\n    if isinstance(x, numbers.Number):\n        return x == 0.0\n    if isinstance(x, np.ndarray):\n        return np.all(x == 0)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sparsify(X, remove_mean=False, modify_data=False, sparse_mode='auto', sparse_tol=0.0):\n    if sparse_mode.lower() == 'sparse':\n        min_const_col_number = 0  # enforce sparsity. A single constant column will lead to sparse treatment\n    elif sparse_mode.lower() == 'dense':\n        min_const_col_number = X.shape[1] + 1  # never use sparsity\n    else:\n        if remove_mean and not modify_data:  # in this case we have to copy the data anyway, and can be permissive\n            min_const_col_number = max(0.1 * X.shape[1], 50)\n        else:\n            # This is a rough heuristic to choose a minimum column number for which sparsity may pay off.\n            # This heuristic is good for large number of samples, i.e. it may be inadequate for small matrices X.\n            if X.shape[1] < 250:\n                min_const_col_number = X.shape[1] - 0.25 * X.shape[1]\n            elif X.shape[1] < 1000:\n                min_const_col_number = X.shape[1] - (0.5 * X.shape[1] - 100)\n            else:\n                min_const_col_number = X.shape[1] - (0.8 * X.shape[1] - 400)\n        # ensure we have an integer again.\n        min_const_col_number = int(min_const_col_number)\n\n    if X.shape[1] > min_const_col_number:\n        mask = covartools.variable_cols(X, tol=sparse_tol, min_constant=min_const_col_number)  # bool vector\n        nconst = len(np.where(~mask)[0])\n        if nconst > min_const_col_number:\n            xconst = X[0, ~mask]\n            X = X[:, mask]  # sparsify\n        else:\n            xconst = None\n            mask = None\n    else:\n        xconst = None\n        mask = None\n\n    return X, mask, xconst", "response": "Determines the sparsity of X and returns a selected sub - matrix of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_convert(X, const=None, remove_mean=False, copy=True):\n    # determine type\n    dtype = np.float64  # default: convert to float64 in order to avoid cancellation errors\n    if X.dtype.kind == 'b' and X.shape[0] < 2**23 and not remove_mean:\n        dtype = np.float32  # convert to float32 if we can represent all numbers\n    # copy/convert if needed\n    if X.dtype not in (np.float64, dtype):  # leave as float64 (conversion is expensive), otherwise convert to dtype\n        X = X.astype(dtype, order='C')\n        if const is not None:\n            const = const.astype(dtype, order='C')\n    elif copy:\n        X = X.copy(order='C')\n        if const is not None:\n            const = const.copy(order='C')\n\n    return X, const", "response": "r Copies the data and converts the data type if unsuitable for covariance computations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sum(X, xmask=None, xconst=None, Y=None, ymask=None, yconst=None, symmetric=False, remove_mean=False,\n         weights=None):\n    r\"\"\" Computes the column sums and centered column sums.\n\n    If symmetric = False, the sums will be determined as\n    .. math:\n        sx &=& \\frac{1}{2} \\sum_t x_t\n        sy &=& \\frac{1}{2} \\sum_t y_t\n\n    If symmetric, the sums will be determined as\n\n    .. math:\n        sx = sy = \\frac{1}{2T} \\sum_t x_t + y_t\n\n    Returns\n    -------\n    w : float\n        statistical weight of sx, sy\n    sx : ndarray\n        effective row sum of X (including symmetrization if requested)\n    sx_raw_centered : ndarray\n        centered raw row sum of X\n\n    optional returns (only if Y is given):\n\n    sy : ndarray\n        effective row sum of X (including symmetrization if requested)\n    sy_raw_centered : ndarray\n        centered raw row sum of Y\n\n    \"\"\"\n    T = X.shape[0]\n    # Check if weights are given:\n    if weights is not None:\n        X = weights[:, None] * X\n        if Y is not None:\n            Y = weights[:, None] * Y\n    # compute raw sums on variable data\n    sx_raw = X.sum(axis=0)  # this is the mean before subtracting it.\n    sy_raw = 0\n    if Y is not None:\n        sy_raw = Y.sum(axis=0)\n\n    # expand raw sums to full data\n    if xmask is not None:\n        if weights is not None:\n            sx_raw = _sum_sparse(sx_raw, xmask, xconst, weights.sum())\n        else:\n            sx_raw = _sum_sparse(sx_raw, xmask, xconst, T)\n    if ymask is not None:\n        if weights is not None:\n            sy_raw = _sum_sparse(sy_raw, ymask, yconst, weights.sum())\n        else:\n            sy_raw = _sum_sparse(sy_raw, ymask, yconst, T)\n\n    # compute effective sums and centered sums\n    if Y is not None and symmetric:\n        sx = sx_raw + sy_raw\n        sy = sx\n        if weights is not None:\n            w = 2*np.sum(weights)\n        else:\n            w = 2 * T\n    else:\n        sx = sx_raw\n        sy = sy_raw\n        if weights is not None:\n            w = np.sum(weights)\n        else:\n            w = T\n\n    sx_raw_centered = sx_raw.copy()\n    if Y is not None:\n        sy_raw_centered = sy_raw.copy()\n\n    # center mean.\n    if remove_mean:\n        if Y is not None and symmetric:\n            sx_raw_centered -= 0.5 * sx\n            sy_raw_centered -= 0.5 * sy\n        else:\n            sx_raw_centered = np.zeros(sx.size)\n            if Y is not None:\n                sy_raw_centered = np.zeros(sy.size)\n\n    # return\n    if Y is not None:\n        return w, sx, sx_raw_centered, sy, sy_raw_centered\n    else:\n        return w, sx, sx_raw_centered", "response": "r Computes the column sums and centered column sums."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _center(X, w, s, mask=None, const=None, inplace=True):\n    xmean = s / float(w)\n    if mask is None:\n        X = np.subtract(X, xmean, out=X if inplace else None)\n    else:\n        X = np.subtract(X, xmean[mask], out=X if inplace else None)\n        const = np.subtract(const, xmean[~mask], const if inplace else None)\n\n    return X, const", "response": "Centers the data.\n\n    Parameters\n    ----------\n    w : float\n        statistical weight of s\n    inplace : bool\n        center in place\n\n    Returns\n    -------\n    sx : ndarray\n        uncentered row sum of X\n    sx_centered : ndarray\n        row sum of X after centering\n\n    optional returns (only if Y is given):\n\n    sy_raw : ndarray\n        uncentered row sum of Y\n    sy_centered : ndarray\n        row sum of Y after centering"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_variable_indices(mask, column_selection):\n    a = np.where(mask)[0]\n    b = column_selection[np.in1d(column_selection, a)]\n    return np.searchsorted(a, b)", "response": "Returns the indices of the variable columns that are not in the given mask and column_selection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _M2_sparse_sym(Xvar, mask_X, Yvar, mask_Y, weights=None, column_selection=None):\n    assert len(mask_X) == len(mask_Y), 'X and Y need to have equal sizes for symmetrization'\n\n    if column_selection is None:\n        mask_Xk = mask_X\n        mask_Yk = mask_Y\n        Xvark = Xvar\n        Yvark = Yvar\n    else:\n        mask_Xk = mask_X[column_selection]\n        mask_Yk = mask_Y[column_selection]\n        Xvark = Xvar[:, _filter_variable_indices(mask_X, column_selection)]\n        Yvark = Yvar[:, _filter_variable_indices(mask_Y, column_selection)]\n\n    Cxxyy = np.zeros((len(mask_X), len(mask_Yk)))\n    Cxxyy[np.ix_(mask_X, mask_Xk)] = _M2_dense(Xvar, Xvark, weights=weights)\n    Cxxyy[np.ix_(mask_Y, mask_Yk)] += _M2_dense(Yvar, Yvark, weights=weights)\n\n    Cxyyx = np.zeros((len(mask_X), len(mask_Yk)))\n    Cxy = _M2_dense(Xvar, Yvark, weights=weights)\n    Cyx = _M2_dense(Yvar, Xvark, weights=weights)\n    Cxyyx[np.ix_(mask_X, mask_Yk)] = Cxy\n    Cxyyx[np.ix_(mask_Y, mask_Xk)] += Cyx\n\n    return Cxxyy, Cxyyx", "response": "2nd self - symmetric moment matrix exploiting zero input columns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _M2(Xvar, Yvar, mask_X=None, mask_Y=None, xsum=0, xconst=0, ysum=0, yconst=0, weights=None, diag_only=False):\n    if mask_X is None and mask_Y is None:\n        return _M2_dense(Xvar, Yvar, weights=weights, diag_only=diag_only)\n    else:\n        # Check if one of the masks is not None, modify it and also adjust the constant columns:\n        if mask_X is None:\n            mask_X = np.ones(Xvar.shape[1], dtype=np.bool)\n            xconst = np.ones(0, dtype=float)\n        if mask_Y is None:\n            mask_Y = np.ones(Yvar.shape[1], dtype=np.bool)\n            yconst = np.ones(0, dtype=float)\n    if _is_zero(xsum) and _is_zero(ysum) or _is_zero(xconst) and _is_zero(yconst):\n        return _M2_sparse(Xvar, mask_X, Yvar, mask_Y, weights=weights)\n    else:\n        return _M2_const(Xvar, mask_X, xsum[mask_X], xconst, Yvar, mask_Y, ysum[mask_Y], yconst, weights=weights)", "response": "direct ( nonsymmetric ) second moment matrix. Decide if we need dense sparse const"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _M2_symmetric(Xvar, Yvar, mask_X=None, mask_Y=None, xsum=0, xconst=0, ysum=0, yconst=0, weights=None,\n                  column_selection=None, diag_only=False):\n    \"\"\" symmetric second moment matrices. Decide if we need dense, sparse, const\"\"\"\n    if mask_X is None and mask_Y is None:\n        if column_selection is None:\n            Xvark = Xvar\n            Yvark = Yvar\n        else:\n            Xvark = Xvar[:, column_selection]\n            Yvark = Yvar[:, column_selection]\n        Cxxyy = _M2_dense(Xvar, Xvark, weights=weights, diag_only=diag_only) \\\n                + _M2_dense(Yvar, Yvark, weights=weights, diag_only=diag_only)\n        Cxy = _M2_dense(Xvar, Yvark, weights=weights, diag_only=diag_only)\n        Cyx = _M2_dense(Yvar, Xvark, weights=weights, diag_only=diag_only)\n        Cxyyx = Cxy + Cyx\n    else:\n        # Check if one of the masks is not None, modify it and also adjust the constant columns:\n        if mask_X is None:\n            mask_X = np.ones(Xvar.shape[1], dtype=np.bool)\n            xconst = np.ones(0, dtype=float)\n        if mask_Y is None:\n            mask_Y = np.ones(Yvar.shape[1], dtype=np.bool)\n            yconst = np.ones(0, dtype=float)\n        if _is_zero(xsum) and _is_zero(ysum) or _is_zero(xconst) and _is_zero(yconst):\n            Cxxyy, Cxyyx = _M2_sparse_sym(Xvar, mask_X, Yvar, mask_Y, weights=weights, column_selection=column_selection)\n        else:\n            xvarsum = xsum[mask_X]  # to variable part\n            yvarsum = ysum[mask_Y]  # to variable part\n            if column_selection is None:\n                Xvark = Xvar\n                mask_Xk = mask_X\n                xkvarsum = xvarsum\n                xkconst = xconst\n                Yvark = Yvar\n                mask_Yk = mask_Y\n                ykvarsum = yvarsum\n                ykconst = yconst\n            else:\n                Xvark = Xvar[:, _filter_variable_indices(mask_X, column_selection)]\n                mask_Xk = mask_X[column_selection]\n                xksum = xsum[column_selection]\n                xkvarsum = xksum[mask_Xk]\n                xkconst = xconst[_filter_variable_indices(~mask_X, column_selection)]\n                Yvark = Yvar[:, _filter_variable_indices(mask_Y, column_selection)]\n                mask_Yk = mask_Y[column_selection]\n                yksum = ysum[column_selection]\n                ykvarsum = yksum[mask_Yk]\n                ykconst = yconst[_filter_variable_indices(~mask_Y, column_selection)]\n            Cxxyy = _M2_const(Xvar, mask_X, xvarsum, xconst, Xvark, mask_Xk, xkvarsum, xkconst, weights=weights) \\\n                    + _M2_const(Yvar, mask_Y, yvarsum, yconst, Yvark, mask_Yk, ykvarsum, ykconst, weights=weights)\n            Cxy = _M2_const(Xvar, mask_X, xvarsum, xconst, Yvark, mask_Yk, ykvarsum, ykconst, weights=weights)\n            Cyx = _M2_const(Yvar, mask_Y, yvarsum, yconst, Xvark, mask_Xk, xkvarsum, xkconst, weights=weights)\n            Cxyyx = Cxy + Cyx\n    return Cxxyy, Cxyyx", "response": "symmetric second moment matrices. Decide if we need dense sparse const"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef moments_XX(X, remove_mean=False, modify_data=False, weights=None, sparse_mode='auto', sparse_tol=0.0,\n               column_selection=None, diag_only=False):\n    r\"\"\" Computes the first two unnormalized moments of X\n\n    Computes :math:`s = \\sum_t x_t` and :math:`C = X^\\top X` while exploiting\n    zero or constant columns in the data matrix.\n\n    Parameters\n    ----------\n    X : ndarray (T, M)\n        Data matrix\n    remove_mean : bool\n        True: remove column mean from the data, False: don't remove mean.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the data matrix X,\n        without creating an independent copy. This option is faster but might\n        lead to surprises because your input array is changed.\n    weights: None or ndarray(T, )\n        weights assigned to each trajectory point. If None, all data points have weight one.\n        If ndarray, each data point is assigned a separate weight.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    sparse_tol: float\n        Threshold for considering column to be zero in order to save computing\n        effort when the data is sparse or almost sparse.\n        If max(abs(X[:, i])) < sparse_tol, then row i (and also column i if Y\n        is not given) of the covariance matrix will be set to zero. If Y is\n        given and max(abs(Y[:, i])) < sparse_tol, then column i of the\n        covariance matrix will be set to zero.\n    column_selection: ndarray(k, dtype=int) or None\n        Indices of those columns that are to be computed. If None, all columns are computed.\n    diag_only: bool\n        If True, the computation is restricted to the diagonal entries (autocorrelations) only.\n\n    Returns\n    -------\n    w : float\n        statistical weight\n    s : ndarray (M)\n        sum\n    C : ndarray (M, M)\n        unnormalized covariance matrix\n\n    \"\"\"\n    # Check consistency of inputs:\n    if weights is not None:\n        assert X.shape[0] == weights.shape[0], 'X and weights_x must have equal length'\n    # diag_only is only implemented for dense mode\n    if diag_only and sparse_mode is not 'dense':\n        if sparse_mode is 'sparse':\n            import warnings\n            warnings.warn('Computing diagonal entries only is not implemented for sparse mode. Switching to dense mode.')\n        sparse_mode = 'dense'\n    # sparsify\n    X0, mask_X, xconst = _sparsify(X, remove_mean=remove_mean, modify_data=modify_data,\n                                   sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    is_sparse = mask_X is not None\n    # copy / convert\n    # TODO: do we need to copy xconst?\n    X0, xconst = _copy_convert(X0, const=xconst, remove_mean=remove_mean,\n                               copy=is_sparse or (remove_mean and not modify_data))\n    # sum / center\n    w, sx, sx0_centered = _sum(X0, xmask=mask_X, xconst=xconst, symmetric=False, remove_mean=remove_mean,\n                               weights=weights)\n    if remove_mean:\n        _center(X0, w, sx, mask=mask_X, const=xconst, inplace=True)  # fast in-place centering\n    # TODO: we could make a second const check here. If after summation not enough zeros have appeared in the\n    # TODO: consts, we switch back to dense treatment here.\n    # compute covariance matrix\n    if column_selection is not None:\n        if is_sparse:\n            Xk = X[:, column_selection]\n            mask_Xk = mask_X[column_selection]\n            X0k = Xk[:, mask_Xk]\n            xksum = sx0_centered[column_selection]\n            xkconst = Xk[0, ~mask_Xk]\n            X0k, xkconst = _copy_convert(X0k, const=xkconst, remove_mean=remove_mean,\n                                         copy=True)\n            C = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_Xk, xsum=sx0_centered, xconst=xconst, ysum=xksum, yconst=xkconst,\n                    weights=weights)\n        else:\n            X0k = X0[:, column_selection]\n            C = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_X, xsum=sx0_centered, xconst=xconst,\n                    ysum=sx0_centered[column_selection], yconst=xconst, weights=weights)\n    else:\n        C = _M2(X0, X0, mask_X=mask_X, mask_Y=mask_X, xsum=sx0_centered, xconst=xconst, ysum=sx0_centered, yconst=xconst,\n                weights=weights, diag_only=diag_only)\n    return w, sx, C", "response": "r Computes the first two unnormalized moments of X."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef moments_XXXY(X, Y, remove_mean=False, symmetrize=False, weights=None,\n                 modify_data=False, sparse_mode='auto', sparse_tol=0.0,\n                 column_selection=None, diag_only=False):\n    \"\"\" Computes the first two unnormalized moments of X and Y\n\n    If symmetrize is False, computes\n\n    .. math:\n        s_x  &=& \\sum_t x_t\n        s_y  &=& \\sum_t y_t\n        C_XX &=& X^\\top X\n        C_XY &=& X^\\top Y\n\n    If symmetrize is True, computes\n\n    .. math:\n        s_x = s_y &=& \\frac{1}{2} \\sum_t(x_t + y_t)\n        C_XX      &=& \\frac{1}{2} (X^\\top X + Y^\\top Y)\n        C_XY      &=& \\frac{1}{2} (X^\\top Y + Y^\\top X)\n\n    while exploiting zero or constant columns in the data matrix.\n\n    Parameters\n    ----------\n    X : ndarray (T, M)\n        Data matrix\n    Y : ndarray (T, N)\n        Second data matrix\n    remove_mean : bool\n        True: remove column mean from the data, False: don't remove mean.\n    symmetrize : bool\n        Computes symmetrized means and moments (see above)\n    weights : None or ndarray(T, )\n        weights assigned to each trajectory point of X. If None, all data points have weight one.\n        If ndarray, each data point is assigned a separate weight.\n    time_lagged : bool,\n        indicates that Y is a time-lagged version of X.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the data matrix X,\n        without creating an independent copy. This option is faster but might\n        lead to surprises because your input array is changed.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    sparse_tol: float\n        Threshold for considering column to be zero in order to save computing\n        effort when the data is sparse or almost sparse.\n        If max(abs(X[:, i])) < sparse_tol, then row i (and also column i if Y\n        is not given) of the covariance matrix will be set to zero. If Y is\n        given and max(abs(Y[:, i])) < sparse_tol, then column i of the\n        covariance matrix will be set to zero.\n    column_selection: ndarray(k, dtype=int) or None\n        Indices of those columns that are to be computed. If None, all columns are computed.\n    diag_only: bool\n        If True, the computation is restricted to the diagonal entries (autocorrelations) only.\n\n    Returns\n    -------\n    w : float\n        statistical weight\n    s_x : ndarray (M)\n        x-sum\n    s_y : ndarray (N)\n        y-sum\n    C_XX : ndarray (M, M)\n        unnormalized covariance matrix of X\n    C_XY : ndarray (M, N)\n        unnormalized covariance matrix of XY\n\n    \"\"\"\n    # Check consistency of inputs:\n    if Y is not None:\n        assert Y.shape[0] == X.shape[0], 'X and Y must have equal length.'\n    if weights is not None:\n        assert X.shape[0] == weights.shape[0], 'X and weights_x must have equal length'\n    # diag_only is only implemented for dense mode\n    if diag_only and sparse_mode is not 'dense':\n        if sparse_mode is 'sparse':\n            import warnings\n            warnings.warn('Computing diagonal entries only is not implemented for sparse mode. Switching to dense mode.')\n        sparse_mode = 'dense'\n    if diag_only and X.shape[1] != Y.shape[1]:\n        raise ValueError('Computing diagonal entries only does not make sense for rectangular covariance matrix.')\n    # sparsify\n    X0, mask_X, xconst, Y0, mask_Y, yconst = _sparsify_pair(X, Y, remove_mean=remove_mean, modify_data=modify_data,\n                                                            symmetrize=symmetrize, sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    is_sparse = mask_X is not None and mask_Y is not None\n    # copy / convert\n    copy = is_sparse or (remove_mean and not modify_data)\n    X0, xconst = _copy_convert(X0, const=xconst, remove_mean=remove_mean, copy=copy)\n    Y0, yconst = _copy_convert(Y0, const=yconst, remove_mean=remove_mean, copy=copy)\n    # sum / center\n    w, sx, sx_centered, sy, sy_centered = _sum(X0, xmask=mask_X, xconst=xconst, Y=Y0, ymask=mask_Y, yconst=yconst,\n                                               symmetric=symmetrize, remove_mean=remove_mean, weights=weights)\n    if remove_mean:\n        _center(X0, w, sx, mask=mask_X, const=xconst, inplace=True)  # fast in-place centering\n        _center(Y0, w, sy, mask=mask_Y, const=yconst, inplace=True)  # fast in-place centering\n\n    if symmetrize:\n        Cxx, Cxy = _M2_symmetric(X0, Y0, mask_X=mask_X, mask_Y=mask_Y,\n                                 xsum=sx_centered, xconst=xconst, ysum=sy_centered, yconst=yconst, weights=weights,\n                                 column_selection=column_selection, diag_only=diag_only)\n    else:\n        if column_selection is not None:\n            if is_sparse:\n                Xk = X[:, column_selection]\n                mask_Xk = mask_X[column_selection]\n                X0k = Xk[:, mask_Xk]\n                xksum = sx_centered[column_selection]\n                xkconst = Xk[0, ~mask_Xk]\n                X0k, xkconst = _copy_convert(X0k, const=xkconst, remove_mean=remove_mean,\n                                             copy=True)\n\n                Yk = Y[:, column_selection]\n                mask_Yk = mask_Y[column_selection]\n                Y0k = Yk[:, mask_Yk]\n                yksum = sy_centered[column_selection]\n                ykconst = Yk[0, ~mask_Yk]\n                Y0k, ykconst = _copy_convert(Y0k, const=ykconst, remove_mean=remove_mean,\n                                             copy=True)\n\n                Cxx = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_Xk, xsum=sx_centered, xconst=xconst, ysum=xksum, yconst=xkconst,\n                        weights=weights)\n                Cxy = _M2(X0, Y0k, mask_X=mask_X, mask_Y=mask_Yk, xsum=sx_centered, xconst=xconst, ysum=yksum, yconst=ykconst,\n                        weights=weights)\n            else:\n                X0k = X0[:, column_selection]\n                Y0k = Y0[:, column_selection]\n                Cxx = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_X, xsum=sx_centered, xconst=xconst,\n                        ysum=sx_centered[column_selection], yconst=xconst, weights=weights)\n                Cxy = _M2(X0, Y0k, mask_X=mask_X, mask_Y=mask_Y, xsum=sx_centered, xconst=xconst,\n                        ysum=sy_centered[column_selection], yconst=yconst, weights=weights)\n        else:\n            Cxx = _M2(X0, X0, mask_X=mask_X, mask_Y=mask_X, xsum=sx_centered, xconst=xconst, ysum=sx_centered, yconst=xconst,\n                      weights=weights, diag_only=diag_only)\n            Cxy = _M2(X0, Y0, mask_X=mask_X, mask_Y=mask_Y, xsum=sx_centered, xconst=xconst, ysum=sy_centered, yconst=yconst,\n                      weights=weights, diag_only=diag_only)\n\n    return w, sx, sy, Cxx, Cxy", "response": "Computes the first two unnormalized moments of X and Y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the first two unnormalized moments of X and Y.", "response": "def moments_block(X, Y, remove_mean=False, modify_data=False,\n                  sparse_mode='auto', sparse_tol=0.0,\n                  column_selection=None, diag_only=False):\n    \"\"\" Computes the first two unnormalized moments of X and Y\n\n    Computes\n\n    .. math:\n        s_x  &=& \\sum_t x_t\n        s_y  &=& \\sum_t y_t\n        C_XX &=& X^\\top X\n        C_XY &=& X^\\top Y\n        C_YX &=& Y^\\top X\n        C_YY &=& Y^\\top Y\n\n    while exploiting zero or constant columns in the data matrix.\n\n    Parameters\n    ----------\n    X : ndarray (T, M)\n        Data matrix\n    Y : ndarray (T, N)\n        Second data matrix\n    remove_mean : bool\n        True: remove column mean from the data, False: don't remove mean.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the data matrix X,\n        without creating an independent copy. This option is faster but might\n        lead to surprises because your input array is changed.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    sparse_tol: float\n        Threshold for considering column to be zero in order to save computing\n        effort when the data is sparse or almost sparse.\n        If max(abs(X[:, i])) < sparse_tol, then row i (and also column i if Y\n        is not given) of the covariance matrix will be set to zero. If Y is\n        given and max(abs(Y[:, i])) < sparse_tol, then column i of the\n        covariance matrix will be set to zero.\n    column_selection: ndarray(k, dtype=int) or None\n        Indices of those columns that are to be computed. If None, all columns are computed.\n    diag_only: bool\n        If True, the computation is restricted to the diagonal entries (autocorrelations) only.\n\n    Returns\n    -------\n    w : float\n        statistical weight of this estimation\n    s : [ndarray (M), ndarray (M)]\n        list of two elements with s[0]=sx and s[1]=sy\n    C : [[ndarray(M,M), ndarray(M,N)], [ndarray(N,M),ndarray(N,N)]]\n        list of two lists with two elements.\n        C[0,0] = Cxx, C[0,1] = Cxy, C[1,0] = Cyx, C[1,1] = Cyy\n\n    \"\"\"\n    # diag_only is only implemented for dense mode\n    if diag_only and sparse_mode is not 'dense':\n        if sparse_mode is 'sparse':\n            import warnings\n            warnings.warn('Computing diagonal entries only is not implemented for sparse mode. Switching to dense mode.')\n        sparse_mode = 'dense'\n    # sparsify\n    X0, mask_X, xconst = _sparsify(X, sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    Y0, mask_Y, yconst = _sparsify(Y, sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    is_sparse = mask_X is not None and mask_Y is not None\n    # copy / convert\n    copy = is_sparse or (remove_mean and not modify_data)\n    X0, xconst = _copy_convert(X0, const=xconst, copy=copy)\n    Y0, yconst = _copy_convert(Y0, const=yconst, copy=copy)\n    # sum / center\n    w, sx, sx_centered, sy, sy_centered = _sum(X0, xmask=mask_X, xconst=xconst, Y=Y0, ymask=mask_Y, yconst=yconst,\n                                               symmetric=False, remove_mean=remove_mean)\n    if remove_mean:\n        _center(X0, w, sx, mask=mask_X, const=xconst, inplace=True)  # fast in-place centering\n        _center(Y0, w, sy, mask=mask_Y, const=yconst, inplace=True)  # fast in-place centering\n\n    if column_selection is not None:\n        if is_sparse:\n            Xk = X[:, column_selection]\n            mask_Xk = mask_X[column_selection] if mask_X is not None else mask_X\n            X0k = Xk[:, mask_Xk]\n            xksum = sx_centered[column_selection]\n            xkconst = Xk[0, ~mask_Xk]\n            X0k, xkconst = _copy_convert(X0k, const=xkconst, remove_mean=remove_mean,\n                                         copy=True)\n\n            Yk = Y[:, column_selection]\n            mask_Yk = mask_Y[column_selection] if mask_Y is not None else mask_Y\n            Y0k = Yk[:, mask_Yk]\n            yksum = sy_centered[column_selection]\n            ykconst = Yk[0, ~mask_Yk]\n            Y0k, ykconst = _copy_convert(Y0k, const=ykconst, remove_mean=remove_mean,\n                                         copy=True)\n\n            Cxx = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_Xk,\n                      xsum=sx_centered, xconst=xconst, ysum=xksum, yconst=xkconst)\n            Cxy = _M2(X0, Y0k, mask_X=mask_X, mask_Y=mask_Yk,\n                      xsum=sx_centered, xconst=xconst, ysum=yksum, yconst=ykconst)\n            Cyx = _M2(Y0, X0k, mask_X=mask_Y, mask_Y=mask_Xk,\n                      xsum=sy_centered, xconst=yconst, ysum=xksum, yconst=xkconst)\n            Cyy = _M2(Y0, Y0k, mask_X=mask_Y, mask_Y=mask_Yk,\n                      xsum=sy_centered, xconst=yconst, ysum=yksum, yconst=ykconst)\n        else:\n            X0k = X0[:, column_selection]\n            Y0k = Y0[:, column_selection]\n            Cxx = _M2(X0, X0k, mask_X=mask_X, mask_Y=mask_X,\n                      xsum=sx_centered, xconst=xconst,\n                      ysum=sx_centered[column_selection], yconst=xconst)\n            Cxy = _M2(X0, Y0k, mask_X=mask_X, mask_Y=mask_Y,\n                      xsum=sx_centered, xconst=xconst,\n                      ysum=sy_centered[column_selection], yconst=yconst)\n            Cyx = _M2(Y0, X0k, mask_X=mask_Y, mask_Y=mask_X,\n                      xsum=sy_centered, xconst=yconst,\n                      ysum=sx_centered[column_selection], yconst=xconst)\n            Cyy = _M2(Y0, Y0k, mask_X=mask_Y, mask_Y=mask_Y,\n                      xsum=sy_centered, xconst=yconst,\n                      ysum=sy_centered[column_selection], yconst=yconst)\n    else:\n        Cxx = _M2(X0, X0, mask_X=mask_X, mask_Y=mask_X,\n                  xsum=sx_centered, xconst=xconst, ysum=sx_centered, yconst=xconst,\n                  diag_only=diag_only)\n        Cxy = _M2(X0, Y0, mask_X=mask_X, mask_Y=mask_Y,\n                  xsum=sx_centered, xconst=xconst, ysum=sy_centered, yconst=yconst,\n                  diag_only=diag_only)\n        Cyx = Cxy.T\n        Cyy = _M2(Y0, Y0, mask_X=mask_Y, mask_Y=mask_Y,\n                  xsum=sy_centered, xconst=yconst, ysum=sy_centered, yconst=yconst,\n                  diag_only=diag_only)\n\n    return w, (sx, sy), ((Cxx, Cxy), (Cyx, Cyy))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the covariance matrix of X and returns the mean of the new data.", "response": "def covar(X, remove_mean=False, modify_data=False, weights=None, sparse_mode='auto', sparse_tol=0.0):\n    \"\"\" Computes the covariance matrix of X\n\n    Computes\n\n    .. math:\n        C_XX &=& X^\\top X\n\n    while exploiting zero or constant columns in the data matrix.\n    WARNING: Directly use moments_XX if you can. This function does an additional\n    constant-matrix multiplication and does not return the mean.\n\n    Parameters\n    ----------\n    X : ndarray (T, M)\n        Data matrix\n    remove_mean : bool\n        True: remove column mean from the data, False: don't remove mean.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the data matrix X,\n        without creating an independent copy. This option is faster but might\n        lead to surprises because your input array is changed.\n    weights : None or ndarray(T, )\n        weights assigned to each trajectory point of X. If None, all data points have weight one.\n        If ndarray, each data point is assigned a separate weight.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    sparse_tol: float\n        Threshold for considering column to be zero in order to save computing\n        effort when the data is sparse or almost sparse.\n        If max(abs(X[:, i])) < sparse_tol, then row i (and also column i if Y\n        is not given) of the covariance matrix will be set to zero. If Y is\n        given and max(abs(Y[:, i])) < sparse_tol, then column i of the\n        covariance matrix will be set to zero.\n\n    Returns\n    -------\n    C_XX : ndarray (M, M)\n        Covariance matrix of X\n\n    See also\n    --------\n    moments_XX\n\n    \"\"\"\n    w, s, M = moments_XX(X, remove_mean=remove_mean, weights=weights, modify_data=modify_data,\n                         sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    return M / float(w)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef covars(X, Y, remove_mean=False, modify_data=False, symmetrize=False, weights=None, sparse_mode='auto',\n           sparse_tol=0.0):\n    \"\"\" Computes the covariance and cross-covariance matrix of X and Y\n\n    If symmetrize is False, computes\n\n    .. math:\n        C_XX &=& X^\\top X\n        C_XY &=& X^\\top Y\n\n    If symmetrize is True, computes\n\n    .. math:\n        C_XX      &=& \\frac{1}{2} (X^\\top X + Y^\\top Y)\n        C_XY      &=& \\frac{1}{2} (X^\\top Y + Y^\\top X)\n\n    while exploiting zero or constant columns in the data matrix.\n    WARNING: Directly use moments_XXXY if you can. This function does an additional\n    constant-matrix multiplication and does not return the mean.\n\n    Parameters\n    ----------\n    X : ndarray (T, M)\n        Data matrix\n    Y : ndarray (T, N)\n        Second data matrix\n    remove_mean : bool\n        True: remove column mean from the data, False: don't remove mean.\n    modify_data : bool\n        If remove_mean=True, the mean will be removed in the data matrix X,\n        without creating an independent copy. This option is faster but might\n        lead to surprises because your input array is changed.\n    symmetrize : bool\n        Computes symmetrized means and moments (see above)\n    weights : None or ndarray(T, )\n        weights assigned to each trajectory point of X. If None, all data points have weight one.\n        If ndarray, each data point is assigned a separate weight.\n    sparse_mode : str\n        one of:\n            * 'dense' : always use dense mode\n            * 'sparse' : always use sparse mode if possible\n            * 'auto' : automatic\n    sparse_tol: float\n        Threshold for considering column to be zero in order to save computing\n        effort when the data is sparse or almost sparse.\n        If max(abs(X[:, i])) < sparse_tol, then row i (and also column i if Y\n        is not given) of the covariance matrix will be set to zero. If Y is\n        given and max(abs(Y[:, i])) < sparse_tol, then column i of the\n        covariance matrix will be set to zero.\n\n    Returns\n    -------\n    C_XX : ndarray (M, M)\n        Covariance matrix of X\n    C_XY : ndarray (M, N)\n        Covariance matrix of XY\n\n    See also\n    --------\n    moments_XXXY\n\n    \"\"\"\n    w, sx, sy, Mxx, Mxy = moments_XXXY(X, Y, remove_mean=remove_mean, modify_data=modify_data, weights=weights,\n                                       symmetrize=symmetrize, sparse_mode=sparse_mode, sparse_tol=sparse_tol)\n    return Mxx / float(w), Mxy / float(w)", "response": "Computes the covariance and cross - covariance matrix of X and Y."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset up the logging system with the configured (in pyemma.cfg) logging config (logging.yml) @param config: instance of pyemma.config module (wrapper)", "response": "def setup_logging(config, D=None):\n    \"\"\" set up the logging system with the configured (in pyemma.cfg) logging config (logging.yml)\n    @param config: instance of pyemma.config module (wrapper)\n    \"\"\"\n    if not D:\n        import yaml\n\n        args = config.logging_config\n        default = False\n\n        if args.upper() == 'DEFAULT':\n            default = True\n            src = config.default_logging_file\n        else:\n            src = args\n\n        # first try to read configured file\n        try:\n            with open(src) as f:\n                D = yaml.load(f)\n        except EnvironmentError as ee:\n            # fall back to default\n            if not default:\n                try:\n                    with open(config.default_logging_file) as f2:\n                        D = yaml.load(f2)\n                except EnvironmentError as ee2:\n                    raise LoggingConfigurationError('Could not read either configured nor '\n                                                    'default logging configuration!\\n%s' % ee2)\n            else:\n                raise LoggingConfigurationError('could not handle default logging '\n                                                'configuration file\\n%s' % ee)\n\n        if D is None:\n            raise LoggingConfigurationError('Empty logging config! Try using default config by'\n                                            ' setting logging_conf=DEFAULT in pyemma.cfg')\n    assert D\n\n    # this has not been set in PyEMMA version prior 2.0.2+\n    D.setdefault('version', 1)\n    # if the user has not explicitly disabled other loggers, we (contrary to Pythons\n    # default value) do not want to override them.\n    D.setdefault('disable_existing_loggers', False)\n\n    # configure using the dict\n    try:\n        dictConfig(D)\n    except ValueError as ve:\n        # issue with file handler?\n        if 'files' in str(ve) and 'rotating_files' in D['handlers']:\n            print(\"cfg dir\", config.cfg_dir)\n            new_file = os.path.join(config.cfg_dir, 'pyemma.log')\n            warnings.warn(\"set logfile to %s, because there was\"\n                          \" an error writing to the desired one\" % new_file)\n            D['handlers']['rotating_files']['filename'] = new_file\n        else:\n            raise\n        dictConfig(D)\n\n    # get log file name of pyemmas root logger\n    logger = logging.getLogger('pyemma')\n    log_files = [getattr(h, 'baseFilename', None) for h in logger.handlers]\n\n    import atexit\n    @atexit.register\n    def clean_empty_log_files():\n        # gracefully shutdown logging system\n        logging.shutdown()\n        for f in log_files:\n            if f is not None and os.path.exists(f):\n                try:\n                    if os.stat(f).st_size == 0:\n                        os.remove(f)\n                except OSError as o:\n                    print(\"during removal of empty logfiles there was a problem: \", o)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign an item to the trajectory at the given index.", "response": "def trajectory_set_item(self, idx, value):\n    \"\"\"\n    :param self: mdtraj.Trajectory\n    :param idx: possible slices over frames,\n    :param value:\n    :return:\n    \"\"\"\n    import mdtraj\n    assert isinstance(self, mdtraj.Trajectory), type(self)\n    if not isinstance(value, mdtraj.Trajectory):\n        raise TypeError(\"value to assign is of incorrect type(%s). Should be mdtraj.Trajectory\" % type(value))\n    idx = np.index_exp[idx]\n    frames, atoms = None, None\n    if isinstance(idx, (list, tuple)):\n        if len(idx) == 1:\n            frames, atoms = idx[0], slice(None, None, None)\n        if len(idx) == 2:\n            frames, atoms = idx[0], idx[1]\n        if len(idx) >= 3 or len(idx) == 0:\n            raise IndexError(\"invalid slice by %s\" % idx)\n\n    self.xyz[frames, atoms] = value.xyz\n    self._time[frames] = value.time\n    self.unitcell_lengths[frames] = value.unitcell_lengths\n    self.unitcell_angles[frames] = value.unitcell_angles"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninspects getargspec replacement using inspect. signature. getargspec", "response": "def getargspec_no_self(func):\n    \"\"\"inspect.getargspec replacement using inspect.signature.\n\n    inspect.getargspec is deprecated in python 3. This is a replacement\n    based on the (new in python 3.3) `inspect.signature`.\n\n    Parameters\n    ----------\n    func : callable\n        A callable to inspect\n\n    Returns\n    -------\n    argspec : ArgSpec(args, varargs, varkw, defaults)\n        This is similar to the result of inspect.getargspec(func) under\n        python 2.x.\n        NOTE: if the first argument of `func` is self, it is *not*, I repeat\n        *not* included in argspec.args.\n        This is done for consistency between inspect.getargspec() under\n        python 2.x, and inspect.signature() under python 3.x.\n    \"\"\"\n    sig = inspect.signature(func)\n    args = [\n        p.name for p in sig.parameters.values()\n        if p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n    ]\n    varargs = [\n        p.name for p in sig.parameters.values()\n        if p.kind == inspect.Parameter.VAR_POSITIONAL\n    ]\n    varargs = varargs[0] if varargs else None\n    varkw = [\n        p.name for p in sig.parameters.values()\n        if p.kind == inspect.Parameter.VAR_KEYWORD\n    ]\n    varkw = varkw[0] if varkw else None\n    defaults = [\n        p.default for p in sig.parameters.values()\n        if (p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD and\n           p.default is not p.empty)\n    ] or None\n\n    if args[0] == 'self':\n        args.pop(0)\n\n    return ArgSpec(args, varargs, varkw, defaults)"}
